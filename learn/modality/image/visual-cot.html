<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Visual Chain of Thought: Guide AI models through step-by-step visual reasoning by explicitly referencing and analyzing image regions in sequence.">
    <!-- SEO Meta -->
    <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1">
    <meta name="author" content="Praxis Library">
    <meta name="theme-color" content="#DC3545">
    <link rel="canonical" href="https://praxislibrary.com/learn/modality/image/visual-cot.html">
    <!-- Open Graph -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Visual Chain of Thought - Praxis">
    <meta property="og:description" content="Visual Chain of Thought: Guide AI models through step-by-step visual reasoning by explicitly referencing and analyzing image regions in sequence.">
    <meta property="og:url" content="https://praxislibrary.com/learn/modality/image/visual-cot.html">
    <meta property="og:image" content="https://praxislibrary.com/assets/images/praxishome.png">
    <meta property="og:site_name" content="Praxis Library">
    <meta property="og:locale" content="en_US">
    <!-- Social Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Visual Chain of Thought - Praxis">
    <meta name="twitter:description" content="Visual Chain of Thought: Guide AI models through step-by-step visual reasoning by explicitly referencing and analyzing image regions in sequence.">
    <meta name="twitter:image" content="https://praxislibrary.com/assets/images/praxishome.png">
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@graph": [
    {
      "@type": [
        "LearningResource",
        "Article"
      ],
      "headline": "Visual Chain of Thought",
      "name": "Visual Chain of Thought",
      "description": "Visual Chain of Thought: Guide AI models through step-by-step visual reasoning by explicitly referencing and analyzing image regions in sequence.",
      "url": "https://praxislibrary.com/learn/modality/image/visual-cot.html",
      "inLanguage": "en-US",
      "learningResourceType": "Tutorial",
      "educationalLevel": "Beginner to Advanced",
      "educationalUse": "AI Prompt Engineering",
      "isAccessibleForFree": true,
      "publisher": {
        "@type": "EducationalOrganization",
        "name": "Praxis Library",
        "alternateName": "The Open Standard in AI Literacy",
        "url": "https://praxislibrary.com",
        "logo": "https://praxislibrary.com/favicon.svg",
        "description": "A comprehensive, living library of 5,000+ AI terms, 177 techniques & frameworks, and interactive tools. The definitive open resource for AI literacy, prompt engineering, and human-AI communication.",
        "sameAs": [
          "https://www.tiktok.com/@thepraxislibrary",
          "https://www.facebook.com/profile.php?id=61587612308104",
          "https://github.com/PowerOfPraxis/PraxisLibrary"
        ],
        "knowsAbout": [
          "Artificial Intelligence",
          "AI Literacy",
          "Prompt Engineering",
          "AI Prompting Techniques",
          "AI Glossary",
          "Large Language Models",
          "Chain-of-Thought Prompting",
          "AI Education",
          "Human-AI Communication",
          "Neurodivergence and AI",
          "AI Safety",
          "AI Ethics"
        ]
      },
      "isPartOf": {
        "@type": "WebSite",
        "name": "Praxis Library",
        "url": "https://praxislibrary.com"
      },
      "about": [
        {
          "@type": "Thing",
          "name": "Prompt Engineering"
        },
        {
          "@type": "Thing",
          "name": "AI Communication"
        }
      ]
    },
    {
      "@type": "BreadcrumbList",
      "itemListElement": [
        {
          "@type": "ListItem",
          "position": 1,
          "name": "Home",
          "item": "https://praxislibrary.com"
        },
        {
          "@type": "ListItem",
          "position": 2,
          "name": "Discover",
          "item": "https://praxislibrary.com/learn/"
        },
        {
          "@type": "ListItem",
          "position": 3,
          "name": "Modality",
          "item": "https://praxislibrary.com/learn/modality/"
        },
        {
          "@type": "ListItem",
          "position": 4,
          "name": "Image",
          "item": "https://praxislibrary.com/learn/modality/image/"
        },
        {
          "@type": "ListItem",
          "position": 5,
          "name": "Visual Chain of Thought"
        }
      ]
    }
  ]
}
    </script>
    <!-- /SEO -->

<title>Visual Chain of Thought - Praxis</title>
    <link rel="icon" type="image/svg+xml" href="../../../favicon.svg">
    <link rel="stylesheet" href="../../../styles.css">
</head>
<body>
    <a href="#main-content" class="skip-link">Skip to main content</a>

        <header class="header" id="header">
        <div class="header-container">
            <a href="../../../index.html" class="logo">&lt;/Praxis <span>Library</span>&gt;</a>
            <nav class="nav" id="nav" aria-label="Main navigation">
                <a href="../../../foundations/index.html" class="nav-link">History</a>
                <div class="nav-item has-dropdown">
                    <a href="../../index.html" class="nav-link active" aria-expanded="false">Discover</a>
                                        <div class="mega-menu mega-menu--categories">
                        <div class="mega-menu-quick-links">
                            <a href="../../../pages/glossary.html">Glossary</a>
                            <a href="../../index.html">Prompt Engineering</a>
                            <a href="../../prompt-basics.html">Prompt Basics</a>
                            <a href="../../facts-fictions.html">Facts &amp; Fictions</a>
                        </div>
                    </div>
                </div>
                <div class="nav-item has-dropdown">
                    <a href="../../../tools/index.html" class="nav-link" aria-expanded="false">Readiness</a>
                    <div class="mega-menu">
                        <div class="mega-menu-section">
                            <h4>Tools</h4>
                            <a href="../../../quiz/index.html">Readiness Quiz</a>
                            <a href="../../../tools/analyzer.html">Prompt Analyzer</a>
                            <a href="../../../tools/guidance.html">Prompt Builder</a>
                            <a href="../../../tools/matcher.html">Technique Finder</a>
                            <a href="../../../tools/checklist.html">Preflight Checklist</a>
                            <a href="../../../tools/persona.html">Persona Architect</a>
                            <a href="../../../patterns/index.html">Patterns Library</a>
                            <a href="../../../pages/ai-safety.html">AI Safety</a>
                        </div>
                    </div>
                </div>
                <div class="nav-item has-dropdown">
                    <a href="../../../pages/resources.html" class="nav-link" aria-expanded="false">Resources</a>
                    <div class="mega-menu mega-menu--categories">
                        <div class="mega-menu-quick-links">
                            <a href="../../../pages/glossary.html">Glossary</a>
                            <a href="../../../pages/faq.html">FAQ</a>
                            <a href="../../../benchmarks/index.html">AI Benchmarks</a>
                            <a href="../../../pages/responsible-ai.html">Responsible AI</a>
                            <a href="../../../pages/security.html">Security</a>
                            <a href="../../../neurodivergence/resources.html">ND Resources</a>
                            <a href="../../../pages/about.html">About Praxis</a>
                            <a href="../../../pages/audit-report.html">Audit Report</a>
                        </div>
                    </div>
                </div>
            </nav>
            <button class="menu-toggle" id="menuToggle" aria-label="Toggle menu">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </header>



    <main id="main-content">
        <!-- === HERO SECTION === -->
        <section class="page-hero">
            <canvas id="page-hero-neural-bg" class="page-hero-neural-bg"></canvas>
            <div class="container">
                <nav class="breadcrumb fade-in" aria-label="Breadcrumb">
                    <a href="../../../index.html">Home</a>
                    <span class="separator">/</span>
                    <a href="../../index.html">Discover</a>
                    <span class="separator">/</span>
                    <span class="current">Visual CoT</span>
                </nav>
                <div class="hero-badge">
                    <span class="hero-badge__text">Image Techniques</span>
                </div>
                <h1 class="page-title fade-in">Visual Chain of Thought</h1>
                <p class="page-subtitle fade-in">Guide multimodal models to reason through images region by region in explicit visual steps &mdash; decomposing complex scenes into spatial observations that build toward a comprehensive, grounded analysis.</p>
            </div>
        </section>
        <!-- /HERO SECTION -->

        <!-- === HISTORICAL CONTEXT === -->
        <section class="section">
            <div class="container">
                <div class="highlight-box highlight-box--warning fade-in-up">
                    <div class="highlight-box__content">
                        <span class="highlight-box__title">Technique Context: 2023</span>
                        <p><strong>Introduced:</strong> Visual Chain of Thought emerged from research on visual spatial reasoning in 2023, building on earlier work in multimodal chain-of-thought prompting. Unlike standard multimodal CoT, which generates text-only rationales about an image, Visual CoT explicitly focuses on spatial regions, bounding boxes, or visual features as reasoning steps. The model is guided to attend to specific parts of an image sequentially, building understanding incrementally rather than attempting to describe everything at once. This spatial decomposition prevents the model from fixating on the most salient feature while missing subtle but important details elsewhere in the scene.</p>
                        <p><strong>Modern LLM Status:</strong> Frontier multimodal models &mdash; including GPT-4o, Claude, and Gemini &mdash; increasingly support region-based visual reasoning, making Visual CoT patterns significantly more practical than when first proposed. These models can follow instructions to examine specific quadrants, describe spatial relationships, and compare distinct areas of an image. Visual CoT remains a <strong>valuable technique for complex scene analysis</strong> where thoroughness matters more than speed. For simple image descriptions or single-object identification, direct prompting typically suffices, but for multi-element scenes, spatial reasoning tasks, and professional image analysis workflows, Visual CoT delivers measurably more complete and accurate results.</p>
                    </div>
                </div>
            </div>
        </section>
        <!-- /HISTORICAL CONTEXT -->

        <!-- === THE CONCEPT === -->
        <section class="section section-alt">
            <div class="container">
                <div class="split-section split-section--center fade-in-up">
                    <div class="split-section__content">
                        <span class="split-section__badge">The Core Insight</span>
                        <h2 class="split-section__title">Treat Images as Structured Space, Not Flat Input</h2>
                        <p class="split-section__text">When a model receives an image with a simple prompt like &ldquo;What&rsquo;s in this image?&rdquo; it tends to produce a general summary dominated by the most visually prominent feature. A bright red car in the foreground captures all the model&rsquo;s attention while the partially obscured street sign, the pedestrian in the background, and the weather conditions go unmentioned. The model sees the image as a single undifferentiated input and responds with the first thing that &ldquo;pops.&rdquo;</p>
                        <p class="split-section__text"><strong>Visual CoT treats the image as a structured space.</strong> Instead of asking for a single holistic description, it guides the model through a deliberate scan: first look at region A, then analyze region B, then compare the two regions, then synthesize findings into a conclusion. This mirrors how expert analysts actually examine images &mdash; a radiologist scans systematically rather than glancing at the whole image, and a satellite imagery analyst works quadrant by quadrant rather than trying to absorb everything simultaneously.</p>
                        <p class="split-section__text">The result is a more thorough, spatially grounded analysis. By forcing sequential attention to different parts of the image, Visual CoT ensures that no region is overlooked and that the relationships between regions are explicitly considered rather than left to implicit association.</p>
                    </div>
                    <div class="split-section__visual">
                        <div class="highlight-box highlight-box--info">
                            <div class="highlight-box__content">
                                <span class="highlight-box__title">Why Spatial Decomposition Works</span>
                                <p>Human visual attention is naturally sequential &mdash; our eyes fixate on different regions and our brain integrates those observations into understanding. Visual CoT replicates this process for AI models by making the sequential scanning explicit in the prompt. Rather than relying on the model to internally attend to all relevant regions (which it may not do), the prompt structure <strong>guarantees</strong> that each specified region receives dedicated analysis. This turns implicit, unreliable attention into explicit, verifiable coverage.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- /THE CONCEPT -->

        <!-- === HOW IT WORKS === -->
        <section class="section">
            <div class="container">
                <h2 class="section-title fade-in-up">The Visual CoT Process</h2>
                <p class="section-subtitle fade-in-up">Four stages from spatial decomposition to visual synthesis</p>

                <div class="element-timeline fade-in-up">
                    <div class="element-timeline__item">
                        <div class="element-timeline__marker">
                            <span class="element-timeline__number">1</span>
                        </div>
                        <div class="element-timeline__content">
                            <h3 class="element-timeline__title">Identify Visual Regions</h3>
                            <p class="element-timeline__text">Divide the image into semantically meaningful areas. These can be spatial quadrants (top-left, top-right, bottom-left, bottom-right), depth layers (foreground, midground, background), functional zones (navigation area, content area, sidebar), or any other partition that matches the analysis task. The key is that the divisions should be meaningful for the question being asked, not arbitrary.</p>
                            <div class="element-timeline__example">
                                <span class="element-timeline__example-label">Example</span>
                                <p>For a cityscape photograph: &ldquo;Divide this image into three depth layers &mdash; foreground (street level), midground (building facades), and background (skyline and sky).&rdquo;</p>
                            </div>
                        </div>
                    </div>

                    <div class="element-timeline__item">
                        <div class="element-timeline__marker">
                            <span class="element-timeline__number">2</span>
                        </div>
                        <div class="element-timeline__content">
                            <h3 class="element-timeline__title">Sequential Region Analysis</h3>
                            <p class="element-timeline__text">Examine each region independently, describing what is present, noting details, and recording observations without yet trying to form a holistic conclusion. This step prevents premature synthesis and ensures each area gets dedicated analytical attention. The model should describe objects, colors, textures, spatial relationships within the region, and any anomalies or notable features.</p>
                            <div class="element-timeline__example">
                                <span class="element-timeline__example-label">Example</span>
                                <p>&ldquo;For each layer, describe: what objects or elements are present, their condition or state, any text or symbols visible, and the lighting conditions in that region.&rdquo;</p>
                            </div>
                        </div>
                    </div>

                    <div class="element-timeline__item">
                        <div class="element-timeline__marker">
                            <span class="element-timeline__number">3</span>
                        </div>
                        <div class="element-timeline__content">
                            <h3 class="element-timeline__title">Cross-Region Reasoning</h3>
                            <p class="element-timeline__text">Connect observations across regions. This is where Visual CoT produces insights that flat image description misses. Compare elements between regions, identify patterns that span multiple areas, note contradictions or inconsistencies, and trace relationships that cross regional boundaries. This step transforms isolated observations into integrated understanding.</p>
                            <div class="element-timeline__example">
                                <span class="element-timeline__example-label">Example</span>
                                <p>&ldquo;Now compare your observations across layers. How does the lighting in the foreground relate to the sky conditions? Do the building styles suggest a particular era or location? Are there visual elements that connect across layers?&rdquo;</p>
                            </div>
                        </div>
                    </div>

                    <div class="element-timeline__item">
                        <div class="element-timeline__marker">
                            <span class="element-timeline__number">4</span>
                        </div>
                        <div class="element-timeline__content">
                            <h3 class="element-timeline__title">Synthesize Visual Conclusion</h3>
                            <p class="element-timeline__text">Combine all regional analyses and cross-region observations into a coherent, grounded answer to the original question. The synthesis should reference specific regional observations as evidence, creating a conclusion that is traceable back to concrete visual features rather than vague impressions. This final step produces an answer that is both comprehensive and verifiable.</p>
                            <div class="element-timeline__example">
                                <span class="element-timeline__example-label">Example</span>
                                <p>&ldquo;Based on your regional analysis and cross-layer comparisons, provide a complete assessment of this urban scene, citing specific observations from each layer as supporting evidence.&rdquo;</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- /HOW IT WORKS -->

        <!-- === COMPARISON PANEL === -->
        <section class="section section-alt">
            <div class="container">
                <h2 class="section-title fade-in-up">See the Difference</h2>
                <p class="section-subtitle fade-in-up">How spatial decomposition transforms image analysis quality</p>

                <div class="comparison-panel fade-in-up">
                    <div class="comparison-panel__side comparison-panel__side--before">
                        <div class="comparison-panel__header">
                            <span class="comparison-panel__icon">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <circle cx="12" cy="12" r="10"/>
                                    <path d="M12 8v4M12 16h.01"/>
                                </svg>
                            </span>
                            <h3 class="comparison-panel__title">Direct Image Prompt</h3>
                        </div>
                        <div class="comparison-panel__content">
                            <div class="comparison-panel__prompt">
                                <span class="comparison-panel__label">Prompt</span>
                                <p>&ldquo;Describe this satellite image.&rdquo;</p>
                            </div>
                            <div class="comparison-panel__result">
                                <span class="comparison-panel__label">Result</span>
                                <p>The model focuses on the most visually dominant feature &mdash; perhaps a large body of water or a dense urban area &mdash; and produces a brief, surface-level description. Smaller land use patterns, transition zones between urban and agricultural areas, infrastructure networks, and environmental features in less prominent regions are overlooked entirely.</p>
                            </div>
                        </div>
                        <div class="comparison-panel__verdict comparison-panel__verdict--weak">
                            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <circle cx="12" cy="12" r="10"/>
                                <path d="M8 12h8"/>
                            </svg>
                            <span>Salient-feature bias, incomplete coverage, no spatial reasoning</span>
                        </div>
                    </div>

                    <div class="comparison-panel__divider">
                        <span>VS</span>
                    </div>

                    <div class="comparison-panel__side comparison-panel__side--after">
                        <div class="comparison-panel__header">
                            <span class="comparison-panel__icon">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M13 2L3 14h9l-1 8 10-12h-9l1-8z"/>
                                </svg>
                            </span>
                            <h3 class="comparison-panel__title">Visual Chain of Thought</h3>
                        </div>
                        <div class="comparison-panel__content">
                            <div class="comparison-panel__prompt">
                                <span class="comparison-panel__label">Approach</span>
                                <p>Systematically scan quadrants, identify land use patterns in each, note transitions between zones, compare infrastructure density across regions, and synthesize findings into a comprehensive spatial analysis.</p>
                            </div>
                            <div class="comparison-panel__result">
                                <span class="comparison-panel__label">Result</span>
                                <p>The model examines each quadrant independently, identifying agricultural plots in the northwest, suburban development in the northeast, a river system running through the center, and industrial zones in the south. Cross-region analysis reveals the urban-rural gradient and infrastructure corridors connecting zones. The final synthesis maps the complete land use picture.</p>
                            </div>
                        </div>
                        <div class="comparison-panel__verdict comparison-panel__verdict--strong">
                            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <circle cx="12" cy="12" r="10"/>
                                <path d="M8 12l3 3 5-5"/>
                            </svg>
                            <span>Complete spatial coverage, grounded observations, cross-region insights</span>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- /COMPARISON PANEL -->

        <!-- === EXAMPLES IN ACTION === -->
        <section class="section">
            <div class="container">
                <h2 class="section-title fade-in-up">Visual CoT in Action</h2>
                <p class="section-subtitle fade-in-up">Practical applications of region-based visual reasoning</p>

                <div class="accordion fade-in-up" id="visual-cot-accordion">
                    <div class="accordion-item">
                        <button class="accordion-header" aria-expanded="false">
                            <span class="accordion-title">Architectural Blueprint Analysis</span>
                            <span class="accordion-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><polyline points="6 9 12 15 18 9"/></svg></span>
                        </button>
                        <div class="accordion-content">
                            <div class="technique-demo">
                                <div class="technique-demo__prompt">
                                    <span class="technique-demo__tag">Visual CoT Prompt</span>
                                    <p><strong>Step 1 &mdash; Region Identification:</strong> &ldquo;Divide this floor plan into functional zones: entrance and circulation areas, living spaces, private rooms, and utility areas.&rdquo;</p>
                                    <p><strong>Step 2 &mdash; Sequential Analysis:</strong> &ldquo;For each zone, describe: room dimensions and proportions, door and window placements, traffic flow paths, and any structural elements like load-bearing walls.&rdquo;</p>
                                    <p><strong>Step 3 &mdash; Cross-Region Reasoning:</strong> &ldquo;How do the circulation paths connect the zones? Are there any bottlenecks? Does the private zone have adequate separation from living spaces?&rdquo;</p>
                                </div>
                                <div class="technique-demo__response">
                                    <span class="technique-demo__tag">Result</span>
                                    <p>The model produces a comprehensive assessment that identifies the main entry flowing into a central hallway (circulation zone), an open-concept kitchen and living area to the west (living zone), three bedrooms clustered in the northeast (private zone), and laundry and mechanical rooms along the south wall (utility zone). Cross-region analysis reveals that the hallway creates a natural buffer between living and private zones, but notes that the utility area requires passing through the living space &mdash; a potential design concern for noise and traffic flow.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <button class="accordion-header" aria-expanded="false">
                            <span class="accordion-title">Wildlife Photograph Identification</span>
                            <span class="accordion-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><polyline points="6 9 12 15 18 9"/></svg></span>
                        </button>
                        <div class="accordion-content">
                            <div class="technique-demo">
                                <div class="technique-demo__prompt">
                                    <span class="technique-demo__tag">Visual CoT Prompt</span>
                                    <p><strong>Step 1 &mdash; Depth Layers:</strong> &ldquo;Examine this wildlife photograph in three layers: background (habitat and sky), midground (vegetation and terrain), and foreground (primary subjects).&rdquo;</p>
                                    <p><strong>Step 2 &mdash; Layer-by-Layer Analysis:</strong> &ldquo;Background: What biome does the habitat suggest? Midground: What plant species are visible, and what do they indicate about season and climate? Foreground: Describe the animal&rsquo;s physical features, posture, and behavior.&rdquo;</p>
                                    <p><strong>Step 3 &mdash; Cross-Layer Synthesis:</strong> &ldquo;How do the habitat clues from the background and midground help narrow the species identification? Does the animal&rsquo;s behavior correlate with the environmental conditions?&rdquo;</p>
                                </div>
                                <div class="technique-demo__response">
                                    <span class="technique-demo__tag">Result</span>
                                    <p>Background analysis reveals a temperate deciduous forest with autumn coloration, placing the scene in a northern hemisphere location during October or November. Midground vegetation shows mixed oak and maple understory with dried grasses, suggesting forest-edge habitat. The foreground subject displays a compact body, reddish-brown fur with white underside, and a bushy tail held low &mdash; consistent with a red fox. Cross-layer synthesis confirms the identification: red foxes frequently hunt along forest edges during autumn when small mammal activity peaks in preparation for winter, matching both the species characteristics and the observed posture of alert, ground-focused attention.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <button class="accordion-header" aria-expanded="false">
                            <span class="accordion-title">Dashboard Monitoring Analysis</span>
                            <span class="accordion-icon"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><polyline points="6 9 12 15 18 9"/></svg></span>
                        </button>
                        <div class="accordion-content">
                            <div class="technique-demo">
                                <div class="technique-demo__prompt">
                                    <span class="technique-demo__tag">Visual CoT Prompt</span>
                                    <p><strong>Step 1 &mdash; Instrument Groups:</strong> &ldquo;Identify the instrument clusters on this industrial control panel: pressure gauges, temperature indicators, flow meters, and status lights.&rdquo;</p>
                                    <p><strong>Step 2 &mdash; Individual Readings:</strong> &ldquo;For each instrument group, read the current values, note whether each reading is within normal operating range, and flag any instruments showing warning or critical levels.&rdquo;</p>
                                    <p><strong>Step 3 &mdash; Cross-Instrument Correlation:</strong> &ldquo;Do the readings across instrument groups tell a coherent story? For example, does a high pressure reading correlate with an expected temperature change? Are there contradictory readings that might indicate sensor failure?&rdquo;</p>
                                </div>
                                <div class="technique-demo__response">
                                    <span class="technique-demo__tag">Result</span>
                                    <p>Pressure gauges show primary system at 142 PSI (normal range 120&ndash;160) and secondary at 87 PSI (below normal range of 100&ndash;140, flagged). Temperature indicators read 285 degrees on the main line (normal) but the secondary loop shows 195 degrees (below expected 240&ndash;270). Flow meters confirm reduced throughput in the secondary loop. Cross-instrument correlation reveals a consistent pattern: the secondary loop&rsquo;s low pressure, low temperature, and reduced flow all point to a partial blockage or valve restriction in that circuit, rather than individual sensor failures. The status lights confirm one amber warning on the secondary system. Recommended action: inspect the secondary loop isolation valve and check for flow restriction.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- /EXAMPLES IN ACTION -->

        <!-- === WHEN TO USE === -->
        <section class="section section-alt">
            <div class="container">
                <h2 class="section-title fade-in-up">When to Use Visual CoT</h2>
                <p class="section-subtitle fade-in-up">Best for complex scenes where thorough spatial analysis matters</p>

                <div class="split-section fade-in-up">
                    <div class="split-section__content">
                        <h3 class="split-section__subtitle">Perfect For</h3>
                        <div class="feature-list">
                            <div class="feature-list__item feature-list__item--positive">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"/>
                                    <polyline points="22 4 12 14.01 9 11.01"/>
                                </svg>
                                <div>
                                    <strong>Complex Scenes with Multiple Elements</strong>
                                    <p>Images containing many objects, people, or features spread across different regions benefit most from systematic spatial decomposition. Visual CoT ensures every element receives attention rather than just the most prominent ones.</p>
                                </div>
                            </div>
                            <div class="feature-list__item feature-list__item--positive">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"/>
                                    <polyline points="22 4 12 14.01 9 11.01"/>
                                </svg>
                                <div>
                                    <strong>Spatial Reasoning Tasks</strong>
                                    <p>Questions about relative positions, distances, arrangements, or spatial relationships between objects require the model to explicitly map out where things are &mdash; exactly what Visual CoT forces.</p>
                                </div>
                            </div>
                            <div class="feature-list__item feature-list__item--positive">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"/>
                                    <polyline points="22 4 12 14.01 9 11.01"/>
                                </svg>
                                <div>
                                    <strong>Images Where Details Are Easily Missed</strong>
                                    <p>When important information is in the periphery, background, or less visually salient areas, region-by-region scanning prevents the model from overlooking what matters.</p>
                                </div>
                            </div>
                            <div class="feature-list__item feature-list__item--positive">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"/>
                                    <polyline points="22 4 12 14.01 9 11.01"/>
                                </svg>
                                <div>
                                    <strong>Comparison of Image Regions</strong>
                                    <p>Tasks requiring before-and-after analysis, left-versus-right comparison, or identifying differences between areas of the same image are natural fits for cross-region reasoning.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="split-section__content">
                        <h3 class="split-section__subtitle">Skip It When</h3>
                        <div class="feature-list">
                            <div class="feature-list__item feature-list__item--neutral">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <circle cx="12" cy="12" r="10"/>
                                    <line x1="8" y1="12" x2="16" y2="12"/>
                                </svg>
                                <div>
                                    <strong>Images Contain a Single Simple Subject</strong>
                                    <p>A photo of one product, one face, or one clearly defined object does not benefit from spatial decomposition. Direct prompting is faster and equally effective for single-subject images.</p>
                                </div>
                            </div>
                            <div class="feature-list__item feature-list__item--neutral">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <circle cx="12" cy="12" r="10"/>
                                    <line x1="8" y1="12" x2="16" y2="12"/>
                                </svg>
                                <div>
                                    <strong>Pixel-Level Precision Is Needed</strong>
                                    <p>Visual CoT works at a semantic region level, not at pixel-level precision. For tasks requiring exact measurements, pixel counts, or sub-pixel analysis, specialized computer vision tools are more appropriate.</p>
                                </div>
                            </div>
                            <div class="feature-list__item feature-list__item--neutral">
                                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <circle cx="12" cy="12" r="10"/>
                                    <line x1="8" y1="12" x2="16" y2="12"/>
                                </svg>
                                <div>
                                    <strong>Real-Time Processing Is Required</strong>
                                    <p>The multi-step nature of Visual CoT increases token usage and response time. For applications requiring instant image classification or real-time visual processing, the overhead of sequential region analysis is impractical.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- /WHEN TO USE -->

        <!-- === USE CASES === -->
        <section class="section">
            <div class="container">
                <h2 class="section-title fade-in-up">Use Cases</h2>
                <p class="section-subtitle fade-in-up">Where Visual CoT delivers the most value</p>

                <div class="use-case-showcase fade-in-up">
                    <div class="use-case-showcase__item">
                        <div class="use-case-showcase__icon">
                            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <circle cx="12" cy="12" r="10"/>
                                <path d="M2 12h20M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"/>
                            </svg>
                        </div>
                        <h3 class="use-case-showcase__title">Satellite Image Analysis</h3>
                        <p class="use-case-showcase__desc">Systematically scan quadrants of satellite imagery to identify land use patterns, infrastructure networks, environmental changes, and spatial relationships between urban, agricultural, and natural zones that a single-pass description would miss.</p>
                    </div>

                    <div class="use-case-showcase__item">
                        <div class="use-case-showcase__icon">
                            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <path d="M22 12h-4l-3 9L9 3l-3 9H2"/>
                            </svg>
                        </div>
                        <h3 class="use-case-showcase__title">Medical Scan Reading</h3>
                        <p class="use-case-showcase__desc">Guide the model through anatomical regions of an X-ray, MRI, or CT scan, examining each area for abnormalities before synthesizing findings into a differential assessment that accounts for the full image rather than just obvious features.</p>
                    </div>

                    <div class="use-case-showcase__item">
                        <div class="use-case-showcase__icon">
                            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"/>
                                <circle cx="12" cy="12" r="3"/>
                            </svg>
                        </div>
                        <h3 class="use-case-showcase__title">Security Camera Review</h3>
                        <p class="use-case-showcase__desc">Analyze surveillance footage frames by dividing the scene into zones &mdash; entry points, high-traffic areas, restricted zones, and peripheral areas &mdash; to ensure comprehensive coverage of all activity and detect anomalies that occur outside the primary focus area.</p>
                    </div>

                    <div class="use-case-showcase__item">
                        <div class="use-case-showcase__icon">
                            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <rect x="2" y="3" width="20" height="14" rx="2" ry="2"/>
                                <line x1="8" y1="21" x2="16" y2="21"/>
                                <line x1="12" y1="17" x2="12" y2="21"/>
                            </svg>
                        </div>
                        <h3 class="use-case-showcase__title">Manufacturing Quality Control</h3>
                        <p class="use-case-showcase__desc">Inspect product images by examining each surface, seam, and component area independently, then cross-referencing observations to identify defects, misalignments, or surface imperfections that might escape a holistic visual check.</p>
                    </div>

                    <div class="use-case-showcase__item">
                        <div class="use-case-showcase__icon">
                            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"/>
                                <polyline points="9 22 9 12 15 12 15 22"/>
                            </svg>
                        </div>
                        <h3 class="use-case-showcase__title">Urban Planning Assessment</h3>
                        <p class="use-case-showcase__desc">Evaluate aerial or street-level images of urban environments by analyzing infrastructure condition, green space distribution, traffic patterns, and building density across different zones to inform planning decisions with spatially grounded evidence.</p>
                    </div>

                    <div class="use-case-showcase__item">
                        <div class="use-case-showcase__icon">
                            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"/>
                                <polyline points="14 2 14 8 20 8"/>
                                <line x1="16" y1="13" x2="8" y2="13"/>
                                <line x1="16" y1="17" x2="8" y2="17"/>
                            </svg>
                        </div>
                        <h3 class="use-case-showcase__title">Archaeological Site Documentation</h3>
                        <p class="use-case-showcase__desc">Analyze excavation photographs by systematically examining grid squares, stratigraphic layers, and artifact clusters to build a spatially accurate record of findings and their contextual relationships within the site.</p>
                    </div>
                </div>
            </div>
        </section>
        <!-- /USE CASES -->

        <!-- === FRAMEWORK POSITIONING === -->
        <section class="section section-alt">
            <div class="container">
                <h2 class="section-title fade-in-up">Where Visual CoT Fits</h2>
                <p class="section-subtitle fade-in-up">Visual CoT bridges basic image prompting and advanced spatial reasoning</p>

                <div class="evolution-timeline fade-in-up">
                    <div class="era-marker">
                        <span class="era-marker__year">Image Prompting</span>
                        <span class="era-marker__title">Direct Description</span>
                        <span class="era-marker__desc">Single-pass image analysis with flat prompts</span>
                    </div>
                    <div class="era-marker">
                        <span class="era-marker__year">Multimodal CoT</span>
                        <span class="era-marker__title">Text-Based Rationales</span>
                        <span class="era-marker__desc">Step-by-step reasoning about visual content</span>
                    </div>
                    <div class="era-marker era-marker--active">
                        <span class="era-marker__year">Visual CoT</span>
                        <span class="era-marker__title">Region-Based Reasoning</span>
                        <span class="era-marker__desc">Spatial decomposition with sequential analysis</span>
                    </div>
                    <div class="era-marker">
                        <span class="era-marker__year">Spatial Reasoning Agents</span>
                        <span class="era-marker__title">Autonomous Visual Analysis</span>
                        <span class="era-marker__desc">Self-directed region selection and iterative scanning</span>
                    </div>
                </div>

                <div class="callout tip fade-in-up">
                    <div class="callout-title">Combine with Standard CoT</div>
                    <p>Visual CoT and text-based Chain-of-Thought are natural partners. Use Visual CoT to extract spatially grounded observations from the image, then apply standard CoT to <strong>reason about those observations</strong> toward a final answer. For example, Visual CoT might identify that a bridge in quadrant B shows visible corrosion on its support beams and that the water level in quadrant C is unusually high. Standard CoT can then reason about whether these observations together indicate a flood risk, drawing on both the visual evidence and domain knowledge about structural engineering.</p>
                </div>
            </div>
        </section>
        <!-- /FRAMEWORK POSITIONING -->

        <!-- === RELATED FRAMEWORKS === -->
        <section class="section">
            <div class="container">
                <h2 class="section-title fade-in-up">Related Techniques</h2>
                <p class="section-subtitle fade-in-up">Explore complementary visual and reasoning techniques</p>

                <a href="multimodal-cot.html" class="evolution-callout fade-in-up">
                    <span class="evolution-callout__icon">
                        <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"/>
                            <polyline points="14 2 14 8 20 8"/>
                            <line x1="16" y1="13" x2="8" y2="13"/>
                            <line x1="16" y1="17" x2="8" y2="17"/>
                        </svg>
                    </span>
                    <div class="evolution-callout__content">
                        <span class="evolution-callout__label">Foundation</span>
                        <span class="evolution-callout__title">Multimodal CoT</span>
                        <span class="evolution-callout__desc">The broader framework for step-by-step reasoning about multimodal inputs &mdash; Visual CoT specializes this approach by making spatial region analysis the core reasoning structure.</span>
                    </div>
                    <span class="evolution-callout__arrow">
                        <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M5 12h14M12 5l7 7-7 7"/>
                        </svg>
                    </span>
                </a>

                <a href="image-prompting.html" class="evolution-callout fade-in-up">
                    <span class="evolution-callout__icon">
                        <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <rect x="3" y="3" width="18" height="18" rx="2" ry="2"/>
                            <circle cx="8.5" cy="8.5" r="1.5"/>
                            <polyline points="21 15 16 10 5 21"/>
                        </svg>
                    </span>
                    <div class="evolution-callout__content">
                        <span class="evolution-callout__label">Foundation</span>
                        <span class="evolution-callout__title">Image Prompting</span>
                        <span class="evolution-callout__desc">General strategies for prompting models with images &mdash; Visual CoT builds on these fundamentals by adding structured spatial decomposition to the prompting process.</span>
                    </div>
                    <span class="evolution-callout__arrow">
                        <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M5 12h14M12 5l7 7-7 7"/>
                        </svg>
                    </span>
                </a>

                <a href="vqa.html" class="evolution-callout fade-in-up">
                    <span class="evolution-callout__icon">
                        <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"/>
                        </svg>
                    </span>
                    <div class="evolution-callout__content">
                        <span class="evolution-callout__label">Complement</span>
                        <span class="evolution-callout__title">Visual Question Answering</span>
                        <span class="evolution-callout__desc">Question-driven image analysis that pairs naturally with Visual CoT &mdash; use VQA to define what to ask, and Visual CoT to structure how the model reasons through the visual evidence to answer.</span>
                    </div>
                    <span class="evolution-callout__arrow">
                        <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M5 12h14M12 5l7 7-7 7"/>
                        </svg>
                    </span>
                </a>
            </div>
        </section>
        <!-- /RELATED FRAMEWORKS -->

        <!-- === CTA SECTION === -->
        <section class="section">
            <div class="container">
                <div class="cta-corporate cta-corporate--dark fade-in-up">
                    <canvas id="cta-neural-bg" class="cta-corporate__canvas"></canvas>
                    <div class="cta-corporate__content">
                        <h2 class="cta-corporate__title">Reason Through Visual Space</h2>
                        <p class="cta-corporate__text">Apply Visual CoT principles to decompose complex images into structured spatial analyses, or explore our tools to build stronger multimodal prompts from the ground up.</p>
                        <div class="cta-corporate__actions">
                            <a href="../../../tools/guidance.html" class="btn btn-primary">Prompt Builder</a>
                            <a href="../../../foundations/index.html" class="btn btn-secondary">All Foundations</a>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- /CTA SECTION -->
    </main>


    <!-- === FOOTER === -->
        <footer class="footer">
    <canvas id="footer-neural-bg" class="footer-neural-bg"></canvas>
    <div class="container">
        <div class="footer-grid">
            <div class="footer-brand">
                <a href="../../../index.html" class="footer-logo">&lt;/Praxis <span>Library</span>&gt;</a>
                <p>Master the Art of AI Communication theory through proven frameworks.</p>
            </div>

            <div class="footer-links">
                <h4>Techniques</h4>
                <a href="../../prompt-basics.html">Prompt Basics</a>
                <a href="../../crisp.html">CRISP Framework</a>
                <a href="../../crispe.html">CRISPE Framework</a>
                <a href="../../costar.html">CO-STAR Framework</a>
                <a href="../../react.html">ReAct Framework</a>
                <a href="../../flipped-interaction.html">Flipped Interaction</a>
                <a href="../../chain-of-thought.html">Chain-of-Thought</a>
            </div>

            <div class="footer-links">
                <h4>AI Readiness Tools</h4>
                <a href="../../../tools/analyzer.html">Prompt Analyzer</a>
                <a href="../../../tools/matcher.html">Technique Finder</a>
                <a href="../../../tools/checklist.html">Preflight Checklist</a>
                <a href="../../../tools/guidance.html">Prompt Builder</a>
                <a href="../../../tools/persona.html">Persona Architect</a>
                <a href="../../../tools/hallucination.html">Hallucination Spotter</a>
                <a href="../../../quiz/index.html">Readiness Quiz</a>
            </div>

            <div class="footer-links">
                <h4>Resources</h4>
                <a href="../../../patterns/index.html">Patterns Library</a>
                <a href="../../../pages/ai-safety.html">AI Safety</a>
                <a href="../../../pages/responsible-ai.html">Responsible AI</a>
                <a href="../../../pages/faq.html">FAQ</a>
                <a href="../../../pages/glossary.html">Glossary</a>
                <a href="../../../pages/security.html">Security</a>
                <a href="../../../pages/performance.html">Performance</a>
                <a href="../../../pages/about.html">About</a>
            </div>
        </div>

        <div class="footer-bottom">
            <p>AI for Everybody</p>
            <p class="footer-quote">&ldquo;True innovation in AI isn&rsquo;t just about companies adopting AI as a new technology&mdash;it&rsquo;s about people learning about, adapting to, and adopting Artificial Intelligence into their daily lives to empower and unlock their own human potential.&rdquo; <span class="footer-quote-author">&mdash; Basiliso (Bas) Rosario</span></p>
        </div>

        <div class="footer-policies">
            <a href="../../../pages/responsible-ai.html">Responsible AI</a>
            <a href="../../../pages/use-policy.html">Use Policy</a>
            <a href="../../../pages/site-policy.html">Site Policy</a>
            <a href="../../../pages/security-policy.html">Security Policy</a>
            <a href="../../../pages/data-retention-policy.html">Data Retention</a>
        </div>
    </div>
</footer>
    <!-- /FOOTER -->

    <!-- === BACK-TO-TOP === -->
    <button class="back-to-top-bar" aria-label="Back to top">
        <span class="back-to-top-arrow">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                <path d="M18 15l-6-6-6 6"/>
            </svg>
        </span>
        <span class="back-to-top-text">Back to Top</span>
    </button>
    <!-- /BACK-TO-TOP -->

    <!-- === ACCESSIBILITY DASHBOARD === -->

    <!-- =============================================
         BADGE LIGHTBOX - Modal popup for badge info
         ============================================= -->
    <div class="badge-lightbox-overlay" aria-hidden="true"></div>
    <div class="badge-lightbox" role="dialog" aria-modal="true" aria-labelledby="badge-lightbox-title">
        <header class="badge-lightbox-header">
            <h2 class="badge-lightbox-title" id="badge-lightbox-title"></h2>
            <button class="badge-lightbox-close" aria-label="Close dialog">
                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor">
                    <path d="M18 6L6 18M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"/>
                </svg>
            </button>
        </header>
        <div class="badge-lightbox-content"></div>
    </div>
    <!-- /BADGE LIGHTBOX -->

    <div class="adl-dim-overlay" aria-hidden="true"></div>
    <button class="adl-toggle" aria-label="Accessibility options" aria-expanded="false" aria-controls="adl-panel">
        <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <circle cx="12" cy="12" r="10"/>
            <circle cx="12" cy="10" r="3"/>
            <path d="M12 13v6M9 17l3 3 3-3"/>
        </svg>
    </button>
    <div class="adl-panel" id="adl-panel" role="dialog" aria-label="Accessibility Settings">
        <div class="adl-panel-header">
            <span class="adl-panel-title">
                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <circle cx="12" cy="12" r="10"/>
                    <circle cx="12" cy="10" r="3"/>
                    <path d="M12 13v6M9 17l3 3 3-3"/>
                </svg>
                Accessibility
            </span>
            <button class="adl-close" aria-label="Close accessibility panel">
                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M18 6L6 18M6 6l12 12"/>
                </svg>
            </button>
        </div>
        <div class="adl-control">
            <span class="adl-label">Text Size</span>
            <div class="adl-btn-group">
                <button class="adl-btn is-active" data-scale="1" aria-label="Normal text size">1x</button>
                <button class="adl-btn" data-scale="2" aria-label="Large text size">2x</button>
                <button class="adl-btn" data-scale="3" aria-label="Extra large text size">3x</button>
            </div>
        </div>
        <div class="adl-control">
            <div class="adl-switch-wrapper">
                <span class="adl-switch-label">High Contrast</span>
                <label class="adl-switch">
                    <input type="checkbox" id="adl-contrast-toggle" aria-label="Toggle high contrast mode">
                    <span class="adl-switch-track"></span>
                </label>
            </div>
        </div>
        <div class="adl-control adl-readaloud">
            <span class="adl-label">Read Aloud</span>
            <div class="adl-readaloud-controls">
                <button class="adl-play-btn" aria-label="Play or pause reading">
                    <svg class="play-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M8 5v14l11-7z"/></svg>
                    <svg class="pause-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M6 4h4v16H6V4zm8 0h4v16h-4V4z"/></svg>
                </button>
                <div class="adl-speed-group">
                    <button class="adl-speed-btn" data-speed="slow">Slow</button>
                    <button class="adl-speed-btn is-active" data-speed="normal">Normal</button>
                    <button class="adl-speed-btn" data-speed="fast">Fast</button>
                </div>
            </div>
            <div class="adl-reading-indicator"></div>
        </div>
        <div class="adl-control">
            <span class="adl-label">Screen Dimming</span>
            <div class="adl-range-wrapper">
                <input type="range" class="adl-range" id="adl-dim-slider" min="0" max="50" value="0" aria-label="Screen dimming level">
                <span class="adl-range-value">0%</span>
            </div>
        </div>
        <button class="adl-reset" aria-label="Reset accessibility settings to defaults">Reset to Defaults</button>
    </div>
    <!-- /ACCESSIBILITY DASHBOARD -->

    <script src="../../../app.js" defer></script>
</body>
</html>

