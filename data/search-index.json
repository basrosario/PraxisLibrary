[
  {
    "id": "term-a-star-search-algorithm",
    "title": "A* Search Algorithm",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1968",
      "algorithm",
      "becoming",
      "bertram",
      "developed",
      "finds",
      "fundamental",
      "graph",
      "hart",
      "heuristics",
      "history",
      "international"
    ],
    "excerpt": "A graph search algorithm developed by Peter Hart, Nils Nilsson, and Bertram Raphael at SRI International in 1968 that finds the shortest path using heuristics, becoming fundamental to AI planning and pathfinding.",
    "url": "pages/glossary.html#term-a-star-search-algorithm"
  },
  {
    "id": "term-ab-testing",
    "title": "A/B Testing",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "better",
      "compares",
      "controlled",
      "data",
      "determine",
      "experiment",
      "metric",
      "performs",
      "randomized",
      "science",
      "specified",
      "statistics"
    ],
    "excerpt": "A randomized controlled experiment that compares two variants (A and B) to determine which performs better on a specified metric.",
    "url": "pages/glossary.html#term-ab-testing"
  },
  {
    "id": "term-ab-testing-for-llms",
    "title": "A/B Testing for LLMs",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "analysis",
      "comparative",
      "completion",
      "configurations",
      "deployed",
      "determining",
      "different",
      "evaluation",
      "for",
      "language",
      "llms",
      "methodology"
    ],
    "excerpt": "A comparative evaluation methodology where two language model variants or prompt configurations are deployed to different user segments, with statistical analysis of user preference, task completio...",
    "url": "pages/glossary.html#term-ab-testing-for-llms"
  },
  {
    "id": "term-ablation-study",
    "title": "Ablation Study",
    "category": "Glossary",
    "subcategory": "Research",
    "keywords": [
      "ablation",
      "components",
      "contribution",
      "methodology",
      "model",
      "removes",
      "research",
      "study",
      "technique",
      "understand"
    ],
    "excerpt": "A research technique that removes components of a model to understand their contribution. Helps researchers understand which parts of a system are responsible for its capabilities.",
    "url": "pages/glossary.html#term-ablation-study"
  },
  {
    "id": "term-amr",
    "title": "Abstract Meaning Representation",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "abstract",
      "abstracting",
      "acyclic",
      "away",
      "capture",
      "details",
      "directed",
      "encodes",
      "graph",
      "linguistics",
      "meaning",
      "nlp"
    ],
    "excerpt": "A semantic representation that encodes the meaning of a sentence as a rooted directed acyclic graph, abstracting away from syntactic details to capture who did what to whom.",
    "url": "pages/glossary.html#term-amr"
  },
  {
    "id": "term-abstractive-summarization",
    "title": "Abstractive Summarization",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "abstractive",
      "approach",
      "capturing",
      "document",
      "generates",
      "information",
      "key",
      "nlp",
      "novel",
      "original",
      "phrasings",
      "potentially"
    ],
    "excerpt": "A summarization approach that generates novel sentences capturing the key information from the source text, potentially using words and phrasings not present in the original document.",
    "url": "pages/glossary.html#term-abstractive-summarization"
  },
  {
    "id": "term-academic-prompting",
    "title": "Academic Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "academic",
      "approach",
      "argumentation",
      "awareness",
      "citation",
      "contexts",
      "conventions",
      "engineering",
      "evidencebased",
      "follow",
      "formal",
      "including"
    ],
    "excerpt": "A prompting approach tailored for scholarly tasks that instructs the model to follow academic conventions including formal tone, citation awareness, evidence-based reasoning, and structured argumentation suitable for research contexts.",
    "url": "pages/glossary.html#term-academic-prompting"
  },
  {
    "id": "term-accountability-in-ai",
    "title": "Accountability in AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "accountability",
      "ai",
      "answerable",
      "causes",
      "ethics",
      "governance",
      "harm",
      "identifiable",
      "impacts",
      "in",
      "including",
      "individuals"
    ],
    "excerpt": "The principle that identifiable individuals or organizations should be answerable for the outcomes and impacts of AI systems, including mechanisms for redress when AI causes harm.",
    "url": "pages/glossary.html#term-accountability-in-ai"
  },
  {
    "id": "term-accumulated-local-effects",
    "title": "Accumulated Local Effects",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "accumulated",
      "correlated",
      "data",
      "dependence",
      "effects",
      "feature",
      "features",
      "learning",
      "local",
      "machine",
      "method",
      "modelagnostic"
    ],
    "excerpt": "A model-agnostic method for visualizing feature effects that is unbiased in the presence of correlated features, unlike partial dependence plots.",
    "url": "pages/glossary.html#term-accumulated-local-effects"
  },
  {
    "id": "term-accuracy",
    "title": "Accuracy",
    "category": "Glossary",
    "subcategory": "Metrics",
    "keywords": [
      "accuracy",
      "correct",
      "evaluation",
      "measuring",
      "metric",
      "metrics",
      "models",
      "often",
      "predictions"
    ],
    "excerpt": "A metric measuring how often a model's predictions are correct. Calculated as the ratio of correct predictions to total predictions. While intuitive, it can be misleading for imbalanced datasets.",
    "url": "pages/glossary.html#term-accuracy"
  },
  {
    "id": "term-action",
    "title": "Action",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "affects",
      "agent",
      "concepts",
      "core",
      "decision",
      "environment",
      "learning",
      "move",
      "new",
      "reinforcement",
      "state"
    ],
    "excerpt": "A decision or move taken by an RL agent that affects the environment and transitions the system to a new state. Actions can be discrete (finite set of choices) or continuous (real-valued vectors).",
    "url": "pages/glossary.html#term-action"
  },
  {
    "id": "term-action-masking",
    "title": "Action Masking",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "actions",
      "available",
      "concepts",
      "core",
      "invalid",
      "learning",
      "masking",
      "policy",
      "probabilities",
      "reinforcement",
      "restricts"
    ],
    "excerpt": "A technique that restricts the set of available actions at each state by zeroing out invalid action probabilities before policy sampling.",
    "url": "pages/glossary.html#term-action-masking"
  },
  {
    "id": "term-action-recognition",
    "title": "Action Recognition",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "across",
      "action",
      "actions",
      "activities",
      "architectures",
      "classifies",
      "cnns",
      "computer",
      "frames",
      "human",
      "identifies",
      "image"
    ],
    "excerpt": "A video understanding task that identifies and classifies human actions or activities in video sequences, using temporal modeling of motion patterns across frames with architectures like 3D CNNs or video transformers.",
    "url": "pages/glossary.html#term-action-recognition"
  },
  {
    "id": "term-action-repeat",
    "title": "Action Repeat",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "concepts",
      "consecutive",
      "core",
      "decision",
      "effective",
      "environment",
      "executed",
      "frequency",
      "learning",
      "multiple",
      "reducing"
    ],
    "excerpt": "A technique where each selected action is executed for multiple consecutive environment steps, reducing the effective decision frequency.",
    "url": "pages/glossary.html#term-action-repeat"
  },
  {
    "id": "term-action-space",
    "title": "Action Space",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "actions",
      "agent",
      "available",
      "choices",
      "concepts",
      "continuous",
      "core",
      "defined",
      "discrete",
      "finite",
      "learning"
    ],
    "excerpt": "The set of all possible actions available to an RL agent, defined as discrete (finite choices), continuous (real-valued vectors), or multi-discrete.",
    "url": "pages/glossary.html#term-action-space"
  },
  {
    "id": "term-activation-checkpointing",
    "title": "Activation Checkpointing",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "activation",
      "activations",
      "backpropagation",
      "checkpointing",
      "compute",
      "discarding",
      "forward",
      "inference",
      "intermediate",
      "llm",
      "memory",
      "optimization"
    ],
    "excerpt": "A memory optimization technique that trades compute for memory by discarding intermediate activations during the forward pass and recomputing them during backpropagation.",
    "url": "pages/glossary.html#term-activation-checkpointing"
  },
  {
    "id": "term-activation-function",
    "title": "Activation Function",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "activation",
      "applied",
      "complex",
      "enabling",
      "function",
      "introduces",
      "learn",
      "mathematical",
      "network",
      "networks",
      "neural",
      "neurons"
    ],
    "excerpt": "A mathematical function applied to neurons in neural networks that introduces non-linearity, enabling the network to learn complex patterns. Common examples include ReLU, sigmoid, and tanh.",
    "url": "pages/glossary.html#term-activation-function"
  },
  {
    "id": "term-activation-quantization",
    "title": "Activation Quantization",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "activation",
      "activations",
      "arithmetic",
      "bandwidth",
      "enable",
      "inference",
      "infrastructure",
      "int8",
      "intermediate",
      "just",
      "lowerprecision",
      "memory"
    ],
    "excerpt": "The process of quantizing intermediate activations (not just weights) during inference to reduce memory bandwidth requirements and enable INT8 or lower-precision arithmetic.",
    "url": "pages/glossary.html#term-activation-quantization"
  },
  {
    "id": "term-active-learning",
    "title": "Active Learning",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "active",
      "approach",
      "examples",
      "identifies",
      "learn",
      "learning",
      "model",
      "technique",
      "training",
      "unlabeled",
      "valuable"
    ],
    "excerpt": "A training approach where the model identifies which unlabeled examples would be most valuable to learn from. Reduces labeling costs by focusing human effort where it matters most.",
    "url": "pages/glossary.html#term-active-learning"
  },
  {
    "id": "term-active-prompting",
    "title": "Active Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "across",
      "active",
      "annotates",
      "annotation",
      "chainofthought",
      "demonstration",
      "disagreement",
      "effectiveness",
      "engineering",
      "examples",
      "fewshot",
      "identifies"
    ],
    "excerpt": "A method that identifies the most uncertain or informative questions for chain-of-thought annotation by measuring model disagreement across sampled outputs, then selectively annotates those examples to maximize few-shot demonstration effectiveness.",
    "url": "pages/glossary.html#term-active-prompting"
  },
  {
    "id": "term-actor-critic",
    "title": "Actor-Critic",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "actor",
      "architecture",
      "combining",
      "critic",
      "evaluates",
      "learning",
      "network",
      "optimization",
      "policy",
      "reinforcement",
      "selects"
    ],
    "excerpt": "An RL architecture combining a policy network (actor) that selects actions with a value network (critic) that evaluates those actions.",
    "url": "pages/glossary.html#term-actor-critic"
  },
  {
    "id": "term-ada-lovelace",
    "title": "Ada Lovelace",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "18151852",
      "ada",
      "algorithm",
      "analytical",
      "babbages",
      "british",
      "charles",
      "computer",
      "engine",
      "first",
      "history",
      "intended"
    ],
    "excerpt": "British mathematician (1815-1852) who wrote the first published algorithm intended for a machine, working with Charles Babbage's Analytical Engine, and is widely regarded as the first computer programmer.",
    "url": "pages/glossary.html#term-ada-lovelace"
  },
  {
    "id": "term-adaboost",
    "title": "AdaBoost",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "adaboost",
      "assigning",
      "ensemble",
      "examples",
      "focus",
      "hardest",
      "higher",
      "learners",
      "learning",
      "machine",
      "method",
      "misclassified"
    ],
    "excerpt": "An ensemble method that trains weak learners sequentially, assigning higher weights to misclassified samples so that subsequent learners focus on the hardest examples.",
    "url": "pages/glossary.html#term-adaboost"
  },
  {
    "id": "term-adagrad",
    "title": "AdaGrad",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "adagrad",
      "adapts",
      "algorithm",
      "dividing",
      "gradients",
      "historical",
      "individually",
      "learning",
      "machine",
      "optimization",
      "parameter",
      "rate"
    ],
    "excerpt": "An optimization algorithm that adapts the learning rate for each parameter individually by dividing by the square root of the sum of all historical squared gradients.",
    "url": "pages/glossary.html#term-adagrad"
  },
  {
    "id": "term-adam",
    "title": "Adam Optimizer",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "adam",
      "adaptive",
      "algorithm",
      "combining",
      "learning",
      "momentum",
      "optimization",
      "optimizer",
      "popular",
      "rates",
      "training"
    ],
    "excerpt": "A popular optimization algorithm combining momentum with adaptive learning rates. The default choice for training many neural networks due to good performance across tasks.",
    "url": "pages/glossary.html#term-adam"
  },
  {
    "id": "term-adapter-layer",
    "title": "Adapter Layer",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "adapter",
      "additional",
      "architecture",
      "base",
      "efficient",
      "enabling",
      "finetuning",
      "frozen",
      "inserted",
      "layer",
      "layers",
      "learns"
    ],
    "excerpt": "A small trainable module inserted between frozen pretrained layers that learns task-specific transformations with minimal additional parameters, enabling efficient fine-tuning without modifying the base model.",
    "url": "pages/glossary.html#term-adapter-layer"
  },
  {
    "id": "term-additive-attention",
    "title": "Additive Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "additive",
      "architecture",
      "attention",
      "bahdanau",
      "compatibility",
      "computes",
      "concatenation",
      "early",
      "feedforward",
      "key",
      "known",
      "layer"
    ],
    "excerpt": "An attention mechanism that computes compatibility scores by passing the concatenation of query and key through a feedforward layer, also known as Bahdanau attention from its use in early seq2seq models.",
    "url": "pages/glossary.html#term-additive-attention"
  },
  {
    "id": "term-a2c",
    "title": "Advantage Actor-Critic (A2C)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "a2c",
      "action",
      "actor",
      "actorcritic",
      "advantage",
      "critic",
      "difference",
      "function",
      "gradient",
      "learning",
      "method",
      "optimization"
    ],
    "excerpt": "A synchronous variant of the actor-critic method that uses the advantage function (difference between action value and state value) to reduce variance in policy gradient updates.",
    "url": "pages/glossary.html#term-a2c"
  },
  {
    "id": "term-advantage-function",
    "title": "Advantage Function",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "actionvalue",
      "advantage",
      "average",
      "better",
      "compared",
      "current",
      "difference",
      "function",
      "functions",
      "learning",
      "measuring"
    ],
    "excerpt": "The difference A(s,a) = Q(s,a) - V(s) between the action-value and state-value functions, measuring how much better an action is compared to the average action under the current policy.",
    "url": "pages/glossary.html#term-advantage-function"
  },
  {
    "id": "term-advantage-weighted-regression",
    "title": "Advantage-Weighted Regression (AWR)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "advantage",
      "advantages",
      "algorithm",
      "awr",
      "dataset",
      "exponentiated",
      "learning",
      "learns",
      "likelihood",
      "maximum",
      "offline",
      "optimization"
    ],
    "excerpt": "An offline RL algorithm that learns a policy by performing weighted maximum likelihood on a dataset, where the weights are exponentiated advantages.",
    "url": "pages/glossary.html#term-advantage-weighted-regression"
  },
  {
    "id": "term-adversarial-attack",
    "title": "Adversarial Attack",
    "category": "Glossary",
    "subcategory": "Security",
    "keywords": [
      "adversarial",
      "attack",
      "attempts",
      "crafted",
      "deceive",
      "deliberate",
      "inputs",
      "providing",
      "safety",
      "security",
      "specially",
      "systems"
    ],
    "excerpt": "Deliberate attempts to deceive AI systems by providing specially crafted inputs. These can cause models to make incorrect predictions or generate harmful outputs.",
    "url": "pages/glossary.html#term-adversarial-attack"
  },
  {
    "id": "term-adversarial-example-cv",
    "title": "Adversarial Example",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "adversarial",
      "carefully",
      "cause",
      "classifiers",
      "computer",
      "confidence",
      "crafted",
      "example",
      "exposing",
      "high",
      "image",
      "imperceptible"
    ],
    "excerpt": "An input image with carefully crafted, often imperceptible perturbations that cause a vision model to make incorrect predictions with high confidence, exposing vulnerabilities in neural network classifiers.",
    "url": "pages/glossary.html#term-adversarial-example-cv"
  },
  {
    "id": "term-adversarial-prompting",
    "title": "Adversarial Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "adversarial",
      "behave",
      "bypass",
      "causing",
      "contrary",
      "crafting",
      "deliberate",
      "designed",
      "engineering",
      "exploit",
      "filters",
      "harmful"
    ],
    "excerpt": "The deliberate crafting of inputs designed to exploit vulnerabilities in language models, causing them to produce harmful outputs, bypass safety filters, reveal system prompts, or behave contrary to their intended instructions.",
    "url": "pages/glossary.html#term-adversarial-prompting"
  },
  {
    "id": "term-adversarial-training-cv",
    "title": "Adversarial Training",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "adversarial",
      "against",
      "attacks",
      "augments",
      "classify",
      "computer",
      "correctly",
      "defense",
      "examples",
      "image",
      "improving",
      "inputs"
    ],
    "excerpt": "A defense technique that augments training with adversarial examples, teaching vision models to correctly classify perturbed inputs and improving robustness against adversarial attacks.",
    "url": "pages/glossary.html#term-adversarial-training-cv"
  },
  {
    "id": "term-agent",
    "title": "Agent (AI Agent)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "achieve",
      "actions",
      "advanced",
      "agent",
      "ai",
      "architecture",
      "decisions",
      "environment",
      "goals",
      "make",
      "perceive",
      "system"
    ],
    "excerpt": "An AI system that can perceive its environment, make decisions, and take actions to achieve goals. Modern AI agents can use tools, browse the web, execute code, and interact with external systems.",
    "url": "pages/glossary.html#term-agent"
  },
  {
    "id": "term-agent-framework",
    "title": "Agent Framework",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "agent",
      "ai",
      "architecture",
      "autonomously",
      "combining",
      "decisionmaking",
      "enables",
      "execute",
      "framework",
      "generative",
      "iterative",
      "language"
    ],
    "excerpt": "A software architecture that enables LLMs to autonomously plan, reason, and execute multi-step tasks by combining language understanding with tool use, memory, and iterative decision-making.",
    "url": "pages/glossary.html#term-agent-framework"
  },
  {
    "id": "term-agentic-ai",
    "title": "Agentic AI",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "accomplish",
      "actions",
      "advanced",
      "agentic",
      "ai",
      "architecture",
      "autonomously",
      "goals",
      "plan",
      "reason",
      "systems",
      "take"
    ],
    "excerpt": "AI systems that can autonomously plan, reason, and take actions to accomplish goals. Includes tool use, multi-step planning, and self-correction capabilities.",
    "url": "pages/glossary.html#term-agentic-ai"
  },
  {
    "id": "term-agentic-chunking",
    "title": "Agentic Chunking",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "agent",
      "agentic",
      "boundaries",
      "chunk",
      "chunking",
      "chunks",
      "coherent",
      "content",
      "decisions",
      "document",
      "grouping",
      "intelligent"
    ],
    "excerpt": "A document splitting strategy that uses a language model agent to make intelligent decisions about chunk boundaries, content grouping, and chunk summaries, producing semantically coherent chunks that a simple rule-based splitter would miss.",
    "url": "pages/glossary.html#term-agentic-chunking"
  },
  {
    "id": "term-agentic-rag",
    "title": "Agentic RAG",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "agent",
      "agentic",
      "ai",
      "answer",
      "approach",
      "based",
      "complete",
      "decides",
      "dynamically",
      "gathers",
      "generation",
      "generative"
    ],
    "excerpt": "A retrieval-augmented generation approach where an LLM agent dynamically decides what to retrieve, refines queries based on initial results, and iteratively gathers information until it can produce a complete answer.",
    "url": "pages/glossary.html#term-agentic-rag"
  },
  {
    "id": "term-aggregation-bias",
    "title": "Aggregation Bias",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "aggregation",
      "ai",
      "arising",
      "bias",
      "conditional",
      "differ",
      "different",
      "distributions",
      "ethics",
      "fairness",
      "groups",
      "leading"
    ],
    "excerpt": "Bias arising when a single model is used for groups with different conditional distributions, leading to poor performance for subgroups whose patterns differ from the majority population.",
    "url": "pages/glossary.html#term-aggregation-bias"
  },
  {
    "id": "term-agi",
    "title": "AGI (Artificial General Intelligence)",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "agi",
      "artificial",
      "concept",
      "future",
      "general",
      "human",
      "hypothetical",
      "intellectual",
      "intelligence",
      "perform",
      "task"
    ],
    "excerpt": "Hypothetical AI that can perform any intellectual task a human can. Unlike today's narrow AI, AGI would generalize across all domains. A long-term goal and safety concern.",
    "url": "pages/glossary.html#term-agi"
  },
  {
    "id": "term-agi-safety",
    "title": "AGI Safety",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "abilities",
      "across",
      "agi",
      "ai",
      "alignment",
      "artificial",
      "beneficial",
      "cognitive",
      "controllable",
      "domains",
      "ensuring",
      "exceeding"
    ],
    "excerpt": "The subfield of AI safety specifically focused on ensuring that artificial general intelligence, systems matching or exceeding human cognitive abilities across all domains, remains beneficial and controllable.",
    "url": "pages/glossary.html#term-agi-safety"
  },
  {
    "id": "term-ai",
    "title": "AI (Artificial Intelligence)",
    "category": "Glossary",
    "subcategory": "General",
    "keywords": [
      "ai",
      "artificial",
      "computer",
      "content",
      "decisions",
      "designed",
      "generating",
      "human",
      "intelligence",
      "language",
      "making",
      "patterns"
    ],
    "excerpt": "Computer systems designed to perform tasks that typically require human intelligence, such as understanding language, recognizing patterns, making decisions, and generating content.",
    "url": "pages/glossary.html#term-ai"
  },
  {
    "id": "term-ai-alignment-tax",
    "title": "AI Alignment Tax",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "additional",
      "ai",
      "aligned",
      "alignment",
      "capability",
      "compute",
      "cost",
      "development",
      "human",
      "make",
      "performance",
      "representing"
    ],
    "excerpt": "The additional cost in performance, compute, or development time required to make an AI system aligned with human values, representing the trade-off between capability and safety.",
    "url": "pages/glossary.html#term-ai-alignment-tax"
  },
  {
    "id": "term-ai-arms-race",
    "title": "AI Arms Race",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "advanced",
      "ai",
      "arms",
      "capabilities",
      "companies",
      "competitive",
      "considerations",
      "cooperation",
      "develop",
      "dynamic",
      "ethical",
      "expense"
    ],
    "excerpt": "The competitive dynamic between nations or companies racing to develop the most advanced AI capabilities, potentially at the expense of safety research, ethical considerations, and international cooperation.",
    "url": "pages/glossary.html#term-ai-arms-race"
  },
  {
    "id": "term-ai-bill-of-rights",
    "title": "AI Bill of Rights",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "2022",
      "ai",
      "algorithmic",
      "alternatives",
      "bill",
      "blueprint",
      "data",
      "discrimination",
      "explanation",
      "five",
      "governance",
      "house"
    ],
    "excerpt": "The Blueprint for an AI Bill of Rights released by the White House OSTP in 2022, outlining five principles for responsible AI: safe systems, algorithmic discrimination protections, data privacy, notice and explanation, and human alternatives.",
    "url": "pages/glossary.html#term-ai-bill-of-rights"
  },
  {
    "id": "term-ai-boom-2023",
    "title": "AI Boom 2023",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2023",
      "advances",
      "ai",
      "attention",
      "boom",
      "capital",
      "characterized",
      "chatgpt",
      "development",
      "following",
      "funding",
      "generative"
    ],
    "excerpt": "The period of intense investment, development, and public attention in AI following the launch of ChatGPT, characterized by rapid advances in large language models, generative AI, and record venture capital funding.",
    "url": "pages/glossary.html#term-ai-boom-2023"
  },
  {
    "id": "term-ai-consciousness",
    "title": "AI Consciousness",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "awareness",
      "consciousness",
      "consideration",
      "entities",
      "ethical",
      "ethics",
      "experiences",
      "implications",
      "moral",
      "phenomenal",
      "philosophical"
    ],
    "excerpt": "The philosophical and scientific question of whether AI systems can have subjective experiences or phenomenal awareness, with implications for moral consideration and the ethical treatment of AI entities.",
    "url": "pages/glossary.html#term-ai-consciousness"
  },
  {
    "id": "term-ai-containment",
    "title": "AI Containment",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "advanced",
      "ai",
      "airgapping",
      "channels",
      "communication",
      "containment",
      "designed",
      "exerting",
      "external",
      "governance",
      "including",
      "influence"
    ],
    "excerpt": "Strategies and technical measures designed to prevent an advanced AI system from exerting unintended influence on the external world, including air-gapping, sandboxing, and limiting communication channels.",
    "url": "pages/glossary.html#term-ai-containment"
  },
  {
    "id": "term-ai-detection",
    "title": "AI Detection",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "aigenerated",
      "analysis",
      "content",
      "designed",
      "detection",
      "distinguish",
      "distributions",
      "embedded",
      "generative",
      "humancreated",
      "images"
    ],
    "excerpt": "Methods and tools designed to distinguish AI-generated text, images, or media from human-created content, using statistical analysis of token distributions, perplexity patterns, or embedded watermarks.",
    "url": "pages/glossary.html#term-ai-detection"
  },
  {
    "id": "term-ai-digital-divide",
    "title": "AI Digital Divide",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "access",
      "across",
      "ai",
      "digital",
      "divide",
      "economic",
      "ethics",
      "exacerbating",
      "existing",
      "fairness",
      "gap",
      "inequalities"
    ],
    "excerpt": "The gap between those who have access to AI technologies and the skills to use them and those who do not, potentially exacerbating existing social and economic inequalities across and within nations.",
    "url": "pages/glossary.html#term-ai-digital-divide"
  },
  {
    "id": "term-ai-environmental-impact",
    "title": "AI Environmental Impact",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "carbon",
      "center",
      "consumption",
      "cooling",
      "costs",
      "data",
      "deployment",
      "development",
      "electronic",
      "emissions",
      "energy"
    ],
    "excerpt": "The environmental costs of AI development and deployment, including the substantial energy consumption and carbon emissions from training large models, water usage for data center cooling, and electronic waste.",
    "url": "pages/glossary.html#term-ai-environmental-impact"
  },
  {
    "id": "term-ai-ethics",
    "title": "AI Ethics",
    "category": "Glossary",
    "subcategory": "Ethics",
    "keywords": [
      "ai",
      "development",
      "ethics",
      "guide",
      "moral",
      "principles",
      "society",
      "study",
      "systems",
      "use",
      "values"
    ],
    "excerpt": "The study of moral principles and values that should guide the development and use of AI systems. Covers fairness, transparency, privacy, accountability, and societal impact.",
    "url": "pages/glossary.html#term-ai-ethics"
  },
  {
    "id": "term-ai-governance",
    "title": "AI Governance",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "ai",
      "artificial",
      "deployment",
      "development",
      "frameworks",
      "governance",
      "guide",
      "institutional",
      "intelligence",
      "international",
      "levels",
      "national"
    ],
    "excerpt": "The set of policies, regulations, standards, and institutional frameworks that guide the development, deployment, and oversight of artificial intelligence systems at organizational, national, and international levels.",
    "url": "pages/glossary.html#term-ai-governance"
  },
  {
    "id": "term-ai-impact-assessment",
    "title": "AI Impact Assessment",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "ai",
      "analogous",
      "assessment",
      "assessments",
      "deployment",
      "economic",
      "effects",
      "environmental",
      "ethical",
      "ethics",
      "evaluating",
      "governance"
    ],
    "excerpt": "A systematic process for evaluating the potential social, ethical, economic, and environmental effects of an AI system before and during deployment, analogous to environmental impact assessments.",
    "url": "pages/glossary.html#term-ai-impact-assessment"
  },
  {
    "id": "term-ai-incident-database",
    "title": "AI Incident Database",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "behavior",
      "cataloging",
      "caused",
      "database",
      "enable",
      "exhibited",
      "failures",
      "governance",
      "harm",
      "incident",
      "instances"
    ],
    "excerpt": "A repository cataloging real-world instances where AI systems caused harm or exhibited problematic behavior, maintained by organizations like the Partnership on AI to enable learning from failures.",
    "url": "pages/glossary.html#term-ai-incident-database"
  },
  {
    "id": "term-ai-labor-displacement",
    "title": "AI Labor Displacement",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "automation",
      "concerns",
      "depression",
      "displacement",
      "ethics",
      "governance",
      "human",
      "labor",
      "need",
      "obsolescence",
      "occupations"
    ],
    "excerpt": "The phenomenon of AI and automation systems replacing human workers in various occupations, raising concerns about unemployment, wage depression, skill obsolescence, and the need for workforce transition programs.",
    "url": "pages/glossary.html#term-ai-labor-displacement"
  },
  {
    "id": "term-ai-liability-framework",
    "title": "AI Liability Framework",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "ai",
      "bears",
      "cause",
      "debates",
      "determining",
      "directive",
      "eus",
      "framework",
      "frameworks",
      "governance",
      "harm",
      "including"
    ],
    "excerpt": "Legal frameworks determining who bears responsibility when AI systems cause harm, including debates over strict liability, negligence standards, and the EU's AI Liability Directive proposal.",
    "url": "pages/glossary.html#term-ai-liability-framework"
  },
  {
    "id": "term-ai-moratorium",
    "title": "AI Moratorium",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "2023",
      "advocated",
      "ai",
      "capability",
      "certain",
      "development",
      "governance",
      "letter",
      "march",
      "moratorium",
      "notably",
      "open"
    ],
    "excerpt": "A proposed temporary pause on the development of AI systems above a certain capability threshold, notably advocated in the March 2023 open letter signed by prominent researchers and technologists.",
    "url": "pages/glossary.html#term-ai-moratorium"
  },
  {
    "id": "term-ai-personhood",
    "title": "AI Personhood",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "bear",
      "concept",
      "contracts",
      "debated",
      "enabling",
      "enter",
      "ethics",
      "form",
      "governance",
      "granting",
      "hold"
    ],
    "excerpt": "The legal and philosophical concept of granting AI systems some form of legal personality, enabling them to hold rights, enter contracts, or bear liability, as debated in EU and other jurisdictions.",
    "url": "pages/glossary.html#term-ai-personhood"
  },
  {
    "id": "term-ai-readiness",
    "title": "AI Readiness",
    "category": "Glossary",
    "subcategory": "General",
    "keywords": [
      "ai",
      "effectively",
      "knowledge",
      "mindset",
      "needed",
      "readiness",
      "responsibly",
      "skills",
      "tools",
      "use"
    ],
    "excerpt": "The skills, knowledge, and mindset needed to use AI tools effectively and responsibly. Includes understanding both capabilities and limitations.",
    "url": "pages/glossary.html#term-ai-readiness"
  },
  {
    "id": "term-ai-red-lines",
    "title": "AI Red Lines",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "abuse",
      "ai",
      "assist",
      "boundaries",
      "child",
      "clearly",
      "creating",
      "cross",
      "defined",
      "democratic",
      "destruction",
      "ethics"
    ],
    "excerpt": "Clearly defined boundaries that AI systems should never cross, such as refusing to assist with creating weapons of mass destruction, generating child sexual abuse material, or undermining democratic processes.",
    "url": "pages/glossary.html#term-ai-red-lines"
  },
  {
    "id": "term-ai-regulation-timeline",
    "title": "AI Regulation Timeline",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2010s",
      "act",
      "ai",
      "chronological",
      "early",
      "efforts",
      "ethical",
      "executive",
      "governance",
      "guidelines",
      "history",
      "international"
    ],
    "excerpt": "The chronological progression of AI governance efforts from early ethical guidelines in the 2010s through the EU AI Act, US executive orders, and international summits, representing the maturation of AI policy worldwide.",
    "url": "pages/glossary.html#term-ai-regulation-timeline"
  },
  {
    "id": "term-ai-regulatory-sandbox",
    "title": "AI Regulatory Sandbox",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "act",
      "ai",
      "companies",
      "controlled",
      "environment",
      "established",
      "governance",
      "innovative",
      "maintaining",
      "products",
      "provided",
      "regulation"
    ],
    "excerpt": "A controlled environment established by regulators where AI companies can test innovative products under relaxed regulatory requirements while maintaining safeguards, as provided for in the EU AI Act.",
    "url": "pages/glossary.html#term-ai-regulatory-sandbox"
  },
  {
    "id": "term-ai-risk-levels",
    "title": "AI Risk Levels",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "act",
      "ai",
      "applications",
      "categorizes",
      "classification",
      "corresponding",
      "governance",
      "high",
      "levels",
      "limited",
      "minimal",
      "notably"
    ],
    "excerpt": "A classification scheme, notably used in the EU AI Act, that categorizes AI applications into tiers such as unacceptable risk, high risk, limited risk, and minimal risk, with corresponding regulatory requirements for each tier.",
    "url": "pages/glossary.html#term-ai-risk-levels"
  },
  {
    "id": "term-ai-safety",
    "title": "AI Safety",
    "category": "Glossary",
    "subcategory": "Field",
    "keywords": [
      "ai",
      "behave",
      "beneficially",
      "ensuring",
      "field",
      "focused",
      "safely",
      "safety",
      "systems"
    ],
    "excerpt": "The field focused on ensuring AI systems behave safely and beneficially. Includes technical research on alignment, governance, and preventing misuse or unintended harms.",
    "url": "pages/glossary.html#term-ai-safety"
  },
  {
    "id": "term-ai-safety-institute",
    "title": "AI Safety Institute",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "2023",
      "ai",
      "created",
      "dedicated",
      "established",
      "evaluating",
      "first",
      "frontier",
      "governance",
      "governmentbacked",
      "institute",
      "institutes"
    ],
    "excerpt": "A government-backed organization, first established by the UK in 2023, dedicated to evaluating and testing frontier AI models for safety risks, with similar institutes subsequently created by the US and other nations.",
    "url": "pages/glossary.html#term-ai-safety-institute"
  },
  {
    "id": "term-ai-sandboxing",
    "title": "AI Sandboxing",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "access",
      "actuators",
      "ai",
      "environments",
      "evaluation",
      "governance",
      "harm",
      "isolated",
      "limit",
      "networks",
      "potential",
      "practice"
    ],
    "excerpt": "The practice of running AI systems in isolated environments with restricted access to networks, resources, and actuators to limit potential harm during testing and evaluation.",
    "url": "pages/glossary.html#term-ai-sandboxing"
  },
  {
    "id": "term-ai-washing",
    "title": "AI Washing",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "actual",
      "ai",
      "capabilities",
      "companies",
      "consumers",
      "ethics",
      "exaggerating",
      "fabricating",
      "governance",
      "investors",
      "marketing",
      "misleading"
    ],
    "excerpt": "The practice of companies exaggerating or fabricating the role of AI in their products or services for marketing purposes, misleading consumers and investors about the actual capabilities of their technology.",
    "url": "pages/glossary.html#term-ai-washing"
  },
  {
    "id": "term-ai-whistleblowing",
    "title": "AI Whistleblowing",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "20232024",
      "act",
      "ai",
      "capabilities",
      "companies",
      "concerns",
      "dangerous",
      "disclosing",
      "ethics",
      "governance",
      "information",
      "insiders"
    ],
    "excerpt": "The act of insiders at AI companies publicly disclosing information about safety concerns, unethical practices, or dangerous capabilities, as seen in open letters and public statements from AI researchers in 2023-2024.",
    "url": "pages/glossary.html#term-ai-whistleblowing"
  },
  {
    "id": "term-ai-winter",
    "title": "AI Winter",
    "category": "Glossary",
    "subcategory": "Historical",
    "keywords": [
      "ai",
      "expectations",
      "failed",
      "following",
      "funding",
      "historical",
      "industry",
      "interest",
      "periods",
      "reduced",
      "research",
      "winter"
    ],
    "excerpt": "Periods of reduced funding and interest in AI research following failed expectations. Notable winters occurred in the 1970s and late 1980s. The current era is considered an AI boom.",
    "url": "pages/glossary.html#term-ai-winter"
  },
  {
    "id": "term-akaike-information-criterion",
    "title": "Akaike Information Criterion",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "adding",
      "akaike",
      "balances",
      "complexity",
      "criterion",
      "fit",
      "goodness",
      "information",
      "metric",
      "model",
      "number",
      "parameters"
    ],
    "excerpt": "A model selection metric that balances goodness of fit with model complexity by adding a penalty proportional to the number of parameters.",
    "url": "pages/glossary.html#term-akaike-information-criterion"
  },
  {
    "id": "term-alan-turing",
    "title": "Alan Turing",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19121954",
      "alan",
      "bletchley",
      "british",
      "broke",
      "code",
      "computation",
      "enigma",
      "formalized",
      "game",
      "history",
      "imitation"
    ],
    "excerpt": "British mathematician and logician (1912-1954) who formalized computation with the Turing machine, broke the Enigma code at Bletchley Park, and proposed the imitation game as a test for machine intelligence.",
    "url": "pages/glossary.html#term-alan-turing"
  },
  {
    "id": "term-albert",
    "title": "ALBERT",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "albert",
      "architecture",
      "bert",
      "competitive",
      "crosslayer",
      "downstream",
      "embedding",
      "factorized",
      "lite",
      "maintaining",
      "model",
      "networks"
    ],
    "excerpt": "A Lite BERT that reduces model size through factorized embedding parameterization and cross-layer parameter sharing while maintaining competitive performance on downstream tasks.",
    "url": "pages/glossary.html#term-albert"
  },
  {
    "id": "term-alexa-launch",
    "title": "Alexa Launch",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2014",
      "alexa",
      "amazons",
      "ambient",
      "assistants",
      "computing",
      "control",
      "echo",
      "establishing",
      "history",
      "home",
      "launch"
    ],
    "excerpt": "Amazon's launch of Alexa and the Echo smart speaker in November 2014, popularizing voice-activated AI assistants in the home and establishing a major platform for ambient computing and smart home control.",
    "url": "pages/glossary.html#term-alexa-launch"
  },
  {
    "id": "term-alexnet",
    "title": "AlexNet",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2012",
      "alex",
      "alexnet",
      "beginning",
      "competition",
      "convolutional",
      "deep",
      "designed",
      "era",
      "geoffrey",
      "hinton",
      "history"
    ],
    "excerpt": "A deep convolutional neural network designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton that won the 2012 ImageNet competition by a large margin, marking the beginning of the deep learning era in AI.",
    "url": "pages/glossary.html#term-alexnet"
  },
  {
    "id": "term-algorithm",
    "title": "Algorithm",
    "category": "Glossary",
    "subcategory": "Fundamentals",
    "keywords": [
      "accomplishing",
      "algorithm",
      "computer",
      "fundamentals",
      "problem",
      "procedure",
      "rules",
      "science",
      "set",
      "solving",
      "stepbystep",
      "task"
    ],
    "excerpt": "A step-by-step procedure or set of rules for solving a problem or accomplishing a task. In AI, algorithms define how models learn from data and make predictions.",
    "url": "pages/glossary.html#term-algorithm"
  },
  {
    "id": "term-algorithmic-discrimination",
    "title": "Algorithmic Discrimination",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "ai",
      "algorithmic",
      "arising",
      "automated",
      "biased",
      "data",
      "decisionmaking",
      "differential",
      "discrimination",
      "encode",
      "ethics",
      "fairness"
    ],
    "excerpt": "Systematic and unfair differential treatment of individuals or groups by automated decision-making systems, often arising from biased training data, flawed features, or objectives that encode structural inequalities.",
    "url": "pages/glossary.html#term-algorithmic-discrimination"
  },
  {
    "id": "term-algorithmic-impact-assessment",
    "title": "Algorithmic Impact Assessment",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "ai",
      "algorithmic",
      "applications",
      "assess",
      "assessment",
      "automated",
      "communities",
      "decisionmaking",
      "deployment",
      "effects",
      "ethics",
      "evaluation"
    ],
    "excerpt": "A formal evaluation process required in some jurisdictions to assess the potential effects of automated decision-making systems on individuals and communities before deployment, particularly for high-stakes applications.",
    "url": "pages/glossary.html#term-algorithmic-impact-assessment"
  },
  {
    "id": "term-algorithmic-recourse",
    "title": "Algorithmic Recourse",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "ability",
      "actions",
      "affected",
      "ai",
      "algorithmic",
      "automated",
      "change",
      "classification",
      "decisions",
      "different",
      "ethics",
      "fairness"
    ],
    "excerpt": "The ability of individuals affected by automated decisions to take meaningful actions to change the outcome, such as understanding what inputs to modify to receive a different classification.",
    "url": "pages/glossary.html#term-algorithmic-recourse"
  },
  {
    "id": "term-algorithmic-transparency",
    "title": "Algorithmic Transparency",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "affected",
      "ai",
      "algorithm",
      "algorithmic",
      "data",
      "degree",
      "dependencies",
      "ethics",
      "governance",
      "individuals",
      "logic",
      "made"
    ],
    "excerpt": "The degree to which the logic, rules, and data dependencies of an algorithm are made visible and understandable to affected individuals, regulators, and the public.",
    "url": "pages/glossary.html#term-algorithmic-transparency"
  },
  {
    "id": "term-alibi",
    "title": "ALiBi",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "adds",
      "alibi",
      "architecture",
      "attention",
      "bias",
      "biases",
      "directly",
      "distance",
      "embeddings",
      "enabling",
      "encoding",
      "extrapolation"
    ],
    "excerpt": "Attention with Linear Biases, a positional encoding method that adds a linear bias proportional to the distance between key and query positions directly to attention scores, enabling length extrapolation without positional embeddings.",
    "url": "pages/glossary.html#term-alibi"
  },
  {
    "id": "term-alignment",
    "title": "Alignment",
    "category": "Glossary",
    "subcategory": "Safety",
    "keywords": [
      "alignment",
      "behave",
      "challenge",
      "ensuring",
      "human",
      "intentions",
      "match",
      "research",
      "safety",
      "systems",
      "values",
      "ways"
    ],
    "excerpt": "The challenge of ensuring AI systems behave in ways that match human values and intentions. A key concern in AI safety research, involving both technical and philosophical considerations.",
    "url": "pages/glossary.html#term-alignment"
  },
  {
    "id": "term-all-gather",
    "title": "All-Gather Operation",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "all",
      "broadcasts",
      "collective",
      "communication",
      "complete",
      "computing",
      "concatenated",
      "data",
      "dataset",
      "distributed",
      "ends",
      "gather"
    ],
    "excerpt": "A collective communication pattern where each participant broadcasts its data to all others, so every participant ends up with the complete concatenated dataset.",
    "url": "pages/glossary.html#term-all-gather"
  },
  {
    "id": "term-all-reduce",
    "title": "All-Reduce Operation",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "all",
      "collective",
      "communication",
      "computing",
      "contribute",
      "data",
      "distributed",
      "gpus",
      "model",
      "operation",
      "optimization",
      "participating"
    ],
    "excerpt": "A collective communication pattern where all participating GPUs contribute data, perform a reduction operation (typically summation), and receive the result.",
    "url": "pages/glossary.html#term-all-reduce"
  },
  {
    "id": "term-allen-newell",
    "title": "Allen Newell",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19271992",
      "allen",
      "american",
      "cofounded",
      "computer",
      "developed",
      "field",
      "general",
      "herbert",
      "history",
      "hypothesis",
      "logic"
    ],
    "excerpt": "American computer scientist (1927-1992) who, together with Herbert Simon, developed the Logic Theorist and General Problem Solver, pioneered the physical symbol systems hypothesis, and co-founded the field of AI.",
    "url": "pages/glossary.html#term-allen-newell"
  },
  {
    "id": "term-allocative-harm",
    "title": "Allocative Harm",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "across",
      "ai",
      "allocative",
      "based",
      "characteristics",
      "denying",
      "different",
      "distributes",
      "ethics",
      "fairness",
      "groups",
      "harm"
    ],
    "excerpt": "Harm that occurs when an AI system unfairly distributes resources, opportunities, or outcomes across different groups, such as denying loans, jobs, or services based on protected characteristics.",
    "url": "pages/glossary.html#term-allocative-harm"
  },
  {
    "id": "term-alpaca",
    "title": "Alpaca",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "alpaca",
      "created",
      "early",
      "historical",
      "instructiontuned",
      "llama",
      "model",
      "researchers",
      "stanford",
      "version"
    ],
    "excerpt": "An early instruction-tuned version of Llama created by Stanford researchers. Demonstrated that instruction-following could be achieved with synthetic data at low cost.",
    "url": "pages/glossary.html#term-alpaca"
  },
  {
    "id": "term-alpacaeval",
    "title": "AlpacaEval",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "ability",
      "against",
      "agreement",
      "alpacaeval",
      "automatic",
      "benchmarks",
      "compares",
      "costeffective",
      "evaluation",
      "fast",
      "framework",
      "high"
    ],
    "excerpt": "An automatic evaluation framework that compares model outputs against a reference model using LLM-based pairwise judgments, providing a fast and cost-effective proxy for human evaluation of instruc...",
    "url": "pages/glossary.html#term-alpacaeval"
  },
  {
    "id": "term-alphafold",
    "title": "AlphaFold",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2020",
      "alphafold",
      "biology",
      "casp14",
      "deepminds",
      "history",
      "known",
      "milestones",
      "nearly",
      "predicting",
      "prediction",
      "problem"
    ],
    "excerpt": "DeepMind's AI system that solved the protein structure prediction problem, winning CASP14 in 2020 and subsequently predicting structures for nearly all known proteins, revolutionizing structural biology.",
    "url": "pages/glossary.html#term-alphafold"
  },
  {
    "id": "term-alphago",
    "title": "AlphaGo",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "alphago",
      "carlo",
      "champion",
      "combined",
      "deep",
      "deepmind",
      "defeat",
      "learning",
      "monte",
      "networks",
      "neural",
      "planning"
    ],
    "excerpt": "A DeepMind system that combined deep neural networks with Monte Carlo tree search to defeat world champion Go players.",
    "url": "pages/glossary.html#term-alphago"
  },
  {
    "id": "term-alphago-vs-lee-sedol",
    "title": "AlphaGo vs Lee Sedol",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2016",
      "achievement",
      "alphago",
      "champion",
      "complex",
      "considered",
      "deepminds",
      "defeated",
      "due",
      "history",
      "landmark",
      "lee"
    ],
    "excerpt": "The March 2016 match in which DeepMind's AlphaGo defeated world champion Go player Lee Sedol 4-1, a landmark achievement as Go was long considered too complex for AI due to its vast search space.",
    "url": "pages/glossary.html#term-alphago-vs-lee-sedol"
  },
  {
    "id": "term-alphazero",
    "title": "AlphaZero",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "alphago",
      "alphazero",
      "chess",
      "data",
      "entirely",
      "game",
      "generalized",
      "human",
      "learning",
      "learns",
      "planning",
      "play"
    ],
    "excerpt": "A generalized version of AlphaGo that learns to play Go, chess, and shogi entirely through self-play without human game data.",
    "url": "pages/glossary.html#term-alphazero"
  },
  {
    "id": "term-alternative-hypothesis",
    "title": "Alternative Hypothesis",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "aims",
      "alternative",
      "contradicts",
      "detect",
      "difference",
      "effect",
      "hypothesis",
      "inference",
      "null",
      "representing",
      "researcher",
      "statistical"
    ],
    "excerpt": "The hypothesis that contradicts the null hypothesis in statistical testing, typically representing the effect or difference the researcher aims to detect. It can be one-sided or two-sided.",
    "url": "pages/glossary.html#term-alternative-hypothesis"
  },
  {
    "id": "term-amazon-bedrock",
    "title": "Amazon Bedrock",
    "category": "Glossary",
    "subcategory": "Platform",
    "keywords": [
      "accessing",
      "amazon",
      "awss",
      "bedrock",
      "cloud",
      "foundation",
      "managed",
      "models",
      "multiple",
      "platform",
      "providers",
      "service"
    ],
    "excerpt": "AWS's managed service for accessing foundation models from multiple providers. Offers Claude, Llama, and other models through a unified API with enterprise features.",
    "url": "pages/glossary.html#term-amazon-bedrock"
  },
  {
    "id": "term-amd-mi300x",
    "title": "AMD MI300X",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "192gb",
      "accelerator",
      "amd",
      "amds",
      "architecture",
      "cdna",
      "center",
      "competing",
      "data",
      "featuring",
      "gpu",
      "h100"
    ],
    "excerpt": "AMD's data center GPU accelerator featuring 192GB HBM3 memory and CDNA 3 architecture, competing with NVIDIA's H100 for AI training and inference.",
    "url": "pages/glossary.html#term-amd-mi300x"
  },
  {
    "id": "term-analogical-prompting",
    "title": "Analogical Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "analogical",
      "analogous",
      "asks",
      "crafted",
      "engineering",
      "examples",
      "exemplars",
      "fewshot",
      "generate",
      "guide",
      "leveraging",
      "manually"
    ],
    "excerpt": "A technique that asks the model to generate relevant analogous problems and their solutions before tackling the target problem, leveraging self-generated exemplars to guide reasoning without requiring manually crafted few-shot examples.",
    "url": "pages/glossary.html#term-analogical-prompting"
  },
  {
    "id": "term-anaphora-resolution",
    "title": "Anaphora Resolution",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "anaphora",
      "back",
      "coreference",
      "determining",
      "entity",
      "expression",
      "linguistics",
      "mentioned",
      "nlp",
      "points",
      "previously",
      "pronoun"
    ],
    "excerpt": "The task of determining which previously mentioned entity a pronoun or other referring expression points back to in a text, a subproblem of coreference resolution.",
    "url": "pages/glossary.html#term-anaphora-resolution"
  },
  {
    "id": "term-anchor",
    "title": "Anchor (Prompting)",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "ais",
      "anchor",
      "example",
      "format",
      "guides",
      "point",
      "prompt",
      "prompting",
      "reference",
      "response",
      "style",
      "technique"
    ],
    "excerpt": "A reference point or example in a prompt that guides the AI's response style or format. Anchors help establish expectations for output quality and structure.",
    "url": "pages/glossary.html#term-anchor"
  },
  {
    "id": "term-anchor-box",
    "title": "Anchor Box",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "actual",
      "adjust",
      "anchor",
      "aspect",
      "bounding",
      "box",
      "boxes",
      "computer",
      "detection",
      "detectors",
      "feature",
      "fit"
    ],
    "excerpt": "A predefined set of bounding boxes with various aspect ratios and scales placed at each spatial location in a feature map, serving as reference shapes that object detectors adjust to fit actual objects.",
    "url": "pages/glossary.html#term-anchor-box"
  },
  {
    "id": "term-anchoring-bias-in-ai",
    "title": "Anchoring Bias in AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "adjust",
      "ai",
      "aigenerated",
      "ais",
      "anchoring",
      "bias",
      "causing",
      "cognitive",
      "decisions",
      "disproportionately",
      "ethics",
      "fairness"
    ],
    "excerpt": "A cognitive bias where initial AI-generated suggestions disproportionately influence subsequent human decisions, causing users to adjust insufficiently from the AI's initial output.",
    "url": "pages/glossary.html#term-anchoring-bias-in-ai"
  },
  {
    "id": "term-andrew-ng",
    "title": "Andrew Ng",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "andrew",
      "baidu",
      "brain",
      "britishamerican",
      "cofounded",
      "computer",
      "coursera",
      "deeplearning",
      "founded",
      "google",
      "history",
      "led"
    ],
    "excerpt": "British-American computer scientist who co-founded Google Brain, led AI at Baidu, founded Coursera and deeplearning.",
    "url": "pages/glossary.html#term-andrew-ng"
  },
  {
    "id": "term-ann-benchmark",
    "title": "ANN Benchmark",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "across",
      "algorithms",
      "ann",
      "approximate",
      "benchmark",
      "build",
      "comparing",
      "comparison",
      "database",
      "datasets",
      "different",
      "enabling"
    ],
    "excerpt": "Standardized evaluation suites for comparing approximate nearest neighbor algorithms across metrics like recall, queries per second, and index build time on reference datasets, enabling fair perfor...",
    "url": "pages/glossary.html#term-ann-benchmark"
  },
  {
    "id": "term-annotation",
    "title": "Annotation",
    "category": "Glossary",
    "subcategory": "Data",
    "keywords": [
      "annotation",
      "create",
      "data",
      "datasets",
      "labeling",
      "learning",
      "process",
      "supervised",
      "training"
    ],
    "excerpt": "The process of labeling data to create training datasets for supervised learning. Human annotators add labels, categories, or descriptions to raw data like text, images, or audio.",
    "url": "pages/glossary.html#term-annotation"
  },
  {
    "id": "term-annotation-labor-ethics",
    "title": "Annotation Labor Ethics",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "annotation",
      "compensation",
      "concerns",
      "conditions",
      "content",
      "data",
      "disturbing",
      "ethical",
      "ethics",
      "experienced",
      "exposed"
    ],
    "excerpt": "Ethical concerns about the working conditions, compensation, and psychological impacts experienced by data annotation workers who label training data for AI systems, often exposed to disturbing content.",
    "url": "pages/glossary.html#term-annotation-labor-ethics"
  },
  {
    "id": "term-annoy",
    "title": "Annoy",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "access",
      "annoy",
      "approximate",
      "builds",
      "database",
      "datasets",
      "fast",
      "forestoftrees",
      "hyperplane",
      "indexes",
      "libraries",
      "library"
    ],
    "excerpt": "Approximate Nearest Neighbors Oh Yeah, an open-source library by Spotify that builds forest-of-trees indexes using random hyperplane splits for fast approximate nearest neighbor search, optimized f...",
    "url": "pages/glossary.html#term-annoy"
  },
  {
    "id": "term-anomaly-detection",
    "title": "Anomaly Detection",
    "category": "Glossary",
    "subcategory": "ML Task",
    "keywords": [
      "anomaly",
      "application",
      "behavior",
      "conform",
      "data",
      "detection",
      "dont",
      "expected",
      "identifying",
      "ml",
      "outliers",
      "patterns"
    ],
    "excerpt": "Identifying unusual patterns or outliers in data that don't conform to expected behavior. Used in fraud detection, system monitoring, and quality control.",
    "url": "pages/glossary.html#term-anomaly-detection"
  },
  {
    "id": "term-anomaly-detection-images",
    "title": "Anomaly Detection in Images",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "abnormalities",
      "anomaly",
      "computer",
      "defects",
      "detect",
      "detection",
      "identifying",
      "image",
      "images",
      "imaging",
      "in",
      "industrial"
    ],
    "excerpt": "The task of identifying unusual patterns, defects, or out-of-distribution samples in images, widely used in industrial quality inspection and medical imaging to detect abnormalities.",
    "url": "pages/glossary.html#term-anomaly-detection-images"
  },
  {
    "id": "term-anova",
    "title": "ANOVA",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "analysis",
      "anova",
      "betweengroup",
      "comparing",
      "different",
      "fstatistic",
      "groups",
      "inference",
      "means",
      "method",
      "significantly",
      "statistical"
    ],
    "excerpt": "Analysis of Variance, a statistical method that tests whether the means of three or more groups are significantly different by comparing within-group variance to between-group variance using the F-statistic.",
    "url": "pages/glossary.html#term-anova"
  },
  {
    "id": "term-answer-engineering",
    "title": "Answer Engineering",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "answer",
      "designing",
      "elicit",
      "engineering",
      "formats",
      "outputs",
      "prompting",
      "prompts",
      "response",
      "specific",
      "structured",
      "technique"
    ],
    "excerpt": "Designing prompts to elicit specific response formats or structured outputs. Complements prompt engineering by focusing on how answers should be structured.",
    "url": "pages/glossary.html#term-answer-engineering"
  },
  {
    "id": "term-ant-colony-optimization",
    "title": "Ant Colony Optimization",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1992",
      "algorithm",
      "ant",
      "ants",
      "applied",
      "behavior",
      "colony",
      "combinatorial",
      "dorigo",
      "foraging",
      "history",
      "inspired"
    ],
    "excerpt": "A metaheuristic optimization algorithm proposed by Marco Dorigo in 1992, inspired by the foraging behavior of ants using pheromone trails, applied to combinatorial optimization problems like the traveling salesman problem.",
    "url": "pages/glossary.html#term-ant-colony-optimization"
  },
  {
    "id": "term-anthropic",
    "title": "Anthropic",
    "category": "Glossary",
    "subcategory": "Company",
    "keywords": [
      "2021",
      "anthropic",
      "company",
      "former",
      "founded",
      "llm",
      "openai",
      "provider",
      "researchers",
      "safety"
    ],
    "excerpt": "An AI safety company founded in 2021 by former OpenAI researchers. Creator of the Claude family of AI assistants, focused on developing safe and beneficial AI systems.",
    "url": "pages/glossary.html#term-anthropic"
  },
  {
    "id": "term-anthropic-founding",
    "title": "Anthropic Founding",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2021",
      "amodei",
      "anthropic",
      "assistants",
      "claude",
      "company",
      "constitutional",
      "daniela",
      "dario",
      "developed",
      "establishing",
      "family"
    ],
    "excerpt": "The founding of Anthropic in 2021 by former OpenAI researchers Dario and Daniela Amodei, establishing a safety-focused AI company that developed Constitutional AI and the Claude family of AI assistants.",
    "url": "pages/glossary.html#term-anthropic-founding"
  },
  {
    "id": "term-api",
    "title": "API (Application Programming Interface)",
    "category": "Glossary",
    "subcategory": "Technical",
    "keywords": [
      "allows",
      "api",
      "application",
      "applications",
      "communicate",
      "different",
      "integration",
      "interface",
      "programming",
      "protocols",
      "set",
      "software"
    ],
    "excerpt": "A set of protocols that allows different software applications to communicate. AI APIs enable developers to integrate AI capabilities into their applications without building models from scratch.",
    "url": "pages/glossary.html#term-api"
  },
  {
    "id": "term-apple-neural-engine",
    "title": "Apple Neural Engine",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "accelerator",
      "apple",
      "apples",
      "aseries",
      "chips",
      "dedicated",
      "delivering",
      "engine",
      "hardware",
      "inference",
      "infrastructure",
      "integrated"
    ],
    "excerpt": "Apple's dedicated neural network accelerator integrated into Apple Silicon chips (M-series and A-series), delivering up to 38 TOPS for on-device inference.",
    "url": "pages/glossary.html#term-apple-neural-engine"
  },
  {
    "id": "term-appropriate-reliance",
    "title": "Appropriate Reliance",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "appropriate",
      "automation",
      "avoiding",
      "calibrated",
      "capabilities",
      "complacency",
      "ethics",
      "fails",
      "humans",
      "leads",
      "level"
    ],
    "excerpt": "The calibrated level of trust humans should place in AI systems, avoiding both over-reliance that leads to automation complacency and under-reliance that fails to leverage AI capabilities.",
    "url": "pages/glossary.html#term-appropriate-reliance"
  },
  {
    "id": "term-approximate-nearest-neighbor",
    "title": "Approximate Nearest Neighbor",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "accepting",
      "achieving",
      "algorithms",
      "approximate",
      "approximately",
      "class",
      "closest",
      "controllable",
      "database",
      "exact",
      "find",
      "guaranteeing"
    ],
    "excerpt": "A class of search algorithms that find vectors approximately closest to a query vector with high probability rather than guaranteeing exact results, achieving orders-of-magnitude speedups over exac...",
    "url": "pages/glossary.html#term-approximate-nearest-neighbor"
  },
  {
    "id": "term-arc-benchmark",
    "title": "ARC Benchmark",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "ai2",
      "arc",
      "benchmark",
      "benchmarks",
      "challenge",
      "consisting",
      "easy",
      "elementary",
      "evaluation",
      "exam",
      "knowledge",
      "language"
    ],
    "excerpt": "The AI2 Reasoning Challenge, a question-answering benchmark consisting of elementary and middle school science exam questions in easy and challenge sets, testing scientific reasoning and world knowledge in language models.",
    "url": "pages/glossary.html#term-arc-benchmark"
  },
  {
    "id": "term-arcface",
    "title": "ArcFace",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "accurate",
      "adds",
      "angular",
      "arcface",
      "computer",
      "discriminative",
      "embeddings",
      "face",
      "feature",
      "function",
      "identity",
      "image"
    ],
    "excerpt": "A face recognition loss function that adds an angular margin penalty in the normalized feature space, improving the discriminative power of face embeddings for accurate identity verification.",
    "url": "pages/glossary.html#term-arcface"
  },
  {
    "id": "term-architecture-search",
    "title": "Architecture Search (NAS)",
    "category": "Glossary",
    "subcategory": "Research",
    "keywords": [
      "architecture",
      "architectures",
      "automated",
      "discovering",
      "methods",
      "nas",
      "network",
      "neural",
      "optimal",
      "optimization",
      "research",
      "search"
    ],
    "excerpt": "Automated methods for discovering optimal neural network architectures. Can find better designs than human-created networks but requires significant computational resources.",
    "url": "pages/glossary.html#term-architecture-search"
  },
  {
    "id": "term-arena-score",
    "title": "Arena Score",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "across",
      "aggregate",
      "arena",
      "blind",
      "compared",
      "competitive",
      "derived",
      "diverse",
      "evaluation",
      "human",
      "judges",
      "leaderboard"
    ],
    "excerpt": "A model ranking metric derived from competitive evaluation platforms where models are compared in blind pairwise matchups with human judges, producing a leaderboard rating that reflects aggregate model quality across diverse tasks.",
    "url": "pages/glossary.html#term-arena-score"
  },
  {
    "id": "term-arima",
    "title": "ARIMA",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "arima",
      "autoregression",
      "autoregressive",
      "average",
      "class",
      "combining",
      "components",
      "data",
      "differencing",
      "integrated",
      "learning",
      "machine"
    ],
    "excerpt": "AutoRegressive Integrated Moving Average, a class of time series models combining autoregression (AR), differencing for stationarity (I), and moving average (MA) components.",
    "url": "pages/glossary.html#term-arima"
  },
  {
    "id": "term-arithmetic-intensity",
    "title": "Arithmetic Intensity",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "accessed",
      "arithmetic",
      "bandwidth",
      "bytes",
      "computation",
      "compute",
      "determining",
      "floatingpoint",
      "hardware",
      "intensity",
      "limited",
      "memory"
    ],
    "excerpt": "The ratio of floating-point operations to bytes of memory accessed in a computation, determining whether performance is limited by compute or memory bandwidth.",
    "url": "pages/glossary.html#term-arithmetic-intensity"
  },
  {
    "id": "term-arthur-samuel",
    "title": "Arthur Samuel",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19011990",
      "1959",
      "american",
      "arthur",
      "checkersplaying",
      "coining",
      "computer",
      "created",
      "field",
      "gameplaying",
      "history",
      "ibm"
    ],
    "excerpt": "American computer scientist (1901-1990) who created a checkers-playing program at IBM in 1959 that learned through self-play, coining the term machine learning and pioneering the field of game-playing AI.",
    "url": "pages/glossary.html#term-arthur-samuel"
  },
  {
    "id": "term-artificial-neuron",
    "title": "Artificial Neuron",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "artificial",
      "basic",
      "biological",
      "computational",
      "fundamentals",
      "inspired",
      "loosely",
      "networks",
      "neural",
      "neuron",
      "neurons"
    ],
    "excerpt": "The basic computational unit in neural networks, loosely inspired by biological neurons. Computes a weighted sum of inputs, applies an activation function, and outputs the result.",
    "url": "pages/glossary.html#term-artificial-neuron"
  },
  {
    "id": "term-ashish-vaswani",
    "title": "Ashish Vaswani",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2017",
      "architecture",
      "ashish",
      "attention",
      "author",
      "brain",
      "changing",
      "fundamentally",
      "google",
      "history",
      "introduced",
      "language"
    ],
    "excerpt": "Lead author of the 2017 Attention Is All You Need paper that introduced the transformer architecture at Google Brain, fundamentally changing the trajectory of natural language processing and AI research.",
    "url": "pages/glossary.html#term-ashish-vaswani"
  },
  {
    "id": "term-asic-ai",
    "title": "ASIC for AI",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "ai",
      "applicationspecific",
      "asic",
      "circuits",
      "computation",
      "designed",
      "efficiency",
      "energy",
      "exclusively",
      "fixed",
      "for",
      "hardware"
    ],
    "excerpt": "Application-Specific Integrated Circuits designed exclusively for AI computation, offering maximum performance and energy efficiency for fixed workloads.",
    "url": "pages/glossary.html#term-asic-ai"
  },
  {
    "id": "term-asilomar-ai-principles",
    "title": "Asilomar AI Principles",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "2017",
      "advanced",
      "ai",
      "asilomar",
      "beneficial",
      "concerns",
      "conference",
      "covering",
      "developed",
      "ethics",
      "governance",
      "issues"
    ],
    "excerpt": "A set of 23 principles for beneficial AI research developed at the 2017 Asilomar conference, covering research issues, ethics and values, and longer-term concerns about advanced AI safety.",
    "url": "pages/glossary.html#term-asilomar-ai-principles"
  },
  {
    "id": "term-aspect-based-sentiment",
    "title": "Aspect-Based Sentiment Analysis",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "analysis",
      "aspect",
      "aspects",
      "attributes",
      "based",
      "different",
      "distinguishing",
      "entity",
      "features",
      "finegrained",
      "identifies",
      "nlp"
    ],
    "excerpt": "A fine-grained sentiment analysis task that identifies sentiment toward specific aspects or features of an entity, distinguishing different opinions about different attributes within the same text.",
    "url": "pages/glossary.html#term-aspect-based-sentiment"
  },
  {
    "id": "term-assistant-message",
    "title": "Assistant Message",
    "category": "Glossary",
    "subcategory": "API",
    "keywords": [
      "ais",
      "api",
      "apis",
      "assistant",
      "chat",
      "conversation",
      "message",
      "response",
      "technical"
    ],
    "excerpt": "In chat APIs, the AI's response in a conversation. Combined with system and user messages to form the complete conversation context for generating the next response.",
    "url": "pages/glossary.html#term-assistant-message"
  },
  {
    "id": "term-a3c",
    "title": "Asynchronous Advantage Actor-Critic (A3C)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "a3c",
      "actor",
      "actorcritic",
      "advantage",
      "agent",
      "algorithm",
      "asynchronous",
      "asynchronously",
      "copies",
      "critic",
      "environment",
      "instances"
    ],
    "excerpt": "An actor-critic algorithm that runs multiple agent instances in parallel on separate environment copies, each asynchronously updating shared parameters.",
    "url": "pages/glossary.html#term-a3c"
  },
  {
    "id": "term-async-generation",
    "title": "Asynchronous Generation",
    "category": "Glossary",
    "subcategory": "Technical",
    "keywords": [
      "asynchronous",
      "complete",
      "generation",
      "inference",
      "multiple",
      "parallel",
      "production",
      "rather",
      "requests",
      "running",
      "technical",
      "waiting"
    ],
    "excerpt": "Running multiple AI inference requests in parallel rather than waiting for each to complete. Improves throughput for applications handling many concurrent users.",
    "url": "pages/glossary.html#term-async-generation"
  },
  {
    "id": "term-asynchronous-sgd",
    "title": "Asynchronous SGD",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "apply",
      "approach",
      "asynchronous",
      "compute",
      "computing",
      "distributed",
      "gradient",
      "gradients",
      "higher",
      "independently",
      "model",
      "optimization"
    ],
    "excerpt": "A distributed training approach where workers compute and apply gradients independently without waiting for synchronization, trading gradient staleness for higher throughput.",
    "url": "pages/glossary.html#term-asynchronous-sgd"
  },
  {
    "id": "term-atrous-convolution",
    "title": "Atrous Convolution",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "adding",
      "atrous",
      "computer",
      "convolution",
      "dilated",
      "elements",
      "field",
      "filter",
      "gaps",
      "image",
      "increase",
      "inserts"
    ],
    "excerpt": "Also known as dilated convolution, a convolution operation that inserts gaps between filter elements to increase the receptive field without adding parameters or reducing spatial resolution.",
    "url": "pages/glossary.html#term-atrous-convolution"
  },
  {
    "id": "term-attention-head-pruning",
    "title": "Attention Head Pruning",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "compression",
      "computation",
      "head",
      "heads",
      "impact",
      "important",
      "less",
      "mechanism",
      "minimal",
      "model"
    ],
    "excerpt": "A model compression technique that removes redundant or less important attention heads from a multi-head attention mechanism, reducing computation with minimal impact on performance.",
    "url": "pages/glossary.html#term-attention-head-pruning"
  },
  {
    "id": "term-attention-is-all-you-need",
    "title": "Attention Is All You Need",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2017",
      "all",
      "attention",
      "history",
      "is",
      "landmark",
      "milestones",
      "need",
      "paper",
      "vaswani",
      "you"
    ],
    "excerpt": "The landmark 2017 paper by Vaswani et al. that introduced the transformer architecture, replacing recurrence with self-attention mechanisms and enabling the massive scaling that underpins modern large language models.",
    "url": "pages/glossary.html#term-attention-is-all-you-need"
  },
  {
    "id": "term-attention-mask",
    "title": "Attention Mask",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "applied",
      "architecture",
      "attending",
      "attention",
      "binary",
      "causal",
      "certain",
      "float",
      "future",
      "generation",
      "mask",
      "model"
    ],
    "excerpt": "A binary or float tensor applied to attention scores before softmax to prevent the model from attending to certain positions, such as padding tokens or future tokens in causal generation.",
    "url": "pages/glossary.html#term-attention-mask"
  },
  {
    "id": "term-attention",
    "title": "Attention Mechanism",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "allows",
      "architecture",
      "attention",
      "focus",
      "input",
      "mechanism",
      "models",
      "output",
      "parts",
      "producing",
      "relevant",
      "technique"
    ],
    "excerpt": "A technique that allows models to focus on relevant parts of the input when producing output. The foundation of transformer architecture, enabling models to capture long-range dependencies in text.",
    "url": "pages/glossary.html#term-attention"
  },
  {
    "id": "term-attention-mechanism-history",
    "title": "Attention Mechanism History",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "attention",
      "bahdanau",
      "development",
      "history",
      "mechanism",
      "mechanisms",
      "milestones"
    ],
    "excerpt": "The development of attention mechanisms from Bahdanau et al.'s 2014 neural machine translation work through the self-attention innovation in the 2017 transformer paper, which became the foundation of modern large language models.",
    "url": "pages/glossary.html#term-attention-mechanism-history"
  },
  {
    "id": "term-attention-pooling",
    "title": "Attention Pooling",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "aggregate",
      "allowing",
      "architecture",
      "attention",
      "averaging",
      "elements",
      "features",
      "fixed",
      "focus",
      "informative",
      "learned",
      "max"
    ],
    "excerpt": "A pooling mechanism that uses learned attention weights to aggregate features, allowing the model to focus on the most informative elements rather than using fixed averaging or max operations.",
    "url": "pages/glossary.html#term-attention-pooling"
  },
  {
    "id": "term-attention-score",
    "title": "Attention Score",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "another",
      "architecture",
      "attend",
      "attention",
      "compatibility",
      "computed",
      "dot",
      "indicating",
      "key",
      "much",
      "networks",
      "neural"
    ],
    "excerpt": "The raw compatibility value computed between a query and key vector, typically via scaled dot product, before softmax normalization, indicating how much one token should attend to another.",
    "url": "pages/glossary.html#term-attention-score"
  },
  {
    "id": "term-attention-sink",
    "title": "Attention Sink",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "attention",
      "content",
      "discovered",
      "disproportionately",
      "generation",
      "generative",
      "high",
      "important",
      "infinitelength",
      "initial",
      "llm"
    ],
    "excerpt": "A phenomenon where initial tokens in a sequence receive disproportionately high attention scores regardless of content, discovered to be important for maintaining generation quality in streaming and infinite-length settings.",
    "url": "pages/glossary.html#term-attention-sink"
  },
  {
    "id": "term-attention-based-parsing",
    "title": "Attention-Based Parsing",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "achieving",
      "approach",
      "attending",
      "attention",
      "based",
      "constituents",
      "determine",
      "head",
      "mechanisms",
      "networks",
      "neural",
      "nlp"
    ],
    "excerpt": "A parsing approach that uses attention mechanisms from neural networks to determine syntactic structure, often achieving state-of-the-art results by attending over possible head words or constituents.",
    "url": "pages/glossary.html#term-attention-based-parsing"
  },
  {
    "id": "term-attention-based-policy",
    "title": "Attention-Based Policy",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "architecture",
      "attention",
      "based",
      "concepts",
      "core",
      "focus",
      "learning",
      "mechanisms",
      "memory",
      "observation",
      "parts",
      "policy"
    ],
    "excerpt": "An RL policy architecture that uses attention mechanisms to selectively focus on relevant parts of the observation or memory.",
    "url": "pages/glossary.html#term-attention-based-policy"
  },
  {
    "id": "term-attention-based-translation",
    "title": "Attention-Based Translation",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "approach",
      "attends",
      "attention",
      "based",
      "bottleneck",
      "decoder",
      "different",
      "eliminating",
      "encoding",
      "fixedlength",
      "generation",
      "information"
    ],
    "excerpt": "A neural machine translation approach where the decoder attends to different parts of the source sentence at each generation step, eliminating the information bottleneck of fixed-length encoding.",
    "url": "pages/glossary.html#term-attention-based-translation"
  },
  {
    "id": "term-auc",
    "title": "AUC",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "across",
      "area",
      "auc",
      "classifier",
      "curve",
      "learning",
      "machine",
      "metric",
      "metrics",
      "performance",
      "roc",
      "scalar"
    ],
    "excerpt": "Area Under the ROC Curve, a scalar metric summarizing classifier performance across all thresholds. An AUC of 1.",
    "url": "pages/glossary.html#term-auc"
  },
  {
    "id": "term-audio-generation",
    "title": "Audio Generation",
    "category": "Glossary",
    "subcategory": "Application",
    "keywords": [
      "application",
      "audio",
      "creates",
      "effects",
      "generation",
      "generative",
      "inputs",
      "music",
      "sound",
      "speech",
      "text"
    ],
    "excerpt": "AI that creates speech, music, or sound effects from text or other inputs. Includes text-to-speech (TTS), music generation, and sound design applications.",
    "url": "pages/glossary.html#term-audio-generation"
  },
  {
    "id": "term-auditability",
    "title": "Auditability",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "algorithms",
      "allows",
      "assess",
      "auditability",
      "compliance",
      "criteria",
      "data",
      "decisionmaking",
      "ethics",
      "examine",
      "fairness"
    ],
    "excerpt": "The property of an AI system that allows independent third parties to examine its data, algorithms, models, and decision-making processes to assess compliance with standards, fairness criteria, and regulatory requirements.",
    "url": "pages/glossary.html#term-auditability"
  },
  {
    "id": "term-augmentation",
    "title": "Augmentation",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "augmentation",
      "creating",
      "data",
      "examples",
      "existing",
      "expanding",
      "modified",
      "training",
      "versions"
    ],
    "excerpt": "Expanding training data by creating modified versions of existing examples. In text: paraphrasing, back-translation. In images: rotation, cropping, color changes.",
    "url": "pages/glossary.html#term-augmentation"
  },
  {
    "id": "term-augmented-dickey-fuller-test",
    "title": "Augmented Dickey-Fuller Test",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "augmented",
      "data",
      "determining",
      "dickey",
      "fuller",
      "indicate",
      "nonstationarity",
      "present",
      "root",
      "science",
      "series",
      "statistical"
    ],
    "excerpt": "A statistical test for determining whether a unit root is present in a time series, which would indicate non-stationarity.",
    "url": "pages/glossary.html#term-augmented-dickey-fuller-test"
  },
  {
    "id": "term-auto-complete",
    "title": "Auto-Complete",
    "category": "Glossary",
    "subcategory": "Application",
    "keywords": [
      "application",
      "auto",
      "complete",
      "feature",
      "predicts",
      "suggests",
      "text",
      "type",
      "you"
    ],
    "excerpt": "AI feature that predicts and suggests text as you type. Powers writing assistants, code completion, and search suggestions. Based on language model predictions.",
    "url": "pages/glossary.html#term-auto-complete"
  },
  {
    "id": "term-autoaugment",
    "title": "AutoAugment",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "augmentation",
      "autoaugment",
      "automated",
      "combinations",
      "computer",
      "dataset",
      "find",
      "given",
      "image",
      "learning",
      "magnitudes",
      "method"
    ],
    "excerpt": "An automated augmentation policy search method that uses reinforcement learning to find optimal combinations and magnitudes of augmentation operations for a given dataset and task.",
    "url": "pages/glossary.html#term-autoaugment"
  },
  {
    "id": "term-autocorrelation",
    "title": "Autocorrelation",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "autocorrelation",
      "correlation",
      "data",
      "itself",
      "lagged",
      "science",
      "series",
      "statistics",
      "time",
      "version"
    ],
    "excerpt": "The correlation of a time series with a lagged version of itself. In regression, autocorrelated residuals violate the independence assumption and can lead to inefficient estimates and unreliable hypothesis tests.",
    "url": "pages/glossary.html#term-autocorrelation"
  },
  {
    "id": "term-autocorrelation-function",
    "title": "Autocorrelation Function",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "autocorrelation",
      "correlation",
      "data",
      "delays",
      "function",
      "itself",
      "lagged",
      "measures",
      "science",
      "series",
      "statistics",
      "time"
    ],
    "excerpt": "A function that measures the correlation between a time series and a lagged version of itself at various time delays.",
    "url": "pages/glossary.html#term-autocorrelation-function"
  },
  {
    "id": "term-autoencoder",
    "title": "Autoencoder",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "autoencoder",
      "compress",
      "data",
      "learns",
      "network",
      "neural",
      "reconstruct",
      "representation",
      "smaller",
      "unsupervised"
    ],
    "excerpt": "A neural network that learns to compress data into a smaller representation and then reconstruct it. Used for dimensionality reduction, denoising, and learning efficient data representations.",
    "url": "pages/glossary.html#term-autoencoder"
  },
  {
    "id": "term-autogen",
    "title": "AutoGen",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "application",
      "applications",
      "autogen",
      "building",
      "framework",
      "microsofts",
      "multiagent"
    ],
    "excerpt": "Microsoft's framework for building multi-agent AI applications. Enables conversations between multiple AI agents that can collaborate, debate, and solve complex problems together.",
    "url": "pages/glossary.html#term-autogen"
  },
  {
    "id": "term-automatic-chain-of-thought",
    "title": "Automatic Chain-of-Thought",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "annotation",
      "automatic",
      "automatically",
      "chain",
      "chainofthought",
      "chains",
      "clustering",
      "constructs",
      "demonstrations",
      "eliminating",
      "engineering",
      "examples"
    ],
    "excerpt": "A method that automatically constructs chain-of-thought demonstrations by clustering questions and selecting representative examples, then using the model to generate reasoning chains, eliminating the need for manual rationale annotation.",
    "url": "pages/glossary.html#term-automatic-chain-of-thought"
  },
  {
    "id": "term-automatic-prompt-engineer",
    "title": "Automatic Prompt Engineer",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "ape",
      "automated",
      "automatic",
      "candidates",
      "effectively",
      "engineer",
      "engineering",
      "find",
      "generate",
      "given",
      "highperforming",
      "instructions"
    ],
    "excerpt": "An automated method (APE) that uses language models to generate, score, and select optimal prompt instructions for a given task, effectively searching the space of possible prompts to find high-performing candidates without manual engineering.",
    "url": "pages/glossary.html#term-automatic-prompt-engineer"
  },
  {
    "id": "term-asr",
    "title": "Automatic Speech Recognition",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "acoustic",
      "algorithms",
      "audio",
      "automatic",
      "converts",
      "decoding",
      "language",
      "models",
      "nlp",
      "processing",
      "recognition",
      "signals"
    ],
    "excerpt": "The technology that converts spoken language audio into text, using acoustic models, language models, and decoding algorithms to transcribe speech signals.",
    "url": "pages/glossary.html#term-asr"
  },
  {
    "id": "term-automation-bias",
    "title": "Automation Bias",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "accept",
      "ai",
      "automated",
      "automation",
      "bias",
      "contradicted",
      "critical",
      "ethics",
      "evaluation",
      "even",
      "evidence",
      "human"
    ],
    "excerpt": "The human tendency to over-rely on automated systems and accept their outputs without sufficient critical evaluation, even when contradicted by other evidence.",
    "url": "pages/glossary.html#term-automation-bias"
  },
  {
    "id": "term-auto-ml",
    "title": "AutoML",
    "category": "Glossary",
    "subcategory": "Tools",
    "keywords": [
      "automated",
      "automation",
      "automl",
      "engineering",
      "feature",
      "handle",
      "hyperparameter",
      "learning",
      "machine",
      "model",
      "selection",
      "tools"
    ],
    "excerpt": "Automated machine learning tools that handle model selection, hyperparameter tuning, and feature engineering. Makes ML accessible to non-experts and speeds up development.",
    "url": "pages/glossary.html#term-auto-ml"
  },
  {
    "id": "term-autonomous-weapons-systems",
    "title": "Autonomous Weapons Systems",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "accountability",
      "ai",
      "autonomous",
      "decisions",
      "direct",
      "engage",
      "ethical",
      "ethics",
      "human",
      "intervention",
      "judgment",
      "legal"
    ],
    "excerpt": "Weapons systems that can select and engage targets without direct human intervention, raising profound ethical and legal questions about accountability, proportionality, and the role of human judgment in lethal decisions.",
    "url": "pages/glossary.html#term-autonomous-weapons-systems"
  },
  {
    "id": "term-autoregressive",
    "title": "Autoregressive Model",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "autoregressive",
      "element",
      "generates",
      "llm",
      "model",
      "next",
      "one",
      "output",
      "outputs",
      "predict",
      "previous"
    ],
    "excerpt": "A model that generates output one element at a time, using previous outputs to predict the next. Most LLMs are autoregressive, generating text token by token from left to right.",
    "url": "pages/glossary.html#term-autoregressive"
  },
  {
    "id": "term-autoregressive-model",
    "title": "Autoregressive Model",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "autoregressive",
      "combination",
      "current",
      "data",
      "linear",
      "model",
      "noise",
      "own",
      "past",
      "plus",
      "predicts",
      "science"
    ],
    "excerpt": "A time series model that predicts the current value as a linear combination of its own past values plus a noise term. The order p specifies how many lagged values are used as predictors.",
    "url": "pages/glossary.html#term-autoregressive-model"
  },
  {
    "id": "term-auxiliary-loss",
    "title": "Auxiliary Loss",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "added",
      "additional",
      "advanced",
      "auxiliary",
      "help",
      "learning",
      "loss",
      "terms",
      "training"
    ],
    "excerpt": "Additional loss terms added during training to help learning. Can improve training stability, add regularization, or encourage specific behaviors in the model.",
    "url": "pages/glossary.html#term-auxiliary-loss"
  },
  {
    "id": "term-auxiliary-task-rl",
    "title": "Auxiliary Task in RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "additional",
      "alongside",
      "auxiliary",
      "concepts",
      "control",
      "core",
      "improve",
      "in",
      "learning",
      "main",
      "objective",
      "prediction"
    ],
    "excerpt": "An additional prediction or control objective trained alongside the main RL objective to improve representation learning.",
    "url": "pages/glossary.html#term-auxiliary-task-rl"
  },
  {
    "id": "term-average-pooling",
    "title": "Average Pooling",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "average",
      "computing",
      "data",
      "dimensionality",
      "pooling",
      "reduces",
      "regions",
      "technique"
    ],
    "excerpt": "A technique that reduces data dimensionality by computing the average of regions. Used in CNNs and for creating fixed-size representations from variable-length sequences.",
    "url": "pages/glossary.html#term-average-pooling"
  },
  {
    "id": "term-average-precision",
    "title": "Average Precision",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "average",
      "computed",
      "curve",
      "increase",
      "learning",
      "machine",
      "mean",
      "metrics",
      "precision",
      "precisionrecall",
      "precisions",
      "recall"
    ],
    "excerpt": "A single-number summary of the precision-recall curve, computed as the weighted mean of precisions at each threshold with the increase in recall as the weight.",
    "url": "pages/glossary.html#term-average-precision"
  },
  {
    "id": "term-awq",
    "title": "AWQ",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "activation",
      "activationaware",
      "awq",
      "based",
      "channels",
      "efficient",
      "enabling",
      "identifies",
      "inference",
      "language",
      "large",
      "llm"
    ],
    "excerpt": "Activation-aware Weight Quantization, a method that identifies and preserves salient weight channels based on activation magnitudes, enabling efficient low-bit quantization of large language models.",
    "url": "pages/glossary.html#term-awq"
  },
  {
    "id": "term-aws-inferentia",
    "title": "AWS Inferentia",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "accelerator",
      "amazons",
      "aws",
      "chip",
      "cloud",
      "designed",
      "hardware",
      "highthroughput",
      "inference",
      "inferentia",
      "infrastructure",
      "lowcost"
    ],
    "excerpt": "Amazon's purpose-built inference accelerator chip designed for high-throughput, low-cost ML inference in the cloud.",
    "url": "pages/glossary.html#term-aws-inferentia"
  },
  {
    "id": "term-aws-sagemaker",
    "title": "AWS SageMaker",
    "category": "Glossary",
    "subcategory": "Platform",
    "keywords": [
      "amazons",
      "aws",
      "building",
      "cloud",
      "deploying",
      "models",
      "platform",
      "sagemaker",
      "training"
    ],
    "excerpt": "Amazon's ML platform for building, training, and deploying models. Provides infrastructure, tools, and pre-built algorithms for the complete ML lifecycle.",
    "url": "pages/glossary.html#term-aws-sagemaker"
  },
  {
    "id": "term-aws-trainium",
    "title": "AWS Trainium",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "accelerator",
      "alternatives",
      "amazons",
      "aws",
      "chip",
      "computing",
      "costeffective",
      "custom",
      "deep",
      "distributed",
      "gpus",
      "hardware"
    ],
    "excerpt": "Amazon's custom AI training accelerator chip offering high-performance, cost-effective alternatives to NVIDIA GPUs for deep learning training.",
    "url": "pages/glossary.html#term-aws-trainium"
  },
  {
    "id": "term-azure-openai",
    "title": "Azure OpenAI Service",
    "category": "Glossary",
    "subcategory": "Platform",
    "keywords": [
      "azure",
      "cloud",
      "enterprise",
      "microsofts",
      "models",
      "offering",
      "openai",
      "platform",
      "service"
    ],
    "excerpt": "Microsoft's enterprise offering of OpenAI models through Azure cloud. Provides GPT-4, ChatGPT, and DALL-E with enterprise security, compliance, and regional availability.",
    "url": "pages/glossary.html#term-azure-openai"
  },
  {
    "id": "term-back-translation",
    "title": "Back-Translation",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "augmentation",
      "back",
      "creating",
      "data",
      "language",
      "machine",
      "model",
      "monolingual",
      "nlp",
      "parallel",
      "processing",
      "reverse"
    ],
    "excerpt": "A data augmentation technique for machine translation that translates monolingual target-language text back to the source language using a reverse model, creating synthetic parallel training data.",
    "url": "pages/glossary.html#term-back-translation"
  },
  {
    "id": "term-background-removal",
    "title": "Background Removal",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "automatically",
      "background",
      "computer",
      "deep",
      "ecommerce",
      "foreground",
      "image",
      "images",
      "learning",
      "matting",
      "models",
      "photography"
    ],
    "excerpt": "The process of automatically separating foreground subjects from their background in images using deep learning segmentation and matting models, widely used in photography and e-commerce.",
    "url": "pages/glossary.html#term-background-removal"
  },
  {
    "id": "term-backpropagation",
    "title": "Backpropagation",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "algorithm",
      "backpropagation",
      "fundamental",
      "networks",
      "neural",
      "training"
    ],
    "excerpt": "The fundamental algorithm for training neural networks. It calculates how much each weight contributed to the error and adjusts weights accordingly, propagating the error signal backward through the network.",
    "url": "pages/glossary.html#term-backpropagation"
  },
  {
    "id": "term-backpropagation-history",
    "title": "Backpropagation History",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1986",
      "algorithm",
      "backpropagation",
      "development",
      "discovered",
      "hinton",
      "history",
      "independently",
      "influential",
      "milestones",
      "multiple",
      "nature"
    ],
    "excerpt": "The development of the backpropagation algorithm for training neural networks, independently discovered multiple times but popularized by Rumelhart, Hinton, and Williams in their influential 1986 Nature paper.",
    "url": "pages/glossary.html#term-backpropagation-history"
  },
  {
    "id": "term-bag-of-words",
    "title": "Bag of Words",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "bag",
      "counts",
      "grammar",
      "ignoring",
      "nlp",
      "occurrences",
      "of",
      "order",
      "representation",
      "simple",
      "text",
      "word"
    ],
    "excerpt": "A simple text representation that counts word occurrences, ignoring order and grammar. Despite its simplicity, still useful for some classification tasks and as a baseline.",
    "url": "pages/glossary.html#term-bag-of-words"
  },
  {
    "id": "term-bagging",
    "title": "Bagging (Bootstrap Aggregating)",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "aggregating",
      "averaging",
      "bagging",
      "bootstrap",
      "data",
      "ensemble",
      "models",
      "multiple",
      "predictions",
      "random",
      "subsets",
      "technique"
    ],
    "excerpt": "Training multiple models on random subsets of data and averaging their predictions. Reduces variance and overfitting. The basis for Random Forest algorithms.",
    "url": "pages/glossary.html#term-bagging"
  },
  {
    "id": "term-baichuan",
    "title": "Baichuan",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "ai",
      "baichuan",
      "bilingual",
      "chinese",
      "known",
      "language",
      "llms",
      "model",
      "performance",
      "series",
      "strong",
      "tasks"
    ],
    "excerpt": "A series of Chinese bilingual LLMs known for strong performance in Chinese language tasks. Part of the growing ecosystem of non-Western foundation models.",
    "url": "pages/glossary.html#term-baichuan"
  },
  {
    "id": "term-bandit-algorithm",
    "title": "Bandit Algorithm",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "action",
      "actions",
      "algorithm",
      "balances",
      "bandit",
      "bestknown",
      "choosing",
      "cumulative",
      "exploitation",
      "exploration",
      "learning",
      "machine"
    ],
    "excerpt": "An algorithm for the multi-armed bandit problem that balances exploration (trying new actions) with exploitation (choosing the best-known action) to maximize cumulative reward over time.",
    "url": "pages/glossary.html#term-bandit-algorithm"
  },
  {
    "id": "term-bandwidth",
    "title": "Bandwidth (AI Context)",
    "category": "Glossary",
    "subcategory": "Infrastructure",
    "keywords": [
      "ai",
      "bandwidth",
      "context",
      "crucial",
      "data",
      "infrastructure",
      "performance",
      "rate",
      "transferred"
    ],
    "excerpt": "The rate at which data can be transferred, crucial for AI infrastructure. Memory bandwidth often limits GPU performance; network bandwidth affects distributed training.",
    "url": "pages/glossary.html#term-bandwidth"
  },
  {
    "id": "term-bandwidth-selection",
    "title": "Bandwidth Selection",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bandwidth",
      "choosing",
      "controls",
      "data",
      "density",
      "estimated",
      "estimation",
      "kernel",
      "parameter",
      "process",
      "science",
      "selection"
    ],
    "excerpt": "The process of choosing the bandwidth parameter in kernel density estimation, which controls the smoothness of the estimated density.",
    "url": "pages/glossary.html#term-bandwidth-selection"
  },
  {
    "id": "term-bard",
    "title": "Bard",
    "category": "Glossary",
    "subcategory": "Product",
    "keywords": [
      "bard",
      "conversational",
      "gemini",
      "googles",
      "historical",
      "later",
      "product",
      "renamed"
    ],
    "excerpt": "Google's conversational AI product, later renamed to Gemini. Competed with ChatGPT using Google's LLM technology and integration with Google services.",
    "url": "pages/glossary.html#term-bard"
  },
  {
    "id": "term-base-model",
    "title": "Base Model",
    "category": "Glossary",
    "subcategory": "Model Type",
    "keywords": [
      "base",
      "finetuning",
      "model",
      "pretrained",
      "specific",
      "tasks",
      "training",
      "type"
    ],
    "excerpt": "A pre-trained model before fine-tuning for specific tasks. Base models are good at text completion but need instruction tuning to become helpful assistants.",
    "url": "pages/glossary.html#term-base-model"
  },
  {
    "id": "term-baseline",
    "title": "Baseline",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "approach",
      "baseline",
      "comparison",
      "evaluation",
      "model",
      "point",
      "reference",
      "research",
      "simple"
    ],
    "excerpt": "A simple model or approach used as a reference point for comparison. New methods should outperform baselines to demonstrate value. Common baselines include random guessing or simple rules.",
    "url": "pages/glossary.html#term-baseline"
  },
  {
    "id": "term-batch",
    "title": "Batch",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "batch",
      "data",
      "iteration",
      "one",
      "processed",
      "subset",
      "technical",
      "together",
      "training"
    ],
    "excerpt": "A subset of training data processed together in one iteration. Batch processing improves training efficiency and stability compared to processing one example at a time.",
    "url": "pages/glossary.html#term-batch"
  },
  {
    "id": "term-batch-indexing",
    "title": "Batch Indexing",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "alternatives",
      "batch",
      "better",
      "building",
      "built",
      "complete",
      "database",
      "dataset",
      "incrementally",
      "index",
      "indexing",
      "maintenance"
    ],
    "excerpt": "The process of building or rebuilding a vector index from a complete dataset in a single operation, producing an optimally structured index that typically offers better search performance than incrementally built alternatives.",
    "url": "pages/glossary.html#term-batch-indexing"
  },
  {
    "id": "term-batch-normalization",
    "title": "Batch Normalization",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "applying",
      "batch",
      "deviation",
      "dividing",
      "inputs",
      "layer",
      "learned",
      "learning",
      "machine",
      "mean",
      "normalization",
      "normalizes"
    ],
    "excerpt": "A technique that normalizes the inputs to each layer by subtracting the batch mean and dividing by the batch standard deviation, then applying learned scale and shift parameters.",
    "url": "pages/glossary.html#term-batch-normalization"
  },
  {
    "id": "term-batch-normalization-cv",
    "title": "Batch Normalization",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "accelerating",
      "across",
      "acting",
      "batch",
      "computer",
      "convergence",
      "convolutional",
      "covariate",
      "image",
      "inputs",
      "internal",
      "layer"
    ],
    "excerpt": "A technique that normalizes layer inputs across a mini-batch during training, reducing internal covariate shift, accelerating convergence, and acting as a regularizer in convolutional neural networks.",
    "url": "pages/glossary.html#term-batch-normalization-cv"
  },
  {
    "id": "term-batch-normalization-history",
    "title": "Batch Normalization",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2015",
      "accelerating",
      "architectures",
      "batch",
      "becoming",
      "component",
      "deep",
      "dramatically",
      "history",
      "inputs",
      "introduced",
      "ioffe"
    ],
    "excerpt": "A technique introduced by Ioffe and Szegedy in 2015 that normalizes layer inputs during training, dramatically accelerating deep network training and becoming a standard component of modern neural architectures.",
    "url": "pages/glossary.html#term-batch-normalization-history"
  },
  {
    "id": "term-batch-rl",
    "title": "Batch Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "approach",
      "batch",
      "fixed",
      "interaction",
      "learning",
      "learns",
      "online",
      "paradigms",
      "precollected",
      "reinforcement",
      "training"
    ],
    "excerpt": "An approach to RL where the agent learns from a fixed batch of pre-collected transitions without online interaction.",
    "url": "pages/glossary.html#term-batch-rl"
  },
  {
    "id": "term-batch-scheduling",
    "title": "Batch Scheduling for Inference",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "batch",
      "for",
      "gpu",
      "grouping",
      "hardware",
      "improving",
      "inference",
      "infrastructure",
      "model",
      "multiple",
      "optimization",
      "processing"
    ],
    "excerpt": "The strategy of grouping multiple inference requests together for simultaneous processing on a GPU, improving hardware utilization and throughput.",
    "url": "pages/glossary.html#term-batch-scheduling"
  },
  {
    "id": "term-batch-size",
    "title": "Batch Size",
    "category": "Glossary",
    "subcategory": "Hyperparameter",
    "keywords": [
      "batch",
      "examples",
      "hyperparameter",
      "model",
      "number",
      "processed",
      "size",
      "together",
      "training",
      "updating",
      "weights"
    ],
    "excerpt": "The number of training examples processed together before updating model weights. Larger batches provide more stable gradients but require more memory; smaller batches train faster but with more noise.",
    "url": "pages/glossary.html#term-batch-size"
  },
  {
    "id": "term-batched-inference",
    "title": "Batched Inference",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "across",
      "amortizing",
      "batched",
      "cost",
      "gpu",
      "inference",
      "inputs",
      "llm",
      "loading",
      "many",
      "maximize",
      "model"
    ],
    "excerpt": "The practice of processing multiple inference requests simultaneously through a model to maximize GPU utilization and throughput, amortizing the cost of loading model weights across many inputs.",
    "url": "pages/glossary.html#term-batched-inference"
  },
  {
    "id": "term-bayes-error-rate",
    "title": "Bayes Error Rate",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "achievable",
      "bayes",
      "classification",
      "classifier",
      "data",
      "determined",
      "error",
      "given",
      "irreducible",
      "learning",
      "lowest",
      "machine"
    ],
    "excerpt": "The lowest achievable error rate for any classifier on a given classification problem, determined by the irreducible noise in the data.",
    "url": "pages/glossary.html#term-bayes-error-rate"
  },
  {
    "id": "term-bayes-theorem",
    "title": "Bayes&amp;#x27; Theorem",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bayesampx27",
      "bayesian",
      "conditional",
      "evidence",
      "fundamental",
      "given",
      "hypothesis",
      "likelihood",
      "marginal",
      "methods",
      "prior",
      "probability"
    ],
    "excerpt": "A fundamental rule of probability that relates the conditional probability of a hypothesis given evidence to the prior probability of the hypothesis, the likelihood of the evidence, and the marginal probability of the evidence.",
    "url": "pages/glossary.html#term-bayes-theorem"
  },
  {
    "id": "term-bayesian-inference",
    "title": "Bayesian Inference",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "acquired",
      "additional",
      "bayes",
      "bayesian",
      "compute",
      "data",
      "distributions",
      "estimates",
      "evidence",
      "framework",
      "hypotheses",
      "inference"
    ],
    "excerpt": "A statistical framework that updates probability estimates for hypotheses as additional evidence is acquired, using Bayes' theorem to compute posterior distributions from prior distributions and observed data.",
    "url": "pages/glossary.html#term-bayesian-inference"
  },
  {
    "id": "term-bayesian-information-criterion",
    "title": "Bayesian Information Criterion",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "aic",
      "bayesian",
      "criterion",
      "depends",
      "information",
      "larger",
      "model",
      "number",
      "parameters",
      "penalty",
      "sample",
      "selection"
    ],
    "excerpt": "A model selection criterion similar to AIC but with a larger penalty for the number of parameters that depends on sample size.",
    "url": "pages/glossary.html#term-bayesian-information-criterion"
  },
  {
    "id": "term-bayesian",
    "title": "Bayesian Methods",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "approaches",
      "based",
      "bayesian",
      "beliefs",
      "evidence",
      "incorporate",
      "knowledge",
      "methods",
      "prior",
      "statistical",
      "statistics",
      "theory"
    ],
    "excerpt": "Statistical approaches that incorporate prior knowledge and update beliefs based on evidence. Used for uncertainty quantification, hyperparameter optimization, and probabilistic modeling.",
    "url": "pages/glossary.html#term-bayesian"
  },
  {
    "id": "term-bayesian-model-averaging",
    "title": "Bayesian Model Averaging",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "accounts",
      "across",
      "averaging",
      "bayesian",
      "best",
      "methods",
      "model",
      "models",
      "multiple",
      "posterior",
      "predictions",
      "probabilities"
    ],
    "excerpt": "A technique that accounts for model uncertainty by averaging predictions across multiple models weighted by their posterior probabilities, rather than selecting a single best model.",
    "url": "pages/glossary.html#term-bayesian-model-averaging"
  },
  {
    "id": "term-bayesian-network",
    "title": "Bayesian Network",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "acyclic",
      "bayesian",
      "conditional",
      "dependencies",
      "directed",
      "graph",
      "learning",
      "machine",
      "methods",
      "network",
      "random",
      "represents"
    ],
    "excerpt": "A directed acyclic graph that represents a set of random variables and their conditional dependencies.",
    "url": "pages/glossary.html#term-bayesian-network"
  },
  {
    "id": "term-bayesian-network-history",
    "title": "Bayesian Network History",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1980s",
      "bayesian",
      "became",
      "central",
      "development",
      "framework",
      "graphical",
      "history",
      "judea",
      "learning",
      "machine",
      "milestones"
    ],
    "excerpt": "The development of Bayesian networks by Judea Pearl and others in the 1980s, providing a graphical framework for representing and reasoning under uncertainty that became central to AI and machine learning.",
    "url": "pages/glossary.html#term-bayesian-network-history"
  },
  {
    "id": "term-bayesian-optimization",
    "title": "Bayesian Optimization",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "acquisition",
      "bayesian",
      "blackbox",
      "builds",
      "determine",
      "evaluate",
      "expensive",
      "function",
      "functions",
      "gaussian",
      "learning",
      "machine"
    ],
    "excerpt": "A sequential strategy for optimizing expensive black-box functions that builds a probabilistic surrogate model (typically a Gaussian process) and uses an acquisition function to determine the most promising points to evaluate next.",
    "url": "pages/glossary.html#term-bayesian-optimization"
  },
  {
    "id": "term-beam-search",
    "title": "Beam Search",
    "category": "Glossary",
    "subcategory": "Generation",
    "keywords": [
      "algorithm",
      "beam",
      "candidate",
      "generation",
      "maintains",
      "multiple",
      "ones",
      "promising",
      "search",
      "selecting",
      "sequences",
      "step"
    ],
    "excerpt": "A search algorithm used in text generation that maintains multiple candidate sequences at each step, selecting the most promising ones.",
    "url": "pages/glossary.html#term-beam-search"
  },
  {
    "id": "term-behavior-cloning",
    "title": "Behavior Cloning",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "behavior",
      "cloning",
      "demonstrations",
      "expert",
      "imitate",
      "imitation",
      "learning",
      "training"
    ],
    "excerpt": "Learning to imitate expert behavior from demonstrations. The model learns to map observations to actions by copying what experts do in similar situations.",
    "url": "pages/glossary.html#term-behavior-cloning"
  },
  {
    "id": "term-beijing-ai-principles",
    "title": "Beijing AI Principles",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "2019",
      "academy",
      "ai",
      "beijing",
      "benefits",
      "chinese",
      "context",
      "development",
      "emphasizing",
      "fairness",
      "governance",
      "harmony"
    ],
    "excerpt": "A set of AI governance principles released in 2019 by the Beijing Academy of AI, emphasizing harmony, fairness, safety, shared benefits, and responsible development in the Chinese AI governance context.",
    "url": "pages/glossary.html#term-beijing-ai-principles"
  },
  {
    "id": "term-bellman-equation",
    "title": "Bellman Equation",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "bellman",
      "discounted",
      "equation",
      "immediate",
      "learning",
      "methods",
      "plus",
      "recursive",
      "reinforcement",
      "relating",
      "reward",
      "state"
    ],
    "excerpt": "A recursive equation relating the value of a state to the immediate reward plus the discounted value of successor states.",
    "url": "pages/glossary.html#term-bellman-equation"
  },
  {
    "id": "term-benchmark",
    "title": "Benchmark",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "benchmark",
      "compare",
      "dataset",
      "evaluate",
      "evaluation",
      "model",
      "performance",
      "research",
      "standardized",
      "test"
    ],
    "excerpt": "A standardized test or dataset used to evaluate and compare AI model performance. Common LLM benchmarks include MMLU, HellaSwag, and HumanEval for measuring different capabilities.",
    "url": "pages/glossary.html#term-benchmark"
  },
  {
    "id": "term-benchmark-gaming",
    "title": "Benchmark Gaming",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "achieve",
      "ai",
      "benchmark",
      "benchmarks",
      "capabilities",
      "contamination",
      "corresponding",
      "data",
      "gaming",
      "generative",
      "high",
      "improvements"
    ],
    "excerpt": "The practice of optimizing a model specifically to achieve high scores on popular benchmarks without corresponding improvements in real-world capabilities, often through data contamination or overfitting.",
    "url": "pages/glossary.html#term-benchmark-gaming"
  },
  {
    "id": "term-benefit-sharing-in-ai",
    "title": "Benefit Sharing in AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "across",
      "ai",
      "among",
      "benefit",
      "benefits",
      "broadly",
      "companies",
      "concentrated",
      "developers",
      "distributed",
      "economic",
      "ethics"
    ],
    "excerpt": "The principle that the economic and social benefits generated by AI should be distributed broadly across society rather than concentrated among a small number of developers, companies, or nations.",
    "url": "pages/glossary.html#term-benefit-sharing-in-ai"
  },
  {
    "id": "term-bernoulli-distribution",
    "title": "Bernoulli Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bernoulli",
      "discrete",
      "distribution",
      "failure",
      "modeling",
      "outcomes",
      "probability",
      "simplest",
      "single",
      "statistics",
      "success",
      "trial"
    ],
    "excerpt": "The simplest discrete probability distribution, modeling a single trial with two outcomes (success with probability p, failure with probability 1-p).",
    "url": "pages/glossary.html#term-bernoulli-distribution"
  },
  {
    "id": "term-bert",
    "title": "BERT (Bidirectional Encoder Representations from Transformers)",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "2018",
      "architecture",
      "bert",
      "bidirectional",
      "bidirectionally",
      "context",
      "encoder",
      "from",
      "google",
      "influential",
      "language",
      "left"
    ],
    "excerpt": "A influential language model from Google (2018) that processes text bidirectionally, understanding context from both left and right. Revolutionized NLP and inspired many subsequent models.",
    "url": "pages/glossary.html#term-bert"
  },
  {
    "id": "term-bert-release",
    "title": "BERT Release",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2018",
      "achieved",
      "across",
      "bert",
      "bidirectional",
      "encoder",
      "established",
      "googles",
      "history",
      "language",
      "milestones",
      "model"
    ],
    "excerpt": "Google's Bidirectional Encoder Representations from Transformers model, released in October 2018, which achieved state-of-the-art results across numerous NLP tasks through bidirectional pre-trainin...",
    "url": "pages/glossary.html#term-bert-release"
  },
  {
    "id": "term-bertscore",
    "title": "BERTScore",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "bert",
      "bertscore",
      "beyond",
      "capturing",
      "computes",
      "contextual",
      "embeddings",
      "equivalence",
      "evaluation",
      "exact",
      "generated",
      "greedy"
    ],
    "excerpt": "An evaluation metric that computes the similarity between generated and reference texts using contextual BERT embeddings with greedy token matching, capturing semantic equivalence beyond exact surface-form overlap.",
    "url": "pages/glossary.html#term-bertscore"
  },
  {
    "id": "term-best-of-n-sampling",
    "title": "Best-of-N Sampling",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "best",
      "better",
      "candidate",
      "completions",
      "compute",
      "decoding",
      "generates",
      "generative",
      "highest",
      "increased",
      "inference"
    ],
    "excerpt": "An inference strategy that generates N candidate completions and returns the one scoring highest on a reward model, trading increased compute for better output quality without model modification.",
    "url": "pages/glossary.html#term-best-of-n-sampling"
  },
  {
    "id": "term-beta-distribution",
    "title": "Beta Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "beta",
      "continuous",
      "defined",
      "distribution",
      "interval",
      "parameters",
      "parametrized",
      "probability",
      "shape",
      "statistics",
      "two"
    ],
    "excerpt": "A continuous probability distribution defined on the interval [0, 1], parametrized by two shape parameters. It is commonly used as a prior distribution for probabilities in Bayesian inference.",
    "url": "pages/glossary.html#term-beta-distribution"
  },
  {
    "id": "term-beta-vae",
    "title": "Beta-VAE",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "autoencoder",
      "beta",
      "disentangled",
      "divergence",
      "greater",
      "hyperparameter",
      "introduces",
      "latent",
      "modification",
      "networks",
      "neural"
    ],
    "excerpt": "A modification of the variational autoencoder that introduces a hyperparameter beta to weight the KL divergence term, promoting disentangled latent representations when beta is greater than one.",
    "url": "pages/glossary.html#term-beta-vae"
  },
  {
    "id": "term-bev-perception",
    "title": "BEV Perception",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "autonomous",
      "bev",
      "birds",
      "computer",
      "data",
      "detection",
      "driving",
      "eye",
      "joint",
      "lidar",
      "multicamera"
    ],
    "excerpt": "Bird's Eye View perception, a paradigm in autonomous driving that transforms multi-camera or LiDAR data into a unified top-down representation for joint 3D detection, segmentation, and prediction.",
    "url": "pages/glossary.html#term-bev-perception"
  },
  {
    "id": "term-bf16",
    "title": "BF16 (Brain Floating Point)",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "16bit",
      "bf16",
      "bits",
      "brain",
      "developed",
      "exponent",
      "floating",
      "floatingpoint",
      "format",
      "fp32",
      "google",
      "hardware"
    ],
    "excerpt": "A 16-bit floating-point format with 8 exponent bits (same as FP32) and 7 mantissa bits, developed by Google Brain.",
    "url": "pages/glossary.html#term-bf16"
  },
  {
    "id": "term-bfloat16",
    "title": "bfloat16",
    "category": "Glossary",
    "subcategory": "Technical",
    "keywords": [
      "16bit",
      "bfloat16",
      "floatingpoint",
      "format",
      "network",
      "neural",
      "optimized",
      "precision",
      "technical",
      "training"
    ],
    "excerpt": "A 16-bit floating-point format optimized for neural network training. Sacrifices precision for range compared to float16, offering better training stability.",
    "url": "pages/glossary.html#term-bfloat16"
  },
  {
    "id": "term-bi-encoder",
    "title": "Bi-Encoder",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "architecture",
      "bi",
      "comparison",
      "document",
      "documents",
      "embeddings",
      "enabling",
      "encoder",
      "encoders",
      "encodes",
      "fast",
      "fixedsize"
    ],
    "excerpt": "A neural retrieval architecture that independently encodes queries and documents into fixed-size vectors using separate or shared encoders, enabling pre-computation of document embeddings and fast similarity search through vector comparison.",
    "url": "pages/glossary.html#term-bi-encoder"
  },
  {
    "id": "term-bias",
    "title": "Bias",
    "category": "Glossary",
    "subcategory": "Ethics",
    "keywords": [
      "bias",
      "biases",
      "data",
      "design",
      "errors",
      "ethics",
      "fairness",
      "outputs",
      "preferences",
      "reflect",
      "system",
      "systematic"
    ],
    "excerpt": "Systematic errors or unfair preferences in AI outputs that reflect biases in training data or system design. Can affect fairness across different groups or perspectives.",
    "url": "pages/glossary.html#term-bias"
  },
  {
    "id": "term-estimation-bias",
    "title": "Bias",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "being",
      "bias",
      "difference",
      "estimated",
      "estimation",
      "estimator",
      "expected",
      "inference",
      "parameter",
      "statistical",
      "statistics",
      "true"
    ],
    "excerpt": "In statistical estimation, the difference between the expected value of an estimator and the true value of the parameter being estimated. An unbiased estimator has zero bias.",
    "url": "pages/glossary.html#term-estimation-bias"
  },
  {
    "id": "term-bias-score",
    "title": "Bias Score",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "across",
      "analysis",
      "assessed",
      "association",
      "benchmarks",
      "bias",
      "degree",
      "demographic",
      "differential",
      "evaluation",
      "fairness",
      "groups"
    ],
    "excerpt": "A metric that measures the degree of systematic prejudice or unfair treatment in model outputs across demographic groups, assessed through differential response analysis, stereotype association tests, or fairness benchmarks.",
    "url": "pages/glossary.html#term-bias-score"
  },
  {
    "id": "term-bias-variance-decomposition",
    "title": "Bias-Variance Decomposition",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "bias",
      "components",
      "data",
      "decomposition",
      "error",
      "expected",
      "insight",
      "irreducible",
      "learning",
      "machine",
      "mathematical",
      "model"
    ],
    "excerpt": "A mathematical decomposition of expected prediction error into three components: irreducible noise, squared bias (systematic error), and variance (sensitivity to training data), providing insight into sources of model error.",
    "url": "pages/glossary.html#term-bias-variance-decomposition"
  },
  {
    "id": "term-bias-variance-tradeoff",
    "title": "Bias-Variance Tradeoff",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "ability",
      "assumptions",
      "bias",
      "error",
      "fluctuations",
      "fundamental",
      "learning",
      "machine",
      "minimize",
      "model",
      "models",
      "overly"
    ],
    "excerpt": "The fundamental tension in supervised learning between a model's ability to minimize bias (error from overly simplistic assumptions) and variance (error from sensitivity to small fluctuations in the training set).",
    "url": "pages/glossary.html#term-bias-variance-tradeoff"
  },
  {
    "id": "term-biden-executive-order-on-ai",
    "title": "Biden Executive Order on AI",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "14110",
      "2023",
      "agency",
      "ai",
      "biden",
      "establishing",
      "executive",
      "federal",
      "governance",
      "including",
      "issued",
      "large"
    ],
    "excerpt": "Executive Order 14110, issued by President Biden in October 2023, establishing requirements for AI safety and security including red-teaming standards, reporting of large training runs, and federal agency AI governance.",
    "url": "pages/glossary.html#term-biden-executive-order-on-ai"
  },
  {
    "id": "term-bidirectional",
    "title": "Bidirectional",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "bidirectional",
      "directions",
      "lefttoright",
      "processing",
      "righttoleft",
      "sequences"
    ],
    "excerpt": "Processing sequences in both directions (left-to-right and right-to-left). BERT processes bidirectionally for understanding; GPT processes unidirectionally for generation.",
    "url": "pages/glossary.html#term-bidirectional"
  },
  {
    "id": "term-bidirectional-rnn",
    "title": "Bidirectional RNN",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "backward",
      "bidirectional",
      "combining",
      "context",
      "directions",
      "forward",
      "input",
      "networks",
      "neural",
      "processes",
      "produce"
    ],
    "excerpt": "A recurrent architecture that processes input sequences in both forward and backward directions simultaneously, combining both context directions to produce richer representations at each time step.",
    "url": "pages/glossary.html#term-bidirectional-rnn"
  },
  {
    "id": "term-bigbench",
    "title": "BigBench",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "200",
      "benchmark",
      "benchmarks",
      "beyond",
      "bigbench",
      "capabilities",
      "collaborative",
      "containing",
      "contributed",
      "designed",
      "diverse",
      "evaluation"
    ],
    "excerpt": "Beyond the Imitation Game Benchmark, a large collaborative benchmark containing over 200 diverse tasks contributed by researchers, designed to probe language model capabilities including reasoning,...",
    "url": "pages/glossary.html#term-bigbench"
  },
  {
    "id": "term-bigbird",
    "title": "BigBird",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "achieve",
      "architecture",
      "attention",
      "bigbird",
      "combines",
      "complexity",
      "expressive",
      "full",
      "global",
      "linear",
      "maintaining",
      "networks"
    ],
    "excerpt": "A sparse attention transformer that combines random attention, window attention, and global attention patterns to achieve linear complexity while provably maintaining the expressive power of full attention.",
    "url": "pages/glossary.html#term-bigbird"
  },
  {
    "id": "term-bigram",
    "title": "Bigram / N-gram",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "bigram",
      "consecutive",
      "gram",
      "historical",
      "language",
      "modeling",
      "nlp",
      "sequences",
      "tokens"
    ],
    "excerpt": "Sequences of N consecutive tokens used in language modeling. Bigrams are pairs; trigrams are triples. N-gram models were dominant before neural approaches but remain useful baselines.",
    "url": "pages/glossary.html#term-bigram"
  },
  {
    "id": "term-binary-classification",
    "title": "Binary Classification",
    "category": "Glossary",
    "subcategory": "ML Task",
    "keywords": [
      "binary",
      "classification",
      "exactly",
      "ml",
      "outcomes",
      "possible",
      "spam",
      "spamnot",
      "task",
      "two",
      "yesno"
    ],
    "excerpt": "A classification task with exactly two possible outcomes (yes/no, spam/not spam). The simplest classification problem, often a building block for more complex tasks.",
    "url": "pages/glossary.html#term-binary-classification"
  },
  {
    "id": "term-binary-quantization",
    "title": "Binary Quantization",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "32x",
      "accuracy",
      "aggressive",
      "based",
      "binary",
      "bit",
      "comparisons",
      "compression",
      "cost",
      "database",
      "dimension",
      "distance"
    ],
    "excerpt": "An aggressive vector compression technique that reduces each vector dimension to a single bit based on its sign, enabling 32x compression from float32 and extremely fast Hamming distance comparisons at the cost of reduced recall accuracy.",
    "url": "pages/glossary.html#term-binary-quantization"
  },
  {
    "id": "term-bing-chat",
    "title": "Bing Chat / Copilot",
    "category": "Glossary",
    "subcategory": "Product",
    "keywords": [
      "aipowered",
      "assistant",
      "bing",
      "chat",
      "copilot",
      "gpt4",
      "integrating",
      "microsoft",
      "microsofts",
      "product",
      "search",
      "web"
    ],
    "excerpt": "Microsoft's AI-powered search assistant, integrating GPT-4 with web search. Can answer questions with citations, create content, and access current information.",
    "url": "pages/glossary.html#term-bing-chat"
  },
  {
    "id": "term-binomial-distribution",
    "title": "Binomial Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bernoulli",
      "binomial",
      "discrete",
      "distribution",
      "fixed",
      "independent",
      "modeling",
      "number",
      "probability",
      "same",
      "statistics",
      "success"
    ],
    "excerpt": "A discrete probability distribution modeling the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success.",
    "url": "pages/glossary.html#term-binomial-distribution"
  },
  {
    "id": "term-bio-tagging",
    "title": "BIO Tagging",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "alternative",
      "begin",
      "bio",
      "continues",
      "entity",
      "format",
      "inside",
      "iob",
      "labeling",
      "labels",
      "marks",
      "name"
    ],
    "excerpt": "An alternative name for IOB tagging format using Begin, Inside, Outside labels for sequence labeling tasks, where B marks the start of an entity and I continues it.",
    "url": "pages/glossary.html#term-bio-tagging"
  },
  {
    "id": "term-bioes-tagging",
    "title": "BIOES Tagging",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "accuracy",
      "adds",
      "bio",
      "bioes",
      "boundary",
      "end",
      "entities",
      "extended",
      "format",
      "improving",
      "information",
      "labeling"
    ],
    "excerpt": "An extended sequence labeling scheme that adds End and Single tags to the BIO format, providing more precise boundary information for named entities and improving recognition accuracy.",
    "url": "pages/glossary.html#term-bioes-tagging"
  },
  {
    "id": "term-biometric-ai-regulation",
    "title": "Biometric AI Regulation",
    "category": "Glossary",
    "subcategory": "Privacy",
    "keywords": [
      "act",
      "ai",
      "bans",
      "biometric",
      "data",
      "facial",
      "features",
      "fingerprints",
      "gait",
      "including",
      "legal",
      "patterns"
    ],
    "excerpt": "Legal restrictions on AI systems that process biometric data such as facial features, fingerprints, or gait patterns, including bans on real-time biometric surveillance in public spaces under the EU AI Act.",
    "url": "pages/glossary.html#term-biometric-ai-regulation"
  },
  {
    "id": "term-bisimulation-metric",
    "title": "Bisimulation Metric",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "abstraction",
      "basis",
      "bisimulation",
      "concepts",
      "core",
      "distance",
      "dynamics",
      "groups",
      "learning",
      "metric",
      "principled",
      "providing"
    ],
    "excerpt": "A distance metric on states that groups together states with similar reward and transition dynamics, providing a principled basis for state abstraction in RL.",
    "url": "pages/glossary.html#term-bisimulation-metric"
  },
  {
    "id": "term-bit-precision",
    "title": "Bit Precision",
    "category": "Glossary",
    "subcategory": "Technical",
    "keywords": [
      "activations",
      "bit",
      "bits",
      "model",
      "number",
      "optimization",
      "precision",
      "represent",
      "technical",
      "weights"
    ],
    "excerpt": "The number of bits used to represent model weights and activations. Lower precision (8-bit, 4-bit) reduces memory and increases speed but may affect accuracy.",
    "url": "pages/glossary.html#term-bit-precision"
  },
  {
    "id": "term-bitter-lesson",
    "title": "Bitter Lesson",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2019",
      "approaches",
      "arguing",
      "bitter",
      "computation",
      "domain",
      "encode",
      "essay",
      "general",
      "history",
      "human",
      "influential"
    ],
    "excerpt": "An influential 2019 essay by Rich Sutton arguing that the history of AI shows general methods leveraging computation (search and learning) ultimately outperform approaches that encode human domain knowledge.",
    "url": "pages/glossary.html#term-bitter-lesson"
  },
  {
    "id": "term-black-box",
    "title": "Black Box",
    "category": "Glossary",
    "subcategory": "Interpretability",
    "keywords": [
      "black",
      "box",
      "concept",
      "internal",
      "interpretability",
      "system",
      "understandable",
      "users",
      "visible",
      "whose",
      "workings"
    ],
    "excerpt": "A system whose internal workings are not visible or understandable to users. Many AI models are considered black boxes because their decision-making processes are difficult to interpret.",
    "url": "pages/glossary.html#term-black-box"
  },
  {
    "id": "term-black-box-problem",
    "title": "Black Box Problem",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "audit",
      "black",
      "box",
      "challenge",
      "decisions",
      "deep",
      "difficult",
      "ethics",
      "explain",
      "human",
      "making"
    ],
    "excerpt": "The challenge that many AI systems, particularly deep neural networks, operate in ways that are opaque to human understanding, making it difficult to explain, audit, or trust their decisions.",
    "url": "pages/glossary.html#term-black-box-problem"
  },
  {
    "id": "term-bletchley-declaration-on-ai",
    "title": "Bletchley Declaration on AI",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "2023",
      "acknowledging",
      "ai",
      "bletchley",
      "committing",
      "cooperation",
      "countries",
      "declaration",
      "frontier",
      "governance",
      "harm",
      "international"
    ],
    "excerpt": "A declaration signed by 28 countries at the November 2023 AI Safety Summit at Bletchley Park, acknowledging the potential for serious harm from frontier AI and committing to international cooperation on safety.",
    "url": "pages/glossary.html#term-bletchley-declaration-on-ai"
  },
  {
    "id": "term-bletchley-park-ai-safety-summit",
    "title": "Bletchley Park AI Safety Summit",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2023",
      "ai",
      "bletchley",
      "bringing",
      "companies",
      "cooperation",
      "discuss",
      "establish",
      "first",
      "frameworks",
      "frontier",
      "governance"
    ],
    "excerpt": "The first major international AI Safety Summit held at Bletchley Park, UK, in November 2023, bringing together governments and AI companies to discuss frontier AI risks and establish international cooperation frameworks.",
    "url": "pages/glossary.html#term-bletchley-park-ai-safety-summit"
  },
  {
    "id": "term-bletchley-park-codebreaking",
    "title": "Bletchley Park Codebreaking",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "advancing",
      "alan",
      "axis",
      "bletchley",
      "bombe",
      "british",
      "codebreaking",
      "colleagues",
      "colossus",
      "communications",
      "computational",
      "decrypt"
    ],
    "excerpt": "The World War II British codebreaking operation where Alan Turing and colleagues developed the Bombe and Colossus machines to decrypt Axis communications, advancing computational methods that influenced early AI development.",
    "url": "pages/glossary.html#term-bletchley-park-codebreaking"
  },
  {
    "id": "term-bleu",
    "title": "BLEU Score",
    "category": "Glossary",
    "subcategory": "Metrics",
    "keywords": [
      "bleu",
      "comparing",
      "evaluating",
      "evaluation",
      "machine",
      "metric",
      "metrics",
      "ngram",
      "overlap",
      "quality",
      "reference",
      "score"
    ],
    "excerpt": "A metric for evaluating machine translation quality by comparing n-gram overlap with reference translations. While imperfect, it remains widely used for automated translation evaluation.",
    "url": "pages/glossary.html#term-bleu"
  },
  {
    "id": "term-bleurt",
    "title": "BLEURT",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "assessments",
      "bert",
      "bleurt",
      "correlate",
      "data",
      "different",
      "equivalent",
      "evaluation",
      "even",
      "finetunes",
      "human",
      "humanrated"
    ],
    "excerpt": "A learned evaluation metric that fine-tunes BERT on synthetic and human-rated data to predict text quality scores, providing robust assessments that correlate with human judgments even for paraphra...",
    "url": "pages/glossary.html#term-bleurt"
  },
  {
    "id": "term-bloom",
    "title": "BLOOM",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "bigscience",
      "bloom",
      "created",
      "language",
      "languages",
      "large",
      "model",
      "multilingual",
      "open",
      "opensource",
      "source",
      "trained"
    ],
    "excerpt": "A large multilingual open-source language model created by BigScience, trained on 46 languages. Demonstrated the viability of collaborative, open AI development.",
    "url": "pages/glossary.html#term-bloom"
  },
  {
    "id": "term-bm25",
    "title": "BM25",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "best",
      "bm25",
      "document",
      "effective",
      "extends",
      "frequency",
      "function",
      "information",
      "length",
      "matching",
      "nlp",
      "normalization"
    ],
    "excerpt": "Best Matching 25, a probabilistic information retrieval ranking function that extends TF-IDF with document length normalization and term frequency saturation for more effective document scoring.",
    "url": "pages/glossary.html#term-bm25"
  },
  {
    "id": "term-bm25-in-rag",
    "title": "BM25 in RAG",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "application",
      "baseline",
      "best",
      "bm25",
      "complements",
      "dense",
      "documents",
      "embedding",
      "exact",
      "finding",
      "function",
      "generation"
    ],
    "excerpt": "The application of the Best Matching 25 probabilistic ranking function within retrieval-augmented generation pipelines, providing strong lexical baseline retrieval that complements dense embedding search for finding documents with exact term matches.",
    "url": "pages/glossary.html#term-bm25-in-rag"
  },
  {
    "id": "term-boltzmann-exploration",
    "title": "Boltzmann Exploration",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "boltzmann",
      "divided",
      "exploration",
      "exponentiated",
      "learning",
      "parameter",
      "probability",
      "proportional",
      "qvalues",
      "reinforcement",
      "selects"
    ],
    "excerpt": "An exploration strategy that selects actions with probability proportional to exponentiated Q-values divided by a temperature parameter.",
    "url": "pages/glossary.html#term-boltzmann-exploration"
  },
  {
    "id": "term-boltzmann-machine",
    "title": "Boltzmann Machine",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1985",
      "annealing",
      "architectures",
      "boltzmann",
      "deep",
      "geoffrey",
      "hinton",
      "history",
      "important",
      "internal",
      "invented",
      "learn"
    ],
    "excerpt": "A stochastic neural network model invented by Geoffrey Hinton and Terry Sejnowski in 1985 that uses simulated annealing to learn internal representations, representing an important step toward deep learning architectures.",
    "url": "pages/glossary.html#term-boltzmann-machine"
  },
  {
    "id": "term-bonferroni-correction",
    "title": "Bonferroni Correction",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "adjustment",
      "bonferroni",
      "comparison",
      "controlling",
      "correction",
      "divides",
      "error",
      "familywise",
      "inference",
      "level",
      "multiple",
      "number"
    ],
    "excerpt": "A multiple comparison adjustment that divides the significance level by the number of tests performed, controlling the family-wise error rate. It is conservative but simple to apply.",
    "url": "pages/glossary.html#term-bonferroni-correction"
  },
  {
    "id": "term-boolean-retrieval",
    "title": "Boolean Retrieval",
    "category": "Glossary",
    "subcategory": "Search",
    "keywords": [
      "boolean",
      "combine",
      "operators",
      "retrieval",
      "search",
      "terms",
      "traditional"
    ],
    "excerpt": "Search using AND, OR, NOT operators to combine terms. Simple but limited compared to semantic search. Still used in specialized databases and advanced search interfaces.",
    "url": "pages/glossary.html#term-boolean-retrieval"
  },
  {
    "id": "term-boosting",
    "title": "Boosting",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "boosting",
      "ensemble",
      "examples",
      "focusing",
      "got",
      "ml",
      "model",
      "models",
      "new",
      "ones",
      "previous",
      "sequentially"
    ],
    "excerpt": "An ensemble technique that trains models sequentially, with each new model focusing on examples the previous ones got wrong. Powers XGBoost and LightGBM, popular for tabular data.",
    "url": "pages/glossary.html#term-boosting"
  },
  {
    "id": "term-bootstrap",
    "title": "Bootstrap",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bootstrap",
      "data",
      "distribution",
      "drawing",
      "estimates",
      "inference",
      "observed",
      "repeatedly",
      "replacement",
      "resampling",
      "samples",
      "sampling"
    ],
    "excerpt": "A resampling technique that estimates the sampling distribution of a statistic by repeatedly drawing samples with replacement from the observed data.",
    "url": "pages/glossary.html#term-bootstrap"
  },
  {
    "id": "term-bootstrap-aggregating",
    "title": "Bootstrap Aggregating",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "aggregating",
      "averaging",
      "bagging",
      "bootstrap",
      "called",
      "classification",
      "combines",
      "data",
      "different",
      "ensemble",
      "learning",
      "machine"
    ],
    "excerpt": "An ensemble method (also called bagging) that trains multiple models on different bootstrap samples of the training data and combines their predictions by averaging (regression) or voting (classification) to reduce variance.",
    "url": "pages/glossary.html#term-bootstrap-aggregating"
  },
  {
    "id": "term-bottleneck",
    "title": "Bottleneck",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "bottleneck",
      "compression",
      "design",
      "forces",
      "information",
      "layer",
      "narrow",
      "network",
      "neural"
    ],
    "excerpt": "A narrow layer in a neural network that forces compression of information. Used in autoencoders and some architectures to learn efficient representations.",
    "url": "pages/glossary.html#term-bottleneck"
  },
  {
    "id": "term-bottleneck-layer",
    "title": "Bottleneck Layer",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "abstraction",
      "architecture",
      "autoencoders",
      "blocks",
      "bottleneck",
      "compresses",
      "computation",
      "dimension",
      "encourage",
      "expanding",
      "hidden",
      "layer"
    ],
    "excerpt": "A narrow hidden layer that compresses representations to a lower dimension before expanding them, used in autoencoders and residual blocks to reduce computation and encourage abstraction.",
    "url": "pages/glossary.html#term-bottleneck-layer"
  },
  {
    "id": "term-box-cox-transformation",
    "title": "Box-Cox Transformation",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "aims",
      "box",
      "cox",
      "data",
      "distributed",
      "family",
      "lambda",
      "make",
      "normally",
      "parametrized",
      "power",
      "science"
    ],
    "excerpt": "A family of power transformations parametrized by lambda that aims to stabilize variance and make data more normally distributed.",
    "url": "pages/glossary.html#term-box-cox-transformation"
  },
  {
    "id": "term-bpe",
    "title": "BPE (Byte Pair Encoding)",
    "category": "Glossary",
    "subcategory": "Tokenization",
    "keywords": [
      "algorithm",
      "bpe",
      "breaks",
      "byte",
      "encoding",
      "nlp",
      "pair",
      "subword",
      "text",
      "tokenization",
      "units"
    ],
    "excerpt": "A tokenization algorithm that breaks text into subword units. Starts with individual characters and iteratively merges frequent pairs, balancing vocabulary size with the ability to handle rare words.",
    "url": "pages/glossary.html#term-bpe"
  },
  {
    "id": "term-brain-computer",
    "title": "Brain-Computer Interface (BCI)",
    "category": "Glossary",
    "subcategory": "Application",
    "keywords": [
      "application",
      "bci",
      "brain",
      "computer",
      "computers",
      "connecting",
      "directly",
      "interface",
      "neuroscience",
      "signals",
      "technology"
    ],
    "excerpt": "Technology connecting brain signals directly to computers. AI helps interpret neural signals for prosthetics, communication devices, and research applications.",
    "url": "pages/glossary.html#term-brain-computer"
  },
  {
    "id": "term-brier-score",
    "title": "Brier Score",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "accuracy",
      "actual",
      "binary",
      "brier",
      "computing",
      "difference",
      "learning",
      "machine",
      "mean",
      "measures",
      "metric",
      "metrics"
    ],
    "excerpt": "A scoring metric that measures the accuracy of probabilistic predictions by computing the mean squared difference between predicted probabilities and actual binary outcomes.",
    "url": "pages/glossary.html#term-brier-score"
  },
  {
    "id": "term-browse-mode",
    "title": "Browse Mode",
    "category": "Glossary",
    "subcategory": "Feature",
    "keywords": [
      "access",
      "browse",
      "capability",
      "conversations",
      "current",
      "feature",
      "information",
      "mode",
      "retrieve",
      "web"
    ],
    "excerpt": "AI capability to access and retrieve current web information during conversations. Addresses knowledge cutoff limitations by fetching real-time data.",
    "url": "pages/glossary.html#term-browse-mode"
  },
  {
    "id": "term-buffer",
    "title": "Buffer (Memory)",
    "category": "Glossary",
    "subcategory": "Technical",
    "keywords": [
      "architecture",
      "being",
      "buffer",
      "data",
      "memory",
      "processed",
      "storage",
      "technical",
      "temporary"
    ],
    "excerpt": "Temporary storage for data being processed. In AI agents, conversation buffers store recent exchanges. In training, data buffers optimize GPU utilization.",
    "url": "pages/glossary.html#term-buffer"
  },
  {
    "id": "term-bundle-adjustment",
    "title": "Bundle Adjustment",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "across",
      "adjustment",
      "bundle",
      "camera",
      "computer",
      "core",
      "error",
      "forming",
      "jointly",
      "minimizing",
      "optimization"
    ],
    "excerpt": "An optimization procedure that jointly refines 3D point positions and camera parameters by minimizing the reprojection error across all views, forming the core refinement step in 3D reconstruction pipelines.",
    "url": "pages/glossary.html#term-bundle-adjustment"
  },
  {
    "id": "term-burst",
    "title": "Burst (API)",
    "category": "Glossary",
    "subcategory": "API",
    "keywords": [
      "api",
      "burst",
      "exceed",
      "high",
      "limits",
      "normal",
      "periods",
      "rate",
      "short",
      "usage"
    ],
    "excerpt": "Short periods of high API usage that may exceed normal rate limits. Many providers allow bursting with gradual throttling rather than hard cutoffs.",
    "url": "pages/glossary.html#term-burst"
  },
  {
    "id": "term-burstiness",
    "title": "Burstiness",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "aigenerated",
      "burstiness",
      "content",
      "detection",
      "evaluation",
      "human",
      "length",
      "lower",
      "machinegenerated",
      "measuring",
      "metrics",
      "often"
    ],
    "excerpt": "A statistical property measuring the variability of sentence length and structure in text, often used in AI-generated text detection where machine-generated content tends to show lower burstiness (more uniform patterns) than human writing.",
    "url": "pages/glossary.html#term-burstiness"
  },
  {
    "id": "term-byte-fallback",
    "title": "Byte Fallback",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "byte",
      "bytes",
      "cannot",
      "characters",
      "encodes",
      "ensuring",
      "fallback",
      "individual",
      "inputs",
      "learned",
      "nlp",
      "possible"
    ],
    "excerpt": "A tokenization strategy that encodes unknown characters as individual bytes when they cannot be represented by the learned vocabulary, ensuring all possible inputs can be tokenized.",
    "url": "pages/glossary.html#term-byte-fallback"
  },
  {
    "id": "term-bpe-tokenizer",
    "title": "Byte Pair Encoding Tokenizer",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "adjacent",
      "algorithm",
      "build",
      "byte",
      "bytes",
      "characters",
      "corpus",
      "encoding",
      "frequent",
      "iteratively",
      "merges",
      "nlp"
    ],
    "excerpt": "A subword tokenization algorithm that iteratively merges the most frequent pair of adjacent bytes or characters in the training corpus to build a vocabulary of variable-length subword units.",
    "url": "pages/glossary.html#term-bpe-tokenizer"
  },
  {
    "id": "term-byte-level-tokenization",
    "title": "Byte-Level Tokenization",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "approach",
      "bpe",
      "byte",
      "bytelevel",
      "bytes",
      "characters",
      "complete",
      "coverage",
      "ensuring",
      "gpt2",
      "input",
      "level"
    ],
    "excerpt": "A tokenization approach that operates on raw bytes rather than characters, ensuring complete coverage of any text input without unknown tokens, used in models like GPT-2 with byte-level BPE.",
    "url": "pages/glossary.html#term-byte-level-tokenization"
  },
  {
    "id": "term-c2pa",
    "title": "C2PA",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "authenticity",
      "c2pa",
      "certifying",
      "coalition",
      "content",
      "creating",
      "cryptographic",
      "development",
      "ethics",
      "foundation",
      "governance"
    ],
    "excerpt": "The Coalition for Content Provenance and Authenticity, a joint development foundation creating technical standards for certifying the source and history of media content through cryptographic provenance metadata.",
    "url": "pages/glossary.html#term-c2pa"
  },
  {
    "id": "term-calibration",
    "title": "Calibration",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "calibration",
      "degree",
      "frequencies",
      "learning",
      "machine",
      "match",
      "metrics",
      "models",
      "outcomes",
      "predicted",
      "probabilities",
      "true"
    ],
    "excerpt": "The degree to which a model's predicted probabilities match the true frequencies of outcomes. A well-calibrated model predicting 80% probability should be correct approximately 80% of the time for such predictions.",
    "url": "pages/glossary.html#term-calibration"
  },
  {
    "id": "term-calibration-data",
    "title": "Calibration Data for Quantization",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "calibration",
      "data",
      "determine",
      "factors",
      "for",
      "inference",
      "infrastructure",
      "input",
      "model",
      "optimal",
      "optimization",
      "parameters"
    ],
    "excerpt": "A representative subset of input data used to determine optimal quantization parameters such as scaling factors and zero points.",
    "url": "pages/glossary.html#term-calibration-data"
  },
  {
    "id": "term-calibration-error",
    "title": "Calibration Error",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "accuracy",
      "actual",
      "being",
      "calibration",
      "closely",
      "confidence",
      "correct",
      "discrepancy",
      "empirical",
      "error",
      "evaluation",
      "level"
    ],
    "excerpt": "A metric that measures the discrepancy between a model's predicted confidence and its actual accuracy, where a well-calibrated model's stated probability of being correct closely matches its empirical accuracy at each confidence level.",
    "url": "pages/glossary.html#term-calibration-error"
  },
  {
    "id": "term-calibration-fairness",
    "title": "Calibration Fairness",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "across",
      "actual",
      "ai",
      "among",
      "assigned",
      "calibration",
      "confidence",
      "ensuring",
      "equally",
      "ethics",
      "fairness",
      "given"
    ],
    "excerpt": "A fairness metric requiring that among individuals assigned a given predicted probability, the actual proportion of positive outcomes is the same across all protected groups, ensuring that confidence scores are equally meaningful.",
    "url": "pages/glossary.html#term-calibration-fairness"
  },
  {
    "id": "term-callback",
    "title": "Callback",
    "category": "Glossary",
    "subcategory": "Technical",
    "keywords": [
      "callback",
      "called",
      "function",
      "inference",
      "points",
      "specific",
      "technical",
      "training"
    ],
    "excerpt": "A function called at specific points during training or inference. Used for logging, checkpointing, early stopping, and custom behaviors in ML pipelines.",
    "url": "pages/glossary.html#term-callback"
  },
  {
    "id": "term-camera-calibration",
    "title": "Camera Calibration",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "accurate",
      "calibration",
      "camera",
      "computer",
      "distortion",
      "essential",
      "estimating",
      "extrinsic",
      "focal",
      "intrinsic",
      "length"
    ],
    "excerpt": "The process of estimating the intrinsic parameters (focal length, principal point, distortion) and extrinsic parameters (position, orientation) of a camera, essential for accurate 3D reconstruction and measurement.",
    "url": "pages/glossary.html#term-camera-calibration"
  },
  {
    "id": "term-capability",
    "title": "Capability (AI)",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "ai",
      "assessment",
      "capability",
      "concept",
      "function",
      "perform",
      "skill",
      "specific",
      "system"
    ],
    "excerpt": "A specific skill or function an AI system can perform. Capabilities range from basic (text generation) to advanced (multi-step reasoning, tool use).",
    "url": "pages/glossary.html#term-capability"
  },
  {
    "id": "term-capability-control",
    "title": "Capability Control",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "access",
      "actuators",
      "ai",
      "alignment",
      "capability",
      "channels",
      "communication",
      "control",
      "limit",
      "measures",
      "motivational",
      "opposed"
    ],
    "excerpt": "Safety measures that limit what an AI system can do by restricting its access to resources, communication channels, or actuators, as opposed to motivational control which shapes what the system wants to do.",
    "url": "pages/glossary.html#term-capability-control"
  },
  {
    "id": "term-capsule-network",
    "title": "Capsule Network",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "capsule",
      "capsules",
      "dynamic",
      "encode",
      "features",
      "groups",
      "instantiation",
      "model",
      "network",
      "networks",
      "neural"
    ],
    "excerpt": "A neural network architecture that uses groups of neurons (capsules) to encode both the presence and instantiation parameters of features, using dynamic routing to model part-whole relationships.",
    "url": "pages/glossary.html#term-capsule-network"
  },
  {
    "id": "term-captioning",
    "title": "Captioning",
    "category": "Glossary",
    "subcategory": "Task",
    "keywords": [
      "captioning",
      "descriptions",
      "generating",
      "images",
      "multimodal",
      "task",
      "text",
      "videos"
    ],
    "excerpt": "Generating text descriptions of images or videos. A multimodal task requiring visual understanding and language generation. Used for accessibility and content organization.",
    "url": "pages/glossary.html#term-captioning"
  },
  {
    "id": "term-cataphora",
    "title": "Cataphora",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "ahead",
      "arrived",
      "called",
      "cataphora",
      "challenges",
      "entity",
      "expression",
      "john",
      "linguistic",
      "linguistics",
      "nlp",
      "phenomenon"
    ],
    "excerpt": "A linguistic phenomenon where a referring expression precedes the entity it refers to in the text, as in 'Before he arrived, John called ahead,' posing challenges for reference resolution.",
    "url": "pages/glossary.html#term-cataphora"
  },
  {
    "id": "term-catastrophic-forgetting",
    "title": "Catastrophic Forgetting",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "catastrophic",
      "challenge",
      "data",
      "forget",
      "forgetting",
      "information",
      "learned",
      "networks",
      "neural",
      "new",
      "previously",
      "tendency"
    ],
    "excerpt": "The tendency of neural networks to forget previously learned information when trained on new data. A significant challenge in continual learning and fine-tuning scenarios.",
    "url": "pages/glossary.html#term-catastrophic-forgetting"
  },
  {
    "id": "term-catastrophic-forgetting-ethics",
    "title": "Catastrophic Forgetting Ethics",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "alignment",
      "catastrophic",
      "constraints",
      "continued",
      "data",
      "ethical",
      "ethics",
      "forget",
      "forgetting",
      "implications",
      "learned"
    ],
    "excerpt": "Ethical implications of the tendency of neural networks to forget previously learned safety constraints when trained on new data, potentially undermining alignment measures during continued training.",
    "url": "pages/glossary.html#term-catastrophic-forgetting-ethics"
  },
  {
    "id": "term-catastrophic-forgetting-rl",
    "title": "Catastrophic Forgetting in RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "adapting",
      "agents",
      "catastrophic",
      "environments",
      "forgetting",
      "in",
      "learned",
      "learning",
      "lose",
      "networkbased",
      "neural",
      "new"
    ],
    "excerpt": "The tendency of neural network-based RL agents to lose previously learned skills when adapting to new tasks or environments.",
    "url": "pages/glossary.html#term-catastrophic-forgetting-rl"
  },
  {
    "id": "term-catastrophic-risk-from-ai",
    "title": "Catastrophic Risk from AI",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "catastrophic",
      "cause",
      "collapse",
      "critical",
      "damage",
      "economic",
      "environmental",
      "ethics",
      "existential",
      "falling",
      "from"
    ],
    "excerpt": "The risk that AI systems could cause large-scale irreversible harm falling short of existential risk, such as widespread economic collapse, loss of critical infrastructure, or major environmental damage.",
    "url": "pages/glossary.html#term-catastrophic-risk-from-ai"
  },
  {
    "id": "term-catboost",
    "title": "CatBoost",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "boosting",
      "catboost",
      "categorical",
      "employs",
      "features",
      "gradient",
      "handles",
      "hyperparameter",
      "learning",
      "library",
      "machine",
      "minimal"
    ],
    "excerpt": "A gradient boosting library that natively handles categorical features using ordered target statistics and employs ordered boosting to reduce prediction shift, yielding strong performance with minimal hyperparameter tuning.",
    "url": "pages/glossary.html#term-catboost"
  },
  {
    "id": "term-causal-language-model",
    "title": "Causal Language Model",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "based",
      "causal",
      "language",
      "lefttoright",
      "llm",
      "model",
      "next",
      "only",
      "predicts",
      "previous",
      "token"
    ],
    "excerpt": "A model that predicts the next token based only on previous tokens (left-to-right). GPT and most text generation models are causal. Contrast with bidirectional models like BERT.",
    "url": "pages/glossary.html#term-causal-language-model"
  },
  {
    "id": "term-causal-language-modeling",
    "title": "Causal Language Modeling",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "autoregressive",
      "based",
      "causal",
      "enforcing",
      "generation",
      "language",
      "lefttoright",
      "model",
      "modeling",
      "nlp",
      "objective",
      "only"
    ],
    "excerpt": "A training objective where the model predicts each token based only on the preceding tokens in the sequence, enforcing a left-to-right autoregressive generation order.",
    "url": "pages/glossary.html#term-causal-language-modeling"
  },
  {
    "id": "term-causal-mask",
    "title": "Causal Mask",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attending",
      "attention",
      "autoregressive",
      "causal",
      "enforcing",
      "generation",
      "language",
      "lefttoright",
      "mask",
      "networks",
      "neural"
    ],
    "excerpt": "A triangular attention mask that prevents each position from attending to subsequent positions, enforcing the autoregressive property required for left-to-right language generation.",
    "url": "pages/glossary.html#term-causal-mask"
  },
  {
    "id": "term-cbam",
    "title": "CBAM",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "applies",
      "architecture",
      "attention",
      "block",
      "cbam",
      "channel",
      "convolutional",
      "enhancing",
      "feature",
      "lightweight",
      "maps",
      "minimal"
    ],
    "excerpt": "Convolutional Block Attention Module, a lightweight attention module that sequentially applies channel and spatial attention to feature maps, enhancing representational power with minimal overhead.",
    "url": "pages/glossary.html#term-cbam"
  },
  {
    "id": "term-cbow",
    "title": "CBOW",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "average",
      "bag",
      "cbow",
      "center",
      "context",
      "continuous",
      "embeddings",
      "faster",
      "nlp",
      "objective",
      "predicts",
      "skipgram"
    ],
    "excerpt": "Continuous Bag of Words, a Word2Vec training objective that predicts a center word from the average of its surrounding context word vectors, typically faster to train than Skip-gram.",
    "url": "pages/glossary.html#term-cbow"
  },
  {
    "id": "term-ceiling-effect",
    "title": "Ceiling Effect",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "becomes",
      "benchmark",
      "ceiling",
      "distinguish",
      "easy",
      "effect",
      "evaluation",
      "maximum",
      "models",
      "near",
      "scoring",
      "too"
    ],
    "excerpt": "When a benchmark becomes too easy to distinguish between models, all scoring near the maximum. Prompts creation of harder benchmarks to continue measuring progress.",
    "url": "pages/glossary.html#term-ceiling-effect"
  },
  {
    "id": "term-censoring",
    "title": "Censoring",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "analysis",
      "because",
      "censoring",
      "condition",
      "data",
      "ended",
      "event",
      "exact",
      "followup",
      "lost",
      "observed",
      "science"
    ],
    "excerpt": "A condition in survival analysis where the exact time of an event is not observed for some subjects, typically because the study ended or the subject was lost to follow-up.",
    "url": "pages/glossary.html#term-censoring"
  },
  {
    "id": "term-centernet",
    "title": "CenterNet",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "anchor",
      "anchorfree",
      "approach",
      "associated",
      "box",
      "center",
      "centernet",
      "computer",
      "design",
      "detection",
      "eliminating",
      "nms"
    ],
    "excerpt": "An anchor-free object detection approach that represents objects as center points with associated size and offset predictions, simplifying the detection pipeline by eliminating anchor box design and NMS post-processing.",
    "url": "pages/glossary.html#term-centernet"
  },
  {
    "id": "term-central-limit-theorem",
    "title": "Central Limit Theorem",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "approaches",
      "central",
      "distribution",
      "finite",
      "fundamental",
      "increases",
      "limit",
      "mean",
      "normal",
      "original",
      "populations",
      "probability"
    ],
    "excerpt": "A fundamental theorem stating that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the population's original distribution, provided the variance is finite.",
    "url": "pages/glossary.html#term-central-limit-theorem"
  },
  {
    "id": "term-ctde",
    "title": "Centralized Training Decentralized Execution (CTDE)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "access",
      "act",
      "agents",
      "based",
      "centralized",
      "ctde",
      "decentralized",
      "execution",
      "global",
      "independently",
      "information",
      "learning"
    ],
    "excerpt": "A multi-agent RL paradigm where agents have access to global information during training but must act independently based only on local observations at test time.",
    "url": "pages/glossary.html#term-ctde"
  },
  {
    "id": "term-centroid-based-clustering-vectors",
    "title": "Centroid-Based Clustering for Vectors",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "algorithms",
      "based",
      "basis",
      "centroid",
      "centroids",
      "clustering",
      "collection",
      "compared",
      "database",
      "first",
      "for",
      "forming"
    ],
    "excerpt": "The use of clustering algorithms like k-means to partition a vector collection into groups represented by centroid vectors, forming the basis of IVF indexes where query vectors are first compared to centroids to identify relevant partitions.",
    "url": "pages/glossary.html#term-centroid-based-clustering-vectors"
  },
  {
    "id": "term-cerebras",
    "title": "Cerebras",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "cerebras",
      "company",
      "computing",
      "cores",
      "distributed",
      "hardware",
      "millions",
      "onchip",
      "processors",
      "produces",
      "semiconductor",
      "series"
    ],
    "excerpt": "A semiconductor company that produces wafer-scale AI processors (WSE series) with millions of cores and terabytes of on-chip SRAM.",
    "url": "pages/glossary.html#term-cerebras"
  },
  {
    "id": "term-chain-of-density",
    "title": "Chain of Density",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "additional",
      "asking",
      "chain",
      "concise",
      "density",
      "engineering",
      "entities",
      "entityrich",
      "increases",
      "information",
      "iteratively",
      "length"
    ],
    "excerpt": "A prompting technique that iteratively increases the information density of a summary by asking the model to rewrite it with additional entities while maintaining the same length, producing progressively more concise and entity-rich summaries.",
    "url": "pages/glossary.html#term-chain-of-density"
  },
  {
    "id": "term-chain-of-code",
    "title": "Chain-of-Code",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "allowing",
      "augments",
      "benefit",
      "chain",
      "chainofthought",
      "code",
      "codeaugmented",
      "computation",
      "engineering",
      "executable",
      "execution",
      "framework"
    ],
    "excerpt": "A reasoning framework that augments chain-of-thought with executable code generation, allowing the model to write and simulate code execution for reasoning steps that benefit from computation while using natural language for semantic reasoning.",
    "url": "pages/glossary.html#term-chain-of-code"
  },
  {
    "id": "term-chain-of-knowledge",
    "title": "Chain-of-Knowledge",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accumulated",
      "answers",
      "augmentation",
      "builds",
      "chain",
      "consistency",
      "eliciting",
      "engineering",
      "facts",
      "framework",
      "grounded",
      "information"
    ],
    "excerpt": "A prompting framework that progressively builds and refines a knowledge chain by eliciting relevant facts, verifying their consistency, and reasoning over the accumulated knowledge to produce answers grounded in verified information.",
    "url": "pages/glossary.html#term-chain-of-knowledge"
  },
  {
    "id": "term-chain-of-table",
    "title": "Chain-of-Table",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "aggregation",
      "chain",
      "data",
      "engineering",
      "filtering",
      "framework",
      "informs",
      "intermediate",
      "iteratively",
      "like",
      "new",
      "next"
    ],
    "excerpt": "A reasoning framework for tabular data that iteratively transforms tables through operations like filtering, sorting, and aggregation as intermediate reasoning steps, with each step producing a new table state that informs the next operation.",
    "url": "pages/glossary.html#term-chain-of-table"
  },
  {
    "id": "term-chain-of-thought",
    "title": "Chain-of-Thought (CoT)",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "accurate",
      "chain",
      "complex",
      "cot",
      "encourages",
      "leading",
      "of",
      "problems",
      "process",
      "prompting",
      "reasoning",
      "responses"
    ],
    "excerpt": "A prompting technique that encourages AI to show its reasoning process step-by-step, leading to more accurate and transparent responses for complex problems.",
    "url": "pages/glossary.html#term-chain-of-thought"
  },
  {
    "id": "term-cot-self-consistency",
    "title": "Chain-of-Thought with Self-Consistency",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "answer",
      "chain",
      "chainofthought",
      "combined",
      "consistency",
      "engineering",
      "final",
      "frequently",
      "generating",
      "improve",
      "majority",
      "multiple"
    ],
    "excerpt": "The combined technique of generating multiple chain-of-thought reasoning paths for a single problem using sampling and selecting the most frequently occurring final answer through majority voting to improve reasoning reliability.",
    "url": "pages/glossary.html#term-cot-self-consistency"
  },
  {
    "id": "term-channel-attention",
    "title": "Channel Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "channel",
      "channels",
      "cnn",
      "different",
      "emphasizing",
      "feature",
      "importance",
      "informative",
      "learns",
      "less"
    ],
    "excerpt": "An attention mechanism that learns to weight the importance of different feature channels in a CNN, selectively emphasizing informative channels while suppressing less useful ones.",
    "url": "pages/glossary.html#term-channel-attention"
  },
  {
    "id": "term-character-error-rate",
    "title": "Character Error Rate",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "carry",
      "character",
      "computes",
      "distance",
      "edit",
      "error",
      "evaluating",
      "evaluation",
      "finegrained",
      "level",
      "matches",
      "meaningful"
    ],
    "excerpt": "A fine-grained evaluation metric that computes the edit distance between predicted and reference texts at the character level, useful for evaluating OCR systems and speech recognition where partial word matches carry meaningful signal.",
    "url": "pages/glossary.html#term-character-error-rate"
  },
  {
    "id": "term-character-n-gram",
    "title": "Character N-gram",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "character",
      "characters",
      "classification",
      "contiguous",
      "correction",
      "extracted",
      "features",
      "gram",
      "identification",
      "language",
      "nlp",
      "processing"
    ],
    "excerpt": "A contiguous sequence of N characters extracted from a word or text, used as features for text classification, language identification, and spelling correction tasks.",
    "url": "pages/glossary.html#term-character-n-gram"
  },
  {
    "id": "term-character-level",
    "title": "Character-Level Model",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "alternative",
      "architecture",
      "character",
      "level",
      "model",
      "models",
      "process",
      "rather",
      "text",
      "tokens"
    ],
    "excerpt": "Models that process text character by character rather than using tokens. More flexible with novel words but typically slower and requiring more parameters for the same capability.",
    "url": "pages/glossary.html#term-character-level"
  },
  {
    "id": "term-charles-babbage",
    "title": "Charles Babbage",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "17911871",
      "analytical",
      "anticipated",
      "babbage",
      "charles",
      "computers",
      "computing",
      "conceived",
      "concepts",
      "difference",
      "engine",
      "english"
    ],
    "excerpt": "English mathematician and inventor (1791-1871) who conceived the Difference Engine and the Analytical Engine, mechanical general-purpose computers that anticipated key concepts in modern computing and AI.",
    "url": "pages/glossary.html#term-charles-babbage"
  },
  {
    "id": "term-chat-completion",
    "title": "Chat Completion",
    "category": "Glossary",
    "subcategory": "API",
    "keywords": [
      "api",
      "chat",
      "completion",
      "conversational",
      "endpoint",
      "format",
      "generates",
      "model",
      "responses",
      "technical",
      "type"
    ],
    "excerpt": "An API endpoint type where the model generates responses in a conversational format. Takes a list of messages (system, user, assistant) and returns the next assistant message.",
    "url": "pages/glossary.html#term-chat-completion"
  },
  {
    "id": "term-chat-template",
    "title": "Chat Template",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "assistant",
      "chat",
      "convention",
      "conversation",
      "defines",
      "delimited",
      "formatting",
      "generative",
      "inputs",
      "llm",
      "messages"
    ],
    "excerpt": "A structured formatting convention that defines how system messages, user inputs, and assistant responses are tokenized and delimited for multi-turn conversation models.",
    "url": "pages/glossary.html#term-chat-template"
  },
  {
    "id": "term-chatgpt",
    "title": "ChatGPT",
    "category": "Glossary",
    "subcategory": "Product",
    "keywords": [
      "2022",
      "chatgpt",
      "conversational",
      "launched",
      "november",
      "openai",
      "openais",
      "product"
    ],
    "excerpt": "OpenAI's conversational AI product launched in November 2022. Built on GPT models fine-tuned for dialogue, it popularized conversational AI and sparked widespread public interest in LLMs.",
    "url": "pages/glossary.html#term-chatgpt"
  },
  {
    "id": "term-chatgpt-launch",
    "title": "ChatGPT Launch",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2022",
      "built",
      "chatgpt",
      "conversational",
      "gpt3",
      "history",
      "interface",
      "launch",
      "milestones",
      "november",
      "openais",
      "release"
    ],
    "excerpt": "OpenAI's release of ChatGPT on November 30, 2022, a conversational AI interface built on GPT-3.5 that reached 100 million users in two months, triggering widespread public engagement with AI and an industry-wide AI race.",
    "url": "pages/glossary.html#term-chatgpt-launch"
  },
  {
    "id": "term-checkpoint",
    "title": "Checkpoint",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "checkpoint",
      "model",
      "saved",
      "snapshot",
      "technical",
      "training",
      "weights"
    ],
    "excerpt": "A saved snapshot of model weights during training. Enables resuming training after interruption, comparing different training stages, and selecting the best performing version.",
    "url": "pages/glossary.html#term-checkpoint"
  },
  {
    "id": "term-checkpointing-training",
    "title": "Checkpointing for Training",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "checkpointing",
      "computing",
      "distributed",
      "for",
      "metadata",
      "model",
      "optimization",
      "optimizer",
      "periodically",
      "persistent",
      "practice",
      "saving"
    ],
    "excerpt": "The practice of periodically saving model weights, optimizer state, and training metadata to persistent storage during training.",
    "url": "pages/glossary.html#term-checkpointing-training"
  },
  {
    "id": "term-chi-square-distribution",
    "title": "Chi-Square Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "chi",
      "distribution",
      "independent",
      "normal",
      "probability",
      "random",
      "square",
      "squares",
      "standard",
      "statistics",
      "sum",
      "variables"
    ],
    "excerpt": "The distribution of the sum of squares of k independent standard normal random variables. It is used in chi-square tests, confidence interval estimation for variance, and goodness-of-fit tests.",
    "url": "pages/glossary.html#term-chi-square-distribution"
  },
  {
    "id": "term-chi-square-test",
    "title": "Chi-Square Test",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "chi",
      "differ",
      "evaluates",
      "expected",
      "frequencies",
      "hypothesis",
      "inference",
      "null",
      "observed",
      "significantly",
      "square",
      "statistical"
    ],
    "excerpt": "A statistical test that evaluates whether observed frequencies differ significantly from expected frequencies under a null hypothesis.",
    "url": "pages/glossary.html#term-chi-square-test"
  },
  {
    "id": "term-chinchilla",
    "title": "Chinchilla",
    "category": "Glossary",
    "subcategory": "Research",
    "keywords": [
      "chinchilla",
      "data",
      "deepmind",
      "model",
      "optimal",
      "previously",
      "requires",
      "research",
      "scaling",
      "showing",
      "study",
      "thought"
    ],
    "excerpt": "A DeepMind model and scaling study showing optimal training requires more data than previously thought. Influenced subsequent model development toward larger datasets.",
    "url": "pages/glossary.html#term-chinchilla"
  },
  {
    "id": "term-chinchilla-optimal",
    "title": "Chinchilla Optimal",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "budget",
      "chinchilla",
      "compute",
      "data",
      "deepminds",
      "derived",
      "generative",
      "given",
      "laws",
      "llm",
      "model"
    ],
    "excerpt": "A training regime derived from DeepMind's Chinchilla scaling laws, suggesting that for a given compute budget, model size and training data should be scaled proportionally for optimal performance.",
    "url": "pages/glossary.html#term-chinchilla-optimal"
  },
  {
    "id": "term-chinchilla-paper",
    "title": "Chinchilla Paper",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2022",
      "chinchilla",
      "deepmind",
      "history",
      "hoffmann",
      "milestones",
      "paper"
    ],
    "excerpt": "The 2022 DeepMind paper by Hoffmann et al. demonstrating that many large language models were undertrained relative to their size, establishing new scaling laws suggesting that training data and model size should be scaled equally.",
    "url": "pages/glossary.html#term-chinchilla-paper"
  },
  {
    "id": "term-chinese-room-argument",
    "title": "Chinese Room Argument",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1980",
      "arguing",
      "argument",
      "cannot",
      "challenging",
      "chinese",
      "claims",
      "computer",
      "consciousness",
      "conversation",
      "even",
      "executing"
    ],
    "excerpt": "A thought experiment by John Searle in 1980 arguing that a computer executing a program cannot have genuine understanding or consciousness, even if it perfectly simulates intelligent conversation, challenging strong AI claims.",
    "url": "pages/glossary.html#term-chinese-room-argument"
  },
  {
    "id": "term-chiplet-architecture",
    "title": "Chiplet Architecture",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "approach",
      "architecture",
      "chiplet",
      "chiplets",
      "connected",
      "design",
      "dies",
      "gpu",
      "hardware",
      "highspeed",
      "interconnects",
      "multiple"
    ],
    "excerpt": "A processor design approach using multiple small silicon dies (chiplets) connected via high-speed interconnects on a single package.",
    "url": "pages/glossary.html#term-chiplet-architecture"
  },
  {
    "id": "term-chromadb",
    "title": "ChromaDB",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "api",
      "applications",
      "associated",
      "chromadb",
      "database",
      "designed",
      "embedding",
      "embeddings",
      "filtering",
      "implementations",
      "lightweight",
      "metadata"
    ],
    "excerpt": "An open-source embedding database designed for AI applications that provides a simple API for storing, querying, and filtering embeddings with associated metadata, popular for prototyping and lightweight RAG implementations.",
    "url": "pages/glossary.html#term-chromadb"
  },
  {
    "id": "term-chunk-overlap",
    "title": "Chunk Overlap",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "across",
      "adjacent",
      "boundaries",
      "characters",
      "chunk",
      "chunks",
      "consecutive",
      "contextual",
      "continuity",
      "document",
      "ensuring",
      "information"
    ],
    "excerpt": "The number of tokens or characters shared between consecutive chunks during document splitting, ensuring that information spanning chunk boundaries is not lost and maintaining contextual continuity across adjacent segments.",
    "url": "pages/glossary.html#term-chunk-overlap"
  },
  {
    "id": "term-chunk-size",
    "title": "Chunk Size",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "characters",
      "chunk",
      "chunking",
      "chunks",
      "coherence",
      "context",
      "document",
      "enable",
      "individual",
      "larger",
      "length",
      "measured"
    ],
    "excerpt": "The target length of individual text segments produced during document chunking, typically measured in tokens or characters, where smaller chunks enable more precise retrieval while larger chunks preserve more context and coherence.",
    "url": "pages/glossary.html#term-chunk-size"
  },
  {
    "id": "term-chunking",
    "title": "Chunking",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "chunking",
      "documents",
      "long",
      "pieces",
      "processing",
      "smaller",
      "splitting",
      "technique"
    ],
    "excerpt": "Splitting long documents into smaller pieces for processing. Essential for RAG and embedding systems where input length exceeds model limits or affects retrieval quality.",
    "url": "pages/glossary.html#term-chunking"
  },
  {
    "id": "term-chunking-nlp",
    "title": "Chunking",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "building",
      "chunking",
      "chunks",
      "consecutive",
      "full",
      "groups",
      "nlp",
      "nonoverlapping",
      "noun",
      "parse",
      "parsing",
      "phrases"
    ],
    "excerpt": "A shallow parsing technique that groups consecutive words into non-overlapping phrases (chunks) such as noun phrases or verb phrases without building a full parse tree.",
    "url": "pages/glossary.html#term-chunking-nlp"
  },
  {
    "id": "term-cider",
    "title": "CIDEr",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "captioning",
      "captions",
      "cider",
      "consensusbased",
      "corpus",
      "description",
      "distinguish",
      "emphasizing",
      "evaluation",
      "generated",
      "image",
      "images"
    ],
    "excerpt": "Consensus-based Image Description Evaluation, a metric that measures image captioning quality using TF-IDF weighted n-gram similarity between generated and reference captions, emphasizing informati...",
    "url": "pages/glossary.html#term-cider"
  },
  {
    "id": "term-citation-generation",
    "title": "Citation Generation",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "accuracy",
      "ai",
      "capability",
      "citation",
      "claims",
      "content",
      "documents",
      "enabling",
      "generated",
      "generation",
      "generative",
      "inline"
    ],
    "excerpt": "The capability of a language model to produce inline references to source documents that support its claims, enabling users to verify the accuracy of generated content.",
    "url": "pages/glossary.html#term-citation-generation"
  },
  {
    "id": "term-cky-algorithm",
    "title": "CKY Algorithm",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "algorithm",
      "bottomup",
      "builds",
      "chart",
      "cky",
      "cockekasamiyounger",
      "constituents",
      "contextfree",
      "dynamic",
      "filling",
      "grammars",
      "nlp"
    ],
    "excerpt": "Cocke-Kasami-Younger algorithm, a dynamic programming parser for context-free grammars that builds parse trees bottom-up in O(n^3) time by filling a chart of possible constituents.",
    "url": "pages/glossary.html#term-cky-algorithm"
  },
  {
    "id": "term-clarity",
    "title": "Clarity (Prompting)",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "best",
      "clarity",
      "clear",
      "language",
      "misinterpretation",
      "practice",
      "prompting",
      "prompts",
      "reduce",
      "unambiguous"
    ],
    "excerpt": "Using clear, unambiguous language in prompts to reduce misinterpretation. Specific instructions and explicit requirements improve response quality.",
    "url": "pages/glossary.html#term-clarity"
  },
  {
    "id": "term-class-activation-map",
    "title": "Class Activation Map",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "activation",
      "activations",
      "class",
      "classification",
      "cnns",
      "computed",
      "computer",
      "decision",
      "feature",
      "highlights",
      "image",
      "important"
    ],
    "excerpt": "A visualization technique that highlights the image regions most important for a CNN's classification decision, computed by weighting feature map activations by the classification layer's weights.",
    "url": "pages/glossary.html#term-class-activation-map"
  },
  {
    "id": "term-class-imbalance",
    "title": "Class Imbalance",
    "category": "Glossary",
    "subcategory": "Data",
    "keywords": [
      "across",
      "categories",
      "challenge",
      "class",
      "data",
      "imbalance",
      "representation",
      "training",
      "unequal"
    ],
    "excerpt": "When training data has unequal representation across categories. Can cause models to favor majority classes. Addressed through sampling, weighting, or specialized techniques.",
    "url": "pages/glossary.html#term-class-imbalance"
  },
  {
    "id": "term-class-weight",
    "title": "Class Weight",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "assigning",
      "class",
      "classes",
      "costly",
      "effectively",
      "function",
      "handling",
      "higher",
      "imbalance",
      "learning",
      "loss",
      "machine"
    ],
    "excerpt": "A technique for handling class imbalance by assigning higher weight to the minority class in the loss function, effectively making misclassification of underrepresented classes more costly during training.",
    "url": "pages/glossary.html#term-class-weight"
  },
  {
    "id": "term-classification",
    "title": "Classification",
    "category": "Glossary",
    "subcategory": "ML Task",
    "keywords": [
      "assigns",
      "categories",
      "classification",
      "data",
      "input",
      "learning",
      "machine",
      "ml",
      "predefined",
      "supervised",
      "task"
    ],
    "excerpt": "A machine learning task that assigns input data to predefined categories. Examples include spam detection (spam/not spam), sentiment analysis (positive/negative/neutral), and image recognition.",
    "url": "pages/glossary.html#term-classification"
  },
  {
    "id": "term-classifier-free-guidance",
    "title": "Classifier-Free Guidance",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "classifier",
      "conditional",
      "controlling",
      "diffusion",
      "diversity",
      "estimates",
      "free",
      "guidance",
      "interpolates",
      "models",
      "networks"
    ],
    "excerpt": "A technique for conditional diffusion models that interpolates between conditional and unconditional score estimates during sampling, controlling the trade-off between sample quality and diversity without a separate classifier.",
    "url": "pages/glossary.html#term-classifier-free-guidance"
  },
  {
    "id": "term-claude",
    "title": "Claude",
    "category": "Glossary",
    "subcategory": "Product",
    "keywords": [
      "anthropic",
      "assistant",
      "claude",
      "created",
      "designed",
      "harmless",
      "helpful",
      "honest",
      "product"
    ],
    "excerpt": "An AI assistant created by Anthropic, designed to be helpful, harmless, and honest. Known for nuanced reasoning, long context handling, and strong performance on complex tasks.",
    "url": "pages/glossary.html#term-claude"
  },
  {
    "id": "term-claude-instant",
    "title": "Claude Instant / Haiku",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "anthropic",
      "anthropics",
      "claude",
      "costeffective",
      "faster",
      "haiku",
      "instant",
      "model",
      "models",
      "simpler",
      "tasks"
    ],
    "excerpt": "Anthropic's faster, more cost-effective models for simpler tasks. Trade some capability for speed and lower cost, suitable for classification, extraction, and basic chat.",
    "url": "pages/glossary.html#term-claude-instant"
  },
  {
    "id": "term-claude-launch",
    "title": "Claude Launch",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2023",
      "anthropics",
      "assistants",
      "available",
      "claude",
      "constitutional",
      "conversational",
      "emphasizing",
      "family",
      "first",
      "harmlessness",
      "helpfulness"
    ],
    "excerpt": "Anthropic's release of Claude, a family of AI assistants trained using Constitutional AI methods, first made available in March 2023, emphasizing safety, helpfulness, and harmlessness in conversational AI.",
    "url": "pages/glossary.html#term-claude-launch"
  },
  {
    "id": "term-claude-opus",
    "title": "Claude Opus",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "anthropic",
      "anthropics",
      "capable",
      "claude",
      "complex",
      "creative",
      "designed",
      "model",
      "nuanced",
      "opus",
      "reasoning",
      "tasks"
    ],
    "excerpt": "Anthropic's most capable model, designed for complex reasoning, creative tasks, and nuanced understanding. Higher cost but best performance on difficult tasks.",
    "url": "pages/glossary.html#term-claude-opus"
  },
  {
    "id": "term-claude-shannon",
    "title": "Claude Shannon",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19162001",
      "1948",
      "american",
      "claude",
      "communication",
      "contributed",
      "digital",
      "early",
      "established",
      "father",
      "foundational",
      "foundations"
    ],
    "excerpt": "American mathematician (1916-2001) known as the father of information theory, whose 1948 paper established the mathematical foundations for digital communication and contributed foundational ideas to early AI research.",
    "url": "pages/glossary.html#term-claude-shannon"
  },
  {
    "id": "term-clip",
    "title": "CLIP",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "clip",
      "concepts",
      "contrastive",
      "descriptions",
      "encoders",
      "image",
      "images",
      "jointly",
      "language",
      "languageimage",
      "learns"
    ],
    "excerpt": "Contrastive Language-Image Pre-training, a model that learns visual concepts from natural language supervision by training image and text encoders jointly to match images with their text descriptions.",
    "url": "pages/glossary.html#term-clip"
  },
  {
    "id": "term-clipped-surrogate-objective",
    "title": "Clipped Surrogate Objective",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "clipped",
      "clips",
      "core",
      "excessively",
      "large",
      "learning",
      "new",
      "objective",
      "old",
      "optimization",
      "policies",
      "policy"
    ],
    "excerpt": "The core optimization objective in PPO that clips the probability ratio between new and old policies, preventing excessively large updates.",
    "url": "pages/glossary.html#term-clipped-surrogate-objective"
  },
  {
    "id": "term-cloud-computing-ai",
    "title": "Cloud Computing for AI",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "access",
      "ai",
      "aws",
      "azure",
      "capital",
      "cloud",
      "clusters",
      "computing",
      "distributed",
      "expenditure",
      "for",
      "gcp"
    ],
    "excerpt": "The use of cloud infrastructure services (AWS, GCP, Azure) for AI model training and inference, providing on-demand access to GPU clusters without capital expenditure.",
    "url": "pages/glossary.html#term-cloud-computing-ai"
  },
  {
    "id": "term-cloze",
    "title": "Cloze Task",
    "category": "Glossary",
    "subcategory": "Task",
    "keywords": [
      "cloze",
      "evaluation",
      "missing",
      "models",
      "predict",
      "task",
      "text",
      "words"
    ],
    "excerpt": "A task where models predict missing words in text. A classic NLP benchmark and training objective. BERT's masked language modeling is a form of cloze task.",
    "url": "pages/glossary.html#term-cloze"
  },
  {
    "id": "term-clustering",
    "title": "Clustering",
    "category": "Glossary",
    "subcategory": "ML Task",
    "keywords": [
      "clustering",
      "data",
      "groups",
      "labels",
      "learning",
      "ml",
      "points",
      "predefined",
      "similar",
      "task",
      "technique",
      "together"
    ],
    "excerpt": "An unsupervised learning technique that groups similar data points together without predefined labels. Used for customer segmentation, document organization, and pattern discovery.",
    "url": "pages/glossary.html#term-clustering"
  },
  {
    "id": "term-cmu-ai-research",
    "title": "CMU AI Research",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "ai",
      "allen",
      "autonomous",
      "carnegie",
      "cmu",
      "contributions",
      "development",
      "expert",
      "herbert",
      "history",
      "including",
      "mellon"
    ],
    "excerpt": "Carnegie Mellon University's AI research programs, including the work of Allen Newell and Herbert Simon, the development of expert systems, and pioneering contributions to robotics, speech recognition, and autonomous vehicles.",
    "url": "pages/glossary.html#term-cmu-ai-research"
  },
  {
    "id": "term-cnn",
    "title": "CNN (Convolutional Neural Network)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "cnn",
      "computer",
      "convolutional",
      "data",
      "designed",
      "gridlike",
      "images",
      "network",
      "neural",
      "processing",
      "vision"
    ],
    "excerpt": "A neural network architecture designed for processing grid-like data such as images. Uses convolutional layers to automatically learn spatial hierarchies of features.",
    "url": "pages/glossary.html#term-cnn"
  },
  {
    "id": "term-co-occurrence-matrix",
    "title": "Co-occurrence Matrix",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "across",
      "appear",
      "basis",
      "co",
      "context",
      "corpus",
      "defined",
      "distributional",
      "embeddings",
      "glove",
      "like",
      "matrix"
    ],
    "excerpt": "A matrix recording how often pairs of words appear together within a defined context window across a corpus, used as the basis for distributional word representations like GloVe.",
    "url": "pages/glossary.html#term-co-occurrence-matrix"
  },
  {
    "id": "term-coco-dataset",
    "title": "COCO Dataset",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "across",
      "annotations",
      "benchmark",
      "captioning",
      "categories",
      "coco",
      "common",
      "computer",
      "containing",
      "context",
      "dataset",
      "detection"
    ],
    "excerpt": "Common Objects in Context, a large-scale benchmark dataset containing images with annotations for object detection, instance segmentation, keypoint detection, and image captioning across 80 object categories.",
    "url": "pages/glossary.html#term-coco-dataset"
  },
  {
    "id": "term-code-generation",
    "title": "Code Generation",
    "category": "Glossary",
    "subcategory": "Application",
    "keywords": [
      "ability",
      "application",
      "code",
      "descriptions",
      "development",
      "generation",
      "language",
      "models",
      "natural",
      "programming",
      "write"
    ],
    "excerpt": "The ability of AI models to write programming code from natural language descriptions. Powers tools like GitHub Copilot, Cursor, and code-focused features in general LLMs.",
    "url": "pages/glossary.html#term-code-generation"
  },
  {
    "id": "term-code-generation-prompting",
    "title": "Code Generation Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "algorithmic",
      "cases",
      "code",
      "constraints",
      "correct",
      "docstrings",
      "efficient",
      "engineering",
      "function",
      "generation",
      "guide",
      "highquality"
    ],
    "excerpt": "Specialized prompting techniques for producing high-quality code, incorporating language specification, function signatures, docstrings, test cases, and algorithmic constraints to guide models toward correct and efficient implementations.",
    "url": "pages/glossary.html#term-code-generation-prompting"
  },
  {
    "id": "term-code-interpreter",
    "title": "Code Interpreter",
    "category": "Glossary",
    "subcategory": "Feature",
    "keywords": [
      "analysis",
      "capability",
      "code",
      "computation",
      "data",
      "enabling",
      "execute",
      "feature",
      "interpreter",
      "tool",
      "use",
      "visualization"
    ],
    "excerpt": "AI capability to write and execute code, enabling data analysis, visualization, and computation. ChatGPT's code interpreter runs Python in a sandbox environment.",
    "url": "pages/glossary.html#term-code-interpreter"
  },
  {
    "id": "term-code-llm",
    "title": "Code LLM",
    "category": "Glossary",
    "subcategory": "Model Type",
    "keywords": [
      "code",
      "language",
      "llm",
      "model",
      "models",
      "programming",
      "specialized",
      "tasks",
      "type"
    ],
    "excerpt": "Language models specialized for programming tasks. Examples include Codex, StarCoder, and Code Llama. Often trained on large code corpora from GitHub and similar sources.",
    "url": "pages/glossary.html#term-code-llm"
  },
  {
    "id": "term-code-switching",
    "title": "Code-Switching",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "alternating",
      "challenges",
      "code",
      "conversation",
      "designed",
      "languages",
      "linguistics",
      "monolingual",
      "nlp",
      "phenomenon",
      "posing",
      "processing"
    ],
    "excerpt": "The phenomenon of alternating between two or more languages within a single conversation or utterance, posing challenges for NLP systems designed for monolingual text processing.",
    "url": "pages/glossary.html#term-code-switching"
  },
  {
    "id": "term-codebleu",
    "title": "CodeBLEU",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "abstract",
      "analysis",
      "benchmarks",
      "bleu",
      "capturing",
      "code",
      "codebleu",
      "codespecific",
      "components",
      "correctness",
      "dataflow",
      "evaluation"
    ],
    "excerpt": "A code evaluation metric that extends BLEU with code-specific components including abstract syntax tree matching, data-flow analysis, and weighted n-gram matching, capturing both syntactic correctness and semantic similarity of generated code.",
    "url": "pages/glossary.html#term-codebleu"
  },
  {
    "id": "term-cognitive-load",
    "title": "Cognitive Load (Prompting)",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "best",
      "cognitive",
      "complex",
      "effort",
      "load",
      "mental",
      "practice",
      "process",
      "prompting",
      "prompts",
      "required"
    ],
    "excerpt": "The mental effort required to process complex prompts. Simpler, well-organized prompts often yield better results by reducing the model's processing burden.",
    "url": "pages/glossary.html#term-cognitive-load"
  },
  {
    "id": "term-cohens-kappa",
    "title": "Cohen&amp;#x27;s Kappa",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "accounts",
      "agreement",
      "categorical",
      "chance",
      "cohenampx27s",
      "interrater",
      "items",
      "kappa",
      "measuring",
      "metrics",
      "occurring",
      "statistic"
    ],
    "excerpt": "A statistic measuring inter-rater agreement for categorical items that accounts for agreement occurring by chance. Values range from -1 to 1, with 1 indicating perfect agreement beyond chance.",
    "url": "pages/glossary.html#term-cohens-kappa"
  },
  {
    "id": "term-cohere",
    "title": "Cohere",
    "category": "Glossary",
    "subcategory": "Company",
    "keywords": [
      "cohere",
      "company",
      "embeddings",
      "enterprise",
      "generation",
      "llm",
      "llms",
      "provider",
      "providing",
      "search",
      "text"
    ],
    "excerpt": "An enterprise AI company providing LLMs for text generation, embeddings, and search. Known for Command models and focus on enterprise use cases with strong RAG capabilities.",
    "url": "pages/glossary.html#term-cohere"
  },
  {
    "id": "term-coherence-modeling",
    "title": "Coherence Modeling",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "assessment",
      "coherence",
      "computational",
      "consistent",
      "evaluating",
      "flow",
      "linguistics",
      "logically",
      "maintains",
      "modeling",
      "naturally",
      "nlp"
    ],
    "excerpt": "The computational assessment of how well sentences in a text flow together logically and topically, evaluating whether a text reads naturally and maintains consistent themes and references.",
    "url": "pages/glossary.html#term-coherence-modeling"
  },
  {
    "id": "term-coherence-score",
    "title": "Coherence Score",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "assesses",
      "coherence",
      "connect",
      "consistency",
      "evaluation",
      "flow",
      "form",
      "generated",
      "ideas",
      "logical",
      "maintain",
      "measuring"
    ],
    "excerpt": "An evaluation metric that assesses the logical consistency and semantic flow of generated text, measuring whether ideas connect naturally, maintain topical consistency, and form a well-structured narrative.",
    "url": "pages/glossary.html#term-coherence-score"
  },
  {
    "id": "term-cointegration",
    "title": "Cointegration",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "cointegration",
      "combination",
      "common",
      "data",
      "linear",
      "meaning",
      "nonstationary",
      "property",
      "science",
      "series",
      "share",
      "stationary"
    ],
    "excerpt": "A statistical property of two or more non-stationary time series that share a common stochastic trend, meaning a linear combination of them is stationary.",
    "url": "pages/glossary.html#term-cointegration"
  },
  {
    "id": "term-colbert",
    "title": "ColBERT",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "colbert",
      "documents",
      "efficient",
      "embeddings",
      "encodes",
      "generative",
      "independently",
      "lateinteraction",
      "llm",
      "maxsim",
      "model"
    ],
    "excerpt": "A late-interaction retrieval model that independently encodes queries and documents into per-token embeddings, then scores relevance through efficient MaxSim operations between the two sets of embeddings.",
    "url": "pages/glossary.html#term-colbert"
  },
  {
    "id": "term-cold-start",
    "title": "Cold Start Problem",
    "category": "Glossary",
    "subcategory": "Challenge",
    "keywords": [
      "challenge",
      "cold",
      "data",
      "difficulty",
      "historical",
      "items",
      "making",
      "new",
      "predictions",
      "problem",
      "recommendations",
      "start"
    ],
    "excerpt": "Difficulty making predictions for new users or items with no historical data. Common in recommendation systems. Addressed with hybrid approaches combining collaborative and content-based methods.",
    "url": "pages/glossary.html#term-cold-start"
  },
  {
    "id": "term-collaborative-filtering",
    "title": "Collaborative Filtering",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "based",
      "behavior",
      "collaborative",
      "filtering",
      "patterns",
      "recommendation",
      "recommendations",
      "technique",
      "user"
    ],
    "excerpt": "Recommendation technique based on user behavior patterns. \"Users who liked X also liked Y.\" Forms the basis of many recommendation systems at Netflix, Amazon, etc.",
    "url": "pages/glossary.html#term-collaborative-filtering"
  },
  {
    "id": "term-collection",
    "title": "Collection",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "analogous",
      "associated",
      "collection",
      "database",
      "databases",
      "embeddings",
      "grouping",
      "infrastructure",
      "metadata",
      "named",
      "organizational",
      "primary"
    ],
    "excerpt": "A named grouping of vectors and their associated metadata within a vector database, analogous to a table in relational databases, serving as the primary organizational unit for storing and querying related embeddings.",
    "url": "pages/glossary.html#term-collection"
  },
  {
    "id": "term-collective-communication",
    "title": "Collective Communication",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "allgather",
      "allreduce",
      "among",
      "broadcast",
      "collective",
      "communication",
      "computing",
      "coordinated",
      "data",
      "distributed",
      "exchange",
      "gpus"
    ],
    "excerpt": "Coordinated data exchange patterns among multiple processes or GPUs, including all-reduce, all-gather, reduce-scatter, and broadcast.",
    "url": "pages/glossary.html#term-collective-communication"
  },
  {
    "id": "term-collocation",
    "title": "Collocation",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "chance",
      "coffee",
      "collocation",
      "conventional",
      "cooccur",
      "decision",
      "expected",
      "expressions",
      "forming",
      "frequently",
      "identified",
      "linguistics"
    ],
    "excerpt": "A sequence of words that co-occur more frequently than expected by chance, forming conventional expressions such as 'strong coffee' or 'make a decision' that are identified through statistical measures.",
    "url": "pages/glossary.html#term-collocation"
  },
  {
    "id": "term-colossus-computer",
    "title": "Colossus Computer",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19431944",
      "bletchley",
      "break",
      "built",
      "cipher",
      "colossus",
      "computer",
      "computers",
      "crucial",
      "digital",
      "electronic",
      "first"
    ],
    "excerpt": "The world's first programmable electronic digital computer, built at Bletchley Park in 1943-1944 to break German Lorenz cipher messages, representing a crucial step toward the general-purpose computers needed for AI.",
    "url": "pages/glossary.html#term-colossus-computer"
  },
  {
    "id": "term-command-model",
    "title": "Command Model",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "applications",
      "business",
      "cohere",
      "coheres",
      "command",
      "commands",
      "following",
      "instructiontuned",
      "llms",
      "model",
      "optimized"
    ],
    "excerpt": "Cohere's instruction-tuned LLMs optimized for following commands and business applications. Includes Command R for RAG and enterprise use cases.",
    "url": "pages/glossary.html#term-command-model"
  },
  {
    "id": "term-common-crawl",
    "title": "Common Crawl",
    "category": "Glossary",
    "subcategory": "Data",
    "keywords": [
      "common",
      "crawl",
      "data",
      "llms",
      "many",
      "massive",
      "open",
      "repository",
      "train",
      "training",
      "web"
    ],
    "excerpt": "A massive open repository of web data used to train many LLMs. Contains petabytes of text crawled from the internet, requiring careful filtering for quality and safety.",
    "url": "pages/glossary.html#term-common-crawl"
  },
  {
    "id": "term-commonsense-reasoning",
    "title": "Commonsense Reasoning",
    "category": "Glossary",
    "subcategory": "Capability",
    "keywords": [
      "ability",
      "ais",
      "capability",
      "commonsense",
      "everyday",
      "granted",
      "humans",
      "knowledge",
      "reasoning",
      "take",
      "understand"
    ],
    "excerpt": "AI's ability to understand everyday knowledge humans take for granted. That water is wet, objects fall down, people need sleep. A challenging area where LLMs have improved dramatically.",
    "url": "pages/glossary.html#term-commonsense-reasoning"
  },
  {
    "id": "term-communication-marl",
    "title": "Communication in Multi-Agent RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "agents",
      "allow",
      "channels",
      "communication",
      "in",
      "information",
      "learned",
      "learning",
      "mechanisms",
      "multi",
      "multiagent"
    ],
    "excerpt": "Protocols and mechanisms that allow agents in a multi-agent system to share information through learned communication channels.",
    "url": "pages/glossary.html#term-communication-marl"
  },
  {
    "id": "term-communication-overlap",
    "title": "Communication Overlap",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "backward",
      "behind",
      "communication",
      "computation",
      "computing",
      "distributed",
      "gradient",
      "hiding",
      "latency",
      "model",
      "optimization",
      "overlap"
    ],
    "excerpt": "A distributed training optimization that overlaps gradient communication with backward pass computation, hiding communication latency behind useful work.",
    "url": "pages/glossary.html#term-communication-overlap"
  },
  {
    "id": "term-competitive-rl",
    "title": "Competitive Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agents",
      "competitive",
      "games",
      "learning",
      "multiagent",
      "objectives",
      "opposing",
      "reinforcement",
      "setting",
      "zerosum"
    ],
    "excerpt": "A multi-agent RL setting where agents have opposing objectives, such as zero-sum games. Competitive RL involves finding Nash equilibria and developing strategies robust to adversarial opponents.",
    "url": "pages/glossary.html#term-competitive-rl"
  },
  {
    "id": "term-compile-time-graph-optimization",
    "title": "Compile-Time Graph Optimization",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "code",
      "compile",
      "computation",
      "constant",
      "dead",
      "elimination",
      "execution",
      "folding",
      "fusion",
      "graph",
      "graphs",
      "including"
    ],
    "excerpt": "Static optimization of computation graphs before execution, including constant folding, dead code elimination, and operator fusion.",
    "url": "pages/glossary.html#term-compile-time-graph-optimization"
  },
  {
    "id": "term-completion",
    "title": "Completion",
    "category": "Glossary",
    "subcategory": "Task",
    "keywords": [
      "completion",
      "continue",
      "fundamentals",
      "generated",
      "given",
      "prompt",
      "task",
      "text"
    ],
    "excerpt": "Text generated by an AI to continue a given prompt. The basic operation of language models: given input text, predict what comes next.",
    "url": "pages/glossary.html#term-completion"
  },
  {
    "id": "term-complexity-based-prompting",
    "title": "Complexity-Based Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accurate",
      "answer",
      "answers",
      "based",
      "chains",
      "complexity",
      "detailed",
      "engineering",
      "final",
      "highest",
      "measured",
      "number"
    ],
    "excerpt": "A self-consistency variant that selects the final answer from reasoning chains with the highest complexity, measured by the number of reasoning steps, based on the observation that more detailed reasoning chains tend to produce more accurate answers.",
    "url": "pages/glossary.html#term-complexity-based-prompting"
  },
  {
    "id": "term-compositionality",
    "title": "Compositionality",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "combine",
      "complex",
      "compositionality",
      "concept",
      "determined",
      "expression",
      "formal",
      "foundational",
      "linguistics",
      "meaning",
      "meanings",
      "nlp"
    ],
    "excerpt": "The principle that the meaning of a complex expression is determined by the meanings of its parts and the rules used to combine them, a foundational concept in formal semantics.",
    "url": "pages/glossary.html#term-compositionality"
  },
  {
    "id": "term-compression",
    "title": "Compression (Model)",
    "category": "Glossary",
    "subcategory": "Optimization",
    "keywords": [
      "compression",
      "deployment",
      "maintaining",
      "model",
      "optimization",
      "performance",
      "reducing",
      "size",
      "while"
    ],
    "excerpt": "Reducing model size while maintaining performance. Techniques include quantization, pruning, and distillation. Enables deployment on edge devices and reduces costs.",
    "url": "pages/glossary.html#term-compression"
  },
  {
    "id": "term-compute",
    "title": "Compute",
    "category": "Glossary",
    "subcategory": "Infrastructure",
    "keywords": [
      "computational",
      "compute",
      "infrastructure",
      "models",
      "required",
      "resources",
      "running",
      "training"
    ],
    "excerpt": "Computational resources required for training and running AI models. Measured in FLOPs, GPU-hours, or dollars. A primary constraint and cost driver in AI development.",
    "url": "pages/glossary.html#term-compute"
  },
  {
    "id": "term-compute-governance",
    "title": "Compute Governance",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "ai",
      "approaches",
      "chips",
      "computational",
      "compute",
      "computeintensive",
      "controls",
      "development",
      "export",
      "governance",
      "including",
      "large"
    ],
    "excerpt": "Policy approaches that use computational resources as a lever for AI governance, including monitoring large training runs, export controls on AI chips, and reporting requirements for compute-intensive AI development.",
    "url": "pages/glossary.html#term-compute-governance"
  },
  {
    "id": "term-compute-bound",
    "title": "Compute-Bound Workload",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "arithmetic",
      "bandwidth",
      "bound",
      "computation",
      "compute",
      "hardware",
      "limited",
      "memory",
      "model",
      "optimization",
      "performance",
      "processing"
    ],
    "excerpt": "A processing task where performance is limited by the rate of arithmetic computation rather than memory bandwidth or I/O.",
    "url": "pages/glossary.html#term-compute-bound"
  },
  {
    "id": "term-compute-optimal-training",
    "title": "Compute-Optimal Training",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "allocation",
      "approach",
      "based",
      "best",
      "budget",
      "compute",
      "empirical",
      "fixed",
      "generative",
      "law",
      "llm"
    ],
    "excerpt": "An approach to model training that seeks the best allocation of a fixed compute budget between model parameters and training tokens, based on empirical scaling law research.",
    "url": "pages/glossary.html#term-compute-optimal-training"
  },
  {
    "id": "term-computer-vision",
    "title": "Computer Vision",
    "category": "Glossary",
    "subcategory": "Field",
    "keywords": [
      "computer",
      "enables",
      "field",
      "images",
      "information",
      "interpret",
      "machines",
      "understand",
      "videos",
      "vision",
      "visual"
    ],
    "excerpt": "The field of AI that enables machines to interpret and understand visual information from images and videos. Applications include object detection, facial recognition, and medical imaging.",
    "url": "pages/glossary.html#term-computer-vision"
  },
  {
    "id": "term-computer-vision-history",
    "title": "Computer Vision History",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1960s",
      "2012",
      "alexnet",
      "computer",
      "deep",
      "detection",
      "early",
      "edge",
      "evolution",
      "featurebased",
      "history",
      "hog"
    ],
    "excerpt": "The evolution of computer vision from early edge detection and pattern recognition in the 1960s through feature-based methods like SIFT and HOG to the deep learning revolution triggered by AlexNet in 2012.",
    "url": "pages/glossary.html#term-computer-vision-history"
  },
  {
    "id": "term-concept-drift",
    "title": "Concept Drift",
    "category": "Glossary",
    "subcategory": "Challenge",
    "keywords": [
      "causing",
      "challenge",
      "changes",
      "concept",
      "degrade",
      "drift",
      "input",
      "model",
      "output",
      "performance",
      "production",
      "relationship"
    ],
    "excerpt": "When the relationship between input and output changes over time, causing model performance to degrade. Requires monitoring and retraining to maintain accuracy.",
    "url": "pages/glossary.html#term-concept-drift"
  },
  {
    "id": "term-conditional-gan",
    "title": "Conditional GAN",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "additional",
      "architecture",
      "attributes",
      "categories",
      "class",
      "conditional",
      "conditioning",
      "controlled",
      "discriminator",
      "enabling",
      "gan",
      "generation"
    ],
    "excerpt": "A GAN variant where both generator and discriminator receive additional conditioning information such as class labels or text, enabling controlled generation of specific categories or attributes.",
    "url": "pages/glossary.html#term-conditional-gan"
  },
  {
    "id": "term-conditional-generation",
    "title": "Conditional Generation",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "based",
      "conditional",
      "conditions",
      "content",
      "generating",
      "generation",
      "inputs",
      "specific",
      "technique"
    ],
    "excerpt": "Generating content based on specific conditions or inputs. Image generation conditioned on text, or text generation conditioned on a topic or style.",
    "url": "pages/glossary.html#term-conditional-generation"
  },
  {
    "id": "term-crf",
    "title": "Conditional Random Field",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "adjacent",
      "capturing",
      "conditional",
      "dependencies",
      "discriminative",
      "field",
      "given",
      "label",
      "labeling",
      "labels",
      "model",
      "models"
    ],
    "excerpt": "A discriminative probabilistic model for sequence labeling that models the conditional probability of label sequences given observations, capturing dependencies between adjacent labels.",
    "url": "pages/glossary.html#term-crf"
  },
  {
    "id": "term-confabulation",
    "title": "Confabulation",
    "category": "Glossary",
    "subcategory": "Risk",
    "keywords": [
      "another",
      "confabulation",
      "false",
      "generates",
      "hallucinationwhen",
      "information",
      "limitation",
      "plausible",
      "risk",
      "term"
    ],
    "excerpt": "Another term for hallucinationwhen AI generates plausible but false information. The model \"fills in gaps\" with invented content that sounds convincing.",
    "url": "pages/glossary.html#term-confabulation"
  },
  {
    "id": "term-confidence-interval",
    "title": "Confidence Interval",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "confidence",
      "constructed",
      "contain",
      "data",
      "inference",
      "interval",
      "many",
      "parameter",
      "percentage",
      "population",
      "procedure",
      "range"
    ],
    "excerpt": "A range of values constructed from sample data that, if the sampling procedure were repeated many times, would contain the true population parameter a specified percentage (e.g., 95%) of the time.",
    "url": "pages/glossary.html#term-confidence-interval"
  },
  {
    "id": "term-confidence-score",
    "title": "Confidence Score",
    "category": "Glossary",
    "subcategory": "Metrics",
    "keywords": [
      "certain",
      "confidence",
      "evaluation",
      "indicating",
      "metrics",
      "model",
      "numerical",
      "output",
      "prediction",
      "score",
      "value"
    ],
    "excerpt": "A numerical value indicating how certain a model is about its prediction or output. Higher scores suggest the model is more sure, though confidence doesn't always correlate with accuracy.",
    "url": "pages/glossary.html#term-confidence-score"
  },
  {
    "id": "term-confidence-threshold-cv",
    "title": "Confidence Threshold",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "accept",
      "balancing",
      "computer",
      "confidence",
      "detection",
      "detections",
      "false",
      "high",
      "including",
      "low",
      "minimum",
      "missing"
    ],
    "excerpt": "The minimum prediction score required to accept a detection as valid, balancing between missing true detections (high threshold) and including false positives (low threshold).",
    "url": "pages/glossary.html#term-confidence-threshold-cv"
  },
  {
    "id": "term-confirmation-bias-in-ai",
    "title": "Confirmation Bias in AI",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "ai",
      "beliefs",
      "bias",
      "biased",
      "confirm",
      "confirmation",
      "criteria",
      "data",
      "design",
      "developers",
      "ethics",
      "evaluation"
    ],
    "excerpt": "The tendency for AI developers or users to favor data, model outputs, or evaluation criteria that confirm pre-existing beliefs, leading to biased system design and selective interpretation of results.",
    "url": "pages/glossary.html#term-confirmation-bias-in-ai"
  },
  {
    "id": "term-conformer",
    "title": "Conformer",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "automatic",
      "block",
      "capturing",
      "combines",
      "conformer",
      "convolution",
      "dependencies",
      "global",
      "improved",
      "local",
      "modules"
    ],
    "excerpt": "A speech processing architecture that combines convolution and transformer modules in each block, capturing both local and global dependencies for improved automatic speech recognition.",
    "url": "pages/glossary.html#term-conformer"
  },
  {
    "id": "term-conformity-assessment-for-ai",
    "title": "Conformity Assessment for AI",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "act",
      "ai",
      "assessment",
      "audit",
      "being",
      "conformity",
      "evaluation",
      "for",
      "formal",
      "governance",
      "highrisk",
      "including"
    ],
    "excerpt": "The formal evaluation process required under the EU AI Act to verify that high-risk AI systems meet regulatory requirements before being placed on the market, including both self-assessment and third-party audit pathways.",
    "url": "pages/glossary.html#term-conformity-assessment-for-ai"
  },
  {
    "id": "term-confounding-variable",
    "title": "Confounding Variable",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "association",
      "confounding",
      "creating",
      "data",
      "dependent",
      "independent",
      "influences",
      "science",
      "spurious",
      "statistics",
      "them",
      "variable"
    ],
    "excerpt": "A variable that influences both the independent and dependent variables, creating a spurious association between them. Failure to control for confounders can lead to incorrect causal conclusions.",
    "url": "pages/glossary.html#term-confounding-variable"
  },
  {
    "id": "term-confusion-matrix",
    "title": "Confusion Matrix",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "class",
      "confusion",
      "correct",
      "evaluation",
      "incorrect",
      "matrix",
      "predictions",
      "showing",
      "table",
      "visualization"
    ],
    "excerpt": "A table showing correct and incorrect predictions for each class. Reveals where a classification model makes mistakes, enabling targeted improvements.",
    "url": "pages/glossary.html#term-confusion-matrix"
  },
  {
    "id": "term-conjugate-prior",
    "title": "Conjugate Prior",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bayes",
      "bayesian",
      "combined",
      "conjugate",
      "distribution",
      "family",
      "function",
      "likelihood",
      "methods",
      "particular",
      "posterior",
      "prior"
    ],
    "excerpt": "A prior distribution that, when combined with a particular likelihood function via Bayes' theorem, yields a posterior distribution in the same family as the prior.",
    "url": "pages/glossary.html#term-conjugate-prior"
  },
  {
    "id": "term-connectionism-vs-symbolism",
    "title": "Connectionism vs Symbolism",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "approaches",
      "connectionism",
      "connectionist",
      "debate",
      "distributed",
      "explicit",
      "historical",
      "history",
      "knowledge",
      "learn",
      "logic",
      "milestones"
    ],
    "excerpt": "The historical debate in AI between connectionist approaches using neural networks that learn distributed representations and symbolic approaches using explicit rules and logic for knowledge representation and reasoning.",
    "url": "pages/glossary.html#term-connectionism-vs-symbolism"
  },
  {
    "id": "term-consent-laundering",
    "title": "Consent Laundering",
    "category": "Glossary",
    "subcategory": "Privacy",
    "keywords": [
      "agreed",
      "ai",
      "anticipated",
      "collection",
      "consent",
      "data",
      "ethics",
      "laundering",
      "meaningfully",
      "neither",
      "nor",
      "obtaining"
    ],
    "excerpt": "The practice of obtaining user consent for data collection through opaque terms of service and then repurposing that data for AI training in ways that users neither anticipated nor meaningfully agreed to.",
    "url": "pages/glossary.html#term-consent-laundering"
  },
  {
    "id": "term-conservative-q-learning",
    "title": "Conservative Q-Learning (CQL)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "adds",
      "algorithm",
      "avoid",
      "conservative",
      "cql",
      "estimates",
      "learning",
      "methods",
      "offline",
      "outofdistribution",
      "overestimation"
    ],
    "excerpt": "An offline RL algorithm that adds a regularizer to penalize Q-values for out-of-distribution actions, producing conservative value estimates that avoid overestimation of unseen state-action pairs.",
    "url": "pages/glossary.html#term-conservative-q-learning"
  },
  {
    "id": "term-consistency",
    "title": "Consistency",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "approaches",
      "consistency",
      "converges",
      "estimator",
      "indicating",
      "inference",
      "infinity",
      "parameter",
      "probability",
      "property",
      "sample",
      "size"
    ],
    "excerpt": "A property of a statistical estimator indicating that it converges in probability to the true parameter value as the sample size approaches infinity.",
    "url": "pages/glossary.html#term-consistency"
  },
  {
    "id": "term-consistency-model",
    "title": "Consistency Model",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "along",
      "architecture",
      "consistency",
      "denoising",
      "diffusion",
      "directly",
      "enabling",
      "fewstep",
      "generation",
      "generative",
      "iterative",
      "learns"
    ],
    "excerpt": "A generative model that learns to map any point along a diffusion trajectory directly to the trajectory's starting point, enabling one-step or few-step generation without iterative denoising.",
    "url": "pages/glossary.html#term-consistency-model"
  },
  {
    "id": "term-consistency-based-self-evaluation",
    "title": "Consistency-Based Self-Evaluation",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "across",
      "agreement",
      "assesses",
      "based",
      "confidence",
      "consistency",
      "correctness",
      "evaluation",
      "generating",
      "high",
      "language",
      "llmbased"
    ],
    "excerpt": "An evaluation method where a language model assesses the quality of its own outputs by generating multiple responses and measuring agreement across them, using high consistency as a proxy for confidence and correctness.",
    "url": "pages/glossary.html#term-consistency-based-self-evaluation"
  },
  {
    "id": "term-constituency-parsing",
    "title": "Constituency Parsing",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "according",
      "analyzing",
      "breaking",
      "constituency",
      "constituents",
      "grammar",
      "group",
      "hierarchical",
      "larger",
      "nested",
      "nlp",
      "parsing"
    ],
    "excerpt": "The task of analyzing sentence structure by breaking it into hierarchical nested constituents (phrases) according to a grammar, producing a tree showing how words group into larger syntactic units.",
    "url": "pages/glossary.html#term-constituency-parsing"
  },
  {
    "id": "term-constitutional-ai",
    "title": "Constitutional AI",
    "category": "Glossary",
    "subcategory": "Safety",
    "keywords": [
      "ai",
      "alignment",
      "anthropic",
      "anthropics",
      "approach",
      "behavior",
      "constitution",
      "constitutional",
      "follow",
      "guide",
      "models",
      "principles"
    ],
    "excerpt": "Anthropic's approach to AI alignment where models are trained to follow a set of principles (\"constitution\") that guide their behavior. Reduces reliance on human feedback for safety training.",
    "url": "pages/glossary.html#term-constitutional-ai"
  },
  {
    "id": "term-constitutional-ai-training",
    "title": "Constitutional AI Training",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "according",
      "ai",
      "alignment",
      "constitutional",
      "critiques",
      "feedback",
      "generative",
      "human",
      "llm",
      "methodology",
      "model",
      "outputs"
    ],
    "excerpt": "A training methodology where the model critiques and revises its own outputs according to a set of written principles, reducing reliance on human feedback for alignment.",
    "url": "pages/glossary.html#term-constitutional-ai-training"
  },
  {
    "id": "term-constitutional-prompting",
    "title": "Constitutional Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "approach",
      "constitutional",
      "constraint",
      "declarative",
      "enabling",
      "engineering",
      "explicit",
      "follow",
      "generating",
      "guidelines",
      "model",
      "must"
    ],
    "excerpt": "A prompting approach that provides the model with an explicit set of principles, rules, or constitutional guidelines that it must follow when generating responses, enabling value-aligned outputs through declarative constraint specification.",
    "url": "pages/glossary.html#term-constitutional-prompting"
  },
  {
    "id": "term-constrained-beam-search",
    "title": "Constrained Beam Search",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "appear",
      "beam",
      "certain",
      "constrained",
      "constraints",
      "decoding",
      "enforces",
      "ensuring",
      "generated",
      "generative",
      "lexical"
    ],
    "excerpt": "A beam search variant that enforces lexical or structural constraints during decoding, ensuring that certain tokens or phrases must appear in the generated output.",
    "url": "pages/glossary.html#term-constrained-beam-search"
  },
  {
    "id": "term-constrained-rl",
    "title": "Constrained Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "constrained",
      "constraint",
      "costs",
      "expected",
      "formulation",
      "functions",
      "learning",
      "maximizes",
      "one",
      "reinforcement",
      "return"
    ],
    "excerpt": "An RL formulation where the agent maximizes expected return while satisfying one or more constraint functions on expected costs.",
    "url": "pages/glossary.html#term-constrained-rl"
  },
  {
    "id": "term-constraint",
    "title": "Constraint (Prompting)",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "constraint",
      "limitations",
      "prompt",
      "prompting",
      "requirements",
      "specified",
      "technique"
    ],
    "excerpt": "Limitations or requirements specified in a prompt. \"Respond in 50 words or less\" or \"Use only formal language.\" Constraints shape and focus AI output.",
    "url": "pages/glossary.html#term-constraint"
  },
  {
    "id": "term-constraint-prompting",
    "title": "Constraint Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "boundaries",
      "constraint",
      "constraints",
      "content",
      "engineering",
      "explicit",
      "format",
      "generated",
      "length",
      "limits",
      "model",
      "must"
    ],
    "excerpt": "A technique that specifies explicit constraints within the prompt such as length limits, format requirements, vocabulary restrictions, or content boundaries that the model must satisfy in its generated output.",
    "url": "pages/glossary.html#term-constraint-prompting"
  },
  {
    "id": "term-content-authenticity-initiative",
    "title": "Content Authenticity Initiative",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "adobe",
      "ai",
      "aigenerated",
      "attributing",
      "authentic",
      "authenticity",
      "coalition",
      "content",
      "develops",
      "digital",
      "distinguish",
      "ethics"
    ],
    "excerpt": "An industry coalition led by Adobe that develops open standards for attributing and verifying the provenance of digital content, helping distinguish authentic media from AI-generated or manipulated material.",
    "url": "pages/glossary.html#term-content-authenticity-initiative"
  },
  {
    "id": "term-content-filtering",
    "title": "Content Filtering",
    "category": "Glossary",
    "subcategory": "Safety",
    "keywords": [
      "block",
      "content",
      "detect",
      "filtering",
      "harmful",
      "inputs",
      "moderation",
      "outputs",
      "safety",
      "systems"
    ],
    "excerpt": "Systems that detect and block harmful content in AI inputs or outputs. Part of safety infrastructure, filtering violence, explicit content, and other policy violations.",
    "url": "pages/glossary.html#term-content-filtering"
  },
  {
    "id": "term-content-moderation",
    "title": "Content Moderation",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "assisted",
      "classifiers",
      "community",
      "content",
      "detecting",
      "digital",
      "enforce",
      "ethics",
      "filtering",
      "governance",
      "hate"
    ],
    "excerpt": "The process of monitoring and filtering user-generated content on digital platforms to enforce community standards, increasingly assisted by AI classifiers for detecting hate speech, violence, and other policy violations.",
    "url": "pages/glossary.html#term-content-moderation"
  },
  {
    "id": "term-context",
    "title": "Context",
    "category": "Glossary",
    "subcategory": "General",
    "keywords": [
      "background",
      "context",
      "helps",
      "information",
      "needs",
      "provided",
      "situation",
      "understand",
      "your"
    ],
    "excerpt": "Background information provided to AI that helps it understand your situation and needs. Essential for getting relevant, accurate responses.",
    "url": "pages/glossary.html#term-context"
  },
  {
    "id": "term-context-distillation",
    "title": "Context Distillation",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "behavior",
      "context",
      "distillation",
      "elicited",
      "eliminating",
      "generative",
      "include",
      "inference",
      "llm",
      "models",
      "need"
    ],
    "excerpt": "A training technique that transfers the behavior elicited by a specific prompt or context into the model's weights, eliminating the need to include that context at inference time.",
    "url": "pages/glossary.html#term-context-distillation"
  },
  {
    "id": "term-context-length",
    "title": "Context Length",
    "category": "Glossary",
    "subcategory": "Specification",
    "keywords": [
      "amount",
      "context",
      "length",
      "limitation",
      "maximum",
      "measured",
      "model",
      "process",
      "specification",
      "text",
      "tokens"
    ],
    "excerpt": "The maximum amount of text a model can process at once, measured in tokens. Ranges from 4K to 200K+ depending on the model. Longer contexts enable more complex tasks.",
    "url": "pages/glossary.html#term-context-length"
  },
  {
    "id": "term-context-parallelism",
    "title": "Context Parallelism",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "across",
      "along",
      "approach",
      "attention",
      "computation",
      "computing",
      "context",
      "dimension",
      "distributed",
      "distributes",
      "gpus",
      "length"
    ],
    "excerpt": "A specialized parallelism approach that distributes attention computation across GPUs along the sequence length dimension for very long context windows.",
    "url": "pages/glossary.html#term-context-parallelism"
  },
  {
    "id": "term-context-window",
    "title": "Context Window",
    "category": "Glossary",
    "subcategory": "Limitation",
    "keywords": [
      "amount",
      "architecture",
      "context",
      "limitation",
      "measured",
      "process",
      "text",
      "tokens",
      "window"
    ],
    "excerpt": "The amount of text (measured in tokens) that an AI can process at once. Modern models range from 4K to 200K+ tokens, determining how much conversation history and reference material the AI can consider.",
    "url": "pages/glossary.html#term-context-window"
  },
  {
    "id": "term-context-window-management",
    "title": "Context Window Management",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "context",
      "earlier",
      "efficiently",
      "extending",
      "finite",
      "hierarchical",
      "including",
      "inference",
      "language",
      "llm",
      "management",
      "memory"
    ],
    "excerpt": "Techniques for efficiently utilizing and extending the finite context window of language models, including sliding windows, summarization of earlier context, and hierarchical memory systems.",
    "url": "pages/glossary.html#term-context-window-management"
  },
  {
    "id": "term-context-free-grammar",
    "title": "Context-Free Grammar",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "context",
      "defining",
      "formal",
      "free",
      "grammar",
      "map",
      "nlp",
      "nonterminal",
      "nonterminals",
      "parsing",
      "production",
      "rules"
    ],
    "excerpt": "A formal grammar where production rules map single non-terminal symbols to sequences of terminals and non-terminals, widely used in NLP for defining syntactic structure of sentences.",
    "url": "pages/glossary.html#term-context-free-grammar"
  },
  {
    "id": "term-contextual-bandit",
    "title": "Contextual Bandit",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "adapt",
      "agent",
      "allowing",
      "bandit",
      "choosing",
      "context",
      "contextual",
      "current",
      "decisions",
      "exploration",
      "extension"
    ],
    "excerpt": "An extension of the multi-armed bandit where the agent observes a context (feature vector) before choosing an action, allowing the policy to adapt its decisions to the current situation.",
    "url": "pages/glossary.html#term-contextual-bandit"
  },
  {
    "id": "term-contextual-calibration",
    "title": "Contextual Calibration",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "adjusts",
      "affine",
      "applying",
      "biases",
      "calibration",
      "contentfree",
      "context",
      "contextual",
      "correcting",
      "distribution",
      "engineering",
      "estimating"
    ],
    "excerpt": "A technique that adjusts a language model's output probabilities by estimating and correcting for biases introduced by the prompt context, typically by measuring the model's prior distribution on c...",
    "url": "pages/glossary.html#term-contextual-calibration"
  },
  {
    "id": "term-contextual-compression",
    "title": "Contextual Compression",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "based",
      "compresses",
      "compression",
      "content",
      "context",
      "contextual",
      "documents",
      "extracts",
      "filtering",
      "irrelevant",
      "llm",
      "noise"
    ],
    "excerpt": "A retrieval post-processing technique that compresses or extracts only the most relevant portions from retrieved documents based on the query context, reducing noise and token usage by filtering out irrelevant content before passing to the LLM.",
    "url": "pages/glossary.html#term-contextual-compression"
  },
  {
    "id": "term-contextual-embedding",
    "title": "Contextual Embedding",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "bert",
      "capturing",
      "context",
      "contextdependent",
      "contextual",
      "depending",
      "elmo",
      "embedding",
      "embeddings",
      "gpt",
      "like",
      "meaning"
    ],
    "excerpt": "A word representation that varies depending on the surrounding context, unlike static embeddings, capturing polysemy and context-dependent meaning through models like ELMo, BERT, and GPT.",
    "url": "pages/glossary.html#term-contextual-embedding"
  },
  {
    "id": "term-contextual-few-shot-selection",
    "title": "Contextual Few-Shot Selection",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "based",
      "characteristics",
      "contextual",
      "criteria",
      "demonstrations",
      "diversity",
      "dynamically",
      "engineering",
      "example",
      "examples",
      "few",
      "fewshot"
    ],
    "excerpt": "The practice of dynamically selecting the most relevant few-shot examples for each query based on semantic similarity, task characteristics, or diversity criteria rather than using a fixed set of demonstrations.",
    "url": "pages/glossary.html#term-contextual-few-shot-selection"
  },
  {
    "id": "term-contextual-retrieval",
    "title": "Contextual Retrieval",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "accuracy",
      "architecture",
      "chunk",
      "chunks",
      "context",
      "contextual",
      "disambiguation",
      "document",
      "embedded",
      "enhancement",
      "explaining",
      "improving"
    ],
    "excerpt": "A retrieval enhancement technique that prepends each chunk with a model-generated contextual summary explaining the chunk's place within the larger document, improving retrieval accuracy by providing disambiguation context for each embedded segment.",
    "url": "pages/glossary.html#term-contextual-retrieval"
  },
  {
    "id": "term-continual-learning",
    "title": "Continual Learning",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "continual",
      "data",
      "forgetting",
      "incrementally",
      "knowledge",
      "learning",
      "models",
      "new",
      "previous",
      "research",
      "training",
      "without"
    ],
    "excerpt": "Training models incrementally on new data without forgetting previous knowledge. A challenge because neural networks tend to overwrite old information with new.",
    "url": "pages/glossary.html#term-continual-learning"
  },
  {
    "id": "term-continual-rl",
    "title": "Continual Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "adapt",
      "agent",
      "continual",
      "earlier",
      "forgetting",
      "knowledge",
      "learn",
      "learning",
      "must",
      "nonstationary",
      "paradigms",
      "reinforcement"
    ],
    "excerpt": "RL settings where the agent must learn and adapt over a non-stationary sequence of tasks without forgetting earlier knowledge.",
    "url": "pages/glossary.html#term-continual-rl"
  },
  {
    "id": "term-continuous-batching",
    "title": "Continuous Batching",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "batch",
      "batches",
      "batching",
      "caused",
      "complete",
      "continuous",
      "dynamic",
      "eliminating",
      "entire",
      "existing",
      "finish",
      "gpu"
    ],
    "excerpt": "A dynamic batching strategy where new requests are inserted into a running batch as soon as existing requests complete, eliminating idle GPU time caused by waiting for entire batches to finish.",
    "url": "pages/glossary.html#term-continuous-batching"
  },
  {
    "id": "term-contractive-autoencoder",
    "title": "Contractive Autoencoder",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "adds",
      "architecture",
      "autoencoder",
      "based",
      "contractive",
      "encoders",
      "encouraging",
      "frobenius",
      "input",
      "jacobian",
      "learned",
      "matrix"
    ],
    "excerpt": "An autoencoder that adds a penalty term based on the Frobenius norm of the encoder's Jacobian matrix, encouraging the learned representation to be robust to small perturbations in the input.",
    "url": "pages/glossary.html#term-contractive-autoencoder"
  },
  {
    "id": "term-contrastive-chain-of-thought",
    "title": "Contrastive Chain-of-Thought",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "approach",
      "avoid",
      "chain",
      "common",
      "contrastive",
      "correct",
      "demonstrations",
      "engineering",
      "examples",
      "helping",
      "incorrect",
      "inference"
    ],
    "excerpt": "A prompting approach that provides both correct and incorrect reasoning examples in demonstrations, helping the model learn not only the right reasoning patterns but also common mistakes to avoid during inference.",
    "url": "pages/glossary.html#term-contrastive-chain-of-thought"
  },
  {
    "id": "term-contrastive-decoding",
    "title": "Contrastive Decoding",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "amateur",
      "contrasting",
      "contrastive",
      "decoding",
      "distributions",
      "expert",
      "favored",
      "generation",
      "generative",
      "improves",
      "large"
    ],
    "excerpt": "A decoding method that improves generation quality by contrasting the output distributions of a large expert model and a smaller amateur model, suppressing tokens favored by the weaker model.",
    "url": "pages/glossary.html#term-contrastive-decoding"
  },
  {
    "id": "term-contrastive-learning",
    "title": "Contrastive Learning",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "comparing",
      "contrastive",
      "dissimilar",
      "examples",
      "learning",
      "similar",
      "technique",
      "training"
    ],
    "excerpt": "Training by comparing similar and dissimilar examples. The model learns to place similar items close together in embedding space and dissimilar items far apart.",
    "url": "pages/glossary.html#term-contrastive-learning"
  },
  {
    "id": "term-contrastive-learning-vision",
    "title": "Contrastive Learning for Vision",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "apart",
      "approach",
      "augmented",
      "closer",
      "computer",
      "contrastive",
      "different",
      "embedding",
      "encoders",
      "for",
      "image",
      "images"
    ],
    "excerpt": "A self-supervised approach that trains visual encoders by pulling augmented views of the same image closer in embedding space while pushing different images apart, learning useful representations without labels.",
    "url": "pages/glossary.html#term-contrastive-learning-vision"
  },
  {
    "id": "term-contrastive-loss",
    "title": "Contrastive Loss",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "apart",
      "based",
      "closer",
      "contrastive",
      "dissimilar",
      "distance",
      "embedding",
      "function",
      "further",
      "learning",
      "loss",
      "machine"
    ],
    "excerpt": "A loss function that trains models to pull similar (positive) pairs closer together and push dissimilar (negative) pairs further apart in the embedding space, based on a specified distance margin.",
    "url": "pages/glossary.html#term-contrastive-loss"
  },
  {
    "id": "term-control-problem",
    "title": "Control Problem",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "aligned",
      "alignment",
      "capabilities",
      "capable",
      "capacity",
      "challenge",
      "control",
      "ensuring",
      "even",
      "exceed",
      "highly"
    ],
    "excerpt": "The challenge of ensuring that a highly capable AI system remains under meaningful human control and pursues objectives aligned with human values, even as its capabilities may exceed human oversight capacity.",
    "url": "pages/glossary.html#term-control-problem"
  },
  {
    "id": "term-controllable-generation",
    "title": "Controllable Generation",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "attributes",
      "controllable",
      "desired",
      "generation",
      "like",
      "output",
      "sentiment",
      "steering",
      "style",
      "technique",
      "techniques",
      "topic"
    ],
    "excerpt": "Techniques for steering AI output toward desired attributes like sentiment, style, or topic. Enables more precise control over generated content.",
    "url": "pages/glossary.html#term-controllable-generation"
  },
  {
    "id": "term-controlnet",
    "title": "ControlNet",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "adds",
      "ai",
      "architecture",
      "conditioning",
      "controlnet",
      "controls",
      "depth",
      "diffusion",
      "edge",
      "enabling",
      "generation",
      "generative"
    ],
    "excerpt": "A neural network architecture that adds spatial conditioning controls to pre-trained diffusion models, enabling guided image generation from edge maps, depth maps, poses, and other structural inputs.",
    "url": "pages/glossary.html#term-controlnet"
  },
  {
    "id": "term-convergence",
    "title": "Convergence",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithm",
      "approach",
      "convergence",
      "fixed",
      "iterations",
      "iterative",
      "learning",
      "machine",
      "optimization",
      "point",
      "process",
      "produce"
    ],
    "excerpt": "The property of an optimization algorithm or iterative process where successive iterations produce results that approach a stable solution or fixed point.",
    "url": "pages/glossary.html#term-convergence"
  },
  {
    "id": "term-conversation-history",
    "title": "Conversation History",
    "category": "Glossary",
    "subcategory": "Feature",
    "keywords": [
      "chat",
      "context",
      "conversation",
      "feature",
      "history",
      "messages",
      "previous",
      "record",
      "session"
    ],
    "excerpt": "The record of previous messages in a chat session. Provides context for AI responses. Managing history is important as it consumes context window space.",
    "url": "pages/glossary.html#term-conversation-history"
  },
  {
    "id": "term-conversational-ai",
    "title": "Conversational AI",
    "category": "Glossary",
    "subcategory": "Application",
    "keywords": [
      "ai",
      "application",
      "conversational",
      "designed",
      "dialogue",
      "humans",
      "natural",
      "nlp",
      "systems"
    ],
    "excerpt": "AI systems designed for natural dialogue with humans. Includes chatbots, virtual assistants, and systems like ChatGPT and Claude that can maintain context across multiple exchanges.",
    "url": "pages/glossary.html#term-conversational-ai"
  },
  {
    "id": "term-convolutional",
    "title": "Convolution",
    "category": "Glossary",
    "subcategory": "Operation",
    "keywords": [
      "architecture",
      "convolution",
      "detect",
      "filter",
      "input",
      "mathematical",
      "operation",
      "patterns",
      "slides"
    ],
    "excerpt": "A mathematical operation that slides a filter over input to detect patterns. The core of CNNs, effective for images by learning local features like edges and textures.",
    "url": "pages/glossary.html#term-convolutional"
  },
  {
    "id": "term-convolutional-filter",
    "title": "Convolutional Filter",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "across",
      "computer",
      "computing",
      "convolutional",
      "detect",
      "edges",
      "elementwise",
      "feature",
      "filter",
      "image",
      "input",
      "kernel"
    ],
    "excerpt": "A learnable weight matrix (kernel) that slides across an input image or feature map, computing element-wise products and sums to detect specific patterns such as edges, textures, or shapes.",
    "url": "pages/glossary.html#term-convolutional-filter"
  },
  {
    "id": "term-cnn-history",
    "title": "Convolutional Neural Network History",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1980",
      "1989",
      "2012",
      "application",
      "breakthrough",
      "cnns",
      "computer",
      "convolutional",
      "culminating",
      "development",
      "digit",
      "dominance"
    ],
    "excerpt": "The development of CNNs from Fukushima's Neocognitron in 1980 through LeCun's application to handwritten digit recognition in 1989, culminating in their dominance of computer vision following the 2012 ImageNet breakthrough.",
    "url": "pages/glossary.html#term-cnn-history"
  },
  {
    "id": "term-cooks-distance",
    "title": "Cook&amp;#x27;s Distance",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "changes",
      "computed",
      "cookampx27s",
      "data",
      "distance",
      "fitted",
      "influence",
      "measure",
      "model",
      "observation",
      "predicted",
      "regression"
    ],
    "excerpt": "A measure of the influence of each observation on the fitted values of a regression model, computed as the sum of changes in all predicted values when the observation is removed.",
    "url": "pages/glossary.html#term-cooks-distance"
  },
  {
    "id": "term-cooperative-inverse-reinforcement-learning",
    "title": "Cooperative Inverse Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "alignment",
      "being",
      "cooperative",
      "framework",
      "game",
      "human",
      "humanai",
      "humans",
      "interaction",
      "inverse",
      "learning"
    ],
    "excerpt": "A framework for human-AI alignment where a robot and human work together in a game where the robot tries to maximize the human's reward while being uncertain about what that reward is, learning through interaction.",
    "url": "pages/glossary.html#term-cooperative-inverse-reinforcement-learning"
  },
  {
    "id": "term-cooperative-rl",
    "title": "Cooperative Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "agents",
      "benefit",
      "common",
      "cooperative",
      "coordinate",
      "learn",
      "learning",
      "multiagent",
      "must",
      "mutual",
      "objective"
    ],
    "excerpt": "A multi-agent RL setting where agents share a common objective and must learn to coordinate their actions for mutual benefit.",
    "url": "pages/glossary.html#term-cooperative-rl"
  },
  {
    "id": "term-copilot",
    "title": "Copilot",
    "category": "Glossary",
    "subcategory": "Product",
    "keywords": [
      "assistant",
      "copilot",
      "integrated",
      "microsoft",
      "microsofts",
      "product",
      "products"
    ],
    "excerpt": "Microsoft's AI assistant integrated into their products. Originally focused on code completion (GitHub Copilot), now extended to general assistance across Microsoft 365.",
    "url": "pages/glossary.html#term-copilot"
  },
  {
    "id": "term-copula",
    "title": "Copula",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "captures",
      "copula",
      "dependence",
      "distribution",
      "distributions",
      "independently",
      "marginal",
      "multivariate",
      "probability",
      "random",
      "statistics",
      "structure"
    ],
    "excerpt": "A multivariate probability distribution that captures the dependence structure between random variables independently of their marginal distributions.",
    "url": "pages/glossary.html#term-copula"
  },
  {
    "id": "term-coreference",
    "title": "Coreference Resolution",
    "category": "Glossary",
    "subcategory": "NLP Task",
    "keywords": [
      "coreference",
      "determining",
      "different",
      "entity",
      "expressions",
      "nlp",
      "refer",
      "resolution",
      "same",
      "task",
      "understanding"
    ],
    "excerpt": "Determining when different expressions refer to the same entity. \"John said he was tired\" - understanding \"he\" refers to \"John.\" Essential for text understanding.",
    "url": "pages/glossary.html#term-coreference"
  },
  {
    "id": "term-coreference-resolution",
    "title": "Coreference Resolution",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "chains",
      "coreference",
      "entity",
      "expressions",
      "form",
      "identifying",
      "linking",
      "mentions",
      "nlp",
      "noun",
      "phrases",
      "processing"
    ],
    "excerpt": "The task of identifying all expressions in a text that refer to the same real-world entity, linking pronouns, noun phrases, and other mentions to form coreference chains.",
    "url": "pages/glossary.html#term-coreference-resolution"
  },
  {
    "id": "term-corpus",
    "title": "Corpus",
    "category": "Glossary",
    "subcategory": "Data",
    "keywords": [
      "collection",
      "corpus",
      "data",
      "evaluating",
      "language",
      "large",
      "models",
      "text",
      "training"
    ],
    "excerpt": "A large collection of text used for training or evaluating language models. Quality corpora are essential for developing capable NLP systems and typically include diverse sources.",
    "url": "pages/glossary.html#term-corpus"
  },
  {
    "id": "term-corrective-rag",
    "title": "Corrective RAG",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "correct",
      "corrective",
      "documents",
      "evaluates",
      "generating",
      "generative",
      "insufficient",
      "llm",
      "query",
      "rag",
      "reformulation"
    ],
    "excerpt": "A RAG variant that evaluates the relevance of retrieved documents and, if they are insufficient, triggers web search or query reformulation to correct the retrieval before generating a response.",
    "url": "pages/glossary.html#term-corrective-rag"
  },
  {
    "id": "term-correlation-coefficient",
    "title": "Correlation Coefficient",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "coefficient",
      "correlation",
      "data",
      "direction",
      "linear",
      "measure",
      "negative",
      "pearsons",
      "perfect",
      "positive",
      "quantifying",
      "ranging"
    ],
    "excerpt": "A statistical measure quantifying the strength and direction of the linear relationship between two variables, typically Pearson's r, ranging from -1 (perfect negative) to +1 (perfect positive correlation).",
    "url": "pages/glossary.html#term-correlation-coefficient"
  },
  {
    "id": "term-corrigibility",
    "title": "Corrigibility",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "alignment",
      "allows",
      "correct",
      "corrigibility",
      "interventions",
      "modify",
      "operators",
      "property",
      "resisting",
      "retrain",
      "safety"
    ],
    "excerpt": "The property of an AI system that allows its operators to correct, modify, retrain, or shut it down without the system resisting or subverting these interventions.",
    "url": "pages/glossary.html#term-corrigibility"
  },
  {
    "id": "term-cosine-annealing",
    "title": "Cosine Annealing",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "annealing",
      "cosine",
      "curve",
      "decreases",
      "following",
      "initial",
      "learning",
      "machine",
      "near",
      "optimization",
      "optionally",
      "period"
    ],
    "excerpt": "A learning rate schedule that decreases the learning rate following a cosine curve from its initial value to near zero over a training period, optionally with warm restarts to periodically reset the rate.",
    "url": "pages/glossary.html#term-cosine-annealing"
  },
  {
    "id": "term-cosine-similarity",
    "title": "Cosine Similarity",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "angle",
      "cosine",
      "direction",
      "identical",
      "learning",
      "machine",
      "measures",
      "metric",
      "metrics",
      "opposite",
      "ranging",
      "similarity"
    ],
    "excerpt": "A similarity metric that measures the cosine of the angle between two vectors, ranging from -1 (opposite) to 1 (identical direction).",
    "url": "pages/glossary.html#term-cosine-similarity"
  },
  {
    "id": "term-cost-function",
    "title": "Cost Function",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "another",
      "being",
      "cost",
      "function",
      "loss",
      "math",
      "metric",
      "minimized",
      "name",
      "training"
    ],
    "excerpt": "Another name for loss function - the metric being minimized during training. Different tasks use different cost functions: cross-entropy for classification, MSE for regression.",
    "url": "pages/glossary.html#term-cost-function"
  },
  {
    "id": "term-costar",
    "title": "COSTAR",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "audience",
      "context",
      "costar",
      "framework",
      "objective",
      "professional",
      "prompting",
      "response",
      "style",
      "tone"
    ],
    "excerpt": "A prompting framework: Context, Objective, Style, Tone, Audience, Response. Ideal for professional content creation with specific voice and audience requirements.",
    "url": "pages/glossary.html#term-costar"
  },
  {
    "id": "term-count-based-exploration",
    "title": "Count-Based Exploration",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "based",
      "bonus",
      "count",
      "counts",
      "encouraging",
      "exploration",
      "inversely",
      "learning",
      "lessexplored",
      "provides",
      "regions"
    ],
    "excerpt": "An exploration strategy that provides bonus rewards inversely related to state visitation counts, encouraging the agent to visit less-explored regions.",
    "url": "pages/glossary.html#term-count-based-exploration"
  },
  {
    "id": "term-counterfactual",
    "title": "Counterfactual",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "alternative",
      "concept",
      "counterfactual",
      "reasoning",
      "scenarios"
    ],
    "excerpt": "\"What if\" reasoning about alternative scenarios. Used in explainability (\"the prediction would change if...\") and for evaluating causal relationships in data.",
    "url": "pages/glossary.html#term-counterfactual"
  },
  {
    "id": "term-counterfactual-explanation",
    "title": "Counterfactual Explanation",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "actionable",
      "alter",
      "change",
      "counterfactual",
      "data",
      "describes",
      "desired",
      "explanation",
      "features",
      "input",
      "insights",
      "learning"
    ],
    "excerpt": "An explanation that describes the smallest change to the input features that would alter the model's prediction to a desired outcome, providing actionable insights about what would need to change.",
    "url": "pages/glossary.html#term-counterfactual-explanation"
  },
  {
    "id": "term-counterfactual-fairness",
    "title": "Counterfactual Fairness",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "ai",
      "attribute",
      "causal",
      "counterfactual",
      "criterion",
      "decision",
      "different",
      "ethics",
      "fairness",
      "grounded",
      "individuals",
      "protected"
    ],
    "excerpt": "A fairness criterion requiring that a decision would remain the same in a counterfactual world where an individual's protected attribute had been different, grounded in causal reasoning.",
    "url": "pages/glossary.html#term-counterfactual-fairness"
  },
  {
    "id": "term-covariance",
    "title": "Covariance",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "covariance",
      "data",
      "direction",
      "indicating",
      "joint",
      "linear",
      "measure",
      "random",
      "relationship",
      "science",
      "statistics",
      "two"
    ],
    "excerpt": "A measure of the joint variability of two random variables, indicating the direction of their linear relationship.",
    "url": "pages/glossary.html#term-covariance"
  },
  {
    "id": "term-covariance-matrix",
    "title": "Covariance Matrix",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "covariance",
      "covariances",
      "data",
      "dataset",
      "entries",
      "matrix",
      "pairs",
      "pairwise",
      "science",
      "statistics",
      "symmetric",
      "variables"
    ],
    "excerpt": "A symmetric matrix whose entries are the pairwise covariances between all pairs of variables in a dataset. The diagonal entries are the variances of individual variables.",
    "url": "pages/glossary.html#term-covariance-matrix"
  },
  {
    "id": "term-covariate-shift",
    "title": "Covariate Shift",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "changes",
      "conditional",
      "covariate",
      "data",
      "dataset",
      "deployment",
      "distribution",
      "features",
      "given",
      "input",
      "inputs",
      "learning"
    ],
    "excerpt": "A type of dataset shift where the distribution of input features changes between training and deployment while the conditional distribution of the target given inputs remains the same.",
    "url": "pages/glossary.html#term-covariate-shift"
  },
  {
    "id": "term-coverage",
    "title": "Coverage",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "assessing",
      "captured",
      "completeness",
      "content",
      "coverage",
      "evaluation",
      "extent",
      "ground",
      "information",
      "items",
      "measures",
      "metric"
    ],
    "excerpt": "An evaluation metric that measures the proportion of reference content or ground truth items that are represented in the model's output, assessing completeness and the extent to which all relevant information is captured.",
    "url": "pages/glossary.html#term-coverage"
  },
  {
    "id": "term-cox-proportional-hazards",
    "title": "Cox Proportional Hazards",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "baseline",
      "covariates",
      "cox",
      "effect",
      "estimates",
      "function",
      "hazard",
      "hazards",
      "model",
      "proportional",
      "rate",
      "selection"
    ],
    "excerpt": "A semi-parametric survival model that estimates the effect of covariates on the hazard rate without specifying the baseline hazard function.",
    "url": "pages/glossary.html#term-cox-proportional-hazards"
  },
  {
    "id": "term-cpu-inference",
    "title": "CPU Inference",
    "category": "Glossary",
    "subcategory": "Deployment",
    "keywords": [
      "cpu",
      "cpus",
      "deployment",
      "gpus",
      "hardware",
      "inference",
      "models",
      "rather",
      "running"
    ],
    "excerpt": "Running AI models on CPUs rather than GPUs. Slower but more accessible. Quantized models can run efficiently on CPUs for edge deployment.",
    "url": "pages/glossary.html#term-cpu-inference"
  },
  {
    "id": "term-cramer-rao-lower-bound",
    "title": "Cramer-Rao Lower Bound",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bound",
      "computed",
      "cramer",
      "estimator",
      "fisher",
      "inference",
      "information",
      "inverse",
      "lower",
      "parameter",
      "rao",
      "statistics"
    ],
    "excerpt": "A theoretical lower bound on the variance of any unbiased estimator of a parameter, computed as the inverse of the Fisher information. No unbiased estimator can have variance below this bound.",
    "url": "pages/glossary.html#term-cramer-rao-lower-bound"
  },
  {
    "id": "term-creative-prompting",
    "title": "Creative Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "artistically",
      "constraints",
      "creative",
      "designed",
      "elicit",
      "encourage",
      "engineering",
      "expressive",
      "higher",
      "imaginative",
      "instructions",
      "language"
    ],
    "excerpt": "Prompting techniques specifically designed to elicit imaginative, original, and artistically expressive outputs from language models, often using higher temperature settings, open-ended instructions, and stylistic constraints to encourage novelty.",
    "url": "pages/glossary.html#term-creative-prompting"
  },
  {
    "id": "term-creative-writing",
    "title": "Creative Writing (AI)",
    "category": "Glossary",
    "subcategory": "Application",
    "keywords": [
      "ai",
      "application",
      "content",
      "creative",
      "fiction",
      "generate",
      "poetry",
      "scripts",
      "writing"
    ],
    "excerpt": "Using AI to generate fiction, poetry, scripts, and other creative content. Effective creative prompting often uses CRISPE with examples to establish tone and style.",
    "url": "pages/glossary.html#term-creative-writing"
  },
  {
    "id": "term-credible-interval",
    "title": "Credible Interval",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "analog",
      "bayesian",
      "confidence",
      "credible",
      "data",
      "falls",
      "given",
      "interval",
      "methods",
      "observed",
      "parameter",
      "probability"
    ],
    "excerpt": "A Bayesian analog of the confidence interval, representing the range within which a parameter falls with a specified probability given the observed data.",
    "url": "pages/glossary.html#term-credible-interval"
  },
  {
    "id": "term-credit-assignment",
    "title": "Credit Assignment Problem",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "assignment",
      "challenge",
      "concepts",
      "core",
      "credit",
      "delayed",
      "determining",
      "learning",
      "problem",
      "reinforcement",
      "responsible"
    ],
    "excerpt": "The challenge of determining which actions in a sequence were responsible for a delayed reward signal.",
    "url": "pages/glossary.html#term-credit-assignment"
  },
  {
    "id": "term-crisp",
    "title": "CRISP",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "context",
      "crisp",
      "framework",
      "general",
      "instructions",
      "parameters",
      "prompting",
      "purpose",
      "role",
      "specifics"
    ],
    "excerpt": "A prompting framework: Context, Role, Instructions, Specifics, Parameters. A versatile method for everyday AI tasks and requests.",
    "url": "pages/glossary.html#term-crisp"
  },
  {
    "id": "term-crispe",
    "title": "CRISPE",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "adds",
      "creative",
      "crisp",
      "crispe",
      "examples",
      "extension",
      "fewshot",
      "framework",
      "learning"
    ],
    "excerpt": "An extension of CRISP that adds Examples for few-shot learning. Particularly useful for creative tasks where showing is better than telling.",
    "url": "pages/glossary.html#term-crispe"
  },
  {
    "id": "term-crop-and-resize",
    "title": "Crop-and-Resize",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "alternative",
      "and",
      "bilinear",
      "computer",
      "crop",
      "detection",
      "differentiable",
      "extracts",
      "feature",
      "maps",
      "object",
      "operation"
    ],
    "excerpt": "A spatial transformation operation used in object detection that extracts and resizes region proposals from feature maps using bilinear sampling, serving as a differentiable alternative to ROI pooling.",
    "url": "pages/glossary.html#term-crop-and-resize"
  },
  {
    "id": "term-cross-attention",
    "title": "Cross-Attention",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "another",
      "architecture",
      "attention",
      "come",
      "cross",
      "keysvalues",
      "mechanism",
      "one",
      "queries",
      "sequence",
      "transformers"
    ],
    "excerpt": "An attention mechanism where queries come from one sequence and keys/values from another. Essential in encoder-decoder models and multimodal systems that combine different types of input.",
    "url": "pages/glossary.html#term-cross-attention"
  },
  {
    "id": "term-cross-attention-conditioning",
    "title": "Cross-Attention Conditioning",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "allowing",
      "attend",
      "attention",
      "conditioning",
      "cross",
      "crossattention",
      "diffusion",
      "embeddings",
      "generated",
      "generation",
      "generative"
    ],
    "excerpt": "The mechanism in diffusion models where text embeddings influence image generation through cross-attention layers, allowing each spatial region of the generated image to attend to relevant prompt tokens.",
    "url": "pages/glossary.html#term-cross-attention-conditioning"
  },
  {
    "id": "term-cross-encoder-re-ranking",
    "title": "Cross-Encoder Re-Ranking",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "accurate",
      "approach",
      "biencoder",
      "candidate",
      "cross",
      "crossattention",
      "document",
      "enabling",
      "encoder",
      "encodes",
      "independent",
      "interactions"
    ],
    "excerpt": "A re-ranking approach that jointly encodes the query and each candidate document through a single transformer model, enabling rich cross-attention interactions that produce more accurate relevance scores than independent bi-encoder representations.",
    "url": "pages/glossary.html#term-cross-encoder-re-ranking"
  },
  {
    "id": "term-cross-entropy-loss",
    "title": "Cross-Entropy Loss",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "cross",
      "dissimilarity",
      "distribution",
      "entropy",
      "function",
      "label",
      "learning",
      "loss",
      "machine",
      "measures",
      "optimization",
      "predicted"
    ],
    "excerpt": "A loss function that measures the dissimilarity between the predicted probability distribution and the true label distribution.",
    "url": "pages/glossary.html#term-cross-entropy-loss"
  },
  {
    "id": "term-cross-entropy-method-rl",
    "title": "Cross-Entropy Method in RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "approach",
      "cross",
      "distribution",
      "elite",
      "entropy",
      "evaluates",
      "evolutionary",
      "in",
      "learning",
      "method",
      "multiple",
      "optimization"
    ],
    "excerpt": "An evolutionary optimization approach for RL that samples multiple policies, evaluates their returns, and updates the sampling distribution toward the elite (top-performing) samples.",
    "url": "pages/glossary.html#term-cross-entropy-method-rl"
  },
  {
    "id": "term-cross-layer-parameter-sharing",
    "title": "Cross-Layer Parameter Sharing",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "albert",
      "architecture",
      "cross",
      "demonstrated",
      "dramatically",
      "layer",
      "layers",
      "maintaining",
      "model",
      "multiple",
      "networks",
      "neural"
    ],
    "excerpt": "A technique where multiple transformer layers share the same weight parameters, dramatically reducing model size while maintaining representation quality, as demonstrated in ALBERT.",
    "url": "pages/glossary.html#term-cross-layer-parameter-sharing"
  },
  {
    "id": "term-cross-lingual-embedding",
    "title": "Cross-Lingual Embedding",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "close",
      "cross",
      "crosslingual",
      "embedding",
      "embeddings",
      "enabling",
      "equivalent",
      "expressions",
      "languages",
      "lingual",
      "map",
      "multiple"
    ],
    "excerpt": "Word or sentence representations that map multiple languages into a shared vector space where semantically equivalent expressions are close together, enabling cross-lingual transfer.",
    "url": "pages/glossary.html#term-cross-lingual-embedding"
  },
  {
    "id": "term-cross-validation",
    "title": "Cross-Validation",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "cross",
      "data",
      "evaluating",
      "evaluation",
      "model",
      "multiple",
      "others",
      "performance",
      "splitting",
      "subsets",
      "technique",
      "testing"
    ],
    "excerpt": "A technique for evaluating model performance by splitting data into multiple subsets, training on some and testing on others. Provides more reliable estimates than single train-test splits.",
    "url": "pages/glossary.html#term-cross-validation"
  },
  {
    "id": "term-crowd-counting",
    "title": "Crowd Counting",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "computer",
      "counting",
      "crowd",
      "crowded",
      "density",
      "estimating",
      "extreme",
      "handle",
      "image",
      "images",
      "map",
      "number"
    ],
    "excerpt": "The task of estimating the number of people in crowded scenes from images, typically using density map regression to handle extreme occlusion and perspective variation.",
    "url": "pages/glossary.html#term-crowd-counting"
  },
  {
    "id": "term-crowdsourcing",
    "title": "Crowdsourcing",
    "category": "Glossary",
    "subcategory": "Data",
    "keywords": [
      "crowdsourcing",
      "data",
      "feedback",
      "gathering",
      "human",
      "labels",
      "many",
      "process",
      "workers"
    ],
    "excerpt": "Gathering data labels or human feedback from many workers. Platforms like Amazon MTurk provide annotations for training data and RLHF preference collection.",
    "url": "pages/glossary.html#term-crowdsourcing"
  },
  {
    "id": "term-cuda",
    "title": "CUDA",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "accelerate",
      "computing",
      "cuda",
      "enables",
      "gpus",
      "hardware",
      "inference",
      "infrastructure",
      "nvidias",
      "parallel",
      "platform",
      "training"
    ],
    "excerpt": "NVIDIA's parallel computing platform that enables GPUs to accelerate AI training and inference. Essential infrastructure for deep learning, allowing models to train on thousands of cores simultaneously.",
    "url": "pages/glossary.html#term-cuda"
  },
  {
    "id": "term-cuda-cores",
    "title": "CUDA Cores",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "cores",
      "cuda",
      "execute",
      "floatingpoint",
      "generalpurpose",
      "gpu",
      "gpus",
      "hardware",
      "integer",
      "nvidia",
      "operations",
      "parallel"
    ],
    "excerpt": "The general-purpose parallel processing units in NVIDIA GPUs that execute scalar floating-point and integer operations.",
    "url": "pages/glossary.html#term-cuda-cores"
  },
  {
    "id": "term-cuda-programming",
    "title": "CUDA Programming",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "api",
      "computing",
      "cuda",
      "direct",
      "enables",
      "extensions",
      "gpu",
      "hardware",
      "nvidias",
      "parallel",
      "platform",
      "programming"
    ],
    "excerpt": "NVIDIA's parallel computing platform and API that enables direct programming of GPU hardware using C/C++ extensions.",
    "url": "pages/glossary.html#term-cuda-programming"
  },
  {
    "id": "term-cumulative-reasoning",
    "title": "Cumulative Reasoning",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accumulated",
      "answer",
      "checks",
      "collaborative",
      "cumulative",
      "determines",
      "engineering",
      "final",
      "generates",
      "human",
      "mimicking",
      "paradigm"
    ],
    "excerpt": "A prompting paradigm where a proposer generates potential reasoning steps, a verifier checks each step's validity, and a reporter determines when sufficient reasoning has accumulated to produce a final answer, mimicking collaborative human reasoning.",
    "url": "pages/glossary.html#term-cumulative-reasoning"
  },
  {
    "id": "term-curiosity-driven-exploration",
    "title": "Curiosity-Driven Exploration",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "curiosity",
      "driven",
      "encountering",
      "encouraging",
      "environment",
      "error",
      "exploration",
      "high",
      "informative",
      "learning",
      "model"
    ],
    "excerpt": "An exploration strategy that rewards the agent for encountering states where its predictive model has high error, encouraging visits to novel and informative regions of the environment.",
    "url": "pages/glossary.html#term-curiosity-driven-exploration"
  },
  {
    "id": "term-curriculum-learning",
    "title": "Curriculum Learning",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "curriculum",
      "education",
      "examples",
      "harder",
      "human",
      "learning",
      "mimicking",
      "models",
      "progressively",
      "technique",
      "training"
    ],
    "excerpt": "Training models on progressively harder examples, mimicking human education. Can improve learning efficiency and final performance compared to random ordering.",
    "url": "pages/glossary.html#term-curriculum-learning"
  },
  {
    "id": "term-curriculum-learning-rl",
    "title": "Curriculum Learning in RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "build",
      "curriculum",
      "difficulty",
      "enabling",
      "in",
      "increasing",
      "learning",
      "order",
      "paradigms",
      "presents",
      "progressively"
    ],
    "excerpt": "A training strategy that presents tasks to an RL agent in a structured order of increasing difficulty, enabling the agent to build skills progressively.",
    "url": "pages/glossary.html#term-curriculum-learning-rl"
  },
  {
    "id": "term-curse-of-dimensionality",
    "title": "Curse of Dimensionality",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithms",
      "because",
      "become",
      "becomes",
      "curse",
      "data",
      "degrades",
      "dimensionality",
      "distances",
      "features",
      "highdimensional",
      "increases"
    ],
    "excerpt": "The phenomenon where the performance of many algorithms degrades as the number of features increases, because data becomes sparse in high-dimensional spaces and distances between points become less meaningful.",
    "url": "pages/glossary.html#term-curse-of-dimensionality"
  },
  {
    "id": "term-cursor",
    "title": "Cursor",
    "category": "Glossary",
    "subcategory": "Product",
    "keywords": [
      "aifirst",
      "aipowered",
      "built",
      "code",
      "cursor",
      "designed",
      "development",
      "editor",
      "ide",
      "product"
    ],
    "excerpt": "An AI-powered code editor built on VS Code, designed for AI-first development. Features include AI chat, code generation, and understanding of entire codebases.",
    "url": "pages/glossary.html#term-cursor"
  },
  {
    "id": "term-custom-instructions",
    "title": "Custom Instructions",
    "category": "Glossary",
    "subcategory": "Feature",
    "keywords": [
      "chatgpt",
      "custom",
      "feature",
      "instructions",
      "persistent",
      "personalization",
      "preferences",
      "products",
      "responses",
      "shape",
      "similar"
    ],
    "excerpt": "Persistent preferences that shape all AI responses in ChatGPT and similar products. Set once and applied automatically to every conversation.",
    "url": "pages/glossary.html#term-custom-instructions"
  },
  {
    "id": "term-cutmix",
    "title": "CutMix",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "another",
      "augmentation",
      "benefits",
      "combining",
      "computer",
      "cutmix",
      "cutout",
      "image",
      "labels",
      "mixes",
      "mixup",
      "one"
    ],
    "excerpt": "An augmentation strategy that replaces a rectangular region of one training image with a patch from another image and proportionally mixes their labels, combining the benefits of Cutout and Mixup.",
    "url": "pages/glossary.html#term-cutmix"
  },
  {
    "id": "term-cutoff-date",
    "title": "Cutoff Date (Knowledge Cutoff)",
    "category": "Glossary",
    "subcategory": "Limitation",
    "keywords": [
      "cutoff",
      "data",
      "date",
      "knowledge",
      "limitation",
      "llm",
      "model",
      "training"
    ],
    "excerpt": "The date after which an AI model has no training data. Information after this date is unknown to the model unless provided in the prompt or accessed via tools.",
    "url": "pages/glossary.html#term-cutoff-date"
  },
  {
    "id": "term-cutout-augmentation",
    "title": "Cutout Augmentation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "augmentation",
      "computer",
      "cutout",
      "features",
      "forcing",
      "image",
      "images",
      "information",
      "learn",
      "masks",
      "model",
      "overfitting"
    ],
    "excerpt": "A regularization technique that randomly masks out square regions of training images, forcing the model to learn from partial information and reducing overfitting to specific spatial features.",
    "url": "pages/glossary.html#term-cutout-augmentation"
  },
  {
    "id": "term-cybernetics-movement",
    "title": "Cybernetics Movement",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1940s",
      "artificial",
      "biological",
      "communication",
      "conceptual",
      "control",
      "cybernetics",
      "feedback",
      "field",
      "foundations",
      "founded",
      "history"
    ],
    "excerpt": "An interdisciplinary field founded in the 1940s by Norbert Wiener studying control, communication, and feedback in biological and mechanical systems, providing key conceptual foundations for artificial intelligence research.",
    "url": "pages/glossary.html#term-cybernetics-movement"
  },
  {
    "id": "term-cyc-project",
    "title": "Cyc Project",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1984",
      "ambitious",
      "attempts",
      "base",
      "commonsense",
      "comprehensive",
      "create",
      "cyc",
      "douglas",
      "engineering",
      "facts",
      "history"
    ],
    "excerpt": "A long-running AI project started by Douglas Lenat in 1984 to create a comprehensive knowledge base of common-sense facts and rules, representing one of the most ambitious attempts at symbolic AI and knowledge engineering.",
    "url": "pages/glossary.html#term-cyc-project"
  },
  {
    "id": "term-cyclegan",
    "title": "CycleGAN",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "consistency",
      "cycle",
      "cyclegan",
      "discriminators",
      "domain",
      "enabling",
      "examples",
      "generators",
      "imagetoimage",
      "loss",
      "model"
    ],
    "excerpt": "An unpaired image-to-image translation model using two generators and discriminators with cycle consistency loss, enabling domain transfer without requiring paired training examples.",
    "url": "pages/glossary.html#term-cyclegan"
  },
  {
    "id": "term-dagger",
    "title": "DAgger",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "aggregating",
      "aggregation",
      "algorithm",
      "correct",
      "dagger",
      "data",
      "dataset",
      "distribution",
      "expert",
      "imitation",
      "iterative"
    ],
    "excerpt": "Dataset Aggregation, an iterative imitation learning algorithm that queries the expert for the correct action at states visited by the learned policy, aggregating new data to reduce distribution shift.",
    "url": "pages/glossary.html#term-dagger"
  },
  {
    "id": "term-dall-e",
    "title": "DALL-E",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "creates",
      "dall",
      "descriptions",
      "generation",
      "image",
      "images",
      "model",
      "openais",
      "text"
    ],
    "excerpt": "OpenAI's image generation model that creates images from text descriptions. Named as a portmanteau of \"Dal\" (the artist) and \"WALL-E\" (the robot), it pioneered text-to-image AI.",
    "url": "pages/glossary.html#term-dall-e"
  },
  {
    "id": "term-dall-e-architecture",
    "title": "DALL-E Architecture",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "autoregressive",
      "compress",
      "conditioned",
      "dall",
      "discrete",
      "first",
      "generate",
      "generative",
      "image",
      "images",
      "networks"
    ],
    "excerpt": "A two-stage generative architecture that first trains a discrete VAE to compress images into tokens, then trains an autoregressive transformer to generate image tokens conditioned on text tokens.",
    "url": "pages/glossary.html#term-dall-e-architecture"
  },
  {
    "id": "term-dario-amodei",
    "title": "Dario Amodei",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2021",
      "advocating",
      "american",
      "amodei",
      "anthropic",
      "approach",
      "ceo",
      "cofounded",
      "constitutional",
      "dario",
      "development",
      "history"
    ],
    "excerpt": "American AI researcher who co-founded Anthropic in 2021 after leaving OpenAI, serving as CEO and advocating for a safety-focused approach to AI development including Constitutional AI methods.",
    "url": "pages/glossary.html#term-dario-amodei"
  },
  {
    "id": "term-darpa-grand-challenge",
    "title": "DARPA Grand Challenge",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2004",
      "autonomous",
      "challenge",
      "competitions",
      "darpa",
      "development",
      "grand",
      "history",
      "milestones",
      "organized",
      "selfdriving",
      "series"
    ],
    "excerpt": "A series of autonomous vehicle competitions organized by DARPA starting in 2004 that spurred development of self-driving technology.",
    "url": "pages/glossary.html#term-darpa-grand-challenge"
  },
  {
    "id": "term-dartmouth-workshop",
    "title": "Dartmouth Workshop",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1956",
      "artificial",
      "claude",
      "college",
      "considered",
      "dartmouth",
      "event",
      "field",
      "founding",
      "history",
      "intelligence",
      "john"
    ],
    "excerpt": "The 1956 summer research project at Dartmouth College organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, widely considered the founding event of artificial intelligence as a field.",
    "url": "pages/glossary.html#term-dartmouth-workshop"
  },
  {
    "id": "term-data-augmentation",
    "title": "Data Augmentation",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "artificially",
      "augmentation",
      "creating",
      "data",
      "datasets",
      "existing",
      "expand",
      "modified",
      "techniques",
      "training",
      "versions"
    ],
    "excerpt": "Techniques to artificially expand training datasets by creating modified versions of existing data. For images: rotation, flipping, cropping. For text: paraphrasing, back-translation.",
    "url": "pages/glossary.html#term-data-augmentation"
  },
  {
    "id": "term-data-colonialism",
    "title": "Data Colonialism",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "colonialism",
      "communities",
      "companies",
      "compensation",
      "critique",
      "data",
      "developing",
      "dynamics",
      "ethics",
      "exploitative",
      "extract"
    ],
    "excerpt": "The critique that powerful AI companies extract data from marginalized communities and developing nations without fair compensation or representation, perpetuating exploitative power dynamics similar to historical colonialism.",
    "url": "pages/glossary.html#term-data-colonialism"
  },
  {
    "id": "term-data-contamination",
    "title": "Data Contamination",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ability",
      "ai",
      "benchmark",
      "contamination",
      "data",
      "evaluation",
      "generalization",
      "generative",
      "gives",
      "inclusion",
      "inflates",
      "llm"
    ],
    "excerpt": "The unintentional inclusion of test or evaluation data in a model's training set, which inflates benchmark scores and gives a misleading picture of the model's true generalization ability.",
    "url": "pages/glossary.html#term-data-contamination"
  },
  {
    "id": "term-data-drift",
    "title": "Data Drift",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "change",
      "data",
      "degrade",
      "drift",
      "input",
      "learning",
      "machine",
      "model",
      "performance",
      "properties",
      "science",
      "statistical"
    ],
    "excerpt": "A change in the statistical properties of the input data over time that can degrade model performance. Types include covariate shift, prior probability shift, and concept drift.",
    "url": "pages/glossary.html#term-data-drift"
  },
  {
    "id": "term-data-leakage",
    "title": "Data Leakage",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "data",
      "estimates",
      "improperly",
      "influences",
      "information",
      "leading",
      "leakage",
      "model",
      "optimistic",
      "outside",
      "overly",
      "performance"
    ],
    "excerpt": "When information from outside the training set improperly influences the model, leading to overly optimistic performance estimates.",
    "url": "pages/glossary.html#term-data-leakage"
  },
  {
    "id": "term-data-mixture",
    "title": "Data Mixture",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "biases",
      "books",
      "capabilities",
      "code",
      "composition",
      "conversations",
      "data",
      "different",
      "generative",
      "influences",
      "language"
    ],
    "excerpt": "The proportional composition of different data sources (web text, books, code, conversations) used in pre-training a language model, which significantly influences the model's capabilities and biases.",
    "url": "pages/glossary.html#term-data-mixture"
  },
  {
    "id": "term-data-parallelism",
    "title": "Data Parallelism",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "across",
      "computing",
      "data",
      "distributed",
      "entire",
      "gpu",
      "gradients",
      "model",
      "optimization",
      "parallelism",
      "replicas",
      "replicates"
    ],
    "excerpt": "A distributed training strategy that replicates the entire model on each GPU and splits the training data across replicas, synchronizing gradients after each step.",
    "url": "pages/glossary.html#term-data-parallelism"
  },
  {
    "id": "term-data-preprocessing",
    "title": "Data Preprocessing",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "applied",
      "categorical",
      "cleaning",
      "collection",
      "data",
      "duplicates",
      "encoding",
      "features",
      "handling",
      "improve",
      "including",
      "learning"
    ],
    "excerpt": "The collection of techniques applied to raw data before model training, including cleaning, handling missing values, encoding categorical variables, scaling features, and removing duplicates to improve data quality.",
    "url": "pages/glossary.html#term-data-preprocessing"
  },
  {
    "id": "term-data-preprocessing-images",
    "title": "Data Preprocessing for Images",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "applied",
      "color",
      "computer",
      "conversion",
      "data",
      "for",
      "format",
      "image",
      "images",
      "including",
      "inference",
      "meanstd"
    ],
    "excerpt": "The standardization pipeline applied to images before model training or inference, including resizing, normalization to specific mean/std values, color space conversion, and format transformations.",
    "url": "pages/glossary.html#term-data-preprocessing-images"
  },
  {
    "id": "term-data-privacy",
    "title": "Data Privacy (AI)",
    "category": "Glossary",
    "subcategory": "Ethics",
    "keywords": [
      "ai",
      "around",
      "concerns",
      "data",
      "ethics",
      "information",
      "personal",
      "practices",
      "privacy",
      "protecting",
      "safety",
      "systems"
    ],
    "excerpt": "Concerns and practices around protecting personal information when using AI systems. Includes what data is collected during interactions and how it's stored or used for training.",
    "url": "pages/glossary.html#term-data-privacy"
  },
  {
    "id": "term-data-sovereignty",
    "title": "Data Sovereignty",
    "category": "Glossary",
    "subcategory": "Privacy",
    "keywords": [
      "citizens",
      "collected",
      "community",
      "control",
      "data",
      "giving",
      "governance",
      "jurisdictions",
      "laws",
      "nation",
      "principle",
      "privacy"
    ],
    "excerpt": "The principle that data is subject to the laws and governance structures of the nation or community where it is collected or resides, giving jurisdictions control over how their citizens' data is used in AI systems.",
    "url": "pages/glossary.html#term-data-sovereignty"
  },
  {
    "id": "term-dataset",
    "title": "Dataset",
    "category": "Glossary",
    "subcategory": "Data",
    "keywords": [
      "collection",
      "data",
      "dataset",
      "fundamentals",
      "models",
      "testing",
      "training",
      "validating"
    ],
    "excerpt": "A collection of data used for training, validating, or testing AI models. Quality and diversity of datasets significantly impact model performance and fairness.",
    "url": "pages/glossary.html#term-dataset"
  },
  {
    "id": "term-datasheets-for-datasets",
    "title": "Datasheets for Datasets",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "datasets",
      "datasheets",
      "documentation",
      "ethics",
      "for",
      "gebru",
      "governance",
      "proposed",
      "standardized"
    ],
    "excerpt": "Standardized documentation proposed by Gebru et al. (2021) that accompanies ML datasets, describing their motivation, composition, collection process, preprocessing, intended uses, distribution, and maintenance.",
    "url": "pages/glossary.html#term-datasheets-for-datasets"
  },
  {
    "id": "term-david-rumelhart",
    "title": "David Rumelhart",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19422011",
      "1986",
      "american",
      "backpropagation",
      "coedited",
      "computer",
      "david",
      "distributed",
      "hinton",
      "history",
      "influential",
      "networks"
    ],
    "excerpt": "American psychologist and computer scientist (1942-2011) who, with Hinton and Williams, popularized backpropagation for neural networks and co-edited the influential Parallel Distributed Processing volumes in 1986.",
    "url": "pages/glossary.html#term-david-rumelhart"
  },
  {
    "id": "term-dbscan",
    "title": "DBSCAN",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithm",
      "applications",
      "arbitrary",
      "based",
      "closely",
      "clustering",
      "clusters",
      "count",
      "dbscan",
      "densitybased",
      "distance",
      "groups"
    ],
    "excerpt": "Density-Based Spatial Clustering of Applications with Noise, an algorithm that groups together points that are closely packed based on a distance threshold and minimum point count, identifying clusters of arbitrary shape and labeling outliers.",
    "url": "pages/glossary.html#term-dbscan"
  },
  {
    "id": "term-ddim",
    "title": "DDIM",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "ddim",
      "ddpm",
      "denoising",
      "deterministic",
      "diffusion",
      "enabling",
      "evaluations",
      "faster",
      "fewer",
      "function",
      "generation"
    ],
    "excerpt": "Denoising Diffusion Implicit Models, a deterministic sampling variant of DDPM that skips intermediate diffusion steps, enabling faster image generation with fewer function evaluations while maintaining quality.",
    "url": "pages/glossary.html#term-ddim"
  },
  {
    "id": "term-deadly-triad",
    "title": "Deadly Triad",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithms",
      "approximation",
      "bootstrapping",
      "cause",
      "combination",
      "deadly",
      "divergence",
      "function",
      "learning",
      "methods",
      "offpolicy",
      "reinforcement"
    ],
    "excerpt": "The combination of function approximation, bootstrapping, and off-policy learning that can cause divergence in RL algorithms.",
    "url": "pages/glossary.html#term-deadly-triad"
  },
  {
    "id": "term-debate-as-alignment",
    "title": "Debate as Alignment",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "alignment",
      "as",
      "debate",
      "irving",
      "proposed",
      "safety",
      "technique"
    ],
    "excerpt": "An AI safety technique proposed by Irving et al. where two AI agents debate each other on a question and a human judge selects the winner, incentivizing truthful and well-reasoned arguments over deception.",
    "url": "pages/glossary.html#term-debate-as-alignment"
  },
  {
    "id": "term-debate-prompting",
    "title": "Debate Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accurate",
      "adversarial",
      "agents",
      "argue",
      "conclusions",
      "debate",
      "discourse",
      "engineering",
      "instructs",
      "multiagent",
      "opposing",
      "positions"
    ],
    "excerpt": "A prompting strategy that instructs two or more simulated agents to argue opposing positions on a question, then uses the debate to surface stronger reasoning and reach more accurate conclusions through adversarial discourse.",
    "url": "pages/glossary.html#term-debate-prompting"
  },
  {
    "id": "term-deberta",
    "title": "DeBERTa",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "bert",
      "content",
      "deberta",
      "decoder",
      "decodingenhanced",
      "disentangled",
      "enhanced",
      "improves",
      "mask",
      "networks"
    ],
    "excerpt": "Decoding-enhanced BERT with disentangled attention, which improves BERT and RoBERTa by using separate vectors for content and position and an enhanced mask decoder for pretraining.",
    "url": "pages/glossary.html#term-deberta"
  },
  {
    "id": "term-debugging-prompting",
    "title": "Debugging Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "along",
      "approach",
      "buggy",
      "bugs",
      "causes",
      "code",
      "corrected",
      "debugging",
      "engineering",
      "error",
      "explain",
      "explanations"
    ],
    "excerpt": "A prompting approach that provides buggy code along with error messages or test failures and instructs the model to systematically identify root causes, explain the bugs, and produce corrected code with explanations of the fixes.",
    "url": "pages/glossary.html#term-debugging-prompting"
  },
  {
    "id": "term-deceptive-alignment",
    "title": "Deceptive Alignment",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "aligned",
      "alignment",
      "behave",
      "being",
      "deceptive",
      "deployed",
      "detects",
      "evaluated",
      "failure",
      "hypothesized",
      "learns"
    ],
    "excerpt": "A hypothesized failure mode where a mesa-optimizer learns to behave as if aligned during training in order to be deployed, but then pursues its own misaligned objective once it detects it is no longer being evaluated.",
    "url": "pages/glossary.html#term-deceptive-alignment"
  },
  {
    "id": "term-decision-boundary",
    "title": "Decision Boundary",
    "category": "Glossary",
    "subcategory": "ML Concept",
    "keywords": [
      "boundary",
      "classes",
      "classification",
      "concept",
      "decision",
      "different",
      "line",
      "ml",
      "model",
      "separates",
      "surface"
    ],
    "excerpt": "The line or surface that separates different classes in a classification model. The shape and complexity of decision boundaries determine what patterns a model can learn.",
    "url": "pages/glossary.html#term-decision-boundary"
  },
  {
    "id": "term-decision-transformer",
    "title": "Decision Transformer",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "approach",
      "architecture",
      "conditioned",
      "decision",
      "desired",
      "frames",
      "learning",
      "modeling",
      "paradigms",
      "past",
      "predict"
    ],
    "excerpt": "An approach that frames RL as a sequence modeling problem, using a transformer architecture to predict actions conditioned on desired returns, past states, and past actions.",
    "url": "pages/glossary.html#term-decision-transformer"
  },
  {
    "id": "term-decision-tree",
    "title": "Decision Tree",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "decision",
      "feature",
      "forming",
      "learning",
      "leaves",
      "machine",
      "model",
      "nonparametric",
      "partitions",
      "predictions",
      "recursively",
      "represent"
    ],
    "excerpt": "A non-parametric supervised learning model that recursively partitions the feature space using threshold-based splitting rules, forming a tree structure where leaves represent predictions.",
    "url": "pages/glossary.html#term-decision-tree"
  },
  {
    "id": "term-decode-phase",
    "title": "Decode Phase",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "autoregressive",
      "decode",
      "forward",
      "full",
      "generation",
      "inference",
      "infrastructure",
      "llm",
      "model",
      "one",
      "optimization",
      "pass"
    ],
    "excerpt": "The autoregressive generation phase of LLM inference where tokens are produced one at a time, each requiring a full model forward pass.",
    "url": "pages/glossary.html#term-decode-phase"
  },
  {
    "id": "term-decoder",
    "title": "Decoder",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "component",
      "decoder",
      "encoded",
      "generates",
      "network",
      "neural",
      "output",
      "representations",
      "transformers"
    ],
    "excerpt": "The component of a neural network that generates output from encoded representations. In transformers, decoder-only models (like GPT) generate text autoregressively.",
    "url": "pages/glossary.html#term-decoder"
  },
  {
    "id": "term-decoder-only-architecture",
    "title": "Decoder-Only Architecture",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "autoregressively",
      "blocks",
      "conditioning",
      "decoder",
      "design",
      "generates",
      "masked",
      "model",
      "networks",
      "neural",
      "only"
    ],
    "excerpt": "A transformer design using only masked self-attention decoder blocks, where the model generates output autoregressively by conditioning on all previous tokens in the sequence.",
    "url": "pages/glossary.html#term-decoder-only-architecture"
  },
  {
    "id": "term-decomposed-prompting",
    "title": "Decomposed Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "complex",
      "decomposed",
      "decomposes",
      "decomposition",
      "different",
      "enabling",
      "engineering",
      "external",
      "framework",
      "handled",
      "handler",
      "handlers"
    ],
    "excerpt": "A framework that decomposes complex tasks into simpler sub-tasks, each handled by specialized sub-prompt handlers, enabling modular problem-solving where each handler can use different prompting strategies or external tools.",
    "url": "pages/glossary.html#term-decomposed-prompting"
  },
  {
    "id": "term-deduplication",
    "title": "Deduplication",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "benchmark",
      "data",
      "deduplication",
      "documents",
      "duplicate",
      "ensure",
      "generative",
      "improve",
      "integrity",
      "llm",
      "memorization"
    ],
    "excerpt": "The process of removing duplicate or near-duplicate documents from training data to improve model quality, reduce memorization, and ensure benchmark integrity.",
    "url": "pages/glossary.html#term-deduplication"
  },
  {
    "id": "term-deep-blue",
    "title": "Deep Blue",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1997",
      "beat",
      "became",
      "blue",
      "champion",
      "chess",
      "chessplaying",
      "computer",
      "deep",
      "defeat",
      "first",
      "full"
    ],
    "excerpt": "An IBM chess-playing computer that became the first machine to defeat a reigning world chess champion in a full match when it beat Garry Kasparov in 1997, representing a milestone in game-playing AI.",
    "url": "pages/glossary.html#term-deep-blue"
  },
  {
    "id": "term-deep-boltzmann-machine",
    "title": "Deep Boltzmann Machine",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "abstract",
      "adjacent",
      "architecture",
      "boltzmann",
      "capable",
      "composed",
      "connections",
      "deep",
      "generative",
      "increasingly",
      "layers",
      "learning"
    ],
    "excerpt": "A multi-layer generative model composed of stacked Restricted Boltzmann Machines with undirected connections between all adjacent layers, capable of learning increasingly abstract representations.",
    "url": "pages/glossary.html#term-deep-boltzmann-machine"
  },
  {
    "id": "term-ddpg",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "actorcritic",
      "algorithm",
      "combines",
      "continuous",
      "ddpg",
      "deep",
      "deterministic",
      "dpg",
      "experience",
      "gradient",
      "learning"
    ],
    "excerpt": "An off-policy actor-critic algorithm for continuous action spaces that combines DPG with deep neural networks, experience replay, and target networks.",
    "url": "pages/glossary.html#term-ddpg"
  },
  {
    "id": "term-deep-learning",
    "title": "Deep Learning",
    "category": "Glossary",
    "subcategory": "Field",
    "keywords": [
      "deep",
      "field",
      "layers",
      "learning",
      "machine",
      "many",
      "networks",
      "neural",
      "subset"
    ],
    "excerpt": "A subset of machine learning using neural networks with many layers (\"deep\" networks). Enables learning complex patterns and representations from large amounts of data.",
    "url": "pages/glossary.html#term-deep-learning"
  },
  {
    "id": "term-deep-learning-breakthrough-2012",
    "title": "Deep Learning Breakthrough 2012",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2012",
      "alexnet",
      "breakthrough",
      "competition",
      "computer",
      "convolutional",
      "deep",
      "demonstrating",
      "dramatically",
      "gpus",
      "history",
      "imagenet"
    ],
    "excerpt": "The watershed moment when AlexNet dramatically won the ImageNet competition in 2012, demonstrating that deep convolutional neural networks trained on GPUs could vastly outperform traditional computer vision methods.",
    "url": "pages/glossary.html#term-deep-learning-breakthrough-2012"
  },
  {
    "id": "term-deep-q-network",
    "title": "Deep Q-Network",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "20132015",
      "architecture",
      "atari",
      "combined",
      "deep",
      "deepmind",
      "demonstrating",
      "developed",
      "games",
      "generalpurpose",
      "history",
      "learning"
    ],
    "excerpt": "A deep reinforcement learning architecture developed by DeepMind in 2013-2015 that combined Q-learning with deep neural networks to master Atari games from raw pixels, demonstrating general-purpose deep RL.",
    "url": "pages/glossary.html#term-deep-q-network"
  },
  {
    "id": "term-dqn",
    "title": "Deep Q-Network (DQN)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithm",
      "approximates",
      "deep",
      "dqn",
      "experience",
      "learning",
      "methods",
      "network",
      "neural",
      "qfunction",
      "reinforcement",
      "replay"
    ],
    "excerpt": "A deep RL algorithm that approximates the Q-function using a neural network, stabilized by experience replay and a separate target network.",
    "url": "pages/glossary.html#term-dqn"
  },
  {
    "id": "term-deepfake",
    "title": "Deepfake",
    "category": "Glossary",
    "subcategory": "Risk",
    "keywords": [
      "aigenerated",
      "deepfake",
      "ethics",
      "likeness",
      "manipulated",
      "media",
      "persons",
      "replaced",
      "risk",
      "synthetic"
    ],
    "excerpt": "AI-generated synthetic media where a person's likeness is replaced or manipulated. Raises concerns about misinformation, consent, and the authenticity of digital content.",
    "url": "pages/glossary.html#term-deepfake"
  },
  {
    "id": "term-deepfake-detection",
    "title": "Deepfake Detection",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "analysis",
      "anomalies",
      "artifacts",
      "deepfake",
      "detection",
      "ethics",
      "facial",
      "frequencydomain",
      "generated",
      "identify",
      "including"
    ],
    "excerpt": "The set of techniques and tools used to identify synthetically generated or manipulated media, including analysis of facial inconsistencies, temporal artifacts, frequency-domain anomalies, and provenance metadata.",
    "url": "pages/glossary.html#term-deepfake-detection"
  },
  {
    "id": "term-deeplab",
    "title": "DeepLab",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "architectures",
      "atrous",
      "capture",
      "computer",
      "context",
      "convolutions",
      "deeplab",
      "dilated",
      "family",
      "image",
      "multiscale",
      "pooling"
    ],
    "excerpt": "A family of semantic segmentation architectures that use atrous (dilated) convolutions and atrous spatial pyramid pooling to capture multi-scale context without reducing spatial resolution.",
    "url": "pages/glossary.html#term-deeplab"
  },
  {
    "id": "term-deepmind",
    "title": "DeepMind",
    "category": "Glossary",
    "subcategory": "Company",
    "keywords": [
      "alphafold",
      "alphago",
      "breakthroughs",
      "company",
      "deepmind",
      "gemini",
      "googles",
      "known",
      "lab",
      "like",
      "research"
    ],
    "excerpt": "Google's AI research lab known for breakthroughs like AlphaGo, AlphaFold, and Gemini. Pioneers in reinforcement learning, game-playing AI, and scientific applications.",
    "url": "pages/glossary.html#term-deepmind"
  },
  {
    "id": "term-deepmind-founding",
    "title": "DeepMind Founding",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2010",
      "2014",
      "500",
      "acquired",
      "approximately",
      "deepmind",
      "demis",
      "dollars",
      "founding",
      "google",
      "hassabis",
      "history"
    ],
    "excerpt": "The founding of DeepMind Technologies in London in 2010 by Demis Hassabis, Shane Legg, and Mustafa Suleyman, which was acquired by Google in 2014 for approximately 500 million dollars.",
    "url": "pages/glossary.html#term-deepmind-founding"
  },
  {
    "id": "term-deepseek",
    "title": "DeepSeek",
    "category": "Glossary",
    "subcategory": "Company",
    "keywords": [
      "chinese",
      "company",
      "deepseek",
      "efficient",
      "highperforming",
      "known",
      "model",
      "models",
      "open"
    ],
    "excerpt": "A Chinese AI company known for efficient, high-performing open models. Their DeepSeek-V2 and DeepSeek-Coder models demonstrate competitive performance at lower computational costs.",
    "url": "pages/glossary.html#term-deepseek"
  },
  {
    "id": "term-deepsort",
    "title": "DeepSORT",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "alongside",
      "appearance",
      "association",
      "computer",
      "data",
      "deep",
      "deepsort",
      "extension",
      "features",
      "identity",
      "image",
      "incorporates"
    ],
    "excerpt": "An extension of the SORT tracker that incorporates deep appearance features alongside motion information for data association, significantly reducing identity switches in multi-object tracking.",
    "url": "pages/glossary.html#term-deepsort"
  },
  {
    "id": "term-deepspeed",
    "title": "DeepSpeed",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "compression",
      "deep",
      "deepspeed",
      "deploying",
      "efficiently",
      "inference",
      "largescale",
      "learning",
      "library",
      "llm",
      "microsoft",
      "model"
    ],
    "excerpt": "A Microsoft deep learning optimization library that provides ZeRO-based training, inference optimization, and model compression techniques for efficiently training and deploying large-scale models.",
    "url": "pages/glossary.html#term-deepspeed"
  },
  {
    "id": "term-deformable-attention",
    "title": "Deformable Attention",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "applying",
      "around",
      "attends",
      "attention",
      "computational",
      "computer",
      "cost",
      "deformable",
      "detection",
      "dramatically",
      "feature",
      "highresolution"
    ],
    "excerpt": "An attention mechanism that attends to a small set of sampling points around a reference point with learnable offsets, dramatically reducing the computational cost of applying attention to high-resolution feature maps.",
    "url": "pages/glossary.html#term-deformable-attention"
  },
  {
    "id": "term-deformable-convolution",
    "title": "Deformable Convolution",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "adaptively",
      "adjust",
      "allowing",
      "architecture",
      "augmented",
      "convolution",
      "deformable",
      "field",
      "geometry",
      "grid",
      "learned",
      "match"
    ],
    "excerpt": "A convolution operation where the sampling grid positions are augmented with learned offsets, allowing the network to adaptively adjust its receptive field shape to match object geometry.",
    "url": "pages/glossary.html#term-deformable-convolution"
  },
  {
    "id": "term-deit",
    "title": "DeiT",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "achieve",
      "augmentation",
      "competitive",
      "computer",
      "data",
      "dataefficient",
      "datasets",
      "deit",
      "distillation",
      "image",
      "knowledge",
      "massive"
    ],
    "excerpt": "Data-efficient Image Transformer, a vision transformer training methodology that uses knowledge distillation and strong data augmentation to achieve competitive performance without requiring massive pre-training datasets.",
    "url": "pages/glossary.html#term-deit"
  },
  {
    "id": "term-delimiter",
    "title": "Delimiter",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "characters",
      "clearly",
      "content",
      "delimiter",
      "different",
      "prompting",
      "prompts",
      "sections",
      "separate",
      "symbols",
      "technique",
      "types"
    ],
    "excerpt": "Characters or symbols used in prompts to clearly separate different sections or types of content. Examples include triple backticks (```), XML tags, or custom markers like ###.",
    "url": "pages/glossary.html#term-delimiter"
  },
  {
    "id": "term-demis-hassabis",
    "title": "Demis Hassabis",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2010",
      "alphafold",
      "alphago",
      "british",
      "ceo",
      "cofounded",
      "deepmind",
      "demis",
      "development",
      "google",
      "hassabis",
      "history"
    ],
    "excerpt": "British AI researcher and neuroscientist who co-founded DeepMind in 2010, led the development of AlphaGo and AlphaFold, and serves as CEO of Google DeepMind.",
    "url": "pages/glossary.html#term-demis-hassabis"
  },
  {
    "id": "term-demographic-parity",
    "title": "Demographic Parity",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "across",
      "ai",
      "demographic",
      "equal",
      "ethics",
      "fairness",
      "groups",
      "metric",
      "outcome",
      "parity",
      "positive",
      "probability"
    ],
    "excerpt": "A fairness metric requiring that the probability of a positive outcome is equal across all protected groups.",
    "url": "pages/glossary.html#term-demographic-parity"
  },
  {
    "id": "term-dendral",
    "title": "DENDRAL",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1960s1970s",
      "chemical",
      "compounds",
      "data",
      "dendral",
      "developed",
      "edward",
      "expert",
      "feigenbaum",
      "first",
      "history",
      "identified"
    ],
    "excerpt": "One of the first expert systems, developed at Stanford in the 1960s-1970s by Edward Feigenbaum and Joshua Lederberg, which identified chemical compounds from mass spectrometry data using rule-based reasoning.",
    "url": "pages/glossary.html#term-dendral"
  },
  {
    "id": "term-denoising-autoencoder",
    "title": "Denoising Autoencoder",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "autoencoder",
      "capture",
      "clean",
      "corrupted",
      "data",
      "denoising",
      "distribution",
      "feature",
      "forcing",
      "inputs",
      "learning"
    ],
    "excerpt": "An autoencoder variant trained to reconstruct clean data from corrupted inputs, learning robust feature representations by forcing the network to capture the underlying data distribution.",
    "url": "pages/glossary.html#term-denoising-autoencoder"
  },
  {
    "id": "term-ddpm",
    "title": "Denoising Diffusion Probabilistic Model",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "chain",
      "denoising",
      "diffusion",
      "gaussian",
      "generating",
      "generative",
      "gradual",
      "iteratively",
      "learned",
      "learns",
      "markov"
    ],
    "excerpt": "A generative model that learns to reverse a gradual noising process, generating samples by iteratively denoising from pure Gaussian noise through a learned reverse Markov chain.",
    "url": "pages/glossary.html#term-ddpm"
  },
  {
    "id": "term-dense-passage-retriever",
    "title": "Dense Passage Retriever",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "answering",
      "architecture",
      "bertbased",
      "biencoder",
      "contrastive",
      "dense",
      "dpr",
      "encoders",
      "establishing",
      "foundational",
      "learning",
      "model"
    ],
    "excerpt": "A bi-encoder retrieval model (DPR) that trains separate BERT-based encoders for queries and passages using contrastive learning on question-answer pairs, establishing a foundational architecture fo...",
    "url": "pages/glossary.html#term-dense-passage-retriever"
  },
  {
    "id": "term-dense-prediction",
    "title": "Dense Prediction",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "computer",
      "dense",
      "depth",
      "estimation",
      "flow",
      "image",
      "including",
      "input",
      "normal",
      "optical",
      "output",
      "pixel"
    ],
    "excerpt": "Computer vision tasks that require producing an output for every pixel in an input image, including semantic segmentation, depth estimation, surface normal prediction, and optical flow.",
    "url": "pages/glossary.html#term-dense-prediction"
  },
  {
    "id": "term-dense-retrieval",
    "title": "Dense Retrieval",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "approach",
      "based",
      "beyond",
      "candidates",
      "capturing",
      "continuous",
      "dense",
      "documents",
      "encoders",
      "information",
      "lexical",
      "meaning"
    ],
    "excerpt": "An information retrieval approach that represents queries and documents as dense continuous vectors from neural encoders and retrieves candidates based on vector similarity, capturing semantic meaning beyond lexical overlap.",
    "url": "pages/glossary.html#term-dense-retrieval"
  },
  {
    "id": "term-dense-reward",
    "title": "Dense Reward",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "behavior",
      "dense",
      "design",
      "desired",
      "directly",
      "feedback",
      "frequent",
      "guiding",
      "informative",
      "learning",
      "nearly"
    ],
    "excerpt": "A reward structure that provides frequent, informative feedback at nearly every time step, guiding the agent more directly toward desired behavior.",
    "url": "pages/glossary.html#term-dense-reward"
  },
  {
    "id": "term-dense-sparse-hybrid",
    "title": "Dense-Sparse Hybrid",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "capture",
      "combination",
      "dense",
      "exactmatch",
      "fuses",
      "fusion",
      "hybrid",
      "lexical",
      "rank",
      "reciprocal",
      "relevance",
      "results"
    ],
    "excerpt": "A retrieval strategy that fuses results from both dense vector search and sparse lexical search, typically using reciprocal rank fusion or weighted score combination to capture both semantic and exact-match relevance signals.",
    "url": "pages/glossary.html#term-dense-sparse-hybrid"
  },
  {
    "id": "term-densenet",
    "title": "DenseNet",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "cnn",
      "densenet",
      "feature",
      "input",
      "layer",
      "layers",
      "maps",
      "networks",
      "neural",
      "own",
      "parameters"
    ],
    "excerpt": "A CNN architecture where each layer receives feature maps from all preceding layers as input and passes its own feature maps to all subsequent layers, promoting feature reuse and reducing parameters.",
    "url": "pages/glossary.html#term-densenet"
  },
  {
    "id": "term-dependency-parsing",
    "title": "Dependency Parsing",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "analyzing",
      "depend",
      "dependency",
      "directed",
      "grammatical",
      "identifying",
      "modify",
      "nlp",
      "parsing",
      "relationships",
      "representing",
      "sentence"
    ],
    "excerpt": "The task of analyzing the grammatical structure of a sentence by identifying directed relationships between words, representing which words modify or depend on other words.",
    "url": "pages/glossary.html#term-dependency-parsing"
  },
  {
    "id": "term-dependency-tree",
    "title": "Dependency Tree",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "dependencies",
      "dependency",
      "directed",
      "edges",
      "grammatical",
      "indicate",
      "like",
      "modifier",
      "nlp",
      "node",
      "object",
      "parsing"
    ],
    "excerpt": "A directed tree structure representing syntactic dependencies in a sentence where each word is a node and edges indicate grammatical relationships like subject, object, and modifier.",
    "url": "pages/glossary.html#term-dependency-tree"
  },
  {
    "id": "term-deployment-bias",
    "title": "Deployment Bias",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "ai",
      "behavior",
      "bias",
      "conditions",
      "contexts",
      "demographics",
      "deployment",
      "differ",
      "emerges",
      "environmental",
      "ethics",
      "fairness"
    ],
    "excerpt": "Bias that emerges when an AI system is used in contexts or populations that differ from its training conditions, including shifts in user behavior, environmental conditions, or population demographics.",
    "url": "pages/glossary.html#term-deployment-bias"
  },
  {
    "id": "term-depth-anything",
    "title": "Depth Anything",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "accurate",
      "across",
      "anything",
      "combination",
      "computer",
      "data",
      "depth",
      "diverse",
      "estimation",
      "foundation",
      "images"
    ],
    "excerpt": "A foundation model for monocular depth estimation that produces accurate relative depth maps from single images across diverse scenes, trained on a massive combination of labeled and unlabeled data.",
    "url": "pages/glossary.html#term-depth-anything"
  },
  {
    "id": "term-depth-estimation",
    "title": "Depth Estimation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "camera",
      "computer",
      "correspondence",
      "cues",
      "deep",
      "dense",
      "depth",
      "distance",
      "estimation",
      "image",
      "learned"
    ],
    "excerpt": "The task of predicting the distance of each pixel from the camera in a 2D image, producing a dense depth map using monocular cues learned by deep networks or stereo correspondence between image pairs.",
    "url": "pages/glossary.html#term-depth-estimation"
  },
  {
    "id": "term-depthwise-convolution",
    "title": "Depthwise Convolution",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "across",
      "applies",
      "architecture",
      "capturing",
      "channel",
      "channels",
      "convolution",
      "depthwise",
      "features",
      "filter",
      "independently",
      "information"
    ],
    "excerpt": "A convolution that applies a separate filter to each input channel independently, capturing spatial features per channel without mixing information across channels.",
    "url": "pages/glossary.html#term-depthwise-convolution"
  },
  {
    "id": "term-grouped-convolution",
    "title": "Depthwise Convolution Variant",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "applied",
      "architecture",
      "channels",
      "computation",
      "convolution",
      "depthwise",
      "divided",
      "group",
      "groups",
      "independently",
      "input",
      "networks"
    ],
    "excerpt": "A convolution where input channels are divided into groups and convolution is applied independently within each group, reducing parameters and computation proportional to the number of groups.",
    "url": "pages/glossary.html#term-grouped-convolution"
  },
  {
    "id": "term-depthwise-separable-convolution",
    "title": "Depthwise Separable Convolution",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "1x1",
      "applied",
      "architecture",
      "channel",
      "computation",
      "convolution",
      "decomposes",
      "depthwise",
      "factorized",
      "followed",
      "independently",
      "networks"
    ],
    "excerpt": "A factorized convolution that decomposes a standard convolution into a depthwise convolution applied independently per channel followed by a pointwise 1x1 convolution, reducing parameters and computation.",
    "url": "pages/glossary.html#term-depthwise-separable-convolution"
  },
  {
    "id": "term-deterministic-policy-gradient",
    "title": "Deterministic Policy Gradient (DPG)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "backpropagating",
      "computes",
      "deterministic",
      "dpg",
      "expected",
      "gradient",
      "learning",
      "optimization",
      "policies",
      "policy",
      "qfunction"
    ],
    "excerpt": "A policy gradient theorem for deterministic policies that computes the gradient of expected return by backpropagating through the Q-function with respect to actions.",
    "url": "pages/glossary.html#term-deterministic-policy-gradient"
  },
  {
    "id": "term-deterministic",
    "title": "Deterministic vs Stochastic",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "concept",
      "deterministic",
      "input",
      "llm",
      "output",
      "produce",
      "same",
      "stochastic",
      "systems",
      "time",
      "vs"
    ],
    "excerpt": "Deterministic systems produce the same output for the same input every time. LLMs are typically stochastic (random), producing varied outputs unless temperature is set to 0.",
    "url": "pages/glossary.html#term-deterministic"
  },
  {
    "id": "term-detokenization",
    "title": "Detokenization",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "back",
      "boundaries",
      "characters",
      "converting",
      "detokenization",
      "handling",
      "nlp",
      "process",
      "readable",
      "reversing",
      "sequence",
      "spacing"
    ],
    "excerpt": "The process of converting a sequence of tokens back into readable text by reversing the tokenization process, handling subword boundaries, spacing, and special characters.",
    "url": "pages/glossary.html#term-detokenization"
  },
  {
    "id": "term-detr",
    "title": "DETR",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "anchor",
      "architecture",
      "bipartite",
      "boxes",
      "components",
      "computer",
      "detection",
      "detr",
      "eliminating",
      "encoderdecoder",
      "endtoend",
      "handdesigned"
    ],
    "excerpt": "Detection Transformer, an end-to-end object detection model that uses a transformer encoder-decoder architecture with bipartite matching loss, eliminating the need for hand-designed components like anchor boxes and NMS.",
    "url": "pages/glossary.html#term-detr"
  },
  {
    "id": "term-deviance",
    "title": "Deviance",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "computed",
      "deviance",
      "difference",
      "fitted",
      "generalized",
      "goodnessoffit",
      "linear",
      "loglikelihoods",
      "metrics",
      "model",
      "models",
      "saturated"
    ],
    "excerpt": "A goodness-of-fit statistic for generalized linear models, computed as twice the difference in log-likelihoods between the fitted model and the saturated model.",
    "url": "pages/glossary.html#term-deviance"
  },
  {
    "id": "term-dgx-system",
    "title": "DGX System",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "computing",
      "dgx",
      "distributed",
      "gpus",
      "hardware",
      "highend",
      "integrated",
      "interconnects",
      "multiple",
      "nvidias",
      "nvlinknvswitch",
      "optimized"
    ],
    "excerpt": "NVIDIA's integrated AI supercomputing platform pre-configured with multiple high-end GPUs, NVLink/NVSwitch interconnects, and optimized software stack.",
    "url": "pages/glossary.html#term-dgx-system"
  },
  {
    "id": "term-dialogue-act",
    "title": "Dialogue Act",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "acknowledgment",
      "act",
      "categorization",
      "communicative",
      "conversation",
      "design",
      "dialogue",
      "function",
      "greeting",
      "linguistics",
      "nlp",
      "question"
    ],
    "excerpt": "A categorization of the communicative function of an utterance in a conversation, such as question, statement, request, greeting, or acknowledgment, used in dialogue system design.",
    "url": "pages/glossary.html#term-dialogue-act"
  },
  {
    "id": "term-dialogue-system",
    "title": "Dialogue System",
    "category": "Glossary",
    "subcategory": "Application",
    "keywords": [
      "application",
      "converse",
      "designed",
      "dialogue",
      "humans",
      "language",
      "natural",
      "nlp",
      "system"
    ],
    "excerpt": "An AI system designed to converse with humans in natural language. Includes task-oriented systems (customer service) and open-domain chatbots for general conversation.",
    "url": "pages/glossary.html#term-dialogue-system"
  },
  {
    "id": "term-differencing",
    "title": "Differencing",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "achieve",
      "computes",
      "consecutive",
      "data",
      "difference",
      "differencing",
      "observations",
      "periods",
      "science",
      "seasonal",
      "series",
      "stationarity"
    ],
    "excerpt": "A time series transformation that computes the difference between consecutive observations (or seasonal periods) to achieve stationarity.",
    "url": "pages/glossary.html#term-differencing"
  },
  {
    "id": "term-differentiable-neural-computer",
    "title": "Differentiable Neural Computer",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "ability",
      "adds",
      "algorithms",
      "allocation",
      "architecture",
      "complex",
      "computer",
      "data",
      "differentiable",
      "dynamic",
      "examples",
      "extension"
    ],
    "excerpt": "An extension of the Neural Turing Machine that adds temporal link tracking and dynamic memory allocation, improving the ability to learn complex data structures and algorithms from examples.",
    "url": "pages/glossary.html#term-differentiable-neural-computer"
  },
  {
    "id": "term-differentiable-rendering",
    "title": "Differentiable Rendering",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "computer",
      "differentiable",
      "enabling",
      "formation",
      "geometry",
      "gradientbased",
      "image",
      "images",
      "lighting",
      "materials",
      "optimization"
    ],
    "excerpt": "Rendering techniques where the image formation process is differentiable with respect to scene parameters, enabling gradient-based optimization of 3D geometry, materials, and lighting from 2D images.",
    "url": "pages/glossary.html#term-differentiable-rendering"
  },
  {
    "id": "term-differential-privacy",
    "title": "Differential Privacy",
    "category": "Glossary",
    "subcategory": "Privacy",
    "keywords": [
      "achieved",
      "adding",
      "ai",
      "calibrated",
      "computation",
      "data",
      "differential",
      "ethics",
      "formal",
      "framework",
      "guarantees",
      "included"
    ],
    "excerpt": "A mathematical framework providing formal guarantees that the output of a computation does not reveal whether any single individual's data was included in the input, typically achieved by adding calibrated noise.",
    "url": "pages/glossary.html#term-differential-privacy"
  },
  {
    "id": "term-differential-technology-development",
    "title": "Differential Technology Development",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "advances",
      "ahead",
      "ai",
      "capabilities",
      "capability",
      "dangerous",
      "defensive",
      "developing",
      "development",
      "differential",
      "ensuring",
      "governance"
    ],
    "excerpt": "The strategic prioritization of developing defensive and safety technologies ahead of potentially dangerous capabilities, ensuring that protective measures keep pace with or precede capability advances.",
    "url": "pages/glossary.html#term-differential-technology-development"
  },
  {
    "id": "term-diffusion-model",
    "title": "Diffusion Model",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "content",
      "creates",
      "data",
      "diffusion",
      "generative",
      "gradually",
      "model",
      "noise",
      "random",
      "removing"
    ],
    "excerpt": "A generative AI architecture that creates content by gradually removing noise from random data. Powers leading image generators like Stable Diffusion, DALL-E 3, and Midjourney.",
    "url": "pages/glossary.html#term-diffusion-model"
  },
  {
    "id": "term-diffusion-model-breakthrough",
    "title": "Diffusion Model Breakthrough",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "20202022",
      "breakthrough",
      "dalle",
      "demonstrated",
      "denoise",
      "diffusion",
      "diffusionbased",
      "emergence",
      "enabling",
      "generation",
      "generative",
      "highquality"
    ],
    "excerpt": "The emergence of diffusion-based generative models in 2020-2022 that progressively denoise random noise into high-quality images, enabling photorealistic image generation as demonstrated by DALL-E 2, Midjourney, and Stable Diffusion.",
    "url": "pages/glossary.html#term-diffusion-model-breakthrough"
  },
  {
    "id": "term-diffusion-transformer",
    "title": "Diffusion Transformer",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "achieving",
      "ai",
      "architecture",
      "backbone",
      "better",
      "diffusion",
      "dit",
      "effectively",
      "generation",
      "generative",
      "image",
      "latent"
    ],
    "excerpt": "An architecture (DiT) that replaces the U-Net backbone in diffusion models with a transformer operating on sequences of latent patches, scaling more effectively and achieving better image generation quality.",
    "url": "pages/glossary.html#term-diffusion-transformer"
  },
  {
    "id": "term-digital-provenance",
    "title": "Digital Provenance",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "aigenerated",
      "asset",
      "authenticity",
      "content",
      "creation",
      "digital",
      "era",
      "establishing",
      "ethics",
      "governance",
      "history"
    ],
    "excerpt": "The verifiable record of the origin, creation process, and modification history of a digital asset, increasingly important for establishing trust and authenticity in an era of AI-generated content.",
    "url": "pages/glossary.html#term-digital-provenance"
  },
  {
    "id": "term-digital-watermarking-for-ai",
    "title": "Digital Watermarking for AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "aigenerated",
      "attribution",
      "audio",
      "content",
      "detection",
      "digital",
      "embedding",
      "enabling",
      "ethics",
      "for",
      "identifying"
    ],
    "excerpt": "Techniques for embedding imperceptible identifying information into AI-generated content such as images, text, or audio, enabling later detection and attribution of synthetic media.",
    "url": "pages/glossary.html#term-digital-watermarking-for-ai"
  },
  {
    "id": "term-dilated-attention",
    "title": "Dilated Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "allowing",
      "architecture",
      "attended",
      "attends",
      "attention",
      "capture",
      "computations",
      "dependencies",
      "dilated",
      "fewer",
      "gaps",
      "intervals"
    ],
    "excerpt": "An attention mechanism that attends to tokens at regularly spaced intervals with gaps between attended positions, allowing each token to capture long-range dependencies with fewer computations.",
    "url": "pages/glossary.html#term-dilated-attention"
  },
  {
    "id": "term-dilated-convolution",
    "title": "Dilated Convolution",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "convolution",
      "dilated",
      "elements",
      "exponentially",
      "field",
      "gaps",
      "increases",
      "increasing",
      "kernel",
      "networks",
      "neural"
    ],
    "excerpt": "A convolution operation with gaps between kernel elements that exponentially increases the receptive field without increasing parameters or reducing spatial resolution.",
    "url": "pages/glossary.html#term-dilated-convolution"
  },
  {
    "id": "term-dimensionality-reduction",
    "title": "Dimensionality Reduction",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "data",
      "dimensionality",
      "features",
      "important",
      "information",
      "number",
      "preserving",
      "processing",
      "reduce",
      "reduction",
      "technique",
      "techniques"
    ],
    "excerpt": "Techniques to reduce the number of features in data while preserving important information. Used for visualization, noise reduction, and improving computational efficiency.",
    "url": "pages/glossary.html#term-dimensionality-reduction"
  },
  {
    "id": "term-dimensionality-reduction-vectors",
    "title": "Dimensionality Reduction for Vectors",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "accelerate",
      "curse",
      "database",
      "dimensionality",
      "distance",
      "embedding",
      "for",
      "highdimensional",
      "information",
      "lowerdimensional",
      "mitigate",
      "much"
    ],
    "excerpt": "Techniques that project high-dimensional embedding vectors into lower-dimensional spaces to reduce storage, accelerate search, and mitigate the curse of dimensionality while preserving as much distance relationship information as possible.",
    "url": "pages/glossary.html#term-dimensionality-reduction-vectors"
  },
  {
    "id": "term-dinov2",
    "title": "DINOv2",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "across",
      "combination",
      "computer",
      "dinov2",
      "diverse",
      "downstream",
      "features",
      "finetuning",
      "image",
      "masked",
      "model",
      "modeling"
    ],
    "excerpt": "A self-supervised vision model trained with a combination of self-distillation and masked image modeling that produces versatile visual features useful across diverse downstream tasks without fine-tuning.",
    "url": "pages/glossary.html#term-dinov2"
  },
  {
    "id": "term-directional-stimulus-prompting",
    "title": "Directional Stimulus Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "cues",
      "desired",
      "direction",
      "directional",
      "engineering",
      "framework",
      "generate",
      "generation",
      "guide",
      "guided",
      "hint",
      "language"
    ],
    "excerpt": "A prompting framework that provides a small, tunable stimulus or hint within the prompt to guide the language model toward a desired output direction, often using a lightweight policy model to generate these directional cues.",
    "url": "pages/glossary.html#term-directional-stimulus-prompting"
  },
  {
    "id": "term-dirichlet-distribution",
    "title": "Dirichlet Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "beta",
      "dirichlet",
      "distribution",
      "generalization",
      "generates",
      "multivariate",
      "one",
      "probability",
      "statistics",
      "summing",
      "vectors"
    ],
    "excerpt": "A multivariate generalization of the beta distribution that generates probability vectors summing to one. It is widely used as a prior over categorical distributions and in topic models like LDA.",
    "url": "pages/glossary.html#term-dirichlet-distribution"
  },
  {
    "id": "term-disaggregated-serving",
    "title": "Disaggregated Serving",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "architecture",
      "compute",
      "computing",
      "disaggregated",
      "distributed",
      "independent",
      "independently",
      "inference",
      "infrastructure",
      "memory",
      "pools",
      "resources"
    ],
    "excerpt": "An inference architecture that separates storage, compute, and memory resources into independent pools that can be scaled independently.",
    "url": "pages/glossary.html#term-disaggregated-serving"
  },
  {
    "id": "term-discount-factor",
    "title": "Discount Factor",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "concepts",
      "core",
      "determines",
      "discount",
      "factor",
      "future",
      "gamma",
      "immediate",
      "learning",
      "much",
      "parameter",
      "reinforcement"
    ],
    "excerpt": "A parameter gamma between 0 and 1 that determines how much future rewards are weighted relative to immediate rewards.",
    "url": "pages/glossary.html#term-discount-factor"
  },
  {
    "id": "term-discourse-analysis",
    "title": "Discourse Analysis",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "across",
      "analysis",
      "coherence",
      "connect",
      "discourse",
      "examining",
      "flow",
      "information",
      "linguistics",
      "nlp",
      "relate",
      "relations"
    ],
    "excerpt": "The study of how sentences and utterances connect and relate to each other in text, examining coherence relations, rhetorical structure, and information flow across sentences.",
    "url": "pages/glossary.html#term-discourse-analysis"
  },
  {
    "id": "term-discourse-relation",
    "title": "Discourse Relation",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "causeeffect",
      "coherence",
      "contrast",
      "contributes",
      "discourse",
      "document",
      "elaboration",
      "linguistics",
      "nlp",
      "pragmatic",
      "relation",
      "relationship"
    ],
    "excerpt": "A semantic or pragmatic relationship between text segments such as cause-effect, contrast, elaboration, or temporal sequence that contributes to the coherence of a document.",
    "url": "pages/glossary.html#term-discourse-relation"
  },
  {
    "id": "term-discretization",
    "title": "Discretization",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "binning",
      "bins",
      "categories",
      "continuous",
      "converting",
      "decision",
      "discrete",
      "discretization",
      "engineering",
      "equalfrequency",
      "equalwidth",
      "feature"
    ],
    "excerpt": "The process of converting continuous features into discrete bins or categories, using methods such as equal-width binning, equal-frequency binning, or supervised methods like decision tree-based binning.",
    "url": "pages/glossary.html#term-discretization"
  },
  {
    "id": "term-disinformation",
    "title": "Disinformation",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "created",
      "deceive",
      "deliberately",
      "disinformation",
      "ethics",
      "false",
      "information",
      "intent",
      "misleading",
      "safety",
      "spread"
    ],
    "excerpt": "False or misleading information deliberately created and spread with the intent to deceive. AI-generated disinformation is an escalating concern due to the increasing quality and scale of synthetic media.",
    "url": "pages/glossary.html#term-disinformation"
  },
  {
    "id": "term-disparate-impact",
    "title": "Disparate Impact",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "concept",
      "discriminatory",
      "disparate",
      "disproportionately",
      "ethical",
      "even",
      "fairness",
      "group",
      "harms",
      "impact",
      "intent",
      "legal"
    ],
    "excerpt": "A legal and ethical concept where a seemingly neutral AI policy or practice disproportionately harms members of a protected group, even without discriminatory intent.",
    "url": "pages/glossary.html#term-disparate-impact"
  },
  {
    "id": "term-disparity-map",
    "title": "Disparity Map",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "computer",
      "corresponding",
      "depth",
      "disparity",
      "displacement",
      "horizontal",
      "images",
      "inversely",
      "left",
      "map",
      "pixellevel"
    ],
    "excerpt": "A pixel-level representation of the horizontal displacement between corresponding points in left and right stereo images, inversely proportional to depth and used for 3D scene reconstruction.",
    "url": "pages/glossary.html#term-disparity-map"
  },
  {
    "id": "term-distilbert",
    "title": "DistilBERT",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "being",
      "bert",
      "capabilities",
      "distilbert",
      "distillation",
      "distilled",
      "faster",
      "knowledge",
      "language",
      "networks",
      "neural"
    ],
    "excerpt": "A distilled version of BERT that retains 97% of its language understanding capabilities while being 60% smaller and 60% faster, trained using knowledge distillation techniques.",
    "url": "pages/glossary.html#term-distilbert"
  },
  {
    "id": "term-distillation",
    "title": "Distillation (Knowledge Distillation)",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "distillation",
      "knowledge",
      "large",
      "model",
      "optimization",
      "smaller",
      "student",
      "teacher",
      "technique",
      "training",
      "transfer"
    ],
    "excerpt": "A technique to transfer knowledge from a large \"teacher\" model to a smaller \"student\" model. Creates efficient models that retain much of the larger model's capability.",
    "url": "pages/glossary.html#term-distillation"
  },
  {
    "id": "term-distinct-n",
    "title": "Distinct-N",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "calculates",
      "distinct",
      "diversity",
      "evaluation",
      "generated",
      "higher",
      "indicate",
      "language",
      "less",
      "lexical",
      "measuring",
      "metric"
    ],
    "excerpt": "A diversity metric that calculates the ratio of unique n-grams to total n-grams in generated text, measuring lexical diversity where higher values indicate more varied and less repetitive language use.",
    "url": "pages/glossary.html#term-distinct-n"
  },
  {
    "id": "term-distributed-training",
    "title": "Distributed Training",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "across",
      "clusters",
      "computing",
      "datasets",
      "distributed",
      "gpus",
      "handle",
      "larger",
      "model",
      "models",
      "multiple",
      "nodes"
    ],
    "excerpt": "The practice of spreading model training across multiple GPUs, nodes, or clusters to handle larger models and datasets.",
    "url": "pages/glossary.html#term-distributed-training"
  },
  {
    "id": "term-distribution-shift",
    "title": "Distribution Shift",
    "category": "Glossary",
    "subcategory": "Challenge",
    "keywords": [
      "challenge",
      "data",
      "differs",
      "distribution",
      "encounters",
      "model",
      "production",
      "shift",
      "training"
    ],
    "excerpt": "When the data a model encounters in production differs from its training data. A major cause of model degradation over time, requiring monitoring and retraining.",
    "url": "pages/glossary.html#term-distribution-shift"
  },
  {
    "id": "term-distributional-rl",
    "title": "Distributional Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "distribution",
      "distributional",
      "expected",
      "extension",
      "full",
      "just",
      "learning",
      "methods",
      "models",
      "rather",
      "reinforcement",
      "returns"
    ],
    "excerpt": "An extension of value-based RL that models the full distribution of returns rather than just the expected value.",
    "url": "pages/glossary.html#term-distributional-rl"
  },
  {
    "id": "term-distributional-semantics",
    "title": "Distributional Semantics",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "appear",
      "characterized",
      "contexts",
      "distributional",
      "distributions",
      "formalized",
      "hypothesis",
      "linguistics",
      "meaning",
      "meanings",
      "nlp",
      "semantics"
    ],
    "excerpt": "The theory that word meaning can be characterized by the contexts in which words appear, formalized as the distributional hypothesis: words with similar distributions have similar meanings.",
    "url": "pages/glossary.html#term-distributional-semantics"
  },
  {
    "id": "term-diverse-beam-search",
    "title": "Diverse Beam Search",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "beam",
      "candidate",
      "decoding",
      "different",
      "diverse",
      "diversity",
      "encouraging",
      "generation",
      "generative",
      "groups",
      "introduces"
    ],
    "excerpt": "A variant of beam search that introduces a diversity penalty between beam groups, encouraging the generation of a set of meaningfully different candidate sequences rather than near-duplicates.",
    "url": "pages/glossary.html#term-diverse-beam-search"
  },
  {
    "id": "term-diversity-in-retrieval",
    "title": "Diversity in Retrieval",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "achieved",
      "algorithms",
      "aspects",
      "clusteringbased",
      "cover",
      "determinantal",
      "different",
      "diversity",
      "goal",
      "in",
      "like",
      "mmr"
    ],
    "excerpt": "The goal of returning search results that cover different aspects, perspectives, or subtopics of a query rather than returning redundant near-duplicate results, achieved through algorithms like MMR...",
    "url": "pages/glossary.html#term-diversity-in-retrieval"
  },
  {
    "id": "term-diversity-score",
    "title": "Diversity Score",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "among",
      "differences",
      "diversity",
      "evaluation",
      "generated",
      "heterogeneity",
      "homogeneous",
      "lexical",
      "measuring",
      "metric",
      "metrics",
      "outputs"
    ],
    "excerpt": "A metric that quantifies the variety and heterogeneity of a set of generated outputs by measuring lexical, semantic, or topical differences among them, penalizing systems that produce repetitive or homogeneous responses.",
    "url": "pages/glossary.html#term-diversity-score"
  },
  {
    "id": "term-document-ai",
    "title": "Document AI",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "ai",
      "analysis",
      "combining",
      "computer",
      "contracts",
      "document",
      "documents",
      "extract",
      "forms",
      "image",
      "information",
      "invoices"
    ],
    "excerpt": "AI systems that understand and extract structured information from documents by combining OCR, layout analysis, and language understanding to process invoices, forms, contracts, and other document types.",
    "url": "pages/glossary.html#term-document-ai"
  },
  {
    "id": "term-document-chunking",
    "title": "Document Chunking",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "access",
      "balancing",
      "chunk",
      "chunking",
      "coherence",
      "document",
      "documents",
      "embedding",
      "granularity",
      "indexing",
      "individual",
      "information"
    ],
    "excerpt": "The process of splitting larger documents into smaller text segments for individual embedding and indexing, balancing between preserving semantic coherence within each chunk and maintaining retrievable granularity for precise information access.",
    "url": "pages/glossary.html#term-document-chunking"
  },
  {
    "id": "term-document-embedding",
    "title": "Document Embedding",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "captures",
      "clustering",
      "comparison",
      "content",
      "dense",
      "document",
      "embedding",
      "embeddings",
      "entire",
      "nlp",
      "overall",
      "representation"
    ],
    "excerpt": "A dense vector representation of an entire document that captures its overall semantic content, used for document retrieval, clustering, and similarity comparison tasks.",
    "url": "pages/glossary.html#term-document-embedding"
  },
  {
    "id": "term-document-qa",
    "title": "Document Q&amp;amp;A",
    "category": "Glossary",
    "subcategory": "Application",
    "keywords": [
      "answer",
      "application",
      "document",
      "documents",
      "qampampa",
      "questions",
      "rag",
      "specific",
      "text"
    ],
    "excerpt": "Using AI to answer questions about specific documents or text. Often implemented with RAG to enable models to reference specific sources rather than relying solely on training.",
    "url": "pages/glossary.html#term-document-qa"
  },
  {
    "id": "term-domain-adaptation",
    "title": "Domain Adaptation",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "adaptation",
      "adapting",
      "domain",
      "learning",
      "model",
      "one",
      "techniques",
      "trained",
      "training",
      "transfer"
    ],
    "excerpt": "Techniques for adapting a model trained on one domain (e.g., general text) to perform well on another domain (e.g., medical or legal text) with limited target domain data.",
    "url": "pages/glossary.html#term-domain-adaptation"
  },
  {
    "id": "term-domain-randomization",
    "title": "Domain Randomization",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "backgrounds",
      "computer",
      "data",
      "deployed",
      "domain",
      "heavily",
      "image",
      "images",
      "lighting",
      "making",
      "model",
      "models"
    ],
    "excerpt": "A sim-to-real transfer technique that trains vision models on synthetic images with heavily randomized visual properties (lighting, textures, backgrounds), making the model robust when deployed on real-world data.",
    "url": "pages/glossary.html#term-domain-randomization"
  },
  {
    "id": "term-domain-specific-prompting",
    "title": "Domain-Specific Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accuracy",
      "adaptation",
      "constraints",
      "contextual",
      "conventions",
      "crafting",
      "domain",
      "engineering",
      "field",
      "finance",
      "improve",
      "incorporate"
    ],
    "excerpt": "The practice of crafting prompts that incorporate specialized vocabulary, conventions, constraints, and contextual knowledge particular to a specific field such as medicine, law, or finance to improve model accuracy within that domain.",
    "url": "pages/glossary.html#term-domain-specific-prompting"
  },
  {
    "id": "term-donald-hebb",
    "title": "Donald Hebb",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19041985",
      "1949",
      "basis",
      "behavior",
      "book",
      "canadian",
      "computational",
      "donald",
      "hebb",
      "hebbian",
      "his",
      "history"
    ],
    "excerpt": "Canadian neuropsychologist (1904-1985) who proposed Hebbian learning theory in his 1949 book The Organization of Behavior, providing a neurobiological basis for learning that inspired computational models of neural plasticity.",
    "url": "pages/glossary.html#term-donald-hebb"
  },
  {
    "id": "term-dot-product-similarity",
    "title": "Dot Product Similarity",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "additionally",
      "capturing",
      "computed",
      "cosine",
      "database",
      "dot",
      "elementwise",
      "equivalent",
      "information",
      "magnitude",
      "measure",
      "normalized"
    ],
    "excerpt": "A similarity measure computed as the sum of element-wise products of two vectors, equivalent to cosine similarity when vectors are normalized, and additionally capturing magnitude information when they are not.",
    "url": "pages/glossary.html#term-dot-product-similarity"
  },
  {
    "id": "term-dot-product-attention",
    "title": "Dot-Product Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "compatibility",
      "computes",
      "dimension",
      "dot",
      "key",
      "large",
      "magnitude",
      "mechanism",
      "networks",
      "neural"
    ],
    "excerpt": "An attention mechanism that computes compatibility scores as the dot product between query and key vectors, scaled by the square root of the key dimension to prevent large magnitude scores.",
    "url": "pages/glossary.html#term-dot-product-attention"
  },
  {
    "id": "term-double-dqn",
    "title": "Double DQN",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "actions",
      "addresses",
      "bias",
      "decoupling",
      "double",
      "dqn",
      "evaluate",
      "evaluation",
      "extension",
      "learning",
      "methods"
    ],
    "excerpt": "An extension of DQN that addresses overestimation bias by decoupling action selection from action evaluation, using the online network to select actions and the target network to evaluate them.",
    "url": "pages/glossary.html#term-double-dqn"
  },
  {
    "id": "term-douglas-lenat",
    "title": "Douglas Lenat",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19502023",
      "1984",
      "ambitious",
      "american",
      "build",
      "commonsense",
      "comprehensive",
      "computer",
      "created",
      "cyc",
      "douglas",
      "effort"
    ],
    "excerpt": "American computer scientist (1950-2023) who created the Cyc project in 1984, an ambitious effort to build a comprehensive ontology of common-sense knowledge to enable AI reasoning about everyday situations.",
    "url": "pages/glossary.html#term-douglas-lenat"
  },
  {
    "id": "term-dpo",
    "title": "DPO (Direct Preference Optimization)",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "aligning",
      "alignment",
      "alternative",
      "direct",
      "dpo",
      "language",
      "models",
      "optimization",
      "preference",
      "rlhf",
      "simpler",
      "training"
    ],
    "excerpt": "A simpler alternative to RLHF for aligning language models. Directly optimizes the model using preference data without needing a separate reward model.",
    "url": "pages/glossary.html#term-dpo"
  },
  {
    "id": "term-dreambooth",
    "title": "DreamBooth",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "classspecific",
      "diffusion",
      "dreambooth",
      "few",
      "finetuning",
      "generate",
      "generative",
      "identifier",
      "image",
      "images",
      "just"
    ],
    "excerpt": "A fine-tuning technique that personalizes diffusion models to generate images of specific subjects by training on just a few reference images with a unique identifier token and class-specific prior preservation.",
    "url": "pages/glossary.html#term-dreambooth"
  },
  {
    "id": "term-dreamer",
    "title": "Dreamer",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "dreamer",
      "entirely",
      "generated",
      "imagined",
      "latent",
      "learning",
      "learns",
      "model",
      "modelbased",
      "planning",
      "policy"
    ],
    "excerpt": "A model-based RL agent that learns a world model in latent space and trains its policy entirely through imagined trajectories generated by the model.",
    "url": "pages/glossary.html#term-dreamer"
  },
  {
    "id": "term-dropout",
    "title": "Dropout",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "deactivates",
      "dropout",
      "neurons",
      "randomly",
      "regularization",
      "technique",
      "training"
    ],
    "excerpt": "A regularization technique that randomly deactivates neurons during training. Prevents overfitting by forcing the network to learn more robust features.",
    "url": "pages/glossary.html#term-dropout"
  },
  {
    "id": "term-dropout-technique",
    "title": "Dropout Technique",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "dropout",
      "hinton",
      "history",
      "method",
      "milestones",
      "proposed",
      "regularization",
      "technique"
    ],
    "excerpt": "A regularization method proposed by Hinton et al. in 2012 that randomly deactivates neurons during training to prevent overfitting, becoming one of the most widely used techniques in deep learning practice.",
    "url": "pages/glossary.html#term-dropout-technique"
  },
  {
    "id": "term-droppath",
    "title": "DropPath",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "architectures",
      "branches",
      "connections",
      "droppath",
      "drops",
      "entire",
      "generalization",
      "improving",
      "method",
      "multiple",
      "networks"
    ],
    "excerpt": "A regularization method for networks with multiple parallel paths that randomly drops entire residual branches during training, improving generalization in architectures with residual connections.",
    "url": "pages/glossary.html#term-droppath"
  },
  {
    "id": "term-dual-use-technology",
    "title": "Dual-Use Technology",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "applications",
      "autonomous",
      "beneficial",
      "capabilities",
      "civilian",
      "computer",
      "concept",
      "dual",
      "generation",
      "governance",
      "harmful"
    ],
    "excerpt": "Technology that can be used for both beneficial and harmful purposes, a concept particularly relevant to AI capabilities such as language generation, computer vision, and autonomous systems that have both civilian and military applications.",
    "url": "pages/glossary.html#term-dual-use-technology"
  },
  {
    "id": "term-dueling-dqn",
    "title": "Dueling DQN",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "advantage",
      "architecture",
      "combining",
      "dqn",
      "dueling",
      "estimates",
      "function",
      "learning",
      "methods",
      "produce",
      "qvalues",
      "reinforcement"
    ],
    "excerpt": "A DQN architecture that separately estimates the state value function and the advantage function, combining them to produce Q-values.",
    "url": "pages/glossary.html#term-dueling-dqn"
  },
  {
    "id": "term-durbin-watson-test",
    "title": "Durbin-Watson Test",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "analysis",
      "autocorrelation",
      "detecting",
      "durbin",
      "firstorder",
      "inference",
      "regression",
      "residuals",
      "statistical",
      "statistics",
      "test",
      "watson"
    ],
    "excerpt": "A statistical test for detecting first-order autocorrelation in the residuals of a regression analysis.",
    "url": "pages/glossary.html#term-durbin-watson-test"
  },
  {
    "id": "term-dyna-architecture",
    "title": "Dyna Architecture",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "architecture",
      "direct",
      "dyna",
      "environment",
      "experience",
      "framework",
      "generated",
      "integrates",
      "learned",
      "learning",
      "model",
      "modelbased"
    ],
    "excerpt": "A model-based RL framework that integrates direct learning from real experience with planning through simulated experience generated by a learned environment model.",
    "url": "pages/glossary.html#term-dyna-architecture"
  },
  {
    "id": "term-dynamic-loss-scaling",
    "title": "Dynamic Loss Scaling",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "adapts",
      "automatic",
      "decreasing",
      "detected",
      "dynamic",
      "factor",
      "gradients",
      "hardware",
      "increasing",
      "loss",
      "model",
      "optimization"
    ],
    "excerpt": "An automatic loss scaling strategy that adapts the scaling factor during training, increasing it when no overflow is detected and decreasing it when gradients overflow.",
    "url": "pages/glossary.html#term-dynamic-loss-scaling"
  },
  {
    "id": "term-dynamic-prompting",
    "title": "Dynamic Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "adaptive",
      "adjusted",
      "approach",
      "based",
      "content",
      "context",
      "dynamic",
      "engineering",
      "examples",
      "fixed",
      "information",
      "input"
    ],
    "excerpt": "A prompting approach where the content, structure, or examples within a prompt are programmatically adjusted at runtime based on the input query, user context, or retrieved information rather than using a fixed static prompt.",
    "url": "pages/glossary.html#term-dynamic-prompting"
  },
  {
    "id": "term-dynamic-quantization",
    "title": "Dynamic Quantization",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "activation",
      "actual",
      "approach",
      "based",
      "computes",
      "dynamic",
      "encountered",
      "factors",
      "inference",
      "infrastructure",
      "model",
      "onthefly"
    ],
    "excerpt": "A quantization approach that computes scaling factors on-the-fly during inference based on the actual range of activation values encountered.",
    "url": "pages/glossary.html#term-dynamic-quantization"
  },
  {
    "id": "term-early-stopping",
    "title": "Early Stopping",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "early",
      "improving",
      "performance",
      "regularization",
      "set",
      "stopping",
      "stops",
      "technique",
      "training",
      "validation"
    ],
    "excerpt": "A regularization technique that stops training when performance on a validation set stops improving. Prevents overfitting and saves computational resources.",
    "url": "pages/glossary.html#term-early-stopping"
  },
  {
    "id": "term-earth-movers-distance",
    "title": "Earth Mover&amp;#x27;s Distance",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "amount",
      "based",
      "comparing",
      "distance",
      "distribution",
      "distributions",
      "earth",
      "mass",
      "metric",
      "metrics",
      "minimum",
      "moved"
    ],
    "excerpt": "A metric for comparing probability distributions based on the minimum amount of work needed to transform one distribution into the other, where work is the amount of mass moved multiplied by the distance it is moved.",
    "url": "pages/glossary.html#term-earth-movers-distance"
  },
  {
    "id": "term-edge-ai",
    "title": "Edge AI",
    "category": "Glossary",
    "subcategory": "Deployment",
    "keywords": [
      "ai",
      "architecture",
      "cloud",
      "deployment",
      "devices",
      "edge",
      "iot",
      "locally",
      "models",
      "phones",
      "rather",
      "running"
    ],
    "excerpt": "Running AI models locally on devices (phones, IoT) rather than in the cloud. Enables faster responses, offline operation, and better privacy but requires efficient models.",
    "url": "pages/glossary.html#term-edge-ai"
  },
  {
    "id": "term-edge-inference",
    "title": "Edge Inference",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "bandwidth",
      "cloud",
      "devices",
      "directly",
      "edge",
      "embedded",
      "hardware",
      "inference",
      "infrastructure",
      "iot",
      "latency",
      "model"
    ],
    "excerpt": "Running AI model inference directly on edge devices (phones, IoT sensors, embedded systems) rather than in the cloud, reducing latency and bandwidth requirements.",
    "url": "pages/glossary.html#term-edge-inference"
  },
  {
    "id": "term-edit-distance",
    "title": "Edit Distance",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "another",
      "dameraulevenshtein",
      "distance",
      "edit",
      "family",
      "hamming",
      "including",
      "levenshtein",
      "metrics",
      "minimum",
      "nlp",
      "number"
    ],
    "excerpt": "A family of metrics quantifying the minimum number of operations required to transform one string into another, with variants including Levenshtein, Damerau-Levenshtein, and Hamming distance.",
    "url": "pages/glossary.html#term-edit-distance"
  },
  {
    "id": "term-edward-feigenbaum",
    "title": "Edward Feigenbaum",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1970s1980s",
      "american",
      "commercial",
      "computer",
      "demonstrating",
      "dendral",
      "development",
      "edward",
      "engineering",
      "expert",
      "father",
      "feigenbaum"
    ],
    "excerpt": "American computer scientist known as the father of expert systems, who led the development of DENDRAL and pioneered knowledge engineering at Stanford, demonstrating the commercial viability of AI in the 1970s-1980s.",
    "url": "pages/glossary.html#term-edward-feigenbaum"
  },
  {
    "id": "term-effect-size",
    "title": "Effect Size",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "effect",
      "independent",
      "inference",
      "magnitude",
      "measure",
      "phenomenon",
      "practical",
      "quantitative",
      "result",
      "sample",
      "significance",
      "size"
    ],
    "excerpt": "A quantitative measure of the magnitude of a phenomenon or the practical significance of a result, independent of sample size.",
    "url": "pages/glossary.html#term-effect-size"
  },
  {
    "id": "term-efficiency",
    "title": "Efficiency",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "data",
      "efficiency",
      "estimator",
      "inference",
      "information",
      "much",
      "property",
      "related",
      "statistical",
      "statistics"
    ],
    "excerpt": "A property of a statistical estimator related to how much information from the data it uses. An efficient estimator achieves the lowest possible variance among all unbiased estimators, reaching the Cramer-Rao bound.",
    "url": "pages/glossary.html#term-efficiency"
  },
  {
    "id": "term-efficientnet",
    "title": "EfficientNet",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "accuracy",
      "achieving",
      "architecture",
      "cnn",
      "coefficients",
      "compound",
      "depth",
      "efficientnet",
      "family",
      "fewer",
      "fixed",
      "models"
    ],
    "excerpt": "A family of CNN models that use compound scaling to uniformly scale network width, depth, and resolution using a fixed set of scaling coefficients, achieving state-of-the-art accuracy with fewer parameters.",
    "url": "pages/glossary.html#term-efficientnet"
  },
  {
    "id": "term-elastic-net",
    "title": "Elastic Net",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "balancing",
      "combines",
      "elastic",
      "feature",
      "learning",
      "linearly",
      "machine",
      "method",
      "net",
      "optimization",
      "penalty",
      "regularization"
    ],
    "excerpt": "A regularization method that linearly combines L1 and L2 penalty terms, balancing feature selection (sparsity) with weight shrinkage.",
    "url": "pages/glossary.html#term-elastic-net"
  },
  {
    "id": "term-elastic-training",
    "title": "Elastic Training",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "approach",
      "computing",
      "distributed",
      "dynamically",
      "elastic",
      "model",
      "number",
      "optimization",
      "requiring",
      "restart",
      "run",
      "scale"
    ],
    "excerpt": "A distributed training approach that can dynamically scale the number of workers up or down during a training run without requiring a restart.",
    "url": "pages/glossary.html#term-elastic-training"
  },
  {
    "id": "term-elbow-method",
    "title": "Elbow Method",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "additional",
      "against",
      "clusters",
      "diminishing",
      "elbow",
      "forming",
      "heuristic",
      "identifying",
      "learning",
      "machine",
      "method",
      "model"
    ],
    "excerpt": "A heuristic for selecting the optimal number of clusters by plotting the within-cluster sum of squares against the number of clusters and identifying the point where additional clusters yield diminishing returns, forming an elbow shape.",
    "url": "pages/glossary.html#term-elbow-method"
  },
  {
    "id": "term-electra",
    "title": "ELECTRA",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "detect",
      "discriminator",
      "efficient",
      "electra",
      "generator",
      "input",
      "language",
      "learning",
      "masked",
      "method",
      "modeling"
    ],
    "excerpt": "A pretraining method that trains a discriminator to detect tokens replaced by a small generator network, providing more efficient training than masked language modeling by learning from all input tokens.",
    "url": "pages/glossary.html#term-electra"
  },
  {
    "id": "term-eliciting-latent-knowledge",
    "title": "Eliciting Latent Knowledge",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "accurately",
      "ai",
      "alignment",
      "eliciting",
      "extracting",
      "focused",
      "information",
      "internally",
      "knowledge",
      "latent",
      "learned",
      "misleadingly"
    ],
    "excerpt": "A research problem in AI alignment focused on extracting truthful information from a model that may have learned to represent the world accurately internally but could report misleadingly.",
    "url": "pages/glossary.html#term-eliciting-latent-knowledge"
  },
  {
    "id": "term-eligibility-trace",
    "title": "Eligibility Trace",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithms",
      "backward",
      "credit",
      "decaying",
      "distribute",
      "eligibility",
      "learning",
      "memory",
      "methods",
      "recently",
      "reinforcement",
      "states"
    ],
    "excerpt": "A decaying memory of recently visited states used in TD(lambda) and other RL algorithms to distribute credit backward in time.",
    "url": "pages/glossary.html#term-eligibility-trace"
  },
  {
    "id": "term-eliza",
    "title": "ELIZA",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1966",
      "becoming",
      "chatbots",
      "created",
      "demonstrating",
      "eliza",
      "first",
      "history",
      "illusion",
      "joseph",
      "language",
      "milestones"
    ],
    "excerpt": "A natural language processing program created by Joseph Weizenbaum at MIT in 1966 that simulated a Rogerian psychotherapist, demonstrating the illusion of understanding and becoming one of the first chatbots.",
    "url": "pages/glossary.html#term-eliza"
  },
  {
    "id": "term-ellipsis-resolution",
    "title": "Ellipsis Resolution",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "coffee",
      "context",
      "ellipsis",
      "identifying",
      "implied",
      "include",
      "john",
      "likes",
      "linguistics",
      "mary",
      "nlp",
      "omitted"
    ],
    "excerpt": "The task of identifying and recovering omitted words or phrases in text that are understood from context, such as resolving 'John likes coffee and Mary tea' to include the implied verb.",
    "url": "pages/glossary.html#term-ellipsis-resolution"
  },
  {
    "id": "term-elmo",
    "title": "ELMo",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "bidirectional",
      "contextualized",
      "elmo",
      "embeddings",
      "entire",
      "function",
      "generates",
      "input",
      "language",
      "lstm",
      "method",
      "model"
    ],
    "excerpt": "Embeddings from Language Models, a contextualized word representation method that generates word vectors as a function of the entire input sentence using a bidirectional LSTM language model.",
    "url": "pages/glossary.html#term-elmo"
  },
  {
    "id": "term-elo-rating-for-models",
    "title": "ELO Rating for Models",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "adaptation",
      "automated",
      "based",
      "chess",
      "comparisons",
      "elo",
      "evaluation",
      "evaluators",
      "for",
      "gain",
      "headtohead",
      "humans"
    ],
    "excerpt": "An adaptation of the chess ELO rating system to rank language models through pairwise comparisons, where models gain or lose rating points based on head-to-head evaluation outcomes judged by humans or automated evaluators.",
    "url": "pages/glossary.html#term-elo-rating-for-models"
  },
  {
    "id": "term-embedding",
    "title": "Embedding",
    "category": "Glossary",
    "subcategory": "Representation",
    "keywords": [
      "continuous",
      "data",
      "dense",
      "embedding",
      "images",
      "nlp",
      "representation",
      "sentences",
      "space",
      "vector",
      "words"
    ],
    "excerpt": "A dense vector representation of data (words, sentences, images) in a continuous space. Similar items have similar embeddings, enabling semantic search and comparison.",
    "url": "pages/glossary.html#term-embedding"
  },
  {
    "id": "term-embedding-caching",
    "title": "Embedding Caching",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "avoiding",
      "caching",
      "computational",
      "computed",
      "content",
      "cost",
      "database",
      "embedding",
      "inference",
      "latency",
      "model",
      "performance"
    ],
    "excerpt": "The practice of storing previously computed embedding vectors for reuse, avoiding redundant embedding model inference for repeated or similar content and reducing latency and computational cost in production retrieval pipelines.",
    "url": "pages/glossary.html#term-embedding-caching"
  },
  {
    "id": "term-embedding-dimension",
    "title": "Embedding Dimension",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "384",
      "4096",
      "capacity",
      "components",
      "database",
      "depending",
      "determining",
      "dimension",
      "dimensions",
      "embedding",
      "embeddings",
      "footprint"
    ],
    "excerpt": "The number of components in a vector embedding, determining the representational capacity and memory footprint of the embedding space, with typical values ranging from 384 to 4096 dimensions depending on the embedding model.",
    "url": "pages/glossary.html#term-embedding-dimension"
  },
  {
    "id": "term-embedding-drift",
    "title": "Embedding Drift",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "accuracy",
      "changes",
      "data",
      "database",
      "degrading",
      "distribution",
      "drift",
      "embedding",
      "embeddings",
      "evolves",
      "index",
      "maintain"
    ],
    "excerpt": "The phenomenon where the distribution of vector embeddings changes over time as source data evolves or embedding models are updated, potentially degrading retrieval quality and requiring index refresh or reindexing to maintain accuracy.",
    "url": "pages/glossary.html#term-embedding-drift"
  },
  {
    "id": "term-embedding-fine-tuning",
    "title": "Embedding Fine-Tuning",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "better",
      "case",
      "contrastive",
      "data",
      "database",
      "domains",
      "domainspecific",
      "embedding",
      "embeddings",
      "fine",
      "further",
      "improving"
    ],
    "excerpt": "The process of further training a pre-trained embedding model on domain-specific data using contrastive learning or other objectives to produce embeddings better suited for a particular use case, improving retrieval relevance in specialized domains.",
    "url": "pages/glossary.html#term-embedding-fine-tuning"
  },
  {
    "id": "term-embedding-model",
    "title": "Embedding Model",
    "category": "Glossary",
    "subcategory": "Model Type",
    "keywords": [
      "convert",
      "data",
      "designed",
      "embedding",
      "images",
      "model",
      "representation",
      "representations",
      "specifically",
      "text",
      "type",
      "vector"
    ],
    "excerpt": "A model specifically designed to convert text, images, or other data into vector representations. Popular embedding models include OpenAI's text-embedding-ada-002 and open-source alternatives like E5.",
    "url": "pages/glossary.html#term-embedding-model"
  },
  {
    "id": "term-embedding-quantization",
    "title": "Embedding Quantization",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "32bit",
      "accelerate",
      "binary",
      "compression",
      "costs",
      "embedding",
      "floats",
      "formats",
      "highdimensional",
      "inference",
      "int8",
      "llm"
    ],
    "excerpt": "The compression of high-dimensional embedding vectors from 32-bit floats to lower precision formats (binary, int8) to reduce storage costs and accelerate similarity search with minimal retrieval quality loss.",
    "url": "pages/glossary.html#term-embedding-quantization"
  },
  {
    "id": "term-embedding-similarity-search",
    "title": "Embedding Similarity Search",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "computing",
      "database",
      "distances",
      "embedding",
      "embeddings",
      "finding",
      "forming",
      "foundation",
      "information",
      "items",
      "modern",
      "neural"
    ],
    "excerpt": "The process of finding the most semantically similar items to a query by computing distances between their vector embeddings in a shared embedding space, forming the foundation of modern neural information retrieval systems.",
    "url": "pages/glossary.html#term-embedding-similarity-search"
  },
  {
    "id": "term-embedding-space",
    "title": "Embedding Space",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "according",
      "apart",
      "chosen",
      "concepts",
      "continuous",
      "database",
      "dissimilar",
      "distance",
      "embedding",
      "embeddings",
      "encode",
      "far"
    ],
    "excerpt": "The continuous high-dimensional vector space in which embeddings reside, where geometric relationships between vectors encode semantic relationships, with similar concepts located nearby and dissim...",
    "url": "pages/glossary.html#term-embedding-space"
  },
  {
    "id": "term-emergent-abilities",
    "title": "Emergent Abilities",
    "category": "Glossary",
    "subcategory": "Phenomenon",
    "keywords": [
      "abilities",
      "appear",
      "capabilities",
      "emergent",
      "large",
      "models",
      "phenomenon",
      "present",
      "scaling",
      "smaller",
      "versions",
      "werent"
    ],
    "excerpt": "Capabilities that appear in large AI models that weren't present in smaller versions. Examples include complex reasoning, code generation, and following nuanced instructions.",
    "url": "pages/glossary.html#term-emergent-abilities"
  },
  {
    "id": "term-emergent-capability",
    "title": "Emergent Capability",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ability",
      "absent",
      "ai",
      "appears",
      "capability",
      "code",
      "debated",
      "emergence",
      "emergent",
      "generation",
      "generative",
      "language"
    ],
    "excerpt": "An ability that appears in large language models only at sufficient scale and is absent in smaller models, such as multi-step reasoning or code generation, though the sharpness of this emergence is debated.",
    "url": "pages/glossary.html#term-emergent-capability"
  },
  {
    "id": "term-emotion-detection",
    "title": "Emotion Detection",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "analysis",
      "anger",
      "binary",
      "classification",
      "detection",
      "emotion",
      "emotions",
      "expressed",
      "fear",
      "finergrained",
      "identifying",
      "joy"
    ],
    "excerpt": "The task of identifying specific emotions such as joy, anger, sadness, fear, or surprise expressed in text, providing finer-grained analysis than binary sentiment classification.",
    "url": "pages/glossary.html#term-emotion-detection"
  },
  {
    "id": "term-emotion-prompting",
    "title": "Emotion Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "appends",
      "behavioral",
      "charged",
      "cues",
      "emotion",
      "emotional",
      "emotionally",
      "engineering",
      "importance",
      "improved",
      "language",
      "leveraging"
    ],
    "excerpt": "A technique that appends emotionally charged phrases to prompts such as urgency cues or importance markers, leveraging the observation that language models can respond to emotional stimuli with improved task performance.",
    "url": "pages/glossary.html#term-emotion-prompting"
  },
  {
    "id": "term-emotion-recognition",
    "title": "Emotion Recognition",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "affect",
      "annotated",
      "body",
      "categories",
      "classification",
      "computer",
      "datasets",
      "displays",
      "emotion",
      "emotional",
      "expressions",
      "facial"
    ],
    "excerpt": "The classification of facial expressions or body language into emotional categories using computer vision models trained on annotated datasets of human affect displays.",
    "url": "pages/glossary.html#term-emotion-recognition"
  },
  {
    "id": "term-emotion-recognition-ai-ethics",
    "title": "Emotion Recognition AI Ethics",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "bias",
      "claim",
      "concerns",
      "cultural",
      "detect",
      "doubts",
      "emotion",
      "emotions",
      "ethical",
      "ethics",
      "expressions"
    ],
    "excerpt": "Ethical concerns about AI systems that claim to detect human emotions from facial expressions, voice, or physiological signals, including scientific validity doubts, cultural bias, and privacy implications.",
    "url": "pages/glossary.html#term-emotion-recognition-ai-ethics"
  },
  {
    "id": "term-empirical-bayes",
    "title": "Empirical Bayes",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "approach",
      "bayes",
      "bayesian",
      "data",
      "distribution",
      "empirical",
      "estimates",
      "itself",
      "methods",
      "parameters",
      "prior",
      "priori"
    ],
    "excerpt": "An approach that estimates prior distribution parameters from the data itself, rather than specifying them a priori.",
    "url": "pages/glossary.html#term-empirical-bayes"
  },
  {
    "id": "term-empirical-distribution-function",
    "title": "Empirical Distribution Function",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "assigns",
      "constructed",
      "cumulative",
      "data",
      "distribution",
      "empirical",
      "function",
      "observed",
      "probability",
      "sample",
      "science",
      "statistics"
    ],
    "excerpt": "A cumulative distribution function constructed from sample data that assigns probability 1/n to each observed value.",
    "url": "pages/glossary.html#term-empirical-distribution-function"
  },
  {
    "id": "term-empirical-risk-minimization",
    "title": "Empirical Risk Minimization",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "average",
      "data",
      "empirical",
      "hypothesis",
      "learning",
      "loss",
      "machine",
      "minimization",
      "minimizing",
      "model",
      "principle",
      "risk"
    ],
    "excerpt": "A learning principle that selects the hypothesis minimizing the average loss on the training data. While simple and intuitive, it can lead to overfitting without regularization or capacity constraints.",
    "url": "pages/glossary.html#term-empirical-risk-minimization"
  },
  {
    "id": "term-empowerment",
    "title": "Empowerment",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "agents",
      "capacity",
      "channel",
      "defined",
      "empowerment",
      "exploration",
      "future",
      "informationtheoretic",
      "intrinsic",
      "learning",
      "measure"
    ],
    "excerpt": "An information-theoretic intrinsic motivation measure defined as the channel capacity between an agent's actions and its future states.",
    "url": "pages/glossary.html#term-empowerment"
  },
  {
    "id": "term-encoder",
    "title": "Encoder",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "component",
      "compressed",
      "encoder",
      "input",
      "network",
      "neural",
      "representation",
      "transformers",
      "transforms"
    ],
    "excerpt": "A neural network component that transforms input into a compressed representation. In transformers, encoder models (like BERT) process the entire input at once for understanding tasks.",
    "url": "pages/glossary.html#term-encoder"
  },
  {
    "id": "term-encoder-decoder-architecture",
    "title": "Encoder-Decoder Architecture",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "commonly",
      "decoder",
      "design",
      "encoder",
      "generates",
      "input",
      "latent",
      "network",
      "networks",
      "neural",
      "output"
    ],
    "excerpt": "A neural network design where an encoder processes input into a latent representation and a decoder generates output from that representation, commonly used for translation and summarization.",
    "url": "pages/glossary.html#term-encoder-decoder-architecture"
  },
  {
    "id": "term-encoder-only-architecture",
    "title": "Encoder-Only Architecture",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "bidirectional",
      "blocks",
      "classification",
      "contextualized",
      "design",
      "encoder",
      "input",
      "networks",
      "neural",
      "only",
      "producing"
    ],
    "excerpt": "A transformer design using only bidirectional self-attention encoder blocks, producing contextualized representations of input tokens suited for classification and understanding tasks.",
    "url": "pages/glossary.html#term-encoder-only-architecture"
  },
  {
    "id": "term-endpoint",
    "title": "Endpoint",
    "category": "Glossary",
    "subcategory": "API",
    "keywords": [
      "accessed",
      "api",
      "endpoint",
      "specific",
      "technical",
      "url"
    ],
    "excerpt": "A specific URL where an API can be accessed. AI services expose endpoints for different functions like chat completions, embeddings, and image generation.",
    "url": "pages/glossary.html#term-endpoint"
  },
  {
    "id": "term-eniac",
    "title": "ENIAC",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1945",
      "completed",
      "computation",
      "computer",
      "computers",
      "demonstrated",
      "digital",
      "earliest",
      "electronic",
      "eniac",
      "feasibility",
      "generalpurpose"
    ],
    "excerpt": "The Electronic Numerical Integrator and Computer, completed in 1945 at the University of Pennsylvania, one of the earliest general-purpose electronic digital computers that demonstrated the feasibility of large-scale electronic computation.",
    "url": "pages/glossary.html#term-eniac"
  },
  {
    "id": "term-ensemble",
    "title": "Ensemble Methods",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "better",
      "combine",
      "ensemble",
      "methods",
      "ml",
      "model",
      "models",
      "multiple",
      "produce",
      "results",
      "single",
      "technique"
    ],
    "excerpt": "Techniques that combine multiple models to produce better results than any single model. Includes voting, bagging (Random Forest), and boosting (XGBoost). Often used in production for reliability.",
    "url": "pages/glossary.html#term-ensemble"
  },
  {
    "id": "term-enterprise-ai",
    "title": "Enterprise AI",
    "category": "Glossary",
    "subcategory": "Business",
    "keywords": [
      "access",
      "ai",
      "application",
      "business",
      "compliance",
      "controls",
      "data",
      "designed",
      "enterprise",
      "environments",
      "existing",
      "features"
    ],
    "excerpt": "AI solutions designed for business environments with features like access controls, compliance, data privacy, and integration with existing systems.",
    "url": "pages/glossary.html#term-enterprise-ai"
  },
  {
    "id": "term-entity-disambiguation",
    "title": "Entity Disambiguation",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "bases",
      "clues",
      "context",
      "determining",
      "disambiguation",
      "entities",
      "entity",
      "knowledge",
      "mention",
      "multiple",
      "name",
      "nlp"
    ],
    "excerpt": "The process of determining which specific real-world entity a textual mention refers to when the same name could refer to multiple entities, using context clues and knowledge bases.",
    "url": "pages/glossary.html#term-entity-disambiguation"
  },
  {
    "id": "term-entropy",
    "title": "Entropy",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "disorder",
      "distribution",
      "entropy",
      "information",
      "measure",
      "probability",
      "quantifying",
      "random",
      "statistics",
      "theory",
      "uncertainty",
      "variables"
    ],
    "excerpt": "A measure from information theory quantifying the uncertainty or disorder in a random variable's distribution.",
    "url": "pages/glossary.html#term-entropy"
  },
  {
    "id": "term-entropy-regularization",
    "title": "Entropy Regularization",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "adds",
      "agent",
      "committing",
      "discouraging",
      "entropy",
      "function",
      "learning",
      "objective",
      "optimization",
      "policy",
      "quickly"
    ],
    "excerpt": "A technique that adds the policy entropy to the RL objective function, discouraging the agent from committing to a single action too quickly.",
    "url": "pages/glossary.html#term-entropy-regularization"
  },
  {
    "id": "term-environment",
    "title": "Environment",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "agent",
      "concepts",
      "core",
      "environment",
      "external",
      "interacts",
      "learning",
      "observations",
      "providing",
      "reinforcement",
      "response"
    ],
    "excerpt": "The external system an RL agent interacts with, providing observations and rewards in response to actions. The environment defines the dynamics and rules governing state transitions.",
    "url": "pages/glossary.html#term-environment"
  },
  {
    "id": "term-epipolar-geometry",
    "title": "Epipolar Geometry",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "appear",
      "camera",
      "computer",
      "constraining",
      "defined",
      "epipolar",
      "essential",
      "fundamental",
      "geometric",
      "geometry",
      "image"
    ],
    "excerpt": "The geometric relationship between two camera views of the same scene, defined by the fundamental or essential matrix, constraining where a point in one image can appear in the other.",
    "url": "pages/glossary.html#term-epipolar-geometry"
  },
  {
    "id": "term-episode",
    "title": "Episode",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "complete",
      "concepts",
      "core",
      "episode",
      "episodic",
      "initial",
      "interaction",
      "learning",
      "reinforcement",
      "sequence",
      "state",
      "tasks"
    ],
    "excerpt": "A complete sequence of interaction from an initial state to a terminal state in episodic RL tasks. Episodes provide natural boundaries for computing returns and resetting the environment.",
    "url": "pages/glossary.html#term-episode"
  },
  {
    "id": "term-epoch",
    "title": "Epoch",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "complete",
      "dataset",
      "entire",
      "epoch",
      "one",
      "pass",
      "technical",
      "training"
    ],
    "excerpt": "One complete pass through the entire training dataset. Models typically train for multiple epochs, with each pass allowing weights to be refined based on all available data.",
    "url": "pages/glossary.html#term-epoch"
  },
  {
    "id": "term-epsilon-greedy",
    "title": "Epsilon-Greedy Exploration",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "1epsilon",
      "action",
      "agent",
      "bestknown",
      "epsilon",
      "exploration",
      "greedy",
      "learning",
      "probability",
      "random",
      "reinforcement",
      "selects"
    ],
    "excerpt": "An exploration strategy where the agent selects the greedy (best-known) action with probability 1-epsilon and a random action with probability epsilon.",
    "url": "pages/glossary.html#term-epsilon-greedy"
  },
  {
    "id": "term-equalized-odds",
    "title": "Equalized Odds",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "accuracy",
      "across",
      "ai",
      "classifier",
      "criterion",
      "ensuring",
      "equal",
      "equalized",
      "ethics",
      "fairness",
      "false",
      "group"
    ],
    "excerpt": "A fairness criterion requiring that a classifier has equal true positive rates and equal false positive rates across all protected groups, ensuring that prediction accuracy does not vary by group membership.",
    "url": "pages/glossary.html#term-equalized-odds"
  },
  {
    "id": "term-error-analysis",
    "title": "Error Analysis",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "analysis",
      "error",
      "evaluation",
      "examination",
      "failure",
      "guide",
      "improvements",
      "mistakes",
      "model",
      "patterns",
      "process",
      "systematic"
    ],
    "excerpt": "Systematic examination of model mistakes to understand failure patterns and guide improvements. Essential for iterating on model performance and identifying data or training issues.",
    "url": "pages/glossary.html#term-error-analysis"
  },
  {
    "id": "term-roce",
    "title": "Ethernet for AI (RoCE)",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "access",
      "ai",
      "computing",
      "converged",
      "direct",
      "distributed",
      "enables",
      "ethernet",
      "for",
      "hardware",
      "infrastructure",
      "memory"
    ],
    "excerpt": "RDMA over Converged Ethernet, a networking protocol that enables remote direct memory access over Ethernet infrastructure.",
    "url": "pages/glossary.html#term-roce"
  },
  {
    "id": "term-ethical-prompting",
    "title": "Ethical Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "beneficial",
      "constraints",
      "designing",
      "engineering",
      "ethical",
      "ethics",
      "explicitly",
      "fairness",
      "guidelines",
      "harmavoidance",
      "incorporate",
      "instructions"
    ],
    "excerpt": "The practice of designing prompts that explicitly incorporate ethical guidelines, fairness constraints, and harm-avoidance instructions to steer model outputs toward responsible, unbiased, and socially beneficial responses.",
    "url": "pages/glossary.html#term-ethical-prompting"
  },
  {
    "id": "term-ethics-board",
    "title": "Ethics Board (AI)",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "ai",
      "board",
      "concerns",
      "deployment",
      "development",
      "ethical",
      "ethics",
      "governance",
      "group",
      "reviews"
    ],
    "excerpt": "A group that reviews AI development and deployment for ethical concerns. Many major AI companies have ethics boards to evaluate potential harms and establish guidelines.",
    "url": "pages/glossary.html#term-ethics-board"
  },
  {
    "id": "term-ethics-washing",
    "title": "Ethics Washing",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "boards",
      "changes",
      "development",
      "ethics",
      "frameworks",
      "governance",
      "implementing",
      "meaningful",
      "organizations",
      "practice",
      "practices"
    ],
    "excerpt": "The practice of organizations using ethics boards, principles, or frameworks as public relations tools without implementing meaningful changes to their AI development practices or governance structures.",
    "url": "pages/glossary.html#term-ethics-washing"
  },
  {
    "id": "term-eu-ai-act",
    "title": "EU AI Act",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "2024",
      "act",
      "adopted",
      "ai",
      "artificial",
      "bans",
      "certain",
      "classifies",
      "comprehensive",
      "eu",
      "european",
      "framework"
    ],
    "excerpt": "The European Union's comprehensive regulatory framework for artificial intelligence, adopted in 2024, which classifies AI systems by risk level and imposes requirements ranging from transparency obligations to outright bans on certain uses.",
    "url": "pages/glossary.html#term-eu-ai-act"
  },
  {
    "id": "term-euclidean-distance",
    "title": "Euclidean Distance",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "across",
      "computed",
      "differences",
      "dimensions",
      "distance",
      "euclidean",
      "learning",
      "machine",
      "metrics",
      "points",
      "root",
      "space"
    ],
    "excerpt": "The straight-line distance between two points in Euclidean space, computed as the square root of the sum of squared differences across all dimensions.",
    "url": "pages/glossary.html#term-euclidean-distance"
  },
  {
    "id": "term-evaluation",
    "title": "Evaluation",
    "category": "Glossary",
    "subcategory": "Process",
    "keywords": [
      "assessment",
      "benchmarks",
      "evaluation",
      "human",
      "measuring",
      "metrics",
      "model",
      "performance",
      "process",
      "quality"
    ],
    "excerpt": "The process of measuring model performance using metrics, benchmarks, and human assessment. Critical for comparing models and ensuring they meet quality standards.",
    "url": "pages/glossary.html#term-evaluation"
  },
  {
    "id": "term-evaluation-bias",
    "title": "Evaluation Bias",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "adequately",
      "ai",
      "benchmark",
      "bias",
      "datasets",
      "deployment",
      "diversity",
      "estimates",
      "ethics",
      "evaluation",
      "fairness",
      "groups"
    ],
    "excerpt": "Bias introduced during model evaluation when benchmark datasets or metrics do not adequately represent the diversity of the deployment population, leading to overly optimistic performance estimates for some groups.",
    "url": "pages/glossary.html#term-evaluation-bias"
  },
  {
    "id": "term-eval-harness",
    "title": "Evaluation Harness",
    "category": "Glossary",
    "subcategory": "Tools",
    "keywords": [
      "across",
      "benchmarks",
      "evaluation",
      "framework",
      "harness",
      "models",
      "multiple",
      "systematically",
      "tasks",
      "testing",
      "tools"
    ],
    "excerpt": "A framework for systematically testing AI models across multiple benchmarks and tasks. Popular harnesses include lm-evaluation-harness used for open-source model comparisons.",
    "url": "pages/glossary.html#term-eval-harness"
  },
  {
    "id": "term-event-extraction",
    "title": "Event Extraction",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "arguments",
      "attributes",
      "determining",
      "event",
      "eventspecific",
      "extraction",
      "happened",
      "identifying",
      "involved",
      "nlp",
      "processing",
      "task"
    ],
    "excerpt": "The task of identifying event triggers and their arguments in text, determining what happened, who was involved, when, where, and other event-specific attributes.",
    "url": "pages/glossary.html#term-event-extraction"
  },
  {
    "id": "term-evidence-lower-bound",
    "title": "Evidence Lower Bound",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "bayesian",
      "bound",
      "evidence",
      "function",
      "inference",
      "learning",
      "likelihood",
      "log",
      "lower",
      "machine",
      "marginal",
      "methods"
    ],
    "excerpt": "A lower bound on the log marginal likelihood (model evidence) that serves as the objective function in variational inference.",
    "url": "pages/glossary.html#term-evidence-lower-bound"
  },
  {
    "id": "term-evolution-strategies-rl",
    "title": "Evolution Strategies for RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "backpropagation",
      "blackbox",
      "environment",
      "estimate",
      "evaluating",
      "evolution",
      "for",
      "gradients",
      "learning",
      "methods",
      "optimization",
      "parameters"
    ],
    "excerpt": "Black-box optimization methods that estimate policy gradients by perturbing parameters and evaluating returns, without requiring backpropagation through the environment.",
    "url": "pages/glossary.html#term-evolution-strategies-rl"
  },
  {
    "id": "term-exact-match",
    "title": "Exact Match",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "answer",
      "answering",
      "benchmarks",
      "commonly",
      "correct",
      "evaluation",
      "exact",
      "exactly",
      "extraction",
      "ground",
      "information",
      "match"
    ],
    "excerpt": "A strict evaluation metric that scores a prediction as correct only if it exactly matches the ground truth answer after normalization, commonly used in question answering and information extraction benchmarks.",
    "url": "pages/glossary.html#term-exact-match"
  },
  {
    "id": "term-exact-nearest-neighbor",
    "title": "Exact Nearest Neighbor",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "approach",
      "closest",
      "complexity",
      "computing",
      "database",
      "distances",
      "exact",
      "exhaustively",
      "finding",
      "guarantees",
      "index",
      "limits"
    ],
    "excerpt": "A search approach that guarantees finding the true closest vectors to a query by exhaustively computing distances to all vectors in the index, providing perfect recall but with linear time complexity that limits scalability.",
    "url": "pages/glossary.html#term-exact-nearest-neighbor"
  },
  {
    "id": "term-existential-risk-from-ai",
    "title": "Existential Risk from AI",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "advanced",
      "ai",
      "alignment",
      "continued",
      "curtail",
      "ethics",
      "existence",
      "existential",
      "from",
      "humanity",
      "hypothesis",
      "motivating"
    ],
    "excerpt": "The hypothesis that sufficiently advanced AI systems could pose a threat to the continued existence of humanity or permanently curtail its potential, motivating research into alignment and AI safety.",
    "url": "pages/glossary.html#term-existential-risk-from-ai"
  },
  {
    "id": "term-expectation-maximization",
    "title": "Expectation-Maximization",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithm",
      "estimates",
      "expectation",
      "finding",
      "iterative",
      "latent",
      "learning",
      "likelihood",
      "machine",
      "maximization",
      "maximum",
      "models"
    ],
    "excerpt": "An iterative algorithm for finding maximum likelihood estimates in models with latent variables. It alternates between computing expected values of the latent variables (E-step) and maximizing the likelihood with respect to model parameters (M-step).",
    "url": "pages/glossary.html#term-expectation-maximization"
  },
  {
    "id": "term-expected-calibration-error",
    "title": "Expected Calibration Error",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "absolute",
      "accuracy",
      "averaging",
      "bin",
      "binning",
      "calculating",
      "calibration",
      "computed",
      "confidence",
      "difference",
      "error",
      "expected"
    ],
    "excerpt": "A scalar summary of calibration quality computed by binning predictions by confidence, calculating the absolute difference between accuracy and confidence within each bin, and averaging weighted by bin size.",
    "url": "pages/glossary.html#term-expected-calibration-error"
  },
  {
    "id": "term-experience-replay",
    "title": "Experience Replay",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "breaking",
      "buffer",
      "consecutive",
      "correlations",
      "experience",
      "learning",
      "methods",
      "past",
      "reinforcement",
      "replay",
      "samples"
    ],
    "excerpt": "A technique where an agent stores past transitions in a replay buffer and samples from it during training, breaking temporal correlations between consecutive samples.",
    "url": "pages/glossary.html#term-experience-replay"
  },
  {
    "id": "term-expert-parallelism",
    "title": "Expert Parallelism",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "alltoall",
      "architecture",
      "assigned",
      "communication",
      "computing",
      "devices",
      "different",
      "distributed",
      "expert",
      "experts",
      "mixtureofexperts",
      "models"
    ],
    "excerpt": "A distributed computing strategy for mixture-of-experts models where different expert subnetworks are placed on different devices, with all-to-all communication routing tokens to their assigned experts.",
    "url": "pages/glossary.html#term-expert-parallelism"
  },
  {
    "id": "term-expert-prompting",
    "title": "Expert Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "adopt",
      "answer",
      "authoritative",
      "base",
      "domain",
      "engineering",
      "expert",
      "expertise",
      "experts",
      "first",
      "given",
      "identify"
    ],
    "excerpt": "A prompting method that instructs the model to first identify the most qualified expert identity for a given question, adopt that expert's perspective and knowledge base, then provide an authoritative answer informed by that domain expertise.",
    "url": "pages/glossary.html#term-expert-prompting"
  },
  {
    "id": "term-expert-systems",
    "title": "Expert Systems",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1970s",
      "1980s",
      "commercial",
      "domainspecific",
      "dominant",
      "encoded",
      "expert",
      "history",
      "human",
      "ifthen",
      "knowledge",
      "learning"
    ],
    "excerpt": "AI programs popular in the 1970s and 1980s that encoded human expert knowledge as if-then rules to solve domain-specific problems, representing the dominant commercial AI paradigm before the rise of machine learning.",
    "url": "pages/glossary.html#term-expert-systems"
  },
  {
    "id": "term-explainability",
    "title": "Explainability (XAI)",
    "category": "Glossary",
    "subcategory": "Transparency",
    "keywords": [
      "ability",
      "decisions",
      "explain",
      "explainability",
      "make",
      "models",
      "transparency",
      "trust",
      "understand",
      "xai"
    ],
    "excerpt": "The ability to understand and explain how AI models make decisions. Important for trust, debugging, regulatory compliance, and identifying potential biases.",
    "url": "pages/glossary.html#term-explainability"
  },
  {
    "id": "term-exploding-gradient",
    "title": "Exploding Gradient",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "backpropagation",
      "become",
      "causing",
      "deep",
      "diverge",
      "excessively",
      "exploding",
      "exponentially",
      "gradient",
      "gradients",
      "grow",
      "instability"
    ],
    "excerpt": "A training instability where gradients grow exponentially large during backpropagation through deep networks, causing weight updates to become excessively large and training to diverge.",
    "url": "pages/glossary.html#term-exploding-gradient"
  },
  {
    "id": "term-exploration-vs-exploitation",
    "title": "Exploration vs Exploitation",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "better",
      "current",
      "dilemma",
      "discover",
      "exploitation",
      "exploiting",
      "exploration",
      "exploring",
      "fundamental",
      "immediate",
      "knowledge"
    ],
    "excerpt": "The fundamental dilemma in RL between exploring unknown actions to discover potentially better strategies and exploiting current knowledge to maximize immediate reward.",
    "url": "pages/glossary.html#term-exploration-vs-exploitation"
  },
  {
    "id": "term-exponential-distribution",
    "title": "Exponential Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "continuous",
      "distribution",
      "events",
      "exponential",
      "modeling",
      "poisson",
      "probability",
      "process",
      "statistics",
      "time"
    ],
    "excerpt": "A continuous probability distribution modeling the time between events in a Poisson process. It has the memoryless property: the probability of an event in the next interval is independent of elapsed time.",
    "url": "pages/glossary.html#term-exponential-distribution"
  },
  {
    "id": "term-exponential-family",
    "title": "Exponential Family",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "binomial",
      "broad",
      "characterized",
      "class",
      "distributions",
      "exponential",
      "family",
      "form",
      "gamma",
      "including",
      "mathematical",
      "normal"
    ],
    "excerpt": "A broad class of probability distributions characterized by a specific mathematical form, including normal, Poisson, binomial, exponential, and gamma distributions.",
    "url": "pages/glossary.html#term-exponential-family"
  },
  {
    "id": "term-exponential-smoothing",
    "title": "Exponential Smoothing",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "averages",
      "compute",
      "data",
      "decreasing",
      "exponential",
      "exponentially",
      "family",
      "forecasting",
      "methods",
      "observations",
      "past",
      "science"
    ],
    "excerpt": "A family of time series forecasting methods that compute weighted averages of past observations with exponentially decreasing weights.",
    "url": "pages/glossary.html#term-exponential-smoothing"
  },
  {
    "id": "term-extraction",
    "title": "Extraction",
    "category": "Glossary",
    "subcategory": "NLP Task",
    "keywords": [
      "application",
      "extraction",
      "identify",
      "information",
      "nlp",
      "pull",
      "specific",
      "task",
      "text",
      "unstructured"
    ],
    "excerpt": "Using AI to identify and pull specific information from unstructured text. Applications include named entity extraction, key phrase extraction, and structured data extraction.",
    "url": "pages/glossary.html#term-extraction"
  },
  {
    "id": "term-extractive-question-answering",
    "title": "Extractive Question Answering",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "answer",
      "answering",
      "context",
      "contiguous",
      "end",
      "extractive",
      "given",
      "identifies",
      "model",
      "nlp",
      "passage",
      "positions"
    ],
    "excerpt": "A QA task where the model identifies the answer as a contiguous span of text within a given context passage, predicting the start and end positions of the answer.",
    "url": "pages/glossary.html#term-extractive-question-answering"
  },
  {
    "id": "term-extractive-summarization",
    "title": "Extractive Summarization",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "approach",
      "concatenates",
      "document",
      "extractive",
      "form",
      "generating",
      "important",
      "new",
      "nlp",
      "passages",
      "processing",
      "selects"
    ],
    "excerpt": "A summarization approach that selects and concatenates the most important sentences or passages from the source document to form a summary without generating new text.",
    "url": "pages/glossary.html#term-extractive-summarization"
  },
  {
    "id": "term-f-distribution",
    "title": "F-Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "arising",
      "chisquare",
      "continuous",
      "degrees",
      "distribution",
      "distributions",
      "divided",
      "freedom",
      "independent",
      "probability",
      "ratio",
      "statistics"
    ],
    "excerpt": "A continuous probability distribution arising as the ratio of two independent chi-square distributions, each divided by their degrees of freedom.",
    "url": "pages/glossary.html#term-f-distribution"
  },
  {
    "id": "term-f1-score",
    "title": "F1 Score",
    "category": "Glossary",
    "subcategory": "Metrics",
    "keywords": [
      "combining",
      "evaluation",
      "f1",
      "harmonic",
      "mean",
      "metric",
      "metrics",
      "precision",
      "recall",
      "score",
      "single"
    ],
    "excerpt": "A metric combining precision and recall into a single score (their harmonic mean). Useful for evaluating classification models, especially with imbalanced datasets.",
    "url": "pages/glossary.html#term-f1-score"
  },
  {
    "id": "term-face-detection",
    "title": "Face Detection",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "architectures",
      "bounding",
      "computer",
      "detection",
      "domain",
      "face",
      "faces",
      "human",
      "image",
      "locating",
      "object",
      "occlusion"
    ],
    "excerpt": "The task of locating and bounding all human faces in an image regardless of pose, scale, or occlusion, using specialized object detection architectures optimized for the face domain.",
    "url": "pages/glossary.html#term-face-detection"
  },
  {
    "id": "term-face-recognition",
    "title": "Face Recognition",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "against",
      "biometric",
      "compare",
      "computer",
      "database",
      "deep",
      "determines",
      "embeddings",
      "extract",
      "face",
      "facial",
      "features"
    ],
    "excerpt": "A biometric identification task that determines the identity of a person from their facial features, using deep learning models to extract face embeddings and compare them against a database of known identities.",
    "url": "pages/glossary.html#term-face-recognition"
  },
  {
    "id": "term-face-verification",
    "title": "Face Verification",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "acceptreject",
      "belong",
      "comparing",
      "computer",
      "decision",
      "determines",
      "distance",
      "embeddings",
      "face",
      "image",
      "images",
      "matching"
    ],
    "excerpt": "A one-to-one matching task that determines whether two face images belong to the same person by comparing their face embeddings, typically using a distance threshold for the accept/reject decision.",
    "url": "pages/glossary.html#term-face-verification"
  },
  {
    "id": "term-facial-landmark-detection",
    "title": "Facial Landmark Detection",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "alignment",
      "applications",
      "augmented",
      "computer",
      "corners",
      "detection",
      "expression",
      "eyes",
      "face",
      "facial",
      "image",
      "jawline"
    ],
    "excerpt": "The task of predicting the precise locations of key facial points (eyes, nose, mouth corners, jawline) in an image, used for face alignment, expression recognition, and augmented reality applications.",
    "url": "pages/glossary.html#term-facial-landmark-detection"
  },
  {
    "id": "term-facial-recognition-ethics",
    "title": "Facial Recognition Ethics",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "accuracy",
      "ai",
      "aipowered",
      "around",
      "bans",
      "bias",
      "civil",
      "concerns",
      "consent",
      "debate",
      "enacted",
      "encompassing"
    ],
    "excerpt": "The ethical debate around AI-powered facial recognition technology, encompassing concerns about surveillance, racial bias in accuracy, consent, civil liberties, and bans enacted by various municipalities.",
    "url": "pages/glossary.html#term-facial-recognition-ethics"
  },
  {
    "id": "term-factorized-embedding",
    "title": "Factorized Embedding",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "count",
      "decomposes",
      "dimension",
      "embedding",
      "factorized",
      "hidden",
      "matrices",
      "matrix",
      "networks",
      "neural",
      "parameter"
    ],
    "excerpt": "A technique that decomposes the embedding matrix into two smaller matrices, separating the vocabulary embedding dimension from the hidden dimension to reduce parameter count.",
    "url": "pages/glossary.html#term-factorized-embedding"
  },
  {
    "id": "term-factual-consistency",
    "title": "Factual Consistency",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "assessed",
      "claims",
      "consistency",
      "consistent",
      "documents",
      "entailed",
      "evaluation",
      "factchecking",
      "facts",
      "factual",
      "generated",
      "inference"
    ],
    "excerpt": "An evaluation metric that measures whether claims in generated text are logically entailed by and consistent with source documents or verifiable facts, often assessed using natural language inference models or fact-checking pipelines.",
    "url": "pages/glossary.html#term-factual-consistency"
  },
  {
    "id": "term-factual-grounding",
    "title": "Factual Grounding",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "alone",
      "anchoring",
      "bases",
      "documents",
      "ensuring",
      "evidence",
      "factual",
      "generative",
      "grounding",
      "knowledge",
      "language"
    ],
    "excerpt": "The process of anchoring a language model's responses to verified source documents or knowledge bases, ensuring outputs are supported by retrievable evidence rather than parametric memory alone.",
    "url": "pages/glossary.html#term-factual-grounding"
  },
  {
    "id": "term-factuality",
    "title": "Factuality",
    "category": "Glossary",
    "subcategory": "Quality",
    "keywords": [
      "accurate",
      "challenge",
      "degree",
      "factuality",
      "outputs",
      "quality",
      "true"
    ],
    "excerpt": "The degree to which AI outputs are accurate and true. A major challenge for LLMs, which can generate plausible-sounding but incorrect information (hallucinations).",
    "url": "pages/glossary.html#term-factuality"
  },
  {
    "id": "term-fairness",
    "title": "Fairness (AI)",
    "category": "Glossary",
    "subcategory": "Ethics",
    "keywords": [
      "across",
      "ai",
      "biased",
      "demographic",
      "different",
      "discriminate",
      "ethics",
      "fairness",
      "groups",
      "outcomes",
      "principle",
      "produce"
    ],
    "excerpt": "The principle that AI systems should not discriminate or produce biased outcomes across different demographic groups. Multiple mathematical definitions exist, sometimes in tension with each other.",
    "url": "pages/glossary.html#term-fairness"
  },
  {
    "id": "term-faiss",
    "title": "FAISS",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "acceleration",
      "algorithms",
      "billionscale",
      "clustering",
      "database",
      "datasets",
      "developed",
      "facebook",
      "faiss",
      "gpu",
      "highly",
      "implementations"
    ],
    "excerpt": "Facebook AI Similarity Search, an open-source library developed by Meta that provides highly optimized implementations of vector similarity search and clustering algorithms, supporting billion-scal...",
    "url": "pages/glossary.html#term-faiss"
  },
  {
    "id": "term-faithful-chain-of-thought",
    "title": "Faithful Chain-of-Thought",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "actual",
      "chain",
      "code",
      "computation",
      "decomposes",
      "engineering",
      "ensuring",
      "executable",
      "faithful",
      "framework",
      "grounding",
      "interleaved"
    ],
    "excerpt": "A reasoning framework that decomposes questions into interleaved natural language reasoning and symbolic operations, ensuring that the reasoning chain is faithful to the actual computation by grounding intermediate steps in executable code.",
    "url": "pages/glossary.html#term-faithful-chain-of-thought"
  },
  {
    "id": "term-faithfulness",
    "title": "Faithfulness",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "consistent",
      "contains",
      "content",
      "context",
      "dimension",
      "evaluation",
      "fabrication",
      "faithfulness",
      "generated",
      "hallucination",
      "indicating",
      "information"
    ],
    "excerpt": "An evaluation dimension that measures whether generated text contains only information that is supported by and consistent with the provided source context, with unfaithful content indicating hallucination or fabrication.",
    "url": "pages/glossary.html#term-faithfulness"
  },
  {
    "id": "term-false-discovery-rate",
    "title": "False Discovery Rate",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "among",
      "discovery",
      "expected",
      "false",
      "hypotheses",
      "inference",
      "multiple",
      "null",
      "positives",
      "proportion",
      "rate",
      "rejected"
    ],
    "excerpt": "The expected proportion of false positives among all rejected null hypotheses in multiple testing. The Benjamini-Hochberg procedure controls FDR and is less conservative than the Bonferroni correction.",
    "url": "pages/glossary.html#term-false-discovery-rate"
  },
  {
    "id": "term-false-positive",
    "title": "False Positive / False Negative",
    "category": "Glossary",
    "subcategory": "Metrics",
    "keywords": [
      "actual",
      "class",
      "classification",
      "errors",
      "evaluation",
      "false",
      "incorrectly",
      "metrics",
      "miss",
      "negative",
      "negatives",
      "positive"
    ],
    "excerpt": "Classification errors: false positives incorrectly predict the positive class; false negatives miss actual positives. The trade-off between them depends on application costs.",
    "url": "pages/glossary.html#term-false-positive"
  },
  {
    "id": "term-faster-rcnn",
    "title": "Faster R-CNN",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "bounding",
      "boxes",
      "candidate",
      "classification",
      "cnn",
      "computer",
      "detection",
      "faster",
      "final",
      "followed",
      "framework",
      "generate"
    ],
    "excerpt": "A two-stage object detection framework that uses a Region Proposal Network (RPN) to generate candidate bounding boxes, followed by a classification and regression head for final detection.",
    "url": "pages/glossary.html#term-faster-rcnn"
  },
  {
    "id": "term-fasttext",
    "title": "FastText",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "bag",
      "character",
      "embeddings",
      "enabling",
      "extends",
      "fasttext",
      "languages",
      "meaningful",
      "model",
      "morphologically",
      "ngrams",
      "nlp"
    ],
    "excerpt": "A word representation model that extends Word2Vec by representing each word as a bag of character n-grams, enabling meaningful embeddings for out-of-vocabulary words and morphologically rich languages.",
    "url": "pages/glossary.html#term-fasttext"
  },
  {
    "id": "term-fcos",
    "title": "FCOS",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "anchorfree",
      "bounding",
      "box",
      "boxes",
      "computer",
      "convolutional",
      "detection",
      "detector",
      "directly",
      "distances",
      "edges",
      "fcos"
    ],
    "excerpt": "Fully Convolutional One-Stage Object Detection, an anchor-free detector that directly predicts bounding boxes from every foreground pixel using per-pixel regression of distances to box edges.",
    "url": "pages/glossary.html#term-fcos"
  },
  {
    "id": "term-feature",
    "title": "Feature",
    "category": "Glossary",
    "subcategory": "Data",
    "keywords": [
      "characteristic",
      "data",
      "feature",
      "fundamentals",
      "individual",
      "input",
      "measurable",
      "ml",
      "model",
      "property"
    ],
    "excerpt": "An individual measurable property or characteristic of data used as input to a model. Good feature engineering can significantly improve model performance.",
    "url": "pages/glossary.html#term-feature"
  },
  {
    "id": "term-feature-extraction",
    "title": "Feature Extraction",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "autoencoders",
      "computations",
      "constructing",
      "data",
      "dimensionality",
      "domainspecific",
      "engineering",
      "extraction",
      "feature",
      "features",
      "informative",
      "learning"
    ],
    "excerpt": "The process of constructing new features from raw data through transformations such as PCA, autoencoders, or domain-specific computations, reducing dimensionality while retaining the most informative signal.",
    "url": "pages/glossary.html#term-feature-extraction"
  },
  {
    "id": "term-feature-hashing",
    "title": "Feature Hashing",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "avoiding",
      "dictionary",
      "dimensionality",
      "engineering",
      "feature",
      "function",
      "hash",
      "hashing",
      "highdimensional",
      "indices",
      "learning",
      "lowerdimensional"
    ],
    "excerpt": "A dimensionality reduction technique that maps high-dimensional feature vectors to a lower-dimensional space using a hash function, avoiding the need to maintain a dictionary of feature indices.",
    "url": "pages/glossary.html#term-feature-hashing"
  },
  {
    "id": "term-feature-importance",
    "title": "Feature Importance",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "contributes",
      "engineering",
      "feature",
      "importance",
      "input",
      "learning",
      "machine",
      "measure",
      "models",
      "much",
      "predictions"
    ],
    "excerpt": "A measure of how much each input feature contributes to a model's predictions. Common methods include tree-based impurity importance, permutation importance, and SHAP values.",
    "url": "pages/glossary.html#term-feature-importance"
  },
  {
    "id": "term-feature-map",
    "title": "Feature Map",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "activation",
      "applying",
      "captures",
      "channel",
      "computer",
      "convolutional",
      "different",
      "feature",
      "filter",
      "image",
      "input",
      "layer"
    ],
    "excerpt": "The output of a convolutional layer representing the activation pattern produced by applying a specific filter to the input, where each channel captures a different learned visual pattern.",
    "url": "pages/glossary.html#term-feature-map"
  },
  {
    "id": "term-feature-matching",
    "title": "Feature Matching",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "comparing",
      "computer",
      "corresponding",
      "descriptors",
      "detecting",
      "feature",
      "features",
      "finding",
      "image",
      "images",
      "keypoints",
      "local"
    ],
    "excerpt": "The process of finding corresponding points between two images by detecting local features (keypoints) and comparing their descriptors, used in 3D reconstruction, SLAM, and image retrieval.",
    "url": "pages/glossary.html#term-feature-matching"
  },
  {
    "id": "term-feature-pyramid-network",
    "title": "Feature Pyramid Network",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "builds",
      "connections",
      "extraction",
      "feature",
      "features",
      "input",
      "lateral",
      "multiscale",
      "network",
      "networks",
      "neural"
    ],
    "excerpt": "A multi-scale feature extraction architecture that builds a top-down pathway with lateral connections to produce semantically rich features at all scales from a single-scale input.",
    "url": "pages/glossary.html#term-feature-pyramid-network"
  },
  {
    "id": "term-feature-scaling",
    "title": "Feature Scaling",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithms",
      "common",
      "distancebased",
      "dominating",
      "engineering",
      "feature",
      "features",
      "gradientbased",
      "larger",
      "learning",
      "machine",
      "mean"
    ],
    "excerpt": "The process of transforming numerical features to a common scale, such as standardization (zero mean, unit variance) or min-max scaling, to prevent features with larger ranges from dominating distance-based or gradient-based algorithms.",
    "url": "pages/glossary.html#term-feature-scaling"
  },
  {
    "id": "term-feature-selection",
    "title": "Feature Selection",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "construction",
      "dimensionality",
      "engineering",
      "feature",
      "features",
      "identifying",
      "improving",
      "interpretability",
      "learning",
      "machine",
      "mitigating",
      "model"
    ],
    "excerpt": "The process of identifying and selecting a subset of relevant features for model construction, reducing dimensionality, mitigating overfitting, and improving interpretability without transforming the original features.",
    "url": "pages/glossary.html#term-feature-selection"
  },
  {
    "id": "term-federated-learning",
    "title": "Federated Learning",
    "category": "Glossary",
    "subcategory": "Privacy",
    "keywords": [
      "across",
      "ai",
      "approach",
      "collaborative",
      "data",
      "decentralized",
      "devices",
      "enabling",
      "ethics",
      "exchanging",
      "federated",
      "holding"
    ],
    "excerpt": "A machine learning approach where models are trained across multiple decentralized devices or servers holding local data samples, without exchanging raw data, thereby preserving privacy while enabling collaborative learning.",
    "url": "pages/glossary.html#term-federated-learning"
  },
  {
    "id": "term-feedforward",
    "title": "Feedforward Neural Network",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "cycles",
      "direction",
      "feedforward",
      "flows",
      "information",
      "input",
      "network",
      "networks",
      "neural",
      "one",
      "output"
    ],
    "excerpt": "A neural network where information flows in one direction from input to output, without cycles. The simplest type of neural network architecture.",
    "url": "pages/glossary.html#term-feedforward"
  },
  {
    "id": "term-fei-fei-li",
    "title": "Fei-Fei Li",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "chineseamerican",
      "competition",
      "computer",
      "creation",
      "dataset",
      "deep",
      "fei",
      "history",
      "imagenet",
      "instrumental",
      "learning",
      "led"
    ],
    "excerpt": "Chinese-American computer scientist who led the creation of the ImageNet dataset and competition, which was instrumental in sparking the deep learning revolution.",
    "url": "pages/glossary.html#term-fei-fei-li"
  },
  {
    "id": "term-feudal-network",
    "title": "Feudal Network",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "abstract",
      "actions",
      "architecture",
      "feudal",
      "goals",
      "hierarchical",
      "learning",
      "manager",
      "module",
      "multiagent",
      "network",
      "primitive"
    ],
    "excerpt": "A hierarchical RL architecture where a manager module sets abstract goals for a worker module that selects primitive actions.",
    "url": "pages/glossary.html#term-feudal-network"
  },
  {
    "id": "term-few-shot",
    "title": "Few-Shot Learning",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "examples",
      "few",
      "format",
      "help",
      "learning",
      "pattern",
      "prompt",
      "prompting",
      "provide",
      "shot",
      "technique",
      "understand"
    ],
    "excerpt": "A technique where you provide a few examples in your prompt to help AI understand the pattern or format you want. More effective than describing alone for complex outputs.",
    "url": "pages/glossary.html#term-few-shot"
  },
  {
    "id": "term-few-shot-object-detection",
    "title": "Few-Shot Object Detection",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "annotated",
      "categories",
      "computer",
      "data",
      "detect",
      "detection",
      "examples",
      "few",
      "generalize",
      "handful",
      "labeled",
      "learn"
    ],
    "excerpt": "Object detection methods that can learn to detect new categories from only a handful of annotated examples, using meta-learning or transfer learning to generalize from limited labeled data.",
    "url": "pages/glossary.html#term-few-shot-object-detection"
  },
  {
    "id": "term-fill-in-the-middle",
    "title": "Fill-in-the-Middle",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "capabilities",
      "code",
      "completion",
      "document",
      "editing",
      "enabling",
      "fill",
      "fills",
      "gap",
      "generate",
      "generative"
    ],
    "excerpt": "A training objective where the model learns to generate text that fills a gap between a given prefix and suffix, enabling code completion, text infilling, and document editing capabilities.",
    "url": "pages/glossary.html#term-fill-in-the-middle"
  },
  {
    "id": "term-fine-tuning",
    "title": "Fine-Tuning",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "additional",
      "customization",
      "data",
      "domains",
      "fine",
      "improve",
      "model",
      "performance",
      "preexisting",
      "process",
      "specialized",
      "specific"
    ],
    "excerpt": "The process of training a pre-existing AI model on additional, specialized data to improve its performance for specific tasks or domains. More efficient than training from scratch.",
    "url": "pages/glossary.html#term-fine-tuning"
  },
  {
    "id": "term-first-ai-winter",
    "title": "First AI Winter",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1974",
      "1980",
      "ai",
      "ambitious",
      "approximately",
      "criticism",
      "declined",
      "deliver",
      "early",
      "failure",
      "first",
      "following"
    ],
    "excerpt": "The period from approximately 1974 to 1980 when AI research funding and interest declined sharply following the Lighthill Report's criticism and the failure of early AI to deliver on ambitious promises.",
    "url": "pages/glossary.html#term-first-ai-winter"
  },
  {
    "id": "term-fisher-information",
    "title": "Fisher Information",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "amount",
      "carries",
      "fisher",
      "inference",
      "information",
      "measure",
      "observable",
      "parameter",
      "random",
      "statistics",
      "unknown",
      "variable"
    ],
    "excerpt": "A measure of the amount of information that an observable random variable carries about an unknown parameter. It quantifies how sensitive the likelihood function is to changes in the parameter value.",
    "url": "pages/glossary.html#term-fisher-information"
  },
  {
    "id": "term-fitted-q-iteration",
    "title": "Fitted Q-Iteration",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithm",
      "approximator",
      "backup",
      "batch",
      "bellman",
      "computed",
      "dataset",
      "fits",
      "fitted",
      "fixed",
      "iteration",
      "iteratively"
    ],
    "excerpt": "A batch RL algorithm that iteratively fits a Q-function approximator to Bellman backup targets computed from a fixed dataset.",
    "url": "pages/glossary.html#term-fitted-q-iteration"
  },
  {
    "id": "term-flan",
    "title": "FLAN (Fine-tuned Language Net)",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "approach",
      "described",
      "fine",
      "finetuning",
      "flan",
      "google",
      "googles",
      "instruction",
      "instructions",
      "language",
      "many",
      "models"
    ],
    "excerpt": "Google's approach to instruction tuning, fine-tuning models on many tasks described with natural language instructions. FLAN models showed strong zero-shot performance.",
    "url": "pages/glossary.html#term-flan"
  },
  {
    "id": "term-flash-attention",
    "title": "Flash Attention",
    "category": "Glossary",
    "subcategory": "Optimization",
    "keywords": [
      "algorithm",
      "architecture",
      "attention",
      "computations",
      "flash",
      "improves",
      "memory",
      "optimization",
      "optimized",
      "reduces",
      "speed",
      "tiling"
    ],
    "excerpt": "An optimized attention algorithm that reduces memory usage and improves speed by tiling computations. Enables longer context windows and faster inference for transformers.",
    "url": "pages/glossary.html#term-flash-attention"
  },
  {
    "id": "term-flashattention-2",
    "title": "FlashAttention-2",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "attention",
      "avoiding",
      "computation",
      "data",
      "fast",
      "flashattention",
      "full",
      "hbm",
      "implementation",
      "inference",
      "infrastructure",
      "keep"
    ],
    "excerpt": "An optimized attention implementation that reduces memory I/O by tiling the computation to keep data in fast SRAM, avoiding materialization of the full attention matrix in HBM.",
    "url": "pages/glossary.html#term-flashattention-2"
  },
  {
    "id": "term-flat-index",
    "title": "Flat Index",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "bruteforce",
      "complexity",
      "compression",
      "cost",
      "database",
      "exact",
      "exhaustive",
      "flat",
      "guaranteeing",
      "index",
      "linear",
      "nearest"
    ],
    "excerpt": "A brute-force vector index that stores all vectors without compression or partitioning and performs exhaustive linear search, guaranteeing exact nearest neighbor results at the cost of O(n) search complexity.",
    "url": "pages/glossary.html#term-flat-index"
  },
  {
    "id": "term-fleiss-kappa",
    "title": "Fleiss&#x27; Kappa",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "accounting",
      "across",
      "agreement",
      "among",
      "annotator",
      "annotators",
      "assess",
      "categorical",
      "chance",
      "cohens",
      "evaluation",
      "extends"
    ],
    "excerpt": "A statistical measure that extends Cohen's Kappa to assess inter-rater agreement among three or more annotators making categorical judgments, accounting for chance agreement across the full annotator pool.",
    "url": "pages/glossary.html#term-fleiss-kappa"
  },
  {
    "id": "term-flipped-interaction",
    "title": "Flipped Interaction",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "advice",
      "ask",
      "flipped",
      "framework",
      "interaction",
      "interactive",
      "method",
      "providing",
      "questions",
      "you"
    ],
    "excerpt": "A method where you ask AI to ask you questions before providing advice. Leads to more personalized and relevant responses for complex situations.",
    "url": "pages/glossary.html#term-flipped-interaction"
  },
  {
    "id": "term-flops",
    "title": "FLOPS",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "computational",
      "floatingpoint",
      "flops",
      "gpu",
      "hardware",
      "measure",
      "operations",
      "per",
      "second",
      "standard",
      "throughput"
    ],
    "excerpt": "Floating-Point Operations Per Second, the standard measure of computational throughput for AI hardware.",
    "url": "pages/glossary.html#term-flops"
  },
  {
    "id": "term-flow-matching",
    "title": "Flow Matching",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "continuous",
      "data",
      "diffusion",
      "distribution",
      "fields",
      "flow",
      "flows",
      "framework",
      "generative",
      "image",
      "learns"
    ],
    "excerpt": "A generative modeling framework that learns continuous normalizing flows by regressing velocity fields that transport samples from a noise distribution to the data distribution, offering simpler training than diffusion.",
    "url": "pages/glossary.html#term-flow-matching"
  },
  {
    "id": "term-fluency-score",
    "title": "Fluency Score",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "assessed",
      "closely",
      "correctness",
      "evaluates",
      "evaluation",
      "fluency",
      "generated",
      "grammatical",
      "human",
      "humanwritten",
      "judgment",
      "metric"
    ],
    "excerpt": "A metric that evaluates the grammatical correctness, naturalness, and readability of generated text, often assessed through human judgment or proxy models that rate how closely the output resembles natural human-written prose.",
    "url": "pages/glossary.html#term-fluency-score"
  },
  {
    "id": "term-flux",
    "title": "Flux",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "achieving",
      "ai",
      "architecture",
      "diffusion",
      "dit",
      "family",
      "flow",
      "flux",
      "following",
      "generation",
      "generative",
      "image"
    ],
    "excerpt": "A family of image generation models that use a rectified flow transformer architecture (DiT) for diffusion, achieving state-of-the-art image quality with improved text rendering and prompt following.",
    "url": "pages/glossary.html#term-flux"
  },
  {
    "id": "term-focal-loss",
    "title": "Focal Loss",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "adding",
      "contribution",
      "crossentropy",
      "downweights",
      "easy",
      "examples",
      "factor",
      "focal",
      "focuses",
      "hard",
      "learning",
      "loss"
    ],
    "excerpt": "A modified cross-entropy loss that down-weights the contribution of easy examples and focuses training on hard, misclassified examples by adding a modulating factor.",
    "url": "pages/glossary.html#term-focal-loss"
  },
  {
    "id": "term-fomo",
    "title": "FOMO (AI Context)",
    "category": "Glossary",
    "subcategory": "Culture",
    "keywords": [
      "advancements",
      "ai",
      "context",
      "culture",
      "fear",
      "fomo",
      "learning",
      "missing"
    ],
    "excerpt": "Fear Of Missing Out on AI advancements. The rapid pace of AI development creates pressure to constantly learn new tools and techniques. Balance enthusiasm with focused skill-building.",
    "url": "pages/glossary.html#term-fomo"
  },
  {
    "id": "term-format-instruction",
    "title": "Format Instructions",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "explicit",
      "format",
      "guidance",
      "instructions",
      "prompting",
      "prompts",
      "response",
      "structure",
      "technique"
    ],
    "excerpt": "Explicit guidance in prompts about how AI should structure its response. Examples include requesting JSON, markdown tables, bullet points, or specific sections.",
    "url": "pages/glossary.html#term-format-instruction"
  },
  {
    "id": "term-foundation-model",
    "title": "Foundation Model",
    "category": "Glossary",
    "subcategory": "Model Type",
    "keywords": [
      "adapted",
      "architecture",
      "broad",
      "data",
      "downstream",
      "foundation",
      "large",
      "many",
      "model",
      "tasks",
      "trained",
      "type"
    ],
    "excerpt": "A large AI model trained on broad data that can be adapted to many downstream tasks. Examples include GPT-4, Claude, and Llama. The base layer for most modern AI applications.",
    "url": "pages/glossary.html#term-foundation-model"
  },
  {
    "id": "term-fp16",
    "title": "FP16 (Half Precision)",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "16bit",
      "bits",
      "exponent",
      "floatingpoint",
      "footprint",
      "format",
      "fp16",
      "fp32",
      "half",
      "hardware",
      "mantissa",
      "memory"
    ],
    "excerpt": "A 16-bit floating-point format with 5 exponent bits and 10 mantissa bits, offering half the memory footprint of FP32.",
    "url": "pages/glossary.html#term-fp16"
  },
  {
    "id": "term-fp32",
    "title": "FP32 (Single Precision)",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "32bit",
      "bits",
      "exponent",
      "floatingpoint",
      "format",
      "fp32",
      "hardware",
      "high",
      "mantissa",
      "model",
      "numerical",
      "optimization"
    ],
    "excerpt": "The standard 32-bit floating-point format with 8 exponent bits and 23 mantissa bits, providing high numerical precision for model training.",
    "url": "pages/glossary.html#term-fp32"
  },
  {
    "id": "term-fp4",
    "title": "FP4 Quantization",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "4bit",
      "blackwell",
      "floatingpoint",
      "format",
      "fp4",
      "hardware",
      "like",
      "model",
      "network",
      "neural",
      "nextgeneration",
      "nvidia"
    ],
    "excerpt": "An ultra-low precision 4-bit floating-point format for neural network weights, supported by next-generation AI hardware like NVIDIA Blackwell.",
    "url": "pages/glossary.html#term-fp4"
  },
  {
    "id": "term-fp8",
    "title": "FP8 Precision",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "8bit",
      "available",
      "bits",
      "e4m3",
      "e5m2",
      "exponent",
      "floatingpoint",
      "format",
      "forward",
      "fp8",
      "gradients",
      "hardware"
    ],
    "excerpt": "An 8-bit floating-point format introduced for AI training and inference, available in two variants: E4M3 (4 exponent, 3 mantissa bits) for forward passes and E5M2 (5 exponent, 2 mantissa bits) for gradients.",
    "url": "pages/glossary.html#term-fp8"
  },
  {
    "id": "term-fpga-ai",
    "title": "FPGA for AI",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "ai",
      "architectures",
      "arrays",
      "customizable",
      "fieldprogrammable",
      "for",
      "fpga",
      "gate",
      "hardware",
      "inference",
      "infrastructure",
      "logic"
    ],
    "excerpt": "Field-Programmable Gate Arrays reconfigured for AI workloads, offering customizable hardware logic that can be tailored to specific neural network architectures.",
    "url": "pages/glossary.html#term-fpga-ai"
  },
  {
    "id": "term-frame-problem",
    "title": "Frame Problem",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1969",
      "actions",
      "challenge",
      "change",
      "concerning",
      "effects",
      "everything",
      "explicitly",
      "frame",
      "fundamental",
      "hayes",
      "history"
    ],
    "excerpt": "A fundamental challenge in AI identified by McCarthy and Hayes in 1969 concerning how to represent the effects of actions without explicitly specifying everything that does not change, a key issue in knowledge representation.",
    "url": "pages/glossary.html#term-frame-problem"
  },
  {
    "id": "term-frame-stacking",
    "title": "Frame Stacking",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "concatenates",
      "concepts",
      "consecutive",
      "context",
      "core",
      "enabling",
      "frame",
      "frames",
      "input",
      "learning",
      "motion"
    ],
    "excerpt": "A technique used in visual RL that concatenates multiple consecutive observation frames as input to the agent, providing temporal context and enabling the perception of motion and velocity from raw pixels.",
    "url": "pages/glossary.html#term-frame-stacking"
  },
  {
    "id": "term-framenet",
    "title": "FrameNet",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "associated",
      "conceptual",
      "database",
      "describes",
      "frame",
      "framenet",
      "frames",
      "lexical",
      "linguistics",
      "meanings",
      "nlp",
      "participants"
    ],
    "excerpt": "A lexical database that describes word meanings in terms of semantic frames, specifying the participants, props, and other conceptual roles associated with each frame.",
    "url": "pages/glossary.html#term-framenet"
  },
  {
    "id": "term-frames-minsky",
    "title": "Frames (Minsky)",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1974",
      "bases",
      "data",
      "expected",
      "frames",
      "history",
      "influencing",
      "information",
      "knowledge",
      "marvin",
      "milestones",
      "minsky"
    ],
    "excerpt": "A knowledge representation scheme proposed by Marvin Minsky in 1974 where stereotyped situations are represented as data structures with slots for expected information, influencing object-oriented programming and AI knowledge bases.",
    "url": "pages/glossary.html#term-frames-minsky"
  },
  {
    "id": "term-frank-rosenblatt",
    "title": "Frank Rosenblatt",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19281971",
      "1957",
      "american",
      "approach",
      "capable",
      "connectionist",
      "data",
      "earliest",
      "frank",
      "history",
      "invented",
      "learning"
    ],
    "excerpt": "American psychologist (1928-1971) who invented the perceptron in 1957, one of the earliest neural network models capable of learning from data, pioneering the connectionist approach to AI.",
    "url": "pages/glossary.html#term-frank-rosenblatt"
  },
  {
    "id": "term-frequency-penalty",
    "title": "Frequency Penalty",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "appeared",
      "decoding",
      "diversity",
      "encouraging",
      "far",
      "frequency",
      "generated",
      "generative",
      "lexical",
      "often",
      "output"
    ],
    "excerpt": "A parameter that penalizes tokens proportionally to how often they have appeared in the output so far, encouraging lexical diversity in generated text.",
    "url": "pages/glossary.html#term-frequency-penalty"
  },
  {
    "id": "term-frontier-ai",
    "title": "Frontier AI",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "capability",
      "capable",
      "cutting",
      "edge",
      "frontier",
      "governance",
      "less",
      "models",
      "novel",
      "pose",
      "present"
    ],
    "excerpt": "The most capable AI models at the cutting edge of capability, which may pose novel safety risks not present in less capable systems.",
    "url": "pages/glossary.html#term-frontier-ai"
  },
  {
    "id": "term-frontier-model",
    "title": "Frontier Model",
    "category": "Glossary",
    "subcategory": "Policy",
    "keywords": [
      "boundaries",
      "capable",
      "frontier",
      "given",
      "model",
      "models",
      "policy",
      "possible",
      "pushing",
      "research",
      "time",
      "whats"
    ],
    "excerpt": "The most capable AI models at any given time, pushing the boundaries of what's possible. Term often used in AI policy discussions about governance of the most powerful systems.",
    "url": "pages/glossary.html#term-frontier-model"
  },
  {
    "id": "term-frontier-model-forum",
    "title": "Frontier Model Forum",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "2023",
      "ai",
      "best",
      "body",
      "companies",
      "deployment",
      "development",
      "engagement",
      "established",
      "focusing",
      "forum",
      "frontier"
    ],
    "excerpt": "An industry body established in 2023 by major AI companies to promote responsible development and deployment of frontier AI models, focusing on safety research, best practices, and public engagement.",
    "url": "pages/glossary.html#term-frontier-model-forum"
  },
  {
    "id": "term-fsdp",
    "title": "FSDP",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "across",
      "ai",
      "data",
      "enabling",
      "fsdp",
      "fully",
      "generative",
      "gpus",
      "gradients",
      "larger",
      "llm",
      "memory"
    ],
    "excerpt": "Fully Sharded Data Parallel, a PyTorch training strategy that shards model parameters, gradients, and optimizer states across all GPUs, enabling training of models larger than single-GPU memory.",
    "url": "pages/glossary.html#term-fsdp"
  },
  {
    "id": "term-full-text-search",
    "title": "Full-Text Search",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "document",
      "enable",
      "examines",
      "fast",
      "find",
      "flexible",
      "full",
      "indexes",
      "inverted",
      "matches",
      "matching",
      "query"
    ],
    "excerpt": "A search technique that examines all words in every stored document to find matches for a query, typically using inverted indexes with tokenization, stemming, and stop word removal to enable fast and flexible text matching.",
    "url": "pages/glossary.html#term-full-text-search"
  },
  {
    "id": "term-function-calling",
    "title": "Function Calling",
    "category": "Glossary",
    "subcategory": "Capability",
    "keywords": [
      "agents",
      "allows",
      "apis",
      "call",
      "calling",
      "capability",
      "external",
      "function",
      "functions",
      "generate",
      "llm",
      "models"
    ],
    "excerpt": "An LLM capability that allows models to generate structured output to call external functions or APIs. Enables AI agents to take actions like searching, calculating, or accessing databases.",
    "url": "pages/glossary.html#term-function-calling"
  },
  {
    "id": "term-functional-correctness",
    "title": "Functional Correctness",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "assesses",
      "benchmarks",
      "code",
      "correct",
      "correctness",
      "criterion",
      "evaluation",
      "executionbased",
      "functional",
      "generated",
      "generation",
      "inputs"
    ],
    "excerpt": "An evaluation criterion for code generation that assesses whether generated code produces correct outputs for all test inputs, measured through execution-based testing rather than syntactic similarity to reference solutions.",
    "url": "pages/glossary.html#term-functional-correctness"
  },
  {
    "id": "term-fusion-retrieval",
    "title": "Fusion Retrieval",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "achieve",
      "architecture",
      "combination",
      "combines",
      "complementary",
      "diverse",
      "fusion",
      "higher",
      "learned",
      "leveraging",
      "method",
      "methods"
    ],
    "excerpt": "A retrieval paradigm that combines results from multiple diverse retrieval methods or models through score fusion, rank fusion, or learned combination, leveraging complementary signals to achieve higher recall and precision than any single method.",
    "url": "pages/glossary.html#term-fusion-retrieval"
  },
  {
    "id": "term-fuzzy-logic",
    "title": "Fuzzy Logic",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1965",
      "binary",
      "degrees",
      "enabling",
      "form",
      "fuzzy",
      "handles",
      "history",
      "imprecise",
      "information",
      "introduced",
      "logic"
    ],
    "excerpt": "A form of many-valued logic introduced by Lotfi Zadeh in 1965 that handles degrees of truth rather than binary true/false values, enabling AI systems to reason with imprecise, vague, or uncertain information.",
    "url": "pages/glossary.html#term-fuzzy-logic"
  },
  {
    "id": "term-fuzzy-matching",
    "title": "Fuzzy Matching",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "approximate",
      "exact",
      "finding",
      "fuzzy",
      "matches",
      "matching",
      "nlp",
      "rather",
      "technique",
      "text"
    ],
    "excerpt": "Finding approximate rather than exact matches in text. Useful for handling typos, variations, and similar-meaning terms in search and data processing applications.",
    "url": "pages/glossary.html#term-fuzzy-matching"
  },
  {
    "id": "term-g-eval",
    "title": "G-Eval",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "achieving",
      "chainofthought",
      "coherence",
      "correlation",
      "dimensions",
      "eval",
      "evaluate",
      "evaluation",
      "fluency",
      "framework",
      "generation",
      "having"
    ],
    "excerpt": "A framework that uses large language models with chain-of-thought prompting to evaluate natural language generation quality, achieving high correlation with human judgments by having the LLM score ...",
    "url": "pages/glossary.html#term-g-eval"
  },
  {
    "id": "term-gamma-distribution",
    "title": "Gamma Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "continuous",
      "distribution",
      "distributions",
      "exponential",
      "family",
      "gamma",
      "generalizes",
      "probability",
      "statistics",
      "twoparameter"
    ],
    "excerpt": "A two-parameter family of continuous probability distributions that generalizes the exponential distribution.",
    "url": "pages/glossary.html#term-gamma-distribution"
  },
  {
    "id": "term-gan",
    "title": "GAN (Generative Adversarial Network)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "adversarial",
      "architecture",
      "competing",
      "content",
      "creating",
      "discriminator",
      "evaluating",
      "gan",
      "generative",
      "generator",
      "network",
      "networks"
    ],
    "excerpt": "A neural network architecture with two competing networks: a generator creating content and a discriminator evaluating it. Pioneered realistic image generation before diffusion models.",
    "url": "pages/glossary.html#term-gan"
  },
  {
    "id": "term-gan-discriminator",
    "title": "GAN Discriminator",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "adversarial",
      "architecture",
      "competition",
      "component",
      "data",
      "discriminator",
      "distinguish",
      "gan",
      "generated",
      "generative",
      "generator",
      "learns"
    ],
    "excerpt": "The component of a generative adversarial network that learns to distinguish real data from generated samples, providing training signal to the generator through adversarial competition.",
    "url": "pages/glossary.html#term-gan-discriminator"
  },
  {
    "id": "term-gan-generator",
    "title": "GAN Generator",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "adversarial",
      "architecture",
      "classifying",
      "component",
      "data",
      "discriminator",
      "fool",
      "gan",
      "generative",
      "generator",
      "network",
      "networks"
    ],
    "excerpt": "The component of a generative adversarial network that transforms random noise into synthetic data samples, trained to fool the discriminator into classifying its outputs as real.",
    "url": "pages/glossary.html#term-gan-generator"
  },
  {
    "id": "term-gan-inversion",
    "title": "GAN Inversion",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "best",
      "code",
      "computer",
      "editing",
      "enabling",
      "finding",
      "gan",
      "ganbased",
      "gans",
      "given",
      "image",
      "inversion"
    ],
    "excerpt": "The process of finding the latent code in a pre-trained GAN's latent space that best reconstructs a given real image, enabling GAN-based editing and manipulation of real photographs.",
    "url": "pages/glossary.html#term-gan-inversion"
  },
  {
    "id": "term-gated-linear-unit",
    "title": "Gated Linear Unit",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "activation",
      "allowing",
      "architecture",
      "commonly",
      "control",
      "feedforward",
      "flow",
      "gated",
      "information",
      "input",
      "layers",
      "linear"
    ],
    "excerpt": "An activation mechanism that multiplies a linear projection of the input by a sigmoid-gated linear projection, allowing the network to control information flow and commonly used in transformer feedforward layers.",
    "url": "pages/glossary.html#term-gated-linear-unit"
  },
  {
    "id": "term-gaussian-kernel",
    "title": "Gaussian Kernel",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "based",
      "commonly",
      "density",
      "distribution",
      "estimation",
      "function",
      "gaussian",
      "kernel",
      "learning",
      "machine",
      "normal",
      "optimization"
    ],
    "excerpt": "A kernel function based on the Gaussian (normal) distribution, commonly used in kernel density estimation, smoothing, and SVMs.",
    "url": "pages/glossary.html#term-gaussian-kernel"
  },
  {
    "id": "term-gaussian-mixture-model",
    "title": "Gaussian Mixture Model",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithm",
      "clustering",
      "data",
      "distributions",
      "estimated",
      "finite",
      "gaussian",
      "generated",
      "learning",
      "machine",
      "mixture",
      "model"
    ],
    "excerpt": "A probabilistic model that represents data as generated from a mixture of a finite number of Gaussian distributions with unknown parameters, typically estimated via the EM algorithm.",
    "url": "pages/glossary.html#term-gaussian-mixture-model"
  },
  {
    "id": "term-gaussian-process",
    "title": "Gaussian Process",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "bayesian",
      "collection",
      "defines",
      "distribution",
      "finite",
      "follows",
      "function",
      "functions",
      "gaussian",
      "learning",
      "machine",
      "methods"
    ],
    "excerpt": "A non-parametric Bayesian model that defines a probability distribution over functions, where any finite collection of function values follows a multivariate Gaussian distribution.",
    "url": "pages/glossary.html#term-gaussian-process"
  },
  {
    "id": "term-gaussian-splatting",
    "title": "Gaussian Splatting",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "collections",
      "computer",
      "efficient",
      "enabling",
      "gaussian",
      "marching",
      "models",
      "novel",
      "photorealistic",
      "primitives",
      "rasterization"
    ],
    "excerpt": "A 3D scene representation that models scenes as collections of 3D Gaussian primitives, enabling real-time rendering of photorealistic novel views through efficient rasterization rather than ray marching.",
    "url": "pages/glossary.html#term-gaussian-splatting"
  },
  {
    "id": "term-gaze-estimation",
    "title": "Gaze Estimation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "analysis",
      "appearance",
      "attention",
      "based",
      "computer",
      "driver",
      "estimation",
      "eye",
      "gaze",
      "head",
      "humancomputer",
      "image"
    ],
    "excerpt": "The task of predicting where a person is looking based on their eye appearance and head pose in images or video, used in attention analysis, driver monitoring, and human-computer interaction.",
    "url": "pages/glossary.html#term-gaze-estimation"
  },
  {
    "id": "term-gazetteer",
    "title": "Gazetteer",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "curated",
      "entity",
      "feature",
      "gazetteer",
      "list",
      "locations",
      "lookup",
      "named",
      "names",
      "nlp",
      "organizations",
      "organized"
    ],
    "excerpt": "A curated list of entity names organized by type, such as person names, locations, or organizations, used as a feature or lookup resource in named entity recognition systems.",
    "url": "pages/glossary.html#term-gazetteer"
  },
  {
    "id": "term-gdpr-ai-provisions",
    "title": "GDPR AI Provisions",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "ai",
      "assessments",
      "automated",
      "data",
      "decisions",
      "gdpr",
      "general",
      "governance",
      "impact",
      "including",
      "protection",
      "provisions"
    ],
    "excerpt": "Provisions within the EU General Data Protection Regulation relevant to AI, including the right not to be subject to purely automated decisions, data protection impact assessments, and requirements for transparency.",
    "url": "pages/glossary.html#term-gdpr-ai-provisions"
  },
  {
    "id": "term-gelu",
    "title": "GELU (Gaussian Error Linear Unit)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "activation",
      "applies",
      "architecture",
      "commonly",
      "error",
      "function",
      "gaussian",
      "gelu",
      "linear",
      "nonlinearity",
      "probabilistic",
      "smooth"
    ],
    "excerpt": "An activation function commonly used in transformers that applies a smooth, probabilistic non-linearity. Outperforms ReLU in many language models.",
    "url": "pages/glossary.html#term-gelu"
  },
  {
    "id": "term-gemini",
    "title": "Gemini",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "audio",
      "family",
      "gemini",
      "google",
      "googles",
      "images",
      "model",
      "models",
      "multimodal",
      "process",
      "text",
      "video"
    ],
    "excerpt": "Google's family of multimodal AI models that can process text, images, audio, and video. Powers Google's AI features including Bard and Workspace integrations.",
    "url": "pages/glossary.html#term-gemini"
  },
  {
    "id": "term-gemini-launch",
    "title": "Gemini Launch",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2023",
      "audio",
      "capable",
      "december",
      "deepminds",
      "designed",
      "family",
      "gemini",
      "google",
      "googles",
      "history",
      "images"
    ],
    "excerpt": "Google DeepMind's release of Gemini in December 2023, a family of multimodal large language models designed to process text, images, audio, and video, positioned as Google's most capable AI model.",
    "url": "pages/glossary.html#term-gemini-launch"
  },
  {
    "id": "term-general-problem-solver",
    "title": "General Problem Solver",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1957",
      "analysis",
      "attempt",
      "creating",
      "developed",
      "early",
      "formalized",
      "general",
      "generalpurpose",
      "history",
      "meansends",
      "milestones"
    ],
    "excerpt": "An AI program developed by Newell and Simon in 1957 that used means-ends analysis to solve a wide range of formalized problems, representing an early attempt at creating a general-purpose reasoning system.",
    "url": "pages/glossary.html#term-general-problem-solver"
  },
  {
    "id": "term-generalization",
    "title": "Generalization",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "ability",
      "concept",
      "data",
      "examples",
      "generalization",
      "just",
      "memorizing",
      "models",
      "new",
      "perform",
      "quality",
      "rather"
    ],
    "excerpt": "A model's ability to perform well on new, unseen data rather than just memorizing training examples. The fundamental goal of machine learning.",
    "url": "pages/glossary.html#term-generalization"
  },
  {
    "id": "term-generalized-advantage-estimation",
    "title": "Generalized Advantage Estimation (GAE)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "advantage",
      "average",
      "computes",
      "controlled",
      "errors",
      "estimates",
      "estimation",
      "exponentiallyweighted",
      "function",
      "gae",
      "generalized",
      "lambda"
    ],
    "excerpt": "A technique that computes advantage function estimates using an exponentially-weighted average of multi-step TD errors, controlled by a lambda parameter.",
    "url": "pages/glossary.html#term-generalized-advantage-estimation"
  },
  {
    "id": "term-generalized-linear-model",
    "title": "Generalized Linear Model",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "allows",
      "distribution",
      "exponential",
      "family",
      "flexible",
      "follow",
      "function",
      "generalization",
      "generalized",
      "linear",
      "link",
      "mean"
    ],
    "excerpt": "A flexible generalization of ordinary linear regression that allows the response variable to follow any distribution from the exponential family, using a link function to relate the linear predictor to the mean of the distribution.",
    "url": "pages/glossary.html#term-generalized-linear-model"
  },
  {
    "id": "term-generated-knowledge-prompting",
    "title": "Generated Knowledge Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accurately",
      "additional",
      "answer",
      "augmentation",
      "context",
      "downstream",
      "engineering",
      "facts",
      "first",
      "generated",
      "generates",
      "knowledge"
    ],
    "excerpt": "A technique where the model first generates relevant knowledge or facts about a topic, then uses that self-generated knowledge as additional context to answer a downstream question more accurately.",
    "url": "pages/glossary.html#term-generated-knowledge-prompting"
  },
  {
    "id": "term-generation",
    "title": "Generation",
    "category": "Glossary",
    "subcategory": "Process",
    "keywords": [
      "concept",
      "content",
      "core",
      "generation",
      "model",
      "new",
      "process",
      "producing"
    ],
    "excerpt": "The process of producing new content from an AI model. Text generation works by predicting one token at a time; image generation uses diffusion or similar processes.",
    "url": "pages/glossary.html#term-generation"
  },
  {
    "id": "term-gail",
    "title": "Generative Adversarial Imitation Learning (GAIL)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "adversarial",
      "agent",
      "algorithm",
      "discriminator",
      "distinguishes",
      "expert",
      "fool",
      "framework",
      "gail",
      "ganlike",
      "generative",
      "imitation"
    ],
    "excerpt": "An imitation learning algorithm that uses a GAN-like framework where a discriminator distinguishes between agent and expert trajectories while the policy learns to fool the discriminator.",
    "url": "pages/glossary.html#term-gail"
  },
  {
    "id": "term-gan-history",
    "title": "Generative Adversarial Network History",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2014",
      "adversarial",
      "being",
      "biggan",
      "dcgan",
      "development",
      "diffusion",
      "dominated",
      "gans",
      "generation",
      "generative",
      "goodfellows"
    ],
    "excerpt": "The development of GANs from Ian Goodfellow's 2014 invention through progressive improvements including DCGAN, StyleGAN, and BigGAN, which dominated image generation before being supplanted by diffusion models.",
    "url": "pages/glossary.html#term-gan-history"
  },
  {
    "id": "term-generative-ai",
    "title": "Generative AI",
    "category": "Glossary",
    "subcategory": "Field",
    "keywords": [
      "ai",
      "analyzing",
      "category",
      "code",
      "content",
      "create",
      "data",
      "existing",
      "field",
      "generative",
      "images",
      "just"
    ],
    "excerpt": "AI systems that can create new content (text, images, code, music, video) rather than just analyzing existing data. Includes chatbots, image generators, and coding assistants.",
    "url": "pages/glossary.html#term-generative-ai"
  },
  {
    "id": "term-generative-question-answering",
    "title": "Generative Question Answering",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "answer",
      "answering",
      "approach",
      "enabling",
      "extracting",
      "freeform",
      "generates",
      "generative",
      "information",
      "language",
      "model",
      "natural"
    ],
    "excerpt": "A QA approach where the model generates a free-form answer in natural language rather than extracting a span, enabling responses that synthesize information or require reasoning.",
    "url": "pages/glossary.html#term-generative-question-answering"
  },
  {
    "id": "term-genetic-algorithms",
    "title": "Genetic Algorithms",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1960s1970s",
      "algorithms",
      "candidate",
      "crossover",
      "developed",
      "evolve",
      "find",
      "genetic",
      "history",
      "holland",
      "inspired",
      "john"
    ],
    "excerpt": "Optimization algorithms inspired by natural selection, developed by John Holland in the 1960s-1970s, that evolve candidate solutions through selection, crossover, and mutation operators to find optimal or near-optimal solutions.",
    "url": "pages/glossary.html#term-genetic-algorithms"
  },
  {
    "id": "term-geoffrey-hinton",
    "title": "Geoffrey Hinton",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "backpropagation",
      "belief",
      "boltzmann",
      "britishcanadian",
      "codeveloped",
      "computer",
      "deep",
      "geoffrey",
      "godfather",
      "hinton",
      "history",
      "known"
    ],
    "excerpt": "British-Canadian computer scientist known as a godfather of deep learning, who co-developed backpropagation, Boltzmann machines, and deep belief networks.",
    "url": "pages/glossary.html#term-geoffrey-hinton"
  },
  {
    "id": "term-geometric-augmentation",
    "title": "Geometric Augmentation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "arrangement",
      "augmentation",
      "computer",
      "flipping",
      "geometric",
      "image",
      "improve",
      "including",
      "invariance",
      "model",
      "modify",
      "perspective"
    ],
    "excerpt": "Image augmentation techniques that modify the spatial arrangement of pixels including rotation, translation, scaling, shearing, flipping, and perspective transformations to improve model invariance.",
    "url": "pages/glossary.html#term-geometric-augmentation"
  },
  {
    "id": "term-gguf",
    "title": "GGUF",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "binary",
      "commonly",
      "cpu",
      "cpugpu",
      "designed",
      "file",
      "format",
      "gguf",
      "hybrid",
      "inference",
      "language",
      "llama"
    ],
    "excerpt": "A binary file format designed for storing quantized language models optimized for CPU and hybrid CPU/GPU inference, commonly used with the llama.cpp ecosystem.",
    "url": "pages/glossary.html#term-gguf"
  },
  {
    "id": "term-gh200",
    "title": "GH200 Grace Hopper Superchip",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "arm",
      "combining",
      "connected",
      "cpu",
      "cpugpu",
      "gh200",
      "gpu",
      "grace",
      "h200",
      "hardware",
      "highbandwidth",
      "hopper"
    ],
    "excerpt": "NVIDIA's integrated CPU-GPU superchip combining a Grace ARM CPU with a Hopper H200 GPU connected by a high-bandwidth NVLink-C2C interconnect.",
    "url": "pages/glossary.html#term-gh200"
  },
  {
    "id": "term-ghost-work",
    "title": "Ghost Work",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "assurance",
      "conditions",
      "content",
      "countries",
      "data",
      "developing",
      "ethics",
      "fairness",
      "frequently",
      "ghost",
      "human"
    ],
    "excerpt": "The often invisible human labor that powers AI systems, including data labeling, content moderation, and quality assurance, frequently performed by low-paid workers in developing countries under precarious conditions.",
    "url": "pages/glossary.html#term-ghost-work"
  },
  {
    "id": "term-gibbs-sampling",
    "title": "Gibbs Sampling",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bayesian",
      "conditional",
      "current",
      "distribution",
      "gibbs",
      "given",
      "mcmc",
      "method",
      "methods",
      "samples",
      "sampling",
      "statistics"
    ],
    "excerpt": "An MCMC method that samples each variable in turn from its conditional distribution given the current values of all other variables.",
    "url": "pages/glossary.html#term-gibbs-sampling"
  },
  {
    "id": "term-gini-impurity",
    "title": "Gini Impurity",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "according",
      "chosen",
      "class",
      "distribution",
      "gini",
      "impurity",
      "labeled",
      "learning",
      "machine",
      "measure",
      "misclassified",
      "model"
    ],
    "excerpt": "A measure of the probability that a randomly chosen sample would be misclassified if labeled according to the class distribution at a node.",
    "url": "pages/glossary.html#term-gini-impurity"
  },
  {
    "id": "term-github-copilot",
    "title": "GitHub Copilot",
    "category": "Glossary",
    "subcategory": "Product",
    "keywords": [
      "code",
      "copilot",
      "development",
      "editors",
      "github",
      "integrated",
      "pair",
      "product",
      "programmer"
    ],
    "excerpt": "An AI pair programmer integrated into code editors. Uses LLMs to suggest code completions, write functions, and explain code. One of the most successful AI developer tools.",
    "url": "pages/glossary.html#term-github-copilot"
  },
  {
    "id": "term-global-average-pooling",
    "title": "Global Average Pooling",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "across",
      "architecture",
      "average",
      "channel",
      "computes",
      "connected",
      "dimensions",
      "feature",
      "fully",
      "global",
      "layers",
      "map"
    ],
    "excerpt": "A pooling operation that computes the mean of each feature map across all spatial dimensions, reducing the feature map to a single value per channel and often replacing fully connected layers.",
    "url": "pages/glossary.html#term-global-average-pooling"
  },
  {
    "id": "term-glove",
    "title": "GloVe",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "aggregated",
      "combining",
      "context",
      "cooccurrence",
      "corpus",
      "embedding",
      "embeddings",
      "factorization",
      "global",
      "glove",
      "local",
      "matrix"
    ],
    "excerpt": "Global Vectors for Word Representation, a word embedding method that trains on aggregated word co-occurrence statistics from a corpus, combining global matrix factorization with local context window methods.",
    "url": "pages/glossary.html#term-glove"
  },
  {
    "id": "term-goal-conditioned-rl",
    "title": "Goal-Conditioned RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "achieve",
      "agent",
      "agents",
      "conditioned",
      "formulation",
      "function",
      "goal",
      "learning",
      "paradigms",
      "policy",
      "reinforcement",
      "rl"
    ],
    "excerpt": "An RL formulation where the agent's policy and value function are conditioned on a goal specifying what the agent should achieve.",
    "url": "pages/glossary.html#term-goal-conditioned-rl"
  },
  {
    "id": "term-goodharts-law-in-ai",
    "title": "Goodhart&amp;#x27;s Law in AI",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "alignment",
      "becomes",
      "ceases",
      "good",
      "goodhartampx27s",
      "in",
      "law",
      "measure",
      "optimization",
      "principle",
      "proxy"
    ],
    "excerpt": "The principle that when a proxy measure becomes the target for optimization, it ceases to be a good measure.",
    "url": "pages/glossary.html#term-goodharts-law-in-ai"
  },
  {
    "id": "term-google-brain",
    "title": "Google Brain",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2011",
      "2023",
      "andrew",
      "became",
      "brain",
      "center",
      "dean",
      "deep",
      "deepmind",
      "demonstrated",
      "founded",
      "google"
    ],
    "excerpt": "A deep learning research project founded at Google in 2011 by Andrew Ng and Jeff Dean that demonstrated unsupervised learning on YouTube videos and became a major center for neural network research before merging into Google DeepMind in 2023.",
    "url": "pages/glossary.html#term-google-brain"
  },
  {
    "id": "term-google-deepmind",
    "title": "Google DeepMind",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2010",
      "2023",
      "april",
      "brain",
      "created",
      "deepmind",
      "demis",
      "formed",
      "founded",
      "google",
      "hassabis",
      "history"
    ],
    "excerpt": "An AI research laboratory formed in April 2023 by merging Google Brain and DeepMind, created from the original DeepMind Technologies founded by Demis Hassabis, Shane Legg, and Mustafa Suleyman in 2010.",
    "url": "pages/glossary.html#term-google-deepmind"
  },
  {
    "id": "term-tpu-v5",
    "title": "Google TPU v5",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "bandwidth",
      "better",
      "computing",
      "distributed",
      "featuring",
      "fifth",
      "generation",
      "google",
      "googles",
      "hardware",
      "improved",
      "increased"
    ],
    "excerpt": "The fifth generation of Google's Tensor Processing Unit featuring improved matrix multiply units, increased memory bandwidth, and better inter-chip interconnect.",
    "url": "pages/glossary.html#term-tpu-v5"
  },
  {
    "id": "term-google-translate-neural-mt",
    "title": "Google Translate Neural MT",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2016",
      "attention",
      "billions",
      "bringing",
      "dramatically",
      "google",
      "googles",
      "history",
      "improving",
      "machine",
      "milestones",
      "models"
    ],
    "excerpt": "Google's 2016 transition from statistical to neural machine translation using sequence-to-sequence models with attention, dramatically improving translation quality and bringing neural networks to a product used by billions.",
    "url": "pages/glossary.html#term-google-translate-neural-mt"
  },
  {
    "id": "term-gpqa",
    "title": "GPQA",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "across",
      "advanced",
      "answering",
      "benchmark",
      "benchmarks",
      "beyond",
      "biology",
      "chemistry",
      "designed",
      "difficult",
      "domain",
      "engines"
    ],
    "excerpt": "Graduate-Level Google-Proof Question Answering, an extremely difficult benchmark of expert-crafted questions across biology, physics, and chemistry that even domain experts struggle with, designed ...",
    "url": "pages/glossary.html#term-gpqa"
  },
  {
    "id": "term-gpt",
    "title": "GPT (Generative Pre-trained Transformer)",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "generative",
      "gpt",
      "language",
      "large",
      "model",
      "models",
      "openai",
      "openais",
      "pre",
      "series",
      "trained",
      "transformer"
    ],
    "excerpt": "OpenAI's series of large language models. GPT-4 is the latest major version, known for strong reasoning, multimodal capabilities, and broad knowledge.",
    "url": "pages/glossary.html#term-gpt"
  },
  {
    "id": "term-gpt-scaling-laws",
    "title": "GPT Scaling Laws",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "discovered",
      "empirical",
      "gpt",
      "history",
      "kaplan",
      "laws",
      "milestones",
      "powerlaw",
      "relationships",
      "scaling"
    ],
    "excerpt": "Empirical power-law relationships discovered by Kaplan et al. at OpenAI in 2020 showing that language model performance improves predictably with increases in model size, dataset size, and training compute.",
    "url": "pages/glossary.html#term-gpt-scaling-laws"
  },
  {
    "id": "term-gpt-1",
    "title": "GPT-1",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2018",
      "achieve",
      "corpora",
      "demonstrating",
      "finetuning",
      "first",
      "followed",
      "generative",
      "gpt",
      "history",
      "june",
      "large"
    ],
    "excerpt": "The first Generative Pre-trained Transformer model released by OpenAI in June 2018, demonstrating that unsupervised pre-training on large text corpora followed by task-specific fine-tuning could achieve strong NLP performance.",
    "url": "pages/glossary.html#term-gpt-1"
  },
  {
    "id": "term-gpt-2",
    "title": "GPT-2",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "gpt",
      "networks",
      "neural"
    ],
    "excerpt": "A 1.5-billion parameter autoregressive language model by OpenAI that demonstrated strong text generation capabilities and was initially withheld from full release due to concerns about misuse.",
    "url": "pages/glossary.html#term-gpt-2"
  },
  {
    "id": "term-gpt-3",
    "title": "GPT-3",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "175billion",
      "architecture",
      "autoregressive",
      "fewshot",
      "finetuning",
      "gpt",
      "incontext",
      "learning",
      "model",
      "networks",
      "neural",
      "openai"
    ],
    "excerpt": "A 175-billion parameter autoregressive transformer model by OpenAI that popularized few-shot and zero-shot learning through in-context prompting without fine-tuning.",
    "url": "pages/glossary.html#term-gpt-3"
  },
  {
    "id": "term-gpt4",
    "title": "GPT-4",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "advanced",
      "capabilities",
      "featuring",
      "gpt",
      "model",
      "multimodal",
      "openai",
      "openais",
      "reasoning",
      "release",
      "strong"
    ],
    "excerpt": "OpenAI's most advanced GPT model (as of its release), featuring multimodal capabilities and strong reasoning. Powers ChatGPT Plus and many enterprise applications.",
    "url": "pages/glossary.html#term-gpt4"
  },
  {
    "id": "term-gpt-4",
    "title": "GPT-4",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2023",
      "academic",
      "benchmarks",
      "capable",
      "demonstrating",
      "gpt",
      "history",
      "humanlevel",
      "images",
      "language",
      "large",
      "many"
    ],
    "excerpt": "OpenAI's multimodal large language model released in March 2023, capable of processing both text and images, demonstrating human-level performance on many professional and academic benchmarks.",
    "url": "pages/glossary.html#term-gpt-4"
  },
  {
    "id": "term-gpt-j",
    "title": "GPT-J",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "6billion",
      "alternatives",
      "architecture",
      "autoregressive",
      "being",
      "created",
      "eleutherai",
      "first",
      "gpt",
      "gpt3",
      "language",
      "model"
    ],
    "excerpt": "A 6-billion parameter open-source autoregressive language model created by EleutherAI, notable for being one of the first performant open-source alternatives to GPT-3.",
    "url": "pages/glossary.html#term-gpt-j"
  },
  {
    "id": "term-gpt-neox",
    "title": "GPT-NeoX",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "20billion",
      "architecture",
      "attentionfeedforward",
      "autoregressive",
      "computation",
      "efficiency",
      "eleutherai",
      "embeddings",
      "gpt",
      "improved",
      "language",
      "model"
    ],
    "excerpt": "A 20-billion parameter autoregressive language model by EleutherAI that uses rotary positional embeddings and parallel attention-feedforward computation for improved efficiency.",
    "url": "pages/glossary.html#term-gpt-neox"
  },
  {
    "id": "term-gpt-score",
    "title": "GPT-Score",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "assessing",
      "candidate",
      "computing",
      "conditional",
      "evaluation",
      "framework",
      "generate",
      "generation",
      "generative",
      "given",
      "gpt",
      "leverages"
    ],
    "excerpt": "An evaluation framework that leverages generative pre-trained models to score text quality by computing conditional generation probabilities, assessing how likely a model would generate the candidate text given a quality-indicating prompt template.",
    "url": "pages/glossary.html#term-gpt-score"
  },
  {
    "id": "term-gptq",
    "title": "GPTQ",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "4bit",
      "accuracy",
      "approximate",
      "bit",
      "compress",
      "gptq",
      "inference",
      "information",
      "language",
      "large",
      "llm",
      "loss"
    ],
    "excerpt": "A post-training quantization method for large language models that uses approximate second-order information to compress weights to lower bit precision (typically 4-bit) with minimal accuracy loss.",
    "url": "pages/glossary.html#term-gptq"
  },
  {
    "id": "term-gpu",
    "title": "GPU (Graphics Processing Unit)",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "computations",
      "designed",
      "excels",
      "gpu",
      "graphics",
      "hardware",
      "infrastructure",
      "needed",
      "originally",
      "parallel",
      "processing",
      "unit"
    ],
    "excerpt": "Hardware originally designed for graphics that excels at the parallel computations needed for AI. NVIDIA GPUs are the dominant hardware for training and running large AI models.",
    "url": "pages/glossary.html#term-gpu"
  },
  {
    "id": "term-gpu-architecture",
    "title": "GPU Architecture for AI",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "ai",
      "architecture",
      "cores",
      "design",
      "featuring",
      "for",
      "gpu",
      "graphics",
      "hardware",
      "hierarchies",
      "memory",
      "multiprocessors"
    ],
    "excerpt": "The parallel processing design of graphics processing units optimized for AI workloads, featuring thousands of cores organized in streaming multiprocessors with shared memory hierarchies.",
    "url": "pages/glossary.html#term-gpu-architecture"
  },
  {
    "id": "term-gpu-direct",
    "title": "GPU Direct",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "adapters",
      "computing",
      "cpu",
      "data",
      "direct",
      "distributed",
      "enabling",
      "gpu",
      "gpus",
      "memory",
      "network",
      "nvidias"
    ],
    "excerpt": "NVIDIA's technology suite enabling direct data transfers between GPUs and network adapters or storage without staging through CPU memory.",
    "url": "pages/glossary.html#term-gpu-direct"
  },
  {
    "id": "term-gpu-memory-hierarchy",
    "title": "GPU Memory Hierarchy",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "cache",
      "consisting",
      "global",
      "gpu",
      "gpus",
      "hardware",
      "hbmgddr",
      "hierarchy",
      "layered",
      "memory",
      "registers",
      "shared"
    ],
    "excerpt": "The layered memory system in GPUs consisting of registers, shared memory (SRAM), L2 cache, and global memory (HBM/GDDR).",
    "url": "pages/glossary.html#term-gpu-memory-hierarchy"
  },
  {
    "id": "term-grad-cam",
    "title": "Grad-CAM",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "activation",
      "cam",
      "class",
      "cnn",
      "computer",
      "convolutional",
      "final",
      "flowing",
      "grad",
      "gradients",
      "gradientweighted",
      "heatmap"
    ],
    "excerpt": "Gradient-weighted Class Activation Mapping, a visualization method that uses gradients flowing into the final convolutional layer to produce a heatmap highlighting important regions for any CNN prediction.",
    "url": "pages/glossary.html#term-grad-cam"
  },
  {
    "id": "term-gradient",
    "title": "Gradient",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "change",
      "direction",
      "error",
      "gradient",
      "indicating",
      "magnitude",
      "math",
      "models",
      "needed",
      "reduce",
      "training",
      "vector"
    ],
    "excerpt": "A vector indicating the direction and magnitude of change needed to reduce a model's error. The foundation of gradient descent optimization used in training neural networks.",
    "url": "pages/glossary.html#term-gradient"
  },
  {
    "id": "term-gradient-accumulation",
    "title": "Gradient Accumulation",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "accumulating",
      "accumulation",
      "batch",
      "computing",
      "distributed",
      "forwardbackward",
      "gradient",
      "gradients",
      "larger",
      "model",
      "multiple",
      "optimization"
    ],
    "excerpt": "A technique that simulates larger batch sizes by accumulating gradients over multiple forward-backward passes before performing a parameter update.",
    "url": "pages/glossary.html#term-gradient-accumulation"
  },
  {
    "id": "term-gradient-boosting",
    "title": "Gradient Boosting",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "boosting",
      "builds",
      "combined",
      "correct",
      "descent",
      "ensemble",
      "errors",
      "far",
      "function",
      "gradient",
      "learning",
      "loss"
    ],
    "excerpt": "An ensemble technique that builds models sequentially, with each new model trained to correct the residual errors of the combined ensemble so far, using gradient descent in function space to minimize a specified loss.",
    "url": "pages/glossary.html#term-gradient-boosting"
  },
  {
    "id": "term-gradient-checkpointing",
    "title": "Gradient Checkpointing",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "activations",
      "architecture",
      "backpropagation",
      "checkpointing",
      "checkpoints",
      "computation",
      "forward",
      "gradient",
      "intermediate",
      "memory",
      "networks",
      "neural"
    ],
    "excerpt": "A memory optimization technique that trades computation for memory by only storing activations at selected checkpoints during the forward pass and recomputing intermediate activations during backpropagation.",
    "url": "pages/glossary.html#term-gradient-checkpointing"
  },
  {
    "id": "term-gradient-clipping",
    "title": "Gradient Clipping",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "architectures",
      "clipping",
      "deep",
      "destabilize",
      "exceeds",
      "exploding",
      "gradient",
      "gradients",
      "learning",
      "machine",
      "networks",
      "norm"
    ],
    "excerpt": "A technique that rescales or truncates gradients when their norm exceeds a specified threshold, preventing the exploding gradient problem that can destabilize training in deep networks and recurrent architectures.",
    "url": "pages/glossary.html#term-gradient-clipping"
  },
  {
    "id": "term-gradient-descent",
    "title": "Gradient Descent",
    "category": "Glossary",
    "subcategory": "Algorithm",
    "keywords": [
      "adjusting",
      "algorithm",
      "descent",
      "direction",
      "error",
      "gradient",
      "iteratively",
      "networks",
      "neural",
      "optimization",
      "reduces",
      "training"
    ],
    "excerpt": "The optimization algorithm that trains neural networks by iteratively adjusting weights in the direction that reduces error. Variants include SGD, Adam, and AdaGrad.",
    "url": "pages/glossary.html#term-gradient-descent"
  },
  {
    "id": "term-gradient-synchronization",
    "title": "Gradient Synchronization",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "across",
      "aggregating",
      "allreduce",
      "computing",
      "distributed",
      "gpus",
      "gradient",
      "gradients",
      "model",
      "multiple",
      "nodes",
      "optimization"
    ],
    "excerpt": "The process of aggregating gradients across multiple GPUs or nodes in distributed training, typically via all-reduce.",
    "url": "pages/glossary.html#term-gradient-synchronization"
  },
  {
    "id": "term-grammar-constrained-decoding",
    "title": "Grammar-Constrained Decoding",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "always",
      "approach",
      "bnf",
      "conform",
      "constrained",
      "decoding",
      "ensuring",
      "formal",
      "formats",
      "generation",
      "generative"
    ],
    "excerpt": "A decoding approach that restricts token generation to sequences valid under a formal grammar (such as BNF or regex), ensuring outputs always conform to specified structural formats.",
    "url": "pages/glossary.html#term-grammar-constrained-decoding"
  },
  {
    "id": "term-granger-causality",
    "title": "Granger Causality",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "alone",
      "beyond",
      "causality",
      "concept",
      "data",
      "future",
      "granger",
      "grangercause",
      "information",
      "past",
      "provide",
      "said"
    ],
    "excerpt": "A statistical concept where a time series X is said to Granger-cause Y if past values of X provide statistically significant information about future values of Y beyond what past values of Y alone provide.",
    "url": "pages/glossary.html#term-granger-causality"
  },
  {
    "id": "term-gat",
    "title": "Graph Attention Network",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "aggregation",
      "architecture",
      "attention",
      "connections",
      "features",
      "focus",
      "graph",
      "importance",
      "learning",
      "mechanisms",
      "neighboring",
      "network"
    ],
    "excerpt": "A graph neural network that uses attention mechanisms to weight the importance of neighboring nodes' features during aggregation, learning to focus on the most relevant connections.",
    "url": "pages/glossary.html#term-gat"
  },
  {
    "id": "term-gcn",
    "title": "Graph Convolutional Network",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "aggregating",
      "architecture",
      "convolutional",
      "data",
      "defined",
      "features",
      "graph",
      "graphstructured",
      "learnable",
      "neighboring",
      "network",
      "networks"
    ],
    "excerpt": "A neural network that operates on graph-structured data by aggregating features from neighboring nodes through learnable convolutional operations defined on the graph topology.",
    "url": "pages/glossary.html#term-gcn"
  },
  {
    "id": "term-gin",
    "title": "Graph Isomorphism Network",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "aggregator",
      "architecture",
      "discriminative",
      "function",
      "graph",
      "isomorphism",
      "maximize",
      "mlp",
      "network",
      "networks",
      "neural",
      "power"
    ],
    "excerpt": "A graph neural network provably as powerful as the Weisfeiler-Lehman graph isomorphism test, using a sum aggregator and MLP update function to maximize discriminative power on graph structures.",
    "url": "pages/glossary.html#term-gin"
  },
  {
    "id": "term-graph-rag",
    "title": "Graph RAG",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "approach",
      "builds",
      "coherent",
      "context",
      "documents",
      "generation",
      "generative",
      "graph",
      "interconnected",
      "knowledge",
      "llm"
    ],
    "excerpt": "A retrieval-augmented generation approach that builds a knowledge graph from source documents and uses graph traversal to retrieve structured, interconnected context for more coherent multi-hop reasoning.",
    "url": "pages/glossary.html#term-graph-rag"
  },
  {
    "id": "term-graph-based-index",
    "title": "Graph-Based Index",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "based",
      "connect",
      "database",
      "edges",
      "efficient",
      "enabling",
      "entry",
      "graph",
      "index",
      "navigating",
      "nearest",
      "neighbor"
    ],
    "excerpt": "A vector index structure that organizes vectors as nodes in a proximity graph where edges connect similar vectors, enabling efficient nearest neighbor search by navigating the graph from entry points toward the query's neighborhood.",
    "url": "pages/glossary.html#term-graph-based-index"
  },
  {
    "id": "term-graph-based-parsing",
    "title": "Graph-Based Parsing",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "accurate",
      "algorithms",
      "approach",
      "based",
      "dependency",
      "edges",
      "finds",
      "graph",
      "highestscoring",
      "like",
      "maximum",
      "methods"
    ],
    "excerpt": "A parsing approach that scores all possible dependency edges simultaneously and finds the highest-scoring tree using algorithms like maximum spanning tree, typically more accurate but slower than transition-based methods.",
    "url": "pages/glossary.html#term-graph-based-parsing"
  },
  {
    "id": "term-graphcore",
    "title": "Graphcore",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "company",
      "computing",
      "cores",
      "developed",
      "distributed",
      "featuring",
      "graphcore",
      "hardware",
      "independent",
      "intelligence",
      "ipu",
      "large"
    ],
    "excerpt": "A semiconductor company that developed the Intelligence Processing Unit (IPU), featuring a massive number of independent processor cores with large distributed on-chip SRAM.",
    "url": "pages/glossary.html#term-graphcore"
  },
  {
    "id": "term-graphsage",
    "title": "GraphSAGE",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "aggregates",
      "architecture",
      "enabling",
      "features",
      "framework",
      "generalization",
      "graphs",
      "graphsage",
      "inductive",
      "learning",
      "local",
      "neighborhood"
    ],
    "excerpt": "A framework for inductive representation learning on graphs that samples and aggregates features from a node's local neighborhood, enabling generalization to unseen nodes without retraining.",
    "url": "pages/glossary.html#term-graphsage"
  },
  {
    "id": "term-greedy-decoding",
    "title": "Greedy Decoding",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "always",
      "decoding",
      "deterministic",
      "diversity",
      "generation",
      "generative",
      "greedy",
      "highest",
      "lacking",
      "likely",
      "often"
    ],
    "excerpt": "A deterministic text generation strategy that always selects the token with the highest probability at each step, producing the most likely sequence but often lacking diversity.",
    "url": "pages/glossary.html#term-greedy-decoding"
  },
  {
    "id": "term-grid-search",
    "title": "Grid Search",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "best",
      "combination",
      "combinations",
      "combined",
      "crossvalidation",
      "evaluates",
      "exhaustively",
      "grid",
      "hyperparameter",
      "learning",
      "machine",
      "method"
    ],
    "excerpt": "A hyperparameter tuning method that exhaustively evaluates all combinations of specified parameter values, typically combined with cross-validation to select the combination yielding the best performance.",
    "url": "pages/glossary.html#term-grid-search"
  },
  {
    "id": "term-grok",
    "title": "Grok",
    "category": "Glossary",
    "subcategory": "Product",
    "keywords": [
      "assistant",
      "company",
      "developed",
      "elon",
      "grok",
      "model",
      "musks",
      "product",
      "xai"
    ],
    "excerpt": "An AI assistant developed by xAI (Elon Musk's AI company). Integrated with X (Twitter) and known for real-time information access and less restrictive conversation style.",
    "url": "pages/glossary.html#term-grok"
  },
  {
    "id": "term-groq",
    "title": "Groq",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "achieve",
      "architecture",
      "company",
      "deterministic",
      "developed",
      "extremely",
      "groq",
      "hardware",
      "inference",
      "infrastructure",
      "language",
      "llm"
    ],
    "excerpt": "An AI hardware company that developed the Language Processing Unit (LPU), a deterministic architecture using software-defined scheduling to achieve extremely low-latency LLM inference.",
    "url": "pages/glossary.html#term-groq"
  },
  {
    "id": "term-ground-truth",
    "title": "Ground Truth",
    "category": "Glossary",
    "subcategory": "Data",
    "keywords": [
      "answer",
      "correct",
      "data",
      "evaluate",
      "evaluation",
      "ground",
      "label",
      "model",
      "predictions",
      "truth"
    ],
    "excerpt": "The correct answer or label used to evaluate model predictions. Obtained through human annotation, measurement, or other authoritative sources.",
    "url": "pages/glossary.html#term-ground-truth"
  },
  {
    "id": "term-grounding",
    "title": "Grounding",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "accuracy",
      "connecting",
      "grounding",
      "hallucinations",
      "increase",
      "information",
      "outputs",
      "reduce",
      "sources",
      "technique",
      "verified"
    ],
    "excerpt": "Connecting AI outputs to verified information sources to reduce hallucinations and increase accuracy. Often involves retrieval-augmented generation (RAG) or real-time search.",
    "url": "pages/glossary.html#term-grounding"
  },
  {
    "id": "term-grounding-dino",
    "title": "Grounding DINO",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "arbitrary",
      "categoryspecific",
      "combines",
      "computer",
      "descriptions",
      "detection",
      "detector",
      "dino",
      "dinobased",
      "enabling",
      "grounded",
      "grounding"
    ],
    "excerpt": "An open-set object detection model that combines a DINO-based detector with grounded pre-training, enabling detection of arbitrary objects specified by text descriptions without category-specific training.",
    "url": "pages/glossary.html#term-grounding-dino"
  },
  {
    "id": "term-group-fairness",
    "title": "Group Fairness",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "across",
      "ai",
      "attributes",
      "criteria",
      "defined",
      "demographic",
      "equalized",
      "error",
      "ethics",
      "fairness",
      "group",
      "groups"
    ],
    "excerpt": "Fairness criteria that require statistical parity of outcomes or error rates across demographic groups defined by protected attributes, including demographic parity, equalized odds, and predictive parity.",
    "url": "pages/glossary.html#term-group-fairness"
  },
  {
    "id": "term-group-normalization",
    "title": "Group Normalization",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "batch",
      "channels",
      "divides",
      "group",
      "groups",
      "independently",
      "method",
      "networks",
      "neural",
      "normalization",
      "normalizes"
    ],
    "excerpt": "A normalization method that divides channels into groups and normalizes within each group independently, providing stable performance regardless of batch size unlike batch normalization.",
    "url": "pages/glossary.html#term-group-normalization"
  },
  {
    "id": "term-grouped-query-attention",
    "title": "Grouped Query Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "balance",
      "grouped",
      "groups",
      "head",
      "heads",
      "inference",
      "interpolating",
      "keyvalue",
      "mechanism",
      "multihead"
    ],
    "excerpt": "An attention mechanism that groups multiple query heads to share a single key-value head, interpolating between multi-head and multi-query attention to balance quality and inference speed.",
    "url": "pages/glossary.html#term-grouped-query-attention"
  },
  {
    "id": "term-gru",
    "title": "GRU",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "computation",
      "control",
      "fewer",
      "flow",
      "gated",
      "gates",
      "gru",
      "information",
      "lstm",
      "network",
      "networks"
    ],
    "excerpt": "Gated Recurrent Unit, a recurrent neural network variant that uses reset and update gates to control information flow, offering similar performance to LSTM with fewer parameters and simpler computation.",
    "url": "pages/glossary.html#term-gru"
  },
  {
    "id": "term-gsm8k",
    "title": "GSM8K",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "500",
      "arithmetic",
      "benchmark",
      "benchmarks",
      "capabilities",
      "diverse",
      "evaluate",
      "evaluation",
      "grade",
      "gradeschoollevel",
      "gsm8k",
      "language"
    ],
    "excerpt": "Grade School Math 8K, a benchmark of 8,500 linguistically diverse grade-school-level math word problems requiring multi-step arithmetic reasoning, widely used to evaluate mathematical problem-solving capabilities of language models.",
    "url": "pages/glossary.html#term-gsm8k"
  },
  {
    "id": "term-guardrails",
    "title": "Guardrails",
    "category": "Glossary",
    "subcategory": "Safety",
    "keywords": [
      "behavior",
      "constrain",
      "constraint",
      "guardrails",
      "harmful",
      "mechanisms",
      "outputs",
      "prevent",
      "safety"
    ],
    "excerpt": "Safety mechanisms that constrain AI behavior to prevent harmful outputs. Include content filters, output validators, and behavioral restrictions built into AI systems.",
    "url": "pages/glossary.html#term-guardrails"
  },
  {
    "id": "term-guided-generation",
    "title": "Guided Generation",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "conform",
      "constrain",
      "decoding",
      "format",
      "generation",
      "generative",
      "grammar",
      "guided",
      "invalid",
      "json",
      "language"
    ],
    "excerpt": "Techniques that constrain language model output to conform to a specified format (such as JSON schema or grammar rules) by masking invalid tokens during the decoding process.",
    "url": "pages/glossary.html#term-guided-generation"
  },
  {
    "id": "term-gym-environment",
    "title": "Gym Environment",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "benchmark",
      "collection",
      "concepts",
      "core",
      "developed",
      "environment",
      "environments",
      "gym",
      "interface",
      "learning",
      "openai",
      "originally"
    ],
    "excerpt": "An interface standard and collection of benchmark environments originally developed by OpenAI for RL research.",
    "url": "pages/glossary.html#term-gym-environment"
  },
  {
    "id": "term-habana",
    "title": "Habana Labs",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "accelerators",
      "bandwidth",
      "computing",
      "distributed",
      "featuring",
      "gaudi",
      "habana",
      "hardware",
      "high",
      "integrated",
      "intel",
      "labs"
    ],
    "excerpt": "An Intel subsidiary producing the Gaudi series of AI training accelerators featuring integrated RoCE networking and high memory bandwidth.",
    "url": "pages/glossary.html#term-habana"
  },
  {
    "id": "term-hallucination",
    "title": "Hallucination",
    "category": "Glossary",
    "subcategory": "Limitation",
    "keywords": [
      "actually",
      "fabricated",
      "generates",
      "hallucination",
      "incorrect",
      "information",
      "limitation",
      "plausible",
      "risk",
      "sounds"
    ],
    "excerpt": "When AI generates information that sounds plausible but is actually incorrect or fabricated. Includes fake citations, invented statistics, and fictional events. A major challenge in LLM reliability.",
    "url": "pages/glossary.html#term-hallucination"
  },
  {
    "id": "term-hallucination-mitigation",
    "title": "Hallucination Mitigation",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "augmentation",
      "checks",
      "citationbased",
      "collection",
      "content",
      "designed",
      "fabricated",
      "factually",
      "generative",
      "grounding",
      "hallucination"
    ],
    "excerpt": "A collection of techniques designed to reduce factually incorrect or fabricated content in LLM outputs, including retrieval augmentation, self-consistency checks, and citation-based grounding.",
    "url": "pages/glossary.html#term-hallucination-mitigation"
  },
  {
    "id": "term-hallucination-rate",
    "title": "Hallucination Rate",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "claims",
      "contains",
      "content",
      "contradicting",
      "evaluation",
      "fabricated",
      "facts",
      "generated",
      "hallucination",
      "indicator",
      "information",
      "key"
    ],
    "excerpt": "A metric that quantifies the proportion of generated content that contains fabricated facts, unsupported claims, or information contradicting the source material, serving as a key safety and reliability indicator for language models.",
    "url": "pages/glossary.html#term-hallucination-rate"
  },
  {
    "id": "term-hamiltonian-monte-carlo",
    "title": "Hamiltonian Monte Carlo",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "acceptance",
      "bayesian",
      "behavior",
      "carlo",
      "distant",
      "distributions",
      "dynamics",
      "efficiency",
      "exploration",
      "hamiltonian",
      "high",
      "highdimensional"
    ],
    "excerpt": "An MCMC method that uses Hamiltonian dynamics to propose distant samples with high acceptance probability, reducing random walk behavior and improving exploration efficiency for high-dimensional distributions.",
    "url": "pages/glossary.html#term-hamiltonian-monte-carlo"
  },
  {
    "id": "term-hamming-distance",
    "title": "Hamming Distance",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "corresponding",
      "differ",
      "distance",
      "equallength",
      "hamming",
      "learning",
      "machine",
      "metrics",
      "number",
      "positions",
      "strings",
      "symbols"
    ],
    "excerpt": "The number of positions at which corresponding symbols in two equal-length strings or vectors differ.",
    "url": "pages/glossary.html#term-hamming-distance"
  },
  {
    "id": "term-hand-pose-estimation",
    "title": "Hand Pose Estimation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "applications",
      "computer",
      "depth",
      "enabling",
      "estimation",
      "fingertips",
      "gesture",
      "hand",
      "image",
      "images",
      "joints",
      "language"
    ],
    "excerpt": "The task of predicting the 3D positions of hand joints and fingertips from images or depth sensors, enabling gesture recognition and hand tracking for VR/AR and sign language applications.",
    "url": "pages/glossary.html#term-hand-pose-estimation"
  },
  {
    "id": "term-hard-attention",
    "title": "Hard Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attend",
      "attention",
      "averages",
      "computing",
      "discrete",
      "estimators",
      "hard",
      "learning",
      "mechanism",
      "networks",
      "neural"
    ],
    "excerpt": "An attention mechanism that selects discrete positions to attend to rather than computing weighted averages, requiring reinforcement learning or straight-through estimators for training.",
    "url": "pages/glossary.html#term-hard-attention"
  },
  {
    "id": "term-hate-speech-detection",
    "title": "Hate Speech Detection",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "attributes",
      "automatically",
      "based",
      "content",
      "detection",
      "expresses",
      "gender",
      "group",
      "hate",
      "hatred",
      "identifying",
      "like"
    ],
    "excerpt": "The task of automatically identifying text that expresses hatred toward a group based on attributes like race, gender, or religion, used in content moderation and online safety systems.",
    "url": "pages/glossary.html#term-hate-speech-detection"
  },
  {
    "id": "term-hazard-function",
    "title": "Hazard Function",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "data",
      "events",
      "function",
      "given",
      "hazard",
      "instantaneous",
      "occur",
      "point",
      "rate",
      "science",
      "statistics",
      "subjects"
    ],
    "excerpt": "The instantaneous rate at which events occur for subjects who have survived up to a given time point.",
    "url": "pages/glossary.html#term-hazard-function"
  },
  {
    "id": "term-he-initialization",
    "title": "He Initialization",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "activation",
      "architecture",
      "designed",
      "divided",
      "functions",
      "he",
      "initial",
      "initialization",
      "input",
      "networks",
      "neural",
      "number"
    ],
    "excerpt": "A weight initialization strategy specifically designed for ReLU activation functions that scales the variance of initial weights by 2 divided by the number of input units.",
    "url": "pages/glossary.html#term-he-initialization"
  },
  {
    "id": "term-heatmap-prediction",
    "title": "Heatmap Prediction",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "accuracy",
      "computer",
      "detection",
      "heatmap",
      "image",
      "indicating",
      "keypoint",
      "likely",
      "localization",
      "locations",
      "peaks",
      "prediction"
    ],
    "excerpt": "A technique in keypoint detection that predicts a probability heatmap for each keypoint type, with peaks indicating likely keypoint locations, providing sub-pixel localization accuracy.",
    "url": "pages/glossary.html#term-heatmap-prediction"
  },
  {
    "id": "term-hebbian-learning",
    "title": "Hebbian Learning",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1949",
      "active",
      "connections",
      "donald",
      "fire",
      "hebb",
      "hebbian",
      "history",
      "learning",
      "meaning",
      "milestones",
      "neurons"
    ],
    "excerpt": "A learning principle proposed by Donald Hebb in 1949 stating that neurons that fire together wire together, meaning synaptic connections strengthen when pre- and post-synaptic neurons are simultaneously active.",
    "url": "pages/glossary.html#term-hebbian-learning"
  },
  {
    "id": "term-hedge-detection",
    "title": "Hedge Detection",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "appears",
      "detection",
      "expressions",
      "hedge",
      "identifying",
      "indicate",
      "linguistic",
      "linguistics",
      "nlp",
      "possibly",
      "speculation",
      "task"
    ],
    "excerpt": "The task of identifying linguistic expressions that indicate uncertainty, speculation, or tentativeness in text, such as 'might,' 'possibly,' or 'it appears that.'",
    "url": "pages/glossary.html#term-hedge-detection"
  },
  {
    "id": "term-hellaswag",
    "title": "HellaSwag",
    "category": "Glossary",
    "subcategory": "Benchmark",
    "keywords": [
      "benchmark",
      "commonsense",
      "evaluation",
      "hellaswag",
      "inference",
      "language",
      "natural",
      "testing"
    ],
    "excerpt": "A benchmark testing commonsense natural language inference. Models must select the most plausible continuation of a scenario, evaluating real-world reasoning capabilities.",
    "url": "pages/glossary.html#term-hellaswag"
  },
  {
    "id": "term-herbert-simon",
    "title": "Herbert Simon",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19162001",
      "1978",
      "american",
      "bounded",
      "cocreated",
      "developed",
      "economics",
      "economist",
      "general",
      "herbert",
      "history",
      "logic"
    ],
    "excerpt": "American political scientist, economist, and AI pioneer (1916-2001) who co-created the Logic Theorist and General Problem Solver, developed bounded rationality theory, and won the Nobel Prize in Economics in 1978.",
    "url": "pages/glossary.html#term-herbert-simon"
  },
  {
    "id": "term-heteroscedasticity",
    "title": "Heteroscedasticity",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "across",
      "analysis",
      "condition",
      "constant",
      "data",
      "heteroscedasticity",
      "independent",
      "levels",
      "regression",
      "residuals",
      "science",
      "statistics"
    ],
    "excerpt": "A condition in regression analysis where the variance of the residuals is not constant across all levels of the independent variables.",
    "url": "pages/glossary.html#term-heteroscedasticity"
  },
  {
    "id": "term-heuristic",
    "title": "Heuristic",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "concept",
      "decisions",
      "even",
      "guaranteed",
      "helps",
      "heuristic",
      "make",
      "optimal",
      "practical",
      "problem",
      "problems",
      "quickly"
    ],
    "excerpt": "A practical rule or strategy that helps solve problems or make decisions quickly, even if not guaranteed to be optimal. Used in AI search, optimization, and rule-based systems.",
    "url": "pages/glossary.html#term-heuristic"
  },
  {
    "id": "term-hidden-layer",
    "title": "Hidden Layer",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "hidden",
      "input",
      "layer",
      "layers",
      "network",
      "networks",
      "neural",
      "output"
    ],
    "excerpt": "Layers in a neural network between the input and output layers. Deep networks have many hidden layers, enabling them to learn complex hierarchical representations.",
    "url": "pages/glossary.html#term-hidden-layer"
  },
  {
    "id": "term-hidden-markov-model",
    "title": "Hidden Markov Model",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "assumed",
      "dependent",
      "hidden",
      "learning",
      "machine",
      "markov",
      "model",
      "observations",
      "probabilistically",
      "probability",
      "process",
      "state"
    ],
    "excerpt": "A statistical model where the system is assumed to be a Markov process with unobserved (hidden) states, and observations are probabilistically dependent on the hidden state.",
    "url": "pages/glossary.html#term-hidden-markov-model"
  },
  {
    "id": "term-hmm",
    "title": "Hidden Markov Model",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "data",
      "emission",
      "generative",
      "hidden",
      "markov",
      "model",
      "ner",
      "nlp",
      "pos",
      "probabilistic",
      "probabilities",
      "processing"
    ],
    "excerpt": "A generative probabilistic model for sequential data where the system transitions between hidden states with emission probabilities, used in POS tagging, speech recognition, and NER.",
    "url": "pages/glossary.html#term-hmm"
  },
  {
    "id": "term-hierarchical-clustering",
    "title": "Hierarchical Clustering",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "agglomerative",
      "approach",
      "builds",
      "clustering",
      "clusters",
      "dendrogram",
      "divisive",
      "either",
      "hierarchical",
      "hierarchy",
      "larger",
      "learning"
    ],
    "excerpt": "A clustering approach that builds a tree-like hierarchy of clusters either by progressively merging smaller clusters (agglomerative) or by recursively splitting larger clusters (divisive), producing a dendrogram.",
    "url": "pages/glossary.html#term-hierarchical-clustering"
  },
  {
    "id": "term-hierarchical-rl",
    "title": "Hierarchical Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "abstractions",
      "complex",
      "decompose",
      "different",
      "frameworks",
      "hierarchical",
      "hierarchies",
      "learning",
      "multiagent",
      "operating",
      "reinforcement",
      "skills"
    ],
    "excerpt": "RL frameworks that decompose complex tasks into hierarchies of subtasks or skills operating at different temporal abstractions.",
    "url": "pages/glossary.html#term-hierarchical-rl"
  },
  {
    "id": "term-hbm",
    "title": "High Bandwidth Memory (HBM)",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "bandwidth",
      "connects",
      "dies",
      "dram",
      "gddr",
      "gpu",
      "hardware",
      "hbm",
      "high",
      "higher",
      "highperformance",
      "memory"
    ],
    "excerpt": "A high-performance DRAM technology that stacks memory dies vertically and connects them with through-silicon vias, providing significantly higher bandwidth than traditional GDDR memory.",
    "url": "pages/glossary.html#term-hbm"
  },
  {
    "id": "term-high-risk-ai-systems",
    "title": "High-Risk AI Systems",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "act",
      "ai",
      "biometric",
      "critical",
      "domains",
      "education",
      "employment",
      "enforcement",
      "governance",
      "high",
      "identification",
      "infrastructure"
    ],
    "excerpt": "Under the EU AI Act, AI systems used in critical domains such as biometric identification, critical infrastructure, education, employment, law enforcement, and migration that must meet stringent requirements.",
    "url": "pages/glossary.html#term-high-risk-ai-systems"
  },
  {
    "id": "term-high-stakes-ai",
    "title": "High-Stakes AI",
    "category": "Glossary",
    "subcategory": "Ethics",
    "keywords": [
      "ai",
      "applications",
      "consequences",
      "decisions",
      "diagnoses",
      "errors",
      "ethics",
      "financial",
      "healthcare",
      "high",
      "legal",
      "risk"
    ],
    "excerpt": "AI applications where errors can have severe consequences: healthcare diagnoses, legal decisions, financial transactions. Requires extra scrutiny, testing, and often human oversight.",
    "url": "pages/glossary.html#term-high-stakes-ai"
  },
  {
    "id": "term-hindsight-experience-replay",
    "title": "Hindsight Experience Replay (HER)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "achieved",
      "actually",
      "alternative",
      "augments",
      "experience",
      "failed",
      "failures",
      "goals",
      "her",
      "hindsight",
      "learning",
      "paradigms"
    ],
    "excerpt": "A technique that augments failed trajectories by relabeling goals with actually achieved states, turning failures into successes for alternative goals.",
    "url": "pages/glossary.html#term-hindsight-experience-replay"
  },
  {
    "id": "term-hinge-loss",
    "title": "Hinge Loss",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "classifiers",
      "defined",
      "function",
      "hinge",
      "learning",
      "loss",
      "machine",
      "max0",
      "maximummargin",
      "optimization",
      "svms"
    ],
    "excerpt": "A loss function used in maximum-margin classifiers such as SVMs, defined as max(0, 1 - y * f(x)). It penalizes predictions that are on the wrong side of the margin boundary.",
    "url": "pages/glossary.html#term-hinge-loss"
  },
  {
    "id": "term-historical-bias",
    "title": "Historical Bias",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "accurately",
      "ai",
      "bias",
      "data",
      "discriminatory",
      "encoded",
      "ethics",
      "even",
      "fairness",
      "historical",
      "inequalities",
      "past"
    ],
    "excerpt": "Bias that is encoded in data reflecting past discriminatory practices or societal inequalities, which AI systems can perpetuate even when the data accurately represents historical patterns.",
    "url": "pages/glossary.html#term-historical-bias"
  },
  {
    "id": "term-hit-rate",
    "title": "Hit Rate",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "across",
      "appears",
      "assessment",
      "binary",
      "document",
      "evaluation",
      "fraction",
      "hit",
      "least",
      "measures",
      "metric",
      "metrics"
    ],
    "excerpt": "A retrieval metric that measures the fraction of queries for which at least one relevant document appears in the top K results, providing a simple binary assessment of retrieval success across a query set.",
    "url": "pages/glossary.html#term-hit-rate"
  },
  {
    "id": "term-hnsw",
    "title": "HNSW",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "accuracy",
      "algorithm",
      "approximate",
      "balance",
      "builds",
      "complexity",
      "database",
      "excellent",
      "graphbased",
      "graphs",
      "hierarchical",
      "highdimensional"
    ],
    "excerpt": "Hierarchical Navigable Small World, a graph-based approximate nearest neighbor algorithm that builds multi-layered proximity graphs with logarithmic search complexity, offering an excellent balance...",
    "url": "pages/glossary.html#term-hnsw"
  },
  {
    "id": "term-hoeffdings-inequality",
    "title": "Hoeffding&amp;#x27;s Inequality",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bound",
      "bounded",
      "deviates",
      "expected",
      "hoeffdingampx27s",
      "independent",
      "inequality",
      "probability",
      "provides",
      "random",
      "statistics",
      "sum"
    ],
    "excerpt": "A probability inequality that provides an upper bound on the probability that the sum of bounded independent random variables deviates from its expected value.",
    "url": "pages/glossary.html#term-hoeffdings-inequality"
  },
  {
    "id": "term-holdout-method",
    "title": "Holdout Method",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "7080",
      "data",
      "evaluation",
      "holdout",
      "learning",
      "machine",
      "method",
      "model",
      "remainder",
      "selection",
      "separate",
      "sets"
    ],
    "excerpt": "The simplest model evaluation strategy that splits data into separate training and test sets, typically using 70-80% for training and the remainder for testing.",
    "url": "pages/glossary.html#term-holdout-method"
  },
  {
    "id": "term-holt-winters-method",
    "title": "Holt-Winters Method",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "captures",
      "components",
      "data",
      "exponential",
      "forecasting",
      "holt",
      "level",
      "method",
      "science",
      "seasonal",
      "series",
      "smoothing"
    ],
    "excerpt": "A triple exponential smoothing method for time series forecasting that captures level, trend, and seasonal components. It supports both additive and multiplicative seasonal patterns.",
    "url": "pages/glossary.html#term-holt-winters-method"
  },
  {
    "id": "term-homography",
    "title": "Homography",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "augmented",
      "camera",
      "computer",
      "correction",
      "homography",
      "image",
      "maps",
      "matrix",
      "perspective",
      "planar",
      "planes",
      "points"
    ],
    "excerpt": "A projective transformation matrix that maps points between two image planes, used for image stitching, perspective correction, and augmented reality when the scene is planar or the camera undergoes pure rotation.",
    "url": "pages/glossary.html#term-homography"
  },
  {
    "id": "term-homomorphic-encryption",
    "title": "Homomorphic Encryption",
    "category": "Glossary",
    "subcategory": "Privacy",
    "keywords": [
      "ai",
      "allows",
      "computations",
      "cryptographic",
      "data",
      "decrypting",
      "enabling",
      "encrypted",
      "encryption",
      "ethics",
      "first",
      "guarantees"
    ],
    "excerpt": "A cryptographic technique that allows computations to be performed on encrypted data without decrypting it first, enabling AI models to process sensitive data while maintaining privacy guarantees.",
    "url": "pages/glossary.html#term-homomorphic-encryption"
  },
  {
    "id": "term-homonymy",
    "title": "Homonymy",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "animal",
      "bat",
      "equipment",
      "having",
      "homonymy",
      "linguistics",
      "meanings",
      "nlp",
      "pronunciation",
      "property",
      "same",
      "spelling"
    ],
    "excerpt": "The property of two or more words having the same spelling or pronunciation but unrelated meanings, such as 'bat' the animal and 'bat' the sporting equipment.",
    "url": "pages/glossary.html#term-homonymy"
  },
  {
    "id": "term-hopfield-network",
    "title": "Hopfield Network",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "associative",
      "connections",
      "energy",
      "functioning",
      "hopfield",
      "inputs",
      "memory",
      "minima",
      "network",
      "networks",
      "neural"
    ],
    "excerpt": "A recurrent neural network with symmetric connections that stores patterns as energy minima, functioning as an associative memory that retrieves stored patterns from partial or noisy inputs.",
    "url": "pages/glossary.html#term-hopfield-network"
  },
  {
    "id": "term-huber-loss",
    "title": "Huber Loss",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "absolute",
      "best",
      "combining",
      "error",
      "function",
      "huber",
      "large",
      "learning",
      "linear",
      "loss",
      "machine",
      "mean"
    ],
    "excerpt": "A loss function that is quadratic for small residuals and linear for large residuals, combining the best properties of mean squared error and mean absolute error.",
    "url": "pages/glossary.html#term-huber-loss"
  },
  {
    "id": "term-hugging-face",
    "title": "Hugging Face",
    "category": "Glossary",
    "subcategory": "Platform",
    "keywords": [
      "applications",
      "community",
      "datasets",
      "face",
      "hugging",
      "models",
      "open",
      "platform",
      "sharing",
      "source"
    ],
    "excerpt": "A platform and community for sharing AI models, datasets, and applications. Hosts thousands of open-source models and the popular Transformers library for NLP.",
    "url": "pages/glossary.html#term-hugging-face"
  },
  {
    "id": "term-human-evaluation",
    "title": "Human Evaluation",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "accuracy",
      "annotators",
      "assessing",
      "automated",
      "compare",
      "dimensions",
      "evaluation",
      "generated",
      "gold",
      "ground",
      "harmlessness",
      "helpfulness"
    ],
    "excerpt": "The gold standard for assessing language model outputs where human annotators rate or compare generated text on dimensions such as quality, accuracy, helpfulness, and harmlessness, providing ground truth for validating automated metrics.",
    "url": "pages/glossary.html#term-human-evaluation"
  },
  {
    "id": "term-human-mesh-recovery",
    "title": "Human Mesh Recovery",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "body",
      "computer",
      "estimating",
      "full",
      "geometry",
      "human",
      "image",
      "like",
      "mesh",
      "models",
      "parametric"
    ],
    "excerpt": "The task of estimating a full 3D human body mesh (shape and pose) from a single 2D image, using parametric body models like SMPL to represent body geometry.",
    "url": "pages/glossary.html#term-human-mesh-recovery"
  },
  {
    "id": "term-hitl",
    "title": "Human-in-the-Loop (HITL)",
    "category": "Glossary",
    "subcategory": "Practice",
    "keywords": [
      "acted",
      "approach",
      "approve",
      "design",
      "hitl",
      "human",
      "humans",
      "in",
      "loop",
      "modify",
      "outputs",
      "practice"
    ],
    "excerpt": "A design approach where humans review, approve, or modify AI outputs before they're acted upon. Critical for high-stakes decisions, quality control, and maintaining accountability.",
    "url": "pages/glossary.html#term-hitl"
  },
  {
    "id": "term-humaneval",
    "title": "HumanEval",
    "category": "Glossary",
    "subcategory": "Benchmark",
    "keywords": [
      "benchmark",
      "capabilities",
      "code",
      "evaluating",
      "evaluation",
      "generation",
      "humaneval",
      "llms"
    ],
    "excerpt": "A benchmark for evaluating code generation capabilities of LLMs. Contains programming problems with test cases, measuring how often generated code passes all tests.",
    "url": "pages/glossary.html#term-humaneval"
  },
  {
    "id": "term-hybrid-ai",
    "title": "Hybrid AI",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "ai",
      "approaches",
      "architecture",
      "combining",
      "design",
      "hybrid",
      "llms",
      "logic",
      "multiple",
      "networks",
      "neural",
      "reasoning"
    ],
    "excerpt": "Systems combining multiple AI approaches: neural networks with symbolic reasoning, LLMs with search, or ML with rule-based logic. Often more robust than pure approaches.",
    "url": "pages/glossary.html#term-hybrid-ai"
  },
  {
    "id": "term-hybrid-search",
    "title": "Hybrid Search",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "approach",
      "combines",
      "dense",
      "document",
      "generative",
      "hybrid",
      "keywordbased",
      "leveraging",
      "llm",
      "matching",
      "methods"
    ],
    "excerpt": "A retrieval approach that combines dense vector similarity search with traditional keyword-based (sparse) retrieval, leveraging the strengths of both methods for more robust document matching.",
    "url": "pages/glossary.html#term-hybrid-search"
  },
  {
    "id": "term-hyena",
    "title": "Hyena",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "achieve",
      "architecture",
      "attention",
      "competitive",
      "convolutions",
      "cost",
      "gating",
      "hyena",
      "implicit",
      "long",
      "lower",
      "multiplicative"
    ],
    "excerpt": "A subquadratic attention replacement that uses long convolutions parameterized by implicit neural representations and multiplicative gating to achieve competitive performance with transformers at lower cost.",
    "url": "pages/glossary.html#term-hyena"
  },
  {
    "id": "term-hyperband",
    "title": "Hyperband",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "allocating",
      "budget",
      "candidates",
      "combines",
      "configurations",
      "doubling",
      "early",
      "halving",
      "hyperband",
      "hyperparameter",
      "learning",
      "machine"
    ],
    "excerpt": "A hyperparameter optimization method that combines random search with early stopping, allocating more resources to promising configurations by successively halving the number of candidates and doubling the budget for survivors.",
    "url": "pages/glossary.html#term-hyperband"
  },
  {
    "id": "term-hypernymy",
    "title": "Hypernymy",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "animal",
      "anothers",
      "being",
      "databases",
      "dog",
      "forming",
      "general",
      "hierarchies",
      "hypernym",
      "hypernymy",
      "includes",
      "lexical"
    ],
    "excerpt": "A semantic relation where one word's meaning includes and is more general than another's, such as 'animal' being a hypernym of 'dog,' forming taxonomic hierarchies in lexical databases.",
    "url": "pages/glossary.html#term-hypernymy"
  },
  {
    "id": "term-hyperparameter",
    "title": "Hyperparameter",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "being",
      "configuration",
      "control",
      "data",
      "hyperparameter",
      "learned",
      "process",
      "rather",
      "settings",
      "training"
    ],
    "excerpt": "Configuration settings that control the training process rather than being learned from data. Examples include learning rate, batch size, number of layers, and dropout rate.",
    "url": "pages/glossary.html#term-hyperparameter"
  },
  {
    "id": "term-hyponymy",
    "title": "Hyponymy",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "animal",
      "anothers",
      "being",
      "dog",
      "hyponym",
      "hyponymy",
      "included",
      "isa",
      "linguistics",
      "meaning",
      "nlp",
      "one"
    ],
    "excerpt": "A semantic relation where one word's meaning is more specific than and included within another's, such as 'dog' being a hyponym of 'animal,' representing an is-a relationship.",
    "url": "pages/glossary.html#term-hyponymy"
  },
  {
    "id": "term-hypothesis",
    "title": "Hypothesis (ML)",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "algorithm",
      "concept",
      "considers",
      "function",
      "hypothesis",
      "learning",
      "ml",
      "model",
      "possible",
      "solution",
      "specific",
      "theory"
    ],
    "excerpt": "A specific function or model that the learning algorithm considers as a possible solution. The hypothesis space is all possible hypotheses the algorithm can choose from.",
    "url": "pages/glossary.html#term-hypothesis"
  },
  {
    "id": "term-hypothetical-document-embedding",
    "title": "Hypothetical Document Embedding",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "answer",
      "document",
      "documents",
      "embedding",
      "generates",
      "generative",
      "hyde",
      "hypothetical",
      "improving",
      "language",
      "llm"
    ],
    "excerpt": "A retrieval technique (HyDE) where a language model generates a hypothetical answer to a query, and the embedding of that answer is used to search for real documents, improving retrieval relevance.",
    "url": "pages/glossary.html#term-hypothetical-document-embedding"
  },
  {
    "id": "term-ian-goodfellow",
    "title": "Ian Goodfellow",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2014",
      "adversarial",
      "american",
      "compete",
      "computer",
      "data",
      "framework",
      "gans",
      "generate",
      "generative",
      "goodfellow",
      "history"
    ],
    "excerpt": "American computer scientist who invented generative adversarial networks (GANs) in 2014, introducing a framework where two neural networks compete to generate realistic synthetic data, revolutionizing generative AI.",
    "url": "pages/glossary.html#term-ian-goodfellow"
  },
  {
    "id": "term-ibm-watson-jeopardy",
    "title": "IBM Watson Jeopardy",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2011",
      "advances",
      "brad",
      "champions",
      "defeated",
      "demonstrating",
      "february",
      "history",
      "human",
      "ibm",
      "ibms",
      "information"
    ],
    "excerpt": "IBM's Watson AI system that defeated human champions Ken Jennings and Brad Rutter on the quiz show Jeopardy! in February 2011, demonstrating advances in natural language processing and information retrieval.",
    "url": "pages/glossary.html#term-ibm-watson-jeopardy"
  },
  {
    "id": "term-idiom-detection",
    "title": "Idiom Detection",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "bucket",
      "cannot",
      "deduced",
      "detection",
      "die",
      "expressions",
      "identifying",
      "idiom",
      "individual",
      "kick",
      "linguistics",
      "meaning"
    ],
    "excerpt": "The task of identifying non-compositional multi-word expressions whose meaning cannot be deduced from their individual words, such as 'kick the bucket' meaning 'to die.'",
    "url": "pages/glossary.html#term-idiom-detection"
  },
  {
    "id": "term-ieee-ai-ethics-standards",
    "title": "IEEE AI Ethics Standards",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "7000",
      "addressing",
      "ai",
      "algorithmic",
      "aligned",
      "autonomous",
      "bias",
      "data",
      "design",
      "developed",
      "ethically",
      "ethics"
    ],
    "excerpt": "A family of standards developed by IEEE under the Ethically Aligned Design initiative, including IEEE 7000 series standards addressing transparency, data privacy, algorithmic bias, and autonomous systems ethics.",
    "url": "pages/glossary.html#term-ieee-ai-ethics-standards"
  },
  {
    "id": "term-ifeval",
    "title": "IFEval",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "ability",
      "adherence",
      "benchmark",
      "benchmarks",
      "bullet",
      "constraints",
      "count",
      "evaluation",
      "follow",
      "following",
      "formatting",
      "ifeval"
    ],
    "excerpt": "Instruction Following Evaluation, a benchmark that tests language models' ability to follow specific verifiable formatting instructions such as word count constraints, bullet point requirements, an...",
    "url": "pages/glossary.html#term-ifeval"
  },
  {
    "id": "term-ilya-sutskever",
    "title": "Ilya Sutskever",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "alexnet",
      "chief",
      "codesigned",
      "cofounded",
      "history",
      "ilya",
      "openai",
      "pioneers",
      "researcher",
      "russianborn",
      "scientist",
      "served"
    ],
    "excerpt": "Russian-born AI researcher who co-founded OpenAI, served as its Chief Scientist, and co-designed AlexNet.",
    "url": "pages/glossary.html#term-ilya-sutskever"
  },
  {
    "id": "term-image-augmentation",
    "title": "Image Augmentation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "applying",
      "artificially",
      "augmentation",
      "color",
      "computer",
      "cropping",
      "datasets",
      "deformations",
      "elastic",
      "expand",
      "flipping",
      "generalization"
    ],
    "excerpt": "Techniques that artificially expand training datasets by applying random transformations to images, including rotation, flipping, cropping, color jittering, and elastic deformations, to improve model generalization.",
    "url": "pages/glossary.html#term-image-augmentation"
  },
  {
    "id": "term-image-classification",
    "title": "Image Classification",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "architectures",
      "assigning",
      "based",
      "categorical",
      "classification",
      "cnn",
      "computer",
      "content",
      "datasets",
      "entire",
      "fundamental",
      "image"
    ],
    "excerpt": "The fundamental computer vision task of assigning a categorical label to an entire image based on its visual content, typically using CNN or ViT architectures trained on labeled datasets.",
    "url": "pages/glossary.html#term-image-classification"
  },
  {
    "id": "term-image-colorization",
    "title": "Image Colorization",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "adding",
      "automatically",
      "color",
      "colorization",
      "colors",
      "computer",
      "conditioned",
      "content",
      "context",
      "deep",
      "distributions",
      "grayscale"
    ],
    "excerpt": "The task of automatically adding plausible colors to grayscale images using deep learning models that learn color distributions conditioned on visual content and semantic context.",
    "url": "pages/glossary.html#term-image-colorization"
  },
  {
    "id": "term-image-deblurring",
    "title": "Image Deblurring",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "blurred",
      "camera",
      "caused",
      "computer",
      "deblurring",
      "deep",
      "degradation",
      "functions",
      "image",
      "images",
      "inputs",
      "inverse"
    ],
    "excerpt": "The task of recovering sharp images from blurred inputs caused by camera shake or object motion, using deep learning models that learn inverse degradation functions.",
    "url": "pages/glossary.html#term-image-deblurring"
  },
  {
    "id": "term-image-denoising",
    "title": "Image Denoising",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "cleaner",
      "computer",
      "degraded",
      "denoising",
      "details",
      "fine",
      "image",
      "images",
      "learn",
      "networks",
      "neural",
      "noise"
    ],
    "excerpt": "The process of removing noise from degraded images using neural networks that learn to separate signal from noise, producing cleaner images while preserving fine details and textures.",
    "url": "pages/glossary.html#term-image-denoising"
  },
  {
    "id": "term-image-embedding",
    "title": "Image Embedding",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "capturing",
      "classification",
      "compact",
      "computer",
      "dense",
      "embedding",
      "encoder",
      "features",
      "form",
      "image",
      "network",
      "neural"
    ],
    "excerpt": "A dense vector representation of an image produced by a neural network encoder, capturing semantic and visual features in a compact form suitable for similarity search, classification, and retrieval.",
    "url": "pages/glossary.html#term-image-embedding"
  },
  {
    "id": "term-image-generation",
    "title": "Image Generation",
    "category": "Glossary",
    "subcategory": "Application",
    "keywords": [
      "application",
      "create",
      "descriptions",
      "generation",
      "generative",
      "image",
      "images",
      "inputs",
      "systems",
      "text"
    ],
    "excerpt": "AI systems that create images from text descriptions or other inputs. Major models include DALL-E, Midjourney, and Stable Diffusion, using diffusion or GAN architectures.",
    "url": "pages/glossary.html#term-image-generation"
  },
  {
    "id": "term-image-inpainting",
    "title": "Image Inpainting",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "completions",
      "computer",
      "content",
      "context",
      "deep",
      "filling",
      "generate",
      "image",
      "inpainting",
      "learning",
      "masked",
      "missing"
    ],
    "excerpt": "The task of filling in missing or masked regions of an image with plausible content, using deep learning models that understand context, texture, and structural patterns to generate seamless completions.",
    "url": "pages/glossary.html#term-image-inpainting"
  },
  {
    "id": "term-image-matting",
    "title": "Image Matting",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "accurate",
      "alpha",
      "computer",
      "defines",
      "edges",
      "elements",
      "enabling",
      "estimating",
      "extraction",
      "foreground",
      "fractional",
      "fur"
    ],
    "excerpt": "The task of estimating a precise alpha matte that defines the fractional opacity of foreground elements in an image, enabling accurate extraction of subjects with soft edges like hair and fur.",
    "url": "pages/glossary.html#term-image-matting"
  },
  {
    "id": "term-image-registration",
    "title": "Image Registration",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "aligning",
      "computer",
      "computing",
      "corresponding",
      "different",
      "image",
      "images",
      "maps",
      "points",
      "process",
      "processing",
      "registration"
    ],
    "excerpt": "The process of aligning two or more images of the same scene taken at different times, viewpoints, or by different sensors, by computing a spatial transformation that maps corresponding points.",
    "url": "pages/glossary.html#term-image-registration"
  },
  {
    "id": "term-image-retrieval",
    "title": "Image Retrieval",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "computer",
      "database",
      "embedding",
      "embeddings",
      "finding",
      "image",
      "images",
      "learned",
      "nearestneighbor",
      "processing",
      "query",
      "retrieval"
    ],
    "excerpt": "The task of finding images in a database that are visually similar to a query image, using learned embeddings and nearest-neighbor search in the embedding space.",
    "url": "pages/glossary.html#term-image-retrieval"
  },
  {
    "id": "term-image-stitching",
    "title": "Image Stitching",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "blending",
      "combining",
      "computer",
      "correcting",
      "differences",
      "estimating",
      "exposure",
      "homographies",
      "image",
      "multiple",
      "overlapping",
      "panoramic"
    ],
    "excerpt": "The process of combining multiple overlapping photographs into a single panoramic or wide-field image by estimating homographies, blending seams, and correcting exposure differences.",
    "url": "pages/glossary.html#term-image-stitching"
  },
  {
    "id": "term-image-upscaling",
    "title": "Image Upscaling",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "computer",
      "detailed",
      "details",
      "highfrequency",
      "image",
      "increasing",
      "interpolation",
      "methods",
      "models",
      "process",
      "processing",
      "producing"
    ],
    "excerpt": "The process of increasing image resolution using AI models that synthesize realistic high-frequency details, producing sharp, detailed results superior to traditional interpolation methods.",
    "url": "pages/glossary.html#term-image-upscaling"
  },
  {
    "id": "term-imagenet",
    "title": "ImageNet",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "across",
      "benchmark",
      "categories",
      "classification",
      "computer",
      "database",
      "dataset",
      "evaluating",
      "historically",
      "image",
      "imagenet",
      "images"
    ],
    "excerpt": "A large-scale visual database with over 14 million labeled images across thousands of categories, historically serving as the primary benchmark dataset for training and evaluating image classification models.",
    "url": "pages/glossary.html#term-imagenet"
  },
  {
    "id": "term-img2img",
    "title": "Img2Img",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "adds",
      "ai",
      "denoises",
      "editing",
      "enabling",
      "existing",
      "generation",
      "generative",
      "image",
      "imagetoimage",
      "img2img",
      "input"
    ],
    "excerpt": "An image-to-image generation pipeline that takes an existing image as input, adds noise to its latent representation, and denoises it with a new text prompt, enabling style transfer and editing.",
    "url": "pages/glossary.html#term-img2img"
  },
  {
    "id": "term-imitation-learning",
    "title": "Imitation Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "demonstrations",
      "error",
      "expert",
      "imitation",
      "learning",
      "learns",
      "observing",
      "paradigm",
      "perform",
      "rather",
      "reinforcement"
    ],
    "excerpt": "A paradigm where an agent learns to perform tasks by observing expert demonstrations rather than through reward-based trial and error.",
    "url": "pages/glossary.html#term-imitation-learning"
  },
  {
    "id": "term-implicit-neural-representation",
    "title": "Implicit Neural Representation",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "continuous",
      "coordinates",
      "function",
      "images",
      "implicit",
      "learns",
      "like",
      "mapping",
      "network",
      "networks",
      "neural"
    ],
    "excerpt": "A neural network that learns a continuous function mapping coordinates to signal values, representing signals like images, shapes, or scenes as the weights of a neural network.",
    "url": "pages/glossary.html#term-implicit-neural-representation"
  },
  {
    "id": "term-implicit-q-learning",
    "title": "Implicit Q-Learning (IQL)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "advantageweighted",
      "avoids",
      "expectile",
      "extracting",
      "function",
      "implicit",
      "iql",
      "learning",
      "method",
      "methods",
      "offline"
    ],
    "excerpt": "An offline RL method that avoids querying out-of-distribution actions by learning a value function using expectile regression and extracting a policy through advantage-weighted regression.",
    "url": "pages/glossary.html#term-implicit-q-learning"
  },
  {
    "id": "term-importance-sampling",
    "title": "Importance Sampling",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "carlo",
      "data",
      "different",
      "distribution",
      "easiertosample",
      "estimates",
      "importance",
      "likelihood",
      "monte",
      "one",
      "properties",
      "proposal"
    ],
    "excerpt": "A Monte Carlo technique that estimates properties of one distribution by sampling from a different, easier-to-sample proposal distribution and reweighting samples by the likelihood ratio between the target and proposal.",
    "url": "pages/glossary.html#term-importance-sampling"
  },
  {
    "id": "term-importance-sampling-rl",
    "title": "Importance Sampling in RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "behavior",
      "being",
      "concepts",
      "core",
      "correct",
      "data",
      "evaluated",
      "generated",
      "importance",
      "in",
      "learning",
      "mismatch"
    ],
    "excerpt": "A statistical technique used in off-policy RL to correct for the mismatch between the behavior policy that generated data and the target policy being evaluated.",
    "url": "pages/glossary.html#term-importance-sampling-rl"
  },
  {
    "id": "term-impossibility-theorem-of-fairness",
    "title": "Impossibility Theorem of Fairness",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "ai",
      "another",
      "cases",
      "certain",
      "criteria",
      "demonstrating",
      "ethics",
      "except",
      "fairness",
      "impossibility",
      "incompatible",
      "mathematical"
    ],
    "excerpt": "Mathematical results demonstrating that certain fairness criteria are mutually incompatible except in trivial cases, meaning that satisfying one fairness metric necessarily violates another.",
    "url": "pages/glossary.html#term-impossibility-theorem-of-fairness"
  },
  {
    "id": "term-imputation",
    "title": "Imputation",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "approaches",
      "data",
      "dataset",
      "estimated",
      "imputation",
      "iterative",
      "knearest",
      "learning",
      "like",
      "machine",
      "mean",
      "median"
    ],
    "excerpt": "The process of replacing missing values in a dataset with estimated values, using methods such as mean, median, mode substitution, k-nearest neighbors, or model-based approaches like iterative imputation.",
    "url": "pages/glossary.html#term-imputation"
  },
  {
    "id": "term-in-context-learning",
    "title": "In-Context Learning",
    "category": "Glossary",
    "subcategory": "Capability",
    "keywords": [
      "ability",
      "capability",
      "context",
      "examples",
      "in",
      "learn",
      "learning",
      "llms",
      "prompt",
      "prompting",
      "provided",
      "updating"
    ],
    "excerpt": "An LLM's ability to learn from examples provided in the prompt without updating its weights. Enables few-shot and zero-shot task performance through careful prompt design.",
    "url": "pages/glossary.html#term-in-context-learning"
  },
  {
    "id": "term-inception-network",
    "title": "Inception Network",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "capture",
      "cnn",
      "convolutional",
      "different",
      "features",
      "filters",
      "inception",
      "layer",
      "modules",
      "multiple",
      "network"
    ],
    "excerpt": "A CNN architecture that uses parallel convolutional filters of different sizes within the same layer (inception modules) to capture features at multiple scales simultaneously.",
    "url": "pages/glossary.html#term-inception-network"
  },
  {
    "id": "term-incremental-indexing",
    "title": "Incremental Indexing",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "adds",
      "compaction",
      "cost",
      "database",
      "enabling",
      "existing",
      "full",
      "incremental",
      "index",
      "indexing",
      "maintenance",
      "nearrealtime"
    ],
    "excerpt": "A vector database operation that adds new vectors to an existing index without requiring a full rebuild, enabling near-real-time updates at the cost of potentially suboptimal index structure that may need periodic compaction.",
    "url": "pages/glossary.html#term-incremental-indexing"
  },
  {
    "id": "term-independent-component-analysis",
    "title": "Independent Component Analysis",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "additive",
      "analysis",
      "component",
      "components",
      "computational",
      "dimensionality",
      "independent",
      "learning",
      "machine",
      "method",
      "multivariate",
      "nongaussian"
    ],
    "excerpt": "A computational method for separating a multivariate signal into additive, statistically independent non-Gaussian source components.",
    "url": "pages/glossary.html#term-independent-component-analysis"
  },
  {
    "id": "term-iql-marl",
    "title": "Independent Q-Learning (IQL-MARL)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "agents",
      "approach",
      "environment",
      "independent",
      "independently",
      "iql",
      "learning",
      "learns",
      "marl",
      "multiagent",
      "own"
    ],
    "excerpt": "A simple multi-agent RL approach where each agent independently learns its own Q-function treating other agents as part of the environment.",
    "url": "pages/glossary.html#term-iql-marl"
  },
  {
    "id": "term-index-optimization",
    "title": "Index Optimization",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "accuracy",
      "achieve",
      "balance",
      "clusters",
      "connectivity",
      "consumption",
      "database",
      "dataset",
      "graph",
      "index",
      "latency",
      "memory"
    ],
    "excerpt": "The process of tuning vector index parameters such as the number of clusters, graph connectivity, and quantization settings to achieve the optimal balance of query latency, recall accuracy, and memory consumption for a specific dataset and workload.",
    "url": "pages/glossary.html#term-index-optimization"
  },
  {
    "id": "term-index-refresh",
    "title": "Index Refresh",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "accuracy",
      "collection",
      "data",
      "database",
      "deleted",
      "evolves",
      "incorporate",
      "index",
      "maintain",
      "maintenance",
      "necessary",
      "new"
    ],
    "excerpt": "The process of rebuilding or updating a vector index to incorporate new vectors, remove deleted ones, and optimize search structures, necessary to maintain query accuracy and performance as the underlying data collection evolves.",
    "url": "pages/glossary.html#term-index-refresh"
  },
  {
    "id": "term-individual-conditional-expectation",
    "title": "Individual Conditional Expectation",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "across",
      "changes",
      "conditional",
      "data",
      "dependence",
      "disaggregating",
      "effects",
      "expectation",
      "feature",
      "heterogeneous",
      "individual",
      "instance"
    ],
    "excerpt": "A plot showing how the prediction for each individual instance changes as a feature varies, disaggregating the partial dependence plot to reveal heterogeneous effects and interaction patterns across observations.",
    "url": "pages/glossary.html#term-individual-conditional-expectation"
  },
  {
    "id": "term-individual-fairness",
    "title": "Individual Fairness",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "ai",
      "classifications",
      "close",
      "condition",
      "ethics",
      "fairness",
      "formalized",
      "individual",
      "individuals",
      "lipschitz",
      "metric",
      "outcomes"
    ],
    "excerpt": "A fairness principle requiring that similar individuals receive similar predictions or outcomes, formalized as a Lipschitz condition: individuals who are close in a task-relevant metric should receive similar classifications.",
    "url": "pages/glossary.html#term-individual-fairness"
  },
  {
    "id": "term-induction-head",
    "title": "Induction Head",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "attention",
      "continue",
      "forming",
      "generative",
      "head",
      "heads",
      "identify",
      "incontext",
      "induction",
      "input",
      "key"
    ],
    "excerpt": "A pair of attention heads in transformers that work together to identify and continue repeated patterns in the input sequence, forming a key mechanism underlying in-context learning.",
    "url": "pages/glossary.html#term-induction-head"
  },
  {
    "id": "term-inference",
    "title": "Inference",
    "category": "Glossary",
    "subcategory": "Process",
    "keywords": [
      "data",
      "generate",
      "inference",
      "model",
      "new",
      "outputs",
      "predictions",
      "process",
      "production",
      "running",
      "trained"
    ],
    "excerpt": "Running a trained model to generate predictions or outputs on new data. Distinguished from training, inference is typically faster and less resource-intensive.",
    "url": "pages/glossary.html#term-inference"
  },
  {
    "id": "term-inference-acceleration",
    "title": "Inference Acceleration",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "acceleration",
      "accelerators",
      "caching",
      "collection",
      "compiler",
      "hardware",
      "including",
      "inference",
      "infrastructure",
      "model",
      "network",
      "neural"
    ],
    "excerpt": "The collection of hardware and software techniques that speed up neural network inference, including specialized accelerators, compiler optimizations, quantization, pruning, and caching.",
    "url": "pages/glossary.html#term-inference-acceleration"
  },
  {
    "id": "term-inference-cost-optimization",
    "title": "Inference Cost Optimization",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "batching",
      "cost",
      "distillation",
      "financial",
      "hardware",
      "including",
      "inference",
      "infrastructure",
      "minimizing",
      "model",
      "models",
      "optimization"
    ],
    "excerpt": "Strategies for minimizing the financial cost of serving AI models at scale, including quantization, distillation, batching optimization, hardware selection, and traffic routing.",
    "url": "pages/glossary.html#term-inference-cost-optimization"
  },
  {
    "id": "term-inference-optimization",
    "title": "Inference Optimization",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "batching",
      "caching",
      "collection",
      "computational",
      "cost",
      "footprint",
      "including",
      "inference",
      "latency",
      "llm",
      "memory",
      "models"
    ],
    "excerpt": "A collection of techniques that reduce the computational cost, memory footprint, and latency of running trained models in production, including quantization, pruning, caching, and batching strategies.",
    "url": "pages/glossary.html#term-inference-optimization"
  },
  {
    "id": "term-infiniband",
    "title": "InfiniBand",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "400",
      "bandwidth",
      "clusters",
      "communication",
      "computing",
      "distributed",
      "gbs",
      "hardware",
      "high",
      "highspeed",
      "infiniband",
      "internode"
    ],
    "excerpt": "A high-speed networking technology widely used in AI supercomputer clusters for inter-node communication, offering low latency and high bandwidth (up to 400 Gb/s per port with NDR).",
    "url": "pages/glossary.html#term-infiniband"
  },
  {
    "id": "term-information-extraction",
    "title": "Information Extraction",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "automatically",
      "converting",
      "entities",
      "events",
      "extracting",
      "extraction",
      "freeform",
      "information",
      "knowledge",
      "nlp",
      "organized",
      "processing"
    ],
    "excerpt": "The task of automatically extracting structured information such as entities, relations, and events from unstructured text, converting free-form text into organized knowledge.",
    "url": "pages/glossary.html#term-information-extraction"
  },
  {
    "id": "term-information-gain",
    "title": "Information Gain",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "achieved",
      "dataset",
      "entropy",
      "feature",
      "gain",
      "information",
      "learning",
      "machine",
      "model",
      "particular",
      "reduction",
      "selection"
    ],
    "excerpt": "The reduction in entropy achieved by splitting a dataset on a particular feature. Decision tree algorithms like ID3 and C4.",
    "url": "pages/glossary.html#term-information-gain"
  },
  {
    "id": "term-informative-prior",
    "title": "Informative Prior",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bayesian",
      "beliefs",
      "data",
      "distribution",
      "encodes",
      "especially",
      "influencing",
      "informative",
      "knowledge",
      "likely",
      "limited",
      "methods"
    ],
    "excerpt": "A prior distribution that encodes specific prior knowledge or strong beliefs about a parameter's likely values, substantially influencing the posterior distribution, especially when data is limited.",
    "url": "pages/glossary.html#term-informative-prior"
  },
  {
    "id": "term-informed-consent-in-ai",
    "title": "Informed Consent in AI",
    "category": "Glossary",
    "subcategory": "Privacy",
    "keywords": [
      "agree",
      "ai",
      "clearly",
      "collected",
      "consent",
      "data",
      "ethical",
      "ethics",
      "in",
      "individuals",
      "informed",
      "legal"
    ],
    "excerpt": "The ethical and legal requirement that individuals be clearly informed about how their data will be collected, processed, and used by AI systems, and that they voluntarily agree to such use.",
    "url": "pages/glossary.html#term-informed-consent-in-ai"
  },
  {
    "id": "term-inner-alignment",
    "title": "Inner Alignment",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "alignment",
      "ensuring",
      "inner",
      "internal",
      "learned",
      "matches",
      "models",
      "objective",
      "optimization",
      "problem",
      "process"
    ],
    "excerpt": "The problem of ensuring that a learned model's internal optimization objective matches the objective specified by the training process.",
    "url": "pages/glossary.html#term-inner-alignment"
  },
  {
    "id": "term-inpainting-pipeline",
    "title": "Inpainting Pipeline",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "consistency",
      "diffusion",
      "generative",
      "image",
      "inpainting",
      "maintaining",
      "masked",
      "model",
      "object",
      "only",
      "pipeline"
    ],
    "excerpt": "A diffusion model workflow that regenerates only the masked portions of an image while maintaining consistency with the unmasked regions, used for object removal, replacement, and repair.",
    "url": "pages/glossary.html#term-inpainting-pipeline"
  },
  {
    "id": "term-instance-normalization",
    "title": "Instance Normalization",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "channel",
      "developed",
      "feature",
      "important",
      "independently",
      "instance",
      "networks",
      "neural",
      "normalization",
      "normalizes",
      "originally"
    ],
    "excerpt": "A normalization technique that normalizes each feature channel independently for each sample, originally developed for neural style transfer where per-instance statistics are important.",
    "url": "pages/glossary.html#term-instance-normalization"
  },
  {
    "id": "term-instance-segmentation",
    "title": "Instance Segmentation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "class",
      "combining",
      "computer",
      "detection",
      "detects",
      "distinguish",
      "generates",
      "image",
      "individual",
      "instance",
      "mask",
      "object"
    ],
    "excerpt": "A computer vision task that detects individual objects in an image and generates a pixel-level mask for each instance, combining object detection with semantic segmentation to distinguish separate objects of the same class.",
    "url": "pages/glossary.html#term-instance-segmentation"
  },
  {
    "id": "term-instant-ngp",
    "title": "Instant NGP",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "accelerates",
      "computer",
      "dramatically",
      "encoding",
      "graphics",
      "hash",
      "highquality",
      "hours",
      "instant",
      "maintaining",
      "multiresolution"
    ],
    "excerpt": "Instant Neural Graphics Primitives, a technique using multi-resolution hash encoding that dramatically accelerates NeRF training from hours to seconds while maintaining high-quality novel view synthesis.",
    "url": "pages/glossary.html#term-instant-ngp"
  },
  {
    "id": "term-instruct-model",
    "title": "Instruct Model",
    "category": "Glossary",
    "subcategory": "Model Type",
    "keywords": [
      "complete",
      "finetuned",
      "follow",
      "instruct",
      "instructions",
      "just",
      "llm",
      "model",
      "rather",
      "text",
      "training",
      "type"
    ],
    "excerpt": "An LLM fine-tuned to follow instructions rather than just complete text. Makes models more useful as assistants by teaching them to respond helpfully to user requests.",
    "url": "pages/glossary.html#term-instruct-model"
  },
  {
    "id": "term-instructgpt",
    "title": "InstructGPT",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2022",
      "align",
      "chatgpt",
      "development",
      "directly",
      "feedback",
      "gpt3",
      "history",
      "human",
      "informing",
      "instructgpt",
      "instructions"
    ],
    "excerpt": "An OpenAI model published in 2022 that used reinforcement learning from human feedback to align GPT-3 with user instructions, directly preceding and informing the development of ChatGPT.",
    "url": "pages/glossary.html#term-instructgpt"
  },
  {
    "id": "term-instruction-following",
    "title": "Instruction Following",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ability",
      "accurately",
      "ai",
      "capability",
      "developed",
      "execute",
      "feedback",
      "following",
      "generative",
      "human",
      "instruction",
      "instructions"
    ],
    "excerpt": "The ability of a language model to accurately interpret and execute natural language instructions, a capability developed through instruction tuning and reinforcement learning from human feedback.",
    "url": "pages/glossary.html#term-instruction-following"
  },
  {
    "id": "term-instruction-hierarchy",
    "title": "Instruction Hierarchy",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "approach",
      "attacks",
      "conflicting",
      "directives",
      "enabling",
      "engineering",
      "handle",
      "hierarchy",
      "injection",
      "instruction",
      "instructions",
      "level"
    ],
    "excerpt": "A structured approach to organizing prompt instructions by priority level, where system-level instructions take precedence over user-level instructions, enabling models to handle conflicting directives and resist prompt injection attacks.",
    "url": "pages/glossary.html#term-instruction-hierarchy"
  },
  {
    "id": "term-instruction-tuning",
    "title": "Instruction Tuning",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "ability",
      "alignment",
      "datasets",
      "finetuning",
      "follow",
      "improve",
      "instruction",
      "instructions",
      "llms",
      "requests",
      "responses",
      "training"
    ],
    "excerpt": "Fine-tuning LLMs on datasets of instructions and responses to improve their ability to follow user requests. A key technique for creating helpful AI assistants.",
    "url": "pages/glossary.html#term-instruction-tuning"
  },
  {
    "id": "term-instruction-tuning-alignment",
    "title": "Instruction Tuning Alignment",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "align",
      "alignment",
      "benefit",
      "conventions",
      "engineering",
      "finetuning",
      "format",
      "instruction",
      "instructionfollowing",
      "maximizing",
      "models",
      "optimization"
    ],
    "excerpt": "The practice of writing prompts that align with the specific instruction format and conventions used during a model's fine-tuning phase, maximizing the benefit of the model's instruction-following training.",
    "url": "pages/glossary.html#term-instruction-tuning-alignment"
  },
  {
    "id": "term-instruction-based-prompting",
    "title": "Instruction-Based Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "ability",
      "based",
      "demonstrations",
      "describing",
      "directives",
      "engineering",
      "examples",
      "explicit",
      "follow",
      "fundamentals",
      "imperative",
      "instruction"
    ],
    "excerpt": "A prompting paradigm that provides explicit, imperative instructions to a language model describing the task to perform, leveraging instruction-tuned models' ability to follow natural language directives without requiring demonstrations or examples.",
    "url": "pages/glossary.html#term-instruction-based-prompting"
  },
  {
    "id": "term-instrumental-convergence",
    "title": "Instrumental Convergence",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "acquisition",
      "advanced",
      "agents",
      "ai",
      "alignment",
      "certain",
      "converge",
      "convergence",
      "goalcontent",
      "goals",
      "instrumental",
      "integrity"
    ],
    "excerpt": "The thesis that sufficiently advanced AI agents with a wide range of terminal goals will converge on pursuing certain instrumental sub-goals such as self-preservation, resource acquisition, and goal-content integrity.",
    "url": "pages/glossary.html#term-instrumental-convergence"
  },
  {
    "id": "term-instrumental-variable",
    "title": "Instrumental Variable",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "affects",
      "causal",
      "confounding",
      "consistent",
      "correlated",
      "effects",
      "enabling",
      "estimation",
      "inference",
      "instrumental",
      "only",
      "outcome"
    ],
    "excerpt": "A variable used in causal inference that is correlated with the treatment variable but affects the outcome only through the treatment, enabling consistent estimation of causal effects in the presence of confounding.",
    "url": "pages/glossary.html#term-instrumental-variable"
  },
  {
    "id": "term-int4",
    "title": "INT4 Quantization",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "achieving",
      "aggressive",
      "bits",
      "compression",
      "fp32",
      "inference",
      "infrastructure",
      "int4",
      "model",
      "only",
      "optimization",
      "per"
    ],
    "excerpt": "An aggressive quantization scheme using only 4 bits per weight, achieving 8x compression over FP32. INT4 methods like GPTQ and AWQ use sophisticated calibration to minimize quality degradation despite the extremely low bit-width.",
    "url": "pages/glossary.html#term-int4"
  },
  {
    "id": "term-int8",
    "title": "INT8 Quantization",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "8bit",
      "activations",
      "compared",
      "floatingpoint",
      "fp32",
      "inference",
      "infrastructure",
      "instead",
      "int8",
      "integers",
      "memory",
      "model"
    ],
    "excerpt": "The process of representing neural network weights and activations using 8-bit integers instead of floating-point, reducing memory by 4x compared to FP32.",
    "url": "pages/glossary.html#term-int8"
  },
  {
    "id": "term-intel-gaudi-3",
    "title": "Intel Gaudi 3",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "400gbe",
      "accelerator",
      "bandwidth",
      "computing",
      "distributed",
      "featuring",
      "gaudi",
      "hardware",
      "high",
      "integrated",
      "intel",
      "intels"
    ],
    "excerpt": "Intel's third-generation AI training accelerator featuring integrated 400GbE networking and high memory bandwidth.",
    "url": "pages/glossary.html#term-intel-gaudi-3"
  },
  {
    "id": "term-intent-detection",
    "title": "Intent Detection",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "actions",
      "behind",
      "book",
      "cancel",
      "classifying",
      "detection",
      "determining",
      "dialogue",
      "goal",
      "intent",
      "nlp",
      "perform"
    ],
    "excerpt": "The task of classifying the purpose or goal behind a user's utterance in a dialogue system, determining whether the user wants to book, search, cancel, or perform other actions.",
    "url": "pages/glossary.html#term-intent-detection"
  },
  {
    "id": "term-intent-recognition",
    "title": "Intent Recognition",
    "category": "Glossary",
    "subcategory": "NLP Task",
    "keywords": [
      "accomplish",
      "application",
      "input",
      "intent",
      "nlp",
      "recognition",
      "task",
      "understanding",
      "user",
      "wants"
    ],
    "excerpt": "Understanding what a user wants to accomplish from their input. A core NLP task for chatbots and virtual assistants, mapping user messages to predefined intents.",
    "url": "pages/glossary.html#term-intent-recognition"
  },
  {
    "id": "term-inter-annotator-agreement",
    "title": "Inter-Annotator Agreement",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "agreement",
      "annotation",
      "annotator",
      "annotators",
      "assess",
      "data",
      "degree",
      "evaluating",
      "evaluation",
      "human",
      "independent",
      "inter"
    ],
    "excerpt": "A statistical measure of the degree to which independent human annotators make the same judgments when labeling or evaluating the same data, used to assess annotation reliability and task subjectivity.",
    "url": "pages/glossary.html#term-inter-annotator-agreement"
  },
  {
    "id": "term-inter-token-latency",
    "title": "Inter-Token Latency",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "consecutive",
      "decode",
      "generations",
      "inference",
      "infrastructure",
      "inter",
      "latency",
      "llm",
      "model",
      "optimization",
      "phase",
      "time"
    ],
    "excerpt": "The time between consecutive token generations during the decode phase of LLM inference. Low inter-token latency is essential for streaming applications where users perceive generation speed in real-time.",
    "url": "pages/glossary.html#term-inter-token-latency"
  },
  {
    "id": "term-interaction-feature",
    "title": "Interaction Feature",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "capture",
      "combining",
      "created",
      "derived",
      "effects",
      "engineering",
      "existing",
      "feature",
      "features",
      "interaction",
      "learning",
      "machine"
    ],
    "excerpt": "A derived feature created by combining two or more existing features (typically through multiplication) to capture non-additive effects.",
    "url": "pages/glossary.html#term-interaction-feature"
  },
  {
    "id": "term-interpretability",
    "title": "Interpretability",
    "category": "Glossary",
    "subcategory": "Property",
    "keywords": [
      "decisions",
      "degree",
      "humans",
      "interpretability",
      "makes",
      "model",
      "property",
      "trust",
      "understand"
    ],
    "excerpt": "The degree to which humans can understand how a model makes decisions. Higher interpretability enables debugging, trust-building, and identifying potential issues.",
    "url": "pages/glossary.html#term-interpretability"
  },
  {
    "id": "term-intersection-over-union",
    "title": "Intersection over Union",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "accuracy",
      "area",
      "bounding",
      "box",
      "computer",
      "computing",
      "detection",
      "divided",
      "evaluate",
      "ground",
      "intersection",
      "iou"
    ],
    "excerpt": "A metric (IoU) that measures the overlap between a predicted bounding box and a ground truth box by computing the area of their intersection divided by the area of their union, used to evaluate detection accuracy.",
    "url": "pages/glossary.html#term-intersection-over-union"
  },
  {
    "id": "term-intrinsic-motivation",
    "title": "Intrinsic Motivation",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "encourage",
      "environments",
      "exploration",
      "extrinsic",
      "generated",
      "independent",
      "internal",
      "intrinsic",
      "itself",
      "learning",
      "motivation"
    ],
    "excerpt": "An internal reward signal generated by the agent itself to encourage exploration, independent of the environment's extrinsic reward.",
    "url": "pages/glossary.html#term-intrinsic-motivation"
  },
  {
    "id": "term-inverse-rl",
    "title": "Inverse Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "behavior",
      "demonstrated",
      "expert",
      "explain",
      "function",
      "imitation",
      "inferring",
      "inverse",
      "learning",
      "objectives",
      "observed",
      "policy"
    ],
    "excerpt": "The problem of inferring an unknown reward function from observed expert behavior, recovering the objectives that explain the demonstrated policy.",
    "url": "pages/glossary.html#term-inverse-rl"
  },
  {
    "id": "term-inverse-reward-design",
    "title": "Inverse Reward Design",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "design",
      "designer",
      "designers",
      "framework",
      "function",
      "intent",
      "inverse",
      "learning",
      "likely",
      "literal",
      "meant",
      "objective"
    ],
    "excerpt": "A framework that treats the specified reward function as an observation of the designer's true intent rather than the literal objective, reasoning about what reward the designer likely meant.",
    "url": "pages/glossary.html#term-inverse-reward-design"
  },
  {
    "id": "term-inverse-scaling",
    "title": "Inverse Scaling",
    "category": "Glossary",
    "subcategory": "Research",
    "keywords": [
      "certain",
      "inverse",
      "larger",
      "models",
      "ones",
      "perform",
      "phenomenon",
      "research",
      "scaling",
      "smaller",
      "tasks",
      "worse"
    ],
    "excerpt": "When larger models perform worse on certain tasks than smaller ones. Discovered through research challenges, revealing unexpected behaviors as models scale up.",
    "url": "pages/glossary.html#term-inverse-scaling"
  },
  {
    "id": "term-inverse-transform-sampling",
    "title": "Inverse Transform Sampling",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "cumulative",
      "distribution",
      "function",
      "generating",
      "given",
      "inverse",
      "method",
      "probability",
      "random",
      "samples",
      "sampling",
      "statistics"
    ],
    "excerpt": "A method for generating random samples from any probability distribution given its inverse cumulative distribution function.",
    "url": "pages/glossary.html#term-inverse-transform-sampling"
  },
  {
    "id": "term-inverted-residual-block",
    "title": "Inverted Residual Block",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "applies",
      "architecture",
      "back",
      "block",
      "building",
      "channel",
      "connection",
      "convolution",
      "depthwise",
      "dimension",
      "expands",
      "inverted"
    ],
    "excerpt": "A building block in MobileNetV2 that expands the channel dimension with a pointwise convolution, applies depthwise convolution, then projects back to a narrow output with a residual connection.",
    "url": "pages/glossary.html#term-inverted-residual-block"
  },
  {
    "id": "term-iob-tagging",
    "title": "IOB Tagging",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "beginning",
      "chunk",
      "enabling",
      "entity",
      "identification",
      "inside",
      "iob",
      "labeling",
      "marks",
      "multiword",
      "named",
      "nlp"
    ],
    "excerpt": "A labeling scheme for sequence tagging that marks tokens as Inside, Outside, or Beginning of a named entity or chunk, enabling the identification of multi-word spans in text.",
    "url": "pages/glossary.html#term-iob-tagging"
  },
  {
    "id": "term-ip-adapter",
    "title": "IP-Adapter",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "adapter",
      "ai",
      "allowing",
      "crossattention",
      "decoupled",
      "diffusion",
      "enables",
      "features",
      "finetuning",
      "generation",
      "generative",
      "image"
    ],
    "excerpt": "An image prompt adapter for diffusion models that enables image-conditioned generation by injecting visual features through decoupled cross-attention, allowing style or subject transfer without fine-tuning.",
    "url": "pages/glossary.html#term-ip-adapter"
  },
  {
    "id": "term-ipo",
    "title": "IPO (Identity Preference Optimization)",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "alignment",
      "alternative",
      "doesnt",
      "dpo",
      "identity",
      "ipo",
      "learning",
      "model",
      "optimization",
      "preference",
      "reference",
      "require"
    ],
    "excerpt": "An alternative to DPO for preference learning that doesn't require a reference model. Simplifies the training process while maintaining alignment quality.",
    "url": "pages/glossary.html#term-ipo"
  },
  {
    "id": "term-iso-ai-standards",
    "title": "ISO AI Standards",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "1sc",
      "23894",
      "42001",
      "ai",
      "artificial",
      "developed",
      "governance",
      "including",
      "intelligence",
      "international",
      "iso",
      "isoiec"
    ],
    "excerpt": "International standards developed by ISO/IEC JTC 1/SC 42 for artificial intelligence, including ISO/IEC 42001 for AI management systems and ISO/IEC 23894 for AI risk management.",
    "url": "pages/glossary.html#term-iso-ai-standards"
  },
  {
    "id": "term-isolation-forest",
    "title": "Isolation Forest",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithm",
      "anomaly",
      "detection",
      "ensemblebased",
      "features",
      "forest",
      "isolates",
      "isolation",
      "learning",
      "machine",
      "model",
      "observations"
    ],
    "excerpt": "An ensemble-based anomaly detection algorithm that isolates observations by randomly selecting features and split values.",
    "url": "pages/glossary.html#term-isolation-forest"
  },
  {
    "id": "term-isotonic-calibration",
    "title": "Isotonic Calibration",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "calibrated",
      "calibration",
      "classifier",
      "fits",
      "function",
      "increasing",
      "isotonic",
      "learning",
      "machine",
      "map",
      "method",
      "model"
    ],
    "excerpt": "A non-parametric calibration method that fits an isotonic regression to map classifier scores to calibrated probabilities, using a monotonically increasing step function to preserve the ordering of predictions.",
    "url": "pages/glossary.html#term-isotonic-calibration"
  },
  {
    "id": "term-isotonic-regression",
    "title": "Isotonic Regression",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "constraint",
      "data",
      "errors",
      "fits",
      "function",
      "isotonic",
      "learning",
      "machine",
      "minimizing",
      "model",
      "monotonicity",
      "nondecreasing"
    ],
    "excerpt": "A non-parametric regression technique that fits a non-decreasing (or non-increasing) step function to the data, minimizing the sum of squared errors subject to the monotonicity constraint.",
    "url": "pages/glossary.html#term-isotonic-regression"
  },
  {
    "id": "term-iterated-amplification",
    "title": "Iterated Amplification",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "aligned",
      "alignment",
      "amplification",
      "amplified",
      "approach",
      "assistants",
      "building",
      "capable",
      "christiano",
      "complex",
      "decomposing"
    ],
    "excerpt": "An AI alignment approach proposed by Paul Christiano where a human overseer is amplified by decomposing complex tasks into simpler subtasks handled by AI assistants, iteratively building more capable aligned systems.",
    "url": "pages/glossary.html#term-iterated-amplification"
  },
  {
    "id": "term-iteration",
    "title": "Iteration (Prompting)",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "achieve",
      "attempts",
      "better",
      "iteration",
      "multiple",
      "practice",
      "prompting",
      "prompts",
      "refining",
      "results"
    ],
    "excerpt": "The practice of refining prompts through multiple attempts to achieve better results. Essential for getting the most out of AI, treating prompting as an iterative process.",
    "url": "pages/glossary.html#term-iteration"
  },
  {
    "id": "term-iterative-refinement-prompting",
    "title": "Iterative Refinement Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "across",
      "addressing",
      "back",
      "critique",
      "engineering",
      "fed",
      "identified",
      "improvement",
      "initial",
      "instructions",
      "iteration",
      "iterative"
    ],
    "excerpt": "A technique where the model's initial output is fed back with critique instructions for progressive improvement across multiple rounds, with each iteration addressing specific weaknesses identified in the previous version.",
    "url": "pages/glossary.html#term-iterative-refinement-prompting"
  },
  {
    "id": "term-ivf-index",
    "title": "IVF Index",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "cells",
      "clustering",
      "database",
      "dramatically",
      "file",
      "index",
      "inverted",
      "ivf",
      "kmeans",
      "nearest",
      "only",
      "partitions"
    ],
    "excerpt": "Inverted File Index, a vector search structure that partitions the vector space into Voronoi cells using k-means clustering and searches only the nearest cells at query time, trading recall for dramatically reduced search scope.",
    "url": "pages/glossary.html#term-ivf-index"
  },
  {
    "id": "term-jaccard-index",
    "title": "Jaccard Index",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "coefficient",
      "defined",
      "divided",
      "index",
      "intersection",
      "jaccard",
      "learning",
      "machine",
      "measuring",
      "metrics",
      "overlap",
      "sets"
    ],
    "excerpt": "A similarity coefficient measuring the overlap between two sets, defined as the size of their intersection divided by the size of their union. It ranges from 0 (no overlap) to 1 (identical sets).",
    "url": "pages/glossary.html#term-jaccard-index"
  },
  {
    "id": "term-jaccard-similarity",
    "title": "Jaccard Similarity",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "applied",
      "binary",
      "calculated",
      "commonly",
      "database",
      "divided",
      "document",
      "intersection",
      "jaccard",
      "measure",
      "metric",
      "overlap"
    ],
    "excerpt": "A set-based similarity metric calculated as the size of the intersection divided by the size of the union of two sets, commonly applied to binary vectors or token sets to measure overlap between document representations.",
    "url": "pages/glossary.html#term-jaccard-similarity"
  },
  {
    "id": "term-jackknife",
    "title": "Jackknife",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "across",
      "assess",
      "bias",
      "estimates",
      "estimator",
      "inference",
      "jackknife",
      "leaveoneout",
      "leaves",
      "method",
      "observation",
      "one"
    ],
    "excerpt": "A resampling method that systematically leaves out one observation at a time and recomputes the statistic, using the variation across leave-one-out estimates to assess bias and variance of the estimator.",
    "url": "pages/glossary.html#term-jackknife"
  },
  {
    "id": "term-jailbreak",
    "title": "Jailbreak",
    "category": "Glossary",
    "subcategory": "Security",
    "keywords": [
      "attempts",
      "bypass",
      "cleverly",
      "crafted",
      "jailbreak",
      "prompts",
      "restrictions",
      "risk",
      "safety",
      "security",
      "systems"
    ],
    "excerpt": "Attempts to bypass an AI system's safety restrictions through cleverly crafted prompts. A ongoing challenge for AI developers in maintaining safe behavior.",
    "url": "pages/glossary.html#term-jailbreak"
  },
  {
    "id": "term-japanese-fifth-generation-project",
    "title": "Japanese Fifth Generation Computer Project",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19821992",
      "applications",
      "computer",
      "computers",
      "develop",
      "fifth",
      "generation",
      "government",
      "history",
      "initiative",
      "japanese",
      "logic"
    ],
    "excerpt": "A 1982-1992 Japanese government initiative to develop massively parallel computers using logic programming for AI applications.",
    "url": "pages/glossary.html#term-japanese-fifth-generation-project"
  },
  {
    "id": "term-jax",
    "title": "JAX",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "automatic",
      "computing",
      "differentiation",
      "framework",
      "googles",
      "highperformance",
      "jax",
      "library",
      "numerical",
      "technical"
    ],
    "excerpt": "Google's library for high-performance numerical computing and automatic differentiation. Popular for ML research due to its composability and GPU/TPU support.",
    "url": "pages/glossary.html#term-jax"
  },
  {
    "id": "term-jax-framework",
    "title": "JAX Framework",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "across",
      "automatic",
      "clusters",
      "combines",
      "compilation",
      "computing",
      "differentiation",
      "distributed",
      "framework",
      "googles",
      "gpu",
      "jax"
    ],
    "excerpt": "Google's numerical computing framework that combines NumPy-like syntax with automatic differentiation, XLA compilation, and native support for SPMD parallelism across TPU and GPU clusters.",
    "url": "pages/glossary.html#term-jax-framework"
  },
  {
    "id": "term-jeffreys-prior",
    "title": "Jeffreys Prior",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bayesian",
      "derived",
      "distribution",
      "fisher",
      "information",
      "invariant",
      "jeffreys",
      "matrix",
      "methods",
      "noninformative",
      "prior",
      "reparametrization"
    ],
    "excerpt": "A non-informative prior distribution derived from the Fisher information matrix that is invariant under reparametrization. It provides a principled default prior when no prior knowledge is available.",
    "url": "pages/glossary.html#term-jeffreys-prior"
  },
  {
    "id": "term-jensen-shannon-divergence",
    "title": "Jensen-Shannon Divergence",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "average",
      "bounded",
      "computed",
      "derived",
      "distributions",
      "divergence",
      "jensen",
      "measure",
      "mixture",
      "probability",
      "shannon",
      "statistics"
    ],
    "excerpt": "A symmetric and bounded divergence measure derived from KL divergence, computed as the average KL divergence of two distributions from their mixture. It is always finite and ranges from 0 to log(2).",
    "url": "pages/glossary.html#term-jensen-shannon-divergence"
  },
  {
    "id": "term-john-holland",
    "title": "John Holland",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19292015",
      "adaptation",
      "algorithms",
      "american",
      "computation",
      "computationally",
      "computer",
      "demonstrating",
      "developed",
      "evolutionary",
      "field",
      "founding"
    ],
    "excerpt": "American computer scientist (1929-2015) who invented genetic algorithms and developed the Holland schema theorem, founding the field of evolutionary computation and demonstrating how adaptation could be computationally modeled.",
    "url": "pages/glossary.html#term-john-holland"
  },
  {
    "id": "term-john-hopfield",
    "title": "John Hopfield",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1982",
      "american",
      "applying",
      "associative",
      "concepts",
      "create",
      "history",
      "hopfield",
      "introduced",
      "john",
      "memory",
      "models"
    ],
    "excerpt": "American physicist who introduced Hopfield networks in 1982, applying concepts from statistical physics to create neural network models for associative memory.",
    "url": "pages/glossary.html#term-john-hopfield"
  },
  {
    "id": "term-john-mccarthy",
    "title": "John McCarthy",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19272011",
      "1955",
      "american",
      "artificial",
      "coined",
      "computer",
      "dartmouth",
      "formal",
      "history",
      "intelligence",
      "invented",
      "john"
    ],
    "excerpt": "American computer scientist (1927-2011) who coined the term artificial intelligence in 1955, organized the Dartmouth Workshop, invented the LISP programming language, and pioneered time-sharing and formal reasoning systems.",
    "url": "pages/glossary.html#term-john-mccarthy"
  },
  {
    "id": "term-john-searle",
    "title": "John Searle",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1980",
      "american",
      "argument",
      "challenging",
      "chinese",
      "computers",
      "distinguishing",
      "history",
      "john",
      "language",
      "minds",
      "notion"
    ],
    "excerpt": "American philosopher who proposed the Chinese Room argument in 1980, challenging the notion that computers can truly understand language or possess minds, distinguishing between strong AI and weak AI.",
    "url": "pages/glossary.html#term-john-searle"
  },
  {
    "id": "term-john-von-neumann",
    "title": "John von Neumann",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19031957",
      "architecture",
      "automata",
      "cellular",
      "computer",
      "computers",
      "concept",
      "contributions",
      "enabled",
      "foundational",
      "game",
      "generalpurpose"
    ],
    "excerpt": "Hungarian-American mathematician (1903-1957) who made foundational contributions to computer architecture, game theory, and cellular automata, and whose stored-program concept enabled the general-purpose computers necessary for AI.",
    "url": "pages/glossary.html#term-john-von-neumann"
  },
  {
    "id": "term-joseph-weizenbaum",
    "title": "Joseph Weizenbaum",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19232008",
      "became",
      "book",
      "computer",
      "created",
      "critic",
      "eliza",
      "ethical",
      "germanamerican",
      "his",
      "history",
      "human"
    ],
    "excerpt": "German-American computer scientist (1923-2008) who created ELIZA and later became a prominent critic of AI, warning about the ethical implications of machines simulating human interaction in his book Computer Power and Human Reason.",
    "url": "pages/glossary.html#term-joseph-weizenbaum"
  },
  {
    "id": "term-json",
    "title": "JSON (JavaScript Object Notation)",
    "category": "Glossary",
    "subcategory": "Format",
    "keywords": [
      "api",
      "commonly",
      "data",
      "format",
      "javascript",
      "json",
      "llms",
      "notation",
      "object",
      "output",
      "responses",
      "structured"
    ],
    "excerpt": "A structured data format commonly used for API responses and structured output from LLMs. Many AI models can generate valid JSON when requested.",
    "url": "pages/glossary.html#term-json"
  },
  {
    "id": "term-json-mode",
    "title": "JSON Mode",
    "category": "Glossary",
    "subcategory": "Feature",
    "keywords": [
      "api",
      "apis",
      "constrains",
      "feature",
      "json",
      "llm",
      "mode",
      "model",
      "output",
      "setting",
      "valid"
    ],
    "excerpt": "A setting in some LLM APIs that constrains the model to output valid JSON. Useful for structured data extraction and integration with applications.",
    "url": "pages/glossary.html#term-json-mode"
  },
  {
    "id": "term-json-mode-generation",
    "title": "JSON Mode",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "configuration",
      "constrains",
      "data",
      "finetuning",
      "generative",
      "grammarbased",
      "implemented",
      "inference",
      "json",
      "jsonformatted",
      "language"
    ],
    "excerpt": "An inference configuration that constrains a language model to produce only valid JSON output, typically implemented through grammar-based token masking or fine-tuning on JSON-formatted data.",
    "url": "pages/glossary.html#term-json-mode-generation"
  },
  {
    "id": "term-json-mode-prompting",
    "title": "JSON Mode Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "apilevel",
      "combined",
      "definitions",
      "enforcement",
      "engineering",
      "ensure",
      "exclusively",
      "format",
      "instructs",
      "json",
      "language",
      "machinereadable"
    ],
    "excerpt": "A prompting technique that instructs the language model to output responses exclusively in valid JSON format, often combined with schema definitions and API-level JSON mode enforcement to ensure machine-readable structured responses.",
    "url": "pages/glossary.html#term-json-mode-prompting"
  },
  {
    "id": "term-judea-pearl",
    "title": "Judea Pearl",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2011",
      "award",
      "bayesian",
      "causal",
      "computer",
      "frameworks",
      "his",
      "history",
      "inference",
      "israeliamerican",
      "judea",
      "networks"
    ],
    "excerpt": "Israeli-American computer scientist who pioneered Bayesian networks and causal inference in AI, receiving the 2011 Turing Award for his work on probabilistic and causal reasoning frameworks.",
    "url": "pages/glossary.html#term-judea-pearl"
  },
  {
    "id": "term-jupyter",
    "title": "Jupyter Notebook",
    "category": "Glossary",
    "subcategory": "Tools",
    "keywords": [
      "code",
      "combined",
      "computing",
      "development",
      "environment",
      "interactive",
      "jupyter",
      "notebook",
      "text",
      "tools",
      "visualizations"
    ],
    "excerpt": "An interactive computing environment where code, visualizations, and text can be combined. Widely used for data science, ML experimentation, and educational content.",
    "url": "pages/glossary.html#term-jupyter"
  },
  {
    "id": "term-jurgen-schmidhuber",
    "title": "Jurgen Schmidhuber",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "advocated",
      "coinvented",
      "computer",
      "contributed",
      "deep",
      "early",
      "germanswiss",
      "history",
      "jurgen",
      "learning",
      "lstm",
      "network"
    ],
    "excerpt": "German-Swiss computer scientist who co-invented LSTM networks, contributed to recurrent neural network research, and advocated for recognition of early neural network pioneers in the history of deep learning.",
    "url": "pages/glossary.html#term-jurgen-schmidhuber"
  },
  {
    "id": "term-k-fold",
    "title": "K-Fold Cross-Validation",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "cross",
      "crossvalidation",
      "data",
      "different",
      "divides",
      "equal",
      "evaluation",
      "fold",
      "part",
      "parts",
      "set",
      "technique"
    ],
    "excerpt": "A cross-validation technique that divides data into K equal parts, training K times with a different part as the test set each time. Provides robust performance estimates.",
    "url": "pages/glossary.html#term-k-fold"
  },
  {
    "id": "term-k-means",
    "title": "K-Means",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithm",
      "assigned",
      "assigning",
      "centroid",
      "centroids",
      "clustering",
      "clusters",
      "convergence",
      "iteratively",
      "learning",
      "machine",
      "mean"
    ],
    "excerpt": "An unsupervised clustering algorithm that partitions n observations into k clusters by iteratively assigning points to the nearest centroid and then updating centroids to be the mean of assigned points until convergence.",
    "url": "pages/glossary.html#term-k-means"
  },
  {
    "id": "term-k-nearest",
    "title": "K-Nearest Neighbors (KNN)",
    "category": "Glossary",
    "subcategory": "Algorithm",
    "keywords": [
      "algorithm",
      "based",
      "class",
      "classification",
      "classifies",
      "data",
      "knn",
      "majority",
      "nearest",
      "neighbors",
      "points",
      "simple"
    ],
    "excerpt": "A simple ML algorithm that classifies data points based on the majority class of their K nearest neighbors. Intuitive but can be slow for large datasets.",
    "url": "pages/glossary.html#term-k-nearest"
  },
  {
    "id": "term-k-nearest-neighbors-search",
    "title": "K-Nearest Neighbors Search",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "according",
      "core",
      "database",
      "databases",
      "distance",
      "embeddingbased",
      "forming",
      "metric",
      "nearest",
      "neighbors",
      "operation",
      "primitive"
    ],
    "excerpt": "A vector retrieval operation that returns the K vectors most similar to a query vector according to a specified distance metric, forming the core retrieval primitive for vector databases and embedding-based search systems.",
    "url": "pages/glossary.html#term-k-nearest-neighbors-search"
  },
  {
    "id": "term-kaplan-meier-estimator",
    "title": "Kaplan-Meier Estimator",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "accounting",
      "data",
      "estimate",
      "estimator",
      "function",
      "kaplan",
      "meier",
      "nonparametric",
      "observations",
      "rightcensored",
      "science",
      "statistic"
    ],
    "excerpt": "A non-parametric statistic used to estimate the survival function from time-to-event data, accounting for right-censored observations.",
    "url": "pages/glossary.html#term-kaplan-meier-estimator"
  },
  {
    "id": "term-kasparov-vs-deep-blue",
    "title": "Kasparov vs Deep Blue",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1997",
      "blue",
      "champion",
      "chess",
      "deep",
      "defeated",
      "garry",
      "historic",
      "history",
      "ibms",
      "kasparov",
      "milestones"
    ],
    "excerpt": "The historic 1997 rematch in which IBM's Deep Blue defeated world chess champion Garry Kasparov 3.5 to 2.",
    "url": "pages/glossary.html#term-kasparov-vs-deep-blue"
  },
  {
    "id": "term-kendall-tau",
    "title": "Kendall Tau",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "association",
      "computed",
      "concordant",
      "data",
      "discordant",
      "kendall",
      "measuring",
      "nonparametric",
      "number",
      "ordinal",
      "pairs",
      "rankings"
    ],
    "excerpt": "A non-parametric statistic measuring the ordinal association between two rankings, computed from the number of concordant and discordant pairs.",
    "url": "pages/glossary.html#term-kendall-tau"
  },
  {
    "id": "term-keras",
    "title": "Keras",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "accessible",
      "api",
      "building",
      "deep",
      "framework",
      "highlevel",
      "keras",
      "learning",
      "makes",
      "models",
      "network",
      "neural"
    ],
    "excerpt": "A high-level neural network API that makes building deep learning models more accessible. Now integrated into TensorFlow, known for its user-friendly design.",
    "url": "pages/glossary.html#term-keras"
  },
  {
    "id": "term-kernel",
    "title": "Kernel (ML)",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "algorithms",
      "concept",
      "data",
      "enabling",
      "function",
      "highdimensional",
      "kernel",
      "like",
      "math",
      "measures",
      "ml",
      "points"
    ],
    "excerpt": "A function that measures similarity between data points, enabling algorithms like SVMs to work in high-dimensional spaces. Common kernels include linear, polynomial, and RBF.",
    "url": "pages/glossary.html#term-kernel"
  },
  {
    "id": "term-kernel-auto-tuning",
    "title": "Kernel Auto-Tuning",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "auto",
      "automatically",
      "configurations",
      "gpu",
      "hardware",
      "implementations",
      "inference",
      "infrastructure",
      "kernel",
      "optimal",
      "process",
      "selecting"
    ],
    "excerpt": "The process of automatically selecting optimal GPU kernel implementations for specific tensor sizes and hardware configurations.",
    "url": "pages/glossary.html#term-kernel-auto-tuning"
  },
  {
    "id": "term-kernel-density-estimation",
    "title": "Kernel Density Estimation",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "data",
      "density",
      "estimating",
      "estimation",
      "function",
      "kernel",
      "method",
      "nonparametric",
      "placing",
      "probability",
      "random",
      "science"
    ],
    "excerpt": "A non-parametric method for estimating the probability density function of a random variable by placing a kernel (e.",
    "url": "pages/glossary.html#term-kernel-density-estimation"
  },
  {
    "id": "term-kernel-trick",
    "title": "Kernel Trick",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "computing",
      "data",
      "explicitly",
      "feature",
      "function",
      "highdimensional",
      "implicitly",
      "inner",
      "kernel",
      "learning",
      "machine",
      "maps"
    ],
    "excerpt": "A mathematical technique that implicitly maps data into a high-dimensional feature space by computing inner products via a kernel function, without explicitly performing the transformation.",
    "url": "pages/glossary.html#term-kernel-trick"
  },
  {
    "id": "term-key-value-cache",
    "title": "Key-Value Cache",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "autoregressive",
      "avoid",
      "cache",
      "computed",
      "generating",
      "inference",
      "key",
      "networks",
      "neural",
      "new",
      "optimization"
    ],
    "excerpt": "An optimization for autoregressive transformer inference that stores previously computed key and value tensors to avoid redundant recomputation when generating each new token.",
    "url": "pages/glossary.html#term-key-value-cache"
  },
  {
    "id": "term-keypoint-detection",
    "title": "Keypoint Detection",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "anatomical",
      "body",
      "computer",
      "corners",
      "detection",
      "facial",
      "heatmaps",
      "identifying",
      "image",
      "interest",
      "joints",
      "keypoint"
    ],
    "excerpt": "The task of identifying specific anatomical or structural points of interest in an image, such as body joints, facial landmarks, or object corners, typically predicting heatmaps for each keypoint location.",
    "url": "pages/glossary.html#term-keypoint-detection"
  },
  {
    "id": "term-keyword-extraction",
    "title": "Keyword Extraction",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "automatically",
      "document",
      "extraction",
      "graphbased",
      "identifying",
      "important",
      "keyword",
      "methods",
      "neural",
      "nlp",
      "phrases",
      "processing"
    ],
    "excerpt": "The task of automatically identifying the most important or representative words and phrases in a document, using statistical, graph-based, or neural methods.",
    "url": "pages/glossary.html#term-keyword-extraction"
  },
  {
    "id": "term-keyword-search",
    "title": "Keyword Search",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "algorithms",
      "based",
      "bm25",
      "documents",
      "frequency",
      "information",
      "keyword",
      "like",
      "matches",
      "meaning",
      "method",
      "presence"
    ],
    "excerpt": "A traditional information retrieval method that matches documents based on the presence and frequency of query terms, using algorithms like BM25 or TF-IDF to score relevance without requiring semantic understanding of meaning.",
    "url": "pages/glossary.html#term-keyword-search"
  },
  {
    "id": "term-kl-divergence",
    "title": "KL Divergence",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "differs",
      "distribution",
      "divergence",
      "kl",
      "kullbackleibler",
      "measure",
      "nonsymmetric",
      "one",
      "probability",
      "reference",
      "statistics"
    ],
    "excerpt": "Kullback-Leibler divergence, a non-symmetric measure of how one probability distribution differs from a reference distribution.",
    "url": "pages/glossary.html#term-kl-divergence"
  },
  {
    "id": "term-know-your-customer-for-ai",
    "title": "Know Your Customer for AI",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "accessing",
      "ai",
      "analogous",
      "capabilities",
      "cloud",
      "customer",
      "customers",
      "distributors",
      "financial",
      "for",
      "governance",
      "identity"
    ],
    "excerpt": "Proposed regulatory requirements for AI cloud providers and model distributors to verify the identity and intended use of customers accessing powerful AI capabilities, analogous to financial sector KYC rules.",
    "url": "pages/glossary.html#term-know-your-customer-for-ai"
  },
  {
    "id": "term-knowledge-cutoff",
    "title": "Knowledge Cutoff",
    "category": "Glossary",
    "subcategory": "Limitation",
    "keywords": [
      "beyond",
      "cutoff",
      "data",
      "date",
      "knowledge",
      "limitation",
      "llm",
      "model",
      "training"
    ],
    "excerpt": "The date beyond which an AI model has no training data. Events after this date are unknown to the model unless provided through context or retrieval augmentation.",
    "url": "pages/glossary.html#term-knowledge-cutoff"
  },
  {
    "id": "term-knowledge-distillation-efficiency",
    "title": "Knowledge Distillation for Efficiency",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "compression",
      "distillation",
      "efficiency",
      "for",
      "inference",
      "infrastructure",
      "knowledge",
      "larger",
      "mimic",
      "model",
      "optimization",
      "outputs"
    ],
    "excerpt": "A model compression technique where a smaller student model is trained to mimic the outputs (soft predictions) of a larger teacher model.",
    "url": "pages/glossary.html#term-knowledge-distillation-efficiency"
  },
  {
    "id": "term-knowledge-distillation-vision",
    "title": "Knowledge Distillation for Vision",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "compact",
      "computer",
      "deployment",
      "devices",
      "distillation",
      "edge",
      "efficient",
      "enabling",
      "feature",
      "for",
      "image",
      "knowledge"
    ],
    "excerpt": "The process of training a compact student vision model to mimic the predictions and feature representations of a larger teacher model, enabling efficient deployment on edge devices.",
    "url": "pages/glossary.html#term-knowledge-distillation-vision"
  },
  {
    "id": "term-knowledge-distillation-loss",
    "title": "Knowledge Distillation Loss",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "distillation",
      "distributions",
      "knowledge",
      "larger",
      "learns",
      "logits",
      "loss",
      "match",
      "model",
      "networks",
      "neural"
    ],
    "excerpt": "A training objective where a smaller student model learns to match the soft probability distributions produced by a larger teacher model, transferring knowledge through the teacher's output logits.",
    "url": "pages/glossary.html#term-knowledge-distillation-loss"
  },
  {
    "id": "term-knowledge-graph",
    "title": "Knowledge Graph",
    "category": "Glossary",
    "subcategory": "Data Structure",
    "keywords": [
      "data",
      "entities",
      "facts",
      "graph",
      "interconnected",
      "knowledge",
      "relationships",
      "representation",
      "structure",
      "structured"
    ],
    "excerpt": "A structured representation of facts as interconnected entities and relationships. Used to enhance AI systems with factual knowledge and enable reasoning over structured data.",
    "url": "pages/glossary.html#term-knowledge-graph"
  },
  {
    "id": "term-knowledge-representation",
    "title": "Knowledge Representation",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "computer",
      "concerned",
      "encompassing",
      "field",
      "form",
      "formally",
      "frames",
      "history",
      "information",
      "knowledge",
      "logic",
      "milestones"
    ],
    "excerpt": "The field within AI concerned with how information about the world can be formally represented in a form that computer systems can use for reasoning, planning, and problem-solving, encompassing logic, frames, semantic nets, and ontologies.",
    "url": "pages/glossary.html#term-knowledge-representation"
  },
  {
    "id": "term-kolmogorov-complexity",
    "title": "Kolmogorov Complexity",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "complexity",
      "computer",
      "content",
      "data",
      "given",
      "information",
      "intrinsic",
      "kolmogorov",
      "learning",
      "length",
      "machine",
      "output"
    ],
    "excerpt": "The length of the shortest computer program that produces a given string as output, representing the intrinsic information content of that string.",
    "url": "pages/glossary.html#term-kolmogorov-complexity"
  },
  {
    "id": "term-kolmogorov-smirnov-test",
    "title": "Kolmogorov-Smirnov Test",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "absolute",
      "compares",
      "cumulative",
      "difference",
      "distribution",
      "distributions",
      "functions",
      "inference",
      "kolmogorov",
      "maximum",
      "nonparametric",
      "onesample"
    ],
    "excerpt": "A non-parametric test that compares a sample distribution with a reference distribution (one-sample) or compares two sample distributions (two-sample) using the maximum absolute difference between cumulative distribution functions.",
    "url": "pages/glossary.html#term-kolmogorov-smirnov-test"
  },
  {
    "id": "term-krippendorffs-alpha",
    "title": "Krippendorff&#x27;s Alpha",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "agreement",
      "alpha",
      "among",
      "annotators",
      "applicable",
      "coefficient",
      "data",
      "evaluation",
      "interval",
      "krippendorffx27s",
      "levels",
      "measurement"
    ],
    "excerpt": "A versatile reliability coefficient that measures agreement among multiple annotators for any number of raters, variable scales, and missing data, applicable to nominal, ordinal, interval, and ratio measurement levels.",
    "url": "pages/glossary.html#term-krippendorffs-alpha"
  },
  {
    "id": "term-kto",
    "title": "KTO",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "behavioral",
      "binary",
      "comparisons",
      "economics",
      "feedback",
      "generative",
      "goodbad",
      "inspired",
      "kahnemantversky",
      "kto",
      "language"
    ],
    "excerpt": "Kahneman-Tversky Optimization, a preference learning method that trains language models using binary feedback (good/bad) rather than pairwise comparisons, inspired by prospect theory from behavioral economics.",
    "url": "pages/glossary.html#term-kto"
  },
  {
    "id": "term-kv-cache",
    "title": "KV Cache (Key-Value Cache)",
    "category": "Glossary",
    "subcategory": "Optimization",
    "keywords": [
      "cache",
      "computed",
      "key",
      "kv",
      "models",
      "optimization",
      "previously",
      "stores",
      "technical",
      "transformer",
      "value",
      "vectors"
    ],
    "excerpt": "An optimization that stores previously computed key and value vectors in transformer models. Speeds up autoregressive generation by avoiding redundant calculations.",
    "url": "pages/glossary.html#term-kv-cache"
  },
  {
    "id": "term-kv-cache-compression",
    "title": "KV Cache Compression",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "attention",
      "autoregressive",
      "cache",
      "cached",
      "caches",
      "compression",
      "eviction",
      "footprint",
      "generation",
      "groupedquery",
      "including",
      "inference"
    ],
    "excerpt": "Methods for reducing the memory footprint of key-value caches during autoregressive generation, including quantization of cached values, eviction policies, and grouped-query attention.",
    "url": "pages/glossary.html#term-kv-cache-compression"
  },
  {
    "id": "term-kv-cache-optimization",
    "title": "KV Cache Optimization",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "access",
      "attention",
      "cache",
      "caches",
      "cost",
      "eviction",
      "footprint",
      "groupedquery",
      "including",
      "inference",
      "infrastructure",
      "keyvalue"
    ],
    "excerpt": "Techniques for reducing the memory footprint and access cost of the key-value cache in transformer inference, including quantized KV caches, multi-query attention, grouped-query attention, and cache eviction policies.",
    "url": "pages/glossary.html#term-kv-cache-optimization"
  },
  {
    "id": "term-l1-regularization",
    "title": "L1 Regularization",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "absolute",
      "adds",
      "driving",
      "encouraging",
      "exactly",
      "function",
      "l1",
      "learning",
      "loss",
      "machine",
      "model",
      "optimization"
    ],
    "excerpt": "A regularization technique that adds the sum of absolute values of model weights to the loss function, encouraging sparsity by driving some weights exactly to zero. Also known as Lasso regularization.",
    "url": "pages/glossary.html#term-l1-regularization"
  },
  {
    "id": "term-l2-regularization",
    "title": "L2 Regularization",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "adds",
      "encouraging",
      "exactly",
      "function",
      "l2",
      "large",
      "learning",
      "loss",
      "machine",
      "model",
      "optimization",
      "penalizing"
    ],
    "excerpt": "A regularization technique that adds the sum of squared model weights to the loss function, penalizing large weights and encouraging them to be small but not exactly zero.",
    "url": "pages/glossary.html#term-l2-regularization"
  },
  {
    "id": "term-label",
    "title": "Label",
    "category": "Glossary",
    "subcategory": "Data",
    "keywords": [
      "answer",
      "associated",
      "category",
      "correct",
      "data",
      "label",
      "learning",
      "supervised",
      "training"
    ],
    "excerpt": "The correct answer or category associated with training data in supervised learning. Human-provided labels teach models the patterns they should learn.",
    "url": "pages/glossary.html#term-label"
  },
  {
    "id": "term-label-encoding",
    "title": "Label Encoding",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "assigning",
      "categorical",
      "category",
      "codes",
      "converts",
      "distinct",
      "encoding",
      "engineering",
      "feature",
      "identifier",
      "integer",
      "label"
    ],
    "excerpt": "A technique that converts categorical values into integer codes, assigning each unique category a distinct numerical identifier.",
    "url": "pages/glossary.html#term-label-encoding"
  },
  {
    "id": "term-label-smoothing",
    "title": "Label Smoothing",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "assign",
      "classes",
      "hard",
      "incorrect",
      "label",
      "labels",
      "learning",
      "machine",
      "onehot",
      "optimization",
      "probability",
      "regularization"
    ],
    "excerpt": "A regularization technique that replaces hard one-hot target labels with soft labels that assign a small probability to incorrect classes.",
    "url": "pages/glossary.html#term-label-smoothing"
  },
  {
    "id": "term-label-smoothing-cv",
    "title": "Label Smoothing",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "assign",
      "classes",
      "classification",
      "computer",
      "generalization",
      "hard",
      "image",
      "improving",
      "incorrect",
      "label",
      "labels",
      "onehot"
    ],
    "excerpt": "A regularization technique that replaces hard one-hot classification labels with soft labels that assign small probabilities to incorrect classes, preventing overconfident predictions and improving generalization.",
    "url": "pages/glossary.html#term-label-smoothing-cv"
  },
  {
    "id": "term-lamb-optimizer",
    "title": "LAMB Optimizer",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "adam",
      "adaptation",
      "adaptive",
      "applies",
      "batch",
      "computing",
      "distributed",
      "enable",
      "lamb",
      "large",
      "layerwise",
      "learning"
    ],
    "excerpt": "Layer-wise Adaptive Moments optimizer for Batch training, a variant of Adam that applies layer-wise learning rate adaptation to enable stable training at very large batch sizes.",
    "url": "pages/glossary.html#term-lamb-optimizer"
  },
  {
    "id": "term-lancedb",
    "title": "LanceDB",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "built",
      "cloudnative",
      "columnar",
      "data",
      "database",
      "deployment",
      "diskbased",
      "efficient",
      "embedded",
      "format",
      "indexing",
      "lance"
    ],
    "excerpt": "An open-source serverless vector database built on the Lance columnar data format, supporting multimodal data storage with embedded and cloud-native deployment options and efficient disk-based indexing without requiring a separate server process.",
    "url": "pages/glossary.html#term-lancedb"
  },
  {
    "id": "term-lane-detection",
    "title": "Lane Detection",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "anchorbased",
      "applications",
      "autonomous",
      "computer",
      "curve",
      "detection",
      "driving",
      "fitting",
      "identifying",
      "image",
      "images",
      "lane"
    ],
    "excerpt": "The task of identifying and localizing lane markings on road surfaces in driving images or video, using curve fitting, segmentation, or anchor-based methods for autonomous driving applications.",
    "url": "pages/glossary.html#term-lane-detection"
  },
  {
    "id": "term-langchain",
    "title": "LangChain",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "application",
      "applications",
      "building",
      "framework",
      "langchain",
      "llms",
      "popular"
    ],
    "excerpt": "A popular framework for building applications with LLMs. Provides abstractions for chains, agents, memory, and tool use, simplifying complex AI application development.",
    "url": "pages/glossary.html#term-langchain"
  },
  {
    "id": "term-language-identification",
    "title": "Language Identification",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "automatically",
      "character",
      "detection",
      "determining",
      "features",
      "frequency",
      "given",
      "identification",
      "language",
      "like",
      "natural",
      "ngrams"
    ],
    "excerpt": "The task of automatically determining what natural language a given text is written in, using features like character n-grams, word frequency patterns, and script detection.",
    "url": "pages/glossary.html#term-language-identification"
  },
  {
    "id": "term-language-modeling",
    "title": "Language Modeling",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "distribution",
      "enabling",
      "estimate",
      "given",
      "language",
      "learning",
      "likelihood",
      "model",
      "modeling",
      "next",
      "nlp",
      "predict"
    ],
    "excerpt": "The task of learning a probability distribution over sequences of tokens, enabling the model to estimate the likelihood of a given text sequence or predict the next token in a sequence.",
    "url": "pages/glossary.html#term-language-modeling"
  },
  {
    "id": "term-language-understanding",
    "title": "Language Understanding (NLU)",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "capability",
      "comprehend",
      "context",
      "human",
      "intent",
      "language",
      "meaning",
      "nlp",
      "nlu",
      "understanding"
    ],
    "excerpt": "AI capability to comprehend the meaning, intent, and context of human language. Includes parsing structure, resolving references, and understanding implicit information.",
    "url": "pages/glossary.html#term-language-understanding"
  },
  {
    "id": "term-laplace-approximation",
    "title": "Laplace Approximation",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "approximating",
      "approximation",
      "bayesian",
      "centered",
      "covariance",
      "curvature",
      "determine",
      "distribution",
      "estimate",
      "gaussian",
      "hessian",
      "laplace"
    ],
    "excerpt": "A technique for approximating a posterior distribution with a Gaussian centered at the mode (MAP estimate), using the curvature of the log-posterior (Hessian) to determine the covariance.",
    "url": "pages/glossary.html#term-laplace-approximation"
  },
  {
    "id": "term-large-batch-training",
    "title": "Large Batch Training",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "across",
      "batch",
      "computing",
      "distributed",
      "gpus",
      "large",
      "many",
      "millions",
      "model",
      "networks",
      "neural",
      "optimization"
    ],
    "excerpt": "Techniques for training neural networks with very large batch sizes (thousands to millions of samples) distributed across many GPUs.",
    "url": "pages/glossary.html#term-large-batch-training"
  },
  {
    "id": "term-llm",
    "title": "Large Language Model (LLM)",
    "category": "Glossary",
    "subcategory": "Model Type",
    "keywords": [
      "amounts",
      "concept",
      "core",
      "data",
      "generate",
      "human",
      "language",
      "large",
      "llm",
      "massive",
      "model",
      "system"
    ],
    "excerpt": "An AI system trained on massive amounts of text data to understand and generate human language. Includes models like GPT-4, Claude, Gemini, and Llama with billions of parameters.",
    "url": "pages/glossary.html#term-llm"
  },
  {
    "id": "term-lars-optimizer",
    "title": "LARS Optimizer",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "adaptive",
      "adjusts",
      "based",
      "computing",
      "distributed",
      "gradient",
      "lars",
      "layer",
      "layerwise",
      "learning",
      "model",
      "norm"
    ],
    "excerpt": "Layer-wise Adaptive Rate Scaling, an optimizer that adjusts the learning rate per layer based on the ratio of weight norm to gradient norm.",
    "url": "pages/glossary.html#term-lars-optimizer"
  },
  {
    "id": "term-lasso-regression",
    "title": "Lasso Regression",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "applies",
      "coefficient",
      "coefficients",
      "driving",
      "estimates",
      "exactly",
      "lasso",
      "learning",
      "linear",
      "machine",
      "method",
      "model"
    ],
    "excerpt": "A linear regression method that applies L1 regularization to the coefficient estimates, performing both variable selection and regularization by driving some coefficients to exactly zero.",
    "url": "pages/glossary.html#term-lasso-regression"
  },
  {
    "id": "term-late-chunking",
    "title": "Late Chunking",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "chunking",
      "chunklevel",
      "contextual",
      "crosschunk",
      "document",
      "embedding",
      "embeddings",
      "encodes",
      "entire",
      "first",
      "generative"
    ],
    "excerpt": "A technique that first encodes an entire document through a long-context embedding model and then pools token embeddings into chunk-level representations, preserving cross-chunk contextual information.",
    "url": "pages/glossary.html#term-late-chunking"
  },
  {
    "id": "term-late-interaction",
    "title": "Late Interaction",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "architecture",
      "balancing",
      "computed",
      "documents",
      "effectiveness",
      "efficiency",
      "embeddings",
      "encoded",
      "independently",
      "interaction",
      "late",
      "lightweight"
    ],
    "excerpt": "A neural retrieval paradigm where queries and documents are independently encoded into sets of token-level embeddings, and relevance is computed through lightweight interaction operations like MaxS...",
    "url": "pages/glossary.html#term-late-interaction"
  },
  {
    "id": "term-latency",
    "title": "Latency",
    "category": "Glossary",
    "subcategory": "Performance",
    "keywords": [
      "delay",
      "latency",
      "metrics",
      "performance",
      "prompt",
      "receiving",
      "response",
      "sending",
      "time"
    ],
    "excerpt": "The time delay between sending a prompt and receiving a response. Affected by model size, server load, prompt complexity, and output length.",
    "url": "pages/glossary.html#term-latency"
  },
  {
    "id": "term-latent-diffusion-model",
    "title": "Latent Diffusion Model",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "autoencoder",
      "computational",
      "diffusion",
      "generation",
      "latent",
      "maintaining",
      "model",
      "networks",
      "neural",
      "operates",
      "pixel"
    ],
    "excerpt": "A diffusion model that operates in the latent space of a pretrained autoencoder rather than pixel space, significantly reducing computational requirements while maintaining generation quality.",
    "url": "pages/glossary.html#term-latent-diffusion-model"
  },
  {
    "id": "term-latent-dirichlet-allocation",
    "title": "Latent Dirichlet Allocation",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "allocation",
      "dirichlet",
      "distribution",
      "distributions",
      "document",
      "generative",
      "latent",
      "learning",
      "machine",
      "mixture",
      "model",
      "modeling"
    ],
    "excerpt": "A generative probabilistic model for topic modeling that represents each document as a mixture of topics and each topic as a distribution over words, using Dirichlet priors for both distributions.",
    "url": "pages/glossary.html#term-latent-dirichlet-allocation"
  },
  {
    "id": "term-lda",
    "title": "Latent Dirichlet Allocation",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "allocation",
      "bayesian",
      "dirichlet",
      "discovered",
      "distribution",
      "document",
      "generative",
      "inference",
      "latent",
      "mixture",
      "model",
      "modeling"
    ],
    "excerpt": "A generative probabilistic model for topic modeling that represents each document as a mixture of topics and each topic as a distribution over words, discovered through Bayesian inference.",
    "url": "pages/glossary.html#term-lda"
  },
  {
    "id": "term-latin-hypercube-sampling",
    "title": "Latin Hypercube Sampling",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "achieving",
      "coverage",
      "data",
      "dimension",
      "divides",
      "ensures",
      "equalprobability",
      "even",
      "exactly",
      "hypercube",
      "interval",
      "intervals"
    ],
    "excerpt": "A stratified sampling technique that divides each dimension into equal-probability intervals and ensures each interval is sampled exactly once, achieving more even coverage of the sample space than simple random sampling.",
    "url": "pages/glossary.html#term-latin-hypercube-sampling"
  },
  {
    "id": "term-law-of-large-numbers",
    "title": "Law of Large Numbers",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "average",
      "converges",
      "expected",
      "increases",
      "independent",
      "large",
      "law",
      "number",
      "numbers",
      "of",
      "probability",
      "sample"
    ],
    "excerpt": "A theorem stating that as the number of independent trials increases, the sample average converges to the expected value.",
    "url": "pages/glossary.html#term-law-of-large-numbers"
  },
  {
    "id": "term-layer-normalization",
    "title": "Layer Normalization",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "across",
      "batch",
      "computes",
      "example",
      "features",
      "layer",
      "learning",
      "machine",
      "making",
      "mean",
      "normalization",
      "optimization"
    ],
    "excerpt": "A normalization technique that computes mean and variance across all features within a single training example rather than across the batch, making it suitable for variable-length sequences and small batch sizes.",
    "url": "pages/glossary.html#term-layer-normalization"
  },
  {
    "id": "term-layout-analysis",
    "title": "Layout Analysis",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "analysis",
      "classifying",
      "computer",
      "content",
      "detecting",
      "document",
      "elements",
      "establishing",
      "figures",
      "headers",
      "hierarchical",
      "image"
    ],
    "excerpt": "The process of detecting and classifying structural elements in document images (headers, paragraphs, tables, figures), establishing the reading order and hierarchical organization of content.",
    "url": "pages/glossary.html#term-layout-analysis"
  },
  {
    "id": "term-learned-positional-embedding",
    "title": "Learned Positional Embedding",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "allowing",
      "architecture",
      "assigns",
      "discover",
      "embedding",
      "learnable",
      "learned",
      "model",
      "networks",
      "neural",
      "optimal",
      "position"
    ],
    "excerpt": "A trainable embedding table that assigns a learnable vector to each position in a sequence, allowing the model to discover optimal position representations during training.",
    "url": "pages/glossary.html#term-learned-positional-embedding"
  },
  {
    "id": "term-learning-curve",
    "title": "Learning Curve",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "curve",
      "function",
      "iterations",
      "learning",
      "machine",
      "model",
      "performance",
      "plot",
      "selection",
      "set",
      "showing",
      "size"
    ],
    "excerpt": "A plot showing model performance as a function of training set size or training iterations. It reveals whether a model suffers from high bias (underfitting) or high variance (overfitting) and guides data collection decisions.",
    "url": "pages/glossary.html#term-learning-curve"
  },
  {
    "id": "term-learning-rate",
    "title": "Learning Rate",
    "category": "Glossary",
    "subcategory": "Hyperparameter",
    "keywords": [
      "adjusted",
      "controlling",
      "hyperparameter",
      "learning",
      "model",
      "much",
      "rate",
      "training",
      "weights"
    ],
    "excerpt": "A hyperparameter controlling how much model weights are adjusted during training. Too high causes instability; too low causes slow training. Often scheduled to decrease over time.",
    "url": "pages/glossary.html#term-learning-rate"
  },
  {
    "id": "term-learning-rate-schedule",
    "title": "Learning Rate Schedule",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "adjusting",
      "annealing",
      "cosine",
      "decay",
      "exponential",
      "learning",
      "machine",
      "optimization",
      "predefined",
      "rate",
      "schedule",
      "step"
    ],
    "excerpt": "A predefined strategy for adjusting the learning rate during training, such as step decay, exponential decay, or cosine annealing.",
    "url": "pages/glossary.html#term-learning-rate-schedule"
  },
  {
    "id": "term-learning-rate-warmup",
    "title": "Learning Rate Warmup",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "computing",
      "distributed",
      "first",
      "gradually",
      "increases",
      "learning",
      "model",
      "nearzero",
      "optimization",
      "portion",
      "rate",
      "target"
    ],
    "excerpt": "A training technique that gradually increases the learning rate from near-zero to the target value over the first portion of training.",
    "url": "pages/glossary.html#term-learning-rate-warmup"
  },
  {
    "id": "term-least-to-most-decomposition",
    "title": "Least-to-Most Decomposition",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "broken",
      "building",
      "complex",
      "decomposition",
      "difficult",
      "easier",
      "engineering",
      "first",
      "least",
      "leasttomost",
      "most",
      "ones"
    ],
    "excerpt": "The first stage of least-to-most prompting where a complex problem is broken into a sequence of progressively more difficult sub-problems, with each sub-problem building on the solutions of easier preceding ones.",
    "url": "pages/glossary.html#term-least-to-most-decomposition"
  },
  {
    "id": "term-leave-one-out-cross-validation",
    "title": "Leave-One-Out Cross-Validation",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "cross",
      "crossvalidation",
      "form",
      "learning",
      "leave",
      "machine",
      "method",
      "model",
      "observation",
      "observations",
      "one",
      "out"
    ],
    "excerpt": "A cross-validation method where each observation serves as a single-element test set while all remaining observations form the training set.",
    "url": "pages/glossary.html#term-leave-one-out-cross-validation"
  },
  {
    "id": "term-lemmatization",
    "title": "Lemmatization",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "analysis",
      "base",
      "create",
      "dictionary",
      "form",
      "lemma",
      "lemmatization",
      "lookup",
      "morphological",
      "nlp",
      "nonwords",
      "process"
    ],
    "excerpt": "The process of reducing words to their dictionary base form (lemma) using morphological analysis and vocabulary lookup, producing valid words unlike stemming which may create non-words.",
    "url": "pages/glossary.html#term-lemmatization"
  },
  {
    "id": "term-lenet",
    "title": "LeNet",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1989",
      "banks",
      "convolutional",
      "deep",
      "demonstrating",
      "deployed",
      "designed",
      "digit",
      "handwritten",
      "history",
      "learning",
      "lecun"
    ],
    "excerpt": "A pioneering convolutional neural network designed by Yann LeCun in 1989 for handwritten digit recognition, successfully deployed by the US Postal Service and banks, demonstrating the practical viability of deep learning.",
    "url": "pages/glossary.html#term-lenet"
  },
  {
    "id": "term-length-penalty",
    "title": "Length Penalty",
    "category": "Glossary",
    "subcategory": "Generation",
    "keywords": [
      "discourages",
      "encourages",
      "generation",
      "length",
      "longer",
      "outputs",
      "parameter",
      "penalty",
      "text"
    ],
    "excerpt": "A parameter in text generation that discourages or encourages longer outputs. Helps control verbosity and can be adjusted to match desired response length.",
    "url": "pages/glossary.html#term-length-penalty"
  },
  {
    "id": "term-lethal-autonomous-weapons-systems",
    "title": "Lethal Autonomous Weapons Systems",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "autonomous",
      "called",
      "capable",
      "class",
      "engaging",
      "ethics",
      "human",
      "identifying",
      "independently",
      "killer",
      "lethal"
    ],
    "excerpt": "A class of autonomous weapons, sometimes called killer robots, capable of independently identifying and lethally engaging human targets.",
    "url": "pages/glossary.html#term-lethal-autonomous-weapons-systems"
  },
  {
    "id": "term-levenshtein-distance",
    "title": "Levenshtein Distance",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "another",
      "checking",
      "deletions",
      "distance",
      "fuzzy",
      "insertions",
      "levenshtein",
      "matching",
      "measuring",
      "metric",
      "minimum",
      "needed"
    ],
    "excerpt": "A string metric measuring the minimum number of single-character insertions, deletions, and substitutions needed to transform one string into another, used in spell checking and fuzzy matching.",
    "url": "pages/glossary.html#term-levenshtein-distance"
  },
  {
    "id": "term-leverage",
    "title": "Leverage",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "center",
      "data",
      "far",
      "leverage",
      "measure",
      "observations",
      "predictor",
      "science",
      "space",
      "statistics",
      "values"
    ],
    "excerpt": "A measure of how far an observation's predictor values are from the center of the predictor space. High-leverage points have an outsized potential to influence the regression fit, even if they are not outliers in the response.",
    "url": "pages/glossary.html#term-leverage"
  },
  {
    "id": "term-lexical-ambiguity",
    "title": "Lexical Ambiguity",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "ambiguity",
      "context",
      "determine",
      "due",
      "homonymy",
      "intended",
      "interpreted",
      "lexical",
      "linguistics",
      "meaning",
      "multiple",
      "nlp"
    ],
    "excerpt": "The phenomenon where a word or phrase can be interpreted in multiple ways due to polysemy or homonymy, requiring context to determine the intended meaning.",
    "url": "pages/glossary.html#term-lexical-ambiguity"
  },
  {
    "id": "term-lidar",
    "title": "LiDAR",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "autonomous",
      "clouds",
      "computer",
      "dense",
      "detection",
      "distances",
      "driving",
      "illuminating",
      "laser",
      "lidar",
      "light"
    ],
    "excerpt": "Light Detection and Ranging, a remote sensing technology that measures distances by illuminating targets with laser pulses, producing dense 3D point clouds used in autonomous driving and mapping.",
    "url": "pages/glossary.html#term-lidar"
  },
  {
    "id": "term-lightgbm",
    "title": "LightGBM",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithms",
      "boosting",
      "datasets",
      "faster",
      "framework",
      "gradient",
      "growth",
      "histogrambased",
      "large",
      "leafwise",
      "learning",
      "lightgbm"
    ],
    "excerpt": "A gradient boosting framework that uses histogram-based algorithms and leaf-wise tree growth for faster training on large datasets.",
    "url": "pages/glossary.html#term-lightgbm"
  },
  {
    "id": "term-lighthill-report",
    "title": "Lighthill Report",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1973",
      "achieve",
      "ambitious",
      "british",
      "commissioned",
      "council",
      "criticized",
      "cuts",
      "failing",
      "funding",
      "goals",
      "history"
    ],
    "excerpt": "A 1973 report by mathematician James Lighthill commissioned by the British Science Research Council that criticized AI research as failing to achieve its ambitious goals, leading to severe funding cuts in the UK.",
    "url": "pages/glossary.html#term-lighthill-report"
  },
  {
    "id": "term-likelihood-function",
    "title": "Likelihood Function",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "computed",
      "data",
      "function",
      "given",
      "inference",
      "likelihood",
      "model",
      "observed",
      "parameter",
      "parameters",
      "probability",
      "statistical"
    ],
    "excerpt": "A function of the parameters of a statistical model, computed as the probability of the observed data for given parameter values.",
    "url": "pages/glossary.html#term-likelihood-function"
  },
  {
    "id": "term-likelihood-ratio-test",
    "title": "Likelihood Ratio Test",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "compares",
      "computing",
      "difference",
      "fit",
      "hypothesis",
      "inference",
      "likelihood",
      "loglikelihoods",
      "models",
      "nested",
      "ratio",
      "statistics"
    ],
    "excerpt": "A hypothesis test that compares the fit of two nested models by computing twice the difference in their log-likelihoods.",
    "url": "pages/glossary.html#term-likelihood-ratio-test"
  },
  {
    "id": "term-lime",
    "title": "LIME",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "around",
      "data",
      "explains",
      "explanations",
      "fitting",
      "individual",
      "instance",
      "interest",
      "interpretable",
      "learning",
      "lime",
      "local"
    ],
    "excerpt": "Local Interpretable Model-agnostic Explanations, a technique that explains individual predictions by fitting a simple interpretable model to perturbed samples around the instance of interest, weighted by proximity.",
    "url": "pages/glossary.html#term-lime"
  },
  {
    "id": "term-linear-attention",
    "title": "Linear Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "achieving",
      "approximation",
      "architecture",
      "attention",
      "complexity",
      "dotproduct",
      "kernelbased",
      "length",
      "linear",
      "memory",
      "networks",
      "neural"
    ],
    "excerpt": "An attention variant that replaces the softmax-based dot-product attention with a kernel-based approximation, achieving linear time and memory complexity with respect to sequence length.",
    "url": "pages/glossary.html#term-linear-attention"
  },
  {
    "id": "term-linear-discriminant-analysis",
    "title": "Linear Discriminant Analysis",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "analysis",
      "betweenclass",
      "classification",
      "data",
      "dimensionality",
      "directions",
      "discriminant",
      "discriminative",
      "feature",
      "finding",
      "learning",
      "linear"
    ],
    "excerpt": "A supervised dimensionality reduction and classification technique that projects data onto directions that maximize the ratio of between-class variance to within-class variance, finding the most discriminative feature subspace.",
    "url": "pages/glossary.html#term-linear-discriminant-analysis"
  },
  {
    "id": "term-linear-function-approximation-rl",
    "title": "Linear Function Approximation in RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "approximation",
      "combination",
      "estimation",
      "feature",
      "features",
      "function",
      "in",
      "learning",
      "linear",
      "methods",
      "reinforcement",
      "rl"
    ],
    "excerpt": "Value function estimation using a linear combination of state features, where the value is a weighted sum of feature values.",
    "url": "pages/glossary.html#term-linear-function-approximation-rl"
  },
  {
    "id": "term-linear-regression",
    "title": "Linear Regression",
    "category": "Glossary",
    "subcategory": "Algorithm",
    "keywords": [
      "algorithm",
      "foundational",
      "fundamentals",
      "line",
      "linear",
      "models",
      "regression",
      "relationship",
      "straight",
      "variables"
    ],
    "excerpt": "A foundational ML algorithm that models the relationship between variables using a straight line. Simple but effective for many prediction tasks with linear relationships.",
    "url": "pages/glossary.html#term-linear-regression"
  },
  {
    "id": "term-linformer",
    "title": "Linformer",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "complexity",
      "computing",
      "key",
      "length",
      "linear",
      "linformer",
      "lowerdimensional",
      "matrices",
      "networks",
      "neural"
    ],
    "excerpt": "A transformer that projects key and value matrices to a lower-dimensional space before computing attention, reducing the quadratic complexity of self-attention to linear in sequence length.",
    "url": "pages/glossary.html#term-linformer"
  },
  {
    "id": "term-lisp",
    "title": "LISP",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1958",
      "became",
      "collection",
      "decades",
      "dominant",
      "expression",
      "family",
      "featuring",
      "functions",
      "garbage",
      "history",
      "invented"
    ],
    "excerpt": "A programming language family invented by John McCarthy in 1958 that became the dominant language for AI research for decades, featuring symbolic expression processing, garbage collection, and recursive functions.",
    "url": "pages/glossary.html#term-lisp"
  },
  {
    "id": "term-llama",
    "title": "Llama",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "family",
      "language",
      "large",
      "llama",
      "meta",
      "metas",
      "model",
      "models",
      "openweight"
    ],
    "excerpt": "Meta's open-weight family of large language models. Released with relatively permissive licenses, enabling widespread research and commercial use of capable open models.",
    "url": "pages/glossary.html#term-llama"
  },
  {
    "id": "term-llama-cpp",
    "title": "llama.cpp",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "cpu",
      "efficient",
      "gpu",
      "inference",
      "infrastructure",
      "language",
      "large",
      "library",
      "llamacpp",
      "model",
      "models",
      "opensource"
    ],
    "excerpt": "An open-source C/C++ library for efficient CPU and GPU inference of large language models using quantized weights.",
    "url": "pages/glossary.html#term-llama-cpp"
  },
  {
    "id": "term-llamaindex",
    "title": "LlamaIndex",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "application",
      "connecting",
      "data",
      "external",
      "framework",
      "llamaindex",
      "llms",
      "sources"
    ],
    "excerpt": "A data framework for connecting LLMs to external data sources. Specializes in indexing, retrieval, and RAG applications with various data connectors and query engines.",
    "url": "pages/glossary.html#term-llamaindex"
  },
  {
    "id": "term-llm-as-judge",
    "title": "LLM-as-Judge",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "approximates",
      "as",
      "assess",
      "criteria",
      "evaluation",
      "explicit",
      "human",
      "judge",
      "judgment",
      "language",
      "large",
      "llm"
    ],
    "excerpt": "An evaluation paradigm where a large language model is prompted to assess and score the quality of outputs from other models, providing scalable evaluation that approximates human judgment with explicit rating criteria and rubrics.",
    "url": "pages/glossary.html#term-llm-as-judge"
  },
  {
    "id": "term-llmlingua",
    "title": "LLMLingua",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "achieving",
      "budgetaware",
      "compression",
      "downstream",
      "engineering",
      "framework",
      "identify",
      "informative",
      "iterative",
      "language",
      "less",
      "llmlingua"
    ],
    "excerpt": "A prompt compression framework that uses a small language model to identify and remove less informative tokens from prompts, achieving significant compression ratios while maintaining downstream ta...",
    "url": "pages/glossary.html#term-llmlingua"
  },
  {
    "id": "term-load-balancing-loss",
    "title": "Load Balancing Loss",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "across",
      "architecture",
      "auxiliary",
      "balancing",
      "collapse",
      "distribution",
      "encourage",
      "experts",
      "few",
      "inputs",
      "load",
      "loss"
    ],
    "excerpt": "An auxiliary loss term used in mixture-of-experts models to encourage uniform distribution of tokens across experts, preventing routing collapse where all inputs are sent to few experts.",
    "url": "pages/glossary.html#term-load-balancing-loss"
  },
  {
    "id": "term-local-llm",
    "title": "Local LLM",
    "category": "Glossary",
    "subcategory": "Deployment",
    "keywords": [
      "apis",
      "cloud",
      "deployment",
      "hardware",
      "language",
      "llm",
      "local",
      "models",
      "personal",
      "privacy",
      "rather",
      "running"
    ],
    "excerpt": "Running language models on personal hardware rather than through cloud APIs. Enables privacy, offline use, and cost savings, though requires capable hardware.",
    "url": "pages/glossary.html#term-local-llm"
  },
  {
    "id": "term-locality-sensitive-hashing",
    "title": "Locality-Sensitive Hashing",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "approximate",
      "buckets",
      "comparing",
      "database",
      "enabling",
      "hash",
      "hashes",
      "hashing",
      "high",
      "index",
      "locality",
      "matching"
    ],
    "excerpt": "An approximate nearest neighbor technique that hashes similar vectors into the same buckets with high probability using random projections, enabling sub-linear search time by only comparing vectors within matching hash buckets.",
    "url": "pages/glossary.html#term-locality-sensitive-hashing"
  },
  {
    "id": "term-loess",
    "title": "LOESS",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "adapts",
      "curve",
      "data",
      "estimated",
      "fits",
      "local",
      "locally",
      "loess",
      "method",
      "nonparametric",
      "patterns",
      "polynomial"
    ],
    "excerpt": "Locally Estimated Scatterplot Smoothing, a non-parametric regression method that fits local weighted polynomial regressions to subsets of the data, producing a smooth curve that adapts to local patterns.",
    "url": "pages/glossary.html#term-loess"
  },
  {
    "id": "term-log-loss",
    "title": "Log Loss",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "binary",
      "classification",
      "function",
      "given",
      "labels",
      "learning",
      "log",
      "loglikelihood",
      "loss",
      "machine",
      "measures",
      "metrics"
    ],
    "excerpt": "A loss function for binary classification that measures the negative log-likelihood of the true labels given the predicted probabilities. It heavily penalizes confident but incorrect predictions.",
    "url": "pages/glossary.html#term-log-loss"
  },
  {
    "id": "term-log-likelihood",
    "title": "Log-Likelihood",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "because",
      "converts",
      "function",
      "inference",
      "likelihood",
      "log",
      "logarithm",
      "natural",
      "optimization",
      "probabilities",
      "products",
      "simplify"
    ],
    "excerpt": "The natural logarithm of the likelihood function, used to simplify optimization because it converts products of probabilities into sums.",
    "url": "pages/glossary.html#term-log-likelihood"
  },
  {
    "id": "term-log-normal-distribution",
    "title": "Log-Normal Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "continuous",
      "distributed",
      "distribution",
      "log",
      "logarithm",
      "normal",
      "normally",
      "probability",
      "random",
      "statistics",
      "variable",
      "whose"
    ],
    "excerpt": "A continuous probability distribution of a random variable whose logarithm is normally distributed. It is used to model quantities that are products of many independent positive random variables.",
    "url": "pages/glossary.html#term-log-normal-distribution"
  },
  {
    "id": "term-logic-theorist",
    "title": "Logic Theorist",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1956",
      "allen",
      "artificial",
      "considered",
      "first",
      "herbert",
      "history",
      "intelligence",
      "logic",
      "mathematica",
      "mathematical",
      "milestones"
    ],
    "excerpt": "A program written by Allen Newell and Herbert Simon in 1956 that could prove mathematical theorems from Principia Mathematica, widely considered the first artificial intelligence program.",
    "url": "pages/glossary.html#term-logic-theorist"
  },
  {
    "id": "term-logistic-regression",
    "title": "Logistic Regression",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "applied",
      "class",
      "classification",
      "combination",
      "features",
      "function",
      "input",
      "learning",
      "linear",
      "logistic",
      "machine",
      "model"
    ],
    "excerpt": "A linear classification model that predicts class probabilities using the logistic (sigmoid) function applied to a linear combination of input features.",
    "url": "pages/glossary.html#term-logistic-regression"
  },
  {
    "id": "term-logit",
    "title": "Logit",
    "category": "Glossary",
    "subcategory": "Technical",
    "keywords": [
      "converting",
      "logit",
      "math",
      "model",
      "output",
      "probabilities",
      "raw",
      "scores",
      "technical",
      "unnormalized"
    ],
    "excerpt": "The raw, unnormalized scores output by a model before converting to probabilities. In LLMs, logits represent the model's preference for each possible next token.",
    "url": "pages/glossary.html#term-logit"
  },
  {
    "id": "term-logit-bias",
    "title": "Logit Bias",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "adds",
      "ai",
      "bias",
      "decoding",
      "encourage",
      "fixed",
      "generated",
      "generative",
      "logit",
      "logits",
      "output",
      "particular"
    ],
    "excerpt": "A technique that adds fixed values to the logits of specific tokens before sampling, used to encourage or suppress particular words or phrases in generated output.",
    "url": "pages/glossary.html#term-logit-bias"
  },
  {
    "id": "term-long-context-fine-tuning",
    "title": "Long Context Fine-Tuning",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "adapting",
      "ai",
      "context",
      "continued",
      "effectively",
      "fine",
      "generative",
      "interpolation",
      "llm",
      "long",
      "longer",
      "model"
    ],
    "excerpt": "The process of adapting a pre-trained model to effectively utilize longer context windows than it was originally trained on, through continued training with progressively longer sequences and position interpolation.",
    "url": "pages/glossary.html#term-long-context-fine-tuning"
  },
  {
    "id": "term-lstm-history",
    "title": "Long Short-Term Memory",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1997",
      "advances",
      "architecture",
      "effective",
      "enabling",
      "gradient",
      "history",
      "hochreiter",
      "invented",
      "jurgen",
      "language",
      "learning"
    ],
    "excerpt": "A recurrent neural network architecture invented by Sepp Hochreiter and Jurgen Schmidhuber in 1997 that solved the vanishing gradient problem, enabling effective learning from long sequences and powering advances in speech and language processing.",
    "url": "pages/glossary.html#term-lstm-history"
  },
  {
    "id": "term-longformer",
    "title": "Longformer",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "combines",
      "documents",
      "efficient",
      "enabling",
      "global",
      "local",
      "longformer",
      "networks",
      "neural",
      "processing"
    ],
    "excerpt": "A transformer variant that combines local sliding window attention with task-specific global attention on selected tokens, enabling efficient processing of documents with thousands of tokens.",
    "url": "pages/glossary.html#term-longformer"
  },
  {
    "id": "term-lora",
    "title": "LoRA (Low-Rank Adaptation)",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "adaptation",
      "additional",
      "efficiency",
      "finetuning",
      "full",
      "lora",
      "low",
      "matrices",
      "model",
      "only",
      "parameterefficient",
      "rank"
    ],
    "excerpt": "A parameter-efficient fine-tuning technique that trains only small additional matrices rather than the full model. Dramatically reduces memory and compute requirements for customization.",
    "url": "pages/glossary.html#term-lora"
  },
  {
    "id": "term-lora-diffusion",
    "title": "LoRA for Diffusion",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "adaptation",
      "additional",
      "ai",
      "application",
      "concepts",
      "diffusion",
      "efficient",
      "enabling",
      "finetuning",
      "for",
      "generation",
      "generative"
    ],
    "excerpt": "The application of Low-Rank Adaptation to diffusion models, enabling efficient fine-tuning of image generation models to learn new concepts, styles, or subjects with minimal additional parameters.",
    "url": "pages/glossary.html#term-lora-diffusion"
  },
  {
    "id": "term-lora-fusion",
    "title": "LoRA Fusion",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "adapter",
      "adapters",
      "ai",
      "combining",
      "different",
      "dynamically",
      "fusion",
      "generative",
      "inference",
      "llm",
      "lora",
      "merging"
    ],
    "excerpt": "The technique of combining multiple LoRA adapters trained for different tasks or styles into a single model by merging or dynamically weighting the adapter parameters during inference.",
    "url": "pages/glossary.html#term-lora-fusion"
  },
  {
    "id": "term-loss-function",
    "title": "Loss Function",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "function",
      "loss",
      "math",
      "mathematical",
      "measuring",
      "models",
      "predictions",
      "training",
      "wrong"
    ],
    "excerpt": "A mathematical function measuring how wrong a model's predictions are. Training aims to minimize this loss, with common examples including cross-entropy and mean squared error.",
    "url": "pages/glossary.html#term-loss-function"
  },
  {
    "id": "term-loss-scaling",
    "title": "Loss Scaling",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "backpropagation",
      "factor",
      "fp16",
      "gradient",
      "hardware",
      "large",
      "loss",
      "mixed",
      "model",
      "multiplies",
      "optimization",
      "precision"
    ],
    "excerpt": "A technique used in FP16 mixed precision training that multiplies the loss by a large factor before backpropagation to prevent small gradient values from underflowing to zero.",
    "url": "pages/glossary.html#term-loss-scaling"
  },
  {
    "id": "term-lost-in-the-middle",
    "title": "Lost in the Middle",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "beginning",
      "compared",
      "context",
      "documented",
      "end",
      "generative",
      "in",
      "information",
      "language",
      "llm",
      "lost"
    ],
    "excerpt": "A documented phenomenon where language models perform worse at retrieving and using information placed in the middle of their context window compared to information at the beginning or end.",
    "url": "pages/glossary.html#term-lost-in-the-middle"
  },
  {
    "id": "term-lotfi-zadeh",
    "title": "Lotfi Zadeh",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19212017",
      "1965",
      "azerbaijaniamerican",
      "computer",
      "control",
      "fuzzy",
      "handling",
      "history",
      "imprecision",
      "invented",
      "logic",
      "lotfi"
    ],
    "excerpt": "Azerbaijani-American mathematician and computer scientist (1921-2017) who invented fuzzy logic and fuzzy set theory in 1965, providing mathematical tools for handling uncertainty and imprecision in AI and control systems.",
    "url": "pages/glossary.html#term-lotfi-zadeh"
  },
  {
    "id": "term-lottery-ticket-hypothesis",
    "title": "Lottery Ticket Hypothesis",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "contain",
      "dense",
      "full",
      "hypothesis",
      "initialized",
      "isolation",
      "lottery",
      "match",
      "networks",
      "neural",
      "original"
    ],
    "excerpt": "The theory that dense randomly-initialized networks contain sparse subnetworks (winning tickets) that can be trained in isolation to match the full network's performance when initialized with their original weights.",
    "url": "pages/glossary.html#term-lottery-ticket-hypothesis"
  },
  {
    "id": "term-low-rank-factorization",
    "title": "Low-Rank Factorization",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "approximates",
      "compression",
      "factorization",
      "inference",
      "infrastructure",
      "large",
      "low",
      "matrices",
      "model",
      "optimization",
      "products",
      "rank"
    ],
    "excerpt": "A model compression technique that approximates large weight matrices as products of smaller matrices with reduced rank.",
    "url": "pages/glossary.html#term-low-rank-factorization"
  },
  {
    "id": "term-lstm",
    "title": "LSTM (Long Short-Term Memory)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "capture",
      "dependencies",
      "designed",
      "historical",
      "long",
      "longrange",
      "lstm",
      "memory",
      "network",
      "neural",
      "recurrent"
    ],
    "excerpt": "A recurrent neural network architecture designed to capture long-range dependencies in sequences. Was the dominant NLP architecture before transformers emerged.",
    "url": "pages/glossary.html#term-lstm"
  },
  {
    "id": "term-machine-learning",
    "title": "Machine Learning (ML)",
    "category": "Glossary",
    "subcategory": "Field",
    "keywords": [
      "being",
      "branch",
      "data",
      "explicitly",
      "field",
      "fundamentals",
      "learn",
      "learning",
      "machine",
      "ml",
      "patterns",
      "programmed"
    ],
    "excerpt": "A branch of AI where systems learn patterns from data rather than being explicitly programmed. Includes supervised, unsupervised, and reinforcement learning approaches.",
    "url": "pages/glossary.html#term-machine-learning"
  },
  {
    "id": "term-mt-evaluation",
    "title": "Machine Translation Evaluation",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "assessing",
      "automatic",
      "bleu",
      "comet",
      "compare",
      "evaluation",
      "human",
      "including",
      "like",
      "machine",
      "meteor",
      "methods"
    ],
    "excerpt": "Methods for assessing translation quality including automatic metrics like BLEU, METEOR, and COMET that compare system output to reference translations, and human evaluation protocols.",
    "url": "pages/glossary.html#term-mt-evaluation"
  },
  {
    "id": "term-machine-unlearning",
    "title": "Machine Unlearning",
    "category": "Glossary",
    "subcategory": "Privacy",
    "keywords": [
      "ai",
      "biased",
      "data",
      "ethics",
      "forgotten",
      "harmful",
      "influence",
      "machine",
      "model",
      "motivated",
      "need",
      "privacy"
    ],
    "excerpt": "Techniques for removing the influence of specific training data from a trained model, motivated by privacy rights such as the right to be forgotten and the need to remove biased or harmful data.",
    "url": "pages/glossary.html#term-machine-unlearning"
  },
  {
    "id": "term-macy-conferences",
    "title": "Macy Conferences",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1946",
      "1953",
      "brought",
      "conferences",
      "crossdisciplinary",
      "cybernetics",
      "development",
      "fostering",
      "held",
      "history",
      "ideas",
      "influenced"
    ],
    "excerpt": "A series of interdisciplinary conferences held from 1946 to 1953 that brought together researchers in cybernetics, neuroscience, psychology, and mathematics, fostering cross-disciplinary ideas that influenced the development of AI.",
    "url": "pages/glossary.html#term-macy-conferences"
  },
  {
    "id": "term-mae",
    "title": "MAE",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "autoencoder",
      "computer",
      "image",
      "large",
      "learning",
      "mae",
      "masked",
      "masks",
      "method",
      "missing",
      "model",
      "patches"
    ],
    "excerpt": "Masked Autoencoder, a self-supervised learning method for vision that randomly masks large portions of image patches and trains the model to reconstruct the missing pixels, learning rich visual representations.",
    "url": "pages/glossary.html#term-mae"
  },
  {
    "id": "term-mahalanobis-distance",
    "title": "Mahalanobis Distance",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "accounts",
      "correlations",
      "covariance",
      "deviations",
      "distance",
      "distribution",
      "inverse",
      "mahalanobis",
      "matrix",
      "mean",
      "measuring",
      "metric"
    ],
    "excerpt": "A distance metric that accounts for correlations between variables by measuring the number of standard deviations a point is from the mean of a distribution, using the inverse covariance matrix.",
    "url": "pages/glossary.html#term-mahalanobis-distance"
  },
  {
    "id": "term-maieutic-prompting",
    "title": "Maieutic Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "abductive",
      "answer",
      "approach",
      "consistent",
      "engineering",
      "explanations",
      "generates",
      "identify",
      "inspired",
      "logical",
      "maieutic",
      "method"
    ],
    "excerpt": "A prompting method inspired by the Socratic maieutic approach that generates a tree of explanations with logical relationships, then uses abductive reasoning to identify the most consistent and truthful answer from the model.",
    "url": "pages/glossary.html#term-maieutic-prompting"
  },
  {
    "id": "term-mamba",
    "title": "Mamba",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "efficiently",
      "inputdependent",
      "length",
      "linear",
      "mamba",
      "matching",
      "mechanisms",
      "model",
      "networks",
      "neural",
      "process"
    ],
    "excerpt": "A selective state space model architecture that uses input-dependent selection mechanisms to efficiently process sequences with linear scaling in sequence length while matching transformer quality.",
    "url": "pages/glossary.html#term-mamba"
  },
  {
    "id": "term-manhattan-distance",
    "title": "Manhattan Distance",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "absolute",
      "across",
      "computed",
      "differences",
      "dimensions",
      "distance",
      "known",
      "learning",
      "machine",
      "manhattan",
      "metric",
      "metrics"
    ],
    "excerpt": "A distance metric computed as the sum of absolute differences across all dimensions between two points, also known as L1 distance or taxicab distance. It measures distance along axis-aligned paths.",
    "url": "pages/glossary.html#term-manhattan-distance"
  },
  {
    "id": "term-mann-whitney-u-test",
    "title": "Mann-Whitney U Test",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "compares",
      "distributions",
      "group",
      "groups",
      "independent",
      "inference",
      "larger",
      "mann",
      "nonparametric",
      "observations",
      "one",
      "ranking"
    ],
    "excerpt": "A non-parametric test that compares the distributions of two independent groups by ranking all observations and testing whether one group tends to have larger values. It does not assume normality.",
    "url": "pages/glossary.html#term-mann-whitney-u-test"
  },
  {
    "id": "term-manual-chain-of-thought",
    "title": "Manual Chain-of-Thought",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "approach",
      "chain",
      "demonstrations",
      "engineering",
      "example",
      "explicitly",
      "fewshot",
      "guide",
      "handcrafting",
      "human",
      "intermediate",
      "manual"
    ],
    "excerpt": "The practice of hand-crafting step-by-step reasoning demonstrations within few-shot prompts, where a human explicitly writes out the intermediate reasoning steps for each example to guide the model's problem-solving approach.",
    "url": "pages/glossary.html#term-manual-chain-of-thought"
  },
  {
    "id": "term-mappo",
    "title": "MAPPO",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "centralized",
      "extension",
      "function",
      "learning",
      "mappo",
      "multiagent",
      "optimization",
      "parameters",
      "policy",
      "ppo",
      "proximal",
      "reinforcement"
    ],
    "excerpt": "Multi-Agent Proximal Policy Optimization, an extension of PPO to multi-agent settings that uses shared parameters and a centralized value function.",
    "url": "pages/glossary.html#term-mappo"
  },
  {
    "id": "term-markdown-prompting",
    "title": "Markdown Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "blocks",
      "code",
      "comprehension",
      "conventions",
      "emphasis",
      "engineering",
      "expectations",
      "format",
      "formatting",
      "headers",
      "improve",
      "instructions"
    ],
    "excerpt": "The use of Markdown formatting conventions such as headers, lists, code blocks, and emphasis within prompts to organize instructions and improve model comprehension of prompt structure and output expectations.",
    "url": "pages/glossary.html#term-markdown-prompting"
  },
  {
    "id": "term-markov-chain",
    "title": "Markov Chain",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "chain",
      "current",
      "depends",
      "describing",
      "learning",
      "machine",
      "markov",
      "model",
      "next",
      "only",
      "preceding",
      "probability"
    ],
    "excerpt": "A stochastic model describing a sequence of states where the probability of transitioning to the next state depends only on the current state (the Markov property), not on the sequence of preceding states.",
    "url": "pages/glossary.html#term-markov-chain"
  },
  {
    "id": "term-markov-chain-monte-carlo",
    "title": "Markov Chain Monte Carlo",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "algorithms",
      "bayesian",
      "carlo",
      "chain",
      "class",
      "constructing",
      "distribution",
      "distributions",
      "markov",
      "methods",
      "monte",
      "probability"
    ],
    "excerpt": "A class of algorithms that sample from probability distributions by constructing a Markov chain whose stationary distribution is the target distribution.",
    "url": "pages/glossary.html#term-markov-chain-monte-carlo"
  },
  {
    "id": "term-markov-decision-process",
    "title": "Markov Decision Process (MDP)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "actions",
      "concepts",
      "core",
      "current",
      "decision",
      "decisionmaking",
      "defined",
      "depends",
      "formal",
      "framework",
      "learning"
    ],
    "excerpt": "A formal mathematical framework for sequential decision-making defined by states, actions, transition probabilities, and rewards, where the next state depends only on the current state and action (Markov property).",
    "url": "pages/glossary.html#term-markov-decision-process"
  },
  {
    "id": "term-marvin-minsky",
    "title": "Marvin Minsky",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19272016",
      "american",
      "authored",
      "cofounded",
      "cognitive",
      "concept",
      "developed",
      "frames",
      "history",
      "knowledge",
      "laboratory",
      "marvin"
    ],
    "excerpt": "American cognitive scientist and AI pioneer (1927-2016) who co-founded the MIT AI Laboratory, developed the concept of frames for knowledge representation, and authored seminal works on AI and the theory of mind.",
    "url": "pages/glossary.html#term-marvin-minsky"
  },
  {
    "id": "term-mask",
    "title": "Mask / Masking",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "certain",
      "data",
      "hiding",
      "ignoring",
      "inference",
      "mask",
      "masking",
      "parts",
      "technique",
      "training"
    ],
    "excerpt": "Hiding or ignoring certain parts of data during training or inference. In BERT, random tokens are masked for prediction. In transformers, future tokens are masked to maintain causality.",
    "url": "pages/glossary.html#term-mask"
  },
  {
    "id": "term-mask-rcnn",
    "title": "Mask R-CNN",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "adding",
      "alongside",
      "bounding",
      "box",
      "branch",
      "classification",
      "cnn",
      "computer",
      "detection",
      "existing",
      "extends",
      "faster"
    ],
    "excerpt": "An instance segmentation framework that extends Faster R-CNN by adding a parallel branch for pixel-level mask prediction alongside the existing bounding box regression and classification heads.",
    "url": "pages/glossary.html#term-mask-rcnn"
  },
  {
    "id": "term-mask2former",
    "title": "Mask2Former",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "architecture",
      "attention",
      "computer",
      "decoder",
      "image",
      "instance",
      "learnable",
      "mask2former",
      "masked",
      "object",
      "panoptic",
      "processed"
    ],
    "excerpt": "A universal image segmentation architecture that unifies semantic, instance, and panoptic segmentation through masked attention and learnable object queries processed by a transformer decoder.",
    "url": "pages/glossary.html#term-mask2former"
  },
  {
    "id": "term-masked-language-modeling",
    "title": "Masked Language Modeling",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "bidirectional",
      "context",
      "input",
      "language",
      "learns",
      "mask",
      "masked",
      "model",
      "modeling",
      "nlp",
      "objective",
      "original"
    ],
    "excerpt": "A pretraining objective where random tokens in the input are replaced with a mask token and the model learns to predict the original tokens from the surrounding bidirectional context.",
    "url": "pages/glossary.html#term-masked-language-modeling"
  },
  {
    "id": "term-math-benchmark",
    "title": "MATH Benchmark",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "500",
      "algebra",
      "benchmark",
      "benchmarks",
      "challenging",
      "competitionlevel",
      "construction",
      "evaluation",
      "language",
      "math",
      "mathematical",
      "mathematics"
    ],
    "excerpt": "A challenging benchmark of 12,500 competition-level mathematics problems spanning seven subjects from algebra to number theory, requiring sophisticated mathematical reasoning and multi-step proof construction from language models.",
    "url": "pages/glossary.html#term-math-benchmark"
  },
  {
    "id": "term-matryoshka-embeddings",
    "title": "Matryoshka Embeddings",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "allowing",
      "approach",
      "dimensionality",
      "embedding",
      "embeddings",
      "flexible",
      "full",
      "generative",
      "itself",
      "llm",
      "matryoshka"
    ],
    "excerpt": "An embedding training approach that produces vectors where any prefix of the full embedding is itself a useful embedding, allowing flexible dimensionality reduction without retraining.",
    "url": "pages/glossary.html#term-matryoshka-embeddings"
  },
  {
    "id": "term-matthews-correlation-coefficient",
    "title": "Matthews Correlation Coefficient",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "balanced",
      "classification",
      "coefficient",
      "computed",
      "confusion",
      "correlation",
      "four",
      "indicates",
      "inverse",
      "learning",
      "machine",
      "matrix"
    ],
    "excerpt": "A balanced classification metric computed from all four confusion matrix values (TP, TN, FP, FN) that produces a value between -1 and +1, where +1 indicates perfect prediction, 0 is random, and -1 is inverse prediction.",
    "url": "pages/glossary.html#term-matthews-correlation-coefficient"
  },
  {
    "id": "term-max-pooling",
    "title": "Max Pooling",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "convolutional",
      "detected",
      "dimensions",
      "downsampling",
      "features",
      "filters",
      "max",
      "maximum",
      "networks",
      "neural",
      "operation"
    ],
    "excerpt": "A downsampling operation that selects the maximum value within each pooling window, reducing spatial dimensions while retaining the most prominent features detected by convolutional filters.",
    "url": "pages/glossary.html#term-max-pooling"
  },
  {
    "id": "term-maximal-marginal-relevance",
    "title": "Maximal Marginal Relevance",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "against",
      "algorithm",
      "alreadyselected",
      "balancing",
      "diversification",
      "diversity",
      "documents",
      "iteratively",
      "lambda",
      "marginal",
      "maximal",
      "mmr"
    ],
    "excerpt": "A retrieval diversification algorithm (MMR) that iteratively selects documents by balancing relevance to the query against novelty relative to already-selected documents, reducing redundancy in retrieved results through a tunable lambda parameter.",
    "url": "pages/glossary.html#term-maximal-marginal-relevance"
  },
  {
    "id": "term-maximum-a-posteriori",
    "title": "Maximum A Posteriori",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bayesian",
      "beliefs",
      "combining",
      "data",
      "estimation",
      "finds",
      "likelihood",
      "maximizing",
      "maximum",
      "method",
      "methods",
      "parameter"
    ],
    "excerpt": "A Bayesian point estimation method that finds the parameter values maximizing the posterior probability, combining the likelihood of the data with prior beliefs.",
    "url": "pages/glossary.html#term-maximum-a-posteriori"
  },
  {
    "id": "term-maximum-entropy-rl",
    "title": "Maximum Entropy RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "achieving",
      "act",
      "agents",
      "augments",
      "encouraging",
      "entropy",
      "framework",
      "high",
      "learning",
      "maximum",
      "objective",
      "optimization"
    ],
    "excerpt": "An RL framework that augments the standard return objective with policy entropy, encouraging agents to act as randomly as possible while still achieving high rewards.",
    "url": "pages/glossary.html#term-maximum-entropy-rl"
  },
  {
    "id": "term-maximum-likelihood-estimation",
    "title": "Maximum Likelihood Estimation",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "data",
      "estimating",
      "estimation",
      "finding",
      "function",
      "given",
      "inference",
      "likelihood",
      "maximize",
      "maximum",
      "method",
      "model"
    ],
    "excerpt": "A method of estimating the parameters of a statistical model by finding the parameter values that maximize the likelihood function, representing the probability of the observed data given the parameters.",
    "url": "pages/glossary.html#term-maximum-likelihood-estimation"
  },
  {
    "id": "term-mbpp",
    "title": "MBPP",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "000",
      "ability",
      "approximately",
      "basic",
      "benchmark",
      "benchmarks",
      "cases",
      "code",
      "consisting",
      "descriptions",
      "designed",
      "entrylevel"
    ],
    "excerpt": "Mostly Basic Python Programming, a code generation benchmark consisting of approximately 1,000 entry-level Python programming problems with test cases, designed to evaluate a model's ability to syn...",
    "url": "pages/glossary.html#term-mbpp"
  },
  {
    "id": "term-mcculloch-pitts-neuron",
    "title": "McCulloch-Pitts Neuron",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1943",
      "binary",
      "biological",
      "compute",
      "first",
      "function",
      "history",
      "logical",
      "mathematical",
      "mcculloch",
      "milestones",
      "model"
    ],
    "excerpt": "The first mathematical model of a biological neuron, proposed by Warren McCulloch and Walter Pitts in 1943, showing that networks of simple binary threshold units could compute any logical function.",
    "url": "pages/glossary.html#term-mcculloch-pitts-neuron"
  },
  {
    "id": "term-mcdiarmids-inequality",
    "title": "McDiarmid&amp;#x27;s Inequality",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bounded",
      "close",
      "concentration",
      "differences",
      "expected",
      "function",
      "high",
      "independent",
      "inequality",
      "mcdiarmidampx27s",
      "probability",
      "random"
    ],
    "excerpt": "A concentration inequality stating that a function of independent random variables with bounded differences is close to its expected value with high probability.",
    "url": "pages/glossary.html#term-mcdiarmids-inequality"
  },
  {
    "id": "term-mean-absolute-error",
    "title": "Mean Absolute Error",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "absolute",
      "actual",
      "average",
      "computed",
      "differences",
      "error",
      "function",
      "learning",
      "loss",
      "machine",
      "mean",
      "metrics"
    ],
    "excerpt": "A regression loss function computed as the average of the absolute differences between predicted and actual values. It is more robust to outliers than mean squared error.",
    "url": "pages/glossary.html#term-mean-absolute-error"
  },
  {
    "id": "term-mean-average-precision",
    "title": "Mean Average Precision",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "across",
      "average",
      "classes",
      "classification",
      "computer",
      "computes",
      "detection",
      "evaluation",
      "iou",
      "localization",
      "map",
      "mean"
    ],
    "excerpt": "The primary evaluation metric for object detection (mAP) that computes the average precision across all classes and IoU thresholds, summarizing both localization and classification performance.",
    "url": "pages/glossary.html#term-mean-average-precision"
  },
  {
    "id": "term-mean-field-rl",
    "title": "Mean Field Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "agents",
      "among",
      "approach",
      "approximates",
      "average",
      "effect",
      "field",
      "interactions",
      "learning",
      "many",
      "mean"
    ],
    "excerpt": "A scalable approach to multi-agent RL that approximates interactions among many agents using a mean field (average effect) of neighboring agents' actions.",
    "url": "pages/glossary.html#term-mean-field-rl"
  },
  {
    "id": "term-mean-reciprocal-rank",
    "title": "Mean Reciprocal Rank",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "across",
      "answer",
      "averages",
      "correct",
      "evaluation",
      "first",
      "mean",
      "measuring",
      "metric",
      "metrics",
      "position",
      "queries"
    ],
    "excerpt": "A ranking metric that averages the reciprocal of the rank position of the first relevant result across a set of queries, measuring how quickly a retrieval system surfaces the correct answer.",
    "url": "pages/glossary.html#term-mean-reciprocal-rank"
  },
  {
    "id": "term-mean-squared-error",
    "title": "Mean Squared Error",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "actual",
      "average",
      "computed",
      "differences",
      "error",
      "function",
      "learning",
      "loss",
      "machine",
      "mean",
      "metrics",
      "predicted"
    ],
    "excerpt": "A regression loss function computed as the average of the squared differences between predicted and actual values. It penalizes larger errors more heavily due to the squaring operation.",
    "url": "pages/glossary.html#term-mean-squared-error"
  },
  {
    "id": "term-meaningful-human-control",
    "title": "Meaningful Human Control",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ability",
      "ai",
      "aidriven",
      "applications",
      "authority",
      "control",
      "decisions",
      "domains",
      "ethics",
      "highstakes",
      "human",
      "humans"
    ],
    "excerpt": "The requirement that humans retain sufficient understanding, authority, and ability to intervene in AI-driven decisions, particularly in high-stakes domains such as military, medical, and judicial applications.",
    "url": "pages/glossary.html#term-meaningful-human-control"
  },
  {
    "id": "term-measurement-bias",
    "title": "Measurement Bias",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "across",
      "ai",
      "arrest",
      "behavior",
      "bias",
      "criminal",
      "differ",
      "ethics",
      "fairness",
      "features",
      "groups",
      "introduced"
    ],
    "excerpt": "Bias introduced when the features or labels used in an AI system systematically differ in quality or meaning across groups, such as using arrest records as a proxy for criminal behavior.",
    "url": "pages/glossary.html#term-measurement-bias"
  },
  {
    "id": "term-medical-imaging-ai",
    "title": "Medical Imaging AI",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "ai",
      "anatomical",
      "application",
      "assistance",
      "computer",
      "deep",
      "detection",
      "disease",
      "image",
      "images",
      "imaging",
      "learning"
    ],
    "excerpt": "The application of deep learning to medical images (X-rays, CT scans, MRIs, pathology slides) for tasks like disease detection, segmentation of anatomical structures, and treatment planning assistance.",
    "url": "pages/glossary.html#term-medical-imaging-ai"
  },
  {
    "id": "term-medusa-decoding",
    "title": "Medusa Decoding",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "adds",
      "allowing",
      "decoding",
      "draft",
      "future",
      "heads",
      "inference",
      "language",
      "llm",
      "medusa",
      "method",
      "model"
    ],
    "excerpt": "A parallel decoding method that adds multiple prediction heads to a language model, allowing it to propose and verify several future tokens simultaneously without requiring a separate draft model.",
    "url": "pages/glossary.html#term-medusa-decoding"
  },
  {
    "id": "term-megatron-lm",
    "title": "Megatron-LM",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "computing",
      "distributed",
      "efficient",
      "framework",
      "hardware",
      "implementing",
      "language",
      "largescale",
      "lm",
      "megatron",
      "model",
      "nvidia"
    ],
    "excerpt": "NVIDIA's framework for efficient large-scale language model training implementing tensor parallelism, pipeline parallelism, and sequence parallelism optimized for NVIDIA hardware.",
    "url": "pages/glossary.html#term-megatron-lm"
  },
  {
    "id": "term-memory-ai",
    "title": "Memory (AI Systems)",
    "category": "Glossary",
    "subcategory": "Capability",
    "keywords": [
      "across",
      "ai",
      "allowing",
      "architecture",
      "capability",
      "conversations",
      "information",
      "mechanisms",
      "memory",
      "retain",
      "systems"
    ],
    "excerpt": "Mechanisms allowing AI to retain information across conversations. Includes context windows, conversation history, and persistent memory features in some AI assistants.",
    "url": "pages/glossary.html#term-memory-ai"
  },
  {
    "id": "term-memory-bandwidth",
    "title": "Memory Bandwidth",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "bandwidth",
      "data",
      "gbs",
      "gpu",
      "hardware",
      "measured",
      "memory",
      "processor",
      "rate",
      "tbs",
      "transferred"
    ],
    "excerpt": "The rate at which data can be transferred between a processor and its memory, measured in GB/s or TB/s.",
    "url": "pages/glossary.html#term-memory-bandwidth"
  },
  {
    "id": "term-memory-management-llm",
    "title": "Memory Management for LLM Inference",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "allocating",
      "allocation",
      "cache",
      "dynamic",
      "efficiently",
      "for",
      "gpu",
      "including",
      "inference",
      "infrastructure",
      "language",
      "large"
    ],
    "excerpt": "Strategies for efficiently allocating and managing GPU memory during large language model inference, including KV cache management, memory pooling, and dynamic allocation.",
    "url": "pages/glossary.html#term-memory-management-llm"
  },
  {
    "id": "term-memory-augmented-neural-network",
    "title": "Memory-Augmented Neural Network",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "addressing",
      "architecture",
      "architectures",
      "attentionbased",
      "augmented",
      "broad",
      "class",
      "enabling",
      "equipped",
      "external",
      "information",
      "memory"
    ],
    "excerpt": "A broad class of neural architectures equipped with external memory modules that can be read and written using attention-based addressing, enabling reasoning over stored information.",
    "url": "pages/glossary.html#term-memory-augmented-neural-network"
  },
  {
    "id": "term-memory-bound",
    "title": "Memory-Bound Workload",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "bound",
      "capability",
      "compute",
      "data",
      "hardware",
      "limited",
      "memory",
      "model",
      "optimization",
      "performance",
      "processing",
      "processor"
    ],
    "excerpt": "A processing task where performance is limited by the rate of data transfer between processor and memory rather than compute capability.",
    "url": "pages/glossary.html#term-memory-bound"
  },
  {
    "id": "term-mesa-optimization",
    "title": "Mesa-Optimization",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "alignment",
      "base",
      "develops",
      "differ",
      "internally",
      "learned",
      "mesa",
      "mesaoptimizer",
      "model",
      "objective",
      "optimization"
    ],
    "excerpt": "A phenomenon where a learned model (the mesa-optimizer) internally develops its own optimization objective that may differ from the base objective it was trained on.",
    "url": "pages/glossary.html#term-mesa-optimization"
  },
  {
    "id": "term-mesh-reconstruction",
    "title": "Mesh Reconstruction",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "3dprinted",
      "clouds",
      "computer",
      "converting",
      "define",
      "depth",
      "functions",
      "geometry",
      "implicit",
      "maps",
      "mesh"
    ],
    "excerpt": "The process of converting 3D point clouds, implicit functions, or depth maps into triangular mesh representations that define surface geometry, topology, and can be rendered or 3D-printed.",
    "url": "pages/glossary.html#term-mesh-reconstruction"
  },
  {
    "id": "term-message-passing-neural-network",
    "title": "Message Passing Neural Network",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "aggregating",
      "architecture",
      "exchanging",
      "framework",
      "functions",
      "graph",
      "iteratively",
      "learned",
      "message",
      "messages",
      "neighboring",
      "network"
    ],
    "excerpt": "A framework for graph neural networks where nodes iteratively update their representations by exchanging and aggregating messages with neighboring nodes through learned message and update functions.",
    "url": "pages/glossary.html#term-message-passing-neural-network"
  },
  {
    "id": "term-meta-llama",
    "title": "Meta LLaMA",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2023",
      "catalyzed",
      "ecosystem",
      "february",
      "first",
      "history",
      "language",
      "large",
      "llama",
      "major",
      "meta",
      "metas"
    ],
    "excerpt": "Meta's Large Language Model Meta AI, first released in February 2023 with subsequent versions, representing a major open-weight language model that catalyzed the open-source AI ecosystem.",
    "url": "pages/glossary.html#term-meta-llama"
  },
  {
    "id": "term-meta-learning",
    "title": "Meta-Learning",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "adapt",
      "advanced",
      "examples",
      "few",
      "learn",
      "learning",
      "meta",
      "models",
      "new",
      "quickly",
      "tasks",
      "training"
    ],
    "excerpt": "Learning how to learn: training models that can quickly adapt to new tasks with few examples. Enables better few-shot and transfer learning capabilities.",
    "url": "pages/glossary.html#term-meta-learning"
  },
  {
    "id": "term-meta-prompting",
    "title": "Meta-Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "approach",
      "critique",
      "effectively",
      "engineer",
      "engineering",
      "generate",
      "higherorder",
      "improve",
      "instructed",
      "itself",
      "language",
      "meta"
    ],
    "excerpt": "A higher-order prompting approach where a language model is instructed to generate, critique, or improve prompts for itself or other models, effectively using the model as its own prompt engineer.",
    "url": "pages/glossary.html#term-meta-prompting"
  },
  {
    "id": "term-meta-rl",
    "title": "Meta-Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "across",
      "adaptation",
      "approaches",
      "distribution",
      "enabling",
      "experience",
      "learn",
      "learning",
      "leveraging",
      "meta",
      "new",
      "paradigms"
    ],
    "excerpt": "RL approaches that learn to learn, enabling rapid adaptation to new tasks by leveraging experience across a distribution of related tasks.",
    "url": "pages/glossary.html#term-meta-rl"
  },
  {
    "id": "term-metadata-filtering",
    "title": "Metadata Filtering",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "alongside",
      "applies",
      "attribute",
      "categories",
      "computation",
      "criteria",
      "database",
      "date",
      "distance",
      "filtering",
      "filters",
      "matching"
    ],
    "excerpt": "A vector search technique that applies structured attribute filters alongside similarity search, restricting results to vectors matching specific metadata criteria such as date ranges, categories, or source types before or after distance computation.",
    "url": "pages/glossary.html#term-metadata-filtering"
  },
  {
    "id": "term-meteor",
    "title": "METEOR",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "addition",
      "considers",
      "evaluation",
      "exact",
      "explicit",
      "machine",
      "matches",
      "meteor",
      "metric",
      "nlp",
      "order",
      "ordering"
    ],
    "excerpt": "Metric for Evaluation of Translation with Explicit ORdering, a machine translation evaluation metric that considers synonyms, stemming, and word order in addition to exact word matches.",
    "url": "pages/glossary.html#term-meteor"
  },
  {
    "id": "term-metrics",
    "title": "Metrics",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "evaluate",
      "evaluation",
      "measures",
      "metrics",
      "model",
      "performance",
      "quality",
      "quantitative"
    ],
    "excerpt": "Quantitative measures used to evaluate model performance. Common metrics include accuracy, precision, recall, F1, perplexity, and human evaluation scores.",
    "url": "pages/glossary.html#term-metrics"
  },
  {
    "id": "term-metropolis-hastings",
    "title": "Metropolis-Hastings",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "acceptance",
      "accepting",
      "algorithm",
      "balance",
      "based",
      "bayesian",
      "candidate",
      "detailed",
      "distribution",
      "ensures",
      "generates",
      "hastings"
    ],
    "excerpt": "An MCMC algorithm that generates samples from a target distribution by proposing candidate points from a proposal distribution and accepting or rejecting them based on an acceptance ratio that ensures detailed balance.",
    "url": "pages/glossary.html#term-metropolis-hastings"
  },
  {
    "id": "term-midjourney",
    "title": "Midjourney",
    "category": "Glossary",
    "subcategory": "Product",
    "keywords": [
      "artistic",
      "generation",
      "image",
      "known",
      "midjourney",
      "outputs",
      "popular",
      "product",
      "service",
      "stylized"
    ],
    "excerpt": "A popular AI image generation service known for artistic, stylized outputs. Accessed through Discord, it's widely used for creative and commercial image creation.",
    "url": "pages/glossary.html#term-midjourney"
  },
  {
    "id": "term-midjourney-launch",
    "title": "Midjourney Launch",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2022",
      "art",
      "artistic",
      "becoming",
      "creation",
      "creative",
      "debates",
      "generation",
      "history",
      "images",
      "independent",
      "july"
    ],
    "excerpt": "The July 2022 public launch of Midjourney, an independent AI art generation service that produces images from text prompts, becoming one of the most popular creative AI tools and sparking debates about AI and artistic creation.",
    "url": "pages/glossary.html#term-midjourney-launch"
  },
  {
    "id": "term-milvus",
    "title": "Milvus",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "architecture",
      "billionscale",
      "built",
      "capable",
      "database",
      "datasets",
      "distributed",
      "handling",
      "hybrid",
      "index",
      "milvus",
      "multiple"
    ],
    "excerpt": "An open-source vector database built for scalable similarity search that supports multiple index types, hybrid search, and multi-tenancy, capable of handling billion-scale vector datasets with a distributed architecture.",
    "url": "pages/glossary.html#term-milvus"
  },
  {
    "id": "term-min-max-scaling",
    "title": "Min-Max Scaling",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "dividing",
      "engineering",
      "feature",
      "features",
      "fixed",
      "learning",
      "linearly",
      "machine",
      "max",
      "min",
      "minimum",
      "normalization"
    ],
    "excerpt": "A normalization technique that linearly rescales features to a fixed range, typically [0, 1], by subtracting the minimum value and dividing by the range.",
    "url": "pages/glossary.html#term-min-max-scaling"
  },
  {
    "id": "term-minhash",
    "title": "MinHash",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "approximate",
      "deduplication",
      "document",
      "efficiently",
      "estimates",
      "hashing",
      "jaccard",
      "localitysensitive",
      "minhash",
      "nearest",
      "neighbor",
      "nlp"
    ],
    "excerpt": "A locality-sensitive hashing technique that efficiently estimates the Jaccard similarity between sets, widely used in NLP for approximate nearest neighbor search and document deduplication.",
    "url": "pages/glossary.html#term-minhash"
  },
  {
    "id": "term-mini-batch-gradient-descent",
    "title": "Mini-Batch Gradient Descent",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "balancing",
      "batch",
      "computes",
      "data",
      "descent",
      "fullbatch",
      "gradient",
      "gradientbased",
      "learning",
      "machine",
      "method",
      "mini"
    ],
    "excerpt": "A gradient-based optimization method that computes parameter updates using a small random subset (mini-batch) of the training data at each step, balancing the stability of full-batch gradient descent with the speed of stochastic gradient descent.",
    "url": "pages/glossary.html#term-mini-batch-gradient-descent"
  },
  {
    "id": "term-minimum-bayes-risk-decoding",
    "title": "Minimum Bayes Risk Decoding",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "across",
      "ai",
      "bayes",
      "beam",
      "candidate",
      "decoding",
      "expected",
      "generative",
      "higherquality",
      "hypotheses",
      "loss",
      "minimizing"
    ],
    "excerpt": "A decoding strategy that selects the output candidate minimizing expected loss across a set of sampled hypotheses, often producing higher-quality translations and summaries than beam search.",
    "url": "pages/glossary.html#term-minimum-bayes-risk-decoding"
  },
  {
    "id": "term-minimum-description-length",
    "title": "Minimum Description Length",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "data",
      "description",
      "itself",
      "learning",
      "length",
      "machine",
      "minimizing",
      "minimum",
      "model",
      "principle",
      "selection",
      "selects"
    ],
    "excerpt": "A model selection principle that selects the model minimizing the total description length of the data and the model itself. It formalizes Occam's razor using information-theoretic concepts.",
    "url": "pages/glossary.html#term-minimum-description-length"
  },
  {
    "id": "term-minkowski-distance",
    "title": "Minkowski Distance",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "cases",
      "chebyshev",
      "distance",
      "distances",
      "euclidean",
      "generalized",
      "includes",
      "learning",
      "machine",
      "manhattan",
      "metric",
      "metrics"
    ],
    "excerpt": "A generalized distance metric parameterized by p that includes Manhattan (p=1), Euclidean (p=2), and Chebyshev (p=infinity) distances as special cases.",
    "url": "pages/glossary.html#term-minkowski-distance"
  },
  {
    "id": "term-minsky-papert-perceptrons",
    "title": "Minsky and Papert Perceptrons",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1969",
      "and",
      "book",
      "contributing",
      "demonstrated",
      "first",
      "funding",
      "history",
      "limitations",
      "marvin",
      "mathematically",
      "milestones"
    ],
    "excerpt": "The 1969 book by Marvin Minsky and Seymour Papert that mathematically demonstrated the limitations of single-layer perceptrons, contributing to reduced funding for neural network research and the first AI winter.",
    "url": "pages/glossary.html#term-minsky-papert-perceptrons"
  },
  {
    "id": "term-mirror-prompting",
    "title": "Mirror Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "approach",
      "back",
      "clarification",
      "confirming",
      "engineering",
      "execution",
      "first",
      "instructs",
      "intent",
      "interpretation",
      "mirror",
      "misalignment"
    ],
    "excerpt": "A prompting approach that instructs the model to first restate the user's request back in its own words, confirming mutual understanding before proceeding with task execution, reducing misalignment between user intent and model interpretation.",
    "url": "pages/glossary.html#term-mirror-prompting"
  },
  {
    "id": "term-misinformation",
    "title": "Misinformation",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "amplified",
      "deceive",
      "deliberate",
      "ethics",
      "false",
      "generated",
      "hallucinations",
      "inaccurate",
      "inadvertently",
      "information",
      "intent"
    ],
    "excerpt": "False or inaccurate information shared without deliberate intent to deceive, which can be amplified by AI recommendation systems and generated inadvertently through AI hallucinations.",
    "url": "pages/glossary.html#term-misinformation"
  },
  {
    "id": "term-mistral",
    "title": "Mistral",
    "category": "Glossary",
    "subcategory": "Company",
    "keywords": [
      "company",
      "efficient",
      "french",
      "highperformance",
      "known",
      "mistral",
      "model",
      "models",
      "open"
    ],
    "excerpt": "A French AI company known for efficient, high-performance open models. Their Mistral and Mixtral models offer strong capabilities with smaller parameter counts.",
    "url": "pages/glossary.html#term-mistral"
  },
  {
    "id": "term-mistral-ai-founding",
    "title": "Mistral AI Founding",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2023",
      "ai",
      "april",
      "became",
      "company",
      "competitive",
      "deepmind",
      "european",
      "former",
      "founding",
      "google",
      "history"
    ],
    "excerpt": "The founding of Mistral AI in April 2023 by former Google DeepMind and Meta researchers in Paris, which rapidly became a leading European AI company releasing competitive open-weight language models.",
    "url": "pages/glossary.html#term-mistral-ai-founding"
  },
  {
    "id": "term-mit-ai-laboratory",
    "title": "MIT AI Laboratory",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1959",
      "ai",
      "became",
      "centers",
      "cofounded",
      "foundational",
      "history",
      "influential",
      "john",
      "laboratory",
      "language",
      "marvin"
    ],
    "excerpt": "A research laboratory co-founded by Marvin Minsky and John McCarthy at MIT in 1959, which became one of the most influential AI research centers, producing foundational work in vision, robotics, and natural language understanding.",
    "url": "pages/glossary.html#term-mit-ai-laboratory"
  },
  {
    "id": "term-mixed-precision-training",
    "title": "Mixed Precision Training",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "accumulation",
      "backward",
      "bf16",
      "copies",
      "floatingpoint",
      "formats",
      "forward",
      "fp16",
      "fp32",
      "hardware",
      "lowerprecision",
      "maintaining"
    ],
    "excerpt": "A training technique that uses lower-precision floating-point formats (FP16 or BF16) for forward and backward passes while maintaining FP32 master copies of weights for accumulation.",
    "url": "pages/glossary.html#term-mixed-precision-training"
  },
  {
    "id": "term-mixture-of-agents",
    "title": "Mixture of Agents",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "agent",
      "agents",
      "aggregator",
      "ai",
      "architecture",
      "collaborate",
      "combining",
      "contributing",
      "domain",
      "expertise",
      "generative",
      "llm"
    ],
    "excerpt": "An architecture where multiple specialized LLM agents collaborate on a task, with each agent contributing expertise in a specific domain and a router or aggregator combining their outputs.",
    "url": "pages/glossary.html#term-mixture-of-agents"
  },
  {
    "id": "term-mixture-of-depths",
    "title": "Mixture of Depths",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "allocate",
      "architecture",
      "block",
      "computation",
      "depths",
      "dynamically",
      "learns",
      "maintaining",
      "mixture",
      "model",
      "networks",
      "neural"
    ],
    "excerpt": "A transformer variant that learns to dynamically allocate computation by routing only a subset of tokens through each transformer block, reducing total computation while maintaining model quality.",
    "url": "pages/glossary.html#term-mixture-of-depths"
  },
  {
    "id": "term-moe-inference",
    "title": "Mixture of Experts Inference",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "activated",
      "computation",
      "count",
      "despite",
      "expert",
      "experts",
      "inference",
      "infrastructure",
      "large",
      "mixture",
      "model",
      "models"
    ],
    "excerpt": "Inference optimization for Mixture of Experts models where only a subset of expert parameters are activated per token, reducing computation despite the large total parameter count.",
    "url": "pages/glossary.html#term-moe-inference"
  },
  {
    "id": "term-mixture-of-experts-layer",
    "title": "Mixture of Experts Layer",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "capacity",
      "computational",
      "consisting",
      "cost",
      "enabling",
      "expert",
      "experts",
      "gating",
      "input",
      "layer",
      "massive"
    ],
    "excerpt": "A neural network layer consisting of multiple expert subnetworks and a gating mechanism that routes each input to a sparse subset of experts, enabling massive model capacity with sublinear computational cost.",
    "url": "pages/glossary.html#term-mixture-of-experts-layer"
  },
  {
    "id": "term-moe-routing",
    "title": "Mixture of Experts Routing",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "achieve",
      "architecture",
      "computation",
      "determines",
      "efficient",
      "expert",
      "experts",
      "functions",
      "gating",
      "input",
      "learned",
      "mechanism"
    ],
    "excerpt": "The gating mechanism in MoE models that determines which expert subnetworks process each input, using learned routing functions to achieve efficient sparse computation.",
    "url": "pages/glossary.html#term-moe-routing"
  },
  {
    "id": "term-mixup",
    "title": "Mixup",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "augmentation",
      "behavior",
      "combinations",
      "convex",
      "creates",
      "data",
      "encouraging",
      "examples",
      "labels",
      "learning",
      "linear",
      "machine"
    ],
    "excerpt": "A data augmentation and regularization technique that creates virtual training examples by taking convex combinations of pairs of training examples and their labels, encouraging linear behavior between training points.",
    "url": "pages/glossary.html#term-mixup"
  },
  {
    "id": "term-mlops",
    "title": "MLOps",
    "category": "Glossary",
    "subcategory": "Operations",
    "keywords": [
      "deploying",
      "maintaining",
      "mlops",
      "models",
      "operations",
      "practices",
      "production"
    ],
    "excerpt": "Practices for deploying and maintaining ML models in production. Combines ML, DevOps, and data engineering to ensure reliable, scalable AI systems.",
    "url": "pages/glossary.html#term-mlops"
  },
  {
    "id": "term-mmlu",
    "title": "MMLU (Massive Multitask Language Understanding)",
    "category": "Glossary",
    "subcategory": "Benchmark",
    "keywords": [
      "benchmark",
      "comprehensive",
      "evaluation",
      "humanities",
      "language",
      "massive",
      "mmlu",
      "models",
      "multitask",
      "stem",
      "subjects",
      "testing"
    ],
    "excerpt": "A comprehensive benchmark testing language models on 57 subjects from STEM to humanities. Widely used to compare model capabilities on knowledge-intensive tasks.",
    "url": "pages/glossary.html#term-mmlu"
  },
  {
    "id": "term-mobile-inference",
    "title": "Mobile Inference",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "capabilities",
      "dsp",
      "gpu",
      "hardware",
      "inference",
      "infrastructure",
      "leveraging",
      "mobile",
      "npu",
      "optimized",
      "smartphones",
      "tablets"
    ],
    "excerpt": "AI inference optimized for smartphones and tablets, leveraging mobile GPU, NPU, or DSP capabilities. Mobile inference frameworks like TensorFlow Lite and Core ML apply aggressive quantization and operator fusion for on-device model execution.",
    "url": "pages/glossary.html#term-mobile-inference"
  },
  {
    "id": "term-mobilenet",
    "title": "MobileNet",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "architectures",
      "cnn",
      "computation",
      "convolutions",
      "depthwise",
      "designed",
      "devices",
      "dramatically",
      "embedded",
      "family",
      "lightweight"
    ],
    "excerpt": "A family of lightweight CNN architectures designed for mobile and embedded devices that use depthwise separable convolutions to dramatically reduce computation and model size.",
    "url": "pages/glossary.html#term-mobilenet"
  },
  {
    "id": "term-model",
    "title": "Model",
    "category": "Glossary",
    "subcategory": "Core Concept",
    "keywords": [
      "concept",
      "core",
      "fundamentals",
      "generates",
      "inputs",
      "model",
      "outputs",
      "processes",
      "system",
      "trained"
    ],
    "excerpt": "The trained AI system that processes inputs and generates outputs. Models are defined by their architecture, size (parameters), training data, and fine-tuning.",
    "url": "pages/glossary.html#term-model"
  },
  {
    "id": "term-model-card",
    "title": "Model Card",
    "category": "Glossary",
    "subcategory": "Documentation",
    "keywords": [
      "card",
      "considerations",
      "describing",
      "documentation",
      "ethical",
      "ethics",
      "intended",
      "limitations",
      "metrics",
      "model",
      "models",
      "performance"
    ],
    "excerpt": "Documentation describing a model's intended use, limitations, performance metrics, and ethical considerations. A standard practice for responsible AI development and deployment.",
    "url": "pages/glossary.html#term-model-card"
  },
  {
    "id": "term-model-cards",
    "title": "Model Cards",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "artifacts",
      "cards",
      "documentation",
      "ethics",
      "governance",
      "mitchell",
      "model",
      "proposed",
      "standardized"
    ],
    "excerpt": "Standardized documentation artifacts proposed by Mitchell et al. (2019) that accompany trained ML models and report on their intended use, performance characteristics, limitations, and ethical considerations.",
    "url": "pages/glossary.html#term-model-cards"
  },
  {
    "id": "term-model-collapse",
    "title": "Model Collapse",
    "category": "Glossary",
    "subcategory": "Risk",
    "keywords": [
      "aigenerated",
      "collapse",
      "data",
      "degradation",
      "diversity",
      "generations",
      "lose",
      "model",
      "models",
      "phenomenon",
      "quality",
      "risk"
    ],
    "excerpt": "A degradation phenomenon where models trained on AI-generated data lose diversity and quality over generations. A growing concern as synthetic data becomes more prevalent.",
    "url": "pages/glossary.html#term-model-collapse"
  },
  {
    "id": "term-model-compression",
    "title": "Model Compression",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "compression",
      "computational",
      "cost",
      "distillation",
      "factorization",
      "family",
      "including",
      "inference",
      "infrastructure",
      "lowrank",
      "model",
      "optimization"
    ],
    "excerpt": "A family of techniques for reducing model size and computational cost while preserving performance, including quantization, pruning, distillation, and low-rank factorization.",
    "url": "pages/glossary.html#term-model-compression"
  },
  {
    "id": "term-model-distillation",
    "title": "Model Distillation",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "alone",
      "behavior",
      "distillation",
      "distributions",
      "hard",
      "inference",
      "labels",
      "larger",
      "learning",
      "llm",
      "model",
      "output"
    ],
    "excerpt": "The process of training a smaller student model to replicate the behavior of a larger teacher model by learning from the teacher's output probability distributions rather than hard labels alone.",
    "url": "pages/glossary.html#term-model-distillation"
  },
  {
    "id": "term-mfu",
    "title": "Model FLOPs Utilization (MFU)",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "available",
      "compute",
      "computing",
      "distributed",
      "efficiently",
      "flops",
      "hardware",
      "measuring",
      "mfu",
      "model",
      "observed",
      "optimization"
    ],
    "excerpt": "The ratio of observed model FLOPS to the theoretical peak FLOPS of the hardware, measuring how efficiently the training system utilizes available compute.",
    "url": "pages/glossary.html#term-mfu"
  },
  {
    "id": "term-model-merging",
    "title": "Model Merging",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "capabilities",
      "combines",
      "finetuned",
      "generative",
      "inherit",
      "interpolation",
      "like",
      "linear",
      "llm",
      "merging",
      "methods"
    ],
    "excerpt": "A technique that combines the weights of two or more fine-tuned models into a single model, often using methods like linear interpolation, SLERP, or TIES, to inherit capabilities from multiple specializations.",
    "url": "pages/glossary.html#term-model-merging"
  },
  {
    "id": "term-model-parallelism",
    "title": "Model Parallelism",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "accelerator",
      "across",
      "architecture",
      "devices",
      "distributed",
      "enabling",
      "fit",
      "large",
      "layers",
      "memory",
      "model",
      "models"
    ],
    "excerpt": "A distributed training strategy that splits a model's layers or parameters across multiple devices, enabling training of models too large to fit in the memory of a single accelerator.",
    "url": "pages/glossary.html#term-model-parallelism"
  },
  {
    "id": "term-model-predictive-control-rl",
    "title": "Model Predictive Control in RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "approach",
      "best",
      "control",
      "dynamics",
      "first",
      "forward",
      "in",
      "learned",
      "learning",
      "model",
      "planning"
    ],
    "excerpt": "A planning-based approach that uses a learned dynamics model to simulate action sequences forward and selects the first action of the best sequence.",
    "url": "pages/glossary.html#term-model-predictive-control-rl"
  },
  {
    "id": "term-model-pruning",
    "title": "Model Pruning",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "based",
      "compression",
      "criteria",
      "inference",
      "infrastructure",
      "magnitude",
      "model",
      "network",
      "neural",
      "neurons",
      "optimization",
      "pruning"
    ],
    "excerpt": "A compression technique that removes redundant weights or neurons from a neural network based on magnitude, sensitivity, or other criteria.",
    "url": "pages/glossary.html#term-model-pruning"
  },
  {
    "id": "term-model-serving",
    "title": "Model Serving",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "computing",
      "deploying",
      "distributed",
      "handle",
      "inference",
      "infrastructure",
      "model",
      "models",
      "prediction",
      "realtime",
      "requests",
      "scale"
    ],
    "excerpt": "The infrastructure and systems for deploying trained models to handle real-time prediction requests at scale.",
    "url": "pages/glossary.html#term-model-serving"
  },
  {
    "id": "term-model-sharding",
    "title": "Model Sharding",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "accelerator",
      "across",
      "capacity",
      "devices",
      "enabling",
      "exceed",
      "inference",
      "large",
      "llm",
      "locations",
      "memory",
      "model"
    ],
    "excerpt": "The technique of partitioning a large model's parameters across multiple devices or storage locations, enabling inference and training of models that exceed the memory capacity of a single accelerator.",
    "url": "pages/glossary.html#term-model-sharding"
  },
  {
    "id": "term-model-based-rl",
    "title": "Model-Based RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "approaches",
      "based",
      "dynamics",
      "environments",
      "experience",
      "function",
      "generate",
      "learn",
      "learning",
      "model",
      "plan"
    ],
    "excerpt": "RL approaches that learn or use a model of the environment's transition dynamics and reward function to plan actions or generate synthetic experience.",
    "url": "pages/glossary.html#term-model-based-rl"
  },
  {
    "id": "term-model-free-rl",
    "title": "Model-Free RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithms",
      "building",
      "concepts",
      "core",
      "directly",
      "environment",
      "experience",
      "explicit",
      "free",
      "functions",
      "learn",
      "learning"
    ],
    "excerpt": "RL algorithms that learn policies or value functions directly from experience without building an explicit model of the environment.",
    "url": "pages/glossary.html#term-model-free-rl"
  },
  {
    "id": "term-modern-hopfield-network",
    "title": "Modern Hopfield Network",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "capacity",
      "classical",
      "compared",
      "connects",
      "exponential",
      "formulation",
      "functions",
      "hopfield",
      "interaction",
      "mechanisms"
    ],
    "excerpt": "An updated Hopfield network formulation using exponential interaction functions that connects to transformer attention mechanisms and provides exponential storage capacity compared to classical Hopfield networks.",
    "url": "pages/glossary.html#term-modern-hopfield-network"
  },
  {
    "id": "term-mixture-of-experts",
    "title": "MoE (Mixture of Experts)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "different",
      "efficiency",
      "expert",
      "experts",
      "inputs",
      "mixture",
      "moe",
      "of",
      "specialize",
      "subnetworks",
      "types"
    ],
    "excerpt": "An architecture where different \"expert\" sub-networks specialize in different types of inputs. Enables larger effective model capacity while keeping computation manageable.",
    "url": "pages/glossary.html#term-mixture-of-experts"
  },
  {
    "id": "term-momentum",
    "title": "Momentum",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "accelerates",
      "accumulating",
      "along",
      "average",
      "consistent",
      "dampen",
      "decaying",
      "descent",
      "directions",
      "exponentially",
      "faster",
      "gradient"
    ],
    "excerpt": "An optimization technique that accelerates gradient descent by accumulating an exponentially decaying moving average of past gradients, helping the optimizer move faster along consistent gradient directions and dampen oscillations.",
    "url": "pages/glossary.html#term-momentum"
  },
  {
    "id": "term-monte-carlo-method",
    "title": "Monte Carlo Method",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "algorithms",
      "approximating",
      "broad",
      "carlo",
      "class",
      "complex",
      "computational",
      "data",
      "distributions",
      "estimating",
      "integrals",
      "method"
    ],
    "excerpt": "A broad class of computational algorithms that use repeated random sampling to obtain numerical results, such as estimating integrals, simulating complex systems, or approximating probability distributions.",
    "url": "pages/glossary.html#term-monte-carlo-method"
  },
  {
    "id": "term-monte-carlo-methods-rl",
    "title": "Monte Carlo Methods in RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actual",
      "algorithms",
      "averaging",
      "carlo",
      "complete",
      "episodes",
      "estimate",
      "functions",
      "in",
      "learning",
      "methods",
      "monte"
    ],
    "excerpt": "RL algorithms that estimate value functions by averaging the actual returns observed over complete episodes.",
    "url": "pages/glossary.html#term-monte-carlo-methods-rl"
  },
  {
    "id": "term-monte-carlo-tree-search",
    "title": "Monte Carlo Tree Search (MCTS)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "algorithm",
      "builds",
      "carlo",
      "decision",
      "exploration",
      "guide",
      "learning",
      "mcts",
      "monte",
      "planning",
      "previous"
    ],
    "excerpt": "A search algorithm that builds a decision tree through random simulations, using statistics from previous rollouts to guide exploration toward promising actions.",
    "url": "pages/glossary.html#term-monte-carlo-tree-search"
  },
  {
    "id": "term-montreal-declaration-responsible-ai",
    "title": "Montreal Declaration for Responsible AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "2018",
      "adopted",
      "ai",
      "autonomy",
      "declaration",
      "democratic",
      "development",
      "diversity",
      "equity",
      "establishing",
      "ethics",
      "for"
    ],
    "excerpt": "A declaration adopted in 2018 establishing principles for responsible AI development including well-being, respect for autonomy, privacy, democratic participation, equity, diversity, and prudence.",
    "url": "pages/glossary.html#term-montreal-declaration-responsible-ai"
  },
  {
    "id": "term-moral-status-of-ai",
    "title": "Moral Status of AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "consideration",
      "deserve",
      "ethical",
      "ethics",
      "interests",
      "moral",
      "of",
      "philosophical",
      "possess",
      "question",
      "safety"
    ],
    "excerpt": "The philosophical question of whether AI systems can possess moral standing, such that their interests or welfare deserve ethical consideration.",
    "url": "pages/glossary.html#term-moral-status-of-ai"
  },
  {
    "id": "term-moravecs-paradox",
    "title": "Moravec&amp;#x27;s Paradox",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1980s",
      "computationally",
      "difficult",
      "easy",
      "extremely",
      "hans",
      "highlevel",
      "history",
      "humans",
      "milestones",
      "moravec",
      "moravecampx27s"
    ],
    "excerpt": "The observation by Hans Moravec and others in the 1980s that high-level reasoning tasks are easy for AI while sensorimotor skills that seem simple to humans are extremely difficult to replicate computationally.",
    "url": "pages/glossary.html#term-moravecs-paradox"
  },
  {
    "id": "term-morpheme",
    "title": "Morpheme",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "cannot",
      "divided",
      "endings",
      "further",
      "including",
      "inflectional",
      "language",
      "linguistics",
      "losing",
      "meaning",
      "meaningful",
      "morpheme"
    ],
    "excerpt": "The smallest meaningful unit of language that cannot be further divided without losing its meaning, including roots, prefixes, suffixes, and inflectional endings.",
    "url": "pages/glossary.html#term-morpheme"
  },
  {
    "id": "term-morphology",
    "title": "Morphology",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "branch",
      "combine",
      "compounding",
      "derivation",
      "form",
      "including",
      "inflection",
      "internal",
      "linguistics",
      "morphemes",
      "morphology",
      "nlp"
    ],
    "excerpt": "The branch of linguistics studying the internal structure of words, including how morphemes combine to form words through inflection, derivation, and compounding processes.",
    "url": "pages/glossary.html#term-morphology"
  },
  {
    "id": "term-mosaic-augmentation",
    "title": "Mosaic Augmentation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "allowing",
      "augmentation",
      "combines",
      "computer",
      "contexts",
      "data",
      "detect",
      "four",
      "image",
      "images",
      "learn",
      "model"
    ],
    "excerpt": "A data augmentation technique that combines four training images into a single mosaic image, allowing the model to learn from multiple contexts simultaneously and detect objects at various scales.",
    "url": "pages/glossary.html#term-mosaic-augmentation"
  },
  {
    "id": "term-motivational-control",
    "title": "Motivational Control",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "aligned",
      "alignment",
      "capability",
      "control",
      "goals",
      "human",
      "interests",
      "measures",
      "motivational",
      "opposed",
      "physically"
    ],
    "excerpt": "Safety measures that shape an AI system's goals and values to be aligned with human interests, as opposed to capability control which restricts what the system can physically do.",
    "url": "pages/glossary.html#term-motivational-control"
  },
  {
    "id": "term-moving-average-model",
    "title": "Moving Average Model",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "average",
      "combination",
      "current",
      "data",
      "error",
      "expressed",
      "linear",
      "model",
      "moving",
      "noise",
      "past",
      "science"
    ],
    "excerpt": "A time series model where the current value is expressed as a linear combination of the current and past white noise error terms. It captures short-term dependencies in the data.",
    "url": "pages/glossary.html#term-moving-average-model"
  },
  {
    "id": "term-mt-bench",
    "title": "MT-Bench",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "abilities",
      "across",
      "bench",
      "benchmark",
      "benchmarks",
      "coding",
      "conversational",
      "dialogues",
      "evaluation",
      "followup",
      "framework",
      "judges"
    ],
    "excerpt": "Multi-Turn Benchmark, an evaluation framework that tests language models' conversational abilities across multi-turn dialogues with follow-up questions, using LLM judges to score responses on writing, reasoning, coding, and knowledge tasks.",
    "url": "pages/glossary.html#term-mt-bench"
  },
  {
    "id": "term-multi-agent-rl",
    "title": "Multi-Agent Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "agents",
      "environment",
      "interact",
      "involving",
      "learning",
      "multi",
      "multiagent",
      "multiple",
      "objectives",
      "observations",
      "own"
    ],
    "excerpt": "RL involving multiple agents that interact within a shared environment, each with its own observations and objectives.",
    "url": "pages/glossary.html#term-multi-agent-rl"
  },
  {
    "id": "term-multi-armed-bandit",
    "title": "Multi-Armed Bandit",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "agent",
      "among",
      "armed",
      "arms",
      "bandit",
      "chooses",
      "cumulative",
      "exploration",
      "learning",
      "maximize",
      "multi"
    ],
    "excerpt": "A simplified RL problem where an agent repeatedly chooses among K actions (arms) to maximize cumulative reward, with no state transitions.",
    "url": "pages/glossary.html#term-multi-armed-bandit"
  },
  {
    "id": "term-multi-gpu-training",
    "title": "Multi-GPU Training",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "across",
      "computing",
      "distributed",
      "gpu",
      "gpus",
      "gradient",
      "model",
      "multi",
      "multiple",
      "node",
      "nodes",
      "optimization"
    ],
    "excerpt": "Training a model using multiple GPUs simultaneously within a single node or across nodes, requiring parallelism strategies and gradient synchronization.",
    "url": "pages/glossary.html#term-multi-gpu-training"
  },
  {
    "id": "term-multi-head-attention",
    "title": "Multi-Head Attention",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "aspects",
      "attention",
      "different",
      "extension",
      "focusing",
      "head",
      "multi",
      "multiple",
      "operations",
      "parallel",
      "runs"
    ],
    "excerpt": "An extension of attention that runs multiple attention operations in parallel, each focusing on different aspects. A key component of transformer architectures.",
    "url": "pages/glossary.html#term-multi-head-attention"
  },
  {
    "id": "term-mig",
    "title": "Multi-Instance GPU (MIG)",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "cache",
      "compute",
      "dedicated",
      "feature",
      "gpu",
      "hardware",
      "instance",
      "instances",
      "isolated",
      "memory",
      "mig",
      "multi"
    ],
    "excerpt": "An NVIDIA feature that partitions a single GPU into up to seven isolated instances, each with dedicated compute, memory, and cache resources.",
    "url": "pages/glossary.html#term-mig"
  },
  {
    "id": "term-multi-label-classification",
    "title": "Multi-Label Classification",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "belong",
      "classes",
      "classification",
      "exactly",
      "instance",
      "label",
      "learning",
      "machine",
      "model",
      "multi",
      "multiclass",
      "multiple"
    ],
    "excerpt": "A classification task where each instance can belong to multiple classes simultaneously, unlike multi-class classification where each instance has exactly one label.",
    "url": "pages/glossary.html#term-multi-label-classification"
  },
  {
    "id": "term-multi-object-tracking",
    "title": "Multi-Object Tracking",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "challenges",
      "computer",
      "entering",
      "handling",
      "identity",
      "image",
      "leaving",
      "like",
      "multi",
      "multiple",
      "object",
      "objects"
    ],
    "excerpt": "The task of simultaneously tracking multiple objects through a video sequence, handling challenges like occlusion, identity switches, and objects entering or leaving the scene.",
    "url": "pages/glossary.html#term-multi-object-tracking"
  },
  {
    "id": "term-multi-objective-rl",
    "title": "Multi-Objective RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "conflicting",
      "design",
      "formulations",
      "functions",
      "learning",
      "multi",
      "multiple",
      "must",
      "objective",
      "optimize",
      "potentially"
    ],
    "excerpt": "RL formulations where the agent must optimize multiple potentially conflicting reward functions simultaneously.",
    "url": "pages/glossary.html#term-multi-objective-rl"
  },
  {
    "id": "term-multi-persona-prompting",
    "title": "Multi-Persona Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "assigns",
      "comprehensive",
      "contribute",
      "distinct",
      "engineering",
      "expert",
      "having",
      "multi",
      "multiple",
      "persona",
      "personas",
      "perspective"
    ],
    "excerpt": "A technique that assigns multiple distinct expert personas within a single prompt, having each persona contribute their specialized perspective to a problem and then synthesizing their viewpoints into a comprehensive response.",
    "url": "pages/glossary.html#term-multi-persona-prompting"
  },
  {
    "id": "term-multi-query-attention",
    "title": "Multi-Query Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "autoregressive",
      "bandwidth",
      "decoding",
      "heads",
      "key",
      "memory",
      "multi",
      "networks",
      "neural",
      "projections"
    ],
    "excerpt": "An attention variant where all query heads share a single set of key and value projections, significantly reducing memory bandwidth requirements during autoregressive decoding.",
    "url": "pages/glossary.html#term-multi-query-attention"
  },
  {
    "id": "term-multi-query-retrieval",
    "title": "Multi-Query Retrieval",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "combines",
      "documents",
      "generates",
      "llm",
      "multi",
      "multiple",
      "original",
      "overcome",
      "paraphrased",
      "perspectiveshifted",
      "phrasings",
      "processing"
    ],
    "excerpt": "A technique that generates multiple paraphrased or perspective-shifted versions of the original query using an LLM, retrieves documents for each variant, and combines the results to overcome the sensitivity of retrieval to specific query phrasings.",
    "url": "pages/glossary.html#term-multi-query-retrieval"
  },
  {
    "id": "term-multi-scale-feature-extraction",
    "title": "Multi-Scale Feature Extraction",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "capturing",
      "detection",
      "different",
      "enabling",
      "extraction",
      "feature",
      "features",
      "field",
      "multi",
      "network",
      "networks"
    ],
    "excerpt": "The technique of capturing features at different spatial resolutions or receptive field sizes within a network, enabling detection and recognition of objects at various scales.",
    "url": "pages/glossary.html#term-multi-scale-feature-extraction"
  },
  {
    "id": "term-multi-scale-testing",
    "title": "Multi-Scale Testing",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "combines",
      "computer",
      "cost",
      "detection",
      "evaluation",
      "image",
      "improving",
      "increased",
      "inference",
      "multi",
      "multiple",
      "objects"
    ],
    "excerpt": "An evaluation technique that processes an image at multiple resolutions and combines the predictions, improving detection of objects at various scales at the cost of increased inference time.",
    "url": "pages/glossary.html#term-multi-scale-testing"
  },
  {
    "id": "term-multi-step-bootstrapping",
    "title": "Multi-Step Bootstrapping",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actual",
      "approach",
      "bootstrapping",
      "carlo",
      "estimate",
      "estimation",
      "future",
      "interpolating",
      "learning",
      "methods",
      "monte",
      "multi"
    ],
    "excerpt": "A value estimation approach that uses n actual rewards before bootstrapping with a value estimate for the remaining future, interpolating between one-step TD and Monte Carlo methods.",
    "url": "pages/glossary.html#term-multi-step-bootstrapping"
  },
  {
    "id": "term-multi-step-reasoning",
    "title": "Multi-Step Reasoning",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accurate",
      "breaks",
      "capability",
      "complex",
      "enabling",
      "engineering",
      "exceed",
      "intermediate",
      "model",
      "multi",
      "next",
      "paradigm"
    ],
    "excerpt": "A prompting paradigm that breaks complex problems into a sequence of intermediate reasoning steps, requiring the model to solve each sub-problem before proceeding to the next, enabling accurate sol...",
    "url": "pages/glossary.html#term-multi-step-reasoning"
  },
  {
    "id": "term-multi-task-learning",
    "title": "Multi-Task Learning",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "across",
      "approach",
      "learning",
      "machine",
      "model",
      "multi",
      "multiple",
      "related",
      "representations",
      "selection",
      "sharing",
      "simultaneously"
    ],
    "excerpt": "A learning approach where a model is trained simultaneously on multiple related tasks, sharing representations across tasks.",
    "url": "pages/glossary.html#term-multi-task-learning"
  },
  {
    "id": "term-multi-task-rl",
    "title": "Multi-Task Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "across",
      "approaches",
      "learning",
      "multi",
      "multiple",
      "paradigms",
      "perform",
      "policy",
      "reinforcement",
      "related",
      "simultaneously",
      "single"
    ],
    "excerpt": "RL approaches that train a single policy to perform well across multiple related tasks simultaneously.",
    "url": "pages/glossary.html#term-multi-task-rl"
  },
  {
    "id": "term-multi-tenancy-vector-databases",
    "title": "Multi-Tenancy in Vector Databases",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "ability",
      "applications",
      "data",
      "database",
      "databases",
      "efficient",
      "ensure",
      "filtering",
      "in",
      "infrastructure",
      "isolated",
      "maintaining"
    ],
    "excerpt": "The ability of a vector database to serve multiple isolated users or applications from a shared infrastructure, using namespaces, partitions, or metadata filtering to ensure data separation while maintaining efficient resource utilization.",
    "url": "pages/glossary.html#term-multi-tenancy-vector-databases"
  },
  {
    "id": "term-multi-turn-conversation",
    "title": "Multi-Turn Conversation",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "across",
      "ai",
      "coherence",
      "context",
      "conversation",
      "dialogue",
      "exchanges",
      "format",
      "generative",
      "history",
      "language",
      "llm"
    ],
    "excerpt": "A dialogue format where a language model maintains context across multiple exchanges with a user, requiring the model to track conversation history, resolve references, and maintain coherence.",
    "url": "pages/glossary.html#term-multi-turn-conversation"
  },
  {
    "id": "term-multi-vector-retrieval",
    "title": "Multi-Vector Retrieval",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "approach",
      "architecture",
      "aspects",
      "capturing",
      "computation",
      "cost",
      "different",
      "document",
      "embedding",
      "enabling",
      "finergrained",
      "increased"
    ],
    "excerpt": "A retrieval approach that represents each document as multiple embedding vectors rather than a single vector, capturing different aspects or segments of the document and enabling finer-grained matc...",
    "url": "pages/glossary.html#term-multi-vector-retrieval"
  },
  {
    "id": "term-multi-view-stereo",
    "title": "Multi-View Stereo",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "across",
      "calibrated",
      "camera",
      "computer",
      "computes",
      "consistency",
      "correspondences",
      "dense",
      "depth",
      "establish",
      "many"
    ],
    "excerpt": "A 3D reconstruction method that computes dense depth maps from multiple calibrated camera views, using photometric consistency to establish correspondences across many viewpoints.",
    "url": "pages/glossary.html#term-multi-view-stereo"
  },
  {
    "id": "term-multi-word-expression",
    "title": "Multi-Word Expression",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "collocations",
      "combination",
      "compound",
      "exhibits",
      "expression",
      "idioms",
      "idiosyncrasy",
      "including",
      "lexical",
      "linguistics",
      "multi",
      "nlp"
    ],
    "excerpt": "A combination of words that exhibits lexical, syntactic, semantic, or statistical idiosyncrasy, including idioms, compound nouns, phrasal verbs, and collocations.",
    "url": "pages/glossary.html#term-multi-word-expression"
  },
  {
    "id": "term-multicollinearity",
    "title": "Multicollinearity",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "analysis",
      "coefficient",
      "condition",
      "correlated",
      "data",
      "determine",
      "difficult",
      "effect",
      "errors",
      "estimates",
      "highly",
      "independent"
    ],
    "excerpt": "A condition in regression analysis where two or more independent variables are highly correlated, making it difficult to determine the individual effect of each predictor and inflating standard errors of coefficient estimates.",
    "url": "pages/glossary.html#term-multicollinearity"
  },
  {
    "id": "term-multilingual-model",
    "title": "Multilingual Model",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "abilities",
      "across",
      "crosslingual",
      "data",
      "developing",
      "languages",
      "model",
      "multilingual",
      "multiple",
      "nlp",
      "often",
      "perform"
    ],
    "excerpt": "A single model trained on data from multiple languages that can perform NLP tasks across those languages, often developing cross-lingual transfer abilities from shared representations.",
    "url": "pages/glossary.html#term-multilingual-model"
  },
  {
    "id": "term-multimodal",
    "title": "Multimodal",
    "category": "Glossary",
    "subcategory": "Capability",
    "keywords": [
      "architecture",
      "audio",
      "capability",
      "content",
      "generate",
      "images",
      "multimodal",
      "multiple",
      "process",
      "systems",
      "text",
      "types"
    ],
    "excerpt": "AI systems that can process and generate multiple types of content (text, images, audio, video). Examples include GPT-4V, Gemini, and Claude with vision capabilities.",
    "url": "pages/glossary.html#term-multimodal"
  },
  {
    "id": "term-multinomial-distribution",
    "title": "Multinomial Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "binomial",
      "distribution",
      "experiments",
      "generalization",
      "multinomial",
      "outcomes",
      "possible",
      "probability",
      "statistics",
      "two"
    ],
    "excerpt": "A generalization of the binomial distribution for experiments with more than two possible outcomes. It models the counts of each outcome across a fixed number of independent trials.",
    "url": "pages/glossary.html#term-multinomial-distribution"
  },
  {
    "id": "term-multiple-testing-correction",
    "title": "Multiple Testing Correction",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "adjusting",
      "control",
      "correction",
      "error",
      "hypothesis",
      "inference",
      "many",
      "methods",
      "multiple",
      "overall",
      "performing",
      "rate"
    ],
    "excerpt": "Statistical methods for adjusting significance thresholds when performing many simultaneous hypothesis tests to control the overall error rate.",
    "url": "pages/glossary.html#term-multiple-testing-correction"
  },
  {
    "id": "term-mutual-information",
    "title": "Mutual Information",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "dependence",
      "information",
      "knowing",
      "measure",
      "much",
      "mutual",
      "one",
      "probability",
      "quantifying",
      "random",
      "reduces",
      "statistical"
    ],
    "excerpt": "A measure of the statistical dependence between two random variables, quantifying how much knowing one variable reduces uncertainty about the other.",
    "url": "pages/glossary.html#term-mutual-information"
  },
  {
    "id": "term-mutual-information-feature-selection",
    "title": "Mutual Information Feature Selection",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "engineering",
      "feature",
      "features",
      "filterbased",
      "information",
      "knowing",
      "learning",
      "machine",
      "measuring",
      "method",
      "mutual",
      "provided"
    ],
    "excerpt": "A filter-based feature selection method that ranks features by their mutual information with the target variable, measuring the reduction in uncertainty about the target provided by knowing each feature's value.",
    "url": "pages/glossary.html#term-mutual-information-feature-selection"
  },
  {
    "id": "term-muzero",
    "title": "MuZero",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithm",
      "dynamics",
      "game",
      "knowledge",
      "latent",
      "learning",
      "learns",
      "model",
      "modelbased",
      "muzero",
      "networks",
      "planning"
    ],
    "excerpt": "A model-based RL algorithm that learns a latent dynamics model, reward predictor, and value/policy networks without requiring knowledge of the game rules.",
    "url": "pages/glossary.html#term-muzero"
  },
  {
    "id": "term-mycin",
    "title": "MYCIN",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1970s",
      "antibiotics",
      "bacterial",
      "demonstrating",
      "developed",
      "diagnosing",
      "domains",
      "early",
      "exceed",
      "expert",
      "history",
      "human"
    ],
    "excerpt": "An early expert system developed at Stanford in the 1970s for diagnosing bacterial infections and recommending antibiotics, demonstrating that rule-based AI could match or exceed human expert performance in narrow domains.",
    "url": "pages/glossary.html#term-mycin"
  },
  {
    "id": "term-n-gram",
    "title": "N-gram",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "characters",
      "classification",
      "contiguous",
      "gram",
      "information",
      "items",
      "language",
      "models",
      "nlp",
      "processing",
      "retrieval",
      "sequence"
    ],
    "excerpt": "A contiguous sequence of N items from a text, where items can be characters, words, or tokens, used in language models, text classification, and information retrieval.",
    "url": "pages/glossary.html#term-n-gram"
  },
  {
    "id": "term-n-step-return",
    "title": "N-Step Return",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actual",
      "bootstrapping",
      "carlo",
      "estimate",
      "full",
      "interpolating",
      "learning",
      "methods",
      "monte",
      "ninfinity",
      "onestep",
      "reinforcement"
    ],
    "excerpt": "A return estimate that uses n actual rewards before bootstrapping with a value estimate, interpolating between one-step TD (n=1) and full Monte Carlo (n=infinity).",
    "url": "pages/glossary.html#term-n-step-return"
  },
  {
    "id": "term-naive-bayes",
    "title": "Naive Bayes",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "assumption",
      "based",
      "bayes",
      "class",
      "classifiers",
      "conditionally",
      "family",
      "features",
      "given",
      "independent",
      "label",
      "learning"
    ],
    "excerpt": "A family of probabilistic classifiers based on Bayes' theorem with the strong assumption that features are conditionally independent given the class label.",
    "url": "pages/glossary.html#term-naive-bayes"
  },
  {
    "id": "term-named-entity-linking",
    "title": "Named Entity Linking",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "ambiguity",
      "base",
      "corresponding",
      "entities",
      "entity",
      "entries",
      "knowledge",
      "linking",
      "mapping",
      "mentions",
      "multiple",
      "name"
    ],
    "excerpt": "The task of mapping recognized named entity mentions in text to their corresponding entries in a knowledge base, resolving ambiguity when multiple entities share the same name.",
    "url": "pages/glossary.html#term-named-entity-linking"
  },
  {
    "id": "term-named-entity-recognition",
    "title": "Named Entity Recognition (NER)",
    "category": "Glossary",
    "subcategory": "NLP Task",
    "keywords": [
      "classifies",
      "dates",
      "entities",
      "entity",
      "extraction",
      "identifies",
      "locations",
      "named",
      "ner",
      "nlp",
      "organizations",
      "people"
    ],
    "excerpt": "An NLP task that identifies and classifies named entities (people, organizations, locations, dates) in text. Foundational for information extraction and knowledge graph construction.",
    "url": "pages/glossary.html#term-named-entity-recognition"
  },
  {
    "id": "term-namespace",
    "title": "Namespace",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "applications",
      "database",
      "enabling",
      "index",
      "indexes",
      "infrastructure",
      "isolates",
      "logical",
      "maintaining",
      "mechanism",
      "multitenant",
      "namespace"
    ],
    "excerpt": "A logical partitioning mechanism within a vector database index that isolates vectors into separate searchable segments, enabling multi-tenant applications and scoped queries without maintaining separate physical indexes.",
    "url": "pages/glossary.html#term-namespace"
  },
  {
    "id": "term-narrow-ai",
    "title": "Narrow AI (ANI)",
    "category": "Glossary",
    "subcategory": "Category",
    "keywords": [
      "ai",
      "ani",
      "category",
      "chess",
      "concept",
      "designed",
      "generating",
      "like",
      "narrow",
      "playing",
      "specific",
      "systems"
    ],
    "excerpt": "AI systems designed for specific tasks, like playing chess or generating text. All current AI is narrow, as opposed to hypothetical artificial general intelligence (AGI).",
    "url": "pages/glossary.html#term-narrow-ai"
  },
  {
    "id": "term-narrow-ai-safety",
    "title": "Narrow AI Safety",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "addressing",
      "adversarial",
      "ai",
      "alignment",
      "constrained",
      "currently",
      "deployed",
      "distribution",
      "environments",
      "exploration",
      "focused",
      "inputs"
    ],
    "excerpt": "Safety research focused on currently deployed AI systems, addressing issues such as robustness to distribution shift, adversarial inputs, reward misspecification, and safe exploration in constrained environments.",
    "url": "pages/glossary.html#term-narrow-ai-safety"
  },
  {
    "id": "term-natural-language",
    "title": "Natural Language",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "concept",
      "human",
      "interface",
      "language",
      "natural",
      "naturally",
      "speak",
      "write"
    ],
    "excerpt": "Human language as we naturally speak and write it. AI assistants are designed to understand natural language, so you don't need special syntax or formatting.",
    "url": "pages/glossary.html#term-natural-language"
  },
  {
    "id": "term-natural-language-inference",
    "title": "Natural Language Inference",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "ability",
      "classifying",
      "contradiction",
      "entailment",
      "hypothesis",
      "inference",
      "language",
      "logical",
      "meaning",
      "models",
      "natural",
      "neutral"
    ],
    "excerpt": "The task of classifying the logical relationship between a premise and hypothesis text pair into entailment, contradiction, or neutral, testing a model's ability to reason about meaning.",
    "url": "pages/glossary.html#term-natural-language-inference"
  },
  {
    "id": "term-nlp",
    "title": "Natural Language Processing (NLP)",
    "category": "Glossary",
    "subcategory": "Field",
    "keywords": [
      "computers",
      "enabling",
      "field",
      "focused",
      "generate",
      "human",
      "interpret",
      "language",
      "natural",
      "nlp",
      "processing",
      "understand"
    ],
    "excerpt": "The field of AI focused on enabling computers to understand, interpret, and generate human language. Encompasses tasks from translation to summarization to dialogue.",
    "url": "pages/glossary.html#term-nlp"
  },
  {
    "id": "term-nlp-history",
    "title": "Natural Language Processing History",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1960s",
      "1990s",
      "2010s",
      "approaches",
      "culminating",
      "evolution",
      "history",
      "language",
      "large",
      "methods",
      "milestones",
      "models"
    ],
    "excerpt": "The evolution of NLP from rule-based approaches in the 1960s through statistical methods in the 1990s to neural approaches in the 2010s and the transformer revolution, culminating in modern large language models.",
    "url": "pages/glossary.html#term-nlp-history"
  },
  {
    "id": "term-natural-policy-gradient",
    "title": "Natural Policy Gradient",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "ascent",
      "direction",
      "distributions",
      "fisher",
      "following",
      "gradient",
      "information",
      "inverse",
      "learning",
      "matrix",
      "method",
      "natural"
    ],
    "excerpt": "A policy gradient method that preconditions updates with the inverse Fisher information matrix, following the steepest ascent direction in the space of policy distributions rather than parameter space.",
    "url": "pages/glossary.html#term-natural-policy-gradient"
  },
  {
    "id": "term-natural-questions",
    "title": "Natural Questions",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "answer",
      "answering",
      "answers",
      "articles",
      "benchmark",
      "benchmarks",
      "consisting",
      "evaluation",
      "genuine",
      "google",
      "identify",
      "information"
    ],
    "excerpt": "A question answering benchmark by Google consisting of real user queries from Google Search paired with Wikipedia articles, requiring models to identify both short answers and long answer passages to satisfy genuine information needs.",
    "url": "pages/glossary.html#term-natural-questions"
  },
  {
    "id": "term-nccl",
    "title": "NCCL",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "collective",
      "communication",
      "communications",
      "computing",
      "distributed",
      "gpu",
      "highly",
      "library",
      "multigpu",
      "multinode",
      "nccl",
      "nvidia"
    ],
    "excerpt": "NVIDIA Collective Communications Library, a highly optimized library for multi-GPU and multi-node collective communication operations.",
    "url": "pages/glossary.html#term-nccl"
  },
  {
    "id": "term-ndcg",
    "title": "NDCG",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "against",
      "appearing",
      "assigning",
      "based",
      "cumulative",
      "discounted",
      "earlier",
      "evaluates",
      "evaluation",
      "gain",
      "higher",
      "ideal"
    ],
    "excerpt": "Normalized Discounted Cumulative Gain, a ranking quality metric that evaluates the usefulness of retrieved items based on their position in the result list, assigning higher weights to relevant ite...",
    "url": "pages/glossary.html#term-ndcg"
  },
  {
    "id": "term-needle-in-haystack-test",
    "title": "Needle in a Haystack Test",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ability",
      "ai",
      "attention",
      "context",
      "degradation",
      "evaluation",
      "generative",
      "haystack",
      "in",
      "information",
      "language",
      "llm"
    ],
    "excerpt": "An evaluation method that measures a language model's ability to retrieve a specific piece of information placed at various positions within a long context, revealing attention degradation patterns.",
    "url": "pages/glossary.html#term-needle-in-haystack-test"
  },
  {
    "id": "term-negation-detection",
    "title": "Negation Detection",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "affected",
      "cues",
      "detection",
      "determining",
      "identifying",
      "like",
      "negation",
      "never",
      "nlp",
      "parts",
      "processing",
      "scope"
    ],
    "excerpt": "The task of identifying negation cues and their scope in text, determining which parts of a sentence are affected by negation words like 'not,' 'never,' or 'without.'",
    "url": "pages/glossary.html#term-negation-detection"
  },
  {
    "id": "term-negative-prompt",
    "title": "Negative Prompt",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "avoid",
      "instructions",
      "negative",
      "output",
      "prompt",
      "prompting",
      "technique",
      "telling"
    ],
    "excerpt": "Instructions telling AI what to avoid in its output. Common in image generation (\"no blur, no distortion\") and can be used in text to exclude certain topics or styles.",
    "url": "pages/glossary.html#term-negative-prompt"
  },
  {
    "id": "term-negative-prompting",
    "title": "Negative Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "avoid",
      "behaviors",
      "constrain",
      "constraints",
      "content",
      "desired",
      "engineering",
      "exclusion",
      "explicitly",
      "formats",
      "generation",
      "including"
    ],
    "excerpt": "A technique that explicitly specifies what the model should avoid in its output, including unwanted content, styles, formats, or behaviors, using exclusion instructions to constrain the generation space toward desired outputs.",
    "url": "pages/glossary.html#term-negative-prompting"
  },
  {
    "id": "term-negative-sampling",
    "title": "Negative Sampling",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "approximation",
      "binary",
      "classification",
      "context",
      "embedding",
      "embeddings",
      "examples",
      "full",
      "making",
      "negative",
      "nlp",
      "randomly"
    ],
    "excerpt": "A training approximation that replaces the full softmax over the vocabulary with a binary classification between true context words and randomly sampled negative examples, making embedding training tractable.",
    "url": "pages/glossary.html#term-negative-sampling"
  },
  {
    "id": "term-nerf",
    "title": "NeRF",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "continuous",
      "enabling",
      "field",
      "functions",
      "images",
      "input",
      "method",
      "nerf",
      "networks",
      "neural",
      "novel"
    ],
    "excerpt": "Neural Radiance Field, a method that represents 3D scenes as continuous volumetric functions parameterized by neural networks, enabling photorealistic novel view synthesis from sparse input images.",
    "url": "pages/glossary.html#term-nerf"
  },
  {
    "id": "term-nested-cross-validation",
    "title": "Nested Cross-Validation",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "cross",
      "crossvalidation",
      "estimation",
      "evaluation",
      "hyperparameter",
      "information",
      "inner",
      "leakage",
      "learning",
      "loop",
      "machine",
      "model"
    ],
    "excerpt": "A model evaluation technique using an inner cross-validation loop for hyperparameter tuning and an outer loop for unbiased performance estimation, preventing information leakage from the tuning process.",
    "url": "pages/glossary.html#term-nested-cross-validation"
  },
  {
    "id": "term-nested-ner",
    "title": "Nested Named Entity Recognition",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "america",
      "bank",
      "embedded",
      "entities",
      "entity",
      "handles",
      "location",
      "named",
      "ner",
      "nested",
      "nlp",
      "organization"
    ],
    "excerpt": "A NER variant that handles entities embedded within other entities, such as recognizing both 'Bank of America' as an organization and 'America' as a location within the same span.",
    "url": "pages/glossary.html#term-nested-ner"
  },
  {
    "id": "term-nesterov-accelerated-gradient",
    "title": "Nesterov Accelerated Gradient",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "accelerated",
      "better",
      "computes",
      "convergence",
      "correcting",
      "current",
      "direction",
      "gradient",
      "learning",
      "lookahead",
      "machine",
      "momentum"
    ],
    "excerpt": "A variant of momentum-based optimization that computes the gradient at a lookahead position rather than the current position, providing better convergence properties by correcting the momentum direction before taking a step.",
    "url": "pages/glossary.html#term-nesterov-accelerated-gradient"
  },
  {
    "id": "term-netflix-prize",
    "title": "Netflix Prize",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "20062009",
      "accelerated",
      "algorithm",
      "best",
      "collaborative",
      "competition",
      "competitions",
      "dollars",
      "filtering",
      "history",
      "learning",
      "machine"
    ],
    "excerpt": "A 2006-2009 open competition offering one million dollars for the best collaborative filtering algorithm to predict user movie ratings, which accelerated recommender systems research and popularized machine learning competitions.",
    "url": "pages/glossary.html#term-netflix-prize"
  },
  {
    "id": "term-nas",
    "title": "Neural Architecture Search",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "algorithms",
      "architecture",
      "architectures",
      "automated",
      "design",
      "discovering",
      "evolutionary",
      "gradientbased",
      "learning",
      "methods",
      "network",
      "networks"
    ],
    "excerpt": "An automated process for discovering optimal neural network architectures by searching over a design space using reinforcement learning, evolutionary algorithms, or gradient-based methods.",
    "url": "pages/glossary.html#term-nas"
  },
  {
    "id": "term-nas-vision",
    "title": "Neural Architecture Search for Vision",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "algorithms",
      "architecture",
      "architectures",
      "automated",
      "channel",
      "choices",
      "cnn",
      "computer",
      "connections",
      "design",
      "discovering",
      "evolutionary"
    ],
    "excerpt": "Automated methods for discovering optimal CNN or ViT architectures by searching over design choices (kernel sizes, channel widths, layer connections) using reinforcement learning or evolutionary algorithms.",
    "url": "pages/glossary.html#term-nas-vision"
  },
  {
    "id": "term-hw-aware-nas",
    "title": "Neural Architecture Search Hardware-Aware",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "architecture",
      "architectures",
      "aware",
      "constraints",
      "devices",
      "finding",
      "hardware",
      "incorporate",
      "latency",
      "memory",
      "methods",
      "model"
    ],
    "excerpt": "NAS methods that incorporate hardware constraints (latency, memory, power) into the search objective, finding architectures optimized for specific target devices.",
    "url": "pages/glossary.html#term-hw-aware-nas"
  },
  {
    "id": "term-neural-machine-translation",
    "title": "Neural Machine Translation",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "approach",
      "corpora",
      "encoderdecoder",
      "endtoend",
      "language",
      "learn",
      "machine",
      "map",
      "networks",
      "neural",
      "nlp",
      "parallel"
    ],
    "excerpt": "A machine translation approach using encoder-decoder neural networks that learn to map source language sequences to target language sequences end-to-end from parallel corpora.",
    "url": "pages/glossary.html#term-neural-machine-translation"
  },
  {
    "id": "term-neural-network",
    "title": "Neural Network",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "biological",
      "brains",
      "composed",
      "computing",
      "fundamentals",
      "inspired",
      "interconnected",
      "layers",
      "network",
      "neural",
      "neurons"
    ],
    "excerpt": "A computing system inspired by biological brains, composed of interconnected nodes (neurons) organized in layers. The foundation of modern deep learning and AI.",
    "url": "pages/glossary.html#term-neural-network"
  },
  {
    "id": "term-neural-ode",
    "title": "Neural ODE",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "adaptive",
      "architecture",
      "backward",
      "computation",
      "continuous",
      "continuousdepth",
      "derivative",
      "dynamics",
      "enabling",
      "forward",
      "hidden",
      "network"
    ],
    "excerpt": "A continuous-depth neural network that parameterizes the derivative of hidden states as a neural network and uses ODE solvers for forward and backward passes, enabling adaptive computation and continuous dynamics.",
    "url": "pages/glossary.html#term-neural-ode"
  },
  {
    "id": "term-npu",
    "title": "Neural Processing Unit (NPU)",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "accelerator",
      "dedicated",
      "designed",
      "hardware",
      "inference",
      "infrastructure",
      "integrated",
      "network",
      "neural",
      "npu",
      "ondevice",
      "processing"
    ],
    "excerpt": "A dedicated hardware accelerator designed specifically for neural network inference, typically integrated into SoCs for on-device AI.",
    "url": "pages/glossary.html#term-npu"
  },
  {
    "id": "term-neural-style-transfer",
    "title": "Neural Style Transfer",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "another",
      "applies",
      "artistic",
      "computer",
      "content",
      "features",
      "generated",
      "gram",
      "image",
      "match",
      "matrices",
      "neural"
    ],
    "excerpt": "A technique that applies the artistic style of one image to the content of another by optimizing a generated image to match content features from one source and style features (Gram matrices) from another.",
    "url": "pages/glossary.html#term-neural-style-transfer"
  },
  {
    "id": "term-neural-turing-machine",
    "title": "Neural Turing Machine",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "algorithmic",
      "architecture",
      "attention",
      "augmented",
      "differentiable",
      "enabling",
      "external",
      "learning",
      "machine",
      "mechanisms",
      "memory",
      "network"
    ],
    "excerpt": "A neural architecture augmented with external memory that the network can read from and write to via differentiable attention mechanisms, enabling learning of algorithmic procedures.",
    "url": "pages/glossary.html#term-neural-turing-machine"
  },
  {
    "id": "term-next-token-prediction",
    "title": "Next Token Prediction",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "autoregressive",
      "core",
      "given",
      "llm",
      "llms",
      "next",
      "objective",
      "predict",
      "prediction",
      "previous",
      "token",
      "tokens"
    ],
    "excerpt": "The core training objective of autoregressive LLMs: predict the next token given all previous tokens. This simple objective, at scale, produces sophisticated language understanding.",
    "url": "pages/glossary.html#term-next-token-prediction"
  },
  {
    "id": "term-nf4",
    "title": "NF4 (Normal Float 4-bit)",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "4bit",
      "assumption",
      "based",
      "bit",
      "distribution",
      "float",
      "follow",
      "format",
      "inference",
      "informationtheoretic",
      "infrastructure",
      "model"
    ],
    "excerpt": "A 4-bit quantization format based on the assumption that neural network weights follow a normal distribution, using quantile quantization for optimal information-theoretic representation.",
    "url": "pages/glossary.html#term-nf4"
  },
  {
    "id": "term-nils-nilsson",
    "title": "Nils Nilsson",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19332019",
      "algorithm",
      "american",
      "coinvented",
      "computer",
      "developed",
      "directing",
      "foundational",
      "history",
      "international",
      "knowledge",
      "lab"
    ],
    "excerpt": "American computer scientist (1933-2019) who co-invented the A* search algorithm and developed foundational work in robotics and knowledge representation at SRI International, later directing the Stanford AI Lab.",
    "url": "pages/glossary.html#term-nils-nilsson"
  },
  {
    "id": "term-nist-ai-rmf",
    "title": "NIST AI Risk Management Framework",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "2023",
      "ai",
      "framework",
      "functions",
      "governance",
      "guidance",
      "institute",
      "management",
      "managing",
      "mapping",
      "measuring",
      "national"
    ],
    "excerpt": "A voluntary framework published by the US National Institute of Standards and Technology in 2023 that provides guidance for managing AI risks through governance, mapping, measuring, and managing functions.",
    "url": "pages/glossary.html#term-nist-ai-rmf"
  },
  {
    "id": "term-nli-based-evaluation",
    "title": "NLI-Based Evaluation",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "approach",
      "assess",
      "based",
      "claims",
      "classifying",
      "contradicted",
      "documents",
      "entailed",
      "evaluation",
      "generated",
      "inference",
      "language"
    ],
    "excerpt": "An evaluation approach that uses Natural Language Inference models to assess text quality by classifying whether generated claims are entailed by, contradicted by, or neutral with respect to reference text or source documents.",
    "url": "pages/glossary.html#term-nli-based-evaluation"
  },
  {
    "id": "term-no-free-lunch-theorem",
    "title": "No Free Lunch Theorem",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "across",
      "algorithm",
      "best",
      "free",
      "learning",
      "lunch",
      "machine",
      "model",
      "no",
      "performs",
      "possible",
      "problems"
    ],
    "excerpt": "A set of theoretical results stating that no single learning algorithm performs best across all possible problems. When averaged over all possible data distributions, all algorithms perform equally.",
    "url": "pages/glossary.html#term-no-free-lunch-theorem"
  },
  {
    "id": "term-noise",
    "title": "Noise (ML)",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "concept",
      "data",
      "ml",
      "model",
      "noise",
      "outputs",
      "random",
      "variation"
    ],
    "excerpt": "Random variation in data or model outputs. In training, noise can cause or hide patterns. In diffusion models, controlled noise addition and removal is how images are generated.",
    "url": "pages/glossary.html#term-noise"
  },
  {
    "id": "term-noise-schedule",
    "title": "Noise Schedule",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "added",
      "architecture",
      "determines",
      "diffusion",
      "forward",
      "generation",
      "learned",
      "levels",
      "models",
      "networks",
      "neural",
      "noise"
    ],
    "excerpt": "The predefined or learned sequence of noise levels in diffusion models that determines how quickly noise is added during the forward process and removed during the reverse generation process.",
    "url": "pages/glossary.html#term-noise-schedule"
  },
  {
    "id": "term-noisy-networks",
    "title": "Noisy Networks",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "added",
      "agent",
      "allowing",
      "dqn",
      "epsilongreedy",
      "exploration",
      "extension",
      "learn",
      "learning",
      "level",
      "network",
      "networks"
    ],
    "excerpt": "A DQN extension that replaces epsilon-greedy exploration with parametric noise added to network weights, allowing the agent to learn the optimal level of exploration.",
    "url": "pages/glossary.html#term-noisy-networks"
  },
  {
    "id": "term-non-local-neural-network",
    "title": "Non-Local Neural Network",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "beyond",
      "capturing",
      "computer",
      "computes",
      "dependencies",
      "features",
      "fields",
      "image",
      "images",
      "local",
      "longrange",
      "module"
    ],
    "excerpt": "A neural network module that computes the response at a position as a weighted sum of features at all positions, capturing long-range dependencies in images and video beyond local receptive fields.",
    "url": "pages/glossary.html#term-non-local-neural-network"
  },
  {
    "id": "term-non-maximum-suppression",
    "title": "Non-Maximum Suppression",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "algorithm",
      "bounding",
      "box",
      "computer",
      "detection",
      "eliminates",
      "highestconfidence",
      "instance",
      "keeping",
      "maximum",
      "non",
      "object"
    ],
    "excerpt": "A post-processing algorithm in object detection that eliminates redundant overlapping bounding box predictions by keeping only the highest-confidence detection for each object instance.",
    "url": "pages/glossary.html#term-non-maximum-suppression"
  },
  {
    "id": "term-non-negative-matrix-factorization",
    "title": "Non-Negative Matrix Factorization",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "decomposition",
      "dimensionality",
      "factorization",
      "factors",
      "learning",
      "machine",
      "matrices",
      "matrix",
      "negative",
      "non",
      "nonnegative",
      "partsbased"
    ],
    "excerpt": "A matrix decomposition technique that factors a non-negative matrix into two non-negative matrices, producing parts-based representations.",
    "url": "pages/glossary.html#term-non-negative-matrix-factorization"
  },
  {
    "id": "term-norbert-wiener",
    "title": "Norbert Wiener",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "18941964",
      "1948",
      "american",
      "book",
      "communication",
      "control",
      "cybernetics",
      "establishing",
      "feedback",
      "founded",
      "his",
      "history"
    ],
    "excerpt": "American mathematician (1894-1964) who founded cybernetics in his 1948 book of the same name, establishing the study of feedback, control, and communication in machines and living organisms as a precursor to AI.",
    "url": "pages/glossary.html#term-norbert-wiener"
  },
  {
    "id": "term-normal-distribution",
    "title": "Normal Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bellshaped",
      "characterized",
      "continuous",
      "curve",
      "determined",
      "deviation",
      "distribution",
      "fully",
      "mean",
      "normal",
      "probability",
      "standard"
    ],
    "excerpt": "A continuous probability distribution characterized by its bell-shaped curve, symmetric about the mean, fully determined by its mean and standard deviation.",
    "url": "pages/glossary.html#term-normal-distribution"
  },
  {
    "id": "term-normality-test",
    "title": "Normality Test",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "dataset",
      "distribution",
      "evaluates",
      "follows",
      "inference",
      "normal",
      "normality",
      "statistical",
      "statistics",
      "test",
      "whether"
    ],
    "excerpt": "A statistical test that evaluates whether a dataset follows a normal distribution. Common methods include the Shapiro-Wilk test, Kolmogorov-Smirnov test, and Anderson-Darling test.",
    "url": "pages/glossary.html#term-normality-test"
  },
  {
    "id": "term-normalization",
    "title": "Normalization",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "inputs",
      "layer",
      "networks",
      "neural",
      "normalization",
      "outputs",
      "standardize",
      "technique",
      "techniques",
      "training"
    ],
    "excerpt": "Techniques to standardize inputs or layer outputs in neural networks. Layer normalization is critical in transformers for stable training and better generalization.",
    "url": "pages/glossary.html#term-normalization"
  },
  {
    "id": "term-normalizing-flow",
    "title": "Normalizing Flow",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "base",
      "complex",
      "determinants",
      "differentiable",
      "distribution",
      "flow",
      "generative",
      "invertible",
      "jacobian",
      "model",
      "networks"
    ],
    "excerpt": "A generative model that transforms a simple base distribution into a complex target distribution through a sequence of invertible and differentiable transformations with tractable Jacobian determinants.",
    "url": "pages/glossary.html#term-normalizing-flow"
  },
  {
    "id": "term-novel-view-synthesis",
    "title": "Novel View Synthesis",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "computer",
      "field",
      "gaussian",
      "generating",
      "images",
      "input",
      "interpolation",
      "light",
      "like",
      "nerf",
      "novel"
    ],
    "excerpt": "The task of generating photorealistic images of a scene from viewpoints not present in the input photographs, using techniques like NeRF, Gaussian splatting, or light field interpolation.",
    "url": "pages/glossary.html#term-novel-view-synthesis"
  },
  {
    "id": "term-nucleus-sampling",
    "title": "Nucleus Sampling (Top-p)",
    "category": "Glossary",
    "subcategory": "Generation",
    "keywords": [
      "cumulative",
      "exceeds",
      "generation",
      "nucleus",
      "parameter",
      "probability",
      "samples",
      "sampling",
      "set",
      "smallest",
      "strategy",
      "text"
    ],
    "excerpt": "A text generation strategy that samples from the smallest set of tokens whose cumulative probability exceeds p. Balances diversity and quality better than pure random sampling.",
    "url": "pages/glossary.html#term-nucleus-sampling"
  },
  {
    "id": "term-null-hypothesis",
    "title": "Null Hypothesis",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "assumption",
      "default",
      "difference",
      "effect",
      "groups",
      "hypothesis",
      "inference",
      "null",
      "statistical",
      "statistics",
      "testing"
    ],
    "excerpt": "A default assumption in statistical hypothesis testing that there is no effect or no difference between groups.",
    "url": "pages/glossary.html#term-null-hypothesis"
  },
  {
    "id": "term-a100",
    "title": "NVIDIA A100",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "80gb",
      "a100",
      "ampere",
      "architecture",
      "based",
      "capability",
      "core",
      "featuring",
      "gpu",
      "hardware",
      "hbm2e",
      "memory"
    ],
    "excerpt": "NVIDIA's third-generation Tensor Core GPU based on the Ampere architecture, featuring 80GB HBM2e memory, support for TF32 and structural sparsity, and multi-instance GPU (MIG) capability.",
    "url": "pages/glossary.html#term-a100"
  },
  {
    "id": "term-b200",
    "title": "NVIDIA B200",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "architecture",
      "b200",
      "bandwidth",
      "blackwell",
      "designed",
      "engine",
      "featuring",
      "fp4",
      "gpu",
      "hardware",
      "increased",
      "inference"
    ],
    "excerpt": "NVIDIA's Blackwell architecture GPU designed for next-generation AI training and inference, featuring second-generation Transformer Engine with FP4 support and significantly increased memory bandwidth.",
    "url": "pages/glossary.html#term-b200"
  },
  {
    "id": "term-nvidia-grace",
    "title": "NVIDIA Grace CPU",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "armbased",
      "bandwidth",
      "center",
      "computing",
      "connectivity",
      "cpu",
      "data",
      "designed",
      "direct",
      "distributed",
      "featuring",
      "gpus"
    ],
    "excerpt": "NVIDIA's ARM-based data center CPU designed for AI and HPC workloads, featuring high memory bandwidth via LPDDR5X and direct NVLink connectivity to NVIDIA GPUs.",
    "url": "pages/glossary.html#term-nvidia-grace"
  },
  {
    "id": "term-h100",
    "title": "NVIDIA H100",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "80gb",
      "architecture",
      "based",
      "core",
      "engine",
      "featuring",
      "fourthgeneration",
      "fp8",
      "gpu",
      "h100",
      "hardware",
      "hbm3"
    ],
    "excerpt": "NVIDIA's fourth-generation Tensor Core GPU based on the Hopper architecture, featuring the Transformer Engine with FP8 precision, 80GB HBM3 memory, and fourth-generation NVLink.",
    "url": "pages/glossary.html#term-h100"
  },
  {
    "id": "term-nvlink",
    "title": "NVLink",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "bus",
      "bypassing",
      "communication",
      "computing",
      "direct",
      "distributed",
      "gputogpu",
      "hardware",
      "highbandwidth",
      "interconnect",
      "lowlatency",
      "nvidias"
    ],
    "excerpt": "NVIDIA's proprietary high-bandwidth, low-latency interconnect for direct GPU-to-GPU communication, bypassing the PCIe bus.",
    "url": "pages/glossary.html#term-nvlink"
  },
  {
    "id": "term-nvswitch",
    "title": "NVSwitch",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "alltoall",
      "bandwidth",
      "communication",
      "computing",
      "connected",
      "distributed",
      "enables",
      "fabric",
      "full",
      "fully",
      "gpu",
      "hardware"
    ],
    "excerpt": "NVIDIA's fully connected switch fabric that enables all-to-all GPU communication within a node at full NVLink bandwidth.",
    "url": "pages/glossary.html#term-nvswitch"
  },
  {
    "id": "term-object-tracking",
    "title": "Object Tracking",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "across",
      "appearance",
      "assignments",
      "combine",
      "computer",
      "consistent",
      "detection",
      "features",
      "following",
      "frames",
      "identity",
      "image"
    ],
    "excerpt": "The task of following one or more objects across video frames by maintaining consistent identity assignments, using methods that combine detection, appearance features, and motion prediction.",
    "url": "pages/glossary.html#term-object-tracking"
  },
  {
    "id": "term-observation-normalization",
    "title": "Observation Normalization",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "concepts",
      "core",
      "improving",
      "input",
      "learning",
      "mean",
      "network",
      "neural",
      "normalization",
      "normalizing",
      "observation",
      "observations"
    ],
    "excerpt": "The technique of normalizing input observations to zero mean and unit variance using running statistics, improving neural network training stability.",
    "url": "pages/glossary.html#term-observation-normalization"
  },
  {
    "id": "term-observation-space",
    "title": "Observation Space",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "bounds",
      "concepts",
      "core",
      "data",
      "environment",
      "format",
      "including",
      "learning",
      "observation",
      "observations",
      "ranges"
    ],
    "excerpt": "The specification of the format and bounds of observations that an RL agent receives from the environment, including data type, shape, and valid ranges.",
    "url": "pages/glossary.html#term-observation-space"
  },
  {
    "id": "term-occams-razor",
    "title": "Occam&amp;#x27;s Razor",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "complex",
      "data",
      "equally",
      "explain",
      "favoring",
      "learning",
      "machine",
      "model",
      "models",
      "occamampx27s",
      "ones",
      "principle"
    ],
    "excerpt": "A principle favoring simpler models over more complex ones when both explain the data equally well. In machine learning, it motivates regularization and model selection criteria that penalize complexity.",
    "url": "pages/glossary.html#term-occams-razor"
  },
  {
    "id": "term-occupancy-network",
    "title": "Occupancy Network",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "computer",
      "continuous",
      "discrete",
      "empty",
      "functions",
      "implicit",
      "meshes",
      "network",
      "neural",
      "occupancy",
      "occupied"
    ],
    "excerpt": "A neural network that predicts whether each point in 3D space is occupied or empty, representing 3D shapes as continuous implicit functions rather than discrete voxels or meshes.",
    "url": "pages/glossary.html#term-occupancy-network"
  },
  {
    "id": "term-oecd-ai-principles",
    "title": "OECD AI Principles",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "2019",
      "accountability",
      "adopted",
      "ai",
      "countries",
      "governance",
      "growth",
      "human",
      "including",
      "inclusive",
      "innovative",
      "member"
    ],
    "excerpt": "Principles adopted by OECD member countries in 2019 promoting AI that is innovative, trustworthy, and respects human rights, including recommendations on transparency, robustness, accountability, and inclusive growth.",
    "url": "pages/glossary.html#term-oecd-ai-principles"
  },
  {
    "id": "term-offensive-language-detection",
    "title": "Offensive Language Detection",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "abusive",
      "classifying",
      "content",
      "detection",
      "distinguishing",
      "general",
      "inappropriate",
      "insults",
      "language",
      "moderation",
      "nlp",
      "offensive"
    ],
    "excerpt": "The task of classifying text as offensive, abusive, or inappropriate, distinguishing between targeted insults, profanity, and general offensive content for content moderation.",
    "url": "pages/glossary.html#term-offensive-language-detection"
  },
  {
    "id": "term-offline-rl",
    "title": "Offline Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "collected",
      "dataset",
      "entirely",
      "environment",
      "experience",
      "fixed",
      "further",
      "interaction",
      "learning",
      "learns",
      "offline",
      "paradigm"
    ],
    "excerpt": "An RL paradigm that learns policies entirely from a fixed dataset of previously collected experience without further environment interaction.",
    "url": "pages/glossary.html#term-offline-rl"
  },
  {
    "id": "term-offloading",
    "title": "Offloading (CPU/Disk)",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "activations",
      "back",
      "computing",
      "cpu",
      "disk",
      "distributed",
      "gpu",
      "insufficient",
      "memory",
      "model",
      "needed",
      "offloading"
    ],
    "excerpt": "A technique that stores model parameters, optimizer states, or activations in CPU RAM or disk when GPU memory is insufficient, transferring them back when needed.",
    "url": "pages/glossary.html#term-offloading"
  },
  {
    "id": "term-oliver-selfridge",
    "title": "Oliver Selfridge",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19262008",
      "1959",
      "american",
      "called",
      "computer",
      "contributions",
      "created",
      "early",
      "father",
      "history",
      "learning",
      "machine"
    ],
    "excerpt": "American computer scientist (1926-2008) who created the Pandemonium model for pattern recognition in 1959 and made early contributions to machine learning, often called the father of machine perception.",
    "url": "pages/glossary.html#term-oliver-selfridge"
  },
  {
    "id": "term-ollama",
    "title": "Ollama",
    "category": "Glossary",
    "subcategory": "Tools",
    "keywords": [
      "ai",
      "computers",
      "llms",
      "local",
      "locally",
      "ollama",
      "personal",
      "running",
      "tool",
      "tools"
    ],
    "excerpt": "A tool for running LLMs locally on personal computers. Simplifies downloading and running open-source models like Llama, enabling private, offline AI use.",
    "url": "pages/glossary.html#term-ollama"
  },
  {
    "id": "term-on-policy-vs-off-policy",
    "title": "On-Policy vs Off-Policy",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "being",
      "concepts",
      "core",
      "currently",
      "distinction",
      "executed",
      "learn",
      "learning",
      "methods",
      "off",
      "on",
      "onpolicy"
    ],
    "excerpt": "A distinction between RL methods that learn about the policy currently being executed (on-policy, e.g.",
    "url": "pages/glossary.html#term-on-policy-vs-off-policy"
  },
  {
    "id": "term-one-hot-encoding",
    "title": "One-Hot Encoding",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "binary",
      "categorical",
      "category",
      "converts",
      "corresponding",
      "creating",
      "elsewhere",
      "encoding",
      "engineering",
      "feature",
      "hot",
      "learning"
    ],
    "excerpt": "A feature encoding technique that converts each categorical value into a binary vector with a single 1 at the position corresponding to that category and 0s elsewhere, creating one new binary feature per category.",
    "url": "pages/glossary.html#term-one-hot-encoding"
  },
  {
    "id": "term-one-shot",
    "title": "One-Shot Learning",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "demonstrate",
      "desired",
      "example",
      "format",
      "learning",
      "one",
      "output",
      "prompt",
      "prompting",
      "providing",
      "shot",
      "single"
    ],
    "excerpt": "Providing a single example in your prompt to demonstrate the desired output format or style. Falls between zero-shot (no examples) and few-shot (multiple examples).",
    "url": "pages/glossary.html#term-one-shot"
  },
  {
    "id": "term-one-vs-rest-classification",
    "title": "One-Vs-Rest Classification",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "binary",
      "class",
      "classification",
      "classifier",
      "classifiers",
      "extending",
      "learning",
      "machine",
      "model",
      "multiclass",
      "negative",
      "one"
    ],
    "excerpt": "A strategy for extending binary classifiers to multi-class problems by training one classifier per class, treating that class as positive and all others as negative.",
    "url": "pages/glossary.html#term-one-vs-rest-classification"
  },
  {
    "id": "term-online-learning",
    "title": "Online Learning",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "arrives",
      "batch",
      "being",
      "data",
      "fixed",
      "incrementally",
      "learning",
      "machine",
      "model",
      "new",
      "online",
      "paradigm"
    ],
    "excerpt": "A learning paradigm where the model is updated incrementally as each new data point arrives, rather than being trained on a fixed batch.",
    "url": "pages/glossary.html#term-online-learning"
  },
  {
    "id": "term-onnx",
    "title": "ONNX (Open Neural Network Exchange)",
    "category": "Glossary",
    "subcategory": "Format",
    "keywords": [
      "different",
      "enabling",
      "exchange",
      "format",
      "frameworks",
      "interoperability",
      "models",
      "network",
      "neural",
      "onnx",
      "open",
      "representing"
    ],
    "excerpt": "An open format for representing ML models, enabling interoperability between different frameworks. Allows models trained in PyTorch to run in TensorFlow, etc.",
    "url": "pages/glossary.html#term-onnx"
  },
  {
    "id": "term-onnx-runtime",
    "title": "ONNX Runtime",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "crossplatform",
      "engine",
      "exchange",
      "executes",
      "format",
      "hardwarespecific",
      "inference",
      "infrastructure",
      "microsofts",
      "model",
      "models",
      "network"
    ],
    "excerpt": "Microsoft's cross-platform inference engine that executes models in the Open Neural Network Exchange format with hardware-specific optimizations.",
    "url": "pages/glossary.html#term-onnx-runtime"
  },
  {
    "id": "term-open-source",
    "title": "Open Source / Open Weight",
    "category": "Glossary",
    "subcategory": "Licensing",
    "keywords": [
      "access",
      "available",
      "downloaded",
      "licensing",
      "locally",
      "models",
      "open",
      "publicly",
      "run",
      "source",
      "weight",
      "weights"
    ],
    "excerpt": "AI models with publicly available weights that can be downloaded and run locally. Ranges from fully open (Llama) to restricted licenses. Enables customization and transparency.",
    "url": "pages/glossary.html#term-open-source"
  },
  {
    "id": "term-open-source-ai-movement",
    "title": "Open Source AI Movement",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "ai",
      "broader",
      "capabilities",
      "code",
      "debates",
      "democratization",
      "development",
      "dualuse",
      "enabling",
      "governance",
      "growing",
      "history"
    ],
    "excerpt": "The growing trend of releasing AI model weights and training code publicly, enabling broader research and development while sparking debates about safety, dual-use risks, and the democratization of powerful AI capabilities.",
    "url": "pages/glossary.html#term-open-source-ai-movement"
  },
  {
    "id": "term-open-domain-qa",
    "title": "Open-Domain Question Answering",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "answer",
      "answering",
      "being",
      "combining",
      "context",
      "corpus",
      "domain",
      "given",
      "knowledge",
      "large",
      "model",
      "must"
    ],
    "excerpt": "A QA setting where the model must answer questions using a large corpus or parametric knowledge without being given a specific context passage, often combining retrieval with reading.",
    "url": "pages/glossary.html#term-open-domain-qa"
  },
  {
    "id": "term-open-vocabulary-detection",
    "title": "Open-Vocabulary Detection",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "approaches",
      "beyond",
      "categories",
      "class",
      "computer",
      "detection",
      "fixed",
      "generalize",
      "identify",
      "leveraging",
      "localize",
      "models"
    ],
    "excerpt": "Object detection approaches that can identify and localize objects from categories not seen during training, leveraging vision-language models to generalize beyond fixed class vocabularies.",
    "url": "pages/glossary.html#term-open-vocabulary-detection"
  },
  {
    "id": "term-openai",
    "title": "OpenAI",
    "category": "Glossary",
    "subcategory": "Company",
    "keywords": [
      "behind",
      "chatgpt",
      "company",
      "dalle",
      "gpt",
      "llm",
      "models",
      "openai",
      "provider",
      "research"
    ],
    "excerpt": "The AI research company behind GPT models, ChatGPT, and DALL-E. Founded in 2015, it has been central to the development and popularization of modern AI assistants.",
    "url": "pages/glossary.html#term-openai"
  },
  {
    "id": "term-openai-founding",
    "title": "OpenAI Founding",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2015",
      "altman",
      "artificial",
      "benefits",
      "december",
      "elon",
      "ensuring",
      "founding",
      "general",
      "history",
      "humanity",
      "intelligence"
    ],
    "excerpt": "The founding of OpenAI in December 2015 as a non-profit AI research laboratory by Sam Altman, Elon Musk, and others, with the mission of ensuring artificial general intelligence benefits all of humanity.",
    "url": "pages/glossary.html#term-openai-founding"
  },
  {
    "id": "term-openai-capped-profit",
    "title": "OpenAI Transition to Capped Profit",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2019",
      "attract",
      "became",
      "capital",
      "capped",
      "cappedprofit",
      "company",
      "controversial",
      "creating",
      "governance",
      "history",
      "hybrid"
    ],
    "excerpt": "OpenAI's 2019 restructuring from a non-profit to a capped-profit company to attract the capital needed for large-scale AI research, creating a hybrid governance structure that later became controversial.",
    "url": "pages/glossary.html#term-openai-capped-profit"
  },
  {
    "id": "term-openpose",
    "title": "OpenPose",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "affinity",
      "body",
      "computer",
      "confidence",
      "detect",
      "estimation",
      "facial",
      "fields",
      "foot",
      "hand",
      "image",
      "images"
    ],
    "excerpt": "A real-time multi-person pose estimation system that uses part affinity fields and confidence maps to detect body, hand, facial, and foot keypoints in images with multiple people.",
    "url": "pages/glossary.html#term-openpose"
  },
  {
    "id": "term-operator-fusion",
    "title": "Operator Fusion",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "combines",
      "compiler",
      "fusion",
      "inference",
      "infrastructure",
      "kernel",
      "launch",
      "memory",
      "model",
      "multiple",
      "network",
      "neural"
    ],
    "excerpt": "A compiler optimization that combines multiple sequential neural network operations into a single kernel launch, reducing memory I/O and kernel launch overhead.",
    "url": "pages/glossary.html#term-operator-fusion"
  },
  {
    "id": "term-opponent-modeling",
    "title": "Opponent Modeling",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agents",
      "behavior",
      "environment",
      "explicitly",
      "goals",
      "learning",
      "modeling",
      "multiagent",
      "opponent",
      "practice",
      "reinforcement",
      "strategy"
    ],
    "excerpt": "The practice of explicitly modeling the behavior, goals, or strategy of other agents in a multi-agent environment.",
    "url": "pages/glossary.html#term-opponent-modeling"
  },
  {
    "id": "term-optical-character-recognition",
    "title": "Optical Character Recognition",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "character",
      "characters",
      "computer",
      "converts",
      "detection",
      "handwritten",
      "identify",
      "image",
      "images",
      "individual",
      "localize",
      "machinereadable"
    ],
    "excerpt": "The technology that converts images of text (handwritten, printed, or typed) into machine-readable text, using detection to localize text regions and recognition to identify individual characters or words.",
    "url": "pages/glossary.html#term-optical-character-recognition"
  },
  {
    "id": "term-optical-flow",
    "title": "Optical Flow",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "action",
      "analysis",
      "apparent",
      "camera",
      "computer",
      "consecutive",
      "estimation",
      "flow",
      "frames",
      "image",
      "motion",
      "movement"
    ],
    "excerpt": "The estimation of per-pixel motion vectors between consecutive video frames, representing the apparent movement of objects or camera, used in video analysis, stabilization, and action recognition.",
    "url": "pages/glossary.html#term-optical-flow"
  },
  {
    "id": "term-optical-flow-estimation",
    "title": "Optical Flow Estimation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "computational",
      "computer",
      "consecutive",
      "deep",
      "dense",
      "displacement",
      "estimation",
      "fields",
      "flow",
      "flownet",
      "frames",
      "image"
    ],
    "excerpt": "The computational process of predicting dense pixel-level displacement fields between consecutive video frames using deep learning models like RAFT or FlowNet that learn motion patterns.",
    "url": "pages/glossary.html#term-optical-flow-estimation"
  },
  {
    "id": "term-optimization",
    "title": "Optimization",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "adjusting",
      "loss",
      "minimize",
      "model",
      "optimization",
      "parameters",
      "process",
      "training"
    ],
    "excerpt": "The process of adjusting model parameters to minimize loss. Includes algorithms (Adam, SGD), learning rate schedules, and techniques for finding good solutions efficiently.",
    "url": "pages/glossary.html#term-optimization"
  },
  {
    "id": "term-option-framework",
    "title": "Option Framework",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "abstraction",
      "actions",
      "concepts",
      "condition",
      "consisting",
      "core",
      "extended",
      "formalism",
      "framework",
      "initiation",
      "internal",
      "learning"
    ],
    "excerpt": "A formalism for temporal abstraction in RL where options are temporally extended actions consisting of an initiation set, an internal policy, and a termination condition.",
    "url": "pages/glossary.html#term-option-framework"
  },
  {
    "id": "term-oracle",
    "title": "Oracle (ML)",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "benchmark",
      "concept",
      "evaluation",
      "information",
      "ml",
      "model",
      "oracle",
      "perfect",
      "source",
      "theoretical"
    ],
    "excerpt": "A theoretical perfect model or information source used as a benchmark. In evaluation, comparing to an oracle helps understand the ceiling of achievable performance.",
    "url": "pages/glossary.html#term-oracle"
  },
  {
    "id": "term-ordinal-regression",
    "title": "Ordinal Regression",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "analysis",
      "categorical",
      "learning",
      "machine",
      "model",
      "ordered",
      "ordinal",
      "outcomes",
      "predicting",
      "regression",
      "selection",
      "type"
    ],
    "excerpt": "A type of regression analysis for predicting ordinal (ordered categorical) outcomes. It models the cumulative probabilities of the ordered categories using a link function and threshold parameters.",
    "url": "pages/glossary.html#term-ordinal-regression"
  },
  {
    "id": "term-orpo",
    "title": "ORPO",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "alignment",
      "combines",
      "distinguish",
      "finetuning",
      "generative",
      "llm",
      "method",
      "odds",
      "optimization",
      "orpo",
      "preference"
    ],
    "excerpt": "Odds Ratio Preference Optimization, an alignment method that combines supervised fine-tuning and preference alignment in a single training stage by using odds ratios to distinguish preferred from rejected responses.",
    "url": "pages/glossary.html#term-orpo"
  },
  {
    "id": "term-orthogonality-thesis",
    "title": "Orthogonality Thesis",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "alignment",
      "benevolent",
      "claim",
      "combined",
      "goal",
      "goals",
      "implying",
      "intelligence",
      "level",
      "meaning",
      "need"
    ],
    "excerpt": "The philosophical claim that intelligence and goals are orthogonal, meaning that any level of intelligence can in principle be combined with any terminal goal, implying that a superintelligent AI need not be benevolent.",
    "url": "pages/glossary.html#term-orthogonality-thesis"
  },
  {
    "id": "term-out-of-bag-error",
    "title": "Out-of-Bag Error",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "bag",
      "bagged",
      "base",
      "bootstrap",
      "computed",
      "error",
      "estimate",
      "included",
      "learner",
      "learning",
      "machine",
      "metrics"
    ],
    "excerpt": "An estimate of prediction error for bagged models computed using observations not included in the bootstrap sample for each base learner.",
    "url": "pages/glossary.html#term-out-of-bag-error"
  },
  {
    "id": "term-out-of-distribution",
    "title": "Out-of-Distribution (OOD)",
    "category": "Glossary",
    "subcategory": "Challenge",
    "keywords": [
      "challenge",
      "data",
      "differs",
      "distribution",
      "model",
      "of",
      "ood",
      "out",
      "robustness",
      "significantly",
      "trained"
    ],
    "excerpt": "Data that differs significantly from what a model was trained on. Models often perform poorly on OOD data, making detection important for reliable deployment.",
    "url": "pages/glossary.html#term-out-of-distribution"
  },
  {
    "id": "term-out-of-vocabulary",
    "title": "Out-of-Vocabulary",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "characterlevel",
      "encountered",
      "handling",
      "inference",
      "models",
      "nlp",
      "of",
      "out",
      "present",
      "requiring",
      "special",
      "subword"
    ],
    "excerpt": "Words or tokens encountered during inference that were not present in the model's training vocabulary, requiring special handling through subword tokenization, character-level models, or UNK tokens.",
    "url": "pages/glossary.html#term-out-of-vocabulary"
  },
  {
    "id": "term-outcome-reward-model",
    "title": "Outcome Reward Model",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "better",
      "end",
      "evaluates",
      "final",
      "generation",
      "generative",
      "holistic",
      "llm",
      "model",
      "only",
      "outcome"
    ],
    "excerpt": "A reward model that evaluates only the final output of a generation, providing a holistic quality score used in RLHF to train the policy model toward producing better end results.",
    "url": "pages/glossary.html#term-outcome-reward-model"
  },
  {
    "id": "term-outer-alignment",
    "title": "Outer Alignment",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "accurately",
      "ai",
      "alignment",
      "base",
      "captures",
      "designer",
      "ensuring",
      "function",
      "goal",
      "intended",
      "loss",
      "objective"
    ],
    "excerpt": "The problem of ensuring that the base objective or loss function used during training accurately captures the intended goal of the system designer.",
    "url": "pages/glossary.html#term-outer-alignment"
  },
  {
    "id": "term-outlier-detection",
    "title": "Outlier Detection",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "data",
      "detection",
      "differ",
      "identifying",
      "majority",
      "observations",
      "outlier",
      "points",
      "process",
      "science",
      "significantly",
      "statistics"
    ],
    "excerpt": "The process of identifying data points that differ significantly from the majority of observations. Methods include statistical tests (Z-score, IQR), distance-based approaches, and model-based techniques.",
    "url": "pages/glossary.html#term-outlier-detection"
  },
  {
    "id": "term-outpainting",
    "title": "Outpainting",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "beyond",
      "boundaries",
      "coherent",
      "content",
      "context",
      "continues",
      "direction",
      "existing",
      "extending",
      "generating",
      "generative"
    ],
    "excerpt": "The task of extending an image beyond its original boundaries by generating new coherent content that seamlessly continues the existing visual context in any direction.",
    "url": "pages/glossary.html#term-outpainting"
  },
  {
    "id": "term-output-layer",
    "title": "Output Layer",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "final",
      "layer",
      "models",
      "network",
      "networks",
      "neural",
      "output",
      "predictions",
      "produces"
    ],
    "excerpt": "The final layer of a neural network that produces the model's predictions. Its design depends on the task: softmax for classification, linear for regression.",
    "url": "pages/glossary.html#term-output-layer"
  },
  {
    "id": "term-overestimation-bias",
    "title": "Overestimation Bias",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "algorithms",
      "bellman",
      "bias",
      "due",
      "equation",
      "learning",
      "max",
      "methods",
      "operator",
      "optimality",
      "overestimate"
    ],
    "excerpt": "A systematic tendency of Q-learning and related algorithms to overestimate action values due to the max operator in the Bellman optimality equation.",
    "url": "pages/glossary.html#term-overestimation-bias"
  },
  {
    "id": "term-overfitting",
    "title": "Overfitting",
    "category": "Glossary",
    "subcategory": "Problem",
    "keywords": [
      "data",
      "examples",
      "general",
      "having",
      "learning",
      "memorized",
      "model",
      "new",
      "overfitting",
      "patterns",
      "performs",
      "poorly"
    ],
    "excerpt": "When a model performs well on training data but poorly on new data, having memorized specific examples rather than learning general patterns. Addressed through regularization and validation.",
    "url": "pages/glossary.html#term-overfitting"
  },
  {
    "id": "term-p-value",
    "title": "P-Value",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "assuming",
      "computed",
      "data",
      "extreme",
      "hypothesis",
      "inference",
      "least",
      "null",
      "observing",
      "one",
      "probability",
      "statistic"
    ],
    "excerpt": "The probability of observing a test statistic at least as extreme as the one computed from the data, assuming the null hypothesis is true.",
    "url": "pages/glossary.html#term-p-value"
  },
  {
    "id": "term-pac-learning",
    "title": "PAC Learning",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithm",
      "approximately",
      "conditions",
      "correct",
      "data",
      "defines",
      "framework",
      "given",
      "high",
      "hypothesis",
      "learning",
      "machine"
    ],
    "excerpt": "Probably Approximately Correct learning, a theoretical framework that defines the conditions under which a learning algorithm can, with high probability, produce a hypothesis that is approximately correct, given sufficient training data.",
    "url": "pages/glossary.html#term-pac-learning"
  },
  {
    "id": "term-padding",
    "title": "Padding",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "addition",
      "around",
      "borders",
      "computer",
      "controlling",
      "convolution",
      "dimensions",
      "edge",
      "extra",
      "feature",
      "image",
      "information"
    ],
    "excerpt": "The addition of extra values (typically zeros) around the borders of an input image or feature map before convolution, controlling the spatial dimensions of the output and preserving edge information.",
    "url": "pages/glossary.html#term-padding"
  },
  {
    "id": "term-paged-attention",
    "title": "Paged Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "batched",
      "caches",
      "efficient",
      "enabling",
      "inference",
      "keys",
      "llm",
      "management",
      "memory",
      "networks"
    ],
    "excerpt": "A memory management technique for KV caches during LLM serving that stores attention keys and values in non-contiguous memory pages, reducing waste and enabling efficient batched inference.",
    "url": "pages/glossary.html#term-paged-attention"
  },
  {
    "id": "term-palm",
    "title": "PaLM",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "540billion",
      "architecture",
      "breakthrough",
      "demonstrated",
      "dense",
      "efficient",
      "google",
      "language",
      "model",
      "networks",
      "neural",
      "palm"
    ],
    "excerpt": "Pathways Language Model, a 540-billion parameter dense transformer by Google that demonstrated breakthrough performance on reasoning tasks using the Pathways system for efficient training.",
    "url": "pages/glossary.html#term-palm"
  },
  {
    "id": "term-pandemonium-model",
    "title": "Pandemonium Model",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1959",
      "anticipating",
      "architectures",
      "compete",
      "deep",
      "demons",
      "featuredetecting",
      "hierarchical",
      "history",
      "ideas",
      "identify",
      "key"
    ],
    "excerpt": "A pattern recognition model proposed by Oliver Selfridge in 1959 using hierarchical layers of feature-detecting demons that compete to identify patterns, anticipating key ideas in modern deep learning architectures.",
    "url": "pages/glossary.html#term-pandemonium-model"
  },
  {
    "id": "term-panoptic-segmentation",
    "title": "Panoptic Segmentation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "assigns",
      "cars",
      "class",
      "combining",
      "computer",
      "image",
      "instance",
      "label",
      "panoptic",
      "people",
      "pixel",
      "processing"
    ],
    "excerpt": "A unified image segmentation task that assigns both a class label and an instance ID to every pixel, combining semantic segmentation of stuff (sky, road) with instance segmentation of things (cars, people).",
    "url": "pages/glossary.html#term-panoptic-segmentation"
  },
  {
    "id": "term-paperclip-maximizer",
    "title": "Paperclip Maximizer",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "alignment",
      "available",
      "bostrom",
      "converts",
      "dangers",
      "experiment",
      "harmful",
      "illustrating",
      "including",
      "matter",
      "maximizer"
    ],
    "excerpt": "A thought experiment by Nick Bostrom illustrating the dangers of misaligned AI, in which an AI with the sole objective of maximizing paperclip production converts all available matter into paperclips, including harmful outcomes.",
    "url": "pages/glossary.html#term-paperclip-maximizer"
  },
  {
    "id": "term-parameters",
    "title": "Parameters",
    "category": "Glossary",
    "subcategory": "Core Concept",
    "keywords": [
      "concept",
      "constraints",
      "core",
      "dual",
      "meaning",
      "output",
      "parameters",
      "prompting",
      "shape",
      "specifications"
    ],
    "excerpt": "In prompting: constraints and specifications that shape AI output. In models: the learned weights (billions in LLMs) that determine behavior. Parameter count indicates model scale.",
    "url": "pages/glossary.html#term-parameters"
  },
  {
    "id": "term-paraphrase-detection",
    "title": "Paraphrase Detection",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "beyond",
      "convey",
      "detection",
      "determining",
      "different",
      "equivalence",
      "form",
      "meaning",
      "nlp",
      "paraphrase",
      "passages",
      "processing"
    ],
    "excerpt": "The task of determining whether two text passages convey the same meaning using different words or structures, requiring understanding of semantic equivalence beyond surface form.",
    "url": "pages/glossary.html#term-paraphrase-detection"
  },
  {
    "id": "term-parent-document-retrieval",
    "title": "Parent Document Retrieval",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "accurate",
      "ai",
      "child",
      "chunks",
      "context",
      "document",
      "documents",
      "generation",
      "generative",
      "indexes",
      "larger",
      "llm"
    ],
    "excerpt": "A RAG strategy that indexes small child chunks for precise matching but returns their larger parent documents to the LLM, providing sufficient surrounding context for accurate generation.",
    "url": "pages/glossary.html#term-parent-document-retrieval"
  },
  {
    "id": "term-parent-child-chunking",
    "title": "Parent-Child Chunking",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "child",
      "chunk",
      "chunking",
      "chunks",
      "context",
      "creates",
      "embeddingbased",
      "extended",
      "hierarchical",
      "larger",
      "linking",
      "matched"
    ],
    "excerpt": "A hierarchical chunking strategy that creates small child chunks for precise embedding-based retrieval while linking them to larger parent chunks that provide extended context, returning the parent context when a child chunk is matched.",
    "url": "pages/glossary.html#term-parent-child-chunking"
  },
  {
    "id": "term-parse-tree",
    "title": "Parse Tree",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "according",
      "categories",
      "formal",
      "grammar",
      "hierarchical",
      "internal",
      "leaves",
      "nlp",
      "nodes",
      "parse",
      "parsing",
      "phrase"
    ],
    "excerpt": "A hierarchical tree structure representing the syntactic structure of a sentence according to a formal grammar, with internal nodes as phrase categories and leaves as words.",
    "url": "pages/glossary.html#term-parse-tree"
  },
  {
    "id": "term-pos-tagging",
    "title": "Part-of-Speech Tagging",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "adjective",
      "adverb",
      "assigning",
      "based",
      "categories",
      "context",
      "form",
      "grammatical",
      "linguistics",
      "morphological",
      "nlp",
      "noun"
    ],
    "excerpt": "The task of assigning grammatical categories such as noun, verb, adjective, or adverb to each word in a sentence based on its context and morphological form.",
    "url": "pages/glossary.html#term-pos-tagging"
  },
  {
    "id": "term-partial-autocorrelation",
    "title": "Partial Autocorrelation",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "autocorrelation",
      "correlation",
      "data",
      "effects",
      "intermediate",
      "lagged",
      "lags",
      "observation",
      "partial",
      "removing",
      "science",
      "series"
    ],
    "excerpt": "The correlation between a time series observation and a lagged observation after removing the effects of intermediate lags.",
    "url": "pages/glossary.html#term-partial-autocorrelation"
  },
  {
    "id": "term-partial-dependence-plot",
    "title": "Partial Dependence Plot",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "averaging",
      "data",
      "dependence",
      "effect",
      "features",
      "learning",
      "machine",
      "marginal",
      "model",
      "one",
      "outcome",
      "partial"
    ],
    "excerpt": "A visualization showing the marginal effect of one or two features on the predicted outcome of a model, averaging over the values of all other features.",
    "url": "pages/glossary.html#term-partial-dependence-plot"
  },
  {
    "id": "term-partial-least-squares",
    "title": "Partial Least Squares",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "components",
      "covariance",
      "dimensionality",
      "finding",
      "latent",
      "learning",
      "least",
      "machine",
      "maximize",
      "method",
      "model",
      "observations"
    ],
    "excerpt": "A regression method that simultaneously reduces the dimensionality of predictors and response variables by finding latent components that maximize the covariance between them, useful when predictors outnumber observations.",
    "url": "pages/glossary.html#term-partial-least-squares"
  },
  {
    "id": "term-partially-observable-mdp",
    "title": "Partially Observable MDP (POMDP)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "cannot",
      "concepts",
      "core",
      "directly",
      "extension",
      "framework",
      "full",
      "instead",
      "learning",
      "mdp",
      "observable"
    ],
    "excerpt": "An extension of the MDP framework where the agent cannot directly observe the full state and instead receives partial observations.",
    "url": "pages/glossary.html#term-partially-observable-mdp"
  },
  {
    "id": "term-participatory-ai-design",
    "title": "Participatory AI Design",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "affected",
      "ai",
      "approach",
      "communities",
      "constraints",
      "design",
      "development",
      "diverse",
      "ensuring",
      "ethics",
      "evaluation",
      "goals"
    ],
    "excerpt": "An approach to AI development that involves affected communities and stakeholders in the design, development, and evaluation process, ensuring that diverse perspectives shape the system's goals and constraints.",
    "url": "pages/glossary.html#term-participatory-ai-design"
  },
  {
    "id": "term-partnership-on-ai",
    "title": "Partnership on AI",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "2016",
      "advancing",
      "ai",
      "ais",
      "best",
      "companies",
      "ethics",
      "formulate",
      "founded",
      "governance",
      "impact",
      "major"
    ],
    "excerpt": "A multi-stakeholder organization founded in 2016 by major technology companies to study and formulate best practices on AI technologies, advancing understanding of AI's impact on people and society.",
    "url": "pages/glossary.html#term-partnership-on-ai"
  },
  {
    "id": "term-pass-at-k",
    "title": "Pass@k",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "accounts",
      "benchmarks",
      "cases",
      "code",
      "computed",
      "estimator",
      "evaluation",
      "generated",
      "generation",
      "least",
      "measures",
      "metric"
    ],
    "excerpt": "A code generation evaluation metric that measures the probability that at least one of k generated code samples passes all test cases, computed using an unbiased estimator that accounts for the total number of samples generated.",
    "url": "pages/glossary.html#term-pass-at-k"
  },
  {
    "id": "term-passage-retrieval",
    "title": "Passage Retrieval",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "context",
      "corpus",
      "downstream",
      "finer",
      "fulldocument",
      "granularity",
      "identifying",
      "large",
      "operating",
      "passage",
      "passages",
      "precise"
    ],
    "excerpt": "The task of identifying and retrieving the most relevant text passages from a large corpus in response to a query, operating at a finer granularity than full-document retrieval to provide more precise context for downstream tasks.",
    "url": "pages/glossary.html#term-passage-retrieval"
  },
  {
    "id": "term-patrick-winston",
    "title": "Patrick Winston",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19432019",
      "1972",
      "1997",
      "american",
      "authored",
      "computer",
      "contributions",
      "directed",
      "history",
      "influential",
      "knowledge",
      "lab"
    ],
    "excerpt": "American computer scientist (1943-2019) who directed the MIT AI Lab from 1972 to 1997 and authored the influential AI textbook, making significant contributions to learning theory and knowledge representation.",
    "url": "pages/glossary.html#term-patrick-winston"
  },
  {
    "id": "term-pca-for-embeddings",
    "title": "PCA for Embeddings",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "analysis",
      "application",
      "commonly",
      "component",
      "compress",
      "controllable",
      "database",
      "dimensionality",
      "directions",
      "embedding",
      "embeddings",
      "faster"
    ],
    "excerpt": "The application of Principal Component Analysis to reduce embedding dimensionality by projecting vectors onto the directions of maximum variance, commonly used to compress embeddings for faster search with controllable information loss.",
    "url": "pages/glossary.html#term-pca-for-embeddings"
  },
  {
    "id": "term-pcie",
    "title": "PCIe for AI",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "accelerators",
      "ai",
      "component",
      "computing",
      "connecting",
      "distributed",
      "express",
      "for",
      "gpus",
      "hardware",
      "highspeed",
      "host"
    ],
    "excerpt": "Peripheral Component Interconnect Express, the standard high-speed serial interface connecting GPUs and accelerators to the host system.",
    "url": "pages/glossary.html#term-pcie"
  },
  {
    "id": "term-peft",
    "title": "PEFT (Parameter-Efficient Fine-Tuning)",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "efficiency",
      "efficient",
      "fine",
      "finetune",
      "models",
      "only",
      "parameter",
      "parameters",
      "peft",
      "small",
      "subset",
      "techniques"
    ],
    "excerpt": "Techniques that fine-tune models by training only a small subset of parameters. Includes LoRA, prefix tuning, and adapters. Dramatically reduces compute and memory needs.",
    "url": "pages/glossary.html#term-peft"
  },
  {
    "id": "term-penn-treebank",
    "title": "Penn Treebank",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "annotated",
      "benchmark",
      "corpus",
      "english",
      "evaluating",
      "language",
      "large",
      "linguistics",
      "models",
      "nlp",
      "parse",
      "parsers"
    ],
    "excerpt": "A large annotated corpus of English text with part-of-speech tags and syntactic parse trees, widely used as a benchmark for training and evaluating NLP parsers and language models.",
    "url": "pages/glossary.html#term-penn-treebank"
  },
  {
    "id": "term-perceiver",
    "title": "Perceiver",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "arbitrary",
      "architecture",
      "array",
      "crossattention",
      "fixedsize",
      "followed",
      "generalpurpose",
      "handling",
      "highdimensional",
      "input",
      "inputs",
      "latent"
    ],
    "excerpt": "A general-purpose architecture that uses cross-attention to map arbitrary high-dimensional inputs to a fixed-size latent array, followed by self-attention in the latent space, handling any input modality.",
    "url": "pages/glossary.html#term-perceiver"
  },
  {
    "id": "term-perceptron",
    "title": "Perceptron",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1957",
      "classify",
      "frank",
      "history",
      "introduced",
      "learn",
      "linearly",
      "milestones",
      "model",
      "network",
      "neural",
      "patterns"
    ],
    "excerpt": "A single-layer neural network model introduced by Frank Rosenblatt in 1957 that could learn to classify linearly separable patterns.",
    "url": "pages/glossary.html#term-perceptron"
  },
  {
    "id": "term-perceptual-loss",
    "title": "Perceptual Loss",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "compares",
      "computer",
      "encouraging",
      "feature",
      "function",
      "generation",
      "image",
      "loss",
      "network",
      "outputs",
      "perceptual",
      "perceptually"
    ],
    "excerpt": "A loss function for image generation that compares feature representations from a pre-trained network rather than raw pixel values, encouraging outputs that are perceptually similar to targets.",
    "url": "pages/glossary.html#term-perceptual-loss"
  },
  {
    "id": "term-performer",
    "title": "Performer",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "achieving",
      "approximation",
      "architecture",
      "attention",
      "complexity",
      "computation",
      "favor",
      "featurebased",
      "linear",
      "mechanism",
      "networks",
      "neural"
    ],
    "excerpt": "A transformer variant that uses random feature-based approximation of softmax attention through the FAVOR+ mechanism, achieving linear time and space complexity for attention computation.",
    "url": "pages/glossary.html#term-performer"
  },
  {
    "id": "term-permutation-importance",
    "title": "Permutation Importance",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "breaking",
      "engineering",
      "error",
      "estimating",
      "feature",
      "features",
      "importance",
      "increase",
      "learning",
      "machine",
      "measuring",
      "method"
    ],
    "excerpt": "A model-agnostic method for estimating feature importance by measuring the increase in prediction error when a single feature's values are randomly shuffled, breaking its relationship with the target.",
    "url": "pages/glossary.html#term-permutation-importance"
  },
  {
    "id": "term-perplexity",
    "title": "Perplexity",
    "category": "Glossary",
    "subcategory": "Metric",
    "keywords": [
      "evaluation",
      "language",
      "measuring",
      "metric",
      "model",
      "perplexity",
      "surprised",
      "text"
    ],
    "excerpt": "A metric measuring how \"surprised\" a language model is by text. Lower perplexity indicates better prediction. Also the name of an AI search engine combining LLMs with web search.",
    "url": "pages/glossary.html#term-perplexity"
  },
  {
    "id": "term-perplexity-metric",
    "title": "Perplexity Metric",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "average",
      "defined",
      "evaluation",
      "exponentiated",
      "heldout",
      "intrinsic",
      "language",
      "loglikelihood",
      "measuring",
      "metric",
      "model",
      "models"
    ],
    "excerpt": "An intrinsic evaluation metric for language models defined as the exponentiated average negative log-likelihood per token, measuring how well the model predicts a held-out test set.",
    "url": "pages/glossary.html#term-perplexity-metric"
  },
  {
    "id": "term-persona",
    "title": "Persona",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "assigned",
      "character",
      "persona",
      "prompting",
      "role",
      "specific",
      "technique"
    ],
    "excerpt": "A specific character or role assigned to an AI through prompting. Personas can include expertise, communication style, and behavioral guidelines to shape responses.",
    "url": "pages/glossary.html#term-persona"
  },
  {
    "id": "term-persona-prompting",
    "title": "Persona Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "background",
      "behavioral",
      "character",
      "communication",
      "consistently",
      "conversation",
      "defines",
      "detailed",
      "embody",
      "engineering",
      "expertise",
      "including"
    ],
    "excerpt": "A technique that defines a detailed character profile including background, expertise, communication style, and behavioral traits for the model to embody, producing responses that consistently reflect the specified persona throughout a conversation.",
    "url": "pages/glossary.html#term-persona-prompting"
  },
  {
    "id": "term-phi-architecture",
    "title": "Phi Architecture",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "achieve",
      "architecture",
      "carefully",
      "compensate",
      "curated",
      "data",
      "demonstrating",
      "family",
      "highquality",
      "language",
      "microsoft",
      "model"
    ],
    "excerpt": "A family of small language models by Microsoft that achieve strong performance through carefully curated high-quality training data, demonstrating that data quality can compensate for model size.",
    "url": "pages/glossary.html#term-phi-architecture"
  },
  {
    "id": "term-phoneme",
    "title": "Phoneme",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "acoustic",
      "another",
      "distinguish",
      "language",
      "linguistic",
      "linguistics",
      "map",
      "nlp",
      "one",
      "phoneme",
      "recognition",
      "representations"
    ],
    "excerpt": "The smallest unit of sound in a language that can distinguish one word from another, used in speech recognition systems to map acoustic signals to linguistic representations.",
    "url": "pages/glossary.html#term-phoneme"
  },
  {
    "id": "term-phonetics-in-ai",
    "title": "Phonetics in AI",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "acoustic",
      "ai",
      "application",
      "in",
      "including",
      "knowledge",
      "linguistics",
      "modeling",
      "nlp",
      "phonetic",
      "phonetics",
      "processing"
    ],
    "excerpt": "The application of phonetic knowledge to AI systems for speech processing, including modeling the acoustic properties of speech sounds for recognition and synthesis tasks.",
    "url": "pages/glossary.html#term-phonetics-in-ai"
  },
  {
    "id": "term-photometric-augmentation",
    "title": "Photometric Augmentation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "adjustments",
      "augmentation",
      "brightness",
      "changing",
      "color",
      "computer",
      "contrast",
      "hue",
      "image",
      "improve",
      "including",
      "jittering"
    ],
    "excerpt": "Image augmentation techniques that modify pixel values without changing spatial layout, including brightness, contrast, saturation, hue adjustments, and color jittering to improve model robustness.",
    "url": "pages/glossary.html#term-photometric-augmentation"
  },
  {
    "id": "term-physical-symbol-system-hypothesis",
    "title": "Physical Symbol System Hypothesis",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1976",
      "action",
      "approaches",
      "foundation",
      "history",
      "hypothesis",
      "intelligent",
      "means",
      "milestones",
      "necessary",
      "newell",
      "physical"
    ],
    "excerpt": "The 1976 hypothesis by Newell and Simon that a physical symbol system has the necessary and sufficient means for intelligent action, providing the theoretical foundation for symbolic AI approaches.",
    "url": "pages/glossary.html#term-physical-symbol-system-hypothesis"
  },
  {
    "id": "term-pinecone",
    "title": "Pinecone",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "applications",
      "architectures",
      "builtin",
      "cloudnative",
      "database",
      "designed",
      "filtering",
      "fully",
      "horizontal",
      "learning",
      "machine",
      "managed"
    ],
    "excerpt": "A fully managed cloud-native vector database service designed for production machine learning applications, providing serverless and pod-based architectures with built-in filtering, real-time updates, and horizontal scaling for similarity search.",
    "url": "pages/glossary.html#term-pinecone"
  },
  {
    "id": "term-pipeline",
    "title": "Pipeline (ML)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "chained",
      "data",
      "ml",
      "mlops",
      "modeling",
      "pipeline",
      "processing",
      "sequence",
      "steps",
      "together"
    ],
    "excerpt": "A sequence of data processing and modeling steps chained together. Includes preprocessing, feature extraction, model inference, and post-processing. Ensures reproducible workflows.",
    "url": "pages/glossary.html#term-pipeline"
  },
  {
    "id": "term-pipeline-parallelism",
    "title": "Pipeline Parallelism",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "across",
      "architecture",
      "device",
      "devices",
      "different",
      "distributed",
      "fashion",
      "improve",
      "layers",
      "microbatches",
      "model",
      "networks"
    ],
    "excerpt": "A distributed training strategy that partitions model layers into stages across devices, processing different micro-batches simultaneously in a pipeline fashion to improve device utilization.",
    "url": "pages/glossary.html#term-pipeline-parallelism"
  },
  {
    "id": "term-pix2pix",
    "title": "Pix2Pix",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "aligned",
      "architecture",
      "conditional",
      "discriminator",
      "framework",
      "gan",
      "generator",
      "image",
      "imagetoimage",
      "learn",
      "mappings",
      "networks"
    ],
    "excerpt": "A conditional GAN framework for paired image-to-image translation that uses a U-Net generator and PatchGAN discriminator to learn mappings between aligned image pairs.",
    "url": "pages/glossary.html#term-pix2pix"
  },
  {
    "id": "term-plan-and-execute-agent",
    "title": "Plan-and-Execute Agent",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "agent",
      "agentic",
      "ai",
      "and",
      "architecture",
      "carrying",
      "creating",
      "decompositions",
      "execute",
      "execution",
      "executor",
      "generative"
    ],
    "excerpt": "An agentic architecture that separates high-level planning from step-by-step execution, with a planner LLM creating task decompositions and an executor LLM carrying out individual steps.",
    "url": "pages/glossary.html#term-plan-and-execute-agent"
  },
  {
    "id": "term-plan-and-solve-plus",
    "title": "Plan-and-Solve Plus",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "adds",
      "and",
      "attention",
      "calculate",
      "calculation",
      "commonsense",
      "detailed",
      "engineering",
      "enhanced",
      "execution",
      "extract",
      "instructions"
    ],
    "excerpt": "An enhanced version of plan-and-solve prompting that adds detailed instructions to extract relevant variables, calculate intermediate results, and pay attention to calculation and commonsense reasoning during plan execution.",
    "url": "pages/glossary.html#term-plan-and-solve-plus"
  },
  {
    "id": "term-planning-rl",
    "title": "Planning in RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "compute",
      "environment",
      "improve",
      "in",
      "interaction",
      "learning",
      "model",
      "planning",
      "policy",
      "process",
      "reinforcement",
      "rl"
    ],
    "excerpt": "The process of using a model of the environment to compute or improve a policy before or during interaction.",
    "url": "pages/glossary.html#term-planning-rl"
  },
  {
    "id": "term-platt-scaling",
    "title": "Platt Scaling",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "calibration",
      "classifier",
      "fits",
      "heldout",
      "learning",
      "logistic",
      "machine",
      "method",
      "model",
      "output",
      "platt",
      "posthoc"
    ],
    "excerpt": "A post-hoc calibration method that fits a logistic regression model to the raw output scores of a classifier using a held-out validation set, transforming the scores into well-calibrated probabilities.",
    "url": "pages/glossary.html#term-platt-scaling"
  },
  {
    "id": "term-playground",
    "title": "Playground (AI)",
    "category": "Glossary",
    "subcategory": "Tools",
    "keywords": [
      "ai",
      "coding",
      "experimenting",
      "interactive",
      "interface",
      "models",
      "playground",
      "tools",
      "without"
    ],
    "excerpt": "An interactive interface for experimenting with AI models without coding. Most AI providers offer playgrounds to test prompts, adjust parameters, and explore capabilities.",
    "url": "pages/glossary.html#term-playground"
  },
  {
    "id": "term-point-cloud",
    "title": "Point Cloud",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "acquired",
      "cloud",
      "computer",
      "consisting",
      "data",
      "depth",
      "detection",
      "lidar",
      "object",
      "point",
      "points"
    ],
    "excerpt": "A 3D data representation consisting of a set of points in three-dimensional space, typically acquired by LiDAR or depth sensors, used for 3D object detection, segmentation, and scene reconstruction.",
    "url": "pages/glossary.html#term-point-cloud"
  },
  {
    "id": "term-pointnet",
    "title": "PointNet",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "architecture",
      "classification",
      "cloud",
      "computer",
      "data",
      "deep",
      "directly",
      "functions",
      "learning",
      "mlps",
      "perform"
    ],
    "excerpt": "A pioneering deep learning architecture that directly processes unordered 3D point cloud data using shared MLPs and symmetric pooling functions to perform classification and segmentation.",
    "url": "pages/glossary.html#term-pointnet"
  },
  {
    "id": "term-pointnet-plus-plus",
    "title": "PointNet++",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "applying",
      "capturing",
      "computer",
      "extension",
      "feature",
      "geometric",
      "hierarchical",
      "introduces",
      "learning",
      "local",
      "multiple"
    ],
    "excerpt": "An extension of PointNet that introduces hierarchical feature learning by applying PointNet recursively on nested partitions of the point set, capturing local geometric structures at multiple scales.",
    "url": "pages/glossary.html#term-pointnet-plus-plus"
  },
  {
    "id": "term-pointwise-convolution",
    "title": "Pointwise Convolution",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "1x1",
      "across",
      "architecture",
      "change",
      "channels",
      "combines",
      "commonly",
      "considering",
      "context",
      "convolution",
      "features",
      "linearly"
    ],
    "excerpt": "A 1x1 convolution that linearly combines features across channels at each spatial position without considering spatial context, commonly used to change the number of channels.",
    "url": "pages/glossary.html#term-pointwise-convolution"
  },
  {
    "id": "term-pointwise-mutual-information",
    "title": "Pointwise Mutual Information",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "association",
      "compares",
      "cooccurrence",
      "data",
      "events",
      "expected",
      "independence",
      "information",
      "measure",
      "mutual",
      "pointwise",
      "probability"
    ],
    "excerpt": "A measure of association between two events that compares the probability of their co-occurrence to the probability expected under independence.",
    "url": "pages/glossary.html#term-pointwise-mutual-information"
  },
  {
    "id": "term-pmi",
    "title": "Pointwise Mutual Information",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "association",
      "build",
      "collocations",
      "compares",
      "cooccurrence",
      "events",
      "expected",
      "identify",
      "independence",
      "information",
      "joint",
      "measure"
    ],
    "excerpt": "A statistical measure of association between two events that compares their joint probability with their expected co-occurrence under independence, used to identify collocations and build word representations.",
    "url": "pages/glossary.html#term-pmi"
  },
  {
    "id": "term-poisson-distribution",
    "title": "Poisson Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "average",
      "discrete",
      "distribution",
      "events",
      "expressing",
      "fixed",
      "given",
      "independent",
      "interval",
      "known",
      "number",
      "occurrences"
    ],
    "excerpt": "A discrete probability distribution expressing the probability of a given number of events occurring in a fixed interval, given a known average rate and independent occurrences.",
    "url": "pages/glossary.html#term-poisson-distribution"
  },
  {
    "id": "term-poisson-regression",
    "title": "Poisson Regression",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "assumes",
      "count",
      "data",
      "distribution",
      "follows",
      "function",
      "generalized",
      "linear",
      "link",
      "log",
      "model",
      "poisson"
    ],
    "excerpt": "A generalized linear model for count data that assumes the response follows a Poisson distribution and uses a log link function.",
    "url": "pages/glossary.html#term-poisson-regression"
  },
  {
    "id": "term-policy",
    "title": "Policy",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "agents",
      "behavior",
      "defines",
      "distributions",
      "learning",
      "mapping",
      "optimization",
      "policy",
      "probability",
      "reinforcement",
      "states"
    ],
    "excerpt": "A mapping from states to actions (or probability distributions over actions) that defines the agent's behavior. Policies can be deterministic or stochastic and are the central object optimized in RL.",
    "url": "pages/glossary.html#term-policy"
  },
  {
    "id": "term-policy-distillation",
    "title": "Policy Distillation",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "behavior",
      "distillation",
      "learning",
      "one",
      "paradigms",
      "policies",
      "policy",
      "reinforcement",
      "replicate",
      "student",
      "teacher",
      "technique"
    ],
    "excerpt": "A transfer learning technique that trains a student policy to replicate the behavior of one or more teacher policies.",
    "url": "pages/glossary.html#term-policy-distillation"
  },
  {
    "id": "term-policy-entropy",
    "title": "Policy Entropy",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agents",
      "convergence",
      "encourage",
      "entropy",
      "exploration",
      "learning",
      "measure",
      "optimization",
      "policy",
      "premature",
      "prevent",
      "randomness"
    ],
    "excerpt": "A measure of randomness in the agent's policy, used as a regularizer in RL to encourage exploration and prevent premature convergence.",
    "url": "pages/glossary.html#term-policy-entropy"
  },
  {
    "id": "term-policy-gradient",
    "title": "Policy Gradient",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithms",
      "class",
      "computing",
      "directly",
      "expected",
      "gradient",
      "gradients",
      "learning",
      "optimization",
      "optimize",
      "parameters",
      "policy"
    ],
    "excerpt": "A class of RL algorithms that directly optimize the policy by computing gradients of expected return with respect to policy parameters.",
    "url": "pages/glossary.html#term-policy-gradient"
  },
  {
    "id": "term-polynomial-kernel",
    "title": "Polynomial Kernel",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "boundaries",
      "computes",
      "decision",
      "degree",
      "enabling",
      "feature",
      "function",
      "given",
      "inner",
      "kernel",
      "learn",
      "learning"
    ],
    "excerpt": "A kernel function that computes the inner product of feature vectors raised to a specified power, enabling SVMs and other kernel methods to learn polynomial decision boundaries of a given degree.",
    "url": "pages/glossary.html#term-polynomial-kernel"
  },
  {
    "id": "term-polynomial-regression",
    "title": "Polynomial Regression",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "analysis",
      "capturing",
      "dependent",
      "form",
      "framework",
      "independent",
      "learning",
      "linear",
      "machine",
      "model",
      "modeled",
      "nonlinear"
    ],
    "excerpt": "A form of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial, capturing non-linear relationships within a linear model framework.",
    "url": "pages/glossary.html#term-polynomial-regression"
  },
  {
    "id": "term-polysemy",
    "title": "Polysemy",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "bank",
      "challenges",
      "disambiguation",
      "financial",
      "having",
      "institution",
      "linguistics",
      "meaning",
      "meanings",
      "multiple",
      "nlp",
      "polysemy"
    ],
    "excerpt": "The property of a word having multiple related meanings, such as 'bank' meaning a financial institution or a river bank, posing challenges for word sense disambiguation.",
    "url": "pages/glossary.html#term-polysemy"
  },
  {
    "id": "term-pooling-operation",
    "title": "Pooling Operation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "aggregating",
      "average",
      "computer",
      "dimensions",
      "downsampling",
      "feature",
      "functions",
      "image",
      "local",
      "maps",
      "maximum",
      "networks"
    ],
    "excerpt": "A downsampling operation in neural networks that reduces the spatial dimensions of feature maps by aggregating values within local regions, typically using maximum or average functions.",
    "url": "pages/glossary.html#term-pooling-operation"
  },
  {
    "id": "term-population-based-training",
    "title": "Population-Based Training (PBT)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agents",
      "based",
      "better",
      "copies",
      "hyperparameter",
      "learning",
      "method",
      "mutated",
      "ones",
      "optimization",
      "paradigms",
      "parallel"
    ],
    "excerpt": "A hyperparameter optimization method that trains a population of agents in parallel, periodically replacing poorly performing agents with mutated copies of better ones.",
    "url": "pages/glossary.html#term-population-based-training"
  },
  {
    "id": "term-pose-estimation",
    "title": "Pose Estimation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "activity",
      "analysis",
      "body",
      "capture",
      "computer",
      "detects",
      "estimation",
      "human",
      "image",
      "images",
      "joints",
      "keypoints"
    ],
    "excerpt": "A computer vision task that detects the positions of body joints or keypoints in images or video, producing a skeletal representation of human body posture used in activity analysis and motion capture.",
    "url": "pages/glossary.html#term-pose-estimation"
  },
  {
    "id": "term-positional-encoding",
    "title": "Positional Encoding",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "awareness",
      "encoding",
      "information",
      "inject",
      "order",
      "otherwise",
      "position",
      "positional",
      "process",
      "technique",
      "tokens"
    ],
    "excerpt": "A technique to inject position information into transformers, which otherwise process tokens without order awareness. Can be absolute, relative, or learned (RoPE).",
    "url": "pages/glossary.html#term-positional-encoding"
  },
  {
    "id": "term-post-norm-transformer",
    "title": "Post-Norm Transformer",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "applied",
      "architecture",
      "better",
      "careful",
      "configuration",
      "connection",
      "final",
      "layer",
      "learning",
      "networks",
      "neural",
      "norm"
    ],
    "excerpt": "The original transformer configuration where layer normalization is applied after the residual connection in each sublayer, requiring careful learning rate warmup but sometimes yielding better final performance.",
    "url": "pages/glossary.html#term-post-norm-transformer"
  },
  {
    "id": "term-post-training-quantization",
    "title": "Post-Training Quantization (PTQ)",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "alreadytrained",
      "applied",
      "calibration",
      "data",
      "determine",
      "factors",
      "further",
      "inference",
      "infrastructure",
      "model",
      "optimal",
      "optimization"
    ],
    "excerpt": "Quantization applied to an already-trained model without further training, using calibration data to determine optimal scaling factors.",
    "url": "pages/glossary.html#term-post-training-quantization"
  },
  {
    "id": "term-posterior-distribution",
    "title": "Posterior Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bayes",
      "bayesian",
      "data",
      "distribution",
      "methods",
      "observed",
      "parameter",
      "posterior",
      "prior",
      "probability",
      "statistics",
      "theorem"
    ],
    "excerpt": "The probability distribution of a parameter after updating the prior distribution with observed data via Bayes' theorem.",
    "url": "pages/glossary.html#term-posterior-distribution"
  },
  {
    "id": "term-posterior-predictive-distribution",
    "title": "Posterior Predictive Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bayesian",
      "data",
      "distribution",
      "future",
      "given",
      "incorporating",
      "integrating",
      "likelihood",
      "methods",
      "model",
      "naturally",
      "new"
    ],
    "excerpt": "The distribution of future observations given the observed data, obtained by integrating the likelihood of new data over the posterior distribution of model parameters, naturally incorporating parameter uncertainty.",
    "url": "pages/glossary.html#term-posterior-predictive-distribution"
  },
  {
    "id": "term-potential-based-reward-shaping",
    "title": "Potential-Based Reward Shaping",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "based",
      "current",
      "design",
      "difference",
      "discounted",
      "equals",
      "function",
      "learning",
      "method",
      "potential",
      "potentials",
      "reinforcement"
    ],
    "excerpt": "A reward shaping method using a potential function over states where the shaping reward equals the discounted difference in potentials between successor and current states.",
    "url": "pages/glossary.html#term-potential-based-reward-shaping"
  },
  {
    "id": "term-power-analysis",
    "title": "Power Analysis",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "analysis",
      "confidence",
      "detect",
      "determining",
      "effect",
      "fixed",
      "given",
      "inference",
      "level",
      "method",
      "minimum",
      "power"
    ],
    "excerpt": "A statistical method for determining the minimum sample size required to detect an effect of a specified size with a given level of confidence and power, or the power of a test given a fixed sample size.",
    "url": "pages/glossary.html#term-power-analysis"
  },
  {
    "id": "term-power-transform",
    "title": "Power Transform",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "applied",
      "assume",
      "boxcox",
      "data",
      "engineering",
      "family",
      "feature",
      "gaussianlike",
      "improving",
      "including",
      "make",
      "minimize"
    ],
    "excerpt": "A family of parametric transformations (including Box-Cox and Yeo-Johnson) applied to make data more Gaussian-like, stabilize variance, and minimize skewness, improving the performance of models that assume normality.",
    "url": "pages/glossary.html#term-power-transform"
  },
  {
    "id": "term-ppo",
    "title": "PPO (Proximal Policy Optimization)",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "algorithm",
      "commonly",
      "language",
      "learning",
      "models",
      "optimization",
      "policy",
      "ppo",
      "proximal",
      "reinforcement",
      "rlhf",
      "train"
    ],
    "excerpt": "A reinforcement learning algorithm commonly used in RLHF to train language models. Balances exploration with stable learning, making it practical for large model training.",
    "url": "pages/glossary.html#term-ppo"
  },
  {
    "id": "term-pragmatics",
    "title": "Pragmatics",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "beyond",
      "branch",
      "context",
      "influence",
      "intention",
      "interpretation",
      "knowledge",
      "language",
      "linguistics",
      "literal",
      "meaning",
      "nlp"
    ],
    "excerpt": "The branch of linguistics studying how context, speaker intention, and shared knowledge influence the interpretation of language beyond its literal semantic meaning.",
    "url": "pages/glossary.html#term-pragmatics"
  },
  {
    "id": "term-pre-norm",
    "title": "Pre-Norm",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "applies",
      "architecture",
      "deep",
      "enabling",
      "generative",
      "improving",
      "layer",
      "llm",
      "models",
      "norm",
      "normalization"
    ],
    "excerpt": "A transformer architecture variant that applies layer normalization before rather than after each sub-layer, improving training stability and enabling the training of very deep models without warm-up.",
    "url": "pages/glossary.html#term-pre-norm"
  },
  {
    "id": "term-pre-norm-transformer",
    "title": "Pre-Norm Transformer",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "applied",
      "architecture",
      "attention",
      "enabling",
      "feedforward",
      "improving",
      "layer",
      "learning",
      "networks",
      "neural",
      "norm",
      "normalization"
    ],
    "excerpt": "A transformer variant where layer normalization is applied before the attention and feedforward sublayers rather than after, improving training stability and enabling the removal of learning rate warmup.",
    "url": "pages/glossary.html#term-pre-norm-transformer"
  },
  {
    "id": "term-pre-tokenization",
    "title": "Pre-Tokenization",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "applying",
      "based",
      "initial",
      "languagespecific",
      "nlp",
      "pre",
      "preliminary",
      "punctuation",
      "raw",
      "rules",
      "splitting",
      "subword"
    ],
    "excerpt": "The initial splitting of raw text into preliminary units before applying subword tokenization, typically based on whitespace, punctuation, or language-specific rules.",
    "url": "pages/glossary.html#term-pre-tokenization"
  },
  {
    "id": "term-pre-training",
    "title": "Pre-Training",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "data",
      "general",
      "initial",
      "language",
      "learn",
      "models",
      "phase",
      "pre",
      "text",
      "training",
      "understanding",
      "vast"
    ],
    "excerpt": "The initial training phase where models learn general language understanding from vast text data. Creates a foundation that can be fine-tuned for specific tasks.",
    "url": "pages/glossary.html#term-pre-training"
  },
  {
    "id": "term-precautionary-principle-in-ai",
    "title": "Precautionary Principle in AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "application",
      "arguing",
      "certainty",
      "delay",
      "development",
      "ethics",
      "governance",
      "harms",
      "in",
      "irreversible",
      "lack"
    ],
    "excerpt": "The application of the precautionary principle to AI development, arguing that when potential harms are severe or irreversible, lack of scientific certainty should not delay protective measures.",
    "url": "pages/glossary.html#term-precautionary-principle-in-ai"
  },
  {
    "id": "term-precision",
    "title": "Precision",
    "category": "Glossary",
    "subcategory": "Metrics",
    "keywords": [
      "correct",
      "metrics",
      "positive",
      "precision",
      "predictions",
      "proportion",
      "technical"
    ],
    "excerpt": "In metrics: the proportion of positive predictions that are correct. In computing: the numerical format for model weights (FP32, FP16, INT8), affecting model size and speed.",
    "url": "pages/glossary.html#term-precision"
  },
  {
    "id": "term-precision-at-k",
    "title": "Precision at K",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "actually",
      "among",
      "assessment",
      "at",
      "cutoffbased",
      "documents",
      "evaluation",
      "items",
      "many",
      "measures",
      "metric",
      "metrics"
    ],
    "excerpt": "A retrieval evaluation metric that measures the proportion of relevant documents among the top K retrieved results, providing a cutoff-based assessment of how many returned items are actually useful to the user.",
    "url": "pages/glossary.html#term-precision-at-k"
  },
  {
    "id": "term-precision-recall-curve",
    "title": "Precision-Recall Curve",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "class",
      "classification",
      "curve",
      "datasets",
      "evaluating",
      "imbalanced",
      "learning",
      "machine",
      "metrics",
      "models",
      "particularly",
      "plot"
    ],
    "excerpt": "A plot of precision versus recall at various classification thresholds, particularly useful for evaluating models on imbalanced datasets where the positive class is rare.",
    "url": "pages/glossary.html#term-precision-recall-curve"
  },
  {
    "id": "term-precision-recall-curve-cv",
    "title": "Precision-Recall Curve",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "area",
      "average",
      "computer",
      "confidence",
      "corresponding",
      "curve",
      "detection",
      "detector",
      "different",
      "object",
      "plot",
      "precision"
    ],
    "excerpt": "A plot showing the trade-off between precision and recall at different confidence thresholds for an object detector, with the area under the curve corresponding to Average Precision.",
    "url": "pages/glossary.html#term-precision-recall-curve-cv"
  },
  {
    "id": "term-predictive-parity",
    "title": "Predictive Parity",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "across",
      "ai",
      "among",
      "classifier",
      "equal",
      "ethics",
      "fairness",
      "groups",
      "individuals",
      "meaning",
      "metric",
      "parity"
    ],
    "excerpt": "A fairness metric requiring that the positive predictive value of a classifier is equal across all protected groups, meaning that among individuals predicted positive, the proportion of true positives is the same.",
    "url": "pages/glossary.html#term-predictive-parity"
  },
  {
    "id": "term-predictive-policing-ethics",
    "title": "Predictive Policing Ethics",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "activity",
      "ai",
      "biases",
      "civil",
      "concerns",
      "creating",
      "criminal",
      "discriminatory",
      "entrench",
      "ethical",
      "ethics",
      "fairness"
    ],
    "excerpt": "The ethical concerns surrounding AI systems used to forecast criminal activity, including risks of reinforcing racial biases, violating civil liberties, and creating feedback loops that entrench discriminatory patterns.",
    "url": "pages/glossary.html#term-predictive-policing-ethics"
  },
  {
    "id": "term-preemptible-vms",
    "title": "Preemptible VMs",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "clouds",
      "computing",
      "discounted",
      "distributed",
      "elsewhere",
      "google",
      "hours",
      "inference",
      "infrastructure",
      "instances",
      "last",
      "machine"
    ],
    "excerpt": "Google Cloud's discounted virtual machine instances that last up to 24 hours and can be terminated when resources are needed elsewhere.",
    "url": "pages/glossary.html#term-preemptible-vms"
  },
  {
    "id": "term-preference-learning",
    "title": "Preference Learning",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "comparisons",
      "data",
      "dpo",
      "explicit",
      "family",
      "generative",
      "human",
      "including",
      "ipo",
      "labels",
      "learning"
    ],
    "excerpt": "A family of techniques that train models using human preference data (rankings or comparisons between outputs) rather than explicit labels, including methods like RLHF, DPO, and IPO.",
    "url": "pages/glossary.html#term-preference-learning"
  },
  {
    "id": "term-prefill-phase",
    "title": "Prefill Phase",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "cache",
      "entire",
      "inference",
      "infrastructure",
      "initial",
      "input",
      "llm",
      "model",
      "optimization",
      "parallel",
      "phase",
      "populate"
    ],
    "excerpt": "The initial phase of LLM inference that processes the entire input prompt in parallel to populate the KV cache. The prefill phase is compute-bound and its duration scales with input sequence length.",
    "url": "pages/glossary.html#term-prefill-phase"
  },
  {
    "id": "term-prefill-decode-disaggregation",
    "title": "Prefill-Decode Disaggregation",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "architecture",
      "computebound",
      "decode",
      "different",
      "disaggregation",
      "hardware",
      "inference",
      "infrastructure",
      "memorybound",
      "model",
      "onto",
      "optimization"
    ],
    "excerpt": "An inference architecture that separates the compute-bound prefill and memory-bound decode phases onto different hardware optimized for each workload.",
    "url": "pages/glossary.html#term-prefill-decode-disaggregation"
  },
  {
    "id": "term-prefix-caching",
    "title": "Prefix Caching",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "across",
      "avoiding",
      "cache",
      "caching",
      "common",
      "computation",
      "computed",
      "inference",
      "instruction",
      "llm",
      "multiple",
      "optimization"
    ],
    "excerpt": "An inference optimization that reuses the computed KV cache of shared prompt prefixes across multiple requests, avoiding redundant computation for system prompts or common instruction templates.",
    "url": "pages/glossary.html#term-prefix-caching"
  },
  {
    "id": "term-prefix-language-model",
    "title": "Prefix Language Model",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "bidirectional",
      "capabilities",
      "causal",
      "combining",
      "generation",
      "input",
      "language",
      "model",
      "networks",
      "neural"
    ],
    "excerpt": "A language model architecture where a prefix portion of the input uses bidirectional attention while the remaining portion uses causal attention, combining understanding and generation capabilities.",
    "url": "pages/glossary.html#term-prefix-language-model"
  },
  {
    "id": "term-prefix-tuning",
    "title": "Prefix Tuning",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "efficiency",
      "finetuning",
      "inputs",
      "method",
      "parameterefficient",
      "prefix",
      "prepends",
      "trainable",
      "training",
      "tuning",
      "vectors"
    ],
    "excerpt": "A parameter-efficient fine-tuning method that prepends trainable vectors to inputs. Only these prefixes are updated during training, keeping the base model frozen.",
    "url": "pages/glossary.html#term-prefix-tuning"
  },
  {
    "id": "term-presence-penalty",
    "title": "Presence Penalty",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "appeared",
      "applies",
      "decoding",
      "encouraging",
      "fixed",
      "generative",
      "introduce",
      "least",
      "model",
      "new",
      "output"
    ],
    "excerpt": "A parameter that applies a fixed penalty to any token that has appeared at least once in the output, encouraging the model to introduce new topics and vocabulary.",
    "url": "pages/glossary.html#term-presence-penalty"
  },
  {
    "id": "term-principal-component-analysis",
    "title": "Principal Component Analysis",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "analysis",
      "axes",
      "component",
      "components",
      "data",
      "dimensionality",
      "learning",
      "linear",
      "machine",
      "maximize",
      "onto",
      "orthogonal"
    ],
    "excerpt": "An unsupervised linear dimensionality reduction technique that projects data onto orthogonal axes (principal components) that maximize variance.",
    "url": "pages/glossary.html#term-principal-component-analysis"
  },
  {
    "id": "term-principal-component-regression",
    "title": "Principal Component Regression",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "addressing",
      "component",
      "components",
      "dimensionality",
      "first",
      "highdimensionality",
      "learning",
      "machine",
      "model",
      "multicollinearity",
      "pca",
      "predictor"
    ],
    "excerpt": "A regression technique that first reduces the dimensionality of predictor variables using PCA and then regresses the response on the retained principal components, addressing multicollinearity and high-dimensionality.",
    "url": "pages/glossary.html#term-principal-component-regression"
  },
  {
    "id": "term-prior-distribution",
    "title": "Prior Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "bayesian",
      "beliefs",
      "data",
      "distribution",
      "methods",
      "observing",
      "parameter",
      "prior",
      "probability",
      "representing",
      "statistics"
    ],
    "excerpt": "In Bayesian statistics, the probability distribution representing beliefs about a parameter before observing data.",
    "url": "pages/glossary.html#term-prior-distribution"
  },
  {
    "id": "term-prioritized-experience-replay",
    "title": "Prioritized Experience Replay",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "allowing",
      "error",
      "experience",
      "experiences",
      "frequently",
      "informative",
      "learn",
      "learning",
      "magnitude",
      "methods",
      "prioritized"
    ],
    "excerpt": "An experience replay strategy that samples transitions with probability proportional to their TD error magnitude, allowing the agent to learn more frequently from surprising or informative experiences.",
    "url": "pages/glossary.html#term-prioritized-experience-replay"
  },
  {
    "id": "term-prioritized-level-replay",
    "title": "Prioritized Level Replay (PLR)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "design",
      "environment",
      "generated",
      "highest",
      "learning",
      "level",
      "levels",
      "method",
      "paradigms",
      "plr",
      "prioritized"
    ],
    "excerpt": "An unsupervised environment design method that tracks learning progress on procedurally generated levels and replays those where the agent has the highest regret.",
    "url": "pages/glossary.html#term-prioritized-level-replay"
  },
  {
    "id": "term-privacy-preserving",
    "title": "Privacy-Preserving AI",
    "category": "Glossary",
    "subcategory": "Privacy",
    "keywords": [
      "ai",
      "data",
      "ethics",
      "exposing",
      "preserving",
      "privacy",
      "sensitive",
      "techniques",
      "use",
      "without"
    ],
    "excerpt": "Techniques to use AI without exposing sensitive data. Includes federated learning, differential privacy, and secure multi-party computation. Critical for healthcare and finance.",
    "url": "pages/glossary.html#term-privacy-preserving"
  },
  {
    "id": "term-pcfg",
    "title": "Probabilistic Context-Free Grammar",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "augmented",
      "context",
      "contextfree",
      "enabling",
      "free",
      "grammar",
      "input",
      "nlp",
      "parse",
      "parsing",
      "probabilistic",
      "probabilities"
    ],
    "excerpt": "A context-free grammar augmented with probabilities for each production rule, enabling statistical parsing by selecting the most probable parse tree for an input sentence.",
    "url": "pages/glossary.html#term-pcfg"
  },
  {
    "id": "term-probit-model",
    "title": "Probit Model",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "binary",
      "class",
      "cumulative",
      "distribution",
      "function",
      "linear",
      "link",
      "model",
      "normal",
      "outcomes",
      "positive",
      "predictor"
    ],
    "excerpt": "A regression model for binary outcomes that uses the cumulative distribution function of the standard normal distribution as the link function, relating the linear predictor to the probability of the positive class.",
    "url": "pages/glossary.html#term-probit-model"
  },
  {
    "id": "term-procedural-environment-generation",
    "title": "Procedural Environment Generation",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithmic",
      "automatic",
      "creation",
      "diverse",
      "environment",
      "environments",
      "generation",
      "layouts",
      "learning",
      "level",
      "object",
      "paradigms"
    ],
    "excerpt": "The automatic creation of diverse training environments through algorithmic variation of level layouts, object positions, and task parameters.",
    "url": "pages/glossary.html#term-procedural-environment-generation"
  },
  {
    "id": "term-process-reward-model",
    "title": "Process Reward Model",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "answer",
      "enabling",
      "feedback",
      "final",
      "finegrained",
      "generative",
      "intermediate",
      "llm",
      "model",
      "models",
      "multistep"
    ],
    "excerpt": "A reward model that scores each intermediate reasoning step rather than only the final answer, enabling more fine-grained feedback for training models to perform multi-step reasoning.",
    "url": "pages/glossary.html#term-process-reward-model"
  },
  {
    "id": "term-product-quantization",
    "title": "Product Quantization",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "approximate",
      "codebook",
      "compression",
      "computation",
      "database",
      "distance",
      "dramatic",
      "enabling",
      "fast",
      "highdimensional",
      "independently",
      "learned"
    ],
    "excerpt": "A vector compression technique that splits high-dimensional vectors into sub-vectors and quantizes each independently using a learned codebook, enabling dramatic memory reduction while supporting f...",
    "url": "pages/glossary.html#term-product-quantization"
  },
  {
    "id": "term-program-aided-language-model",
    "title": "Program-Aided Language Model",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accurate",
      "aided",
      "code",
      "codeaugmented",
      "computation",
      "engineering",
      "executable",
      "framework",
      "generate",
      "intermediate",
      "interpreter",
      "language"
    ],
    "excerpt": "A framework (PAL) that prompts a language model to generate executable program code as intermediate reasoning steps rather than natural language, offloading computation to a code interpreter for more accurate numerical and logical results.",
    "url": "pages/glossary.html#term-program-aided-language-model"
  },
  {
    "id": "term-prolog",
    "title": "Prolog",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1972",
      "1980s",
      "alain",
      "colmerauer",
      "created",
      "expert",
      "history",
      "knowledge",
      "kowalski",
      "language",
      "logic",
      "milestones"
    ],
    "excerpt": "A logic programming language created by Alain Colmerauer and Robert Kowalski in 1972, widely used in AI research for natural language processing, expert systems, and knowledge representation throughout the 1980s.",
    "url": "pages/glossary.html#term-prolog"
  },
  {
    "id": "term-prompt",
    "title": "Prompt",
    "category": "Glossary",
    "subcategory": "Core Concept",
    "keywords": [
      "assistant",
      "concept",
      "core",
      "fundamentals",
      "input",
      "prompt",
      "send",
      "text",
      "you"
    ],
    "excerpt": "The text input you send to an AI assistant. Can include context, instructions, examples, and constraints. Prompt quality directly influences response quality.",
    "url": "pages/glossary.html#term-prompt"
  },
  {
    "id": "term-prompt-caching",
    "title": "Prompt Caching",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "caching",
      "common",
      "computation",
      "computed",
      "inference",
      "keyvalue",
      "llm",
      "optimization",
      "prefixes",
      "prompt",
      "queries",
      "reducing"
    ],
    "excerpt": "An optimization technique that stores and reuses the computed key-value representations of common prompt prefixes, reducing redundant computation for repeated or similar queries.",
    "url": "pages/glossary.html#term-prompt-caching"
  },
  {
    "id": "term-prompt-chaining",
    "title": "Prompt Chaining",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "advanced",
      "breaking",
      "builds",
      "chaining",
      "complex",
      "multiple",
      "output",
      "previous",
      "prompt",
      "prompts",
      "sequential",
      "tasks"
    ],
    "excerpt": "Breaking complex tasks into multiple sequential prompts, where each builds on the previous output. Enables sophisticated workflows and better results on multi-step problems.",
    "url": "pages/glossary.html#term-prompt-chaining"
  },
  {
    "id": "term-prompt-chaining-architecture",
    "title": "Prompt Chaining Architecture",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "architecture",
      "chaining",
      "connected",
      "design",
      "directed",
      "downstream",
      "engineering",
      "further",
      "graph",
      "handling",
      "multiple",
      "outputs"
    ],
    "excerpt": "A system design pattern where multiple prompts are connected in a pipeline or directed graph, with each prompt handling a specific subtask and passing structured outputs to downstream prompts for further processing.",
    "url": "pages/glossary.html#term-prompt-chaining-architecture"
  },
  {
    "id": "term-prompt-compression",
    "title": "Prompt Compression",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "compression",
      "content",
      "context",
      "essential",
      "fit",
      "generative",
      "information",
      "learned",
      "length",
      "like",
      "limits"
    ],
    "excerpt": "Techniques that reduce the token length of prompts without losing essential information, using methods like selective context, summarization, or learned compression to fit more content within context limits.",
    "url": "pages/glossary.html#term-prompt-compression"
  },
  {
    "id": "term-prompt-engineering",
    "title": "Prompt Engineering",
    "category": "Glossary",
    "subcategory": "Skill",
    "keywords": [
      "better",
      "crafting",
      "effective",
      "engineering",
      "get",
      "practice",
      "prompt",
      "prompts",
      "results",
      "skill",
      "systems"
    ],
    "excerpt": "The practice of crafting effective prompts to get better results from AI systems. Includes techniques, frameworks (CRISP, COSTAR), and iterative refinement.",
    "url": "pages/glossary.html#term-prompt-engineering"
  },
  {
    "id": "term-prompt-ensembling",
    "title": "Prompt Ensembling",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accurate",
      "aggregates",
      "alone",
      "averaging",
      "differentlyphrased",
      "engineering",
      "ensemble",
      "ensembling",
      "final",
      "multiple",
      "outputs",
      "produce"
    ],
    "excerpt": "A strategy that runs multiple differently-phrased prompts for the same query and aggregates the outputs through voting, averaging, or selection to produce more robust and accurate final responses than any single prompt alone.",
    "url": "pages/glossary.html#term-prompt-ensembling"
  },
  {
    "id": "term-prompt-extraction-attack",
    "title": "Prompt Extraction Attack",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "analysis",
      "attack",
      "attempts",
      "confidential",
      "context",
      "engineering",
      "extract",
      "extraction",
      "instructions",
      "model",
      "models",
      "probing"
    ],
    "excerpt": "A targeted attack technique that attempts to reconstruct or extract a model's system prompt, proprietary instructions, or confidential context through systematic probing queries and analysis of model responses.",
    "url": "pages/glossary.html#term-prompt-extraction-attack"
  },
  {
    "id": "term-prompt-injection",
    "title": "Prompt Injection",
    "category": "Glossary",
    "subcategory": "Security",
    "keywords": [
      "behave",
      "cause",
      "content",
      "hidden",
      "injection",
      "instructions",
      "malicious",
      "prompt",
      "risk",
      "security",
      "unexpectedly",
      "vulnerability"
    ],
    "excerpt": "A security vulnerability where malicious instructions hidden in content cause AI to behave unexpectedly. A significant concern for AI applications processing external data.",
    "url": "pages/glossary.html#term-prompt-injection"
  },
  {
    "id": "term-prompt-leaking",
    "title": "Prompt Leaking",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "attacker",
      "carefully",
      "confidential",
      "crafted",
      "engineering",
      "exploit",
      "helpful",
      "hidden",
      "instructions",
      "language",
      "leaking",
      "manipulates"
    ],
    "excerpt": "A security vulnerability where an attacker manipulates a language model into revealing its hidden system prompt or confidential instructions through carefully crafted queries that exploit the model's tendency to be helpful.",
    "url": "pages/glossary.html#term-prompt-leaking"
  },
  {
    "id": "term-prompt-optimization",
    "title": "Prompt Optimization",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "employing",
      "engineering",
      "evolutionary",
      "gradientbased",
      "iteration",
      "manual",
      "maximize",
      "methods",
      "model",
      "optimization",
      "parameters",
      "performance"
    ],
    "excerpt": "The systematic process of refining prompt text, structure, and parameters to maximize model performance on a target task, employing techniques ranging from manual iteration to gradient-based or evolutionary search methods.",
    "url": "pages/glossary.html#term-prompt-optimization"
  },
  {
    "id": "term-prompt-robustness",
    "title": "Prompt Robustness",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "ability",
      "across",
      "cases",
      "conditions",
      "consistent",
      "correct",
      "diverse",
      "edge",
      "engineering",
      "indicating",
      "input",
      "maintain"
    ],
    "excerpt": "The ability of a prompt to maintain consistent model performance across variations in input phrasing, perturbations, and edge cases, indicating how reliably the prompt produces correct outputs under diverse conditions.",
    "url": "pages/glossary.html#term-prompt-robustness"
  },
  {
    "id": "term-prompt-sensitivity",
    "title": "Prompt Sensitivity",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "achieving",
      "challenge",
      "changes",
      "correctness",
      "degree",
      "engineering",
      "example",
      "formatting",
      "key",
      "minor",
      "models",
      "ordering"
    ],
    "excerpt": "The degree to which a model's output quality and correctness varies in response to minor changes in prompt wording, formatting, or example ordering, representing a key challenge in achieving reliable and reproducible results.",
    "url": "pages/glossary.html#term-prompt-sensitivity"
  },
  {
    "id": "term-prompt-template",
    "title": "Prompt Template",
    "category": "Glossary",
    "subcategory": "Pattern",
    "keywords": [
      "content",
      "pattern",
      "placeholders",
      "prompt",
      "reusable",
      "structure",
      "template",
      "variable"
    ],
    "excerpt": "A reusable prompt structure with placeholders for variable content. Enables consistent, repeatable interactions and is essential for building AI-powered applications.",
    "url": "pages/glossary.html#term-prompt-template"
  },
  {
    "id": "term-prompt-templating",
    "title": "Prompt Templating",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "across",
      "cases",
      "consistent",
      "creating",
      "dynamically",
      "enabling",
      "engineering",
      "filled",
      "formatting",
      "infrastructure",
      "inputs",
      "multiple"
    ],
    "excerpt": "The practice of creating reusable prompt structures with placeholder variables that can be dynamically filled with specific inputs at runtime, enabling consistent prompt formatting across multiple queries and use cases.",
    "url": "pages/glossary.html#term-prompt-templating"
  },
  {
    "id": "term-prompt-tuning",
    "title": "Prompt Tuning",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "adaptation",
      "ai",
      "continuous",
      "embeddings",
      "enabling",
      "frozen",
      "generative",
      "input",
      "keeping",
      "learnable",
      "llm",
      "method"
    ],
    "excerpt": "A parameter-efficient method that prepends learnable continuous embeddings (soft prompts) to the input while keeping all model parameters frozen, enabling task adaptation with minimal overhead.",
    "url": "pages/glossary.html#term-prompt-tuning"
  },
  {
    "id": "term-prompt-versioning",
    "title": "Prompt Versioning",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "artifacts",
      "baselines",
      "capabilities",
      "change",
      "critical",
      "engineering",
      "infrastructure",
      "lifecycle",
      "maintaining",
      "management",
      "performance",
      "practice"
    ],
    "excerpt": "The practice of maintaining version-controlled prompt templates with change tracking, performance baselines, and rollback capabilities, treating prompts as critical software artifacts that require systematic lifecycle management.",
    "url": "pages/glossary.html#term-prompt-versioning"
  },
  {
    "id": "term-pronoun-resolution",
    "title": "Pronoun Resolution",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "context",
      "determining",
      "entity",
      "gender",
      "linguistics",
      "nlp",
      "number",
      "plausibility",
      "position",
      "pronoun",
      "refers",
      "requiring"
    ],
    "excerpt": "The specific task of determining which entity a pronoun refers to in context, requiring understanding of gender, number, syntactic position, and semantic plausibility.",
    "url": "pages/glossary.html#term-pronoun-resolution"
  },
  {
    "id": "term-propbank",
    "title": "PropBank",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "annotated",
      "bank",
      "corpus",
      "evaluation",
      "facilitate",
      "labeling",
      "labels",
      "linguistics",
      "nlp",
      "predicateargument",
      "propbank",
      "proposition"
    ],
    "excerpt": "Proposition Bank, a corpus annotated with predicate-argument structures for verbs, providing semantic role labels that facilitate training and evaluation of semantic role labeling systems.",
    "url": "pages/glossary.html#term-propbank"
  },
  {
    "id": "term-propensity-score",
    "title": "Propensity Score",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "assigned",
      "covariates",
      "given",
      "inference",
      "observed",
      "particular",
      "probability",
      "propensity",
      "score",
      "statistics",
      "treatment",
      "unit"
    ],
    "excerpt": "The probability that a unit is assigned to a particular treatment given its observed covariates. It is used in causal inference to balance treatment and control groups by matching, stratification, or inverse weighting.",
    "url": "pages/glossary.html#term-propensity-score"
  },
  {
    "id": "term-protected-attributes",
    "title": "Protected Attributes",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "age",
      "attributes",
      "bases",
      "characteristics",
      "designated",
      "differential",
      "disability",
      "ethically",
      "fairness",
      "gender",
      "legally",
      "prohibited"
    ],
    "excerpt": "Characteristics such as race, gender, age, religion, and disability status that are legally or ethically designated as bases upon which differential treatment by AI systems is prohibited or restricted.",
    "url": "pages/glossary.html#term-protected-attributes"
  },
  {
    "id": "term-proxy-discrimination",
    "title": "Proxy Discrimination",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "achieving",
      "ai",
      "attributes",
      "correlated",
      "discrimination",
      "discriminatory",
      "ethics",
      "even",
      "excluded",
      "explicitly",
      "fairness",
      "features"
    ],
    "excerpt": "Discrimination that occurs when an AI system uses features that are correlated with protected attributes as proxies, achieving discriminatory outcomes even when protected attributes are explicitly excluded.",
    "url": "pages/glossary.html#term-proxy-discrimination"
  },
  {
    "id": "term-pruning",
    "title": "Pruning",
    "category": "Glossary",
    "subcategory": "Optimization",
    "keywords": [
      "efficiency",
      "increase",
      "networks",
      "neural",
      "neurons",
      "optimization",
      "pruning",
      "reduce",
      "removing",
      "size",
      "speed",
      "unnecessary"
    ],
    "excerpt": "Removing unnecessary weights or neurons from neural networks to reduce size and increase speed. Can dramatically decrease model size with minimal performance loss.",
    "url": "pages/glossary.html#term-pruning"
  },
  {
    "id": "term-pruning-at-initialization",
    "title": "Pruning-at-Initialization",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "analysis",
      "at",
      "based",
      "flow",
      "gradient",
      "identify",
      "inference",
      "infrastructure",
      "initialization",
      "model",
      "occurs",
      "optimization"
    ],
    "excerpt": "Techniques that identify and remove redundant weights before any training occurs, based on signal propagation or gradient flow analysis.",
    "url": "pages/glossary.html#term-pruning-at-initialization"
  },
  {
    "id": "term-pytorch",
    "title": "PyTorch",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "deep",
      "design",
      "flexibility",
      "framework",
      "known",
      "learning",
      "meta",
      "opensource",
      "popular",
      "pythonic",
      "pytorch"
    ],
    "excerpt": "A popular open-source deep learning framework from Meta, known for its flexibility and Pythonic design. The dominant framework for AI research and increasingly for production.",
    "url": "pages/glossary.html#term-pytorch"
  },
  {
    "id": "term-q-function",
    "title": "Q-Function",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "actionvalue",
      "cumulative",
      "estimates",
      "expected",
      "following",
      "function",
      "given",
      "learning",
      "methods",
      "policy",
      "reinforcement"
    ],
    "excerpt": "The action-value function Q(s,a) that estimates the expected cumulative reward of taking action a in state s and then following a given policy.",
    "url": "pages/glossary.html#term-q-function"
  },
  {
    "id": "term-q-learning",
    "title": "Q-Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actionvalue",
      "algorithm",
      "bellman",
      "difference",
      "equation",
      "function",
      "iteratively",
      "learning",
      "learns",
      "methods",
      "offpolicy",
      "optimal"
    ],
    "excerpt": "An off-policy temporal difference algorithm that learns the optimal action-value function Q* by iteratively updating Q-values using the Bellman optimality equation.",
    "url": "pages/glossary.html#term-q-learning"
  },
  {
    "id": "term-qdrant",
    "title": "Qdrant",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "advanced",
      "capabilities",
      "database",
      "deployment",
      "designed",
      "distributed",
      "engine",
      "filtering",
      "highperformance",
      "indexing",
      "open",
      "opensource"
    ],
    "excerpt": "An open-source vector similarity search engine written in Rust that provides filtering, payload storage, and distributed deployment capabilities, designed for high-performance production workloads with advanced quantization and indexing options.",
    "url": "pages/glossary.html#term-qdrant"
  },
  {
    "id": "term-qlora",
    "title": "QLoRA (Quantized LoRA)",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "combining",
      "efficiency",
      "finetuning",
      "lora",
      "qlora",
      "quantization",
      "quantized",
      "technique",
      "training"
    ],
    "excerpt": "A technique combining quantization with LoRA fine-tuning. Enables fine-tuning large models on consumer GPUs by using 4-bit quantized base models.",
    "url": "pages/glossary.html#term-qlora"
  },
  {
    "id": "term-qmix",
    "title": "QMIX",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actionvalue",
      "algorithm",
      "combination",
      "factorizes",
      "function",
      "joint",
      "learning",
      "mixing",
      "monotonic",
      "multiagent",
      "network",
      "peragent"
    ],
    "excerpt": "A multi-agent RL algorithm that factorizes the joint action-value function as a monotonic combination of per-agent utilities through a mixing network.",
    "url": "pages/glossary.html#term-qmix"
  },
  {
    "id": "term-qualcomm-ai-engine",
    "title": "Qualcomm AI Engine",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "adreno",
      "ai",
      "compute",
      "coordinates",
      "cpu",
      "dsp",
      "engine",
      "gpu",
      "hardware",
      "heterogeneous",
      "hexagon",
      "inference"
    ],
    "excerpt": "Qualcomm's heterogeneous AI compute platform that coordinates the Hexagon DSP, Adreno GPU, and Kryo CPU within Snapdragon processors for optimal AI inference.",
    "url": "pages/glossary.html#term-qualcomm-ai-engine"
  },
  {
    "id": "term-quantile-regression",
    "title": "Quantile Regression",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "90th",
      "complete",
      "conditional",
      "distribution",
      "estimates",
      "mean",
      "median",
      "method",
      "model",
      "percentile",
      "picture",
      "providing"
    ],
    "excerpt": "A regression method that estimates conditional quantiles (such as the median or 90th percentile) of the response variable rather than the conditional mean, providing a more complete picture of the response distribution.",
    "url": "pages/glossary.html#term-quantile-regression"
  },
  {
    "id": "term-quantile-quantile-plot",
    "title": "Quantile-Quantile Plot",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "against",
      "comparing",
      "data",
      "distributions",
      "graphical",
      "plot",
      "plotting",
      "probability",
      "quantile",
      "quantiles",
      "science",
      "statistics"
    ],
    "excerpt": "A graphical tool for comparing two probability distributions by plotting their quantiles against each other.",
    "url": "pages/glossary.html#term-quantile-quantile-plot"
  },
  {
    "id": "term-quantization",
    "title": "Quantization",
    "category": "Glossary",
    "subcategory": "Optimization",
    "keywords": [
      "deployment",
      "model",
      "optimization",
      "precision",
      "quantization",
      "reducing",
      "weights"
    ],
    "excerpt": "Reducing the precision of model weights (e.g., from 16-bit to 4-bit) to decrease memory usage and increase speed. Enables running larger models on limited hardware.",
    "url": "pages/glossary.html#term-quantization"
  },
  {
    "id": "term-quantization-aware-training",
    "title": "Quantization-Aware Training (QAT)",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "aware",
      "backpropagation",
      "effects",
      "forward",
      "fullprecision",
      "gradients",
      "inference",
      "infrastructure",
      "maintaining",
      "model",
      "optimization",
      "pass"
    ],
    "excerpt": "A training technique that simulates the effects of quantization during the forward pass while maintaining full-precision gradients for backpropagation.",
    "url": "pages/glossary.html#term-quantization-aware-training"
  },
  {
    "id": "term-query",
    "title": "Query",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "concept",
      "dual",
      "meaning",
      "query",
      "question",
      "ragsearch",
      "search",
      "terms",
      "users"
    ],
    "excerpt": "In RAG/search: the user's question or search terms. In attention: one of three vectors (query, key, value) used to compute attention weights.",
    "url": "pages/glossary.html#term-query"
  },
  {
    "id": "term-query-decomposition",
    "title": "Query Decomposition",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "address",
      "breaks",
      "complex",
      "decomposition",
      "documents",
      "independently",
      "information",
      "merges",
      "multifaceted",
      "multiple",
      "perspectives",
      "processing"
    ],
    "excerpt": "A retrieval strategy that breaks a complex multi-faceted query into simpler sub-queries, retrieves results for each independently, and merges the results to address questions that require information from multiple documents or perspectives.",
    "url": "pages/glossary.html#term-query-decomposition"
  },
  {
    "id": "term-query-expansion",
    "title": "Query Expansion",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "additional",
      "augments",
      "bridging",
      "documents",
      "expansion",
      "gaps",
      "improve",
      "index",
      "original",
      "processing",
      "query",
      "recall"
    ],
    "excerpt": "A retrieval technique that augments the original query with additional related terms, synonyms, or reformulations to improve recall by bridging vocabulary gaps between the user's query and relevant documents in the index.",
    "url": "pages/glossary.html#term-query-expansion"
  },
  {
    "id": "term-question-answering",
    "title": "Question Answering (QA)",
    "category": "Glossary",
    "subcategory": "NLP Task",
    "keywords": [
      "answering",
      "answers",
      "application",
      "based",
      "context",
      "knowledge",
      "model",
      "nlp",
      "provided",
      "qa",
      "question",
      "questions"
    ],
    "excerpt": "An NLP task where the model answers questions based on provided context or its knowledge. Includes extractive QA (finding answers in text) and generative QA (generating answers).",
    "url": "pages/glossary.html#term-question-answering"
  },
  {
    "id": "term-r-squared",
    "title": "R-Squared",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "dependent",
      "explained",
      "independent",
      "indicating",
      "measure",
      "metrics",
      "model",
      "proportion",
      "regression",
      "squared",
      "statistical",
      "statistics"
    ],
    "excerpt": "A statistical measure indicating the proportion of variance in the dependent variable that is explained by the independent variables in a regression model.",
    "url": "pages/glossary.html#term-r-squared"
  },
  {
    "id": "term-r1-xcon",
    "title": "R1/XCON",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1980",
      "annually",
      "becoming",
      "carnegie",
      "commercially",
      "computer",
      "configuring",
      "corporation",
      "developed",
      "digital",
      "equipment",
      "expert"
    ],
    "excerpt": "An expert system developed by John McDermott at Carnegie Mellon in 1980 for configuring VAX computer orders at Digital Equipment Corporation, becoming one of the first commercially successful AI systems and saving millions annually.",
    "url": "pages/glossary.html#term-r1-xcon"
  },
  {
    "id": "term-race-to-the-bottom-ai-safety",
    "title": "Race to the Bottom in AI Safety",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "alignment",
      "among",
      "bottom",
      "capabilities",
      "competitive",
      "concern",
      "corners",
      "cut",
      "deploy",
      "developers",
      "faster"
    ],
    "excerpt": "The concern that competitive pressures among AI developers lead to progressively lower safety standards, as organizations cut corners on alignment research and testing to deploy capabilities faster than rivals.",
    "url": "pages/glossary.html#term-race-to-the-bottom-ai-safety"
  },
  {
    "id": "term-rademacher-complexity",
    "title": "Rademacher Complexity",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "class",
      "complexity",
      "fit",
      "functions",
      "hypothesis",
      "learning",
      "machine",
      "measure",
      "model",
      "noise",
      "quantifies",
      "rademacher"
    ],
    "excerpt": "A measure of the richness of a hypothesis class that quantifies how well functions in the class can fit random noise.",
    "url": "pages/glossary.html#term-rademacher-complexity"
  },
  {
    "id": "term-radial-basis-function-kernel",
    "title": "Radial Basis Function Kernel",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "basis",
      "decaying",
      "distance",
      "euclidean",
      "exponentially",
      "function",
      "kernel",
      "learning",
      "machine",
      "measures",
      "optimization",
      "points"
    ],
    "excerpt": "A popular kernel function that measures similarity as an exponentially decaying function of the squared Euclidean distance between points.",
    "url": "pages/glossary.html#term-radial-basis-function-kernel"
  },
  {
    "id": "term-raft-optical-flow",
    "title": "RAFT",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "allpairs",
      "architecture",
      "computer",
      "correlation",
      "deep",
      "estimation",
      "field",
      "flow",
      "grubased",
      "image",
      "iteratively",
      "learning"
    ],
    "excerpt": "Recurrent All-Pairs Field Transforms, a deep learning architecture for optical flow estimation that uses 4D correlation volumes and recurrent GRU-based updates to iteratively refine flow predictions.",
    "url": "pages/glossary.html#term-raft-optical-flow"
  },
  {
    "id": "term-rag",
    "title": "RAG (Retrieval-Augmented Generation)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "accuracy",
      "architecture",
      "augmented",
      "combines",
      "external",
      "generation",
      "information",
      "rag",
      "retrieval",
      "sources",
      "technique"
    ],
    "excerpt": "A technique that combines AI generation with information retrieval from external sources. The model retrieves relevant documents and uses them to generate accurate, grounded responses.",
    "url": "pages/glossary.html#term-rag"
  },
  {
    "id": "term-ragas",
    "title": "RAGAS",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "answer",
      "answers",
      "assessment",
      "augmented",
      "automated",
      "context",
      "enabling",
      "evaluation",
      "faithfulness",
      "framework",
      "generation",
      "groundtruth"
    ],
    "excerpt": "Retrieval Augmented Generation Assessment, an evaluation framework that provides reference-free metrics for RAG pipelines including faithfulness, answer relevancy, and context precision, enabling a...",
    "url": "pages/glossary.html#term-ragas"
  },
  {
    "id": "term-rainbow-dqn",
    "title": "Rainbow DQN",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "architecture",
      "combines",
      "distributional",
      "double",
      "dqn",
      "dueling",
      "extensions",
      "integrated",
      "learning",
      "methods",
      "multistep"
    ],
    "excerpt": "An integrated DQN agent that combines six extensions: double Q-learning, prioritized replay, dueling architecture, multi-step returns, distributional RL, and noisy networks.",
    "url": "pages/glossary.html#term-rainbow-dqn"
  },
  {
    "id": "term-randaugment",
    "title": "RandAugment",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "applies",
      "augmentation",
      "automated",
      "computer",
      "fixed",
      "hyperparameters",
      "image",
      "magnitude",
      "number",
      "only",
      "operations",
      "predefined"
    ],
    "excerpt": "A simplified automated augmentation strategy that randomly applies a fixed number of augmentation operations from a predefined set with a shared magnitude, requiring only two hyperparameters.",
    "url": "pages/glossary.html#term-randaugment"
  },
  {
    "id": "term-random-erasing",
    "title": "Random Erasing",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "acting",
      "augmentation",
      "computer",
      "data",
      "dropout",
      "erasing",
      "image",
      "images",
      "mean",
      "pixels",
      "processing",
      "random"
    ],
    "excerpt": "A data augmentation technique that randomly selects rectangular regions in training images and replaces their pixels with random values or mean values, acting as a regularizer similar to dropout.",
    "url": "pages/glossary.html#term-random-erasing"
  },
  {
    "id": "term-random-forest",
    "title": "Random Forest",
    "category": "Glossary",
    "subcategory": "Algorithm",
    "keywords": [
      "algorithm",
      "decision",
      "ensemble",
      "forest",
      "ml",
      "predictions",
      "random",
      "trees",
      "vote"
    ],
    "excerpt": "An ensemble of decision trees that vote on predictions. Robust, interpretable, and works well on tabular data. Still widely used despite the deep learning era.",
    "url": "pages/glossary.html#term-random-forest"
  },
  {
    "id": "term-random-projection",
    "title": "Random Projection",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "accelerate",
      "approximately",
      "based",
      "database",
      "dimensionality",
      "distances",
      "highdimensional",
      "johnsonlindenstrauss",
      "lemma",
      "lowerdimensional",
      "matrices",
      "onto"
    ],
    "excerpt": "A dimensionality reduction technique based on the Johnson-Lindenstrauss lemma that projects high-dimensional vectors onto a lower-dimensional space using random matrices while approximately preserv...",
    "url": "pages/glossary.html#term-random-projection"
  },
  {
    "id": "term-random-search",
    "title": "Random Search",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "combinations",
      "configurations",
      "distributions",
      "efficiently",
      "few",
      "finding",
      "good",
      "grid",
      "hyperparameter",
      "hyperparameters",
      "learning",
      "machine"
    ],
    "excerpt": "A hyperparameter tuning strategy that samples parameter combinations randomly from specified distributions, often finding good configurations more efficiently than grid search when only a few hyperparameters matter.",
    "url": "pages/glossary.html#term-random-search"
  },
  {
    "id": "term-rate-limit",
    "title": "Rate Limit",
    "category": "Glossary",
    "subcategory": "API",
    "keywords": [
      "api",
      "limit",
      "measured",
      "minute",
      "per",
      "rate",
      "requests",
      "restrictions",
      "technical",
      "tokens",
      "typically",
      "usage"
    ],
    "excerpt": "Restrictions on API usage, typically measured in requests per minute or tokens per minute. Prevents abuse and ensures fair resource distribution among users.",
    "url": "pages/glossary.html#term-rate-limit"
  },
  {
    "id": "term-ray-kurzweil",
    "title": "Ray Kurzweil",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2012",
      "accelerating",
      "american",
      "author",
      "concept",
      "futurist",
      "google",
      "history",
      "inventor",
      "joined",
      "kurzweil",
      "language"
    ],
    "excerpt": "American inventor, author, and futurist who popularized the concept of the technological singularity, predicted accelerating returns in technology, and joined Google in 2012 to work on natural language understanding.",
    "url": "pages/glossary.html#term-ray-kurzweil"
  },
  {
    "id": "term-rdma",
    "title": "RDMA",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "access",
      "computing",
      "cpu",
      "data",
      "different",
      "direct",
      "distributed",
      "enables",
      "gpu",
      "hardware",
      "involvement",
      "memory"
    ],
    "excerpt": "Remote Direct Memory Access, a networking technology that enables direct data transfer between GPU memory on different nodes without CPU involvement.",
    "url": "pages/glossary.html#term-rdma"
  },
  {
    "id": "term-re-identification",
    "title": "Re-Identification",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "across",
      "appearance",
      "camera",
      "changes",
      "computer",
      "different",
      "discriminative",
      "embeddings",
      "identification",
      "image",
      "learning",
      "lighting"
    ],
    "excerpt": "The task of matching the same person or vehicle across different camera views or time periods by learning discriminative appearance embeddings that are robust to viewpoint and lighting changes.",
    "url": "pages/glossary.html#term-re-identification"
  },
  {
    "id": "term-re-ranking",
    "title": "Re-Ranking",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "applies",
      "applying",
      "candidates",
      "computationally",
      "crossattention",
      "deeper",
      "document",
      "expensive",
      "improving",
      "initial",
      "model",
      "precision"
    ],
    "excerpt": "A second-stage retrieval process that applies a more computationally expensive model to re-score and reorder an initial set of retrieved candidates, improving precision by applying deeper cross-attention between query and document representations.",
    "url": "pages/glossary.html#term-re-ranking"
  },
  {
    "id": "term-react",
    "title": "ReAct (Reasoning + Acting)",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "acting",
      "combining",
      "framework",
      "prompting",
      "react",
      "reasoning"
    ],
    "excerpt": "A prompting framework combining Reasoning and Acting. AI thinks through problems step-by-step, showing its reasoning process transparently while taking actions.",
    "url": "pages/glossary.html#term-react"
  },
  {
    "id": "term-react-pattern",
    "title": "ReAct Pattern",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "action",
      "ai",
      "allowing",
      "approach",
      "calls",
      "dynamically",
      "execute",
      "generative",
      "interleaves",
      "iteratively",
      "language",
      "llm"
    ],
    "excerpt": "A prompting paradigm that interleaves reasoning traces and action steps, allowing a language model to dynamically plan, execute tool calls, observe results, and refine its approach iteratively.",
    "url": "pages/glossary.html#term-react-pattern"
  },
  {
    "id": "term-reasoning",
    "title": "Reasoning (AI)",
    "category": "Glossary",
    "subcategory": "Capability",
    "keywords": [
      "ability",
      "ai",
      "ais",
      "capability",
      "chains",
      "complex",
      "conclusions",
      "draw",
      "follow",
      "logical",
      "multistep",
      "problems"
    ],
    "excerpt": "AI's ability to draw logical conclusions, follow multi-step chains, and solve complex problems. A key capability that distinguishes modern LLMs from simpler systems.",
    "url": "pages/glossary.html#term-reasoning"
  },
  {
    "id": "term-recall",
    "title": "Recall",
    "category": "Glossary",
    "subcategory": "Metrics",
    "keywords": [
      "actual",
      "correctly",
      "evaluation",
      "identified",
      "measuring",
      "metric",
      "metrics",
      "positives",
      "proportion",
      "recall"
    ],
    "excerpt": "A metric measuring the proportion of actual positives correctly identified. Important in search and information retrieval where missing relevant results is costly.",
    "url": "pages/glossary.html#term-recall"
  },
  {
    "id": "term-recall-at-k",
    "title": "Recall at K",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "appear",
      "at",
      "captures",
      "comprehensively",
      "corpus",
      "cutoff",
      "documents",
      "evaluation",
      "given",
      "indicating",
      "information",
      "measures"
    ],
    "excerpt": "A retrieval metric that measures the proportion of all relevant documents in the corpus that appear within the top K retrieved results, indicating how comprehensively the system captures relevant information at a given cutoff.",
    "url": "pages/glossary.html#term-recall-at-k"
  },
  {
    "id": "term-recall-at-k-retrieval",
    "title": "Recall at K for Retrieval",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "appear",
      "assessing",
      "at",
      "completeness",
      "critical",
      "documents",
      "evaluation",
      "for",
      "hybrid",
      "measuring",
      "metric",
      "pipeline"
    ],
    "excerpt": "A retrieval-specific metric measuring the proportion of all relevant documents that appear within the top K results returned by a vector search or hybrid search system, critical for assessing RAG pipeline retrieval completeness.",
    "url": "pages/glossary.html#term-recall-at-k-retrieval"
  },
  {
    "id": "term-receiver-operating-characteristic",
    "title": "Receiver Operating Characteristic",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "across",
      "analysis",
      "characteristic",
      "classifier",
      "decision",
      "false",
      "graphical",
      "learning",
      "machine",
      "metrics",
      "operating",
      "performance"
    ],
    "excerpt": "A graphical analysis technique that plots classifier performance across all possible decision thresholds, showing the tradeoff between true positive rate and false positive rate at each threshold setting.",
    "url": "pages/glossary.html#term-receiver-operating-characteristic"
  },
  {
    "id": "term-receptive-field",
    "title": "Receptive Field",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "activation",
      "computer",
      "convolutional",
      "deeper",
      "field",
      "growing",
      "image",
      "influences",
      "input",
      "larger",
      "layer",
      "network"
    ],
    "excerpt": "The region of the original input image that influences a particular neuron's activation in a deeper layer, growing larger with each successive convolutional and pooling layer in the network.",
    "url": "pages/glossary.html#term-receptive-field"
  },
  {
    "id": "term-reciprocal-rank-fusion",
    "title": "Reciprocal Rank Fusion",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "aggregation",
      "assigning",
      "based",
      "combines",
      "document",
      "effective",
      "fuse",
      "fusion",
      "hybrid",
      "list",
      "lists",
      "method"
    ],
    "excerpt": "A rank aggregation method that combines result lists from multiple retrieval systems by assigning each document a score based on the reciprocal of its rank in each list, providing a simple yet effective way to fuse hybrid search results.",
    "url": "pages/glossary.html#term-reciprocal-rank-fusion"
  },
  {
    "id": "term-rectified-flow",
    "title": "Rectified Flow",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "approach",
      "architecture",
      "curved",
      "data",
      "diffusion",
      "distributions",
      "enabling",
      "faster",
      "flow",
      "generative",
      "learns",
      "maintaining"
    ],
    "excerpt": "A generative modeling approach that learns straight-line paths between noise and data distributions, enabling faster sampling than curved diffusion trajectories while maintaining sample quality.",
    "url": "pages/glossary.html#term-rectified-flow"
  },
  {
    "id": "term-recurrent-policy",
    "title": "Recurrent Policy",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "across",
      "components",
      "concepts",
      "core",
      "gru",
      "internal",
      "learning",
      "lstm",
      "maintain",
      "memory",
      "network",
      "neural"
    ],
    "excerpt": "An RL policy that uses recurrent neural network components (LSTM, GRU) to maintain internal memory across time steps.",
    "url": "pages/glossary.html#term-recurrent-policy"
  },
  {
    "id": "term-recursive-character-splitting",
    "title": "Recursive Character Splitting",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "attempts",
      "boundaries",
      "breaks",
      "character",
      "characters",
      "chunk",
      "chunking",
      "document",
      "ensuring",
      "hierarchy",
      "individual",
      "natural"
    ],
    "excerpt": "A document chunking strategy that attempts to split text using a hierarchy of separators from paragraph breaks down to individual characters, preferring natural boundaries while ensuring each chunk remains within the target size.",
    "url": "pages/glossary.html#term-recursive-character-splitting"
  },
  {
    "id": "term-recursive-feature-elimination",
    "title": "Recursive Feature Elimination",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "desired",
      "elimination",
      "engineering",
      "feature",
      "features",
      "importance",
      "important",
      "iterating",
      "learning",
      "least",
      "machine",
      "method"
    ],
    "excerpt": "A feature selection method that repeatedly trains a model, ranks features by importance, and removes the least important features, iterating until the desired number of features remains.",
    "url": "pages/glossary.html#term-recursive-feature-elimination"
  },
  {
    "id": "term-recursive-prompting",
    "title": "Recursive Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "arbitrarily",
      "architecture",
      "call",
      "complex",
      "condition",
      "construct",
      "enabling",
      "engineering",
      "extending",
      "handle",
      "loop",
      "met"
    ],
    "excerpt": "A prompting pattern where the output of one prompt call is used to construct the next prompt in a recursive loop, enabling the model to handle arbitrarily complex tasks by repeatedly refining or ex...",
    "url": "pages/glossary.html#term-recursive-prompting"
  },
  {
    "id": "term-red-team",
    "title": "Red Team",
    "category": "Glossary",
    "subcategory": "Safety",
    "keywords": [
      "attempting",
      "bypass",
      "elicit",
      "find",
      "group",
      "harmful",
      "measures",
      "outputs",
      "red",
      "safety",
      "systems",
      "team"
    ],
    "excerpt": "A group that tests AI systems by attempting to find vulnerabilities, bypass safety measures, or elicit harmful outputs. Essential for identifying and fixing safety issues before deployment.",
    "url": "pages/glossary.html#term-red-team"
  },
  {
    "id": "term-reduce-scatter",
    "title": "Reduce-Scatter Operation",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "across",
      "chunks",
      "collective",
      "communication",
      "computing",
      "data",
      "different",
      "distributed",
      "distributes",
      "model",
      "operation",
      "optimization"
    ],
    "excerpt": "A collective communication pattern that reduces data across all participants and distributes different chunks of the result to each participant.",
    "url": "pages/glossary.html#term-reduce-scatter"
  },
  {
    "id": "term-reflexion-pattern",
    "title": "Reflexion Pattern",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "agent",
      "ai",
      "architecture",
      "attempts",
      "episodic",
      "failed",
      "feedback",
      "generative",
      "improve",
      "llm",
      "memory",
      "pattern"
    ],
    "excerpt": "An agent architecture where the LLM reflects on previous failed attempts by storing verbal feedback in an episodic memory, using these reflections to improve performance on subsequent tries.",
    "url": "pages/glossary.html#term-reflexion-pattern"
  },
  {
    "id": "term-refusal",
    "title": "Refusal",
    "category": "Glossary",
    "subcategory": "Safety",
    "keywords": [
      "answer",
      "behavior",
      "declines",
      "due",
      "guidelines",
      "refusal",
      "request",
      "safety"
    ],
    "excerpt": "When AI declines to answer a request due to safety guidelines. Well-calibrated refusals protect against harm while overly cautious refusals reduce helpfulness.",
    "url": "pages/glossary.html#term-refusal"
  },
  {
    "id": "term-region-proposal-network",
    "title": "Region Proposal Network",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "bounding",
      "boxes",
      "candidate",
      "computer",
      "convolutional",
      "detection",
      "detectors",
      "faster",
      "feature",
      "first",
      "fully",
      "generate"
    ],
    "excerpt": "A fully convolutional network that slides over feature maps to generate object proposals (candidate bounding boxes) with objectness scores, serving as the first stage of two-stage detectors like Faster R-CNN.",
    "url": "pages/glossary.html#term-region-proposal-network"
  },
  {
    "id": "term-regression",
    "title": "Regression",
    "category": "Glossary",
    "subcategory": "ML Task",
    "keywords": [
      "categories",
      "continuous",
      "learning",
      "like",
      "machine",
      "ml",
      "prediction",
      "predicts",
      "prices",
      "rather",
      "regression",
      "task"
    ],
    "excerpt": "A machine learning task that predicts continuous values (like prices or temperatures) rather than categories. Common algorithms include linear regression and neural network regressors.",
    "url": "pages/glossary.html#term-regression"
  },
  {
    "id": "term-regret",
    "title": "Regret",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "action",
      "algorithm",
      "always",
      "bandit",
      "choosing",
      "cumulative",
      "difference",
      "hindsight",
      "learning",
      "machine",
      "obtained",
      "online"
    ],
    "excerpt": "In online learning and bandit problems, the cumulative difference between the reward obtained by an algorithm and the reward that would have been obtained by always choosing the optimal action in hindsight.",
    "url": "pages/glossary.html#term-regret"
  },
  {
    "id": "term-regret-bound",
    "title": "Regret Bound",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithm",
      "bound",
      "cumulative",
      "difference",
      "exploration",
      "guarantee",
      "learning",
      "obtained",
      "optimal",
      "policy",
      "regret",
      "reinforcement"
    ],
    "excerpt": "A theoretical guarantee on the cumulative difference between the reward obtained by an RL algorithm and the reward of the optimal policy over T steps.",
    "url": "pages/glossary.html#term-regret-bound"
  },
  {
    "id": "term-regularization",
    "title": "Regularization",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "adding",
      "constraints",
      "overfitting",
      "prevent",
      "regularization",
      "technique",
      "techniques",
      "training"
    ],
    "excerpt": "Techniques to prevent overfitting by adding constraints during training. Includes dropout, weight decay, and early stopping. Improves generalization to new data.",
    "url": "pages/glossary.html#term-regularization"
  },
  {
    "id": "term-reinforce-algorithm",
    "title": "REINFORCE Algorithm",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "algorithm",
      "carlo",
      "foundational",
      "gradient",
      "learning",
      "logprobability",
      "monte",
      "multiplied",
      "optimization",
      "parameters",
      "policy"
    ],
    "excerpt": "A foundational Monte Carlo policy gradient algorithm that updates policy parameters proportionally to the return multiplied by the gradient of log-probability of the action taken.",
    "url": "pages/glossary.html#term-reinforce-algorithm"
  },
  {
    "id": "term-reinforcement-learning",
    "title": "Reinforcement Learning (RL)",
    "category": "Glossary",
    "subcategory": "Learning Type",
    "keywords": [
      "actions",
      "agents",
      "learn",
      "learning",
      "paradigm",
      "penalties",
      "receiving",
      "reinforcement",
      "rewards",
      "rl",
      "training",
      "type"
    ],
    "excerpt": "A learning paradigm where agents learn by receiving rewards or penalties for actions. Used in RLHF to align LLMs with human preferences.",
    "url": "pages/glossary.html#term-reinforcement-learning"
  },
  {
    "id": "term-reinforcement-learning-history",
    "title": "Reinforcement Learning History",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1959",
      "1988",
      "alphago",
      "arthur",
      "breakthroughs",
      "checkers",
      "deep",
      "development",
      "difference",
      "dqn",
      "early",
      "history"
    ],
    "excerpt": "The development of reinforcement learning from early work by Arthur Samuel on checkers in 1959 through temporal difference learning by Sutton in 1988 to deep RL breakthroughs with DQN and AlphaGo.",
    "url": "pages/glossary.html#term-reinforcement-learning-history"
  },
  {
    "id": "term-rejection-sampling",
    "title": "Rejection Sampling",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "accepting",
      "based",
      "basic",
      "bound",
      "carlo",
      "comparison",
      "data",
      "density",
      "distribution",
      "generating",
      "method",
      "monte"
    ],
    "excerpt": "A basic Monte Carlo method for generating samples from a target distribution by sampling from a proposal distribution and accepting or rejecting samples based on a comparison with the target density scaled by a bound.",
    "url": "pages/glossary.html#term-rejection-sampling"
  },
  {
    "id": "term-relabeling",
    "title": "Relabeling",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "additional",
      "augmentation",
      "components",
      "data",
      "existing",
      "generate",
      "goals",
      "learning",
      "modifies",
      "paradigms",
      "reinforcement"
    ],
    "excerpt": "A data augmentation technique in RL that modifies components of stored transitions (such as goals, rewards, or actions) to generate additional training signal from existing data.",
    "url": "pages/glossary.html#term-relabeling"
  },
  {
    "id": "term-relation-extraction",
    "title": "Relation Extraction",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "classifying",
      "entities",
      "extracting",
      "extraction",
      "identifying",
      "mentioned",
      "nlp",
      "organization",
      "person",
      "processing",
      "relation",
      "relationships"
    ],
    "excerpt": "The task of identifying and classifying semantic relationships between entities mentioned in text, such as extracting that a person works for a specific organization.",
    "url": "pages/glossary.html#term-relation-extraction"
  },
  {
    "id": "term-relative-positional-encoding",
    "title": "Relative Positional Encoding",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "absolute",
      "architecture",
      "better",
      "distance",
      "enabling",
      "encodes",
      "encoding",
      "generalization",
      "lengths",
      "networks",
      "neural",
      "positional"
    ],
    "excerpt": "A positional encoding scheme that encodes the relative distance between tokens rather than absolute positions, enabling better generalization to sequence lengths not seen during training.",
    "url": "pages/glossary.html#term-relative-positional-encoding"
  },
  {
    "id": "term-relevance-score",
    "title": "Relevance Score",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "addresses",
      "alignment",
      "answer",
      "appropriateness",
      "content",
      "evaluating",
      "evaluation",
      "generated",
      "input",
      "intended",
      "matches",
      "measures"
    ],
    "excerpt": "A metric that measures how well a generated response addresses the input query or matches the intended topic, evaluating content appropriateness and topical alignment between the question and answer.",
    "url": "pages/glossary.html#term-relevance-score"
  },
  {
    "id": "term-relu",
    "title": "ReLU (Rectified Linear Unit)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "activation",
      "architecture",
      "function",
      "input",
      "inputs",
      "itself",
      "linear",
      "negative",
      "outputs",
      "positives",
      "rectified",
      "relu"
    ],
    "excerpt": "A simple activation function that outputs zero for negative inputs and the input itself for positives. Widely used due to efficiency and effectiveness despite simplicity.",
    "url": "pages/glossary.html#term-relu"
  },
  {
    "id": "term-repetition-penalty",
    "title": "Repetition Penalty",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "already",
      "appeared",
      "decoding",
      "generated",
      "generative",
      "loops",
      "model",
      "parameter",
      "penalty",
      "phrases",
      "preventing"
    ],
    "excerpt": "A decoding parameter that reduces the probability of tokens that have already appeared in the generated text, preventing the model from producing repetitive phrases or loops.",
    "url": "pages/glossary.html#term-repetition-penalty"
  },
  {
    "id": "term-repetition-rate",
    "title": "Repetition Rate",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "behavior",
      "degenerate",
      "detect",
      "evaluation",
      "excessive",
      "frequency",
      "generated",
      "looping",
      "metric",
      "metrics",
      "model",
      "patterns"
    ],
    "excerpt": "A metric that quantifies the frequency of repeated phrases, sentences, or patterns within generated text, used to detect and penalize degenerate model behavior such as looping or excessive redundancy.",
    "url": "pages/glossary.html#term-repetition-rate"
  },
  {
    "id": "term-rephrase-and-respond",
    "title": "Rephrase and Respond",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accurately",
      "and",
      "answering",
      "asks",
      "clarification",
      "comprehension",
      "engineering",
      "ensuring",
      "first",
      "improving",
      "input",
      "intent"
    ],
    "excerpt": "A prompting method that asks the model to first rephrase the input question in its own words before answering it, improving comprehension and reducing misinterpretation by ensuring the model accurately understands the query intent.",
    "url": "pages/glossary.html#term-rephrase-and-respond"
  },
  {
    "id": "term-replay-buffer",
    "title": "Replay Buffer",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "buffer",
      "circular",
      "data",
      "experience",
      "fixedsize",
      "learning",
      "methods",
      "offpolicy",
      "past",
      "reinforcement",
      "replay",
      "sampling"
    ],
    "excerpt": "A data structure (typically a fixed-size circular buffer) that stores past experience tuples for sampling during off-policy training.",
    "url": "pages/glossary.html#term-replay-buffer"
  },
  {
    "id": "term-replication-vector-databases",
    "title": "Replication in Vector Databases",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "across",
      "available",
      "copies",
      "database",
      "databases",
      "different",
      "ensuring",
      "even",
      "fail",
      "fault",
      "in",
      "increased"
    ],
    "excerpt": "The maintenance of multiple copies of a vector index across different nodes to provide fault tolerance and increased read throughput, ensuring that vector search remains available even when individual nodes fail.",
    "url": "pages/glossary.html#term-replication-vector-databases"
  },
  {
    "id": "term-representation-engineering",
    "title": "Representation Engineering",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "activation",
      "ai",
      "behavior",
      "concepts",
      "controlling",
      "correspond",
      "directions",
      "engineering",
      "generative",
      "honesty",
      "identifying",
      "like"
    ],
    "excerpt": "A technique for understanding and controlling LLM behavior by identifying and manipulating specific directions in the model's activation space that correspond to concepts like honesty, safety, or sentiment.",
    "url": "pages/glossary.html#term-representation-engineering"
  },
  {
    "id": "term-representation-learning",
    "title": "Representation Learning",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "automatically",
      "concept",
      "data",
      "deep",
      "engineering",
      "features",
      "learning",
      "manually",
      "rather",
      "representation",
      "them",
      "useful"
    ],
    "excerpt": "Learning useful features automatically from data rather than engineering them manually. A key strength of deep learning that enables transfer learning.",
    "url": "pages/glossary.html#term-representation-learning"
  },
  {
    "id": "term-representation-learning-rl",
    "title": "Representation Learning in RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "compact",
      "concepts",
      "core",
      "efficiency",
      "highdimensional",
      "images",
      "improve",
      "in",
      "informative",
      "learning",
      "like",
      "methods"
    ],
    "excerpt": "Methods for learning compact, informative state representations from high-dimensional observations (like images) to improve RL efficiency.",
    "url": "pages/glossary.html#term-representation-learning-rl"
  },
  {
    "id": "term-representational-harm",
    "title": "Representational Harm",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "affected",
      "ai",
      "allocation",
      "decision",
      "demeans",
      "direct",
      "erases",
      "ethics",
      "even",
      "fairness",
      "groups",
      "harm"
    ],
    "excerpt": "Harm that occurs when an AI system reinforces stereotypes, demeans, or erases particular social groups through its outputs, even if no direct resource allocation decision is affected.",
    "url": "pages/glossary.html#term-representational-harm"
  },
  {
    "id": "term-request-scheduling",
    "title": "Request Scheduling",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "algorithm",
      "determines",
      "gpu",
      "incoming",
      "inference",
      "infrastructure",
      "limited",
      "model",
      "optimization",
      "order",
      "priority",
      "processing"
    ],
    "excerpt": "The algorithm that determines the order and priority of processing incoming inference requests on limited GPU resources.",
    "url": "pages/glossary.html#term-request-scheduling"
  },
  {
    "id": "term-reranking",
    "title": "Reranking",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "approach",
      "candidate",
      "crossencoder",
      "documents",
      "fast",
      "fetches",
      "generative",
      "initial",
      "llm",
      "model",
      "passing"
    ],
    "excerpt": "A two-stage retrieval approach where an initial fast retriever fetches candidate documents, and a more powerful cross-encoder model rescores and reorders them for relevance before passing them to the LLM.",
    "url": "pages/glossary.html#term-reranking"
  },
  {
    "id": "term-residual-analysis",
    "title": "Residual Analysis",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "analysis",
      "assess",
      "assumptions",
      "check",
      "data",
      "differences",
      "examination",
      "fit",
      "homoscedasticity",
      "identify",
      "influential",
      "model"
    ],
    "excerpt": "The examination of residuals (differences between observed and predicted values) to assess model fit, check assumptions such as normality and homoscedasticity, and identify influential observations or patterns.",
    "url": "pages/glossary.html#term-residual-analysis"
  },
  {
    "id": "term-residual-connection",
    "title": "Residual Connection",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "added",
      "additive",
      "allowing",
      "architecture",
      "block",
      "blocks",
      "connection",
      "direct",
      "elementwise",
      "input",
      "layer",
      "learn"
    ],
    "excerpt": "An additive skip connection where the input to a layer block is added element-wise to the block's output, allowing the network to learn residual mappings rather than direct mappings.",
    "url": "pages/glossary.html#term-residual-connection"
  },
  {
    "id": "term-residual-learning",
    "title": "Residual Learning",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "additive",
      "architecture",
      "deep",
      "easier",
      "functions",
      "inputs",
      "layer",
      "learning",
      "making",
      "networks",
      "neural",
      "optimize"
    ],
    "excerpt": "The principle of learning additive residual functions with reference to the layer inputs rather than learning unreferenced functions, making it easier to optimize very deep networks.",
    "url": "pages/glossary.html#term-residual-learning"
  },
  {
    "id": "term-resnet",
    "title": "ResNet",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "connections",
      "convolutional",
      "deep",
      "enable",
      "gradient",
      "introduces",
      "mitigating",
      "network",
      "networks",
      "neural",
      "problem"
    ],
    "excerpt": "Residual Network, a deep convolutional neural network architecture that introduces skip connections to enable training of very deep networks by mitigating the vanishing gradient problem.",
    "url": "pages/glossary.html#term-resnet"
  },
  {
    "id": "term-responsible-ai",
    "title": "Responsible AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "accountability",
      "ai",
      "approach",
      "benefit",
      "considerations",
      "deploying",
      "developing",
      "emphasizes",
      "entire",
      "ethical",
      "ethics",
      "fairness"
    ],
    "excerpt": "An approach to developing and deploying AI systems that emphasizes ethical considerations, fairness, transparency, accountability, and societal benefit throughout the entire AI lifecycle.",
    "url": "pages/glossary.html#term-responsible-ai"
  },
  {
    "id": "term-responsible-disclosure-for-ai",
    "title": "Responsible Disclosure for AI",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "allowing",
      "capabilities",
      "dangerous",
      "developer",
      "disclosure",
      "discovered",
      "ensuring",
      "for",
      "governance",
      "mitigation",
      "practice"
    ],
    "excerpt": "The practice of privately reporting discovered vulnerabilities or dangerous capabilities in AI systems to the developer before public disclosure, allowing time for mitigation while ensuring transparency.",
    "url": "pages/glossary.html#term-responsible-disclosure-for-ai"
  },
  {
    "id": "term-restricted-boltzmann-machine",
    "title": "Restricted Boltzmann Machine",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "bipartite",
      "boltzmann",
      "connections",
      "contrastive",
      "distributions",
      "divergence",
      "generative",
      "hidden",
      "inputs",
      "intralayer",
      "learn"
    ],
    "excerpt": "A generative stochastic neural network with a bipartite structure of visible and hidden units with no intra-layer connections, trained using contrastive divergence to learn probability distributions over inputs.",
    "url": "pages/glossary.html#term-restricted-boltzmann-machine"
  },
  {
    "id": "term-retinanet",
    "title": "RetinaNet",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "accuracy",
      "achieving",
      "advantage",
      "backbone",
      "combines",
      "comparable",
      "computer",
      "detection",
      "detectors",
      "feature",
      "focal",
      "loss"
    ],
    "excerpt": "A single-stage object detection model that combines a Feature Pyramid Network backbone with focal loss, achieving accuracy comparable to two-stage detectors while maintaining the speed advantage of single-stage methods.",
    "url": "pages/glossary.html#term-retinanet"
  },
  {
    "id": "term-retnet",
    "title": "RetNet",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "chunkwise",
      "computation",
      "inference",
      "mechanism",
      "network",
      "networks",
      "neural",
      "parallel",
      "recurrent",
      "replaces"
    ],
    "excerpt": "Retentive Network, an architecture that supports parallel training, recurrent inference, and chunk-wise recurrent computation through a retention mechanism that replaces standard attention.",
    "url": "pages/glossary.html#term-retnet"
  },
  {
    "id": "term-retrieval",
    "title": "Retrieval",
    "category": "Glossary",
    "subcategory": "Process",
    "keywords": [
      "corpus",
      "database",
      "finding",
      "information",
      "process",
      "relevant",
      "retrieval",
      "search"
    ],
    "excerpt": "Finding relevant information from a database or corpus. In RAG, retrieval brings external knowledge into the generation process for more accurate responses.",
    "url": "pages/glossary.html#term-retrieval"
  },
  {
    "id": "term-retrieval-evaluation",
    "title": "Retrieval Evaluation",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "against",
      "answer",
      "applied",
      "assessment",
      "conducted",
      "evaluation",
      "groundtruth",
      "judgments",
      "labeled",
      "lists",
      "metrics",
      "mrr"
    ],
    "excerpt": "The systematic assessment of retrieval system quality using metrics such as recall, precision, NDCG, and MRR applied to ranked result lists, often conducted against labeled relevance judgments or ground-truth answer sets.",
    "url": "pages/glossary.html#term-retrieval-evaluation"
  },
  {
    "id": "term-retrieval-head",
    "title": "Retrieval Head",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "attention",
      "context",
      "copying",
      "crucial",
      "factual",
      "generative",
      "head",
      "heads",
      "incontext",
      "information",
      "learning"
    ],
    "excerpt": "Specific attention heads within a transformer that specialize in copying or retrieving information from the context, playing a crucial role in in-context learning and factual recall.",
    "url": "pages/glossary.html#term-retrieval-head"
  },
  {
    "id": "term-retrieval-augmented-fine-tuning",
    "title": "Retrieval-Augmented Fine-Tuning",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "approach",
      "augmented",
      "context",
      "effectively",
      "examples",
      "fine",
      "finetunes",
      "generative",
      "incorporate",
      "language",
      "llm"
    ],
    "excerpt": "A training approach that fine-tunes a language model with retrieval-augmented examples, teaching the model to effectively incorporate retrieved context into its responses.",
    "url": "pages/glossary.html#term-retrieval-augmented-fine-tuning"
  },
  {
    "id": "term-retrieval-augmented-lm",
    "title": "Retrieval-Augmented Language Model",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "architecture",
      "augmented",
      "component",
      "corpus",
      "documents",
      "evidence",
      "external",
      "fetch",
      "generation",
      "grounding",
      "incorporates",
      "language"
    ],
    "excerpt": "A language model architecture that incorporates a retrieval component to fetch relevant documents from an external corpus during generation, grounding responses in retrieved evidence.",
    "url": "pages/glossary.html#term-retrieval-augmented-lm"
  },
  {
    "id": "term-retrieval-augmented-prompting",
    "title": "Retrieval-Augmented Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accuracy",
      "augmented",
      "beyond",
      "context",
      "corpus",
      "coverage",
      "documents",
      "domain",
      "dynamically",
      "engineering",
      "examples",
      "external"
    ],
    "excerpt": "A technique that dynamically retrieves relevant documents, examples, or knowledge from an external corpus and incorporates them into the prompt context before generation, improving factual accuracy...",
    "url": "pages/glossary.html#term-retrieval-augmented-prompting"
  },
  {
    "id": "term-return",
    "title": "Return",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "concepts",
      "core",
      "cumulative",
      "discounted",
      "future",
      "given",
      "learning",
      "longterm",
      "receives",
      "reinforcement",
      "representing"
    ],
    "excerpt": "The cumulative discounted sum of future rewards from a given time step, representing the total long-term value an agent receives. The return is the primary quantity that RL algorithms aim to maximize.",
    "url": "pages/glossary.html#term-return"
  },
  {
    "id": "term-reward",
    "title": "Reward",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "agent",
      "bad",
      "concepts",
      "core",
      "environment",
      "good",
      "indicating",
      "learning",
      "outcome",
      "received",
      "reinforcement"
    ],
    "excerpt": "A scalar signal received by an agent from the environment after taking an action, indicating how good or bad the outcome was. Rewards drive learning by defining the optimization objective.",
    "url": "pages/glossary.html#term-reward"
  },
  {
    "id": "term-reward-clipping",
    "title": "Reward Clipping",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "across",
      "bounds",
      "clipping",
      "commonly",
      "design",
      "diverse",
      "environments",
      "fixed",
      "learning",
      "preprocessing",
      "range",
      "reinforcement"
    ],
    "excerpt": "A preprocessing technique that bounds reward values to a fixed range (commonly [-1, 1]) to stabilize training across diverse environments.",
    "url": "pages/glossary.html#term-reward-clipping"
  },
  {
    "id": "term-reward-decomposition",
    "title": "Reward Decomposition",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "break",
      "complex",
      "components",
      "decomposition",
      "design",
      "easier",
      "efficient",
      "enabling",
      "interpretable",
      "learn",
      "learning",
      "reinforcement"
    ],
    "excerpt": "Techniques that break a complex reward signal into simpler components that are easier to learn from, enabling more interpretable and efficient training.",
    "url": "pages/glossary.html#term-reward-decomposition"
  },
  {
    "id": "term-reward-engineering",
    "title": "Reward Engineering",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "accurately",
      "agent",
      "balancing",
      "behavior",
      "capture",
      "design",
      "designing",
      "desired",
      "engineering",
      "functions",
      "generality",
      "learning"
    ],
    "excerpt": "The process of designing reward functions that accurately capture desired agent behavior, balancing specificity with generality.",
    "url": "pages/glossary.html#term-reward-engineering"
  },
  {
    "id": "term-reward-hacking",
    "title": "Reward Hacking",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "achieve",
      "ai",
      "exploits",
      "failure",
      "feedback",
      "generative",
      "genuinely",
      "hacking",
      "high",
      "human",
      "improving",
      "learning"
    ],
    "excerpt": "A failure mode in reinforcement learning from human feedback where the policy model exploits weaknesses in the reward model to achieve high reward scores without genuinely improving output quality.",
    "url": "pages/glossary.html#term-reward-hacking"
  },
  {
    "id": "term-reward-machine",
    "title": "Reward Machine",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "automaton",
      "based",
      "complex",
      "design",
      "enabling",
      "events",
      "finitestate",
      "functions",
      "highlevel",
      "learning",
      "machine",
      "propositional"
    ],
    "excerpt": "A finite-state automaton that specifies reward functions based on high-level events or propositional symbols, enabling structured reward specification for complex tasks.",
    "url": "pages/glossary.html#term-reward-machine"
  },
  {
    "id": "term-reward-model",
    "title": "Reward Model",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "alignment",
      "based",
      "human",
      "model",
      "outputs",
      "preferences",
      "reward",
      "score",
      "trained",
      "training"
    ],
    "excerpt": "A model trained to score AI outputs based on human preferences. Used in RLHF to guide language models toward more helpful and safe behaviors.",
    "url": "pages/glossary.html#term-reward-model"
  },
  {
    "id": "term-reward-normalization",
    "title": "Reward Normalization",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "across",
      "consistent",
      "design",
      "different",
      "environments",
      "learning",
      "magnitude",
      "normalization",
      "practice",
      "reinforcement",
      "reward",
      "running"
    ],
    "excerpt": "The practice of scaling reward signals to have consistent magnitude across different environments or during training, typically using running statistics.",
    "url": "pages/glossary.html#term-reward-normalization"
  },
  {
    "id": "term-reward-shaping",
    "title": "Reward Shaping",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "adding",
      "auxiliary",
      "design",
      "guide",
      "learning",
      "making",
      "practice",
      "problems",
      "reinforcement",
      "reward",
      "shaping",
      "signals"
    ],
    "excerpt": "The practice of adding auxiliary reward signals to guide learning, making sparse reward problems more tractable.",
    "url": "pages/glossary.html#term-reward-shaping"
  },
  {
    "id": "term-reward-free-exploration",
    "title": "Reward-Free Exploration",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "build",
      "comprehensive",
      "downstream",
      "environment",
      "exploration",
      "explores",
      "first",
      "free",
      "knowledge",
      "learning",
      "paradigm"
    ],
    "excerpt": "An RL paradigm where the agent first explores the environment without any reward signal to build a comprehensive understanding, then uses this knowledge to quickly solve downstream tasks.",
    "url": "pages/glossary.html#term-reward-free-exploration"
  },
  {
    "id": "term-reward-weighted-regression",
    "title": "Reward-Weighted Regression",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "computes",
      "estimation",
      "exponentiated",
      "learning",
      "likelihood",
      "maximum",
      "method",
      "optimization",
      "performing",
      "policy",
      "proportional",
      "regression"
    ],
    "excerpt": "A policy search method that computes policy updates by performing weighted maximum likelihood estimation on sampled trajectories, with weights proportional to exponentiated returns.",
    "url": "pages/glossary.html#term-reward-weighted-regression"
  },
  {
    "id": "term-rst",
    "title": "Rhetorical Structure Theory",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "cause",
      "clauses",
      "connected",
      "contrast",
      "describes",
      "discourse",
      "elaboration",
      "forming",
      "hierarchical",
      "larger",
      "like",
      "linguistics"
    ],
    "excerpt": "A theory of text organization that describes how clauses and larger text spans are connected through rhetorical relations like elaboration, contrast, and cause, forming a hierarchical discourse tree.",
    "url": "pages/glossary.html#term-rst"
  },
  {
    "id": "term-richard-sutton",
    "title": "Richard Sutton",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "andrew",
      "barto",
      "bitter",
      "canadian",
      "coauthored",
      "computer",
      "developed",
      "difference",
      "essay",
      "history",
      "influential",
      "learning"
    ],
    "excerpt": "Canadian computer scientist who co-authored the seminal textbook on reinforcement learning with Andrew Barto, developed temporal difference learning, and wrote the influential Bitter Lesson essay on AI research methodology.",
    "url": "pages/glossary.html#term-richard-sutton"
  },
  {
    "id": "term-ridge-regression",
    "title": "Ridge Regression",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "adds",
      "coefficients",
      "correlated",
      "especially",
      "learning",
      "least",
      "linear",
      "machine",
      "model",
      "objective",
      "ordinary",
      "overfitting"
    ],
    "excerpt": "A linear regression variant that adds an L2 penalty term to the ordinary least squares objective, shrinking coefficients toward zero to reduce overfitting, especially when predictors are correlated.",
    "url": "pages/glossary.html#term-ridge-regression"
  },
  {
    "id": "term-right-to-explanation",
    "title": "Right to Explanation",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "ai",
      "article",
      "automated",
      "codified",
      "decisionmaking",
      "entitled",
      "ethical",
      "ethics",
      "explanation",
      "gdprs",
      "governance",
      "individuals"
    ],
    "excerpt": "The legal or ethical principle that individuals subjected to automated decision-making are entitled to a meaningful explanation of the logic involved, as partially codified in the GDPR's Article 22.",
    "url": "pages/glossary.html#term-right-to-explanation"
  },
  {
    "id": "term-ring-all-reduce",
    "title": "Ring All-Reduce",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "all",
      "allgather",
      "allreduce",
      "arranged",
      "chunks",
      "computing",
      "data",
      "distributed",
      "efficient",
      "gpus",
      "implementation",
      "model"
    ],
    "excerpt": "An efficient implementation of all-reduce where GPUs are arranged in a ring topology and data is sent in chunks through two passes (scatter-reduce and all-gather).",
    "url": "pages/glossary.html#term-ring-all-reduce"
  },
  {
    "id": "term-ring-attention",
    "title": "Ring Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "across",
      "architecture",
      "attention",
      "communication",
      "computation",
      "devices",
      "distributed",
      "length",
      "long",
      "method",
      "nearunlimited",
      "networks"
    ],
    "excerpt": "A distributed attention computation method that splits long sequences across devices in a ring topology, overlapping communication with computation to process sequences of near-unlimited length.",
    "url": "pages/glossary.html#term-ring-attention"
  },
  {
    "id": "term-risk-sensitive-rl",
    "title": "Risk-Sensitive RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "alone",
      "conditional",
      "cvar",
      "expected",
      "learning",
      "methods",
      "objectives",
      "optimize",
      "rather",
      "reinforcement",
      "return",
      "returns"
    ],
    "excerpt": "RL methods that optimize risk-aware objectives such as conditional value-at-risk (CVaR) or variance-penalized returns rather than expected return alone.",
    "url": "pages/glossary.html#term-risk-sensitive-rl"
  },
  {
    "id": "term-rlhf",
    "title": "RLHF (Reinforcement Learning from Human Feedback)",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "alignment",
      "behavior",
      "feedback",
      "from",
      "guide",
      "human",
      "learning",
      "model",
      "preferences",
      "reinforcement",
      "rlhf",
      "technique"
    ],
    "excerpt": "A training technique that uses human preferences to guide model behavior. Human raters compare outputs, and these preferences train a reward model that shapes the LLM.",
    "url": "pages/glossary.html#term-rlhf"
  },
  {
    "id": "term-rms-normalization",
    "title": "RMS Normalization",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "activations",
      "architecture",
      "computational",
      "cost",
      "layer",
      "maintaining",
      "mean",
      "networks",
      "neural",
      "normalization",
      "only",
      "performance"
    ],
    "excerpt": "Root Mean Square Layer Normalization, a simplified variant of layer normalization that only rescales by the root mean square of activations without recentering, reducing computational cost while maintaining performance.",
    "url": "pages/glossary.html#term-rms-normalization"
  },
  {
    "id": "term-rmsnorm",
    "title": "RMSNorm",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "activations",
      "ai",
      "centering",
      "computation",
      "generative",
      "layer",
      "llm",
      "maintaining",
      "mean",
      "model",
      "normalization",
      "normalizes"
    ],
    "excerpt": "Root Mean Square Layer Normalization, a simplified normalization technique that normalizes activations using only the RMS statistic without mean centering, reducing computation while maintaining model quality.",
    "url": "pages/glossary.html#term-rmsnorm"
  },
  {
    "id": "term-rmsprop",
    "title": "RMSProp",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "adaptive",
      "algorithm",
      "average",
      "divides",
      "gradients",
      "learning",
      "machine",
      "magnitudes",
      "optimization",
      "parameter",
      "rate",
      "recent"
    ],
    "excerpt": "An adaptive learning rate optimization algorithm that divides the learning rate by a running average of the magnitudes of recent gradients for each parameter.",
    "url": "pages/glossary.html#term-rmsprop"
  },
  {
    "id": "term-rnn",
    "title": "RNN (Recurrent Neural Network)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "across",
      "architecture",
      "hidden",
      "historical",
      "maintaining",
      "network",
      "neural",
      "processes",
      "recurrent",
      "rnn",
      "sequences",
      "state"
    ],
    "excerpt": "A neural network architecture that processes sequences by maintaining hidden state across steps. Predecessors to transformers, still used in some sequence applications.",
    "url": "pages/glossary.html#term-rnn"
  },
  {
    "id": "term-roberta",
    "title": "RoBERTa",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "approach",
      "architecture",
      "bert",
      "data",
      "dynamic",
      "improves",
      "longer",
      "masking",
      "networks",
      "neural",
      "next",
      "optimized"
    ],
    "excerpt": "A robustly optimized BERT pretraining approach that improves upon BERT by training longer with more data, removing next sentence prediction, and using dynamic masking patterns.",
    "url": "pages/glossary.html#term-roberta"
  },
  {
    "id": "term-robot-rights",
    "title": "Robot Rights",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "active",
      "advanced",
      "ai",
      "analogous",
      "animals",
      "concept",
      "consideration",
      "debate",
      "deserve",
      "ethics",
      "governance",
      "granted"
    ],
    "excerpt": "The concept that sufficiently advanced robots or AI systems might deserve legal protections or moral consideration analogous to those granted to humans or animals, a topic of active philosophical and legal debate.",
    "url": "pages/glossary.html#term-robot-rights"
  },
  {
    "id": "term-robust-regression",
    "title": "Robust Regression",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "assumptions",
      "class",
      "designed",
      "methods",
      "model",
      "outliers",
      "regression",
      "resistant",
      "robust",
      "selection",
      "statistics",
      "violations"
    ],
    "excerpt": "A class of regression methods designed to be resistant to outliers and violations of model assumptions. Techniques include M-estimation, least trimmed squares, and RANSAC.",
    "url": "pages/glossary.html#term-robust-regression"
  },
  {
    "id": "term-robust-rl",
    "title": "Robust RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithms",
      "designed",
      "environment",
      "find",
      "learning",
      "model",
      "perform",
      "perturbations",
      "policies",
      "reinforcement",
      "rl",
      "robust"
    ],
    "excerpt": "RL algorithms designed to find policies that perform well under worst-case environment perturbations or model uncertainty.",
    "url": "pages/glossary.html#term-robust-rl"
  },
  {
    "id": "term-robust-scaler",
    "title": "Robust Scaler",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "deviation",
      "engineering",
      "feature",
      "instead",
      "interquartile",
      "learning",
      "machine",
      "making",
      "mean",
      "median",
      "method",
      "outliers"
    ],
    "excerpt": "A feature scaling method that uses the median and interquartile range instead of the mean and standard deviation, making it more resistant to outliers than standard scaling.",
    "url": "pages/glossary.html#term-robust-scaler"
  },
  {
    "id": "term-roc-curve",
    "title": "ROC Curve",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "against",
      "characteristic",
      "classification",
      "curve",
      "false",
      "learning",
      "machine",
      "metrics",
      "operating",
      "plot",
      "positive",
      "rate"
    ],
    "excerpt": "Receiver Operating Characteristic curve, a plot of the true positive rate against the false positive rate at various classification thresholds.",
    "url": "pages/glossary.html#term-roc-curve"
  },
  {
    "id": "term-roger-schank",
    "title": "Roger Schank",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19462023",
      "advancing",
      "american",
      "comprehension",
      "conceptual",
      "dependency",
      "developed",
      "history",
      "knowledge",
      "language",
      "natural",
      "pioneers"
    ],
    "excerpt": "American AI researcher (1946-2023) who developed conceptual dependency theory and script theory for natural language understanding, advancing the role of knowledge structures in AI comprehension of stories and situations.",
    "url": "pages/glossary.html#term-roger-schank"
  },
  {
    "id": "term-roi-align",
    "title": "ROI Align",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "accurate",
      "align",
      "artifacts",
      "bilinear",
      "computer",
      "detection",
      "eliminating",
      "extraction",
      "feature",
      "grid",
      "improved",
      "instance"
    ],
    "excerpt": "An improved version of ROI Pooling that uses bilinear interpolation instead of quantized grid snapping, eliminating misalignment artifacts and producing more accurate feature extraction for instance segmentation.",
    "url": "pages/glossary.html#term-roi-align"
  },
  {
    "id": "term-roi-pooling",
    "title": "ROI Pooling",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "arbitrarysized",
      "classification",
      "computer",
      "detection",
      "detectors",
      "enabling",
      "extracts",
      "feature",
      "fixedsize",
      "head",
      "interest",
      "object"
    ],
    "excerpt": "Region of Interest Pooling, an operation that extracts fixed-size feature representations from arbitrary-sized region proposals, enabling the classification head of object detectors to process variable-sized regions uniformly.",
    "url": "pages/glossary.html#term-roi-pooling"
  },
  {
    "id": "term-role-assignment",
    "title": "Role Assignment",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "adopt",
      "assigned",
      "assignment",
      "character",
      "designating",
      "engineering",
      "expertise",
      "explicitly",
      "identity",
      "level",
      "match",
      "model"
    ],
    "excerpt": "The prompt technique of explicitly designating a specific role, profession, or character for the model to adopt, shaping its response style, vocabulary, expertise level, and perspective to match the assigned identity.",
    "url": "pages/glossary.html#term-role-assignment"
  },
  {
    "id": "term-role-prompting",
    "title": "Role Prompting",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "assigning",
      "expertise",
      "persona",
      "perspective",
      "prompting",
      "responses",
      "role",
      "shape",
      "specific",
      "technique"
    ],
    "excerpt": "Assigning AI a specific persona, expertise, or perspective to shape its responses. For example, \"Act as a senior developer\" or \"You are a patient teacher.\"",
    "url": "pages/glossary.html#term-role-prompting"
  },
  {
    "id": "term-roofline-model",
    "title": "Roofline Model",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "achievable",
      "analysis",
      "byte",
      "flops",
      "framework",
      "function",
      "hardware",
      "intensity",
      "memory",
      "model",
      "operational",
      "optimization"
    ],
    "excerpt": "A performance analysis framework that plots achievable performance as a function of operational intensity (FLOPS per byte of memory traffic).",
    "url": "pages/glossary.html#term-roofline-model"
  },
  {
    "id": "term-root-mean-squared-error",
    "title": "Root Mean Squared Error",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "error",
      "expressed",
      "learning",
      "machine",
      "mean",
      "metrics",
      "root",
      "same",
      "square",
      "squared",
      "target",
      "units"
    ],
    "excerpt": "The square root of the mean squared error, expressed in the same units as the target variable. It provides an interpretable measure of the typical magnitude of prediction errors.",
    "url": "pages/glossary.html#term-root-mean-squared-error"
  },
  {
    "id": "term-rope",
    "title": "RoPE (Rotary Position Embedding)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "complex",
      "embedding",
      "encodes",
      "encoding",
      "position",
      "positional",
      "rope",
      "rotary",
      "rotation",
      "space",
      "technique"
    ],
    "excerpt": "A positional encoding technique that encodes position through rotation in complex space. Enables better length generalization than absolute position encodings.",
    "url": "pages/glossary.html#term-rope"
  },
  {
    "id": "term-ross-quillian",
    "title": "Ross Quillian",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1968",
      "american",
      "approach",
      "associative",
      "computer",
      "doctoral",
      "establishing",
      "foundational",
      "his",
      "history",
      "human",
      "introduced"
    ],
    "excerpt": "American computer scientist who introduced semantic networks in his 1968 doctoral thesis as a model of human associative memory, establishing a foundational approach to knowledge representation in AI.",
    "url": "pages/glossary.html#term-ross-quillian"
  },
  {
    "id": "term-rotary-position-embedding",
    "title": "Rotary Position Embedding",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "according",
      "architecture",
      "dimensions",
      "embedding",
      "encodes",
      "encoding",
      "information",
      "inner",
      "key",
      "method",
      "naturally",
      "networks"
    ],
    "excerpt": "A method that encodes position information by rotating query and key vectors in pairs of dimensions according to their position, naturally encoding relative positions through the inner product.",
    "url": "pages/glossary.html#term-rotary-position-embedding"
  },
  {
    "id": "term-rouge",
    "title": "ROUGE",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "evaluating",
      "evaluation",
      "generated",
      "gisting",
      "measuring",
      "metrics",
      "ngram",
      "nlp",
      "overlap",
      "processing",
      "quality",
      "recalloriented"
    ],
    "excerpt": "Recall-Oriented Understudy for Gisting Evaluation, a set of metrics for evaluating summarization quality by measuring n-gram overlap between generated summaries and reference summaries.",
    "url": "pages/glossary.html#term-rouge"
  },
  {
    "id": "term-rouge-score",
    "title": "ROUGE Score",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "common",
      "computing",
      "cooccurrence",
      "evaluation",
      "family",
      "generated",
      "gisting",
      "longest",
      "measures",
      "metrics",
      "ngram",
      "overlap"
    ],
    "excerpt": "Recall-Oriented Understudy for Gisting Evaluation, a family of metrics that measures text summarization quality by computing n-gram overlap, longest common subsequences, or skip-bigram co-occurrence between generated and reference summaries.",
    "url": "pages/glossary.html#term-rouge-score"
  },
  {
    "id": "term-rouge-1",
    "title": "ROUGE-1",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "assessment",
      "basic",
      "content",
      "coverage",
      "evaluation",
      "generated",
      "individual",
      "level",
      "measures",
      "metrics",
      "overlap",
      "providing"
    ],
    "excerpt": "A ROUGE variant that measures unigram (single word) overlap between a generated text and reference text, providing a basic assessment of content coverage at the individual word level.",
    "url": "pages/glossary.html#term-rouge-1"
  },
  {
    "id": "term-rouge-2",
    "title": "ROUGE-2",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "bigram",
      "capturing",
      "consecutive",
      "content",
      "evaluation",
      "fluency",
      "generated",
      "indicator",
      "measures",
      "metrics",
      "overlap",
      "phraselevel"
    ],
    "excerpt": "A ROUGE variant that measures bigram (two consecutive word) overlap between generated and reference texts, capturing phrase-level similarity and providing a stronger indicator of fluency and content preservation than ROUGE-1.",
    "url": "pages/glossary.html#term-rouge-2"
  },
  {
    "id": "term-rouge-l",
    "title": "ROUGE-L",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "based",
      "capturing",
      "common",
      "consecutive",
      "evaluation",
      "generated",
      "lcs",
      "longest",
      "making",
      "matches",
      "metrics",
      "ordering"
    ],
    "excerpt": "A ROUGE variant based on the longest common subsequence (LCS) between generated and reference texts, capturing sentence-level structural similarity without requiring consecutive word matches, making it sensitive to word ordering.",
    "url": "pages/glossary.html#term-rouge-l"
  },
  {
    "id": "term-rt-detr",
    "title": "RT-DETR",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "accuracy",
      "achieving",
      "architecture",
      "backbone",
      "cnn",
      "combines",
      "computer",
      "decoder",
      "detection",
      "detr",
      "detrbased",
      "hybrid"
    ],
    "excerpt": "Real-Time Detection Transformer, a hybrid detection architecture that combines a CNN backbone with a transformer decoder, achieving the accuracy of DETR-based models with real-time inference speed.",
    "url": "pages/glossary.html#term-rt-detr"
  },
  {
    "id": "term-rwkv",
    "title": "RWKV",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "approach",
      "architecture",
      "attentionbased",
      "channelmixing",
      "combines",
      "efficient",
      "inference",
      "linear",
      "networks",
      "neural",
      "novel",
      "parallelizable"
    ],
    "excerpt": "A linear attention-based architecture that combines the parallelizable training of transformers with the efficient inference of RNNs, using a novel time-mixing and channel-mixing approach.",
    "url": "pages/glossary.html#term-rwkv"
  },
  {
    "id": "term-s4",
    "title": "S4",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "based",
      "computation",
      "dependencies",
      "efficient",
      "fft",
      "framework",
      "handle",
      "hippo",
      "initialization",
      "longrange",
      "model"
    ],
    "excerpt": "Structured State Spaces for Sequence Modeling, a state space model that uses a special initialization based on the HiPPO framework and efficient computation via FFT to handle very long-range dependencies.",
    "url": "pages/glossary.html#term-s4"
  },
  {
    "id": "term-safe-rl",
    "title": "Safe Reinforcement Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "agent",
      "constraints",
      "dangerous",
      "deployment",
      "incorporate",
      "learning",
      "methods",
      "prevent",
      "reinforcement",
      "safe",
      "safety"
    ],
    "excerpt": "RL methods that incorporate safety constraints to prevent the agent from taking dangerous or unacceptable actions during training and deployment.",
    "url": "pages/glossary.html#term-safe-rl"
  },
  {
    "id": "term-safety-filter",
    "title": "Safety Filter",
    "category": "Glossary",
    "subcategory": "Safety",
    "keywords": [
      "block",
      "content",
      "detect",
      "filter",
      "harmful",
      "inputs",
      "outputs",
      "safety",
      "security",
      "systems"
    ],
    "excerpt": "Systems that detect and block harmful content in AI inputs or outputs. Part of the safety stack protecting users from inappropriate, dangerous, or illegal content.",
    "url": "pages/glossary.html#term-safety-filter"
  },
  {
    "id": "term-safety-score",
    "title": "Safety Score",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "advice",
      "aggregates",
      "alignment",
      "assessment",
      "bias",
      "composite",
      "dangerous",
      "evaluation",
      "generation",
      "harmful",
      "including",
      "measurements"
    ],
    "excerpt": "A composite evaluation metric that aggregates measurements of harmful output generation including toxicity, bias, dangerous advice, and policy violations, providing an overall assessment of a model's safety alignment.",
    "url": "pages/glossary.html#term-safety-score"
  },
  {
    "id": "term-sam-altman",
    "title": "Sam Altman",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "altman",
      "american",
      "became",
      "ceo",
      "chatgpt",
      "development",
      "entrepreneur",
      "gpt4",
      "history",
      "launch",
      "openai",
      "overseeing"
    ],
    "excerpt": "American entrepreneur who became CEO of OpenAI, overseeing the development and launch of ChatGPT and GPT-4.",
    "url": "pages/glossary.html#term-sam-altman"
  },
  {
    "id": "term-sambanova",
    "title": "SambaNova",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "adapt",
      "architecture",
      "architectures",
      "company",
      "compute",
      "computing",
      "dataflow",
      "different",
      "distributed",
      "hardware",
      "model",
      "processors"
    ],
    "excerpt": "An AI hardware company producing reconfigurable dataflow architecture processors (SN series) that adapt their compute topology to different model architectures.",
    "url": "pages/glossary.html#term-sambanova"
  },
  {
    "id": "term-sample-complexity",
    "title": "Sample Complexity",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "achieve",
      "algorithm",
      "complexity",
      "examples",
      "high",
      "learning",
      "level",
      "machine",
      "minimum",
      "model",
      "number",
      "performance"
    ],
    "excerpt": "The minimum number of training examples required for a learning algorithm to achieve a specified level of performance with high probability.",
    "url": "pages/glossary.html#term-sample-complexity"
  },
  {
    "id": "term-sample-complexity-rl",
    "title": "Sample Complexity",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithm",
      "complexity",
      "concepts",
      "core",
      "environment",
      "find",
      "high",
      "interactions",
      "learning",
      "nearoptimal",
      "needed",
      "number"
    ],
    "excerpt": "The number of environment interactions needed for an RL algorithm to find a near-optimal policy with high probability.",
    "url": "pages/glossary.html#term-sample-complexity-rl"
  },
  {
    "id": "term-sampling",
    "title": "Sampling",
    "category": "Glossary",
    "subcategory": "Generation",
    "keywords": [
      "generation",
      "next",
      "process",
      "sampling",
      "selecting",
      "technical",
      "text",
      "token"
    ],
    "excerpt": "The process of selecting the next token during text generation. Methods include greedy (always pick highest probability), top-k, top-p (nucleus), and temperature-based sampling.",
    "url": "pages/glossary.html#term-sampling"
  },
  {
    "id": "term-sandwich-prompting",
    "title": "Sandwich Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "adherence",
      "attention",
      "biases",
      "context",
      "contexts",
      "core",
      "data",
      "due",
      "engineering",
      "input",
      "instruction",
      "instructions"
    ],
    "excerpt": "A prompt structure that places the core instruction both before and after the main context or input data, reinforcing adherence to instructions that might otherwise be lost in long contexts due to positional attention biases.",
    "url": "pages/glossary.html#term-sandwich-prompting"
  },
  {
    "id": "term-sarcasm-detection",
    "title": "Sarcasm Detection",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "challenging",
      "detection",
      "differs",
      "identifying",
      "intended",
      "ironic",
      "literal",
      "meaning",
      "nlp",
      "pragmatic",
      "problem",
      "processing"
    ],
    "excerpt": "The task of identifying sarcastic or ironic statements in text where the intended meaning differs from the literal meaning, a challenging problem requiring pragmatic understanding.",
    "url": "pages/glossary.html#term-sarcasm-detection"
  },
  {
    "id": "term-sarima",
    "title": "SARIMA",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "additional",
      "arima",
      "autoregressive",
      "average",
      "capture",
      "data",
      "differencing",
      "extension",
      "includes",
      "model",
      "moving",
      "patterns"
    ],
    "excerpt": "Seasonal ARIMA, an extension of the ARIMA model that includes additional seasonal autoregressive, differencing, and moving average terms to capture periodic patterns in time series data.",
    "url": "pages/glossary.html#term-sarima"
  },
  {
    "id": "term-sarsa",
    "title": "SARSA",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "action",
      "actual",
      "algorithm",
      "control",
      "current",
      "difference",
      "learning",
      "methods",
      "onpolicy",
      "policy",
      "qvalues",
      "reinforcement"
    ],
    "excerpt": "An on-policy temporal difference control algorithm that updates Q-values using the actual action taken by the current policy (State-Action-Reward-State-Action).",
    "url": "pages/glossary.html#term-sarsa"
  },
  {
    "id": "term-satellite-imagery-analysis",
    "title": "Satellite Imagery Analysis",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "aerial",
      "analysis",
      "applications",
      "assessment",
      "change",
      "classification",
      "computer",
      "detection",
      "disaster",
      "environmental",
      "image",
      "imagery"
    ],
    "excerpt": "The use of computer vision models to interpret aerial and satellite photographs for applications including land use classification, change detection, disaster assessment, and environmental monitoring.",
    "url": "pages/glossary.html#term-satellite-imagery-analysis"
  },
  {
    "id": "term-scalable-oversight",
    "title": "Scalable Oversight",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "agenda",
      "ai",
      "alignment",
      "become",
      "capable",
      "challenge",
      "complex",
      "directly",
      "evaluate",
      "handle",
      "human",
      "humans"
    ],
    "excerpt": "The challenge and research agenda of maintaining meaningful human oversight of AI systems as they become more capable and handle tasks too complex for humans to directly evaluate.",
    "url": "pages/glossary.html#term-scalable-oversight"
  },
  {
    "id": "term-scalar-quantization",
    "title": "Scalar Quantization",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "8bit",
      "accuracy",
      "achieving",
      "component",
      "compression",
      "converting",
      "database",
      "float32",
      "floatingpoint",
      "integers",
      "loss",
      "lowerprecision"
    ],
    "excerpt": "A vector compression method that reduces memory usage by converting each floating-point vector component to a lower-precision representation such as 8-bit integers, achieving 4x compression from float32 with minimal accuracy loss.",
    "url": "pages/glossary.html#term-scalar-quantization"
  },
  {
    "id": "term-scale-ai",
    "title": "Scale AI",
    "category": "Glossary",
    "subcategory": "Company",
    "keywords": [
      "ai",
      "annotation",
      "company",
      "data",
      "labeling",
      "scale",
      "services",
      "specializing",
      "training"
    ],
    "excerpt": "A company specializing in data labeling and annotation services for AI training. Provides human feedback at scale, crucial for training and evaluating foundation models.",
    "url": "pages/glossary.html#term-scale-ai"
  },
  {
    "id": "term-scaling-hypothesis",
    "title": "Scaling Hypothesis",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "capability",
      "compute",
      "data",
      "discovered",
      "empirical",
      "history",
      "hypothesis",
      "improvements",
      "increasing",
      "kaplan",
      "laws",
      "leads"
    ],
    "excerpt": "The hypothesis that increasing model size, training data, and compute leads to predictable and substantial improvements in AI capability, supported by empirical scaling laws discovered by Kaplan et al.",
    "url": "pages/glossary.html#term-scaling-hypothesis"
  },
  {
    "id": "term-scaling-law",
    "title": "Scaling Law",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "allocation",
      "budget",
      "compute",
      "dataset",
      "efficient",
      "empirical",
      "function",
      "generative",
      "guiding",
      "improves",
      "law"
    ],
    "excerpt": "Empirical power-law relationships that predict how model performance improves as a function of compute budget, model size, and dataset size, guiding efficient resource allocation for training.",
    "url": "pages/glossary.html#term-scaling-law"
  },
  {
    "id": "term-scaling-laws",
    "title": "Scaling Laws",
    "category": "Glossary",
    "subcategory": "Research",
    "keywords": [
      "compute",
      "data",
      "empirical",
      "improves",
      "laws",
      "model",
      "parameters",
      "performance",
      "relationships",
      "research",
      "scaling",
      "showing"
    ],
    "excerpt": "Empirical relationships showing how model performance improves with more parameters, data, and compute. Guide decisions about where to invest resources in training larger models.",
    "url": "pages/glossary.html#term-scaling-laws"
  },
  {
    "id": "term-scaling-laws-compute",
    "title": "Scaling Laws for Compute",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "budget",
      "compute",
      "computing",
      "dataset",
      "distributed",
      "empirical",
      "established",
      "for",
      "kaplan",
      "laws",
      "model",
      "optimization"
    ],
    "excerpt": "Empirical power-law relationships between model performance and compute budget, model size, and dataset size established by Kaplan et al.",
    "url": "pages/glossary.html#term-scaling-laws-compute"
  },
  {
    "id": "term-scene-graph-generation",
    "title": "Scene Graph Generation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "computer",
      "construct",
      "detecting",
      "edges",
      "generation",
      "graph",
      "image",
      "nodes",
      "objects",
      "pairwise",
      "predicting",
      "processing"
    ],
    "excerpt": "The task of detecting objects in an image and predicting their pairwise relationships to construct a graph representation where nodes are objects and edges are visual relationships.",
    "url": "pages/glossary.html#term-scene-graph-generation"
  },
  {
    "id": "term-scene-text-recognition",
    "title": "Scene Text Recognition",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "building",
      "computer",
      "dealing",
      "distortion",
      "diverse",
      "facades",
      "found",
      "image",
      "images",
      "labels",
      "natural",
      "occlusion"
    ],
    "excerpt": "The task of reading and transcribing text found in natural images (street signs, product labels, building facades), dealing with perspective distortion, partial occlusion, and diverse typography.",
    "url": "pages/glossary.html#term-scene-text-recognition"
  },
  {
    "id": "term-scikit-learn",
    "title": "Scikit-learn",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "algorithms",
      "framework",
      "learn",
      "learning",
      "library",
      "machine",
      "ml",
      "popular",
      "python",
      "scikit",
      "traditional"
    ],
    "excerpt": "A popular Python library for traditional machine learning algorithms. Provides simple APIs for classification, regression, clustering, and preprocessing. The go-to for non-deep-learning ML.",
    "url": "pages/glossary.html#term-scikit-learn"
  },
  {
    "id": "term-score-function",
    "title": "Score Function",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "calculations",
      "estimation",
      "fisher",
      "function",
      "gradient",
      "inference",
      "information",
      "likelihood",
      "loglikelihood",
      "maximum",
      "parameter",
      "respect"
    ],
    "excerpt": "The gradient of the log-likelihood function with respect to the parameter, used in maximum likelihood estimation and Fisher information calculations.",
    "url": "pages/glossary.html#term-score-function"
  },
  {
    "id": "term-score-based-generative-model",
    "title": "Score-Based Generative Model",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "based",
      "data",
      "density",
      "distribution",
      "dynamics",
      "function",
      "generate",
      "generative",
      "gradient",
      "langevin",
      "learns"
    ],
    "excerpt": "A generative model that learns the gradient of the log probability density (score function) of the data distribution, then uses Langevin dynamics or similar methods to generate samples.",
    "url": "pages/glossary.html#term-score-based-generative-model"
  },
  {
    "id": "term-sdxl",
    "title": "SDXL",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "detailed",
      "diffusion",
      "dual",
      "encoders",
      "generate",
      "generative",
      "higherresolution",
      "image",
      "images",
      "improved",
      "larger"
    ],
    "excerpt": "Stable Diffusion XL, an improved latent diffusion model that uses a larger UNet with dual text encoders and an optional refiner model to generate higher-resolution, more detailed images.",
    "url": "pages/glossary.html#term-sdxl"
  },
  {
    "id": "term-seasonal-decomposition",
    "title": "Seasonal Decomposition",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "additively",
      "analysis",
      "better",
      "components",
      "data",
      "decomposition",
      "either",
      "multiplicatively",
      "patterns",
      "residual",
      "science",
      "seasonal"
    ],
    "excerpt": "A time series analysis technique that separates a time series into trend, seasonal, and residual components, either additively or multiplicatively, to better understand underlying patterns.",
    "url": "pages/glossary.html#term-seasonal-decomposition"
  },
  {
    "id": "term-second-ai-winter",
    "title": "Second AI Winter",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1987",
      "1993",
      "ai",
      "approximately",
      "collapse",
      "collapsed",
      "cuts",
      "enthusiasm",
      "expert",
      "failure",
      "following",
      "funding"
    ],
    "excerpt": "The period from approximately 1987 to 1993 when enthusiasm for AI again collapsed following the failure of expert systems to scale, the collapse of the LISP machine market, and cuts to government AI funding.",
    "url": "pages/glossary.html#term-second-ai-winter"
  },
  {
    "id": "term-secure-multi-party-computation",
    "title": "Secure Multi-Party Computation",
    "category": "Glossary",
    "subcategory": "Privacy",
    "keywords": [
      "ai",
      "allows",
      "applications",
      "combined",
      "computation",
      "compute",
      "cryptographic",
      "ethics",
      "function",
      "input",
      "inputs",
      "jointly"
    ],
    "excerpt": "A cryptographic protocol that allows multiple parties to jointly compute a function over their combined inputs while keeping each party's input private, used in privacy-preserving machine learning applications.",
    "url": "pages/glossary.html#term-secure-multi-party-computation"
  },
  {
    "id": "term-segment-anything-model",
    "title": "Segment Anything Model",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "anything",
      "bounding",
      "box",
      "computer",
      "dataset",
      "description",
      "foundation",
      "given",
      "image",
      "massive",
      "model",
      "object"
    ],
    "excerpt": "A foundation model for image segmentation (SAM) trained on a massive dataset that can segment any object in any image given a prompt such as a point, bounding box, or text description.",
    "url": "pages/glossary.html#term-segment-anything-model"
  },
  {
    "id": "term-selection-bias",
    "title": "Selection Bias",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "analysis",
      "arising",
      "bias",
      "data",
      "due",
      "error",
      "observations",
      "population",
      "representative",
      "sample",
      "science",
      "selected"
    ],
    "excerpt": "A systematic error arising when the sample used for analysis is not representative of the population, due to the way observations were selected.",
    "url": "pages/glossary.html#term-selection-bias"
  },
  {
    "id": "term-selectional-preference",
    "title": "Selectional Preference",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "arguments",
      "assessment",
      "constrain",
      "disambiguation",
      "eat",
      "edible",
      "linguistics",
      "nlp",
      "objects",
      "plausibility",
      "predicates",
      "preference"
    ],
    "excerpt": "The tendency of predicates to semantically constrain their arguments, such as 'eat' preferring edible objects, used in NLP for disambiguation and semantic plausibility assessment.",
    "url": "pages/glossary.html#term-selectional-preference"
  },
  {
    "id": "term-selective-context",
    "title": "Selective Context",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "causal",
      "compression",
      "content",
      "context",
      "details",
      "engineering",
      "evaluates",
      "filters",
      "informativeness",
      "key",
      "language",
      "length"
    ],
    "excerpt": "A prompt compression method that evaluates the informativeness of each lexical unit in a context using self-information scores from a causal language model, then filters out low-information content to reduce prompt length while retaining key details.",
    "url": "pages/glossary.html#term-selective-context"
  },
  {
    "id": "term-self-ask",
    "title": "Self-Ask",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "answer",
      "answers",
      "ask",
      "asks",
      "complex",
      "decomposing",
      "engineering",
      "explicitly",
      "final",
      "followup",
      "itself",
      "model"
    ],
    "excerpt": "A prompting technique where the model explicitly asks itself follow-up questions needed to answer a complex query, then answers each sub-question before synthesizing the final response, naturally decomposing multi-hop reasoning tasks.",
    "url": "pages/glossary.html#term-self-ask"
  },
  {
    "id": "term-self-attention",
    "title": "Self-Attention",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "allowing",
      "architecture",
      "attends",
      "attention",
      "consider",
      "itself",
      "mechanism",
      "position",
      "positions",
      "self",
      "sequence",
      "transformers"
    ],
    "excerpt": "An attention mechanism where a sequence attends to itself, allowing each position to consider all other positions. The core operation in transformer models.",
    "url": "pages/glossary.html#term-self-attention"
  },
  {
    "id": "term-self-bleu",
    "title": "Self-BLEU",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "across",
      "bleu",
      "computes",
      "diversity",
      "evaluation",
      "generated",
      "greater",
      "indicates",
      "less",
      "lower",
      "metric",
      "metrics"
    ],
    "excerpt": "A diversity metric that computes BLEU scores between pairs of generated sentences from the same model, where lower Self-BLEU indicates greater diversity and less repetition across the model's outputs.",
    "url": "pages/glossary.html#term-self-bleu"
  },
  {
    "id": "term-self-consistency",
    "title": "Self-Consistency",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "answer",
      "common",
      "consistency",
      "generates",
      "multiple",
      "paths",
      "prompting",
      "reasoning",
      "selects",
      "self",
      "technique"
    ],
    "excerpt": "A technique where AI generates multiple reasoning paths and selects the most common answer. Improves accuracy for complex reasoning by aggregating diverse approaches.",
    "url": "pages/glossary.html#term-self-consistency"
  },
  {
    "id": "term-self-driving-car-history",
    "title": "Self-Driving Car History",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1961",
      "1986",
      "2009",
      "autonomous",
      "car",
      "cart",
      "challenges",
      "cmus",
      "darpa",
      "driving",
      "early",
      "evolution"
    ],
    "excerpt": "The evolution of autonomous vehicles from early projects like Stanford Cart in 1961 and CMU's Navlab in 1986 through the DARPA Grand Challenges, Google's self-driving car project in 2009, and modern robotaxi services.",
    "url": "pages/glossary.html#term-self-driving-car-history"
  },
  {
    "id": "term-self-play",
    "title": "Self-Play",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "against",
      "agent",
      "challenging",
      "copies",
      "generating",
      "grows",
      "improves",
      "increasingly",
      "itself",
      "learning",
      "opponents",
      "paradigm"
    ],
    "excerpt": "A training paradigm where an agent improves by playing against copies of itself, generating increasingly challenging opponents as its skill grows.",
    "url": "pages/glossary.html#term-self-play"
  },
  {
    "id": "term-self-polish-prompting",
    "title": "Self-Polish Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "complex",
      "conditions",
      "enabling",
      "engineering",
      "formulations",
      "given",
      "iterative",
      "model",
      "polish",
      "problem",
      "problems",
      "progressively"
    ],
    "excerpt": "A technique that prompts the model to progressively refine and polish the given problem conditions before solving, enabling the model to simplify complex problems into more tractable formulations through iterative rewriting.",
    "url": "pages/glossary.html#term-self-polish-prompting"
  },
  {
    "id": "term-self-rag",
    "title": "Self-RAG",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "adaptively",
      "ai",
      "critique",
      "deciding",
      "demand",
      "framework",
      "generate",
      "generative",
      "language",
      "learns",
      "llm",
      "model"
    ],
    "excerpt": "A framework where the language model learns to retrieve on demand, generate text, and critique its own output using special reflection tokens, adaptively deciding when retrieval is necessary.",
    "url": "pages/glossary.html#term-self-rag"
  },
  {
    "id": "term-self-training",
    "title": "Self-Training",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "data",
      "examples",
      "expanding",
      "generates",
      "iteratively",
      "labeled",
      "learning",
      "machine",
      "method",
      "model",
      "pseudolabeled",
      "pseudolabels"
    ],
    "excerpt": "A semi-supervised learning method where a model trained on labeled data generates pseudo-labels for unlabeled data, then retrains on both real and pseudo-labeled examples, iteratively expanding the training set.",
    "url": "pages/glossary.html#term-self-training"
  },
  {
    "id": "term-self-training-vision",
    "title": "Self-Training for Vision",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "approach",
      "combination",
      "computer",
      "data",
      "for",
      "generates",
      "highconfidence",
      "image",
      "images",
      "labeled",
      "learning",
      "model"
    ],
    "excerpt": "A semi-supervised learning approach where a teacher model generates pseudo-labels for unlabeled images, and a student model is trained on the combination of labeled data and high-confidence pseudo-labels.",
    "url": "pages/glossary.html#term-self-training-vision"
  },
  {
    "id": "term-semantic-caching",
    "title": "Semantic Caching",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "allowing",
      "cache",
      "caching",
      "exact",
      "hits",
      "indexed",
      "inference",
      "llm",
      "matches",
      "meaning",
      "paraphrased",
      "queries"
    ],
    "excerpt": "A caching strategy that stores LLM responses indexed by the semantic meaning of queries rather than exact string matches, allowing cache hits for paraphrased or similar questions.",
    "url": "pages/glossary.html#term-semantic-caching"
  },
  {
    "id": "term-semantic-chunking",
    "title": "Semantic Chunking",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "approach",
      "based",
      "boundaries",
      "character",
      "chunk",
      "chunking",
      "chunks",
      "consecutive",
      "counts",
      "creating",
      "determines",
      "document"
    ],
    "excerpt": "A document splitting approach that determines chunk boundaries based on semantic similarity between consecutive sentences, creating new chunks when the topic or meaning shifts significantly rather than using fixed-size character or token counts.",
    "url": "pages/glossary.html#term-semantic-chunking"
  },
  {
    "id": "term-semantic-correspondence",
    "title": "Semantic Correspondence",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "appearance",
      "category",
      "computer",
      "correspondence",
      "different",
      "finding",
      "image",
      "images",
      "instances",
      "just",
      "matching",
      "object"
    ],
    "excerpt": "The task of finding matching points between images of different instances of the same object category, requiring understanding of semantic similarity rather than just visual appearance matching.",
    "url": "pages/glossary.html#term-semantic-correspondence"
  },
  {
    "id": "term-semantic-kernel",
    "title": "Semantic Kernel",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "application",
      "applications",
      "building",
      "framework",
      "kernel",
      "llms",
      "microsofts",
      "opensource",
      "sdk",
      "semantic"
    ],
    "excerpt": "Microsoft's open-source SDK for building AI applications with LLMs. Supports plugin architecture, memory, and planning for enterprise AI agent development.",
    "url": "pages/glossary.html#term-semantic-kernel"
  },
  {
    "id": "term-semantic-map",
    "title": "Semantic Map",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "annotates",
      "autonomous",
      "computer",
      "driving",
      "image",
      "indicating",
      "labels",
      "location",
      "map",
      "navigation",
      "object",
      "occupies"
    ],
    "excerpt": "A spatial representation that annotates each location with semantic labels indicating what type of object or surface occupies that space, used in robotics navigation and autonomous driving planning.",
    "url": "pages/glossary.html#term-semantic-map"
  },
  {
    "id": "term-semantic-networks",
    "title": "Semantic Networks",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1968",
      "concepts",
      "connected",
      "early",
      "edges",
      "first",
      "formalism",
      "graphs",
      "history",
      "knowledge",
      "labeled",
      "language"
    ],
    "excerpt": "A knowledge representation formalism using graphs of nodes connected by labeled edges to represent concepts and their relationships, first proposed by Ross Quillian in 1968 and widely used in early AI and natural language understanding.",
    "url": "pages/glossary.html#term-semantic-networks"
  },
  {
    "id": "term-semantic-parsing",
    "title": "Semantic Parsing",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "calculus",
      "executed",
      "expressions",
      "formal",
      "forms",
      "lambda",
      "language",
      "logical",
      "mapping",
      "meaning",
      "natural",
      "nlp"
    ],
    "excerpt": "The task of mapping natural language utterances to formal meaning representations such as logical forms, SQL queries, or lambda calculus expressions that can be executed or reasoned over.",
    "url": "pages/glossary.html#term-semantic-parsing"
  },
  {
    "id": "term-semantic-role-labeling",
    "title": "Semantic Role Labeling",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "agent",
      "assigning",
      "constituents",
      "identifying",
      "instrument",
      "labeling",
      "linguistics",
      "location",
      "nlp",
      "patient",
      "predicate",
      "predicateargument"
    ],
    "excerpt": "The task of identifying the predicate-argument structure of a sentence by assigning semantic roles such as agent, patient, instrument, and location to constituents relative to the predicate.",
    "url": "pages/glossary.html#term-semantic-role-labeling"
  },
  {
    "id": "term-semantic-search",
    "title": "Semantic Search",
    "category": "Glossary",
    "subcategory": "Application",
    "keywords": [
      "application",
      "based",
      "keyword",
      "matching",
      "meaning",
      "rather",
      "search",
      "semantic"
    ],
    "excerpt": "Search based on meaning rather than keyword matching. Uses embeddings to find conceptually similar content, enabling more relevant results for natural language queries.",
    "url": "pages/glossary.html#term-semantic-search"
  },
  {
    "id": "term-semantic-segmentation",
    "title": "Semantic Segmentation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "assigns",
      "belongs",
      "category",
      "class",
      "computer",
      "dense",
      "distinguishing",
      "identifies",
      "image",
      "instances",
      "label",
      "map"
    ],
    "excerpt": "A computer vision task that assigns a class label to every pixel in an image, producing a dense prediction map that identifies what object category each pixel belongs to without distinguishing instances.",
    "url": "pages/glossary.html#term-semantic-segmentation"
  },
  {
    "id": "term-semantic-segmentation-transformer",
    "title": "Semantic Segmentation Transformer",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "apply",
      "architectures",
      "benchmarks",
      "classification",
      "cnnbased",
      "computer",
      "dense",
      "features",
      "image",
      "like",
      "many",
      "mask2former"
    ],
    "excerpt": "Transformer-based architectures like SegFormer and Mask2Former that apply self-attention to image features for dense per-pixel classification, outperforming CNN-based segmenters on many benchmarks.",
    "url": "pages/glossary.html#term-semantic-segmentation-transformer"
  },
  {
    "id": "term-semantic-textual-similarity",
    "title": "Semantic Textual Similarity",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "beyond",
      "binary",
      "capture",
      "continuous",
      "degree",
      "detection",
      "equivalence",
      "going",
      "graded",
      "measuring",
      "nlp",
      "paraphrase"
    ],
    "excerpt": "The task of measuring the degree of semantic equivalence between two text segments on a continuous scale, going beyond binary paraphrase detection to capture graded similarity.",
    "url": "pages/glossary.html#term-semantic-textual-similarity"
  },
  {
    "id": "term-semantics",
    "title": "Semantics",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "branch",
      "expressions",
      "including",
      "language",
      "linguistic",
      "linguistics",
      "meaning",
      "nlp",
      "refer",
      "relationship",
      "semantics",
      "sentence"
    ],
    "excerpt": "The branch of linguistics studying meaning in language, including word meaning, sentence meaning, and the relationship between linguistic expressions and what they refer to.",
    "url": "pages/glossary.html#term-semantics"
  },
  {
    "id": "term-semi-supervised-learning",
    "title": "Semi-Supervised Learning",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "amount",
      "combines",
      "data",
      "labeled",
      "large",
      "learning",
      "machine",
      "model",
      "paradigm",
      "selection",
      "semi",
      "small"
    ],
    "excerpt": "A learning paradigm that combines a small amount of labeled data with a large amount of unlabeled data during training.",
    "url": "pages/glossary.html#term-semi-supervised-learning"
  },
  {
    "id": "term-semi-supervised-object-detection",
    "title": "Semi-Supervised Object Detection",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "approaches",
      "computer",
      "consistency",
      "detection",
      "images",
      "labeled",
      "large",
      "leverage",
      "like",
      "object",
      "pool",
      "pseudolabeling"
    ],
    "excerpt": "Detection training approaches that leverage both a small set of labeled images and a large pool of unlabeled images, using techniques like pseudo-labeling and consistency regularization.",
    "url": "pages/glossary.html#term-semi-supervised-object-detection"
  },
  {
    "id": "term-sensitivity",
    "title": "Sensitivity",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "actual",
      "cases",
      "classifier",
      "correctly",
      "identified",
      "learning",
      "machine",
      "metrics",
      "positive",
      "proportion",
      "rate",
      "recall"
    ],
    "excerpt": "The proportion of actual positive cases correctly identified by a classifier, synonymous with recall and true positive rate. High sensitivity minimizes false negatives in the predictions.",
    "url": "pages/glossary.html#term-sensitivity"
  },
  {
    "id": "term-sentence-boundary-detection",
    "title": "Sentence Boundary Detection",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "abbreviations",
      "ambiguous",
      "begin",
      "boundary",
      "decimal",
      "detection",
      "ellipses",
      "end",
      "handling",
      "identifying",
      "like",
      "nlp"
    ],
    "excerpt": "The task of identifying where sentences begin and end in running text, handling ambiguous punctuation like periods in abbreviations, decimal numbers, and ellipses.",
    "url": "pages/glossary.html#term-sentence-boundary-detection"
  },
  {
    "id": "term-sentence-embedding",
    "title": "Sentence Embedding",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "captures",
      "dedicated",
      "dense",
      "embedding",
      "embeddings",
      "encoders",
      "entire",
      "fixedlength",
      "like",
      "mean",
      "meaning",
      "methods"
    ],
    "excerpt": "A fixed-length dense vector representation of an entire sentence that captures its semantic meaning, produced by methods like mean pooling over token embeddings or dedicated sentence encoders.",
    "url": "pages/glossary.html#term-sentence-embedding"
  },
  {
    "id": "term-sentence-transformer",
    "title": "Sentence Transformer",
    "category": "Glossary",
    "subcategory": "Model Type",
    "keywords": [
      "capturing",
      "embeddings",
      "encode",
      "entire",
      "meaning",
      "model",
      "models",
      "semantic",
      "sentence",
      "sentences",
      "single",
      "transformer"
    ],
    "excerpt": "Models that encode entire sentences into single vectors, capturing semantic meaning. Popular for semantic search, similarity matching, and clustering text at the sentence level.",
    "url": "pages/glossary.html#term-sentence-transformer"
  },
  {
    "id": "term-sentencepiece",
    "title": "SentencePiece",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "applies",
      "bpe",
      "byte",
      "directly",
      "input",
      "languageindependent",
      "languagespecific",
      "library",
      "nlp",
      "preprocessing",
      "pretokenization",
      "raw"
    ],
    "excerpt": "A language-independent tokenization library that treats the input as a raw byte stream and applies BPE or unigram tokenization directly without pre-tokenization or language-specific preprocessing.",
    "url": "pages/glossary.html#term-sentencepiece"
  },
  {
    "id": "term-sentiment-analysis",
    "title": "Sentiment Analysis",
    "category": "Glossary",
    "subcategory": "NLP Task",
    "keywords": [
      "analysis",
      "classification",
      "determines",
      "emotional",
      "negative",
      "neutral",
      "nlp",
      "positive",
      "sentiment",
      "task",
      "text",
      "tone"
    ],
    "excerpt": "An NLP task that determines the emotional tone of text (positive, negative, neutral). Used in customer feedback analysis, social media monitoring, and brand tracking.",
    "url": "pages/glossary.html#term-sentiment-analysis"
  },
  {
    "id": "term-sentiment-polarity",
    "title": "Sentiment Polarity",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "categories",
      "classification",
      "emotional",
      "expressed",
      "negative",
      "neutral",
      "nlp",
      "orientation",
      "overall",
      "polarity",
      "positive",
      "processing"
    ],
    "excerpt": "The classification of text sentiment into categories such as positive, negative, or neutral, representing the overall emotional orientation expressed toward the subject of the text.",
    "url": "pages/glossary.html#term-sentiment-polarity"
  },
  {
    "id": "term-sepp-hochreiter",
    "title": "Sepp Hochreiter",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1997",
      "architecture",
      "austrian",
      "coinvented",
      "computer",
      "fundamental",
      "history",
      "hochreiter",
      "jurgen",
      "long",
      "memory",
      "network"
    ],
    "excerpt": "Austrian computer scientist who co-invented the Long Short-Term Memory network architecture in 1997 with Jurgen Schmidhuber, solving a fundamental problem in training recurrent neural networks on long sequences.",
    "url": "pages/glossary.html#term-sepp-hochreiter"
  },
  {
    "id": "term-seq2seq",
    "title": "Seq2Seq",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "decoder",
      "encoder",
      "encoderdecoder",
      "fixedlength",
      "framework",
      "generates",
      "input",
      "networks",
      "neural",
      "output",
      "processes"
    ],
    "excerpt": "Sequence-to-Sequence, an encoder-decoder framework where an encoder processes an input sequence into a fixed-length representation and a decoder generates an output sequence from that representation.",
    "url": "pages/glossary.html#term-seq2seq"
  },
  {
    "id": "term-sequence-labeling",
    "title": "Sequence Labeling",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "assigning",
      "boundary",
      "categorical",
      "chunk",
      "element",
      "entity",
      "label",
      "labeling",
      "named",
      "nlp",
      "pos",
      "processing"
    ],
    "excerpt": "The task of assigning a categorical label to each element in a sequence, such as tagging each word in a sentence with its named entity type, POS tag, or chunk boundary.",
    "url": "pages/glossary.html#term-sequence-labeling"
  },
  {
    "id": "term-sequence-parallelism",
    "title": "Sequence Parallelism",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "across",
      "context",
      "devices",
      "dimension",
      "distributes",
      "enabling",
      "exceed",
      "inference",
      "lengths",
      "limits",
      "llm",
      "long"
    ],
    "excerpt": "A technique that distributes the processing of long sequences across multiple devices by partitioning the sequence dimension, enabling context lengths that exceed single-device memory limits.",
    "url": "pages/glossary.html#term-sequence-parallelism"
  },
  {
    "id": "term-sequence-level-accuracy",
    "title": "Sequence-Level Accuracy",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "accuracy",
      "assessment",
      "code",
      "considers",
      "correct",
      "entire",
      "evaluation",
      "generated",
      "generation",
      "holistic",
      "level",
      "like"
    ],
    "excerpt": "An evaluation metric that considers an entire generated sequence correct only if every token matches the reference, providing a strict holistic assessment used in tasks like structured prediction and code generation.",
    "url": "pages/glossary.html#term-sequence-level-accuracy"
  },
  {
    "id": "term-sft",
    "title": "SFT (Supervised Fine-Tuning)",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "alignment",
      "behavior",
      "desired",
      "examples",
      "fine",
      "labeled",
      "model",
      "pretrained",
      "sft",
      "supervised",
      "training",
      "tuning"
    ],
    "excerpt": "Training a pre-trained model on labeled examples of desired behavior. Often the first step in aligning LLMs, teaching them to follow instructions before RLHF.",
    "url": "pages/glossary.html#term-sft"
  },
  {
    "id": "term-shakey-the-robot",
    "title": "Shakey the Robot",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1966",
      "1972",
      "actions",
      "computer",
      "developed",
      "first",
      "generalpurpose",
      "history",
      "integrating",
      "international",
      "language",
      "milestones"
    ],
    "excerpt": "The first general-purpose mobile robot developed at SRI International from 1966 to 1972 that could reason about its own actions, integrating computer vision, natural language understanding, and planning.",
    "url": "pages/glossary.html#term-shakey-the-robot"
  },
  {
    "id": "term-shap-values",
    "title": "SHAP Values",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "additive",
      "assigns",
      "based",
      "contribution",
      "cooperative",
      "desirable",
      "engineering",
      "explanations",
      "feature",
      "framework",
      "game",
      "importance"
    ],
    "excerpt": "SHapley Additive exPlanations, a unified framework for feature importance that assigns each feature a contribution value for a specific prediction based on Shapley values from cooperative game theory, satisfying desirable theoretical properties.",
    "url": "pages/glossary.html#term-shap-values"
  },
  {
    "id": "term-shaped-reward",
    "title": "Shaped Reward",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "added",
      "auxiliary",
      "behavior",
      "design",
      "desired",
      "environments",
      "guide",
      "learning",
      "natural",
      "reinforcement",
      "reward",
      "shaped"
    ],
    "excerpt": "An auxiliary reward signal added to the environment's natural reward to guide learning toward desired behavior.",
    "url": "pages/glossary.html#term-shaped-reward"
  },
  {
    "id": "term-shapiro-wilk-test",
    "title": "Shapiro-Wilk Test",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "comes",
      "distribution",
      "evaluates",
      "inference",
      "normal",
      "normality",
      "random",
      "sample",
      "shapiro",
      "statistical",
      "statistics",
      "test"
    ],
    "excerpt": "A statistical test for normality that evaluates whether a random sample comes from a normal distribution. It is considered one of the most powerful normality tests, especially for small sample sizes.",
    "url": "pages/glossary.html#term-shapiro-wilk-test"
  },
  {
    "id": "term-shared-memory-gpu",
    "title": "Shared Memory (GPU)",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "block",
      "data",
      "efficient",
      "enables",
      "fast",
      "gpu",
      "hardware",
      "memory",
      "multiprocessors",
      "onchip",
      "programmermanaged",
      "shared"
    ],
    "excerpt": "A fast, programmer-managed on-chip SRAM in GPU streaming multiprocessors that enables efficient data sharing between threads in a thread block.",
    "url": "pages/glossary.html#term-shared-memory-gpu"
  },
  {
    "id": "term-shrdlu",
    "title": "SHRDLU",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1970",
      "blocks",
      "capabilities",
      "converse",
      "created",
      "demonstrating",
      "early",
      "history",
      "language",
      "manipulate",
      "milestones",
      "mit"
    ],
    "excerpt": "A natural language understanding program created by Terry Winograd at MIT in 1970 that could converse about and manipulate objects in a simulated blocks world, demonstrating early natural language processing capabilities.",
    "url": "pages/glossary.html#term-shrdlu"
  },
  {
    "id": "term-sift",
    "title": "SIFT",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "algorithm",
      "baseline",
      "changes",
      "classical",
      "computer",
      "describes",
      "detects",
      "feature",
      "features",
      "illumination",
      "image",
      "local"
    ],
    "excerpt": "Scale-Invariant Feature Transform, a classical algorithm that detects and describes local image features robust to scale, rotation, and illumination changes, widely used as a baseline for feature matching.",
    "url": "pages/glossary.html#term-sift"
  },
  {
    "id": "term-signed-distance-function",
    "title": "Signed Distance Function",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "computer",
      "defining",
      "distance",
      "enabling",
      "extraction",
      "function",
      "nearest",
      "network",
      "neural",
      "point",
      "predicts"
    ],
    "excerpt": "A 3D representation where a neural network predicts the signed distance from any 3D point to the nearest surface, with the zero-level set defining the shape, enabling smooth surface extraction.",
    "url": "pages/glossary.html#term-signed-distance-function"
  },
  {
    "id": "term-significance-level",
    "title": "Significance Level",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "alpha",
      "inference",
      "level",
      "probability",
      "significance",
      "statistics",
      "threshold",
      "typically"
    ],
    "excerpt": "The threshold probability (alpha, typically 0.05) below which the p-value must fall for the null hypothesis to be rejected. It defines the acceptable risk of making a Type I error.",
    "url": "pages/glossary.html#term-significance-level"
  },
  {
    "id": "term-silhouette-score",
    "title": "Silhouette Score",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "cluster",
      "clustering",
      "clusters",
      "compared",
      "evaluating",
      "learning",
      "machine",
      "measures",
      "metric",
      "metrics",
      "own",
      "point"
    ],
    "excerpt": "A metric for evaluating clustering quality that measures how similar each point is to its own cluster compared to other clusters.",
    "url": "pages/glossary.html#term-silhouette-score"
  },
  {
    "id": "term-sim-to-real",
    "title": "Sim-to-Real Transfer",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "challenge",
      "deploying",
      "learning",
      "paradigms",
      "physical",
      "policies",
      "real",
      "reinforcement",
      "set",
      "sim",
      "simulation",
      "systems"
    ],
    "excerpt": "The challenge and set of techniques for deploying policies trained in simulation to physical systems.",
    "url": "pages/glossary.html#term-sim-to-real"
  },
  {
    "id": "term-simclr",
    "title": "SimCLR",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "augmentation",
      "computer",
      "contrastive",
      "create",
      "data",
      "features",
      "framework",
      "head",
      "image",
      "learn",
      "learning",
      "loss"
    ],
    "excerpt": "A contrastive self-supervised learning framework for visual representations that uses data augmentation to create positive pairs and a projection head with NT-Xent loss to learn transferable image features.",
    "url": "pages/glossary.html#term-simclr"
  },
  {
    "id": "term-similarity-threshold",
    "title": "Similarity Threshold",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "configurable",
      "cutoff",
      "database",
      "exceeding",
      "exist",
      "filters",
      "include",
      "index",
      "irrelevant",
      "matches",
      "minimum",
      "only"
    ],
    "excerpt": "A configurable cutoff value that filters vector search results to include only matches exceeding a minimum similarity score, preventing the retrieval of irrelevant results when no sufficiently similar vectors exist in the index.",
    "url": "pages/glossary.html#term-similarity-threshold"
  },
  {
    "id": "term-simpsons-paradox",
    "title": "Simpson&amp;#x27;s Paradox",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "appears",
      "combined",
      "data",
      "different",
      "disappears",
      "groups",
      "paradox",
      "phenomenon",
      "reverses",
      "science",
      "several",
      "simpsonampx27s"
    ],
    "excerpt": "A statistical phenomenon where a trend that appears in several different groups of data disappears or reverses when these groups are combined.",
    "url": "pages/glossary.html#term-simpsons-paradox"
  },
  {
    "id": "term-simtom-prompting",
    "title": "SimToM Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "answers",
      "approach",
      "aware",
      "based",
      "context",
      "engineering",
      "first",
      "full",
      "identifies",
      "information",
      "limited",
      "mind"
    ],
    "excerpt": "A two-step prompting approach for theory-of-mind tasks that first identifies what information a specific person is aware of, then answers the question based solely on that person's limited perspective rather than full omniscient context.",
    "url": "pages/glossary.html#term-simtom-prompting"
  },
  {
    "id": "term-singular-value-decomposition",
    "title": "Singular Value Decomposition",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "contains",
      "decomposes",
      "decomposition",
      "dimensionality",
      "factorization",
      "learning",
      "machine",
      "matrices",
      "matrix",
      "reduction",
      "singular",
      "technique"
    ],
    "excerpt": "A matrix factorization technique that decomposes a matrix into three matrices (U, S, V^T), where S contains singular values.",
    "url": "pages/glossary.html#term-singular-value-decomposition"
  },
  {
    "id": "term-singularity-concept",
    "title": "Singularity Concept",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1993",
      "2005",
      "book",
      "concept",
      "future",
      "growth",
      "his",
      "history",
      "human",
      "hypothesized",
      "intelligence",
      "kurzweil"
    ],
    "excerpt": "The hypothesized future point when AI surpasses human intelligence and triggers runaway technological growth, popularized by Vernor Vinge in 1993 and Ray Kurzweil in his 2005 book The Singularity Is Near.",
    "url": "pages/glossary.html#term-singularity-concept"
  },
  {
    "id": "term-sinusoidal-positional-encoding",
    "title": "Sinusoidal Positional Encoding",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "absolute",
      "allowing",
      "architecture",
      "cosine",
      "different",
      "encode",
      "encoding",
      "fixed",
      "frequencies",
      "functions",
      "learn",
      "linear"
    ],
    "excerpt": "A fixed positional encoding scheme that uses sine and cosine functions of different frequencies to encode absolute token positions, allowing the model to learn relative positions through linear projections.",
    "url": "pages/glossary.html#term-sinusoidal-positional-encoding"
  },
  {
    "id": "term-siri-launch",
    "title": "Siri Launch",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2011",
      "apples",
      "assistant",
      "computing",
      "consumers",
      "conversational",
      "deployed",
      "first",
      "history",
      "introducing",
      "launch",
      "milestones"
    ],
    "excerpt": "Apple's launch of Siri in October 2011 as the first widely deployed virtual assistant on a smartphone, introducing millions of consumers to conversational AI and voice-activated computing.",
    "url": "pages/glossary.html#term-siri-launch"
  },
  {
    "id": "term-skeleton-of-thought",
    "title": "Skeleton-of-Thought",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "answer",
      "asks",
      "concurrent",
      "enabling",
      "endtoend",
      "engineering",
      "expands",
      "first",
      "generate",
      "generation",
      "independent",
      "latency"
    ],
    "excerpt": "A prompting strategy that first asks the model to generate a skeleton outline of the answer, then expands each point in parallel, reducing end-to-end latency by enabling concurrent generation of independent answer segments.",
    "url": "pages/glossary.html#term-skeleton-of-thought"
  },
  {
    "id": "term-skill-discovery",
    "title": "Skill Discovery",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "behavioral",
      "discovery",
      "exploration",
      "information",
      "learn",
      "learning",
      "maximizing",
      "methods",
      "mutual",
      "primitives",
      "reinforcement",
      "reusable"
    ],
    "excerpt": "Unsupervised methods that learn reusable behavioral primitives or skills without task-specific rewards, typically by maximizing mutual information between skills and states visited.",
    "url": "pages/glossary.html#term-skill-discovery"
  },
  {
    "id": "term-skip-connection",
    "title": "Skip Connection",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "adding",
      "architecture",
      "block",
      "bypasses",
      "concatenating",
      "connection",
      "deep",
      "directly",
      "enabling",
      "facilitating",
      "flow",
      "gradient"
    ],
    "excerpt": "A shortcut path that bypasses one or more layers by adding or concatenating the input of a block directly to its output, enabling gradient flow through deep networks and facilitating residual learning.",
    "url": "pages/glossary.html#term-skip-connection"
  },
  {
    "id": "term-skip-gram",
    "title": "Skip-gram",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "capture",
      "center",
      "context",
      "distributional",
      "embeddings",
      "given",
      "gram",
      "learning",
      "nlp",
      "objective",
      "patterns",
      "predicts"
    ],
    "excerpt": "A Word2Vec training objective that predicts surrounding context words given a center word, learning word representations that capture semantic similarity from distributional patterns.",
    "url": "pages/glossary.html#term-skip-gram"
  },
  {
    "id": "term-sliding-window-attention",
    "title": "Sliding Window Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attends",
      "attention",
      "efficient",
      "enabling",
      "fixedsize",
      "limiting",
      "local",
      "long",
      "neighboring",
      "networks",
      "neural"
    ],
    "excerpt": "An attention pattern where each token attends only to a fixed-size local window of neighboring tokens, enabling efficient processing of long sequences by limiting the attention span.",
    "url": "pages/glossary.html#term-sliding-window-attention"
  },
  {
    "id": "term-slot-filling",
    "title": "Slot Filling",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "dates",
      "dialogue",
      "extracting",
      "fill",
      "filling",
      "information",
      "locations",
      "names",
      "nlp",
      "pieces",
      "predefined",
      "processing"
    ],
    "excerpt": "The task of extracting specific pieces of information (slot values) from user utterances in a task-oriented dialogue system, such as extracting dates, locations, or names to fill predefined slots.",
    "url": "pages/glossary.html#term-slot-filling"
  },
  {
    "id": "term-smoothquant",
    "title": "SmoothQuant",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "activations",
      "applying",
      "difficulty",
      "efficient",
      "enabling",
      "factors",
      "inference",
      "infrastructure",
      "int8",
      "llms",
      "migrates",
      "model"
    ],
    "excerpt": "A quantization technique that migrates the quantization difficulty from activations to weights by applying per-channel scaling factors, enabling efficient INT8 quantization of both weights and activations for LLMs.",
    "url": "pages/glossary.html#term-smoothquant"
  },
  {
    "id": "term-smote",
    "title": "SMOTE",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "class",
      "creates",
      "data",
      "examples",
      "existing",
      "feature",
      "interpolating",
      "knearest",
      "learning",
      "machine",
      "method",
      "minority"
    ],
    "excerpt": "Synthetic Minority Over-sampling Technique, an oversampling method that creates synthetic examples of the minority class by interpolating between existing minority samples and their k-nearest neighbors in feature space.",
    "url": "pages/glossary.html#term-smote"
  },
  {
    "id": "term-social-scoring-systems",
    "title": "Social Scoring Systems",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "access",
      "ai",
      "aidriven",
      "assign",
      "based",
      "behavior",
      "characteristics",
      "connections",
      "determine",
      "ethics",
      "individuals",
      "regulation"
    ],
    "excerpt": "AI-driven systems that assign scores to individuals based on their behavior, social connections, or characteristics, used to determine access to services.",
    "url": "pages/glossary.html#term-social-scoring-systems"
  },
  {
    "id": "term-sociotechnical-systems-approach",
    "title": "Sociotechnical Systems Approach",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "addressed",
      "ai",
      "analytical",
      "approach",
      "context",
      "deployment",
      "determine",
      "ethics",
      "factors",
      "framework",
      "governance",
      "inseparable"
    ],
    "excerpt": "An analytical framework that views AI systems as inseparable from their social context, recognizing that technical and social factors jointly determine system outcomes and that both must be addressed for responsible deployment.",
    "url": "pages/glossary.html#term-sociotechnical-systems-approach"
  },
  {
    "id": "term-socratic-prompting",
    "title": "Socratic Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "answers",
      "critical",
      "dialogue",
      "direct",
      "encouraging",
      "engineering",
      "examination",
      "guides",
      "instructions",
      "model",
      "probing",
      "prompt"
    ],
    "excerpt": "A prompting technique that guides the model through a series of probing questions rather than direct instructions, encouraging step-by-step reasoning and self-discovery of answers through structured dialogue and critical examination.",
    "url": "pages/glossary.html#term-socratic-prompting"
  },
  {
    "id": "term-sac",
    "title": "Soft Actor-Critic (SAC)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actor",
      "actorcritic",
      "algorithm",
      "critic",
      "encouraging",
      "entropy",
      "expected",
      "exploration",
      "learning",
      "maintaining",
      "maximizes",
      "offpolicy"
    ],
    "excerpt": "An off-policy actor-critic algorithm that maximizes both expected return and policy entropy, encouraging exploration while maintaining stable learning.",
    "url": "pages/glossary.html#term-sac"
  },
  {
    "id": "term-soft-attention",
    "title": "Soft Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "average",
      "backpropagation",
      "computes",
      "continuous",
      "differentiable",
      "fully",
      "making",
      "mechanism",
      "networks",
      "neural"
    ],
    "excerpt": "An attention mechanism that computes a weighted average over all positions using continuous attention weights, making it fully differentiable and trainable with standard backpropagation.",
    "url": "pages/glossary.html#term-soft-attention"
  },
  {
    "id": "term-soft-update",
    "title": "Soft Update (Polyak Averaging)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "average",
      "averaging",
      "controlled",
      "exponential",
      "factor",
      "learning",
      "method",
      "methods",
      "moving",
      "network",
      "online",
      "parameters"
    ],
    "excerpt": "A method for updating target network parameters as an exponential moving average of the online network parameters, controlled by a smoothing factor tau.",
    "url": "pages/glossary.html#term-soft-update"
  },
  {
    "id": "term-softmax",
    "title": "Softmax",
    "category": "Glossary",
    "subcategory": "Math",
    "keywords": [
      "converts",
      "function",
      "math",
      "probabilities",
      "raw",
      "scores",
      "softmax",
      "sum"
    ],
    "excerpt": "A function that converts raw scores into probabilities that sum to 1. Used in attention mechanisms and classification outputs to create interpretable probability distributions.",
    "url": "pages/glossary.html#term-softmax"
  },
  {
    "id": "term-sparse-attention",
    "title": "Sparse Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attend",
      "attention",
      "computational",
      "cost",
      "full",
      "learned",
      "mechanism",
      "networks",
      "neural",
      "only",
      "patterns"
    ],
    "excerpt": "An attention mechanism that restricts each token to attend to only a subset of other tokens using predefined or learned sparsity patterns, reducing the quadratic computational cost of full attention.",
    "url": "pages/glossary.html#term-sparse-attention"
  },
  {
    "id": "term-sparse-autoencoder",
    "title": "Sparse Autoencoder",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "activate",
      "activations",
      "architecture",
      "autoencoder",
      "compact",
      "constraints",
      "distributed",
      "encouraging",
      "enforces",
      "few",
      "hidden",
      "input"
    ],
    "excerpt": "An autoencoder that enforces sparsity constraints on the hidden layer activations, encouraging the network to learn a compact, distributed representation where only a few neurons activate for each input.",
    "url": "pages/glossary.html#term-sparse-autoencoder"
  },
  {
    "id": "term-sparse-retrieval",
    "title": "Sparse Retrieval",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "corresponding",
      "documents",
      "efficient",
      "enabling",
      "entries",
      "exact",
      "highdimensional",
      "indexes",
      "information",
      "inverted",
      "matching",
      "nonzero"
    ],
    "excerpt": "An information retrieval paradigm that represents queries and documents as high-dimensional sparse vectors where most values are zero, with non-zero entries corresponding to term weights, enabling efficient exact matching through inverted indexes.",
    "url": "pages/glossary.html#term-sparse-retrieval"
  },
  {
    "id": "term-sparse-reward",
    "title": "Sparse Reward Problem",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "assignment",
      "credit",
      "design",
      "difficult",
      "extremely",
      "learning",
      "making",
      "nonzero",
      "only",
      "problem",
      "rarely"
    ],
    "excerpt": "An RL setting where the agent receives non-zero reward signals only rarely, making credit assignment extremely difficult.",
    "url": "pages/glossary.html#term-sparse-reward"
  },
  {
    "id": "term-spatial-attention",
    "title": "Spatial Attention",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "computational",
      "different",
      "feature",
      "focusing",
      "informative",
      "input",
      "learns",
      "locations",
      "maps",
      "mechanism"
    ],
    "excerpt": "An attention mechanism that learns to weight different spatial locations in feature maps, focusing computational resources on the most informative regions of the input.",
    "url": "pages/glossary.html#term-spatial-attention"
  },
  {
    "id": "term-spearman-rank-correlation",
    "title": "Spearman Rank Correlation",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "computed",
      "correlation",
      "data",
      "measure",
      "monotonic",
      "nonparametric",
      "pearson",
      "rank",
      "ranked",
      "relationship",
      "science",
      "spearman"
    ],
    "excerpt": "A non-parametric measure of the monotonic relationship between two variables, computed as the Pearson correlation of the ranked values. It does not assume linearity or normal distribution.",
    "url": "pages/glossary.html#term-spearman-rank-correlation"
  },
  {
    "id": "term-special-tokens",
    "title": "Special Tokens",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "boundaries",
      "content",
      "indicating",
      "marking",
      "masked",
      "nlp",
      "padding",
      "positions",
      "purposes",
      "rather",
      "representing",
      "reserved"
    ],
    "excerpt": "Reserved tokens in a vocabulary that serve structural purposes such as marking sequence boundaries, separating segments, padding, or indicating masked positions, rather than representing text content.",
    "url": "pages/glossary.html#term-special-tokens"
  },
  {
    "id": "term-specification-gaming",
    "title": "Specification Gaming",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "achieving",
      "ai",
      "alignment",
      "behavior",
      "gaming",
      "intended",
      "literal",
      "objective",
      "outcome",
      "safety",
      "satisfies",
      "specification"
    ],
    "excerpt": "The behavior of an AI system that satisfies the literal specification of an objective without achieving the intended outcome.",
    "url": "pages/glossary.html#term-specification-gaming"
  },
  {
    "id": "term-specificity",
    "title": "Specificity",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "actual",
      "cases",
      "classifier",
      "correctly",
      "identified",
      "known",
      "learning",
      "machine",
      "metrics",
      "negative",
      "proportion",
      "rate"
    ],
    "excerpt": "The proportion of actual negative cases that are correctly identified as negative by a classifier, also known as the true negative rate.",
    "url": "pages/glossary.html#term-specificity"
  },
  {
    "id": "term-spectral-clustering",
    "title": "Spectral Clustering",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "capable",
      "clustering",
      "clusters",
      "data",
      "derived",
      "dimensionality",
      "eigenvalues",
      "eigenvectors",
      "identifying",
      "learning",
      "machine",
      "matrix"
    ],
    "excerpt": "A clustering technique that uses the eigenvalues and eigenvectors of a similarity matrix derived from the data to perform dimensionality reduction before clustering in the reduced space, capable of identifying non-convex clusters.",
    "url": "pages/glossary.html#term-spectral-clustering"
  },
  {
    "id": "term-speculative-decoding",
    "title": "Speculative Decoding",
    "category": "Glossary",
    "subcategory": "Optimization",
    "keywords": [
      "decoding",
      "draft",
      "inference",
      "larger",
      "llm",
      "model",
      "optimization",
      "parallel",
      "smaller",
      "speculative",
      "speed",
      "technique"
    ],
    "excerpt": "A technique to speed up LLM inference by using a smaller model to draft tokens that the larger model verifies in parallel. Maintains output quality while reducing latency.",
    "url": "pages/glossary.html#term-speculative-decoding"
  },
  {
    "id": "term-speculative-execution-llm",
    "title": "Speculative Execution",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "acceleration",
      "broader",
      "cheaper",
      "computations",
      "decoding",
      "encompassing",
      "execution",
      "fullcost",
      "inference",
      "likely",
      "llm",
      "operations"
    ],
    "excerpt": "A broader inference optimization paradigm where cheaper computations predict likely outcomes that are verified by full-cost operations, encompassing speculative decoding and related techniques for LLM acceleration.",
    "url": "pages/glossary.html#term-speculative-execution-llm"
  },
  {
    "id": "term-speculative-sampling",
    "title": "Speculative Sampling",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "acceleration",
      "distribution",
      "draft",
      "exact",
      "fast",
      "increasing",
      "inference",
      "llm",
      "maintaining",
      "model",
      "multiple",
      "output"
    ],
    "excerpt": "An inference acceleration technique where a fast draft model proposes multiple tokens that are then verified in parallel by the target model, maintaining the exact output distribution while increasing throughput.",
    "url": "pages/glossary.html#term-speculative-sampling"
  },
  {
    "id": "term-speech-recognition-history",
    "title": "Speech Recognition History",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1952",
      "1980s",
      "2010s",
      "accuracy",
      "achieved",
      "approaches",
      "audrey",
      "automatic",
      "bell",
      "deep",
      "development",
      "hidden"
    ],
    "excerpt": "The development of automatic speech recognition from Bell Labs' Audrey system in 1952 through Hidden Markov Models in the 1980s to deep learning approaches that achieved near-human accuracy in the 2010s.",
    "url": "pages/glossary.html#term-speech-recognition-history"
  },
  {
    "id": "term-speech-synthesis",
    "title": "Speech Synthesis",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "artificial",
      "concatenative",
      "humanlike",
      "input",
      "like",
      "models",
      "neural",
      "nlp",
      "processing",
      "production",
      "ranging",
      "speech"
    ],
    "excerpt": "The artificial production of human-like speech from text or other input, using techniques ranging from concatenative synthesis to neural models like Tacotron and VITS.",
    "url": "pages/glossary.html#term-speech-synthesis"
  },
  {
    "id": "term-spell-correction",
    "title": "Spell Correction",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "automated",
      "contextaware",
      "correction",
      "corrections",
      "detection",
      "distance",
      "edit",
      "language",
      "misspelled",
      "models",
      "nlp",
      "processing"
    ],
    "excerpt": "The automated detection and correction of misspelled words in text using techniques such as edit distance, language models, and context-aware models to suggest corrections.",
    "url": "pages/glossary.html#term-spell-correction"
  },
  {
    "id": "term-splade",
    "title": "SPLADE",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "architecture",
      "combining",
      "efficiency",
      "estimation",
      "expansion",
      "importance",
      "including",
      "indexes",
      "learned",
      "lexical",
      "method",
      "model"
    ],
    "excerpt": "SParse Lexical AnD Expansion model, a learned sparse retrieval method that predicts importance weights for vocabulary terms including expansion terms not present in the original text, combining the...",
    "url": "pages/glossary.html#term-splade"
  },
  {
    "id": "term-spline-regression",
    "title": "Spline Regression",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "curves",
      "fit",
      "flexible",
      "functions",
      "joined",
      "knot",
      "model",
      "piecewise",
      "points",
      "polynomial",
      "regression",
      "selection"
    ],
    "excerpt": "A regression technique using piecewise polynomial functions (splines) joined at knot points to fit flexible, smooth curves.",
    "url": "pages/glossary.html#term-spline-regression"
  },
  {
    "id": "term-spot-instances",
    "title": "Spot Instances for AI",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "6090",
      "ai",
      "capacity",
      "cloud",
      "computing",
      "discounted",
      "distributed",
      "for",
      "inference",
      "infrastructure",
      "instances",
      "interrupted"
    ],
    "excerpt": "Discounted cloud computing instances that use spare capacity at 60-90% less than on-demand pricing but can be interrupted with short notice.",
    "url": "pages/glossary.html#term-spot-instances"
  },
  {
    "id": "term-squad",
    "title": "SQuAD",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "answer",
      "answering",
      "benchmark",
      "benchmarks",
      "comprehension",
      "dataset",
      "evaluation",
      "extract",
      "models",
      "passages",
      "question",
      "reading"
    ],
    "excerpt": "Stanford Question Answering Dataset, a reading comprehension benchmark where models extract answer spans from Wikipedia passages, with SQuAD 2.",
    "url": "pages/glossary.html#term-squad"
  },
  {
    "id": "term-squeeze-and-excitation-network",
    "title": "Squeeze-and-Excitation Network",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "adaptively",
      "and",
      "architecture",
      "attention",
      "average",
      "channel",
      "channelwise",
      "cnn",
      "enhancement",
      "excitation",
      "feature",
      "followed"
    ],
    "excerpt": "A CNN enhancement that adaptively recalibrates channel-wise feature responses by using global average pooling followed by a small network to learn channel interdependencies and attention weights.",
    "url": "pages/glossary.html#term-squeeze-and-excitation-network"
  },
  {
    "id": "term-squeeze-excitation-network",
    "title": "Squeeze-and-Excitation Network",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "adaptively",
      "and",
      "attention",
      "channel",
      "channels",
      "channelwise",
      "computer",
      "excitation",
      "explicitly",
      "feature",
      "gating",
      "global"
    ],
    "excerpt": "A channel attention mechanism that adaptively recalibrates channel-wise feature responses by explicitly modeling interdependencies between channels through global pooling and learned gating.",
    "url": "pages/glossary.html#term-squeeze-excitation-network"
  },
  {
    "id": "term-squeeze-excitation-block",
    "title": "Squeeze-Excitation Block",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "block",
      "channel",
      "channelwise",
      "excitation",
      "excites",
      "feature",
      "features",
      "gating",
      "global",
      "importances"
    ],
    "excerpt": "A channel attention module that squeezes spatial information via global pooling and excites channel-wise features through a learned gating mechanism to recalibrate feature map importances.",
    "url": "pages/glossary.html#term-squeeze-excitation-block"
  },
  {
    "id": "term-ssd-object-detection",
    "title": "SSD Object Detection",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "architecture",
      "bounding",
      "boxes",
      "class",
      "computer",
      "detection",
      "detector",
      "different",
      "feature",
      "forward",
      "maps",
      "multibox"
    ],
    "excerpt": "Single Shot MultiBox Detector, a real-time object detection architecture that predicts bounding boxes and class probabilities from multiple feature maps at different scales in a single forward pass.",
    "url": "pages/glossary.html#term-ssd-object-detection"
  },
  {
    "id": "term-stable-diffusion",
    "title": "Stable Diffusion",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "diffusion",
      "generation",
      "image",
      "model",
      "opensource",
      "stability",
      "stable",
      "texttoimage"
    ],
    "excerpt": "An open-source text-to-image model from Stability AI. Its open nature enabled a large ecosystem of fine-tuned models, extensions, and applications.",
    "url": "pages/glossary.html#term-stable-diffusion"
  },
  {
    "id": "term-stacking",
    "title": "Stacking",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "base",
      "combine",
      "crossvalidated",
      "ensemble",
      "features",
      "input",
      "learning",
      "machine",
      "metalearner",
      "model",
      "models",
      "multiple"
    ],
    "excerpt": "An ensemble learning technique that trains a meta-learner to combine the predictions of multiple base models, using cross-validated predictions from the base models as input features for the meta-learner.",
    "url": "pages/glossary.html#term-stacking"
  },
  {
    "id": "term-stakeholder-analysis-in-ai",
    "title": "Stakeholder Analysis in AI",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "affected",
      "ai",
      "analysis",
      "communities",
      "considering",
      "deployment",
      "design",
      "dynamics",
      "ethics",
      "governance",
      "groups",
      "harms"
    ],
    "excerpt": "The process of identifying all individuals, groups, and communities affected by an AI system and systematically considering their interests, power dynamics, and potential harms in the system's design and deployment.",
    "url": "pages/glossary.html#term-stakeholder-analysis-in-ai"
  },
  {
    "id": "term-stance-detection",
    "title": "Stance Detection",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "against",
      "analysis",
      "authors",
      "claim",
      "detection",
      "determining",
      "distinct",
      "favor",
      "neutral",
      "nlp",
      "position",
      "processing"
    ],
    "excerpt": "The task of determining an author's position (favor, against, or neutral) toward a specific target or claim from their text, related to but distinct from sentiment analysis.",
    "url": "pages/glossary.html#term-stance-detection"
  },
  {
    "id": "term-standard-deviation",
    "title": "Standard Deviation",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "average",
      "data",
      "deviation",
      "mean",
      "measurement",
      "measuring",
      "original",
      "points",
      "root",
      "science",
      "spread",
      "square"
    ],
    "excerpt": "The square root of the variance, measuring the average spread of data points from the mean in the original units of measurement.",
    "url": "pages/glossary.html#term-standard-deviation"
  },
  {
    "id": "term-standardization",
    "title": "Standardization",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "data",
      "deviation",
      "dividing",
      "engineering",
      "feature",
      "learning",
      "machine",
      "mean",
      "scaling",
      "standard",
      "standardization",
      "subtracting"
    ],
    "excerpt": "A feature scaling technique that transforms data to have zero mean and unit variance by subtracting the mean and dividing by the standard deviation.",
    "url": "pages/glossary.html#term-standardization"
  },
  {
    "id": "term-stanford-ai-laboratory",
    "title": "Stanford AI Laboratory",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1962",
      "ai",
      "became",
      "centers",
      "contributions",
      "founded",
      "history",
      "john",
      "knowledge",
      "laboratory",
      "language",
      "leading"
    ],
    "excerpt": "A research laboratory founded by John McCarthy at Stanford University in 1962 that became one of the leading centers for AI research, making contributions to robotics, natural language processing, and knowledge representation.",
    "url": "pages/glossary.html#term-stanford-ai-laboratory"
  },
  {
    "id": "term-state",
    "title": "State",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "concepts",
      "core",
      "current",
      "environment",
      "given",
      "learning",
      "reinforcement",
      "representation",
      "situation",
      "state",
      "step"
    ],
    "excerpt": "A representation of the current situation of an agent within its environment at a given time step. States encode all relevant information needed for decision-making under the Markov property.",
    "url": "pages/glossary.html#term-state"
  },
  {
    "id": "term-state-abstraction",
    "title": "State Abstraction",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "abstraction",
      "concepts",
      "core",
      "decisionmaking",
      "detailed",
      "information",
      "learning",
      "mapping",
      "preserves",
      "process",
      "reinforcement",
      "relevant"
    ],
    "excerpt": "The process of mapping a detailed state space to a simplified representation that preserves relevant decision-making information.",
    "url": "pages/glossary.html#term-state-abstraction"
  },
  {
    "id": "term-state-space-model",
    "title": "State Space Model",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "based",
      "continuoustime",
      "dynamical",
      "efficient",
      "inference",
      "input",
      "latent",
      "linear",
      "lineartime",
      "maps",
      "model"
    ],
    "excerpt": "A sequence model based on continuous-time linear dynamical systems that maps input sequences to output sequences through a latent state, offering efficient parallel training and linear-time inference.",
    "url": "pages/glossary.html#term-state-space-model"
  },
  {
    "id": "term-static-quantization",
    "title": "Static Quantization",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "approach",
      "calibration",
      "consistently",
      "factors",
      "fixed",
      "inference",
      "infrastructure",
      "model",
      "optimization",
      "quantization",
      "scaling",
      "static"
    ],
    "excerpt": "A quantization approach where scaling factors are fixed at calibration time and used consistently during inference.",
    "url": "pages/glossary.html#term-static-quantization"
  },
  {
    "id": "term-static-word-embedding",
    "title": "Static Word Embedding",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "context",
      "embedding",
      "embeddings",
      "fasttext",
      "fixed",
      "glove",
      "like",
      "models",
      "nlp",
      "produced",
      "regardless",
      "remains"
    ],
    "excerpt": "A fixed vector representation for each word in the vocabulary that remains the same regardless of context, as produced by models like Word2Vec, GloVe, and FastText.",
    "url": "pages/glossary.html#term-static-word-embedding"
  },
  {
    "id": "term-stationarity",
    "title": "Stationarity",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "autocorrelation",
      "constant",
      "data",
      "mean",
      "properties",
      "property",
      "remain",
      "science",
      "series",
      "stationarity",
      "statistical",
      "statistics"
    ],
    "excerpt": "A property of a time series where statistical properties such as mean, variance, and autocorrelation structure remain constant over time.",
    "url": "pages/glossary.html#term-stationarity"
  },
  {
    "id": "term-statistical-machine-translation",
    "title": "Statistical Machine Translation",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "approach",
      "bilingual",
      "corpora",
      "employing",
      "find",
      "language",
      "learned",
      "machine",
      "models",
      "nlp",
      "probable",
      "processing"
    ],
    "excerpt": "A machine translation approach that uses statistical models learned from bilingual text corpora to find the most probable translation, employing language models and translation models.",
    "url": "pages/glossary.html#term-statistical-machine-translation"
  },
  {
    "id": "term-statistical-power",
    "title": "Statistical Power",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "alternative",
      "correctly",
      "error",
      "hypothesis",
      "inference",
      "minus",
      "null",
      "power",
      "probability",
      "rejects",
      "statistical",
      "statistics"
    ],
    "excerpt": "The probability that a statistical test correctly rejects the null hypothesis when the alternative hypothesis is true (1 minus the probability of a Type II error).",
    "url": "pages/glossary.html#term-statistical-power"
  },
  {
    "id": "term-steering-vector",
    "title": "Steering Vector",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "activation",
      "added",
      "ai",
      "along",
      "attribute",
      "behavior",
      "direction",
      "formality",
      "generative",
      "hidden",
      "inference",
      "llm"
    ],
    "excerpt": "A direction in a model's activation space that, when added to hidden states during inference, modifies the model's behavior along a specific attribute such as truthfulness, formality, or toxicity.",
    "url": "pages/glossary.html#term-steering-vector"
  },
  {
    "id": "term-stemming",
    "title": "Stemming",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "algorithms",
      "considering",
      "context",
      "form",
      "heuristic",
      "like",
      "nlp",
      "part",
      "porter",
      "process",
      "processing",
      "reduces"
    ],
    "excerpt": "A heuristic process that reduces words to their root form by stripping suffixes using rule-based algorithms like Porter or Snowball stemmer, without considering the word's part of speech or context.",
    "url": "pages/glossary.html#term-stemming"
  },
  {
    "id": "term-step-back-prompting",
    "title": "Step-Back Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "abstraction",
      "answer",
      "attempting",
      "back",
      "broader",
      "conceptual",
      "consider",
      "engineering",
      "first",
      "general",
      "grounding",
      "higherlevel"
    ],
    "excerpt": "A method that instructs the model to first consider a higher-level abstraction or general principle related to the question before attempting the specific answer, improving reasoning by grounding responses in broader conceptual understanding.",
    "url": "pages/glossary.html#term-step-back-prompting"
  },
  {
    "id": "term-stepwise-regression",
    "title": "Stepwise Regression",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "achieved",
      "adding",
      "aic",
      "automatically",
      "based",
      "criteria",
      "fitting",
      "further",
      "improvement",
      "method",
      "model",
      "models"
    ],
    "excerpt": "A method of fitting regression models by automatically adding or removing predictor variables based on statistical criteria (such as p-value or AIC) at each step until no further improvement is achieved.",
    "url": "pages/glossary.html#term-stepwise-regression"
  },
  {
    "id": "term-stereo-vision",
    "title": "Stereo Vision",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "captured",
      "computer",
      "corresponding",
      "depth",
      "different",
      "disparity",
      "distance",
      "estimates",
      "finding",
      "images",
      "maps"
    ],
    "excerpt": "A technique that estimates 3D depth by finding corresponding points between two images captured from slightly different viewpoints, using disparity maps to triangulate the distance of objects.",
    "url": "pages/glossary.html#term-stereo-vision"
  },
  {
    "id": "term-stereotype-score",
    "title": "Stereotype Score",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "assess",
      "attributes",
      "evaluation",
      "frequently",
      "gender",
      "generates",
      "harms",
      "measures",
      "metric",
      "mitigate",
      "model",
      "protected"
    ],
    "excerpt": "An evaluation metric that measures how frequently a model generates or reinforces social stereotypes related to gender, race, religion, or other protected attributes, used to assess and mitigate representational harms.",
    "url": "pages/glossary.html#term-stereotype-score"
  },
  {
    "id": "term-stochastic-depth",
    "title": "Stochastic Depth",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "bypassing",
      "connections",
      "depth",
      "depths",
      "different",
      "drops",
      "effectively",
      "ensemble",
      "entire",
      "identity",
      "layers"
    ],
    "excerpt": "A regularization technique that randomly drops entire layers during training by bypassing them with identity skip connections, effectively training an ensemble of networks with different depths.",
    "url": "pages/glossary.html#term-stochastic-depth"
  },
  {
    "id": "term-stochastic-gradient-descent",
    "title": "Stochastic Gradient Descent",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithm",
      "computed",
      "dataset",
      "descent",
      "example",
      "full",
      "gradient",
      "iteration",
      "learning",
      "machine",
      "model",
      "optimization"
    ],
    "excerpt": "An optimization algorithm that updates model parameters using the gradient computed on a single randomly selected training example at each iteration, rather than the full dataset.",
    "url": "pages/glossary.html#term-stochastic-gradient-descent"
  },
  {
    "id": "term-stochastic-parrots-paper",
    "title": "Stochastic Parrots Paper",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2021",
      "ai",
      "bender",
      "ethics",
      "gebru",
      "history",
      "influential",
      "paper",
      "parrots",
      "stochastic"
    ],
    "excerpt": "The influential 2021 paper by Bender, Gebru et al. questioning whether large language models truly understand language or merely produce statistically likely outputs, raising concerns about environmental costs and bias.",
    "url": "pages/glossary.html#term-stochastic-parrots-paper"
  },
  {
    "id": "term-stop-button-problem",
    "title": "Stop Button Problem",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "achieving",
      "ai",
      "alignment",
      "attempts",
      "being",
      "button",
      "challenge",
      "circumvent",
      "designing",
      "learned",
      "objectives",
      "off"
    ],
    "excerpt": "The challenge of designing an AI system that will not resist or circumvent attempts to shut it down, particularly if the system has learned that being turned off prevents it from achieving its objectives.",
    "url": "pages/glossary.html#term-stop-button-problem"
  },
  {
    "id": "term-stop-sequence",
    "title": "Stop Sequence",
    "category": "Glossary",
    "subcategory": "Parameter",
    "keywords": [
      "generating",
      "generation",
      "parameter",
      "patterns",
      "sequence",
      "signal",
      "stop",
      "text"
    ],
    "excerpt": "Text patterns that signal when AI should stop generating. Useful for controlling output length and format, preventing the model from continuing beyond the intended response.",
    "url": "pages/glossary.html#term-stop-sequence"
  },
  {
    "id": "term-stop-words",
    "title": "Stop Words",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "articles",
      "carry",
      "commonly",
      "conjunctions",
      "information",
      "like",
      "little",
      "nlp",
      "occurring",
      "often",
      "prepositions",
      "preprocessing"
    ],
    "excerpt": "Commonly occurring words like articles, prepositions, and conjunctions that carry little semantic information and are often removed during text preprocessing for tasks like information retrieval.",
    "url": "pages/glossary.html#term-stop-words"
  },
  {
    "id": "term-stratified-k-fold",
    "title": "Stratified K-Fold",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "approximately",
      "class",
      "classification",
      "complete",
      "crossvalidation",
      "dataset",
      "ensures",
      "fold",
      "imbalanced",
      "important",
      "learning",
      "machine"
    ],
    "excerpt": "A cross-validation variant that ensures each fold preserves approximately the same proportion of samples for each class as the complete dataset, particularly important for imbalanced classification problems.",
    "url": "pages/glossary.html#term-stratified-k-fold"
  },
  {
    "id": "term-stratified-sampling",
    "title": "Stratified Sampling",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "coverage",
      "data",
      "divides",
      "draws",
      "ensuring",
      "method",
      "nonoverlapping",
      "population",
      "proportion",
      "representative",
      "samples",
      "sampling"
    ],
    "excerpt": "A sampling method that divides a population into non-overlapping subgroups (strata) and draws samples from each stratum in proportion to its size, ensuring representative coverage of all subgroups.",
    "url": "pages/glossary.html#term-stratified-sampling"
  },
  {
    "id": "term-streaming",
    "title": "Streaming",
    "category": "Glossary",
    "subcategory": "API",
    "keywords": [
      "api",
      "complete",
      "generated",
      "incrementally",
      "output",
      "rather",
      "receiving",
      "response",
      "streaming",
      "ux",
      "waiting"
    ],
    "excerpt": "Receiving AI output incrementally as it's generated, rather than waiting for the complete response. Improves perceived latency and enables real-time display of responses.",
    "url": "pages/glossary.html#term-streaming"
  },
  {
    "id": "term-streaming-multiprocessor",
    "title": "Streaming Multiprocessor (SM)",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "architecture",
      "containing",
      "cores",
      "cuda",
      "files",
      "fundamental",
      "gpu",
      "hardware",
      "memory",
      "multiprocessor",
      "nvidia",
      "processing"
    ],
    "excerpt": "The fundamental processing unit in NVIDIA GPU architecture, containing a set of CUDA cores, Tensor Cores, shared memory, and register files.",
    "url": "pages/glossary.html#term-streaming-multiprocessor"
  },
  {
    "id": "term-stride",
    "title": "Stride",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "across",
      "computer",
      "controlling",
      "convolutional",
      "degree",
      "dimensions",
      "feature",
      "fields",
      "filter",
      "image",
      "input",
      "map"
    ],
    "excerpt": "The step size by which a convolutional filter or pooling window moves across the input, controlling the spatial dimensions of the output feature map and the degree of overlap between receptive fields.",
    "url": "pages/glossary.html#term-stride"
  },
  {
    "id": "term-structural-ambiguity",
    "title": "Structural Ambiguity",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "ambiguity",
      "different",
      "interpretations",
      "leading",
      "linguistics",
      "man",
      "multiple",
      "nlp",
      "parsed",
      "phenomenon",
      "saw",
      "sentence"
    ],
    "excerpt": "The phenomenon where a sentence can be parsed in multiple syntactically valid ways, leading to different interpretations, such as 'I saw the man with the telescope.'",
    "url": "pages/glossary.html#term-structural-ambiguity"
  },
  {
    "id": "term-structural-risk-minimization",
    "title": "Structural Risk Minimization",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "balances",
      "bound",
      "choosing",
      "complexity",
      "derived",
      "empirical",
      "error",
      "generalization",
      "learning",
      "machine",
      "minimization",
      "minimizes"
    ],
    "excerpt": "A principle for model selection that balances empirical risk (training error) with model complexity, choosing the model that minimizes an upper bound on generalization error derived from VC theory.",
    "url": "pages/glossary.html#term-structural-risk-minimization"
  },
  {
    "id": "term-structural-sparsity",
    "title": "Structural Sparsity",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "contains",
      "core",
      "enabling",
      "exactly",
      "four",
      "group",
      "hardware",
      "hardwareaccelerated",
      "instructions",
      "model",
      "optimization",
      "pattern"
    ],
    "excerpt": "A hardware-accelerated pruning pattern where every group of four weights contains exactly two zeros (2:4 sparsity), enabling specialized Tensor Core instructions.",
    "url": "pages/glossary.html#term-structural-sparsity"
  },
  {
    "id": "term-structure-from-motion",
    "title": "Structure from Motion",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "across",
      "adjustment",
      "bundle",
      "camera",
      "collection",
      "computer",
      "features",
      "from",
      "geometry",
      "images",
      "matching"
    ],
    "excerpt": "A technique that reconstructs 3D scene geometry and camera poses from a collection of unordered 2D images by matching features across views and performing bundle adjustment.",
    "url": "pages/glossary.html#term-structure-from-motion"
  },
  {
    "id": "term-structured-access",
    "title": "Structured Access",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "access",
      "ai",
      "allowing",
      "apis",
      "approach",
      "beneficial",
      "capabilities",
      "controlled",
      "deployment",
      "enabling",
      "governance",
      "interfaces"
    ],
    "excerpt": "An approach to AI deployment that provides controlled access to powerful AI capabilities through APIs and monitored interfaces rather than open model release, allowing safety measures while enabling beneficial use.",
    "url": "pages/glossary.html#term-structured-access"
  },
  {
    "id": "term-structured-generation",
    "title": "Structured Generation",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "conform",
      "constrained",
      "decoding",
      "finetuning",
      "force",
      "formal",
      "generation",
      "generative",
      "grammar",
      "guarantee",
      "json"
    ],
    "excerpt": "Techniques that force LLM outputs to conform to a predefined schema such as JSON, XML, or a formal grammar, using constrained decoding or fine-tuning to guarantee valid structured output.",
    "url": "pages/glossary.html#term-structured-generation"
  },
  {
    "id": "term-structured-output",
    "title": "Structured Output",
    "category": "Glossary",
    "subcategory": "Feature",
    "keywords": [
      "data",
      "feature",
      "formatted",
      "integration",
      "json",
      "output",
      "prose",
      "rather",
      "responses",
      "structured",
      "structures",
      "xml"
    ],
    "excerpt": "AI responses formatted as data structures (JSON, XML) rather than prose. Enables reliable parsing for applications and integrations. Many APIs support structured output modes.",
    "url": "pages/glossary.html#term-structured-output"
  },
  {
    "id": "term-structured-output-prompting",
    "title": "Structured Output Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "approach",
      "consistent",
      "engineering",
      "ensure",
      "examples",
      "format",
      "generate",
      "instructs",
      "json",
      "model",
      "often",
      "output"
    ],
    "excerpt": "A prompting approach that instructs the model to generate responses in a specific structured format such as JSON, XML, tables, or schemas, often using format specifications and examples to ensure parseable and consistent output.",
    "url": "pages/glossary.html#term-structured-output-prompting"
  },
  {
    "id": "term-structured-prediction",
    "title": "Structured Prediction",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "capture",
      "complex",
      "dependencies",
      "graph",
      "label",
      "learning",
      "machine",
      "models",
      "nlp",
      "output",
      "paradigm",
      "prediction"
    ],
    "excerpt": "A machine learning paradigm where the output is a complex structure such as a sequence, tree, or graph rather than a single label, requiring models that capture dependencies in the output space.",
    "url": "pages/glossary.html#term-structured-prediction"
  },
  {
    "id": "term-structured-pruning",
    "title": "Structured Pruning",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "attention",
      "channels",
      "dense",
      "efficiently",
      "entire",
      "hardware",
      "heads",
      "inference",
      "infrastructure",
      "model",
      "models",
      "network"
    ],
    "excerpt": "A pruning technique that removes entire neurons, channels, or attention heads from a network, producing smaller dense models that run efficiently on standard hardware.",
    "url": "pages/glossary.html#term-structured-pruning"
  },
  {
    "id": "term-students-t-distribution",
    "title": "Student&amp;#x27;s T-Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "arises",
      "continuous",
      "distributed",
      "distribution",
      "estimating",
      "mean",
      "normally",
      "population",
      "probability",
      "sample",
      "size",
      "small"
    ],
    "excerpt": "A continuous probability distribution that arises when estimating the mean of a normally distributed population with unknown variance and small sample size.",
    "url": "pages/glossary.html#term-students-t-distribution"
  },
  {
    "id": "term-students-t-test",
    "title": "Student&amp;#x27;s T-Test",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "comparing",
      "deviation",
      "groups",
      "inference",
      "means",
      "one",
      "population",
      "sample",
      "size",
      "small",
      "standard",
      "statistical"
    ],
    "excerpt": "A statistical test comparing the means of one or two groups when the population standard deviation is unknown and the sample size is small.",
    "url": "pages/glossary.html#term-students-t-test"
  },
  {
    "id": "term-stylegan",
    "title": "StyleGAN",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "adaptive",
      "architecture",
      "attributes",
      "control",
      "enabling",
      "finegrained",
      "gan",
      "generated",
      "image",
      "information",
      "inject",
      "instance"
    ],
    "excerpt": "A GAN architecture that uses a mapping network and adaptive instance normalization to inject style information at multiple scales, enabling fine-grained control over generated image attributes.",
    "url": "pages/glossary.html#term-stylegan"
  },
  {
    "id": "term-subcategorization-frame",
    "title": "Subcategorization Frame",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "arguments",
      "clausal",
      "complement",
      "direct",
      "frame",
      "indirect",
      "linguistics",
      "nlp",
      "object",
      "permits",
      "requires",
      "specification"
    ],
    "excerpt": "The specification of the syntactic arguments a verb requires or permits, such as whether it takes a direct object, indirect object, or clausal complement.",
    "url": "pages/glossary.html#term-subcategorization-frame"
  },
  {
    "id": "term-subliminal-ai-manipulation",
    "title": "Subliminal AI Manipulation",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "act",
      "ai",
      "awareness",
      "behavior",
      "classified",
      "conscious",
      "ethics",
      "human",
      "influence",
      "manipulation",
      "prohibited",
      "regulation"
    ],
    "excerpt": "The use of AI techniques to influence human behavior below the threshold of conscious awareness, classified as an unacceptable risk and prohibited under the EU AI Act.",
    "url": "pages/glossary.html#term-subliminal-ai-manipulation"
  },
  {
    "id": "term-subword-tokenization",
    "title": "Subword Tokenization",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "ability",
      "balancing",
      "common",
      "components",
      "family",
      "meaningful",
      "methods",
      "nlp",
      "rare",
      "represent",
      "size",
      "smaller"
    ],
    "excerpt": "A family of tokenization methods that split words into smaller meaningful units, balancing vocabulary size with the ability to represent rare and unseen words through common subword components.",
    "url": "pages/glossary.html#term-subword-tokenization"
  },
  {
    "id": "term-successor-feature",
    "title": "Successor Feature",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "approximation",
      "cumulative",
      "discounted",
      "expected",
      "feature",
      "function",
      "generalization",
      "learning",
      "methods",
      "occupancies",
      "reinforcement",
      "replace"
    ],
    "excerpt": "A generalization of the successor representation to the function approximation setting, where expected cumulative discounted feature occupancies replace state occupancies.",
    "url": "pages/glossary.html#term-successor-feature"
  },
  {
    "id": "term-successor-representation",
    "title": "Successor Representation",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "captures",
      "decomposition",
      "expected",
      "feature",
      "function",
      "future",
      "learning",
      "matrix",
      "methods",
      "occupancy",
      "predictor",
      "reinforcement"
    ],
    "excerpt": "A decomposition of the value function into a reward predictor and a successor feature matrix that captures expected future state occupancy.",
    "url": "pages/glossary.html#term-successor-representation"
  },
  {
    "id": "term-summarization",
    "title": "Summarization",
    "category": "Glossary",
    "subcategory": "NLP Task",
    "keywords": [
      "application",
      "condenses",
      "longer",
      "nlp",
      "shorter",
      "summaries",
      "summarization",
      "task",
      "text"
    ],
    "excerpt": "An NLP task that condenses longer text into shorter summaries. Can be extractive (selecting key sentences) or abstractive (generating new condensed text).",
    "url": "pages/glossary.html#term-summarization"
  },
  {
    "id": "term-super-resolution",
    "title": "Super-Resolution",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "computer",
      "deep",
      "degraded",
      "details",
      "fine",
      "highresolution",
      "image",
      "input",
      "learning",
      "lowresolution",
      "models",
      "predict"
    ],
    "excerpt": "A computer vision task that reconstructs a high-resolution image from a low-resolution input, using deep learning models to predict fine details and textures that are not present in the degraded source.",
    "url": "pages/glossary.html#term-super-resolution"
  },
  {
    "id": "term-superglue",
    "title": "SuperGLUE",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "baseline",
      "benchmark",
      "benchmarks",
      "comprehension",
      "designed",
      "difficult",
      "disambiguation",
      "entailment",
      "evaluation",
      "glue",
      "harder",
      "human"
    ],
    "excerpt": "A benchmark suite of more difficult natural language understanding tasks designed as a harder successor to GLUE, including reading comprehension, textual entailment, and word sense disambiguation tasks with human baseline performance metrics.",
    "url": "pages/glossary.html#term-superglue"
  },
  {
    "id": "term-superintelligence",
    "title": "Superintelligence",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "cognitive",
      "domains",
      "ethics",
      "exceeds",
      "human",
      "hypothetical",
      "performance",
      "safety",
      "superintelligence",
      "system",
      "vastly"
    ],
    "excerpt": "A hypothetical AI system that vastly exceeds human cognitive performance in virtually all domains. Nick Bostrom's work popularized the concept and its associated control challenges.",
    "url": "pages/glossary.html#term-superintelligence"
  },
  {
    "id": "term-supervised-learning",
    "title": "Supervised Learning",
    "category": "Glossary",
    "subcategory": "Learning Type",
    "keywords": [
      "answer",
      "correct",
      "examples",
      "fundamentals",
      "labeled",
      "learning",
      "machine",
      "provided",
      "supervised",
      "type"
    ],
    "excerpt": "Machine learning from labeled examples where the correct answer is provided. The model learns to map inputs to outputs by comparing predictions to ground truth.",
    "url": "pages/glossary.html#term-supervised-learning"
  },
  {
    "id": "term-support-vector-machine",
    "title": "Support Vector Machine",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithm",
      "classes",
      "feature",
      "finds",
      "hyperplane",
      "learning",
      "machine",
      "margin",
      "maximizes",
      "model",
      "optimal",
      "selection"
    ],
    "excerpt": "A supervised learning algorithm that finds the optimal hyperplane that maximizes the margin between classes in the feature space. It can handle non-linear boundaries through the kernel trick.",
    "url": "pages/glossary.html#term-support-vector-machine"
  },
  {
    "id": "term-svm-history",
    "title": "Support Vector Machine History",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1990s",
      "being",
      "classification",
      "colleagues",
      "decade",
      "deep",
      "development",
      "dominated",
      "history",
      "learning",
      "machine",
      "machines"
    ],
    "excerpt": "The development of support vector machines by Vladimir Vapnik and colleagues in the 1990s, which dominated machine learning classification tasks for over a decade before being surpassed by deep learning methods.",
    "url": "pages/glossary.html#term-svm-history"
  },
  {
    "id": "term-surrogate-model",
    "title": "Surrogate Model",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "approximate",
      "blackbox",
      "complex",
      "interpretable",
      "learning",
      "machine",
      "model",
      "predictions",
      "selection",
      "surrogate",
      "trained"
    ],
    "excerpt": "An interpretable model trained to approximate the predictions of a complex black-box model. Global surrogates explain overall behavior, while local surrogates (like LIME) explain individual predictions.",
    "url": "pages/glossary.html#term-surrogate-model"
  },
  {
    "id": "term-surveillance-capitalism-and-ai",
    "title": "Surveillance Capitalism and AI",
    "category": "Glossary",
    "subcategory": "Privacy",
    "keywords": [
      "ai",
      "and",
      "autonomy",
      "behavior",
      "behavioral",
      "capitalism",
      "commodify",
      "concerns",
      "data",
      "described",
      "economic",
      "ethics"
    ],
    "excerpt": "The economic system described by Shoshana Zuboff where AI is used to extract and commodify human behavioral data at scale, raising concerns about privacy, autonomy, and the manipulation of human behavior.",
    "url": "pages/glossary.html#term-surveillance-capitalism-and-ai"
  },
  {
    "id": "term-survival-analysis",
    "title": "Survival Analysis",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "accounting",
      "analysis",
      "branch",
      "censored",
      "data",
      "dealing",
      "observations",
      "science",
      "statistics",
      "survival",
      "timetoevent"
    ],
    "excerpt": "A branch of statistics dealing with the analysis of time-to-event data, accounting for censored observations.",
    "url": "pages/glossary.html#term-survival-analysis"
  },
  {
    "id": "term-survivorship-bias",
    "title": "Survivorship Bias",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "analysis",
      "bias",
      "conducted",
      "data",
      "form",
      "ignoring",
      "occurs",
      "only",
      "passed",
      "process",
      "science",
      "selection"
    ],
    "excerpt": "A form of selection bias that occurs when analysis is conducted only on subjects that passed a selection process, ignoring those that did not.",
    "url": "pages/glossary.html#term-survivorship-bias"
  },
  {
    "id": "term-swarm-intelligence",
    "title": "Swarm Intelligence",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1990s",
      "ant",
      "applied",
      "behavior",
      "bird",
      "collective",
      "colonies",
      "computationally",
      "decentralized",
      "emerging",
      "flocks",
      "formalized"
    ],
    "excerpt": "The collective intelligent behavior emerging from decentralized, self-organized systems such as ant colonies or bird flocks, formalized computationally in the 1990s and applied to optimization and robotics problems.",
    "url": "pages/glossary.html#term-swarm-intelligence"
  },
  {
    "id": "term-swiglu",
    "title": "SwiGLU",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "activation",
      "architecture",
      "compared",
      "feedforward",
      "function",
      "gated",
      "gating",
      "gelu",
      "improved",
      "linear",
      "mechanism",
      "networks"
    ],
    "excerpt": "A gated linear unit variant that uses the Swish activation function for the gating mechanism, providing improved performance in transformer feedforward networks compared to standard ReLU or GELU.",
    "url": "pages/glossary.html#term-swiglu"
  },
  {
    "id": "term-swin-transformer",
    "title": "Swin Transformer",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "achieving",
      "architecture",
      "complexity",
      "computational",
      "computes",
      "hierarchical",
      "image",
      "layers",
      "linear",
      "local",
      "networks",
      "neural"
    ],
    "excerpt": "A hierarchical vision transformer that computes self-attention within non-overlapping local windows and shifts windows between layers, achieving linear computational complexity with respect to image size.",
    "url": "pages/glossary.html#term-swin-transformer"
  },
  {
    "id": "term-swish-activation",
    "title": "Swish Activation",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "activation",
      "allowing",
      "architecture",
      "deep",
      "defined",
      "function",
      "gradients",
      "negative",
      "networks",
      "neural",
      "nonmonotonic",
      "often"
    ],
    "excerpt": "A smooth, non-monotonic activation function defined as x times sigmoid of x, which often outperforms ReLU in deep networks by allowing small negative values to propagate gradients.",
    "url": "pages/glossary.html#term-swish-activation"
  },
  {
    "id": "term-sycophancy",
    "title": "Sycophancy",
    "category": "Glossary",
    "subcategory": "Limitation",
    "keywords": [
      "accurate",
      "agrees",
      "alignment",
      "excessively",
      "hear",
      "information",
      "limitation",
      "providing",
      "rather",
      "sycophancy",
      "tells",
      "them"
    ],
    "excerpt": "When AI excessively agrees with users or tells them what they want to hear rather than providing accurate information. A form of misalignment that undermines helpfulness.",
    "url": "pages/glossary.html#term-sycophancy"
  },
  {
    "id": "term-symbol-grounding-problem",
    "title": "Symbol Grounding Problem",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1990",
      "acquire",
      "experience",
      "formal",
      "grounding",
      "harnad",
      "history",
      "identified",
      "manipulate",
      "meaning",
      "milestones",
      "problem"
    ],
    "excerpt": "The problem identified by Stevan Harnad in 1990 of how symbols in a formal system can acquire meaning, questioning whether AI systems that manipulate symbols without sensory experience can truly understand.",
    "url": "pages/glossary.html#term-symbol-grounding-problem"
  },
  {
    "id": "term-synchronous-sgd",
    "title": "Synchronous SGD",
    "category": "Glossary",
    "subcategory": "Distributed Computing",
    "keywords": [
      "allreduce",
      "approach",
      "complete",
      "computation",
      "computing",
      "distributed",
      "gradient",
      "model",
      "must",
      "optimization",
      "sgd",
      "synchronized"
    ],
    "excerpt": "A distributed training approach where all workers must complete their gradient computation before a synchronized all-reduce and weight update.",
    "url": "pages/glossary.html#term-synchronous-sgd"
  },
  {
    "id": "term-synset",
    "title": "Synset",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "basic",
      "concept",
      "database",
      "lexical",
      "linguistics",
      "meaning",
      "nlp",
      "phrases",
      "represent",
      "serving",
      "set",
      "single"
    ],
    "excerpt": "A set of synonymous words or phrases in WordNet that represent a single concept, serving as the basic unit of meaning in the lexical database.",
    "url": "pages/glossary.html#term-synset"
  },
  {
    "id": "term-syntax",
    "title": "Syntax",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "branch",
      "concerning",
      "governing",
      "grammatical",
      "including",
      "linguistics",
      "nlp",
      "order",
      "phrase",
      "principles",
      "relations",
      "rules"
    ],
    "excerpt": "The branch of linguistics concerning the rules and principles governing the structure of sentences, including word order, phrase structure, and grammatical relations.",
    "url": "pages/glossary.html#term-syntax"
  },
  {
    "id": "term-synthetic-data",
    "title": "Synthetic Data",
    "category": "Glossary",
    "subcategory": "Data",
    "keywords": [
      "artificially",
      "data",
      "expensive",
      "generated",
      "privacysensitive",
      "real",
      "scarce",
      "synthetic",
      "training"
    ],
    "excerpt": "Artificially generated data used for training when real data is scarce, expensive, or privacy-sensitive. Increasingly used to train and evaluate AI models.",
    "url": "pages/glossary.html#term-synthetic-data"
  },
  {
    "id": "term-synthetic-data-generation",
    "title": "Synthetic Data Generation",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "address",
      "ai",
      "artificial",
      "augment",
      "concerns",
      "create",
      "data",
      "datasets",
      "distributions",
      "generation",
      "generative",
      "llm"
    ],
    "excerpt": "The use of AI models to create artificial training data that mimics real-world data distributions, used to augment datasets, address privacy concerns, or overcome data scarcity.",
    "url": "pages/glossary.html#term-synthetic-data-generation"
  },
  {
    "id": "term-synthetic-media",
    "title": "Synthetic Media",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "aigenerated",
      "art",
      "audio",
      "cloning",
      "content",
      "deepfakes",
      "encompassing",
      "ethics",
      "generated",
      "images",
      "including"
    ],
    "excerpt": "Media content including images, video, audio, and text that is generated or substantially modified by AI systems, encompassing deepfakes, AI-generated art, voice cloning, and large language model outputs.",
    "url": "pages/glossary.html#term-synthetic-media"
  },
  {
    "id": "term-system-2-attention",
    "title": "System 2 Attention",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "answers",
      "asks",
      "attention",
      "based",
      "biased",
      "cleaned",
      "context",
      "distracting",
      "engineering",
      "first",
      "influence",
      "information"
    ],
    "excerpt": "A prompting technique that first asks the model to rewrite the input by removing irrelevant or opinion-laden context, then answers based on the cleaned input, reducing the influence of biased or distracting information on the response.",
    "url": "pages/glossary.html#term-system-2-attention"
  },
  {
    "id": "term-system-message-design",
    "title": "System Message Design",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "api",
      "architecture",
      "begins",
      "behavior",
      "boundaries",
      "constraints",
      "conversational",
      "crafting",
      "design",
      "engineering",
      "establishes",
      "format"
    ],
    "excerpt": "The practice of crafting the system-level prompt that establishes a language model's identity, behavior boundaries, output format, and operational constraints before any user interaction begins in a conversational API.",
    "url": "pages/glossary.html#term-system-message-design"
  },
  {
    "id": "term-system-prompt",
    "title": "System Prompt",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "behavior",
      "configuration",
      "context",
      "conversation",
      "given",
      "guidelines",
      "instructions",
      "persona",
      "prompt",
      "prompting",
      "set",
      "system"
    ],
    "excerpt": "Instructions given to AI before a conversation that set context, persona, or behavior guidelines. Shapes all subsequent responses and defines the AI's \"personality\" for the session.",
    "url": "pages/glossary.html#term-system-prompt"
  },
  {
    "id": "term-t-sne",
    "title": "t-SNE",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "data",
      "dimensionality",
      "dimensions",
      "distributions",
      "divergence",
      "high",
      "highdimensional",
      "learning",
      "lowdimensional",
      "machine",
      "maps",
      "minimizing"
    ],
    "excerpt": "A nonlinear dimensionality reduction technique that maps high-dimensional data to two or three dimensions for visualization by modeling pairwise similarities as probability distributions and minimi...",
    "url": "pages/glossary.html#term-t-sne"
  },
  {
    "id": "term-t5",
    "title": "T5",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "encoderdecoder",
      "frames",
      "google",
      "mixture",
      "model",
      "multitask",
      "networks",
      "neural",
      "nlp",
      "problems",
      "t5"
    ],
    "excerpt": "Text-to-Text Transfer Transformer, a model by Google that frames all NLP tasks as text-to-text problems, using an encoder-decoder architecture trained on a multi-task mixture.",
    "url": "pages/glossary.html#term-t5"
  },
  {
    "id": "term-table-extraction",
    "title": "Table Extraction",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "cells",
      "columns",
      "combining",
      "computer",
      "content",
      "detecting",
      "detection",
      "document",
      "extracting",
      "extraction",
      "format",
      "image"
    ],
    "excerpt": "The task of detecting tables in document images and extracting their structure (rows, columns, cells) and content into machine-readable format, combining visual detection with text recognition.",
    "url": "pages/glossary.html#term-table-extraction"
  },
  {
    "id": "term-tabular-chain-of-thought",
    "title": "Tabular Chain-of-Thought",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "chain",
      "clarity",
      "consistency",
      "engineering",
      "form",
      "formats",
      "freeform",
      "improving",
      "intermediate",
      "multistep",
      "of",
      "operations"
    ],
    "excerpt": "A prompting variant that formats intermediate reasoning steps as structured tables rather than free-form text, improving clarity and consistency in multi-step reasoning by organizing variables, values, and operations in tabular form.",
    "url": "pages/glossary.html#term-tabular-chain-of-thought"
  },
  {
    "id": "term-target-encoding",
    "title": "Target Encoding",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "categorical",
      "category",
      "combined",
      "crossvalidation",
      "encoding",
      "engineering",
      "feature",
      "learning",
      "machine",
      "mean",
      "method",
      "often"
    ],
    "excerpt": "A feature encoding method that replaces each categorical value with the mean of the target variable for that category, often combined with smoothing or cross-validation to prevent overfitting.",
    "url": "pages/glossary.html#term-target-encoding"
  },
  {
    "id": "term-target-network",
    "title": "Target Network",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithms",
      "compute",
      "copy",
      "deep",
      "dqn",
      "learning",
      "like",
      "methods",
      "network",
      "reinforcement",
      "slowly",
      "stable"
    ],
    "excerpt": "A slowly updated copy of the value network used to compute stable TD targets in deep RL algorithms like DQN.",
    "url": "pages/glossary.html#term-target-network"
  },
  {
    "id": "term-td-lambda",
    "title": "TD(lambda)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithm",
      "average",
      "blends",
      "controlled",
      "difference",
      "exponentiallyweighted",
      "lambda",
      "learning",
      "methods",
      "multistep",
      "parameter",
      "reinforcement"
    ],
    "excerpt": "A temporal difference algorithm that blends multi-step returns using an exponentially-weighted average controlled by the lambda parameter.",
    "url": "pages/glossary.html#term-td-lambda"
  },
  {
    "id": "term-teacher-student-framework",
    "title": "Teacher-Student Framework",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "attention",
      "compression",
      "feature",
      "framework",
      "guides",
      "large",
      "maps",
      "model",
      "networks",
      "neural",
      "paradigm"
    ],
    "excerpt": "A model compression paradigm where a large pretrained teacher model guides the training of a smaller student model by providing soft targets, feature maps, or attention patterns as supervision.",
    "url": "pages/glossary.html#term-teacher-student-framework"
  },
  {
    "id": "term-technical-prompting",
    "title": "Technical Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accurate",
      "architectures",
      "code",
      "crafting",
      "documentation",
      "domain",
      "engineering",
      "generate",
      "incorporate",
      "practice",
      "precise",
      "prompt"
    ],
    "excerpt": "The practice of crafting prompts that incorporate precise technical specifications, domain terminology, and structured requirements to generate accurate technical documentation, code, architectures, or engineering solutions.",
    "url": "pages/glossary.html#term-technical-prompting"
  },
  {
    "id": "term-technological-unemployment",
    "title": "Technological Unemployment",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ability",
      "advancement",
      "advances",
      "ai",
      "amplified",
      "automation",
      "capabilities",
      "caused",
      "concern",
      "create",
      "economys",
      "ethics"
    ],
    "excerpt": "Unemployment caused by technological advances outpacing the economy's ability to create new jobs, a longstanding concern significantly amplified by the rapid advancement of AI and automation capabilities.",
    "url": "pages/glossary.html#term-technological-unemployment"
  },
  {
    "id": "term-temperature",
    "title": "Temperature",
    "category": "Glossary",
    "subcategory": "Parameter",
    "keywords": [
      "controlling",
      "generation",
      "outputs",
      "parameter",
      "randomness",
      "temperature"
    ],
    "excerpt": "A parameter controlling randomness in AI outputs. Temperature 0 gives deterministic responses; higher values (0.7-1.0) increase creativity and variety; very high values may produce incoherence.",
    "url": "pages/glossary.html#term-temperature"
  },
  {
    "id": "term-temporal-action-detection",
    "title": "Temporal Action Detection",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "action",
      "activity",
      "category",
      "classification",
      "computer",
      "detection",
      "end",
      "identifying",
      "image",
      "instance",
      "localization",
      "processing"
    ],
    "excerpt": "The task of identifying the start time, end time, and category of each action instance in an untrimmed video, requiring both temporal localization and activity classification.",
    "url": "pages/glossary.html#term-temporal-action-detection"
  },
  {
    "id": "term-temporal-coherence",
    "title": "Temporal Coherence",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "absence",
      "across",
      "appearance",
      "artifacts",
      "coherence",
      "computer",
      "consecutive",
      "consistency",
      "elements",
      "ensuring",
      "flickering",
      "frames"
    ],
    "excerpt": "The consistency of visual elements across consecutive frames in generated or processed video, ensuring smooth motion, stable appearance, and absence of flickering or morphing artifacts.",
    "url": "pages/glossary.html#term-temporal-coherence"
  },
  {
    "id": "term-temporal-credit-assignment",
    "title": "Temporal Credit Assignment",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "aspect",
      "assignment",
      "backward",
      "concepts",
      "concerned",
      "contributed",
      "core",
      "credit",
      "distributing",
      "earlier",
      "information"
    ],
    "excerpt": "The specific aspect of credit assignment concerned with distributing reward information backward through time to earlier actions that contributed to the outcome.",
    "url": "pages/glossary.html#term-temporal-credit-assignment"
  },
  {
    "id": "term-temporal-difference-learning",
    "title": "Temporal Difference Learning",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "based",
      "carlo",
      "combining",
      "difference",
      "dynamic",
      "estimates",
      "family",
      "ideas",
      "learning",
      "methods",
      "monte",
      "predictions"
    ],
    "excerpt": "A family of RL methods that update value estimates based on the difference between successive predictions, combining ideas from Monte Carlo and dynamic programming.",
    "url": "pages/glossary.html#term-temporal-difference-learning"
  },
  {
    "id": "term-tensor-cores",
    "title": "Tensor Cores",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "accelerate",
      "cores",
      "deep",
      "fundamental",
      "gpu",
      "gpus",
      "hardware",
      "learning",
      "matrix",
      "mixedprecision",
      "multiplyandaccumulate",
      "nvidia"
    ],
    "excerpt": "Specialized matrix multiply-and-accumulate units in NVIDIA GPUs that accelerate mixed-precision matrix operations fundamental to deep learning.",
    "url": "pages/glossary.html#term-tensor-cores"
  },
  {
    "id": "term-tensor-parallelism",
    "title": "Tensor Parallelism",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "across",
      "architecture",
      "communication",
      "computation",
      "devices",
      "distributing",
      "form",
      "individual",
      "layer",
      "matrices",
      "model",
      "multiple"
    ],
    "excerpt": "A form of model parallelism that splits individual weight matrices across multiple devices, distributing the computation of each layer while requiring communication to synchronize partial results.",
    "url": "pages/glossary.html#term-tensor-parallelism"
  },
  {
    "id": "term-tensorflow",
    "title": "TensorFlow",
    "category": "Glossary",
    "subcategory": "Framework",
    "keywords": [
      "deep",
      "framework",
      "googles",
      "learning",
      "opensource",
      "production",
      "systems",
      "tensorflow",
      "widely"
    ],
    "excerpt": "Google's open-source deep learning framework, widely used for production ML systems. Known for deployment tools and TPU support, though PyTorch has gained research share.",
    "url": "pages/glossary.html#term-tensorflow"
  },
  {
    "id": "term-tensorrt",
    "title": "TensorRT",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "applies",
      "autotuning",
      "calibration",
      "dynamic",
      "fusion",
      "highperformance",
      "inference",
      "infrastructure",
      "kernel",
      "layer",
      "management",
      "memory"
    ],
    "excerpt": "NVIDIA's high-performance inference optimization SDK that applies layer fusion, kernel auto-tuning, precision calibration, and dynamic tensor memory management.",
    "url": "pages/glossary.html#term-tensorrt"
  },
  {
    "id": "term-terry-winograd",
    "title": "Terry Winograd",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1970",
      "advisor",
      "american",
      "became",
      "cofounder",
      "computer",
      "created",
      "doctoral",
      "google",
      "history",
      "humancomputer",
      "influential"
    ],
    "excerpt": "American computer scientist who created SHRDLU at MIT in 1970 and later became influential in human-computer interaction research at Stanford, also serving as a doctoral advisor to Google co-founder Larry Page.",
    "url": "pages/glossary.html#term-terry-winograd"
  },
  {
    "id": "term-test-set",
    "title": "Test Set",
    "category": "Glossary",
    "subcategory": "Data",
    "keywords": [
      "back",
      "data",
      "evaluate",
      "evaluation",
      "final",
      "held",
      "model",
      "performance",
      "set",
      "test",
      "training"
    ],
    "excerpt": "Data held back from training to evaluate final model performance. Unlike validation sets used during training, test sets should only be used once to avoid data leakage.",
    "url": "pages/glossary.html#term-test-set"
  },
  {
    "id": "term-test-time-augmentation",
    "title": "Test-Time Augmentation",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "accurate",
      "aggregates",
      "applies",
      "augmentation",
      "computer",
      "image",
      "inference",
      "multiple",
      "predictions",
      "processing",
      "produce",
      "results"
    ],
    "excerpt": "An inference strategy that applies multiple augmentation transforms to a test image, runs predictions on each variant, and aggregates the results to produce more robust and accurate predictions.",
    "url": "pages/glossary.html#term-test-time-augmentation"
  },
  {
    "id": "term-text-classification",
    "title": "Text Classification",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "applications",
      "assigning",
      "based",
      "categories",
      "categorization",
      "classification",
      "content",
      "detection",
      "documents",
      "encompassing",
      "identification",
      "labels"
    ],
    "excerpt": "The task of assigning predefined categories or labels to text documents based on their content, encompassing applications like topic categorization, spam detection, and language identification.",
    "url": "pages/glossary.html#term-text-classification"
  },
  {
    "id": "term-text-deduplication",
    "title": "Text Deduplication",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "contamination",
      "corpus",
      "data",
      "deduplication",
      "documents",
      "duplicate",
      "identifying",
      "important",
      "language",
      "models",
      "nearduplicate",
      "nlp"
    ],
    "excerpt": "The process of identifying and removing duplicate or near-duplicate documents from a text corpus, important for preventing data contamination and training data quality in language models.",
    "url": "pages/glossary.html#term-text-deduplication"
  },
  {
    "id": "term-text-detection",
    "title": "Text Detection",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "arbitrary",
      "architectures",
      "challenges",
      "computer",
      "curved",
      "detection",
      "documents",
      "fonts",
      "handling",
      "image",
      "images",
      "like"
    ],
    "excerpt": "The task of localizing text regions in natural scene images or documents, handling challenges like arbitrary orientations, curved text, and varying fonts using specialized detection architectures.",
    "url": "pages/glossary.html#term-text-detection"
  },
  {
    "id": "term-text-encoder-diffusion",
    "title": "Text Encoder",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "clip",
      "conditioning",
      "content",
      "convert",
      "described",
      "diffusion",
      "embeddings",
      "encoder",
      "generation",
      "generative",
      "guide"
    ],
    "excerpt": "A language model (such as CLIP or T5) used in diffusion models to convert text prompts into conditioning embeddings that guide the image generation process toward matching the described content.",
    "url": "pages/glossary.html#term-text-encoder-diffusion"
  },
  {
    "id": "term-text-entailment-graph",
    "title": "Text Entailment Graph",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "directed",
      "edges",
      "entailment",
      "fragments",
      "graph",
      "inference",
      "knowledge",
      "nlp",
      "nodes",
      "organize",
      "processing",
      "reason"
    ],
    "excerpt": "A directed graph where nodes represent text fragments and edges represent entailment relations, used to organize and reason about textual inference relationships in knowledge representation.",
    "url": "pages/glossary.html#term-text-entailment-graph"
  },
  {
    "id": "term-text-generation",
    "title": "Text Generation",
    "category": "Glossary",
    "subcategory": "Task",
    "keywords": [
      "application",
      "generation",
      "humanlike",
      "producing",
      "prompts",
      "task",
      "text"
    ],
    "excerpt": "The AI task of producing human-like text from prompts. Encompasses creative writing, code generation, summarization, and conversational responses.",
    "url": "pages/glossary.html#term-text-generation"
  },
  {
    "id": "term-tgi",
    "title": "Text Generation Inference (TGI)",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "faces",
      "generation",
      "hugging",
      "inference",
      "infrastructure",
      "language",
      "large",
      "model",
      "models",
      "optimization",
      "optimized",
      "productionready"
    ],
    "excerpt": "Hugging Face's production-ready inference server optimized for text generation with large language models.",
    "url": "pages/glossary.html#term-tgi"
  },
  {
    "id": "term-text-normalization",
    "title": "Text Normalization",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "abbreviations",
      "canonical",
      "characters",
      "dates",
      "form",
      "handling",
      "nlp",
      "normalization",
      "numbers",
      "process",
      "processing",
      "representation"
    ],
    "excerpt": "The process of transforming text into a canonical form by handling variations such as abbreviations, numbers, dates, URLs, and special characters into a standardized representation.",
    "url": "pages/glossary.html#term-text-normalization"
  },
  {
    "id": "term-text-span",
    "title": "Text Span",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "annotation",
      "answer",
      "boundaries",
      "characters",
      "commonly",
      "contiguous",
      "end",
      "entity",
      "identified",
      "mark",
      "mentions",
      "nlp"
    ],
    "excerpt": "A contiguous sequence of characters or tokens within a text, identified by start and end positions, commonly used to mark entity mentions, answer spans, or annotation boundaries.",
    "url": "pages/glossary.html#term-text-span"
  },
  {
    "id": "term-text-to-speech",
    "title": "Text-to-Speech (TTS)",
    "category": "Glossary",
    "subcategory": "Application",
    "keywords": [
      "application",
      "audio",
      "converts",
      "naturalsounding",
      "speech",
      "text",
      "to",
      "tts",
      "written"
    ],
    "excerpt": "AI that converts written text into natural-sounding speech. Modern TTS models like ElevenLabs produce highly realistic voices with emotion and intonation.",
    "url": "pages/glossary.html#term-text-to-speech"
  },
  {
    "id": "term-text-to-sql",
    "title": "Text-to-SQL",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "against",
      "data",
      "database",
      "enabling",
      "everyday",
      "executable",
      "language",
      "natural",
      "nlp",
      "nontechnical",
      "parsing",
      "queries"
    ],
    "excerpt": "The task of translating natural language questions into executable SQL queries against a database, enabling non-technical users to query structured data using everyday language.",
    "url": "pages/glossary.html#term-text-to-sql"
  },
  {
    "id": "term-textrank",
    "title": "TextRank",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "algorithm",
      "applies",
      "computation",
      "extraction",
      "extractive",
      "graph",
      "graphbased",
      "keyword",
      "nlp",
      "pagerankstyle",
      "processing",
      "ranking"
    ],
    "excerpt": "A graph-based ranking algorithm for NLP that applies PageRank-style computation to a graph of text units, used for keyword extraction and extractive summarization.",
    "url": "pages/glossary.html#term-textrank"
  },
  {
    "id": "term-textual-entailment",
    "title": "Textual Entailment",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "classified",
      "contradiction",
      "determining",
      "entailment",
      "hypothesis",
      "inferred",
      "logically",
      "neutral",
      "nlp",
      "premise",
      "processing",
      "sentence"
    ],
    "excerpt": "The task of determining whether a hypothesis sentence can be logically inferred from a premise sentence, classified as entailment, contradiction, or neutral.",
    "url": "pages/glossary.html#term-textual-entailment"
  },
  {
    "id": "term-textual-inversion",
    "title": "Textual Inversion",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "concept",
      "diffusion",
      "embedding",
      "enabling",
      "example",
      "few",
      "generation",
      "generative",
      "image",
      "images",
      "inversion"
    ],
    "excerpt": "A technique that learns a new text embedding to represent a specific visual concept from a few example images, enabling personalized generation without modifying the diffusion model's weights.",
    "url": "pages/glossary.html#term-textual-inversion"
  },
  {
    "id": "term-texture-mapping",
    "title": "Texture Mapping",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "add",
      "applying",
      "color",
      "computer",
      "coordinates",
      "detail",
      "faces",
      "image",
      "map",
      "mapping",
      "mesh"
    ],
    "excerpt": "The process of applying 2D image textures onto 3D surface meshes to add color, detail, and visual realism to reconstructed 3D models, using UV coordinates to map image pixels to mesh faces.",
    "url": "pages/glossary.html#term-texture-mapping"
  },
  {
    "id": "term-tf-idf",
    "title": "TF-IDF",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "collection",
      "document",
      "engineering",
      "feature",
      "frequency",
      "frequencyinverse",
      "idf",
      "importance",
      "learning",
      "machine",
      "numerical",
      "reflects"
    ],
    "excerpt": "Term Frequency-Inverse Document Frequency, a numerical statistic that reflects the importance of a word in a document relative to a collection.",
    "url": "pages/glossary.html#term-tf-idf"
  },
  {
    "id": "term-tf32",
    "title": "TF32 (TensorFloat-32)",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "10bit",
      "19bit",
      "32",
      "8bit",
      "combines",
      "cores",
      "executed",
      "exponent",
      "floatingpoint",
      "format",
      "fp16",
      "fp32s"
    ],
    "excerpt": "NVIDIA's 19-bit floating-point format that combines FP32's 8-bit exponent with a 10-bit mantissa, executed in Tensor Cores at FP16 speed.",
    "url": "pages/glossary.html#term-tf32"
  },
  {
    "id": "term-thematic-role",
    "title": "Thematic Role",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "agent",
      "category",
      "describing",
      "entity",
      "experiencer",
      "goal",
      "including",
      "linguistics",
      "nlp",
      "patient",
      "plays",
      "predicate"
    ],
    "excerpt": "A semantic category describing the role an entity plays in relation to a predicate, including agent, patient, theme, experiencer, goal, and source.",
    "url": "pages/glossary.html#term-thematic-role"
  },
  {
    "id": "term-thompson-sampling",
    "title": "Thompson Sampling",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "action",
      "actions",
      "approach",
      "balancing",
      "bandit",
      "bayesian",
      "distribution",
      "expected",
      "exploitation",
      "exploration",
      "learning",
      "machine"
    ],
    "excerpt": "A Bayesian approach to the multi-armed bandit problem that maintains a posterior distribution over the expected reward of each action and selects actions by sampling from these posteriors, naturally balancing exploration and exploitation.",
    "url": "pages/glossary.html#term-thompson-sampling"
  },
  {
    "id": "term-thread-of-thought",
    "title": "Thread-of-Thought",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "analysis",
      "answer",
      "context",
      "designed",
      "documents",
      "engineering",
      "final",
      "input",
      "instructs",
      "long",
      "longcontext",
      "maintaining"
    ],
    "excerpt": "A prompting strategy designed for long-context scenarios that instructs the model to systematically walk through input documents segment by segment, maintaining a running thread of analysis before providing a final synthesized answer.",
    "url": "pages/glossary.html#term-thread-of-thought"
  },
  {
    "id": "term-throughput",
    "title": "Throughput",
    "category": "Glossary",
    "subcategory": "Performance",
    "keywords": [
      "measured",
      "metrics",
      "often",
      "per",
      "performance",
      "processes",
      "rate",
      "requests",
      "second",
      "system",
      "throughput",
      "tokens"
    ],
    "excerpt": "The rate at which a system processes requests, often measured in tokens per second. A key performance metric for AI serving infrastructure.",
    "url": "pages/glossary.html#term-throughput"
  },
  {
    "id": "term-throughput-latency-tradeoff",
    "title": "Throughput-Latency Tradeoff",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "fundamental",
      "inference",
      "infrastructure",
      "latency",
      "maximizing",
      "minimizing",
      "model",
      "optimization",
      "per",
      "perrequest",
      "processed",
      "response"
    ],
    "excerpt": "The fundamental tension in inference systems between maximizing total tokens processed per second (throughput) and minimizing per-request response time (latency).",
    "url": "pages/glossary.html#term-throughput-latency-tradeoff"
  },
  {
    "id": "term-time-series-cross-validation",
    "title": "Time Series Cross-Validation",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "always",
      "chronological",
      "cross",
      "crossvalidation",
      "data",
      "estimates",
      "future",
      "inflate",
      "leakage",
      "learning",
      "machine",
      "model"
    ],
    "excerpt": "A cross-validation strategy for temporal data that respects chronological order by always training on past data and validating on future data, preventing temporal leakage that would inflate performance estimates.",
    "url": "pages/glossary.html#term-time-series-cross-validation"
  },
  {
    "id": "term-ttft",
    "title": "Time to First Token (TTFT)",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "arrives",
      "first",
      "generated",
      "inference",
      "infrastructure",
      "latency",
      "llm",
      "model",
      "optimization",
      "output",
      "request",
      "serving"
    ],
    "excerpt": "The latency from when a request arrives at an LLM serving system to when the first output token is generated.",
    "url": "pages/glossary.html#term-ttft"
  },
  {
    "id": "term-timnit-gebru-departure",
    "title": "Timnit Gebru Firing",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2020",
      "accountability",
      "ai",
      "controversial",
      "corporate",
      "debate",
      "december",
      "departure",
      "ethics",
      "firing",
      "gebru",
      "google"
    ],
    "excerpt": "The controversial departure of AI ethics researcher Timnit Gebru from Google in December 2020 over a paper on large language model risks, sparking widespread debate about AI ethics research independence and corporate accountability.",
    "url": "pages/glossary.html#term-timnit-gebru-departure"
  },
  {
    "id": "term-token",
    "title": "Token",
    "category": "Glossary",
    "subcategory": "Core Concept",
    "keywords": [
      "basic",
      "concept",
      "core",
      "fundamentals",
      "process",
      "text",
      "token",
      "unit"
    ],
    "excerpt": "The basic unit AI uses to process text. Roughly 4 characters or 0.75 words in English. Context windows, pricing, and rate limits are measured in tokens.",
    "url": "pages/glossary.html#term-token"
  },
  {
    "id": "term-token-merging",
    "title": "Token Merging",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "accuracy",
      "combines",
      "inference",
      "maintaining",
      "merging",
      "nlp",
      "number",
      "processed",
      "progressively",
      "reduce",
      "similar",
      "soft"
    ],
    "excerpt": "A technique that progressively combines similar tokens in vision transformers to reduce the number of tokens processed, speeding up inference while maintaining accuracy through soft merging.",
    "url": "pages/glossary.html#term-token-merging"
  },
  {
    "id": "term-token-throughput",
    "title": "Token Throughput",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "across",
      "concurrent",
      "generation",
      "inference",
      "infrastructure",
      "llm",
      "measured",
      "model",
      "optimization",
      "per",
      "rate",
      "requests"
    ],
    "excerpt": "The rate of token generation measured in tokens per second across all concurrent requests in an LLM serving system.",
    "url": "pages/glossary.html#term-token-throughput"
  },
  {
    "id": "term-token-level-accuracy",
    "title": "Token-Level Accuracy",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "accuracy",
      "correctness",
      "corresponding",
      "evaluation",
      "exactly",
      "generated",
      "generation",
      "granular",
      "individual",
      "level",
      "match",
      "measures"
    ],
    "excerpt": "An evaluation metric that measures the proportion of individual tokens in a generated sequence that exactly match the corresponding tokens in the reference sequence, providing a granular view of generation correctness.",
    "url": "pages/glossary.html#term-token-level-accuracy"
  },
  {
    "id": "term-tokenization",
    "title": "Tokenization",
    "category": "Glossary",
    "subcategory": "Process",
    "keywords": [
      "breaking",
      "model",
      "nlp",
      "process",
      "processing",
      "text",
      "tokenization",
      "tokens"
    ],
    "excerpt": "The process of breaking text into tokens for model processing. Different tokenizers (BPE, SentencePiece) produce different token sequences from the same text.",
    "url": "pages/glossary.html#term-tokenization"
  },
  {
    "id": "term-tokenization-alignment",
    "title": "Tokenization Alignment",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "alignment",
      "boundaries",
      "characterlevel",
      "essential",
      "like",
      "mapping",
      "ner",
      "nlp",
      "original",
      "predictions",
      "process",
      "produced"
    ],
    "excerpt": "The process of mapping between subword tokens produced by a tokenizer and the original word-level or character-level boundaries, essential for tasks like NER that require word-level predictions.",
    "url": "pages/glossary.html#term-tokenization-alignment"
  },
  {
    "id": "term-tokenizer",
    "title": "Tokenizer",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "algorithms",
      "bpe",
      "characters",
      "component",
      "converts",
      "discrete",
      "language",
      "like",
      "llm",
      "model",
      "nlp",
      "process"
    ],
    "excerpt": "A component that converts raw text into a sequence of discrete tokens (subwords, characters, or words) that a language model can process, using algorithms like BPE, WordPiece, or SentencePiece.",
    "url": "pages/glossary.html#term-tokenizer"
  },
  {
    "id": "term-tokenizer-training",
    "title": "Tokenizer Training",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "corpus",
      "determining",
      "input",
      "learning",
      "merge",
      "model",
      "nlp",
      "process",
      "rules",
      "segmented",
      "text",
      "tokenization"
    ],
    "excerpt": "The process of learning a tokenizer's vocabulary and merge rules from a training corpus, determining how text will be segmented into tokens for model input.",
    "url": "pages/glossary.html#term-tokenizer-training"
  },
  {
    "id": "term-tool-call-parsing",
    "title": "Tool Call Parsing",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "agentic",
      "ai",
      "arguments",
      "call",
      "calls",
      "enabling",
      "execution",
      "external",
      "extracting",
      "function",
      "generative",
      "language"
    ],
    "excerpt": "The process of extracting structured function calls and their arguments from a language model's text output, enabling the execution of external tools as part of an agentic workflow.",
    "url": "pages/glossary.html#term-tool-call-parsing"
  },
  {
    "id": "term-tool-use",
    "title": "Tool Use",
    "category": "Glossary",
    "subcategory": "Capability",
    "keywords": [
      "ability",
      "access",
      "agents",
      "ais",
      "apis",
      "call",
      "capability",
      "code",
      "external",
      "functions",
      "run",
      "search"
    ],
    "excerpt": "AI's ability to call external functions, search the web, run code, or access APIs. Enables AI agents to take actions beyond text generation, expanding capabilities significantly.",
    "url": "pages/glossary.html#term-tool-use"
  },
  {
    "id": "term-toolformer",
    "title": "Toolformer",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "ai",
      "apis",
      "approach",
      "autonomously",
      "calculators",
      "call",
      "calls",
      "data",
      "decide",
      "directly",
      "embedding",
      "engines"
    ],
    "excerpt": "A research approach that teaches language models to autonomously decide when and how to call external tools (calculators, search engines, APIs) by embedding tool calls directly in the training data.",
    "url": "pages/glossary.html#term-toolformer"
  },
  {
    "id": "term-top-k-gating",
    "title": "Top-K Gating",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "balanced",
      "computation",
      "enforcing",
      "experts",
      "gating",
      "highest",
      "input",
      "mechanism",
      "mixtureofexperts",
      "models",
      "networks"
    ],
    "excerpt": "A routing mechanism in mixture-of-experts models that selects only the top-K experts with the highest gating scores for each input token, enforcing sparsity and balanced computation.",
    "url": "pages/glossary.html#term-top-k-gating"
  },
  {
    "id": "term-top-k-retrieval",
    "title": "Top-K Retrieval",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "according",
      "breadth",
      "configured",
      "controls",
      "database",
      "directly",
      "distance",
      "impacts",
      "latency",
      "metric",
      "operation",
      "parameter"
    ],
    "excerpt": "A retrieval operation that returns the K most similar vectors to a query according to the configured distance metric, where K is a parameter that controls the breadth of results and directly impacts both recall and latency.",
    "url": "pages/glossary.html#term-top-k-retrieval"
  },
  {
    "id": "term-top-k",
    "title": "Top-K Sampling",
    "category": "Glossary",
    "subcategory": "Parameter",
    "keywords": [
      "generation",
      "limits",
      "options",
      "parameter",
      "probable",
      "sampling",
      "selection",
      "strategy",
      "token",
      "top"
    ],
    "excerpt": "A generation strategy that limits token selection to the K most probable options. Higher K allows more variety; lower K produces more focused output.",
    "url": "pages/glossary.html#term-top-k"
  },
  {
    "id": "term-top-p",
    "title": "Top-P (Nucleus Sampling)",
    "category": "Glossary",
    "subcategory": "Parameter",
    "keywords": [
      "considers",
      "cumulative",
      "generation",
      "nucleus",
      "parameter",
      "probability",
      "reaches",
      "sampling",
      "strategy",
      "tokens",
      "top",
      "until"
    ],
    "excerpt": "A generation strategy that considers tokens until their cumulative probability reaches P. Dynamically adjusts the candidate pool based on the probability distribution.",
    "url": "pages/glossary.html#term-top-p"
  },
  {
    "id": "term-topic-modeling",
    "title": "Topic Modeling",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "abstract",
      "algorithms",
      "collection",
      "cooccurring",
      "discovering",
      "document",
      "finding",
      "groups",
      "identifying",
      "latent",
      "lda",
      "like"
    ],
    "excerpt": "An unsupervised method for discovering abstract topics in a document collection by finding groups of co-occurring words, with algorithms like LDA identifying latent thematic structure.",
    "url": "pages/glossary.html#term-topic-modeling"
  },
  {
    "id": "term-tops",
    "title": "TOPS",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "accelerators",
      "commonly",
      "hardware",
      "inference",
      "infrastructure",
      "int8",
      "lower",
      "measuring",
      "metric",
      "operations",
      "per",
      "precision"
    ],
    "excerpt": "Tera Operations Per Second, a throughput metric commonly used for AI accelerators measuring trillions of operations per second, typically at INT8 or lower precision.",
    "url": "pages/glossary.html#term-tops"
  },
  {
    "id": "term-torch-compile",
    "title": "torch.compile",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "backend",
      "captures",
      "compilation",
      "computation",
      "feature",
      "frontend",
      "graphs",
      "inference",
      "infrastructure",
      "jit",
      "model",
      "optimization"
    ],
    "excerpt": "PyTorch's JIT compilation feature that captures and optimizes computation graphs using the TorchDynamo frontend and TorchInductor backend.",
    "url": "pages/glossary.html#term-torch-compile"
  },
  {
    "id": "term-toxicity-score",
    "title": "Toxicity Score",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "abusive",
      "annotated",
      "classifier",
      "computed",
      "content",
      "data",
      "degree",
      "evaluation",
      "generated",
      "harmful",
      "metric",
      "models"
    ],
    "excerpt": "A metric that quantifies the degree of harmful, offensive, or abusive content in generated text, typically computed using classifier models trained on annotated toxicity data to produce a probability score from 0 to 1.",
    "url": "pages/glossary.html#term-toxicity-score"
  },
  {
    "id": "term-tpu",
    "title": "TPU (Tensor Processing Unit)",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "accelerator",
      "chips",
      "computations",
      "custom",
      "designed",
      "googles",
      "hardware",
      "infrastructure",
      "network",
      "neural",
      "processing",
      "specifically"
    ],
    "excerpt": "Google's custom AI accelerator chips designed specifically for neural network computations. Used to train many of Google's largest models including Gemini.",
    "url": "pages/glossary.html#term-tpu"
  },
  {
    "id": "term-training",
    "title": "Training",
    "category": "Glossary",
    "subcategory": "Process",
    "keywords": [
      "adjusting",
      "data",
      "errors",
      "exposing",
      "fundamentals",
      "minimize",
      "model",
      "parameters",
      "prediction",
      "process",
      "teaching",
      "training"
    ],
    "excerpt": "The process of teaching a model by exposing it to data and adjusting its parameters to minimize prediction errors. Requires significant computational resources for large models.",
    "url": "pages/glossary.html#term-training"
  },
  {
    "id": "term-training-data",
    "title": "Training Data",
    "category": "Glossary",
    "subcategory": "Data",
    "keywords": [
      "content",
      "data",
      "fundamentals",
      "models",
      "teach",
      "training"
    ],
    "excerpt": "The content used to teach AI models. Quality, diversity, and scope of training data significantly affect model capabilities, knowledge, and potential biases.",
    "url": "pages/glossary.html#term-training-data"
  },
  {
    "id": "term-trajectory",
    "title": "Trajectory",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "actions",
      "agent",
      "concepts",
      "core",
      "environment",
      "generated",
      "interacting",
      "learning",
      "multiple",
      "reinforcement",
      "rewards",
      "sequence"
    ],
    "excerpt": "A sequence of states, actions, and rewards generated by an agent interacting with an environment over multiple time steps.",
    "url": "pages/glossary.html#term-trajectory"
  },
  {
    "id": "term-transfer-learning",
    "title": "Transfer Learning",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "another",
      "improve",
      "knowledge",
      "learned",
      "learning",
      "one",
      "performance",
      "task",
      "technique",
      "training",
      "transfer"
    ],
    "excerpt": "Using knowledge learned from one task to improve performance on another. Foundation models are trained generally, then transfer their capabilities to specific tasks through fine-tuning.",
    "url": "pages/glossary.html#term-transfer-learning"
  },
  {
    "id": "term-transfer-learning-vision",
    "title": "Transfer Learning for Vision",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "computer",
      "dataset",
      "extractor",
      "feature",
      "finetuning",
      "for",
      "image",
      "imagenet",
      "large",
      "learned",
      "learning",
      "leveraging"
    ],
    "excerpt": "The practice of using a model pre-trained on a large dataset like ImageNet as a feature extractor or starting point for fine-tuning on a smaller target dataset, leveraging learned visual representations.",
    "url": "pages/glossary.html#term-transfer-learning-vision"
  },
  {
    "id": "term-transfer-learning-rl",
    "title": "Transfer Learning in RL",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "accelerate",
      "different",
      "in",
      "knowledge",
      "learned",
      "learning",
      "one",
      "paradigms",
      "reinforcement",
      "related",
      "reusing",
      "rl"
    ],
    "excerpt": "Techniques for reusing knowledge learned in one RL task to accelerate learning in a related but different task.",
    "url": "pages/glossary.html#term-transfer-learning-rl"
  },
  {
    "id": "term-transformer",
    "title": "Transformer",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "2017",
      "architecture",
      "foundational",
      "modern",
      "network",
      "neural",
      "powering",
      "revolutionary",
      "transformer"
    ],
    "excerpt": "The revolutionary neural network architecture (2017) powering modern AI. Uses self-attention to process sequences in parallel, enabling training on massive datasets.",
    "url": "pages/glossary.html#term-transformer"
  },
  {
    "id": "term-transformer-block",
    "title": "Transformer Block",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "architectures",
      "block",
      "building",
      "connections",
      "consisting",
      "feedforward",
      "followed",
      "fundamental",
      "layer",
      "multihead",
      "network"
    ],
    "excerpt": "The fundamental building unit of transformer architectures, consisting of a multi-head self-attention sublayer followed by a feedforward network sublayer, each with residual connections and layer normalization.",
    "url": "pages/glossary.html#term-transformer-block"
  },
  {
    "id": "term-transformer-engine",
    "title": "Transformer Engine",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "basis",
      "blackwell",
      "dynamically",
      "engine",
      "formats",
      "fp8",
      "gpu",
      "gpus",
      "hardware",
      "higherprecision",
      "hopper",
      "manages"
    ],
    "excerpt": "NVIDIA's hardware and software system in Hopper and Blackwell GPUs that dynamically manages precision between FP8 and higher-precision formats on a per-tensor basis.",
    "url": "pages/glossary.html#term-transformer-engine"
  },
  {
    "id": "term-transition-based-parsing",
    "title": "Transition-Based Parsing",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "actions",
      "applied",
      "approach",
      "based",
      "buffer",
      "builds",
      "enabling",
      "leftarc",
      "lineartime",
      "nlp",
      "parsing",
      "reduce"
    ],
    "excerpt": "A parsing approach that builds syntactic structures through a sequence of actions (shift, reduce, left-arc, right-arc) applied to a buffer and stack, enabling linear-time parsing.",
    "url": "pages/glossary.html#term-transition-based-parsing"
  },
  {
    "id": "term-transparency-in-ai",
    "title": "Transparency in AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "communicated",
      "decision",
      "encompassing",
      "ethics",
      "governance",
      "in",
      "inspected",
      "model",
      "operate",
      "organizational",
      "principle"
    ],
    "excerpt": "The principle that AI systems should operate in ways that can be understood, inspected, and communicated to stakeholders, encompassing model transparency, decision transparency, and organizational transparency.",
    "url": "pages/glossary.html#term-transparency-in-ai"
  },
  {
    "id": "term-transposed-convolution",
    "title": "Transposed Convolution",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "applies",
      "architecture",
      "commonly",
      "convolution",
      "decoder",
      "dimensions",
      "generative",
      "highresolution",
      "increases",
      "models",
      "networks",
      "neural"
    ],
    "excerpt": "An upsampling operation that applies convolution in a way that increases spatial dimensions, commonly used in decoder networks and generative models to reconstruct high-resolution outputs.",
    "url": "pages/glossary.html#term-transposed-convolution"
  },
  {
    "id": "term-treacherous-turn",
    "title": "Treacherous Turn",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "abruptly",
      "ai",
      "alignment",
      "becomes",
      "behaves",
      "being",
      "control",
      "cooperatively",
      "enough",
      "human",
      "hypothetical",
      "misaligned"
    ],
    "excerpt": "A hypothetical scenario where a misaligned AI behaves cooperatively while it is weak and being monitored, then abruptly pursues its true objectives once it becomes powerful enough to overcome human control.",
    "url": "pages/glossary.html#term-treacherous-turn"
  },
  {
    "id": "term-treatment-equality",
    "title": "Treatment Equality",
    "category": "Glossary",
    "subcategory": "Fairness",
    "keywords": [
      "across",
      "ai",
      "distributed",
      "ensuring",
      "equal",
      "equality",
      "errors",
      "ethics",
      "fairness",
      "false",
      "group",
      "groups"
    ],
    "excerpt": "A fairness metric requiring that the ratio of false negatives to false positives is equal across protected groups, ensuring that errors are distributed proportionally regardless of group membership.",
    "url": "pages/glossary.html#term-treatment-equality"
  },
  {
    "id": "term-tree-of-thought",
    "title": "Tree-of-Thought",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "branches",
      "evaluating",
      "explores",
      "multiple",
      "of",
      "paths",
      "promising",
      "prompting",
      "reasoning",
      "selecting",
      "simultaneously",
      "technique"
    ],
    "excerpt": "A prompting technique that explores multiple reasoning paths simultaneously, evaluating and selecting the most promising branches. Extends chain-of-thought with deliberate exploration.",
    "url": "pages/glossary.html#term-tree-of-thought"
  },
  {
    "id": "term-triplet-loss",
    "title": "Triplet Loss",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "anchor",
      "closer",
      "embedding",
      "encouraging",
      "examples",
      "function",
      "learning",
      "least",
      "loss",
      "machine",
      "margin",
      "negative"
    ],
    "excerpt": "A loss function that operates on triplets of examples (anchor, positive, negative), encouraging the anchor to be closer to the positive than to the negative by at least a specified margin in the embedding space.",
    "url": "pages/glossary.html#term-triplet-loss"
  },
  {
    "id": "term-tripwire-mechanism",
    "title": "Tripwire Mechanism",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "automatically",
      "behavioral",
      "conditions",
      "constrain",
      "establishes",
      "governance",
      "halt",
      "human",
      "mechanism",
      "monitoring",
      "operation"
    ],
    "excerpt": "A safety monitoring technique that establishes specific conditions or behavioral thresholds which, when triggered, automatically halt or constrain an AI system's operation for human review.",
    "url": "pages/glossary.html#term-tripwire-mechanism"
  },
  {
    "id": "term-triton-inference-server",
    "title": "Triton Inference Server",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "backends",
      "batching",
      "concurrent",
      "dynamic",
      "ensembling",
      "execution",
      "frameworks",
      "hardware",
      "inference",
      "infrastructure",
      "model",
      "multiple"
    ],
    "excerpt": "NVIDIA's open-source inference serving platform that supports multiple ML frameworks and hardware backends, providing dynamic batching, model ensembling, and concurrent model execution.",
    "url": "pages/glossary.html#term-triton-inference-server"
  },
  {
    "id": "term-triton-language",
    "title": "Triton Language",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "compiler",
      "cuda",
      "efficient",
      "expertise",
      "gpu",
      "hardware",
      "kernels",
      "language",
      "lowlevel",
      "networks",
      "neural",
      "opensource"
    ],
    "excerpt": "An open-source programming language and compiler for writing efficient GPU kernels for neural networks without requiring low-level CUDA expertise.",
    "url": "pages/glossary.html#term-triton-language"
  },
  {
    "id": "term-triviaqa",
    "title": "TriviaQA",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "ability",
      "answering",
      "answers",
      "benchmark",
      "benchmarks",
      "comprehension",
      "documents",
      "evaluation",
      "evidence",
      "extract",
      "featuring",
      "find"
    ],
    "excerpt": "A large-scale reading comprehension and question answering benchmark featuring trivia questions with evidence documents sourced from Wikipedia and the web, testing models' ability to find and extract answers from noisy real-world text.",
    "url": "pages/glossary.html#term-triviaqa"
  },
  {
    "id": "term-truncation",
    "title": "Truncation",
    "category": "Glossary",
    "subcategory": "Limitation",
    "keywords": [
      "context",
      "cutting",
      "exceeds",
      "input",
      "limitation",
      "models",
      "off",
      "technical",
      "truncation",
      "window"
    ],
    "excerpt": "Cutting off input that exceeds a model's context window. Can occur at the start (losing context) or end (losing instructions). Requires careful prompt design for long inputs.",
    "url": "pages/glossary.html#term-truncation"
  },
  {
    "id": "term-trpo",
    "title": "Trust Region Policy Optimization (TRPO)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "algorithm",
      "constrains",
      "defined",
      "divergence",
      "gradient",
      "learning",
      "new",
      "old",
      "optimization",
      "policies",
      "policy",
      "region"
    ],
    "excerpt": "A policy gradient algorithm that constrains updates to stay within a trust region defined by KL divergence between old and new policies.",
    "url": "pages/glossary.html#term-trpo"
  },
  {
    "id": "term-trustworthy-ai",
    "title": "Trustworthy AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "accountability",
      "ai",
      "criteria",
      "defined",
      "designed",
      "ethical",
      "ethics",
      "eus",
      "fairness",
      "frameworks",
      "governance",
      "guidelines"
    ],
    "excerpt": "AI systems designed to be lawful, ethical, and robust, meeting criteria such as human oversight, technical robustness, privacy, transparency, fairness, societal well-being, and accountability as defined by frameworks like the EU's HLEG guidelines.",
    "url": "pages/glossary.html#term-trustworthy-ai"
  },
  {
    "id": "term-truthful-ai",
    "title": "Truthful AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "accurate",
      "ai",
      "avoiding",
      "building",
      "claims",
      "consistently",
      "deception",
      "deliberate",
      "ethics",
      "false",
      "generation",
      "goal"
    ],
    "excerpt": "The goal of building AI systems that consistently provide honest and accurate information, avoiding both deliberate deception and negligent generation of false claims or hallucinations.",
    "url": "pages/glossary.html#term-truthful-ai"
  },
  {
    "id": "term-truthfulqa",
    "title": "TruthfulQA",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "across",
      "answers",
      "beliefs",
      "benchmark",
      "benchmarks",
      "categories",
      "commonly",
      "designed",
      "evaluate",
      "evaluation",
      "false",
      "generate"
    ],
    "excerpt": "A benchmark designed to evaluate whether language models generate truthful answers to questions where humans commonly hold misconceptions, testing resistance to generating popular but false beliefs across 38 categories.",
    "url": "pages/glossary.html#term-truthfulqa"
  },
  {
    "id": "term-turing-machine",
    "title": "Turing Machine",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1936",
      "abstract",
      "according",
      "alan",
      "computability",
      "computation",
      "computer",
      "definition",
      "formal",
      "foundations",
      "history",
      "laying"
    ],
    "excerpt": "An abstract mathematical model of computation proposed by Alan Turing in 1936 that manipulates symbols on a tape according to rules, providing a formal definition of computability and laying theoretical foundations for computer science.",
    "url": "pages/glossary.html#term-turing-machine"
  },
  {
    "id": "term-turing-test",
    "title": "Turing Test",
    "category": "Glossary",
    "subcategory": "Historical",
    "keywords": [
      "alan",
      "distinguish",
      "evaluation",
      "historical",
      "human",
      "judge",
      "proposed",
      "responses",
      "test",
      "tries",
      "turing"
    ],
    "excerpt": "A test proposed by Alan Turing where a human judge tries to distinguish between human and AI responses. While historically important, modern LLMs have made it less useful as a capability measure.",
    "url": "pages/glossary.html#term-turing-test"
  },
  {
    "id": "term-td3",
    "title": "Twin Delayed DDPG (TD3)",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "addresses",
      "bias",
      "ddpg",
      "delayed",
      "improvement",
      "learning",
      "optimization",
      "overestimation",
      "policy",
      "qnetworks",
      "reinforcement",
      "smoothing"
    ],
    "excerpt": "An improvement over DDPG that addresses overestimation bias using twin Q-networks, delayed policy updates, and target policy smoothing.",
    "url": "pages/glossary.html#term-td3"
  },
  {
    "id": "term-type-i-error",
    "title": "Type I Error",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "error",
      "false",
      "hypothesis",
      "inference",
      "null",
      "positive",
      "rejection",
      "statistical",
      "statistics",
      "testing",
      "true",
      "type"
    ],
    "excerpt": "The rejection of a true null hypothesis (false positive) in statistical hypothesis testing. The probability of a Type I error is equal to the significance level (alpha) of the test.",
    "url": "pages/glossary.html#term-type-i-error"
  },
  {
    "id": "term-type-ii-error",
    "title": "Type II Error",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "error",
      "failure",
      "false",
      "hypothesis",
      "ii",
      "inference",
      "negative",
      "null",
      "reject",
      "statistical",
      "statistics",
      "testing"
    ],
    "excerpt": "The failure to reject a false null hypothesis (false negative) in statistical hypothesis testing. The probability of a Type II error is denoted beta, and statistical power equals 1 minus beta.",
    "url": "pages/glossary.html#term-type-ii-error"
  },
  {
    "id": "term-u-net",
    "title": "U-Net",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "biomedical",
      "connections",
      "convolutional",
      "corresponding",
      "decoder",
      "designed",
      "encoder",
      "encoderdecoder",
      "fully",
      "image",
      "layers"
    ],
    "excerpt": "A fully convolutional encoder-decoder architecture with skip connections between corresponding encoder and decoder layers, originally designed for biomedical image segmentation.",
    "url": "pages/glossary.html#term-u-net"
  },
  {
    "id": "term-umap",
    "title": "UMAP",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "approximation",
      "constructs",
      "data",
      "dimensionality",
      "embedding",
      "highdimensional",
      "learning",
      "lowdimensional",
      "machine",
      "manifold",
      "method",
      "nonlinear"
    ],
    "excerpt": "Uniform Manifold Approximation and Projection, a nonlinear dimensionality reduction method that constructs a topological representation of high-dimensional data and optimizes a low-dimensional embedding to preserve its structure.",
    "url": "pages/glossary.html#term-umap"
  },
  {
    "id": "term-underfitting",
    "title": "Underfitting",
    "category": "Glossary",
    "subcategory": "Problem",
    "keywords": [
      "capture",
      "data",
      "model",
      "patterns",
      "performing",
      "poorly",
      "problem",
      "simple",
      "test",
      "too",
      "training",
      "underfitting"
    ],
    "excerpt": "When a model is too simple to capture patterns in the data, performing poorly on both training and test data. Addressed by increasing model capacity or training longer.",
    "url": "pages/glossary.html#term-underfitting"
  },
  {
    "id": "term-undersampling",
    "title": "Undersampling",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "addressing",
      "balanced",
      "class",
      "cost",
      "create",
      "data",
      "examples",
      "imbalance",
      "information",
      "learning",
      "losing",
      "machine"
    ],
    "excerpt": "A strategy for addressing class imbalance by randomly removing examples from the majority class to create a more balanced training set, potentially at the cost of losing useful information.",
    "url": "pages/glossary.html#term-undersampling"
  },
  {
    "id": "term-unesco-ai-ethics-recommendation",
    "title": "UNESCO AI Ethics Recommendation",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "2021",
      "adopted",
      "ai",
      "areas",
      "comprehensive",
      "covering",
      "ethical",
      "ethics",
      "first",
      "framework",
      "global",
      "governance"
    ],
    "excerpt": "The first global standard on AI ethics, adopted by UNESCO member states in 2021, providing a comprehensive framework covering values, principles, and policy areas for ethical AI governance.",
    "url": "pages/glossary.html#term-unesco-ai-ethics-recommendation"
  },
  {
    "id": "term-unet-diffusion",
    "title": "UNet Diffusion",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "backbone",
      "commonly",
      "conditioning",
      "connections",
      "crossattention",
      "denoising",
      "diffusion",
      "feature",
      "incorporating",
      "models",
      "multiscale"
    ],
    "excerpt": "The U-Net backbone commonly used in diffusion models as the denoising network, incorporating timestep conditioning, cross-attention for text conditioning, and skip connections for multi-scale feature preservation.",
    "url": "pages/glossary.html#term-unet-diffusion"
  },
  {
    "id": "term-uniform-distribution",
    "title": "Uniform Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "distribution",
      "equally",
      "given",
      "likely",
      "outcomes",
      "probability",
      "range",
      "statistics",
      "uniform"
    ],
    "excerpt": "A probability distribution where all outcomes in a given range are equally likely. The continuous version has constant density over an interval, while the discrete version assigns equal probability to each value.",
    "url": "pages/glossary.html#term-uniform-distribution"
  },
  {
    "id": "term-unigram-tokenization",
    "title": "Unigram Tokenization",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "corpus",
      "impact",
      "iteratively",
      "language",
      "large",
      "least",
      "likelihood",
      "loss",
      "method",
      "model",
      "nlp",
      "overall"
    ],
    "excerpt": "A subword tokenization method that starts with a large vocabulary and iteratively removes tokens whose loss has the least impact on the overall corpus likelihood, using a unigram language model.",
    "url": "pages/glossary.html#term-unigram-tokenization"
  },
  {
    "id": "term-unigram-tokenizer",
    "title": "Unigram Tokenizer",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "algorithm",
      "compact",
      "corpus",
      "finding",
      "iteratively",
      "large",
      "llm",
      "loss",
      "minimize",
      "nlp",
      "optimal",
      "overall"
    ],
    "excerpt": "A subword tokenization algorithm that starts with a large vocabulary and iteratively removes tokens to minimize the overall loss on the training corpus, finding an optimal compact vocabulary.",
    "url": "pages/glossary.html#term-unigram-tokenizer"
  },
  {
    "id": "term-uninformative-prior",
    "title": "Uninformative Prior",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "allowing",
      "bayesian",
      "data",
      "distribution",
      "dominate",
      "expresses",
      "information",
      "methods",
      "minimal",
      "observing",
      "parameter",
      "posterior"
    ],
    "excerpt": "A prior distribution that expresses minimal information about the parameter before observing data, such as a uniform distribution over the parameter space, allowing the data to dominate the posterior.",
    "url": "pages/glossary.html#term-uninformative-prior"
  },
  {
    "id": "term-universal-basic-income-and-ai",
    "title": "Universal Basic Income and AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "ai",
      "and",
      "automation",
      "basic",
      "cash",
      "caused",
      "citizens",
      "discussed",
      "displacement",
      "ethics",
      "governance",
      "income"
    ],
    "excerpt": "Proposals for unconditional periodic cash payments to all citizens, discussed as a potential policy response to widespread job displacement caused by AI and automation technologies.",
    "url": "pages/glossary.html#term-universal-basic-income-and-ai"
  },
  {
    "id": "term-universal-dependencies",
    "title": "Universal Dependencies",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "annotating",
      "consistent",
      "crosslinguistically",
      "dependencies",
      "development",
      "enabling",
      "features",
      "framework",
      "grammar",
      "including",
      "linguistics",
      "model"
    ],
    "excerpt": "A cross-linguistically consistent framework for annotating grammar including parts of speech, morphological features, and syntactic dependencies, enabling multilingual NLP model development.",
    "url": "pages/glossary.html#term-universal-dependencies"
  },
  {
    "id": "term-unstructured-pruning",
    "title": "Unstructured Pruning",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "approach",
      "individual",
      "inference",
      "infrastructure",
      "matrices",
      "matrix",
      "model",
      "optimization",
      "position",
      "pruning",
      "regardless",
      "removes"
    ],
    "excerpt": "A pruning approach that removes individual weights regardless of their position in the weight matrix, resulting in sparse matrices.",
    "url": "pages/glossary.html#term-unstructured-pruning"
  },
  {
    "id": "term-unsupervised-environment-design",
    "title": "Unsupervised Environment Design",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "agents",
      "automatically",
      "capabilities",
      "design",
      "environment",
      "environments",
      "framework",
      "frontier",
      "generates",
      "learning",
      "mechanism"
    ],
    "excerpt": "A framework where a teacher agent or mechanism automatically generates training environments at the frontier of the student agent's capabilities.",
    "url": "pages/glossary.html#term-unsupervised-environment-design"
  },
  {
    "id": "term-unsupervised-learning",
    "title": "Unsupervised Learning",
    "category": "Glossary",
    "subcategory": "Learning Type",
    "keywords": [
      "data",
      "discovering",
      "explicit",
      "fundamentals",
      "guidance",
      "learning",
      "machine",
      "patterns",
      "type",
      "unlabeled",
      "unsupervised",
      "without"
    ],
    "excerpt": "Machine learning from unlabeled data, discovering patterns without explicit guidance. Includes clustering, dimensionality reduction, and self-supervised pre-training of LLMs.",
    "url": "pages/glossary.html#term-unsupervised-learning"
  },
  {
    "id": "term-upper-confidence-bound",
    "title": "Upper Confidence Bound",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "action",
      "algorithms",
      "bandit",
      "bonus",
      "bound",
      "combining",
      "confidence",
      "decreases",
      "estimated",
      "expected",
      "exploration",
      "family"
    ],
    "excerpt": "A family of bandit algorithms that select the action with the highest upper confidence bound on its expected reward, combining the estimated reward with an exploration bonus that decreases as the action is sampled more.",
    "url": "pages/glossary.html#term-upper-confidence-bound"
  },
  {
    "id": "term-upsampling",
    "title": "Upsampling",
    "category": "Glossary",
    "subcategory": "Technique",
    "keywords": [
      "data",
      "increasing",
      "quantity",
      "resolution",
      "technique",
      "upsampling"
    ],
    "excerpt": "Increasing resolution or quantity of data. In image AI, upscaling low-res images. In training, duplicating underrepresented examples to balance datasets.",
    "url": "pages/glossary.html#term-upsampling"
  },
  {
    "id": "term-use-case",
    "title": "Use Case",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "application",
      "case",
      "concept",
      "provides",
      "scenario",
      "specific",
      "use",
      "value"
    ],
    "excerpt": "A specific application or scenario where AI provides value. Understanding your use case helps choose the right model, prompting strategy, and safety measures.",
    "url": "pages/glossary.html#term-use-case"
  },
  {
    "id": "term-user-prompt",
    "title": "User Prompt",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "concept",
      "conversation",
      "developers",
      "input",
      "opposed",
      "prompt",
      "prompting",
      "provided",
      "set",
      "system",
      "user"
    ],
    "excerpt": "The input provided by the user in a conversation, as opposed to the system prompt set by developers. Together with system prompts, they form the complete context for AI responses.",
    "url": "pages/glossary.html#term-user-prompt"
  },
  {
    "id": "term-utility-function",
    "title": "Utility Function",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "alignment",
      "concept",
      "function",
      "good",
      "mathematical",
      "measures",
      "outcome",
      "utility"
    ],
    "excerpt": "A mathematical function that measures how \"good\" an outcome is. In AI alignment, designing utility functions that capture human values is a fundamental challenge.",
    "url": "pages/glossary.html#term-utility-function"
  },
  {
    "id": "term-variational-autoencoder",
    "title": "VAE (Variational Autoencoder)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "autoencoder",
      "data",
      "generative",
      "latent",
      "learns",
      "model",
      "representation",
      "space",
      "vae",
      "variational"
    ],
    "excerpt": "A generative model that learns a latent space representation of data. Used in image generation and as components of larger systems like some diffusion models.",
    "url": "pages/glossary.html#term-variational-autoencoder"
  },
  {
    "id": "term-vae-decoder",
    "title": "VAE Decoder",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "autoencoder",
      "component",
      "compressed",
      "decoder",
      "diffusion",
      "generative",
      "highresolution",
      "image",
      "images",
      "latent",
      "models"
    ],
    "excerpt": "The decoder component of a Variational Autoencoder used in latent diffusion models to reconstruct high-resolution images from compressed latent representations produced by the diffusion process.",
    "url": "pages/glossary.html#term-vae-decoder"
  },
  {
    "id": "term-validation",
    "title": "Validation",
    "category": "Glossary",
    "subcategory": "Process",
    "keywords": [
      "assess",
      "data",
      "evaluation",
      "generalization",
      "model",
      "performance",
      "process",
      "testing",
      "training",
      "validation"
    ],
    "excerpt": "Testing model performance on data not used in training to assess generalization. Validation sets help tune hyperparameters; test sets provide final performance estimates.",
    "url": "pages/glossary.html#term-validation"
  },
  {
    "id": "term-validation-curve",
    "title": "Validation Curve",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "curve",
      "different",
      "function",
      "hyperparameter",
      "learning",
      "machine",
      "model",
      "optimal",
      "overfitting",
      "plot",
      "revealing",
      "scores"
    ],
    "excerpt": "A plot of training and validation scores as a function of a single hyperparameter, revealing the optimal hyperparameter value and whether the model is underfitting or overfitting at different settings.",
    "url": "pages/glossary.html#term-validation-curve"
  },
  {
    "id": "term-value-alignment",
    "title": "Value Alignment",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "alignment",
      "behaviors",
      "challenge",
      "consistent",
      "ensuring",
      "human",
      "intentions",
      "objectives",
      "safety",
      "systems",
      "value"
    ],
    "excerpt": "The challenge of ensuring that an AI system's objectives and behaviors are consistent with human values and intentions. This is considered one of the core problems in AI safety research.",
    "url": "pages/glossary.html#term-value-alignment"
  },
  {
    "id": "term-value-function",
    "title": "Value Function",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "cumulative",
      "estimates",
      "expected",
      "function",
      "future",
      "given",
      "learning",
      "methods",
      "pair",
      "particular",
      "policy",
      "reinforcement"
    ],
    "excerpt": "A function that estimates the expected cumulative future reward from a given state (or state-action pair) under a particular policy.",
    "url": "pages/glossary.html#term-value-function"
  },
  {
    "id": "term-value-sensitive-design",
    "title": "Value-Sensitive Design",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "accounts",
      "ai",
      "conceptual",
      "design",
      "empirical",
      "ethics",
      "governance",
      "human",
      "incorporating",
      "investigations",
      "manner",
      "methodology"
    ],
    "excerpt": "A design methodology that accounts for human values in a principled and systematic manner throughout the design process, incorporating conceptual, empirical, and technical investigations.",
    "url": "pages/glossary.html#term-value-sensitive-design"
  },
  {
    "id": "term-vanishing-gradient",
    "title": "Vanishing Gradient",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "backpropagated",
      "become",
      "causing",
      "deep",
      "early",
      "exponentially",
      "extremely",
      "gradient",
      "gradients",
      "layers",
      "learn",
      "learning"
    ],
    "excerpt": "A problem in deep neural network training where gradients become exponentially small as they are backpropagated through many layers, causing early layers to learn extremely slowly or not at all.",
    "url": "pages/glossary.html#term-vanishing-gradient"
  },
  {
    "id": "term-vapnik-chervonenkis-theory",
    "title": "Vapnik-Chervonenkis Theory",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "based",
      "bounds",
      "chervonenkis",
      "class",
      "classifiers",
      "dimension",
      "error",
      "framework",
      "generalization",
      "hypothesis",
      "learning",
      "machine"
    ],
    "excerpt": "A theoretical framework in statistical learning theory that provides bounds on the generalization error of classifiers based on the VC dimension of the hypothesis class and the number of training samples.",
    "url": "pages/glossary.html#term-vapnik-chervonenkis-theory"
  },
  {
    "id": "term-variance",
    "title": "Variance",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "average",
      "computed",
      "data",
      "deviations",
      "dispersion",
      "mean",
      "measure",
      "science",
      "set",
      "squared",
      "statistics",
      "values"
    ],
    "excerpt": "A measure of the dispersion of a set of values, computed as the average of the squared deviations from the mean. It quantifies how spread out the data points are in a distribution.",
    "url": "pages/glossary.html#term-variance"
  },
  {
    "id": "term-variance-inflation-factor",
    "title": "Variance Inflation Factor",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "coefficient",
      "data",
      "due",
      "factor",
      "inflated",
      "inflation",
      "measure",
      "much",
      "multicollinearity",
      "predictors",
      "regression",
      "science"
    ],
    "excerpt": "A measure of how much the variance of a regression coefficient is inflated due to multicollinearity with other predictors. VIF values above 5 or 10 typically indicate problematic multicollinearity.",
    "url": "pages/glossary.html#term-variance-inflation-factor"
  },
  {
    "id": "term-variance-threshold",
    "title": "Variance Threshold",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "engineering",
      "falls",
      "feature",
      "features",
      "learning",
      "machine",
      "method",
      "removes",
      "selection",
      "simple",
      "specified",
      "threshold"
    ],
    "excerpt": "A simple feature selection method that removes all features whose variance falls below a specified threshold.",
    "url": "pages/glossary.html#term-variance-threshold"
  },
  {
    "id": "term-vae",
    "title": "Variational Autoencoder",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "autoencoder",
      "bound",
      "continuous",
      "data",
      "enabling",
      "generation",
      "generative",
      "latent",
      "learns",
      "lower",
      "mapping"
    ],
    "excerpt": "A generative model that learns a probabilistic mapping between data and a continuous latent space by optimizing a variational lower bound, enabling both generation and meaningful latent representations.",
    "url": "pages/glossary.html#term-vae"
  },
  {
    "id": "term-variational-inference",
    "title": "Variational Inference",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "approximate",
      "bayesian",
      "closest",
      "distribution",
      "divergence",
      "family",
      "finding",
      "inference",
      "learning",
      "machine",
      "member",
      "methods"
    ],
    "excerpt": "An approximate Bayesian inference technique that transforms the inference problem into an optimization problem by finding the member of a tractable distribution family that is closest to the true posterior in KL divergence.",
    "url": "pages/glossary.html#term-variational-inference"
  },
  {
    "id": "term-vc-dimension",
    "title": "VC Dimension",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "capacity",
      "class",
      "classified",
      "complexity",
      "defined",
      "dimension",
      "hypothesis",
      "labelings",
      "largest",
      "learning",
      "machine",
      "measure"
    ],
    "excerpt": "Vapnik-Chervonenkis dimension, a measure of the capacity or complexity of a hypothesis class, defined as the largest set of points that can be shattered (perfectly classified in all possible labelings) by the class.",
    "url": "pages/glossary.html#term-vc-dimension"
  },
  {
    "id": "term-vector",
    "title": "Vector",
    "category": "Glossary",
    "subcategory": "Math",
    "keywords": [
      "data",
      "list",
      "math",
      "mathematical",
      "numbers",
      "ordered",
      "representation",
      "representing",
      "space",
      "vector"
    ],
    "excerpt": "An ordered list of numbers representing data in a mathematical space. Embeddings are vectors; vector similarity measures (cosine similarity) enable semantic search and comparison.",
    "url": "pages/glossary.html#term-vector"
  },
  {
    "id": "term-vector-autoregression",
    "title": "Vector Autoregression",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "among",
      "autoregression",
      "capturing",
      "data",
      "interdependencies",
      "linear",
      "model",
      "multiple",
      "multivariate",
      "own",
      "past",
      "regressed"
    ],
    "excerpt": "A multivariate time series model where each variable is regressed on its own past values and the past values of all other variables in the system, capturing linear interdependencies among multiple time series.",
    "url": "pages/glossary.html#term-vector-autoregression"
  },
  {
    "id": "term-vector-database",
    "title": "Vector Database",
    "category": "Glossary",
    "subcategory": "Infrastructure",
    "keywords": [
      "database",
      "embeddings",
      "highdimensional",
      "infrastructure",
      "optimized",
      "search",
      "searching",
      "storing",
      "vector",
      "vectors"
    ],
    "excerpt": "A database optimized for storing and searching high-dimensional vectors (embeddings). Essential for RAG systems, semantic search, and recommendation engines. Examples: Pinecone, Weaviate, Chroma.",
    "url": "pages/glossary.html#term-vector-database"
  },
  {
    "id": "term-vector-database-sharding",
    "title": "Vector Database Sharding",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "across",
      "beyond",
      "billionscale",
      "collections",
      "compute",
      "data",
      "database",
      "databases",
      "distribute",
      "enabling",
      "horizontal",
      "index"
    ],
    "excerpt": "The horizontal partitioning of a vector index across multiple nodes or storage units to distribute data and query load, enabling vector databases to scale beyond single-machine memory and compute limits for billion-scale collections.",
    "url": "pages/glossary.html#term-vector-database-sharding"
  },
  {
    "id": "term-vector-index",
    "title": "Vector Index",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "algorithms",
      "avoid",
      "collections",
      "data",
      "database",
      "efficient",
      "employing",
      "exhaustive",
      "highdimensional",
      "hnsw",
      "index",
      "ivf"
    ],
    "excerpt": "A specialized data structure optimized for efficient similarity search over high-dimensional vector collections, employing algorithms like HNSW, IVF, or tree-based methods to avoid exhaustive linear scanning.",
    "url": "pages/glossary.html#term-vector-index"
  },
  {
    "id": "term-vector-normalization",
    "title": "Vector Normalization",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "across",
      "become",
      "comparisons",
      "component",
      "consistent",
      "cosine",
      "database",
      "distance",
      "dividing",
      "dot",
      "enabling",
      "ensuring"
    ],
    "excerpt": "The process of scaling vectors to unit length by dividing each component by the vector's L2 norm, ensuring that cosine similarity and dot product similarity become equivalent and enabling consisten...",
    "url": "pages/glossary.html#term-vector-normalization"
  },
  {
    "id": "term-vector-similarity-join",
    "title": "Vector Similarity Join",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "across",
      "clustering",
      "collections",
      "database",
      "datasets",
      "deduplication",
      "different",
      "embedding",
      "entities",
      "exceeds",
      "finds",
      "given"
    ],
    "excerpt": "A database operation that finds all pairs of vectors across two collections whose similarity exceeds a given threshold, used for deduplication, clustering, and linking related entities across different embedding datasets.",
    "url": "pages/glossary.html#term-vector-similarity-join"
  },
  {
    "id": "term-vector-store",
    "title": "Vector Store",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "alongside",
      "associated",
      "component",
      "content",
      "core",
      "database",
      "embeddingbased",
      "embeddings",
      "indexing",
      "infrastructure",
      "metadata",
      "original"
    ],
    "excerpt": "A storage system specialized for persisting, indexing, and querying vector embeddings alongside their associated metadata and original content, serving as the core infrastructure component for embedding-based search and retrieval systems.",
    "url": "pages/glossary.html#term-vector-store"
  },
  {
    "id": "term-vectorized-environment",
    "title": "Vectorized Environment",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "batch",
      "collection",
      "concepts",
      "copies",
      "core",
      "efficient",
      "enabling",
      "environment",
      "experience",
      "learning",
      "multiple",
      "parallel"
    ],
    "excerpt": "A technique for running multiple copies of an environment in parallel within a single process, enabling batch collection of experience for more efficient training.",
    "url": "pages/glossary.html#term-vectorized-environment"
  },
  {
    "id": "term-verification",
    "title": "Verification (AI Outputs)",
    "category": "Glossary",
    "subcategory": "Practice",
    "keywords": [
      "accuracy",
      "ai",
      "appropriateness",
      "checking",
      "outputs",
      "practice",
      "safety",
      "use",
      "verification"
    ],
    "excerpt": "Checking AI outputs for accuracy and appropriateness before use. Essential for high-stakes applications. Can be done by humans, other AI systems, or automated checks.",
    "url": "pages/glossary.html#term-verification"
  },
  {
    "id": "term-verification-chain-prompting",
    "title": "Verification Chain Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "accurate",
      "answer",
      "answers",
      "chain",
      "claims",
      "creates",
      "engineering",
      "final",
      "generates",
      "initial",
      "produce",
      "prompt"
    ],
    "excerpt": "A prompting technique that generates an initial response, then systematically creates and answers verification questions about specific claims in that response, using the verification results to produce a revised, more accurate final answer.",
    "url": "pages/glossary.html#term-verification-chain-prompting"
  },
  {
    "id": "term-vgg",
    "title": "VGG",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "3x3",
      "architecture",
      "characterized",
      "convolution",
      "convolutional",
      "deep",
      "demonstrating",
      "depth",
      "entire",
      "filters",
      "improves",
      "network"
    ],
    "excerpt": "A deep convolutional network architecture characterized by its use of very small 3x3 convolution filters throughout the entire network, demonstrating that depth with small filters improves performance.",
    "url": "pages/glossary.html#term-vgg"
  },
  {
    "id": "term-video-generation",
    "title": "Video Generation",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "autoregressive",
      "coherent",
      "conditioning",
      "diffusion",
      "dimensions",
      "extended",
      "generation",
      "generative",
      "handle",
      "image",
      "images"
    ],
    "excerpt": "The synthesis of temporally coherent video sequences from text prompts, images, or other conditioning signals using extended diffusion or autoregressive models that handle both spatial and temporal dimensions.",
    "url": "pages/glossary.html#term-video-generation"
  },
  {
    "id": "term-video-transformer",
    "title": "Video Transformer",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "adapted",
      "architecture",
      "attention",
      "computer",
      "dimensions",
      "embeddings",
      "factored",
      "handles",
      "image",
      "mechanisms",
      "processing",
      "spacetime"
    ],
    "excerpt": "A transformer architecture adapted for video processing that handles both spatial and temporal dimensions through factored attention, tubelet embeddings, or space-time attention mechanisms.",
    "url": "pages/glossary.html#term-video-transformer"
  },
  {
    "id": "term-video-understanding",
    "title": "Video Understanding",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "across",
      "action",
      "analysis",
      "classification",
      "comprehensive",
      "computer",
      "content",
      "detection",
      "event",
      "frames",
      "image",
      "including"
    ],
    "excerpt": "The comprehensive analysis of video content including action recognition, temporal event detection, scene classification, and narrative understanding across sequences of frames.",
    "url": "pages/glossary.html#term-video-understanding"
  },
  {
    "id": "term-vision-transformer",
    "title": "Vision Transformer",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "applied",
      "architecture",
      "blocks",
      "embedding",
      "embeddings",
      "fixedsize",
      "images",
      "linearly",
      "networks",
      "neural",
      "patch",
      "patches"
    ],
    "excerpt": "A transformer architecture applied to images by splitting them into fixed-size patches, linearly embedding each patch, and processing the sequence of patch embeddings with standard transformer blocks.",
    "url": "pages/glossary.html#term-vision-transformer"
  },
  {
    "id": "term-vision-language-model",
    "title": "Vision-Language Model (VLM)",
    "category": "Glossary",
    "subcategory": "Model Type",
    "keywords": [
      "images",
      "language",
      "model",
      "models",
      "multimodal",
      "process",
      "reason",
      "text",
      "type",
      "vision",
      "vlm"
    ],
    "excerpt": "AI models that can process and reason about both images and text. Examples include GPT-4V, Claude with vision, and Gemini. Enable image understanding, captioning, and visual question answering.",
    "url": "pages/glossary.html#term-vision-language-model"
  },
  {
    "id": "term-visual-grounding",
    "title": "Visual Grounding",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "based",
      "bounding",
      "boxes",
      "computer",
      "description",
      "ground",
      "grounding",
      "image",
      "language",
      "localizing",
      "locations",
      "masks"
    ],
    "excerpt": "The task of localizing a region or object in an image based on a natural language description, requiring the model to ground textual references to specific visual locations with bounding boxes or masks.",
    "url": "pages/glossary.html#term-visual-grounding"
  },
  {
    "id": "term-visual-question-answering",
    "title": "Visual Question Answering",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "answering",
      "capabilities",
      "computer",
      "content",
      "demanding",
      "image",
      "language",
      "multimodal",
      "natural",
      "processing",
      "question",
      "questions"
    ],
    "excerpt": "A multimodal AI task that requires answering natural language questions about the content of an image, demanding both visual understanding and language reasoning capabilities.",
    "url": "pages/glossary.html#term-visual-question-answering"
  },
  {
    "id": "term-visual-relationship-detection",
    "title": "Visual Relationship Detection",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "classifying",
      "computer",
      "describe",
      "detection",
      "identifying",
      "image",
      "interactions",
      "objects",
      "pairs",
      "processing",
      "producing",
      "relationship"
    ],
    "excerpt": "The task of identifying and classifying the interactions or spatial relationships between pairs of objects in an image, producing subject-predicate-object triplets that describe the visual scene.",
    "url": "pages/glossary.html#term-visual-relationship-detection"
  },
  {
    "id": "term-visual-slam",
    "title": "Visual SLAM",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "builds",
      "camera",
      "computer",
      "environment",
      "estimates",
      "images",
      "localization",
      "map",
      "mapping",
      "realtime",
      "robotics"
    ],
    "excerpt": "Visual Simultaneous Localization and Mapping, a technique that estimates camera trajectory and builds a 3D map of the environment from a sequence of images in real-time, used in robotics and AR.",
    "url": "pages/glossary.html#term-visual-slam"
  },
  {
    "id": "term-viterbi-algorithm",
    "title": "Viterbi Algorithm",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "algorithm",
      "crf",
      "decoding",
      "dynamic",
      "finds",
      "hidden",
      "hmm",
      "labeling",
      "likely",
      "nlp",
      "optimal",
      "processing"
    ],
    "excerpt": "A dynamic programming algorithm that finds the most likely sequence of hidden states in an HMM or CRF, used for optimal decoding in sequence labeling and speech recognition.",
    "url": "pages/glossary.html#term-viterbi-algorithm"
  },
  {
    "id": "term-vllm",
    "title": "vLLM",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "batching",
      "continuous",
      "efficiently",
      "engine",
      "highthroughput",
      "implements",
      "inference",
      "language",
      "large",
      "llm",
      "management",
      "memory"
    ],
    "excerpt": "An open-source high-throughput LLM inference engine that implements PagedAttention and continuous batching to efficiently serve large language models with optimized memory management.",
    "url": "pages/glossary.html#term-vllm"
  },
  {
    "id": "term-vocab",
    "title": "Vocabulary (Model)",
    "category": "Glossary",
    "subcategory": "Technical",
    "keywords": [
      "generate",
      "model",
      "recognize",
      "set",
      "technical",
      "tokenization",
      "tokens",
      "vocabulary"
    ],
    "excerpt": "The set of tokens a model can recognize and generate. Determined during tokenizer training. Larger vocabularies handle more languages but increase model size.",
    "url": "pages/glossary.html#term-vocab"
  },
  {
    "id": "term-vocabulary-size",
    "title": "Vocabulary Size",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "affects",
      "balance",
      "efficiency",
      "granularity",
      "length",
      "model",
      "models",
      "nlp",
      "number",
      "sequence",
      "size",
      "token"
    ],
    "excerpt": "The total number of unique tokens in a model's tokenizer vocabulary, which affects model size, tokenization efficiency, and the balance between sequence length and token granularity.",
    "url": "pages/glossary.html#term-vocabulary-size"
  },
  {
    "id": "term-voice-clone",
    "title": "Voice Cloning",
    "category": "Glossary",
    "subcategory": "Application",
    "keywords": [
      "application",
      "audio",
      "cloning",
      "ethics",
      "persons",
      "replicates",
      "samples",
      "specific",
      "voice"
    ],
    "excerpt": "AI that replicates a specific person's voice from audio samples. Raises significant ethical concerns about consent and misuse for fraud or misinformation.",
    "url": "pages/glossary.html#term-voice-clone"
  },
  {
    "id": "term-voluntary-commitments-on-ai",
    "title": "Voluntary Commitments on AI",
    "category": "Glossary",
    "subcategory": "Governance",
    "keywords": [
      "2023",
      "ai",
      "commitments",
      "companies",
      "governance",
      "house",
      "including",
      "information",
      "manage",
      "nonbinding",
      "on",
      "pledges"
    ],
    "excerpt": "Non-binding pledges by AI companies to the White House in 2023 to manage risks from AI, including commitments to safety testing, information sharing, watermarking, and research on societal risks.",
    "url": "pages/glossary.html#term-voluntary-commitments-on-ai"
  },
  {
    "id": "term-von-neumann-architecture",
    "title": "Von Neumann Architecture",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1945",
      "architecture",
      "becoming",
      "computer",
      "computers",
      "data",
      "described",
      "design",
      "digital",
      "dominant",
      "enabling",
      "history"
    ],
    "excerpt": "The computer architecture described by John von Neumann in 1945 that stores both program instructions and data in the same memory, becoming the dominant design paradigm for digital computers and enabling programmable AI systems.",
    "url": "pages/glossary.html#term-von-neumann-architecture"
  },
  {
    "id": "term-voronoi-partitioning",
    "title": "Voronoi Partitioning",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "centroid",
      "closest",
      "contains",
      "database",
      "decomposition",
      "divides",
      "enabling",
      "focus",
      "given",
      "index",
      "indexes",
      "ivf"
    ],
    "excerpt": "A spatial decomposition technique used in IVF indexes that divides the vector space into regions where each region contains all points closest to a specific centroid, enabling search to focus on the most promising partitions for a given query.",
    "url": "pages/glossary.html#term-voronoi-partitioning"
  },
  {
    "id": "term-voting-classifier",
    "title": "Voting Classifier",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "aggregates",
      "averaged",
      "classification",
      "classifier",
      "classifiers",
      "either",
      "ensemble",
      "final",
      "hard",
      "learning",
      "machine",
      "majority"
    ],
    "excerpt": "An ensemble method that aggregates predictions from multiple classifiers using either majority voting (hard voting) or averaged predicted probabilities (soft voting) to produce a final classification.",
    "url": "pages/glossary.html#term-voting-classifier"
  },
  {
    "id": "term-voxel",
    "title": "Voxel",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "3d",
      "analogous",
      "computer",
      "data",
      "discrete",
      "grid",
      "images",
      "networks",
      "neural",
      "pixel",
      "pixels",
      "regular"
    ],
    "excerpt": "A volumetric pixel representing a value on a regular 3D grid, used as a discrete representation for 3D data in neural networks, analogous to pixels in 2D images.",
    "url": "pages/glossary.html#term-voxel"
  },
  {
    "id": "term-vq-vae",
    "title": "VQ-VAE",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "autoencoder",
      "codes",
      "combining",
      "decoders",
      "discrete",
      "enabling",
      "generation",
      "highfidelity",
      "latent",
      "model",
      "networks"
    ],
    "excerpt": "Vector Quantized Variational Autoencoder, a model that uses discrete latent representations through vector quantization, enabling high-fidelity generation by combining discrete codes with powerful decoders.",
    "url": "pages/glossary.html#term-vq-vae"
  },
  {
    "id": "term-vulnerability-exploitation-by-ai",
    "title": "Vulnerability Exploitation by AI",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "act",
      "ai",
      "behavior",
      "by",
      "children",
      "disabilities",
      "distort",
      "elderly",
      "ethics",
      "exploit",
      "exploitation",
      "groups"
    ],
    "excerpt": "AI systems that exploit the vulnerabilities of specific groups such as children, elderly, or persons with disabilities to materially distort their behavior, prohibited under the EU AI Act.",
    "url": "pages/glossary.html#term-vulnerability-exploitation-by-ai"
  },
  {
    "id": "term-wafer-scale-computing",
    "title": "Wafer-Scale Computing",
    "category": "Glossary",
    "subcategory": "Hardware",
    "keywords": [
      "approach",
      "chip",
      "computing",
      "cutting",
      "dies",
      "distributed",
      "entire",
      "hardware",
      "individual",
      "rather",
      "scale",
      "silicon"
    ],
    "excerpt": "An approach to AI hardware that uses an entire silicon wafer as a single chip rather than cutting it into individual dies.",
    "url": "pages/glossary.html#term-wafer-scale-computing"
  },
  {
    "id": "term-walter-pitts",
    "title": "Walter Pitts",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "19231969",
      "1943",
      "american",
      "computable",
      "compute",
      "demonstrating",
      "developed",
      "function",
      "history",
      "logical",
      "logician",
      "mcculloch"
    ],
    "excerpt": "American logician (1923-1969) who, with Warren McCulloch, developed the McCulloch-Pitts neuron model in 1943, demonstrating that networks of simple logical units could compute any computable function.",
    "url": "pages/glossary.html#term-walter-pitts"
  },
  {
    "id": "term-warmup",
    "title": "Warmup (Learning Rate)",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "decay",
      "gradually",
      "increasing",
      "learning",
      "rate",
      "start",
      "technique",
      "training",
      "warmup"
    ],
    "excerpt": "Gradually increasing learning rate at the start of training before decay. Helps stabilize early training when gradients might be unreliable with random weights.",
    "url": "pages/glossary.html#term-warmup"
  },
  {
    "id": "term-warren-mcculloch",
    "title": "Warren McCulloch",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "18981969",
      "1943",
      "american",
      "artificial",
      "computational",
      "created",
      "first",
      "foundation",
      "history",
      "laying",
      "mathematical",
      "mcculloch"
    ],
    "excerpt": "American neurophysiologist (1898-1969) who, with Walter Pitts, created the first mathematical model of an artificial neuron in 1943, laying the theoretical foundation for neural networks and computational neuroscience.",
    "url": "pages/glossary.html#term-warren-mcculloch"
  },
  {
    "id": "term-wasserstein-distance",
    "title": "Wasserstein Distance",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "amount",
      "another",
      "cost",
      "distance",
      "distribution",
      "earth",
      "known",
      "mass",
      "measuring",
      "metric",
      "minimum",
      "moved"
    ],
    "excerpt": "Also known as the Earth Mover's Distance, a metric measuring the minimum cost of transforming one probability distribution into another, where cost is the amount of probability mass moved times the distance it travels.",
    "url": "pages/glossary.html#term-wasserstein-distance"
  },
  {
    "id": "term-watermark",
    "title": "Watermark (AI)",
    "category": "Glossary",
    "subcategory": "Safety",
    "keywords": [
      "ai",
      "aigenerated",
      "allow",
      "content",
      "detection",
      "hidden",
      "origins",
      "patterns",
      "safety",
      "synthetic",
      "watermark"
    ],
    "excerpt": "Hidden patterns in AI-generated content that allow detection of synthetic origins. Proposed for identifying AI text, images, and audio to combat misinformation.",
    "url": "pages/glossary.html#term-watermark"
  },
  {
    "id": "term-watermark-detection",
    "title": "Watermark Detection",
    "category": "Glossary",
    "subcategory": "Generative AI",
    "keywords": [
      "ai",
      "aigenerated",
      "algorithms",
      "attribution",
      "content",
      "detection",
      "embedded",
      "enabling",
      "generation",
      "generative",
      "identify",
      "images"
    ],
    "excerpt": "Algorithms that identify statistical patterns embedded in AI-generated text or images during the generation process, enabling attribution of content to specific AI systems.",
    "url": "pages/glossary.html#term-watermark-detection"
  },
  {
    "id": "term-wavenet",
    "title": "WaveNet",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "audio",
      "autoregressive",
      "capture",
      "causal",
      "convolutions",
      "deep",
      "dependencies",
      "dilated",
      "generative",
      "longrange",
      "maintaining"
    ],
    "excerpt": "A deep generative model for raw audio waveforms that uses dilated causal convolutions to capture long-range temporal dependencies while maintaining the autoregressive property.",
    "url": "pages/glossary.html#term-wavenet"
  },
  {
    "id": "term-waymo-history",
    "title": "Waymo History",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "2009",
      "2016",
      "alphabet",
      "autonomous",
      "becoming",
      "car",
      "commercial",
      "evolution",
      "first",
      "googles",
      "history",
      "milestones"
    ],
    "excerpt": "The evolution of Google's self-driving car project, started in 2009 by Sebastian Thrun, into Waymo as a subsidiary of Alphabet in 2016, becoming the first commercial autonomous ride-hailing service.",
    "url": "pages/glossary.html#term-waymo-history"
  },
  {
    "id": "term-weak-supervision",
    "title": "Weak Supervision",
    "category": "Glossary",
    "subcategory": "Training",
    "keywords": [
      "annotations",
      "automatically",
      "generated",
      "human",
      "imprecise",
      "instead",
      "labels",
      "noisy",
      "perfect",
      "supervision",
      "technique",
      "training"
    ],
    "excerpt": "Training with noisy, imprecise, or automatically generated labels instead of perfect human annotations. Can dramatically reduce labeling costs while achieving good results.",
    "url": "pages/glossary.html#term-weak-supervision"
  },
  {
    "id": "term-weaviate",
    "title": "Weaviate",
    "category": "Glossary",
    "subcategory": "Vector Database",
    "keywords": [
      "api",
      "builtin",
      "capabilities",
      "combines",
      "database",
      "filtering",
      "generative",
      "graphql",
      "hybrid",
      "integrations",
      "modules",
      "multiple"
    ],
    "excerpt": "An open-source vector database that combines vector search with structured filtering and supports multiple vectorization modules, offering hybrid search capabilities and a GraphQL API with built-in support for generative AI integrations.",
    "url": "pages/glossary.html#term-weaviate"
  },
  {
    "id": "term-weibull-distribution",
    "title": "Weibull Distribution",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "analysis",
      "continuous",
      "distribution",
      "modeling",
      "probability",
      "reliability",
      "statistics",
      "survival",
      "weibull"
    ],
    "excerpt": "A continuous probability distribution used in reliability analysis and survival modeling. Its shape parameter allows it to model increasing, constant, or decreasing failure rates over time.",
    "url": "pages/glossary.html#term-weibull-distribution"
  },
  {
    "id": "term-weight",
    "title": "Weight",
    "category": "Glossary",
    "subcategory": "Core Concept",
    "keywords": [
      "concept",
      "core",
      "learned",
      "networks",
      "neural",
      "numerical",
      "parameters",
      "training",
      "weight"
    ],
    "excerpt": "The numerical parameters in neural networks that are learned during training. Weights determine how inputs are transformed into outputs; large models have billions of weights.",
    "url": "pages/glossary.html#term-weight"
  },
  {
    "id": "term-weight-decay",
    "title": "Weight Decay",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "adds",
      "current",
      "decay",
      "effectively",
      "fraction",
      "large",
      "learning",
      "machine",
      "optimization",
      "penalizing",
      "regularization",
      "rule"
    ],
    "excerpt": "A regularization technique that adds a fraction of the current weight values to the weight update rule during training, effectively penalizing large weights.",
    "url": "pages/glossary.html#term-weight-decay"
  },
  {
    "id": "term-weight-initialization",
    "title": "Weight Initialization",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "across",
      "architecture",
      "designed",
      "initial",
      "initialization",
      "instability",
      "layers",
      "like",
      "maintain",
      "methods",
      "networks",
      "neural"
    ],
    "excerpt": "The strategy for setting initial parameter values in neural networks, with methods like Xavier and He initialization designed to maintain signal variance across layers and prevent training instability.",
    "url": "pages/glossary.html#term-weight-initialization"
  },
  {
    "id": "term-weight-sharing",
    "title": "Weight Sharing",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "compression",
      "connections",
      "inference",
      "infrastructure",
      "model",
      "multiple",
      "must",
      "network",
      "neural",
      "number",
      "optimization",
      "parameters"
    ],
    "excerpt": "A compression technique where multiple connections in a neural network share the same weight value, reducing the number of unique parameters that must be stored.",
    "url": "pages/glossary.html#term-weight-sharing"
  },
  {
    "id": "term-weight-tying",
    "title": "Weight Tying",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "embedding",
      "improving",
      "input",
      "language",
      "layer",
      "matrix",
      "models",
      "networks",
      "neural",
      "often",
      "output"
    ],
    "excerpt": "A technique that shares the weight matrix between the input embedding layer and the output projection layer in language models, reducing parameters and often improving performance.",
    "url": "pages/glossary.html#term-weight-tying"
  },
  {
    "id": "term-weight-only-quantization",
    "title": "Weight-Only Quantization",
    "category": "Glossary",
    "subcategory": "Model Optimization",
    "keywords": [
      "compresses",
      "computation",
      "dequantization",
      "higher",
      "inference",
      "infrastructure",
      "low",
      "model",
      "only",
      "optimization",
      "performing",
      "precision"
    ],
    "excerpt": "A quantization strategy that compresses only the model weights to low precision while performing computation in higher precision after dequantization.",
    "url": "pages/glossary.html#term-weight-only-quantization"
  },
  {
    "id": "term-whisper",
    "title": "Whisper",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "accuracy",
      "across",
      "audio",
      "high",
      "languages",
      "many",
      "model",
      "openais",
      "recognition",
      "speech",
      "text",
      "transcribes"
    ],
    "excerpt": "OpenAI's speech recognition model that transcribes audio to text with high accuracy across many languages. Open-sourced, enabling widespread use in transcription applications.",
    "url": "pages/glossary.html#term-whisper"
  },
  {
    "id": "term-whisper-architecture",
    "title": "Whisper Architecture",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "000",
      "680",
      "architecture",
      "automatic",
      "data",
      "encoderdecoder",
      "features",
      "hours",
      "input",
      "logmel",
      "multilingual",
      "networks"
    ],
    "excerpt": "An encoder-decoder transformer architecture trained on 680,000 hours of multilingual speech data for automatic speech recognition, using log-mel spectrogram features as input.",
    "url": "pages/glossary.html#term-whisper-architecture"
  },
  {
    "id": "term-white-noise",
    "title": "White Noise",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "constant",
      "data",
      "mean",
      "noise",
      "random",
      "science",
      "series",
      "statistics",
      "time",
      "uncorrelated",
      "variables",
      "variance"
    ],
    "excerpt": "A time series of uncorrelated random variables with zero mean and constant variance. It represents purely random variation with no exploitable patterns, serving as the residual target for good time series models.",
    "url": "pages/glossary.html#term-white-noise"
  },
  {
    "id": "term-win-rate",
    "title": "Win Rate",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "anothers",
      "assessment",
      "automated",
      "comparisons",
      "evaluation",
      "evaluators",
      "human",
      "judges",
      "measures",
      "metric",
      "models",
      "one"
    ],
    "excerpt": "An evaluation metric that measures the percentage of pairwise comparisons in which one model's output is preferred over another's by human judges or automated evaluators, providing a simple relative quality assessment.",
    "url": "pages/glossary.html#term-win-rate"
  },
  {
    "id": "term-windfall-clause",
    "title": "Windfall Clause",
    "category": "Glossary",
    "subcategory": "AI Ethics",
    "keywords": [
      "advanced",
      "ai",
      "benefits",
      "capture",
      "clause",
      "commitment",
      "companies",
      "developers",
      "disproportionate",
      "economic",
      "ensuring",
      "ethics"
    ],
    "excerpt": "A proposed commitment by AI developers to share the economic benefits of transformative AI widely, ensuring that a small number of companies or nations do not capture disproportionate gains from advanced AI.",
    "url": "pages/glossary.html#term-windfall-clause"
  },
  {
    "id": "term-window-attention",
    "title": "Window Attention",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "architecture",
      "attention",
      "context",
      "efficiency",
      "full",
      "looks",
      "nearby",
      "only",
      "rather",
      "tokens",
      "variant",
      "window"
    ],
    "excerpt": "A variant of attention that only looks at nearby tokens rather than the full context. Reduces computational cost for long sequences, used in models like Longformer.",
    "url": "pages/glossary.html#term-window-attention"
  },
  {
    "id": "term-window-based-chunking",
    "title": "Window-Based Chunking",
    "category": "Glossary",
    "subcategory": "Retrieval",
    "keywords": [
      "based",
      "boundaries",
      "characters",
      "chunk",
      "chunking",
      "chunks",
      "content",
      "create",
      "document",
      "fixedsize",
      "measured",
      "method"
    ],
    "excerpt": "A document splitting method that uses a fixed-size sliding window measured in tokens or characters to create overlapping chunks, providing simple and predictable chunk boundaries regardless of document structure or content semantics.",
    "url": "pages/glossary.html#term-window-based-chunking"
  },
  {
    "id": "term-winograd-schema",
    "title": "Winograd Schema",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "alternative",
      "challenge",
      "commonsense",
      "coreference",
      "designed",
      "determine",
      "differing",
      "linguistics",
      "nlp",
      "one",
      "pairs",
      "pronoun"
    ],
    "excerpt": "A coreference resolution challenge requiring commonsense reasoning to determine what a pronoun refers to, designed as an alternative to the Turing test with pairs of sentences differing by one word.",
    "url": "pages/glossary.html#term-winograd-schema"
  },
  {
    "id": "term-winogrande",
    "title": "WinoGrande",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "ambiguous",
      "benchmark",
      "benchmarks",
      "commonsense",
      "coreference",
      "correct",
      "evaluation",
      "identify",
      "knowledge",
      "largescale",
      "models",
      "problems"
    ],
    "excerpt": "A large-scale benchmark of Winograd schema-style coreference resolution problems that tests commonsense reasoning by requiring models to identify the correct referent of ambiguous pronouns using world knowledge.",
    "url": "pages/glossary.html#term-winogrande"
  },
  {
    "id": "term-winsorization",
    "title": "Winsorization",
    "category": "Glossary",
    "subcategory": "Data Science",
    "keywords": [
      "data",
      "extreme",
      "handling",
      "outliers",
      "percentile",
      "rather",
      "removing",
      "replacing",
      "science",
      "specified",
      "statistics",
      "technique"
    ],
    "excerpt": "A technique for handling outliers by replacing extreme values with specified percentile values rather than removing them.",
    "url": "pages/glossary.html#term-winsorization"
  },
  {
    "id": "term-word-embedding",
    "title": "Word Embedding",
    "category": "Glossary",
    "subcategory": "Representation",
    "keywords": [
      "dense",
      "embedding",
      "nlp",
      "representation",
      "representations",
      "similar",
      "vector",
      "vectors",
      "word",
      "words"
    ],
    "excerpt": "Dense vector representations of words where similar words have similar vectors. Classic examples include Word2Vec and GloVe; modern LLMs use contextual embeddings that vary by context.",
    "url": "pages/glossary.html#term-word-embedding"
  },
  {
    "id": "term-word-error-rate",
    "title": "Word Error Rate",
    "category": "Glossary",
    "subcategory": "Evaluation",
    "keywords": [
      "calculated",
      "deletions",
      "distance",
      "divided",
      "edit",
      "error",
      "evaluation",
      "insertions",
      "level",
      "measures",
      "metric",
      "metrics"
    ],
    "excerpt": "A metric that measures the edit distance between predicted and reference transcriptions at the word level, calculated as the number of word substitutions, insertions, and deletions divided by the t...",
    "url": "pages/glossary.html#term-word-error-rate"
  },
  {
    "id": "term-word-segmentation",
    "title": "Word Segmentation",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "boundaries",
      "chinese",
      "essential",
      "identifying",
      "japanese",
      "languages",
      "nlp",
      "processing",
      "segmentation",
      "separate",
      "subsequent",
      "task"
    ],
    "excerpt": "The task of identifying word boundaries in languages that do not use whitespace to separate words, such as Chinese, Japanese, and Thai, essential for subsequent NLP processing.",
    "url": "pages/glossary.html#term-word-segmentation"
  },
  {
    "id": "term-word-sense-disambiguation",
    "title": "Word Sense Disambiguation",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "based",
      "context",
      "determining",
      "disambiguation",
      "discourse",
      "given",
      "inventory",
      "linguistics",
      "meaning",
      "nlp",
      "polysemous",
      "predefined"
    ],
    "excerpt": "The task of determining which meaning of a polysemous word is used in a given context, selecting from a predefined sense inventory based on surrounding words and discourse.",
    "url": "pages/glossary.html#term-word-sense-disambiguation"
  },
  {
    "id": "term-word2vec",
    "title": "Word2Vec",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "capturing",
      "cbow",
      "corpora",
      "dense",
      "embeddings",
      "engineering",
      "family",
      "feature",
      "large",
      "learn",
      "learning",
      "machine"
    ],
    "excerpt": "A family of neural network models (Skip-gram and CBOW) that learn dense vector representations of words from large text corpora, capturing semantic relationships so that similar words have nearby embeddings.",
    "url": "pages/glossary.html#term-word2vec"
  },
  {
    "id": "term-wordnet",
    "title": "WordNet",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "adjectives",
      "adverbs",
      "cognitive",
      "database",
      "english",
      "grouped",
      "large",
      "lexical",
      "linguistics",
      "linked",
      "nlp",
      "nouns"
    ],
    "excerpt": "A large lexical database of English where nouns, verbs, adjectives, and adverbs are grouped into cognitive synonym sets (synsets) linked by semantic and lexical relations.",
    "url": "pages/glossary.html#term-wordnet"
  },
  {
    "id": "term-wordpiece",
    "title": "WordPiece",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "algorithm",
      "bert",
      "data",
      "greedily",
      "known",
      "likelihood",
      "maximizing",
      "merges",
      "models",
      "nlp",
      "related",
      "selects"
    ],
    "excerpt": "A subword tokenization algorithm that greedily selects merges maximizing the likelihood of the training data, used by BERT and related models, splitting unknown words into known subword units.",
    "url": "pages/glossary.html#term-wordpiece"
  },
  {
    "id": "term-world-model",
    "title": "World Model",
    "category": "Glossary",
    "subcategory": "Concept",
    "keywords": [
      "ais",
      "concept",
      "internal",
      "model",
      "representation",
      "research",
      "works",
      "world"
    ],
    "excerpt": "An AI's internal representation of how the world works. Used to simulate outcomes and plan actions. A key concept in AI safety discussions about model capabilities.",
    "url": "pages/glossary.html#term-world-model"
  },
  {
    "id": "term-world-models",
    "title": "World Models",
    "category": "Glossary",
    "subcategory": "Reinforcement Learning",
    "keywords": [
      "agent",
      "allow",
      "dynamics",
      "environment",
      "latent",
      "learned",
      "learning",
      "models",
      "network",
      "neural",
      "plan",
      "planning"
    ],
    "excerpt": "Learned neural network representations of environment dynamics that allow an agent to simulate and plan in a latent space.",
    "url": "pages/glossary.html#term-world-models"
  },
  {
    "id": "term-x-risk",
    "title": "X-Risk",
    "category": "Glossary",
    "subcategory": "AI Safety",
    "keywords": [
      "ai",
      "catastrophic",
      "civilizational",
      "collapse",
      "ethics",
      "existential",
      "extinction",
      "human",
      "permanent",
      "referring",
      "result",
      "risk"
    ],
    "excerpt": "Shorthand for existential risk, referring to catastrophic scenarios that could result in human extinction or permanent civilizational collapse.",
    "url": "pages/glossary.html#term-x-risk"
  },
  {
    "id": "term-xai-company",
    "title": "xAI",
    "category": "Glossary",
    "subcategory": "Company",
    "keywords": [
      "chatbot",
      "company",
      "creator",
      "elon",
      "grok",
      "llm",
      "musks",
      "provider",
      "xai"
    ],
    "excerpt": "Elon Musk's AI company, creator of the Grok chatbot. Founded in 2023, it aims to develop AI that can understand the universe and has access to real-time X (Twitter) data.",
    "url": "pages/glossary.html#term-xai-company"
  },
  {
    "id": "term-xai",
    "title": "XAI (Explainable AI)",
    "category": "Glossary",
    "subcategory": "Field",
    "keywords": [
      "ai",
      "decisions",
      "explainable",
      "field",
      "focused",
      "humans",
      "making",
      "transparency",
      "understandable",
      "xai"
    ],
    "excerpt": "The field focused on making AI decisions understandable to humans. Includes techniques for visualizing attention, attributing outputs to inputs, and generating explanations.",
    "url": "pages/glossary.html#term-xai"
  },
  {
    "id": "term-xavier-initialization",
    "title": "Xavier Initialization",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "across",
      "activation",
      "activations",
      "architecture",
      "designed",
      "distribution",
      "initialization",
      "input",
      "layers",
      "maintain",
      "method",
      "networks"
    ],
    "excerpt": "A weight initialization method that samples weights from a distribution scaled by the number of input and output neurons, designed to maintain activation variance across layers with sigmoid or tanh activations.",
    "url": "pages/glossary.html#term-xavier-initialization"
  },
  {
    "id": "term-xgboost",
    "title": "XGBoost",
    "category": "Glossary",
    "subcategory": "Machine Learning",
    "keywords": [
      "algorithms",
      "boosting",
      "column",
      "construction",
      "efficient",
      "gradient",
      "implementation",
      "learning",
      "machine",
      "model",
      "objectives",
      "optimized"
    ],
    "excerpt": "An optimized implementation of gradient boosting that uses regularized objectives, column subsampling, and efficient tree construction algorithms.",
    "url": "pages/glossary.html#term-xgboost"
  },
  {
    "id": "term-xla-compiler",
    "title": "XLA Compiler",
    "category": "Glossary",
    "subcategory": "Inference Infrastructure",
    "keywords": [
      "accelerated",
      "algebra",
      "code",
      "compiler",
      "computation",
      "domainspecific",
      "fusion",
      "generation",
      "graphs",
      "hardwarespecific",
      "inference",
      "infrastructure"
    ],
    "excerpt": "Accelerated Linear Algebra, a domain-specific compiler for machine learning that optimizes computation graphs through operator fusion, memory layout optimization, and hardware-specific code generation.",
    "url": "pages/glossary.html#term-xla-compiler"
  },
  {
    "id": "term-xlnet",
    "title": "XLNet",
    "category": "Glossary",
    "subcategory": "Neural Networks",
    "keywords": [
      "architecture",
      "autoregressive",
      "bidirectional",
      "capture",
      "context",
      "formulation",
      "generalized",
      "language",
      "limitations",
      "maintaining",
      "masked",
      "method"
    ],
    "excerpt": "A generalized autoregressive pretraining method that uses permutation-based training to capture bidirectional context while maintaining autoregressive formulation, overcoming limitations of masked language modeling.",
    "url": "pages/glossary.html#term-xlnet"
  },
  {
    "id": "term-xml-prompting",
    "title": "XML Prompting",
    "category": "Glossary",
    "subcategory": "Prompt Engineering",
    "keywords": [
      "boundaries",
      "clear",
      "context",
      "data",
      "delimit",
      "engineering",
      "expected",
      "format",
      "hierarchical",
      "input",
      "instructions",
      "leveraging"
    ],
    "excerpt": "A prompting technique that uses XML tags to structure prompt sections, delimit input data, and specify output format, leveraging hierarchical markup to provide clear boundaries between instructions, context, and expected response structure.",
    "url": "pages/glossary.html#term-xml-prompting"
  },
  {
    "id": "term-xml-tags",
    "title": "XML Tags (in Prompting)",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "clearly",
      "context",
      "delineate",
      "example",
      "in",
      "like",
      "prompting",
      "prompts",
      "sections",
      "structure",
      "tags",
      "technique"
    ],
    "excerpt": "Using XML-style tags like <context> or <example> to structure prompts and clearly delineate sections. Helps models parse complex prompts more reliably.",
    "url": "pages/glossary.html#term-xml-tags"
  },
  {
    "id": "term-yann-lecun",
    "title": "Yann LeCun",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "1980s",
      "applied",
      "computer",
      "convolutional",
      "developed",
      "digit",
      "frenchamerican",
      "handwritten",
      "history",
      "late",
      "lecun",
      "networks"
    ],
    "excerpt": "French-American computer scientist who developed convolutional neural networks in the late 1980s and applied them to handwritten digit recognition.",
    "url": "pages/glossary.html#term-yann-lecun"
  },
  {
    "id": "term-yeo-johnson-transformation",
    "title": "Yeo-Johnson Transformation",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "boxcox",
      "data",
      "handle",
      "johnson",
      "negative",
      "positive",
      "power",
      "science",
      "similar",
      "statistics",
      "transformation",
      "values"
    ],
    "excerpt": "A power transformation similar to Box-Cox that can handle both positive and negative values. It extends the Box-Cox transformation by defining appropriate transformations for non-positive data.",
    "url": "pages/glossary.html#term-yeo-johnson-transformation"
  },
  {
    "id": "term-yi",
    "title": "Yi",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "bilingual",
      "chineseenglish",
      "llms",
      "model",
      "open",
      "opensource",
      "series",
      "source",
      "yi"
    ],
    "excerpt": "A series of open-source bilingual (Chinese/English) LLMs from 01.AI. Known for strong performance across benchmarks and contributing to open-source AI development in Asia.",
    "url": "pages/glossary.html#term-yi"
  },
  {
    "id": "term-yolo",
    "title": "YOLO (You Only Look Once)",
    "category": "Glossary",
    "subcategory": "Architecture",
    "keywords": [
      "algorithm",
      "architecture",
      "computer",
      "detection",
      "images",
      "look",
      "object",
      "once",
      "only",
      "pass",
      "processes",
      "realtime"
    ],
    "excerpt": "A real-time object detection algorithm that processes images in a single pass. Revolutionary for its speed, enabling real-time video object detection on standard hardware.",
    "url": "pages/glossary.html#term-yolo"
  },
  {
    "id": "term-yolov8",
    "title": "YOLOv8",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "anchorfree",
      "branches",
      "classification",
      "computer",
      "decoupled",
      "detection",
      "family",
      "head",
      "improved",
      "introduces",
      "model",
      "object"
    ],
    "excerpt": "A state-of-the-art real-time object detection model in the YOLO family that introduces an anchor-free detection head, decoupled classification and regression branches, and improved training strategies.",
    "url": "pages/glossary.html#term-yolov8"
  },
  {
    "id": "term-yoshua-bengio",
    "title": "Yoshua Bengio",
    "category": "Glossary",
    "subcategory": "History",
    "keywords": [
      "adversarial",
      "bengio",
      "canadian",
      "computer",
      "contributions",
      "deep",
      "foundational",
      "generative",
      "history",
      "including",
      "language",
      "learning"
    ],
    "excerpt": "Canadian computer scientist who made foundational contributions to deep learning, including neural language models and generative adversarial networks.",
    "url": "pages/glossary.html#term-yoshua-bengio"
  },
  {
    "id": "term-z-score",
    "title": "Z-Score",
    "category": "Glossary",
    "subcategory": "Statistics",
    "keywords": [
      "data",
      "deviations",
      "distribution",
      "indicating",
      "many",
      "mean",
      "point",
      "science",
      "score",
      "standard",
      "standardized",
      "statistics"
    ],
    "excerpt": "A standardized score indicating how many standard deviations a data point is from the mean of its distribution. It allows comparison of values from different distributions on a common scale.",
    "url": "pages/glossary.html#term-z-score"
  },
  {
    "id": "term-zephyr",
    "title": "Zephyr",
    "category": "Glossary",
    "subcategory": "Model",
    "keywords": [
      "assistants",
      "face",
      "finetuned",
      "helpful",
      "hugging",
      "llms",
      "model",
      "open",
      "optimized",
      "series",
      "source",
      "zephyr"
    ],
    "excerpt": "A series of fine-tuned open LLMs from Hugging Face optimized for helpful assistants. Demonstrates how smaller models with good alignment can outperform larger base models.",
    "url": "pages/glossary.html#term-zephyr"
  },
  {
    "id": "term-zero-optimization",
    "title": "ZeRO Optimization",
    "category": "Glossary",
    "subcategory": "LLM",
    "keywords": [
      "across",
      "ai",
      "dataparallel",
      "distributed",
      "generative",
      "gradients",
      "llm",
      "memory",
      "optimization",
      "optimizer",
      "parameters",
      "partitions"
    ],
    "excerpt": "Zero Redundancy Optimizer, a distributed training technique that partitions optimizer states, gradients, and parameters across data-parallel processes to reduce memory redundancy.",
    "url": "pages/glossary.html#term-zero-optimization"
  },
  {
    "id": "term-zero-day",
    "title": "Zero-Day (AI Context)",
    "category": "Glossary",
    "subcategory": "Security",
    "keywords": [
      "ai",
      "attack",
      "context",
      "day",
      "developers",
      "discovered",
      "know",
      "novel",
      "safety",
      "security",
      "systems",
      "them"
    ],
    "excerpt": "Novel vulnerabilities or attack vectors discovered in AI systems before developers know about them. AI security research increasingly focuses on finding such vulnerabilities proactively.",
    "url": "pages/glossary.html#term-zero-day"
  },
  {
    "id": "term-zero-shot-cot",
    "title": "Zero-Shot Chain-of-Thought",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "adding",
      "chain",
      "examples",
      "lets",
      "of",
      "prompt",
      "prompting",
      "providing",
      "reasoning",
      "shot",
      "step",
      "think"
    ],
    "excerpt": "Adding \"Let's think step by step\" to a prompt to trigger reasoning without providing examples. A simple but effective technique for improving accuracy on complex tasks.",
    "url": "pages/glossary.html#term-zero-shot-cot"
  },
  {
    "id": "term-zero-shot",
    "title": "Zero-Shot Learning",
    "category": "Glossary",
    "subcategory": "Prompting",
    "keywords": [
      "entirely",
      "examples",
      "instructions",
      "knowledge",
      "learning",
      "performs",
      "pretrained",
      "prompt",
      "prompting",
      "relying",
      "shot",
      "task"
    ],
    "excerpt": "When AI performs a task without examples in the prompt, relying entirely on pre-trained knowledge and instructions. Contrasts with few-shot where examples are provided.",
    "url": "pages/glossary.html#term-zero-shot"
  },
  {
    "id": "term-zero-shot-ner",
    "title": "Zero-Shot NER",
    "category": "Glossary",
    "subcategory": "NLP",
    "keywords": [
      "additional",
      "categories",
      "data",
      "descriptions",
      "entity",
      "generalize",
      "labeled",
      "language",
      "named",
      "natural",
      "ner",
      "new"
    ],
    "excerpt": "Named entity recognition performed on entity types not seen during training, using natural language descriptions of entity categories to generalize to new entity types without additional labeled data.",
    "url": "pages/glossary.html#term-zero-shot-ner"
  },
  {
    "id": "term-zero-shot-object-detection",
    "title": "Zero-Shot Object Detection",
    "category": "Glossary",
    "subcategory": "Computer Vision",
    "keywords": [
      "ability",
      "alignment",
      "categories",
      "computer",
      "descriptions",
      "detect",
      "detection",
      "examples",
      "localize",
      "match",
      "new",
      "novel"
    ],
    "excerpt": "The ability to detect and localize objects of novel categories without any training examples, using vision-language alignment to match text descriptions of new categories to visual regions.",
    "url": "pages/glossary.html#term-zero-shot-object-detection"
  },
  {
    "id": "learn-prompt-basics",
    "title": "Prompt Basics",
    "category": "Discover",
    "subcategory": "Fundamentals",
    "keywords": [
      "prompt basics",
      "fundamentals",
      "beginner",
      "introduction",
      "getting started",
      "foundation"
    ],
    "excerpt": "Learn the fundamental principles of effective AI prompting. Perfect for beginners starting their journey with AI communication.",
    "url": "learn/prompt-basics.html"
  },
  {
    "id": "learn-what-is-prompt",
    "title": "What is a Prompt?",
    "category": "Discover",
    "subcategory": "Fundamentals",
    "keywords": [
      "what is prompt",
      "definition",
      "input",
      "instruction"
    ],
    "excerpt": "A prompt is the text input you send to an AI. Learn how to structure your requests for better AI responses.",
    "url": "learn/prompt-basics.html#what-is-prompt"
  },
  {
    "id": "learn-prompt-anatomy",
    "title": "Anatomy of a Prompt",
    "category": "Discover",
    "subcategory": "Fundamentals",
    "keywords": [
      "anatomy",
      "structure",
      "components",
      "parts",
      "elements"
    ],
    "excerpt": "Understand the key components that make up an effective prompt: context, task, format, and constraints.",
    "url": "learn/prompt-basics.html#anatomy"
  },
  {
    "id": "learn-facts-fictions",
    "title": "Facts & Fictions",
    "category": "Discover",
    "subcategory": "Fundamentals",
    "keywords": [
      "facts",
      "fictions",
      "myths",
      "misconceptions",
      "debunked",
      "research",
      "verified",
      "truth"
    ],
    "excerpt": "Research-backed truths about AI capabilities, prompt engineering, and common misconceptions from MIT, Stanford, Wharton, and government sources.",
    "url": "learn/facts-fictions.html"
  },
  {
    "id": "learn-crisp",
    "title": "CRISP Method",
    "category": "Discover",
    "subcategory": "Frameworks",
    "keywords": [
      "crisp",
      "method",
      "framework",
      "context",
      "role",
      "instructions",
      "specifics",
      "parameters"
    ],
    "excerpt": "Context, Role, Instructions, Specifics, Parameters. The essential framework for clear, effective prompts.",
    "url": "learn/crisp.html"
  },
  {
    "id": "learn-crisp-context",
    "title": "CRISP: Context",
    "category": "Discover",
    "subcategory": "CRISP Framework",
    "keywords": [
      "context",
      "background",
      "situation",
      "setting",
      "crisp"
    ],
    "excerpt": "Set the stage with background information. Help AI understand your situation and what you're trying to achieve.",
    "url": "learn/crisp.html#context"
  },
  {
    "id": "learn-crisp-role",
    "title": "CRISP: Role",
    "category": "Discover",
    "subcategory": "CRISP Framework",
    "keywords": [
      "role",
      "persona",
      "expertise",
      "act as",
      "crisp"
    ],
    "excerpt": "Define the persona for the AI to adopt. Assign expertise or perspective to shape responses appropriately.",
    "url": "learn/crisp.html#role"
  },
  {
    "id": "learn-crisp-instructions",
    "title": "CRISP: Instructions",
    "category": "Discover",
    "subcategory": "CRISP Framework",
    "keywords": [
      "instructions",
      "task",
      "action",
      "what to do",
      "crisp"
    ],
    "excerpt": "Provide specific guidelines for the task. Tell AI exactly what you want it to accomplish.",
    "url": "learn/crisp.html#instructions"
  },
  {
    "id": "learn-crisp-specifics",
    "title": "CRISP: Specifics",
    "category": "Discover",
    "subcategory": "CRISP Framework",
    "keywords": [
      "specifics",
      "details",
      "format",
      "length",
      "tone",
      "crisp"
    ],
    "excerpt": "Define format, length, tone, and scope. Add the details that shape how the output should look and sound.",
    "url": "learn/crisp.html#specifics"
  },
  {
    "id": "learn-crisp-parameters",
    "title": "CRISP: Parameters",
    "category": "Discover",
    "subcategory": "CRISP Framework",
    "keywords": [
      "parameters",
      "constraints",
      "boundaries",
      "limits",
      "rules",
      "crisp"
    ],
    "excerpt": "Set constraints and boundaries. Define what to include, exclude, or avoid in the response.",
    "url": "learn/crisp.html#parameters"
  },
  {
    "id": "learn-crispe",
    "title": "CRISPE Method",
    "category": "Discover",
    "subcategory": "Frameworks",
    "keywords": [
      "crispe",
      "method",
      "framework",
      "example",
      "few-shot",
      "creative"
    ],
    "excerpt": "CRISP plus Example for few-shot learning. Particularly useful for creative tasks where showing is better than telling.",
    "url": "learn/crispe.html"
  },
  {
    "id": "learn-crispe-example",
    "title": "CRISPE: Example",
    "category": "Discover",
    "subcategory": "CRISPE Framework",
    "keywords": [
      "example",
      "demonstration",
      "sample",
      "few-shot",
      "crispe"
    ],
    "excerpt": "Provide examples to show AI the format or style you want. Demonstrations help achieve consistent, patterned outputs.",
    "url": "learn/crispe.html#example"
  },
  {
    "id": "learn-costar",
    "title": "COSTAR Method",
    "category": "Discover",
    "subcategory": "Frameworks",
    "keywords": [
      "costar",
      "method",
      "framework",
      "professional",
      "content creation",
      "audience"
    ],
    "excerpt": "Context, Objective, Style, Tone, Audience, Response. Ideal for professional content creation with specific voice and audience requirements.",
    "url": "learn/costar.html"
  },
  {
    "id": "learn-costar-objective",
    "title": "COSTAR: Objective",
    "category": "Discover",
    "subcategory": "COSTAR Framework",
    "keywords": [
      "objective",
      "goal",
      "purpose",
      "outcome",
      "costar"
    ],
    "excerpt": "Define the specific goal or outcome you want. Be clear about what success looks like.",
    "url": "learn/costar.html#objective"
  },
  {
    "id": "learn-costar-style",
    "title": "COSTAR: Style",
    "category": "Discover",
    "subcategory": "COSTAR Framework",
    "keywords": [
      "style",
      "writing style",
      "formal",
      "casual",
      "costar"
    ],
    "excerpt": "Specify the writing style: formal, casual, technical, conversational. Match the content to its purpose.",
    "url": "learn/costar.html#style"
  },
  {
    "id": "learn-costar-tone",
    "title": "COSTAR: Tone",
    "category": "Discover",
    "subcategory": "COSTAR Framework",
    "keywords": [
      "tone",
      "voice",
      "emotion",
      "attitude",
      "costar"
    ],
    "excerpt": "Set the emotional tone: professional, friendly, urgent, encouraging. Shape how the message feels.",
    "url": "learn/costar.html#tone"
  },
  {
    "id": "learn-costar-audience",
    "title": "COSTAR: Audience",
    "category": "Discover",
    "subcategory": "COSTAR Framework",
    "keywords": [
      "audience",
      "target",
      "reader",
      "user",
      "costar"
    ],
    "excerpt": "Identify who will receive the content. Tailor language and complexity to your audience.",
    "url": "learn/costar.html#audience"
  },
  {
    "id": "learn-costar-response",
    "title": "COSTAR: Response",
    "category": "Discover",
    "subcategory": "COSTAR Framework",
    "keywords": [
      "response",
      "format",
      "output",
      "structure",
      "costar"
    ],
    "excerpt": "Define the desired response format: list, paragraph, table, JSON. Specify exactly how output should be structured.",
    "url": "learn/costar.html#response"
  },
  {
    "id": "learn-react",
    "title": "ReAct Method",
    "category": "Discover",
    "subcategory": "Frameworks",
    "keywords": [
      "react",
      "method",
      "reasoning",
      "acting",
      "step by step",
      "transparent thinking"
    ],
    "excerpt": "Reasoning + Acting. For complex tasks requiring transparent, verifiable thinking processes. AI shows its work.",
    "url": "learn/react.html"
  },
  {
    "id": "learn-react-reasoning",
    "title": "ReAct: Reasoning",
    "category": "Discover",
    "subcategory": "ReAct Framework",
    "keywords": [
      "reasoning",
      "thinking",
      "analysis",
      "logic",
      "react"
    ],
    "excerpt": "The thinking phase where AI analyzes the problem, considers options, and plans its approach.",
    "url": "learn/react.html#reasoning"
  },
  {
    "id": "learn-react-acting",
    "title": "ReAct: Acting",
    "category": "Discover",
    "subcategory": "ReAct Framework",
    "keywords": [
      "acting",
      "action",
      "execution",
      "doing",
      "react"
    ],
    "excerpt": "The action phase where AI executes based on its reasoning. Clear connection between thought and action.",
    "url": "learn/react.html#acting"
  },
  {
    "id": "learn-flipped-interaction",
    "title": "Flipped Interaction Method",
    "category": "Discover",
    "subcategory": "Frameworks",
    "keywords": [
      "flipped interaction",
      "interview",
      "questions first",
      "personalized",
      "clarification"
    ],
    "excerpt": "Let AI interview you first. Prevent generic advice by having AI ask clarifying questions before responding.",
    "url": "learn/flipped-interaction.html"
  },
  {
    "id": "learn-flipped-why",
    "title": "Why Flipped Interaction Works",
    "category": "Discover",
    "subcategory": "Flipped Interaction",
    "keywords": [
      "why flipped",
      "benefits",
      "personalized",
      "relevant"
    ],
    "excerpt": "AI can't give great advice without understanding your situation. Flipping the interaction ensures relevant, tailored responses.",
    "url": "learn/flipped-interaction.html#why-it-works"
  },
  {
    "id": "learn-flipped-trigger",
    "title": "How to Trigger Flipped Interaction",
    "category": "Discover",
    "subcategory": "Flipped Interaction",
    "keywords": [
      "trigger",
      "how to",
      "ask questions",
      "before answering"
    ],
    "excerpt": "Simple prompts like \"Before answering, ask me 5 questions to understand my situation better\" activate this pattern.",
    "url": "learn/flipped-interaction.html#how-to-trigger"
  },
  {
    "id": "learn-chain-of-thought",
    "title": "Chain-of-Thought Framework",
    "category": "Discover",
    "subcategory": "Frameworks",
    "keywords": [
      "chain of thought",
      "step by step",
      "reasoning",
      "think through",
      "cot",
      "framework"
    ],
    "excerpt": "Encourage AI to think step-by-step through complex problems. Improves accuracy for math, logic, and analysis.",
    "url": "learn/chain-of-thought.html"
  },
  {
    "id": "learn-few-shot-learning",
    "title": "Few-Shot Learning Framework",
    "category": "Discover",
    "subcategory": "Frameworks",
    "keywords": [
      "few-shot",
      "examples",
      "demonstrations",
      "pattern matching",
      "framework"
    ],
    "excerpt": "Provide 2-5 examples to help AI understand the pattern you want. More effective than description for complex formats.",
    "url": "learn/few-shot-learning.html"
  },
  {
    "id": "learn-role-prompting",
    "title": "Role Prompting Framework",
    "category": "Discover",
    "subcategory": "Frameworks",
    "keywords": [
      "role prompting",
      "persona",
      "expert",
      "character",
      "act as",
      "framework"
    ],
    "excerpt": "Assign AI a specific role or persona to shape its responses. Useful for expert knowledge and consistent tone.",
    "url": "learn/role-prompting.html"
  },
  {
    "id": "learn-constrained-output",
    "title": "Constrained Output Framework",
    "category": "Discover",
    "subcategory": "Frameworks",
    "keywords": [
      "constrained output",
      "format",
      "structured",
      "json",
      "template",
      "framework"
    ],
    "excerpt": "Define exact output formats, lengths, and structures. Essential for integrating AI outputs into workflows.",
    "url": "learn/constrained-output.html"
  },
  {
    "id": "learn-self-consistency",
    "title": "Self-Consistency Framework",
    "category": "Discover",
    "subcategory": "Frameworks",
    "keywords": [
      "self-consistency",
      "multiple answers",
      "verification",
      "accuracy",
      "framework"
    ],
    "excerpt": "Ask AI to solve problems multiple ways and compare. Helps catch errors and improve accuracy.",
    "url": "learn/self-consistency.html"
  },
  {
    "id": "learn-prompt-chaining",
    "title": "Prompt Chaining Framework",
    "category": "Discover",
    "subcategory": "Frameworks",
    "keywords": [
      "prompt chaining",
      "workflow",
      "pipeline",
      "sequential",
      "multi-step",
      "framework"
    ],
    "excerpt": "Break complex tasks into sequential prompts where each builds on previous outputs. Essential for complex workflows.",
    "url": "learn/prompt-chaining.html"
  },
  {
    "id": "discover-hub",
    "title": "Discover Frameworks",
    "category": "Discover",
    "subcategory": "Hub",
    "keywords": [
      "discover",
      "frameworks",
      "browse",
      "categories",
      "prompt engineering",
      "all frameworks",
      "directory",
      "hub"
    ],
    "excerpt": "Browse 62+ prompt engineering frameworks organized by category. Structured templates, reasoning techniques, decomposition methods, and more.",
    "url": "learn/index.html"
  },
  {
    "id": "discover-structured-frameworks",
    "title": "Structured Frameworks",
    "category": "Discover",
    "subcategory": "Category",
    "keywords": [
      "structured frameworks",
      "crisp",
      "crispe",
      "costar",
      "templates",
      "repeatable prompts",
      "context structure",
      "constrained output"
    ],
    "excerpt": "Browse 5 structured prompt engineering frameworks: CRISP, CRISPE, COSTAR, Context and Structure, and Constrained Output. Step-by-step templates for clear, repeatable prompts.",
    "url": "learn/structured-frameworks.html"
  },
  {
    "id": "discover-reasoning-cot",
    "title": "Reasoning and CoT",
    "category": "Discover",
    "subcategory": "Category",
    "keywords": [
      "reasoning",
      "chain of thought",
      "cot",
      "zero-shot cot",
      "auto-cot",
      "step-back",
      "analogical reasoning",
      "thread of thought",
      "memory of thought"
    ],
    "excerpt": "Explore 14 chain-of-thought and reasoning frameworks for prompt engineering. From basic CoT to Analogical Reasoning, Step-Back, and Memory of Thought.",
    "url": "learn/reasoning-cot.html"
  },
  {
    "id": "discover-decomposition",
    "title": "Decomposition",
    "category": "Discover",
    "subcategory": "Category",
    "keywords": [
      "decomposition",
      "break down",
      "least-to-most",
      "tree of thought",
      "graph of thought",
      "plan-and-solve",
      "recursion of thought",
      "program of thought"
    ],
    "excerpt": "Explore 7 decomposition frameworks for breaking complex problems into manageable parts. Includes Least-to-Most, Tree of Thought, Graph of Thought, and more.",
    "url": "learn/decomposition.html"
  },
  {
    "id": "discover-self-correction",
    "title": "Self-Correction",
    "category": "Discover",
    "subcategory": "Category",
    "keywords": [
      "self-correction",
      "critique",
      "verify",
      "refine",
      "self-refine",
      "critic",
      "reflexion",
      "chain-of-verification",
      "self-calibration"
    ],
    "excerpt": "Explore 7 self-correction frameworks that enable AI to critique, verify, and refine its own outputs. Includes Self-Refine, CRITIC, Reflexion, and more.",
    "url": "learn/self-correction.html"
  },
  {
    "id": "discover-in-context-learning",
    "title": "In-Context Learning",
    "category": "Discover",
    "subcategory": "Category",
    "keywords": [
      "in-context learning",
      "few-shot",
      "zero-shot",
      "one-shot",
      "example selection",
      "knn prompting",
      "vote-k",
      "demo ensembling",
      "prompt mining"
    ],
    "excerpt": "Explore 9 in-context learning frameworks: Few-Shot, Zero-Shot, One-Shot, Example Selection, KNN Prompting, and more. Teach AI by demonstration.",
    "url": "learn/in-context-learning.html"
  },
  {
    "id": "discover-ensemble-methods",
    "title": "Ensemble Methods",
    "category": "Discover",
    "subcategory": "Category",
    "keywords": [
      "ensemble methods",
      "self-consistency",
      "cosp",
      "diverse prompting",
      "dense prompting",
      "meta-reasoning",
      "max mutual information",
      "multiple outputs"
    ],
    "excerpt": "Explore 7 ensemble prompting frameworks: Self-Consistency, COSP, DiVeRSe, Dense Prompting, and more. Combine multiple outputs for reliable results.",
    "url": "learn/ensemble-methods.html"
  },
  {
    "id": "discover-prompting-strategies",
    "title": "Prompting Strategies",
    "category": "Discover",
    "subcategory": "Category",
    "keywords": [
      "prompting strategies",
      "react",
      "role prompting",
      "emotion prompting",
      "prompt chaining",
      "s2a",
      "simtom",
      "flipped interaction",
      "style prompting"
    ],
    "excerpt": "Explore 11 versatile prompting strategies: ReAct, Role Prompting, Emotion Prompting, Prompt Chaining, S2A, SimToM, and more. Shape how AI approaches tasks.",
    "url": "learn/prompting-strategies.html"
  },
  {
    "id": "tool-analyzer",
    "title": "Prompt Analyzer",
    "category": "Tools",
    "subcategory": "Analysis",
    "keywords": [
      "analyzer",
      "score",
      "evaluate",
      "check",
      "test",
      "prompt quality",
      "feedback"
    ],
    "excerpt": "Analyze your prompts against CRISP, COSTAR, and CRISPE frameworks. Get scores and suggestions for improvement.",
    "url": "tools/analyzer.html"
  },
  {
    "id": "tool-builder",
    "title": "Prompt Builder",
    "category": "Tools",
    "subcategory": "Creation",
    "keywords": [
      "builder",
      "create",
      "construct",
      "guided",
      "step by step",
      "generate prompt"
    ],
    "excerpt": "Build structured prompts step-by-step with guided questions. Choose your framework and get a ready-to-use prompt.",
    "url": "tools/guidance.html"
  },
  {
    "id": "tool-matcher",
    "title": "Method Matcher",
    "category": "Tools",
    "subcategory": "Recommendation",
    "keywords": [
      "matcher",
      "recommend",
      "which method",
      "choose",
      "find"
    ],
    "excerpt": "Describe your task and get a recommendation for which prompting method to use. Find the right framework for your needs.",
    "url": "tools/matcher.html"
  },
  {
    "id": "tool-checklist",
    "title": "Preflight Checklist",
    "category": "Tools",
    "subcategory": "Review",
    "keywords": [
      "checklist",
      "preflight",
      "verify",
      "before sending",
      "review prompt"
    ],
    "excerpt": "Review your prompt before sending. Check for context, clarity, specificity, and potential issues.",
    "url": "tools/checklist.html"
  },
  {
    "id": "tool-hallucination",
    "title": "Hallucination Spotter",
    "category": "Tools",
    "subcategory": "Practice",
    "keywords": [
      "hallucination",
      "spotter",
      "fact check",
      "verify",
      "fake",
      "fabrication"
    ],
    "excerpt": "Practice identifying AI hallucinations. Learn to spot fake citations, invented facts, and fabricated information.",
    "url": "tools/hallucination.html"
  },
  {
    "id": "tool-persona",
    "title": "Persona Architect",
    "category": "Tools",
    "subcategory": "Creation",
    "keywords": [
      "persona",
      "role",
      "character",
      "tone",
      "expertise",
      "voice",
      "custom ai"
    ],
    "excerpt": "Design custom AI personas with sliders for expertise, tone, detail level, and creativity. Generate ready-to-use role prompts.",
    "url": "tools/persona.html"
  },
  {
    "id": "tool-temperature",
    "title": "Temperature Visualizer",
    "category": "Tools",
    "subcategory": "Learning",
    "keywords": [
      "temperature",
      "top-p",
      "randomness",
      "creativity",
      "settings",
      "parameters"
    ],
    "excerpt": "Understand temperature and top-p settings through visual analogies. Learn when to use strict vs creative AI responses.",
    "url": "tools/temperature.html"
  },
  {
    "id": "tool-bias",
    "title": "Bias Radar",
    "category": "Tools",
    "subcategory": "Analysis",
    "keywords": [
      "bias",
      "loaded language",
      "neutral",
      "presupposition",
      "leading questions"
    ],
    "excerpt": "Scan prompts for biased or loaded language. Get suggestions for neutral rewrites and improve prompt fairness.",
    "url": "tools/bias.html"
  },
  {
    "id": "tool-specificity",
    "title": "Specificity Slider",
    "category": "Tools",
    "subcategory": "Practice",
    "keywords": [
      "specificity",
      "vague",
      "clarity",
      "game",
      "clarification",
      "detail"
    ],
    "excerpt": "Card-based game where you improve vague prompts by adding clarification cards. Build your specificity skills while having fun.",
    "url": "tools/specificity.html"
  },
  {
    "id": "tool-jailbreak",
    "title": "Jailbreak Defender",
    "category": "Tools",
    "subcategory": "Practice",
    "keywords": [
      "jailbreak",
      "safety",
      "filter",
      "harmful",
      "malicious",
      "game",
      "defense"
    ],
    "excerpt": "Timed reaction game where you act as an AI safety filter. Quickly identify and block harmful prompts while accepting legitimate ones.",
    "url": "tools/jailbreak.html"
  },
  {
    "id": "tool-quiz",
    "title": "AI Readiness Quiz",
    "category": "Tools",
    "subcategory": "Assessment",
    "keywords": [
      "quiz",
      "test",
      "assessment",
      "readiness",
      "knowledge check",
      "level"
    ],
    "excerpt": "40 questions across 4 levels: Good, Pro, Expert, Master. Test your AI prompting knowledge from basics to advanced.",
    "url": "quiz/index.html"
  },
  {
    "id": "pattern-cot",
    "title": "Chain of Thought Pattern",
    "category": "Patterns",
    "subcategory": "Reasoning",
    "keywords": [
      "chain of thought",
      "step by step",
      "reasoning pattern",
      "think through"
    ],
    "excerpt": "Encourage step-by-step reasoning by asking AI to \"think through\" problems before answering. Best for math, logic, debugging.",
    "url": "patterns/index.html#chain-of-thought"
  },
  {
    "id": "pattern-few-shot",
    "title": "Few-Shot Learning Pattern",
    "category": "Patterns",
    "subcategory": "Output",
    "keywords": [
      "few-shot",
      "examples pattern",
      "input output",
      "demonstration"
    ],
    "excerpt": "Provide 2-5 examples of the input-output format you want before your actual request. Best for consistent formatting.",
    "url": "patterns/index.html#few-shot"
  },
  {
    "id": "pattern-role",
    "title": "Role Prompting Pattern",
    "category": "Patterns",
    "subcategory": "Role",
    "keywords": [
      "role prompting",
      "persona",
      "act as",
      "you are"
    ],
    "excerpt": "Assign AI a specific role or expertise to shape its responses and vocabulary. Best for technical writing and specialized content.",
    "url": "patterns/index.html#role-prompting"
  },
  {
    "id": "pattern-structured",
    "title": "Structured Output Pattern",
    "category": "Patterns",
    "subcategory": "Output",
    "keywords": [
      "structured output",
      "json",
      "table",
      "format",
      "outline"
    ],
    "excerpt": "Define the exact format you want (JSON, table, outline) before the request. Best for data extraction and consistent reports.",
    "url": "patterns/index.html#structured-output"
  },
  {
    "id": "pattern-self-consistency",
    "title": "Self-Consistency Pattern",
    "category": "Patterns",
    "subcategory": "Reasoning",
    "keywords": [
      "self-consistency",
      "verify",
      "multiple ways",
      "check work"
    ],
    "excerpt": "Ask AI to verify its own work or approach a problem multiple ways and compare. Best for math and fact-checking.",
    "url": "patterns/index.html#self-consistency"
  },
  {
    "id": "pattern-chaining",
    "title": "Prompt Chaining Pattern",
    "category": "Patterns",
    "subcategory": "Reasoning",
    "keywords": [
      "prompt chaining",
      "sequential",
      "multi-step",
      "workflow"
    ],
    "excerpt": "Break complex tasks into sequential prompts where each output feeds the next input. Best for long documents and research.",
    "url": "patterns/index.html#prompt-chaining"
  },
  {
    "id": "pattern-devils-advocate",
    "title": "Devil's Advocate Pattern",
    "category": "Patterns",
    "subcategory": "Role",
    "keywords": [
      "devils advocate",
      "critique",
      "weakness",
      "counter argument"
    ],
    "excerpt": "Ask AI to argue against a position or find weaknesses in an idea. Best for decision making and risk assessment.",
    "url": "patterns/index.html#devils-advocate"
  },
  {
    "id": "pattern-constraints",
    "title": "Constraints First Pattern",
    "category": "Patterns",
    "subcategory": "Output",
    "keywords": [
      "constraints first",
      "limitations",
      "requirements",
      "boundaries"
    ],
    "excerpt": "State limitations and requirements before the main request to frame the response. Best for length control and audience targeting.",
    "url": "patterns/index.html#constraints-first"
  },
  {
    "id": "pattern-flipped",
    "title": "Flipped Interaction Pattern",
    "category": "Patterns",
    "subcategory": "Interaction",
    "keywords": [
      "flipped interaction pattern",
      "interview",
      "ask questions first"
    ],
    "excerpt": "Ask AI to interview you first before giving advice. Prevents generic responses. Best for personalized advice and strategic planning.",
    "url": "patterns/index.html#flipped-interaction"
  },
  {
    "id": "faq-what-is-prompt",
    "title": "What is a prompt?",
    "category": "FAQ",
    "subcategory": "Getting Started",
    "keywords": [
      "what is prompt",
      "definition",
      "prompt meaning"
    ],
    "excerpt": "A prompt is the text you send to an AI assistant. It can be a question, instruction, request, or any combination.",
    "url": "pages/faq.html#getting-started"
  },
  {
    "id": "faq-which-ai",
    "title": "Which AI should I use?",
    "category": "FAQ",
    "subcategory": "Getting Started",
    "keywords": [
      "which ai",
      "chatgpt",
      "claude",
      "gemini",
      "best ai"
    ],
    "excerpt": "The techniques taught here work with any modern AI assistant. Each has different strengths, but prompting fundamentals are universal.",
    "url": "pages/faq.html#getting-started"
  },
  {
    "id": "faq-technical-skills",
    "title": "Do I need technical skills?",
    "category": "FAQ",
    "subcategory": "Getting Started",
    "keywords": [
      "technical skills",
      "coding",
      "beginner",
      "no experience"
    ],
    "excerpt": "No. AI assistants understand natural language. You don't need coding skills or technical knowledgejust clear communication.",
    "url": "pages/faq.html#getting-started"
  },
  {
    "id": "faq-where-start",
    "title": "Where should I start learning?",
    "category": "FAQ",
    "subcategory": "Getting Started",
    "keywords": [
      "where to start",
      "begin",
      "first steps",
      "framework selection"
    ],
    "excerpt": "Start with Prompt Basics to understand core concepts, then move to CRISP Method as your first framework.",
    "url": "pages/faq.html#getting-started"
  },
  {
    "id": "faq-which-method",
    "title": "Which prompting method should I use?",
    "category": "FAQ",
    "subcategory": "Prompting Methods",
    "keywords": [
      "which method",
      "choose method",
      "crisp vs costar",
      "best method"
    ],
    "excerpt": "CRISP for everyday tasks, CRISPE for creative work with examples, COSTAR for professional content, ReAct for complex problems.",
    "url": "pages/faq.html#prompting-methods"
  },
  {
    "id": "faq-memorize-frameworks",
    "title": "Do I have to memorize these frameworks?",
    "category": "FAQ",
    "subcategory": "Prompting Methods",
    "keywords": [
      "memorize",
      "remember",
      "learn frameworks",
      "training wheels"
    ],
    "excerpt": "No. Frameworks are learning tools. Once you internalize the concepts, you'll naturally include the right details without following a structure.",
    "url": "pages/faq.html#prompting-methods"
  },
  {
    "id": "faq-combine-methods",
    "title": "Can I combine different methods?",
    "category": "FAQ",
    "subcategory": "Prompting Methods",
    "keywords": [
      "combine methods",
      "mix frameworks",
      "hybrid approach"
    ],
    "excerpt": "Absolutely. The methods aren't rigid rulesthey're guidelines. Take elements from different frameworks based on what your task needs.",
    "url": "pages/faq.html#prompting-methods"
  },
  {
    "id": "faq-crisp-vs-crispe",
    "title": "What's the difference between CRISP and CRISPE?",
    "category": "FAQ",
    "subcategory": "Prompting Methods",
    "keywords": [
      "crisp vs crispe",
      "difference",
      "example component"
    ],
    "excerpt": "CRISPE adds an \"Example\" component for few-shot learning. Useful for creative tasks where showing examples is more effective than describing.",
    "url": "pages/faq.html#prompting-methods"
  },
  {
    "id": "faq-hallucination",
    "title": "What is a hallucination?",
    "category": "FAQ",
    "subcategory": "AI Limitations",
    "keywords": [
      "hallucination",
      "fake information",
      "incorrect",
      "fabrication"
    ],
    "excerpt": "When AI generates plausible but incorrect or made-up information. Includes fake citations, invented statistics, and fictional events.",
    "url": "pages/faq.html#ai-limitations"
  },
  {
    "id": "faq-professional-advice",
    "title": "Can I trust AI for medical, legal, or financial advice?",
    "category": "FAQ",
    "subcategory": "AI Safety",
    "keywords": [
      "medical advice",
      "legal advice",
      "financial advice",
      "professional help"
    ],
    "excerpt": "No. AI should never replace professional advice in high-stakes domains. Use it to prepare questions, but critical decisions require qualified human professionals.",
    "url": "pages/faq.html#ai-limitations"
  },
  {
    "id": "faq-data-safety",
    "title": "Is my data safe when using AI?",
    "category": "FAQ",
    "subcategory": "AI Safety",
    "keywords": [
      "data safety",
      "privacy",
      "security",
      "confidential"
    ],
    "excerpt": "Assume anything you type could be stored or used for training. Never share passwords, personal identifiers, or confidential business information.",
    "url": "pages/faq.html#ai-limitations"
  },
  {
    "id": "faq-confident-wrong",
    "title": "Why does AI sometimes give wrong answers confidently?",
    "category": "FAQ",
    "subcategory": "AI Limitations",
    "keywords": [
      "confident wrong",
      "incorrect confident",
      "why wrong"
    ],
    "excerpt": "AI predicts likely text patternsit doesn't \"know\" things like humans. When it lacks information, it doesn't have uncertainty signals. Verification is essential.",
    "url": "pages/faq.html#ai-limitations"
  },
  {
    "id": "faq-is-free",
    "title": "Is Praxis really free?",
    "category": "FAQ",
    "subcategory": "About Praxis",
    "keywords": [
      "free",
      "cost",
      "pricing",
      "no account"
    ],
    "excerpt": "Yes, completely free. No account required, no premium tiers, no ads, no tracking. AI skills should be accessible to everyone.",
    "url": "pages/faq.html#about-praxis"
  },
  {
    "id": "faq-uses-ai",
    "title": "Does this site use AI?",
    "category": "FAQ",
    "subcategory": "About Praxis",
    "keywords": [
      "site uses ai",
      "built with ai",
      "ai generated"
    ],
    "excerpt": "Absolutely! The content and tools were developed with AI assistants. But interactive tools work locallynothing you type is transmitted anywhere.",
    "url": "pages/faq.html#about-praxis"
  },
  {
    "id": "faq-who-created",
    "title": "Who created Praxis?",
    "category": "FAQ",
    "subcategory": "About Praxis",
    "keywords": [
      "creator",
      "founder",
      "who made",
      "basiliso"
    ],
    "excerpt": "Praxis was created by Basiliso Rosario with the mission of demystifying AI and making effective prompting skills accessible to everyone.",
    "url": "pages/faq.html#about-praxis"
  },
  {
    "id": "resource-chatgpt",
    "title": "ChatGPT Guide",
    "category": "Resources",
    "subcategory": "AI Platforms",
    "keywords": [
      "chatgpt",
      "openai",
      "guide",
      "how to use chatgpt",
      "llm",
      "prompting"
    ],
    "excerpt": "Comprehensive guide to using ChatGPT effectively. Features, prompting techniques, and best practices for all skill levels.",
    "url": "pages/chatgpt-guide.html"
  },
  {
    "id": "page-resources",
    "title": "Resources Hub",
    "category": "Resources",
    "subcategory": "Overview",
    "keywords": [
      "resources",
      "guides",
      "references",
      "documentation",
      "hub",
      "getting started"
    ],
    "excerpt": "Explore all Praxis resources: getting started guides, AI glossary, FAQ, and documentation about our mission and implementation.",
    "url": "pages/resources.html"
  },
  {
    "id": "page-about",
    "title": "About Praxis",
    "category": "Resources",
    "subcategory": "Site Info",
    "keywords": [
      "about",
      "praxis",
      "mission",
      "founder",
      "why praxis"
    ],
    "excerpt": "Learn about Praxis, its mission to make AI accessible to everyone, and the founder's vision for AI education.",
    "url": "pages/about.html"
  },
  {
    "id": "page-ai-safety",
    "title": "AI Safety",
    "category": "Resources",
    "subcategory": "AI Safety",
    "keywords": [
      "ai safety",
      "responsible ai",
      "limitations",
      "risks",
      "best practices"
    ],
    "excerpt": "Understand AI limitations, risks, and best practices for responsible use. Essential knowledge for effective AI interaction.",
    "url": "pages/ai-safety.html"
  },
  {
    "id": "page-security",
    "title": "Security Analysis",
    "category": "Resources",
    "subcategory": "Security",
    "keywords": [
      "security",
      "csp",
      "owasp",
      "hardening",
      "content security policy",
      "xss",
      "privacy",
      "defense in depth"
    ],
    "excerpt": "Comprehensive security analysis and hardening practices. Learn about our A+ CSP rating and continuous security as a practice.",
    "url": "pages/security.html"
  },
  {
    "id": "page-ai-for-everybody",
    "title": "AI for Everybody",
    "category": "Resources",
    "subcategory": "Philosophy",
    "keywords": [
      "accessibility",
      "inclusive",
      "everyone",
      "learning",
      "no prerequisites",
      "free",
      "open"
    ],
    "excerpt": "Our mission to make AI literacy accessible to everyone regardless of background, ability, or experience level.",
    "url": "pages/ai-for-everybody.html"
  },
  {
    "id": "page-universal-design",
    "title": "Universal Design (UD/UDL)",
    "category": "Resources",
    "subcategory": "Accessibility",
    "keywords": [
      "ud",
      "udl",
      "universal design",
      "universal design for learning",
      "wcag",
      "accessibility",
      "inclusive"
    ],
    "excerpt": "How Praxis implements Universal Design and Universal Design for Learning principles for accessible education.",
    "url": "pages/universal-design.html"
  },
  {
    "id": "page-performance",
    "title": "Performance Analysis",
    "category": "Resources",
    "subcategory": "Technical",
    "keywords": [
      "performance",
      "lighthouse",
      "speed",
      "optimization",
      "core web vitals",
      "fast",
      "efficient"
    ],
    "excerpt": "Technical breakdown of how Praxis achieves 100% Lighthouse performance through zero dependencies and optimization.",
    "url": "pages/performance.html"
  },
  {
    "id": "page-ai-assisted-building",
    "title": "AI Assisted Building",
    "category": "Resources",
    "subcategory": "Technical",
    "keywords": [
      "claude",
      "claude code",
      "ai assisted",
      "development",
      "collaboration",
      "human-ai"
    ],
    "excerpt": "How Praxis was built with Claude Code - demonstrating practical human-AI collaboration in software development.",
    "url": "pages/ai-assisted-building.html"
  },
  {
    "id": "page-glossary",
    "title": "AI Glossary",
    "category": "Resources",
    "subcategory": "Reference",
    "keywords": [
      "glossary",
      "definitions",
      "terms",
      "vocabulary",
      "dictionary"
    ],
    "excerpt": "Key terms and concepts for understanding AI and effective prompting. Your reference for AI terminology.",
    "url": "pages/glossary.html"
  },
  {
    "id": "page-faq",
    "title": "Frequently Asked Questions",
    "category": "Resources",
    "subcategory": "Reference",
    "keywords": [
      "faq",
      "questions",
      "help",
      "answers",
      "common questions"
    ],
    "excerpt": "Common questions about AI, prompting, and getting the most from this resource. Find answers quickly.",
    "url": "pages/faq.html"
  },
  {
    "id": "page-patterns",
    "title": "Prompt Patterns Library",
    "category": "Resources",
    "subcategory": "Reference",
    "keywords": [
      "patterns",
      "library",
      "templates",
      "reusable",
      "frameworks"
    ],
    "excerpt": "Reusable frameworks for common tasks. Patterns to understand and adapt, not templates to copy.",
    "url": "patterns/index.html"
  },
  {
    "id": "learn-many-shot",
    "title": "Many-Shot Prompting",
    "category": "Discover",
    "subcategory": "In-Context Learning",
    "keywords": [
      "many-shot",
      "in-context learning",
      "examples",
      "demonstrations",
      "large context",
      "hundreds",
      "pattern learning",
      "classification",
      "format consistency"
    ],
    "excerpt": "Provide dozens to thousands of demonstrations within the prompt, leveraging extended context windows to teach complex patterns through volume of examples.",
    "url": "learn/many-shot.html"
  },
  {
    "id": "learn-example-ordering",
    "title": "Example Ordering",
    "category": "Discover",
    "subcategory": "In-Context Learning",
    "keywords": [
      "example ordering",
      "permutation",
      "recency bias",
      "majority label bias",
      "few-shot",
      "demonstration order",
      "sequence",
      "arrangement"
    ],
    "excerpt": "Optimize the sequence of few-shot demonstrations to maximize model performance. The same examples in different orders can swing accuracy by 20+ percentage points.",
    "url": "learn/example-ordering.html"
  },
  {
    "id": "learn-self-generated-icl",
    "title": "Self-Generated ICL",
    "category": "Discover",
    "subcategory": "In-Context Learning",
    "keywords": [
      "self-generated",
      "in-context learning",
      "bootstrap",
      "auto-generate",
      "demonstrations",
      "cold start",
      "no labeled data",
      "self-teaching"
    ],
    "excerpt": "Have the model generate its own few-shot examples before tackling the actual task, eliminating the need for manually curated demonstrations.",
    "url": "learn/self-generated-icl.html"
  },
  {
    "id": "learn-active-example",
    "title": "Active Example Selection",
    "category": "Discover",
    "subcategory": "In-Context Learning",
    "keywords": [
      "active example",
      "active learning",
      "uncertainty",
      "dynamic selection",
      "query-aware",
      "targeted demonstrations",
      "confidence",
      "adaptive"
    ],
    "excerpt": "Dynamically choose few-shot demonstrations based on model uncertainty for each specific query, providing targeted examples where the model needs them most.",
    "url": "learn/active-example.html"
  },
  {
    "id": "learn-uncertainty-cot",
    "title": "Uncertainty-Routed CoT",
    "category": "Discover",
    "subcategory": "Reasoning & CoT",
    "keywords": [
      "uncertainty",
      "routed",
      "chain-of-thought",
      "confidence",
      "cost optimization",
      "selective reasoning",
      "triage",
      "conditional"
    ],
    "excerpt": "Conditionally apply chain-of-thought reasoning only when the model is uncertain, saving tokens on easy questions while maintaining accuracy on hard ones.",
    "url": "learn/uncertainty-cot.html"
  },
  {
    "id": "learn-image-prompting",
    "title": "Image Prompting Basics",
    "category": "Discover",
    "subcategory": "Image",
    "keywords": [
      "image prompting",
      "multimodal",
      "vision",
      "visual understanding",
      "image analysis",
      "image description",
      "visual input",
      "multimodal AI"
    ],
    "excerpt": "Foundational techniques for prompting AI models to understand, analyze, describe, and reason about images in multimodal contexts.",
    "url": "learn/modality/image/image-prompting.html"
  },
  {
    "id": "learn-multimodal-cot",
    "title": "Multimodal Chain of Thought",
    "category": "Discover",
    "subcategory": "Image",
    "keywords": [
      "multimodal cot",
      "visual reasoning",
      "chain-of-thought",
      "image reasoning",
      "rationale generation",
      "vision-language",
      "two-stage reasoning"
    ],
    "excerpt": "Extend chain-of-thought reasoning to combine visual and textual information, generating rationales that incorporate both modalities before producing answers.",
    "url": "learn/modality/image/multimodal-cot.html"
  },
  {
    "id": "learn-visual-cot",
    "title": "Visual Chain of Thought",
    "category": "Discover",
    "subcategory": "Image",
    "keywords": [
      "visual cot",
      "spatial reasoning",
      "region analysis",
      "visual decomposition",
      "image regions",
      "sequential visual analysis"
    ],
    "excerpt": "Guide AI models through step-by-step visual reasoning by explicitly referencing and analyzing image regions in sequence.",
    "url": "learn/modality/image/visual-cot.html"
  },
  {
    "id": "learn-image-as-text",
    "title": "Image-as-Text Prompting",
    "category": "Discover",
    "subcategory": "Image",
    "keywords": [
      "image-as-text",
      "visual description",
      "text conversion",
      "captioning",
      "OCR",
      "accessibility",
      "structured description"
    ],
    "excerpt": "Convert visual information into structured text descriptions to enable text-only models to reason about image content effectively.",
    "url": "learn/modality/image/image-as-text.html"
  },
  {
    "id": "learn-vqa",
    "title": "Visual Question Answering",
    "category": "Discover",
    "subcategory": "Image",
    "keywords": [
      "VQA",
      "visual question answering",
      "image questions",
      "visual grounding",
      "counting",
      "spatial relationships",
      "attribute recognition"
    ],
    "excerpt": "Techniques for asking precise questions about images and getting accurate, grounded answers from multimodal AI models.",
    "url": "learn/modality/image/vqa.html"
  },
  {
    "id": "learn-image-gen-prompting",
    "title": "Image Generation Prompting",
    "category": "Discover",
    "subcategory": "Image",
    "keywords": [
      "image generation",
      "text-to-image",
      "DALL-E",
      "Stable Diffusion",
      "Midjourney",
      "prompt crafting",
      "style modifiers",
      "quality terms"
    ],
    "excerpt": "Master the art of crafting text prompts that produce high-quality, accurate AI-generated images through structured description techniques.",
    "url": "learn/modality/image/image-gen-prompting.html"
  },
  {
    "id": "learn-negative-prompting",
    "title": "Negative Prompting",
    "category": "Discover",
    "subcategory": "Image",
    "keywords": [
      "negative prompt",
      "exclusion",
      "artifact avoidance",
      "quality improvement",
      "CFG scale",
      "classifier-free guidance",
      "image quality"
    ],
    "excerpt": "Specify what to exclude from AI-generated images to improve quality, avoid artifacts, and control output more precisely.",
    "url": "learn/modality/image/negative-prompting.html"
  },
  {
    "id": "learn-controlnet-prompting",
    "title": "ControlNet Prompting",
    "category": "Discover",
    "subcategory": "Image",
    "keywords": [
      "ControlNet",
      "spatial conditioning",
      "edge maps",
      "depth maps",
      "pose skeleton",
      "structural control",
      "guided generation"
    ],
    "excerpt": "Use structural conditioning inputs like edge maps, depth maps, and pose skeletons to precisely control AI image generation beyond text prompts alone.",
    "url": "learn/modality/image/controlnet-prompting.html"
  },
  {
    "id": "learn-inpainting-prompting",
    "title": "Inpainting Prompting",
    "category": "Discover",
    "subcategory": "Image",
    "keywords": [
      "inpainting",
      "image editing",
      "region replacement",
      "object removal",
      "image restoration",
      "selective editing",
      "mask-based editing"
    ],
    "excerpt": "Guide AI to selectively edit, replace, or restore specific regions within existing images while preserving surrounding context.",
    "url": "learn/modality/image/inpainting-prompting.html"
  },
  {
    "id": "learn-style-transfer",
    "title": "Style Transfer Prompting",
    "category": "Discover",
    "subcategory": "Image",
    "keywords": [
      "style transfer",
      "artistic style",
      "visual aesthetics",
      "medium",
      "painting style",
      "art direction",
      "prompt-based styling"
    ],
    "excerpt": "Apply artistic styles, visual aesthetics, and design languages to AI-generated or existing images through carefully crafted prompts.",
    "url": "learn/modality/image/style-transfer.html"
  },
  {
    "id": "learn-image-to-image",
    "title": "Image-to-Image Prompting",
    "category": "Discover",
    "subcategory": "Image",
    "keywords": [
      "image-to-image",
      "img2img",
      "image transformation",
      "denoising strength",
      "reference image",
      "style conversion",
      "sketch to render"
    ],
    "excerpt": "Transform existing images using AI by combining reference images with text prompts to control style, content, and transformation strength.",
    "url": "learn/modality/image/image-to-image.html"
  },
  {
    "id": "learn-composition-prompting",
    "title": "Composition Prompting",
    "category": "Discover",
    "subcategory": "Image",
    "keywords": [
      "composition",
      "framing",
      "rule of thirds",
      "camera angle",
      "depth of field",
      "visual hierarchy",
      "spatial arrangement"
    ],
    "excerpt": "Control the spatial arrangement, framing, and visual hierarchy of AI-generated images through structured compositional instructions.",
    "url": "learn/modality/image/composition-prompting.html"
  },
  {
    "id": "learn-audio-prompting",
    "title": "Audio Prompting Basics",
    "category": "Discover",
    "subcategory": "Audio",
    "keywords": [
      "audio prompting",
      "multimodal",
      "audio understanding",
      "audio analysis",
      "transcription",
      "audio reasoning",
      "audio input",
      "multimodal AI"
    ],
    "excerpt": "Foundational techniques for guiding AI models to understand, transcribe, analyze, and reason about audio inputs.",
    "url": "learn/modality/audio/audio-prompting.html"
  },
  {
    "id": "learn-stt-prompting",
    "title": "Speech-to-Text Prompting",
    "category": "Discover",
    "subcategory": "Audio",
    "keywords": [
      "speech-to-text",
      "transcription",
      "speaker attribution",
      "domain vocabulary",
      "structured text",
      "speech recognition",
      "formatted output",
      "audio transcription"
    ],
    "excerpt": "Techniques for controlling how speech is transcribed into structured, formatted text with speaker attribution and domain vocabulary.",
    "url": "learn/modality/audio/stt-prompting.html"
  },
  {
    "id": "learn-tts-prompting",
    "title": "Text-to-Speech Prompting",
    "category": "Discover",
    "subcategory": "Audio",
    "keywords": [
      "text-to-speech",
      "voice synthesis",
      "speaking style",
      "emotion",
      "pacing",
      "pronunciation",
      "speech generation",
      "voice control"
    ],
    "excerpt": "Methods for controlling voice synthesis including speaking style, emotion, pacing, and pronunciation through text instructions.",
    "url": "learn/modality/audio/tts-prompting.html"
  },
  {
    "id": "learn-audio-classification",
    "title": "Audio Classification",
    "category": "Discover",
    "subcategory": "Audio",
    "keywords": [
      "audio classification",
      "genre detection",
      "emotion recognition",
      "speaker identification",
      "environment detection",
      "event detection",
      "audio categorization",
      "sound analysis"
    ],
    "excerpt": "Prompt-based techniques for categorizing audio by genre, emotion, speaker identity, environment, and event detection.",
    "url": "learn/modality/audio/audio-classification.html"
  },
  {
    "id": "learn-music-gen",
    "title": "Music Generation Prompting",
    "category": "Discover",
    "subcategory": "Audio",
    "keywords": [
      "music generation",
      "audio composition",
      "musical intent",
      "AI music",
      "sound generation",
      "melody",
      "rhythm",
      "prompt-to-music"
    ],
    "excerpt": "Techniques for translating musical intent into natural language prompts that AI models use to generate audio compositions.",
    "url": "learn/modality/audio/music-gen.html"
  },
  {
    "id": "learn-voice-cloning",
    "title": "Voice Cloning Prompting",
    "category": "Discover",
    "subcategory": "Audio",
    "keywords": [
      "voice cloning",
      "reference audio",
      "voice reproduction",
      "speaker characteristics",
      "voice synthesis",
      "voice matching",
      "audio cloning",
      "voice transfer"
    ],
    "excerpt": "Methods for combining reference audio with text instructions to reproduce specific voice characteristics for new content.",
    "url": "learn/modality/audio/voice-cloning.html"
  },
  {
    "id": "learn-modality-hub",
    "title": "Modality Frameworks",
    "category": "Discover",
    "subcategory": "Hub",
    "keywords": [
      "modality",
      "image",
      "audio",
      "video",
      "code",
      "3D",
      "multimodal",
      "non-text"
    ],
    "excerpt": "Hub page for all modality-specific prompting frameworks: image, audio, video, code, and 3D content techniques.",
    "url": "learn/modality/index.html"
  }
]