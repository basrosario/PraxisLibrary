{
  "letter": "b",
  "count": 133,
  "terms": [
    {
      "id": "term-back-translation",
      "term": "Back-Translation",
      "definition": "A data augmentation technique for machine translation that translates monolingual target-language text back to the source language using a reverse model, creating synthetic parallel training data.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-backdoor-attack",
      "term": "Backdoor Attack",
      "definition": "A type of poisoning attack where an adversary introduces a hidden trigger into a model during training that causes targeted misclassification when the trigger is present in test inputs.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-background-removal",
      "term": "Background Removal",
      "definition": "The process of automatically separating foreground subjects from their background in images using deep learning segmentation and matting models, widely used in photography and e-commerce.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-backpropagation",
      "term": "Backpropagation",
      "definition": "The fundamental algorithm for training neural networks. It calculates how much each weight contributed to the error and adjusts weights accordingly, propagating the error signal backward through the network.",
      "tags": [
        "Training",
        "Neural Networks"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-backpropagation-discovery",
      "term": "Backpropagation Discovery",
      "definition": "The development of the backpropagation algorithm for training multi-layer neural networks. While the mathematical foundations existed earlier the 1986 paper by Rumelhart Hinton and Williams demonstrated its practical effectiveness and revived interest in neural networks.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-backpropagation-history",
      "term": "Backpropagation History",
      "definition": "The development of the backpropagation algorithm for training neural networks, independently discovered multiple times but popularized by Rumelhart, Hinton, and Williams in their influential 1986 Nature paper.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-backpropagation-through-time",
      "term": "Backpropagation Through Time",
      "definition": "An extension of backpropagation for training recurrent neural networks that unrolls the network through time steps and applies the chain rule. Computational cost is proportional to sequence length. Can suffer from vanishing or exploding gradients for long sequences.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bag-of-words",
      "term": "Bag of Words",
      "definition": "A simple text representation that counts word occurrences, ignoring order and grammar. Despite its simplicity, still useful for some classification tasks and as a baseline.",
      "tags": [
        "NLP",
        "Representation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-bagging",
      "term": "Bagging (Bootstrap Aggregating)",
      "definition": "Training multiple models on random subsets of data and averaging their predictions. Reduces variance and overfitting. The basis for Random Forest algorithms.",
      "tags": [
        "Technique",
        "Ensemble"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-baichuan",
      "term": "Baichuan",
      "definition": "A series of Chinese bilingual LLMs known for strong performance in Chinese language tasks. Part of the growing ecosystem of non-Western foundation models.",
      "tags": [
        "Model",
        "Chinese AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bandit-algorithm",
      "term": "Bandit Algorithm",
      "definition": "An algorithm for the multi-armed bandit problem that balances exploration (trying new actions) with exploitation (choosing the best-known action) to maximize cumulative reward over time.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bandwidth",
      "term": "Bandwidth (AI Context)",
      "definition": "The rate at which data can be transferred, crucial for AI infrastructure. Memory bandwidth often limits GPU performance; network bandwidth affects distributed training.",
      "tags": [
        "Infrastructure",
        "Performance"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-bandwidth-selection",
      "term": "Bandwidth Selection",
      "definition": "The process of choosing the bandwidth parameter in kernel density estimation, which controls the smoothness of the estimated density. Methods include cross-validation, Silverman's rule of thumb, and plug-in estimators.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-barbara-liskov",
      "term": "Barbara Liskov",
      "definition": "American computer scientist who received the 2008 Turing Award for contributions to programming language design including data abstraction and the Liskov substitution principle. Her work on CLU and Argus influenced software engineering practices used in AI systems.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-bard",
      "term": "Bard",
      "definition": "Google's conversational AI product, later renamed to Gemini. Competed with ChatGPT using Google's LLM technology and integration with Google services.",
      "tags": [
        "Product",
        "Historical"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-bark",
      "term": "Bark",
      "definition": "An open-source text-to-audio model by Suno AI that generates realistic speech in multiple languages including nonverbal sounds like laughter and music. Uses a GPT-style architecture for audio token generation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bart",
      "term": "BART",
      "definition": "Bidirectional and Auto-Regressive Transformers combines a bidirectional encoder with an autoregressive decoder. Pretrained by corrupting text with arbitrary noise and learning to reconstruct the original. Effective for generation summarization and translation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-base-model",
      "term": "Base Model",
      "definition": "A pre-trained model before fine-tuning for specific tasks. Base models are good at text completion but need instruction tuning to become helpful assistants.",
      "tags": [
        "Model Type",
        "Training"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-baseline",
      "term": "Baseline",
      "definition": "A simple model or approach used as a reference point for comparison. New methods should outperform baselines to demonstrate value. Common baselines include random guessing or simple rules.",
      "tags": [
        "Evaluation",
        "Research"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-batch",
      "term": "Batch",
      "definition": "A subset of training data processed together in one iteration. Batch processing improves training efficiency and stability compared to processing one example at a time.",
      "tags": [
        "Training",
        "Technical"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-batch-indexing",
      "term": "Batch Indexing",
      "definition": "The process of building or rebuilding a vector index from a complete dataset in a single operation, producing an optimally structured index that typically offers better search performance than incrementally built alternatives.",
      "tags": [
        "Vector Database",
        "Maintenance"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-batch-normalization",
      "term": "Batch Normalization",
      "definition": "A technique that normalizes the inputs to each layer by subtracting the batch mean and dividing by the batch standard deviation, then applying learned scale and shift parameters. It stabilizes and accelerates training.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-batch-processing-in-ai",
      "term": "Batch Processing in AI",
      "definition": "The practice of processing data in groups rather than individually during neural network training. Stochastic gradient descent with mini-batches became standard practice balancing computational efficiency with gradient estimate quality. The batch size is a key training hyperparameter.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-batch-rl",
      "term": "Batch Reinforcement Learning",
      "definition": "An approach to RL where the agent learns from a fixed batch of pre-collected transitions without online interaction. Batch RL methods like fitted Q-iteration address the challenges of learning from static datasets.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-batch-renormalization",
      "term": "Batch Renormalization",
      "definition": "An extension of batch normalization that introduces correction terms to reduce dependence on mini-batch statistics. Proposed by Ioffe in 2017 to address batch normalization failures with small batch sizes or non-i.i.d. mini-batches.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-batch-scheduling",
      "term": "Batch Scheduling for Inference",
      "definition": "The strategy of grouping multiple inference requests together for simultaneous processing on a GPU, improving hardware utilization and throughput. Batch scheduling involves tradeoffs between latency (waiting to fill batches) and throughput (processing more requests per second).",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-batch-size",
      "term": "Batch Size",
      "definition": "The number of training examples processed together before updating model weights. Larger batches provide more stable gradients but require more memory; smaller batches train faster but with more noise.",
      "tags": [
        "Hyperparameter",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-batched-inference",
      "term": "Batched Inference",
      "definition": "The practice of processing multiple inference requests simultaneously through a model to maximize GPU utilization and throughput, amortizing the cost of loading model weights across many inputs.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bayes-error-rate",
      "term": "Bayes Error Rate",
      "definition": "The lowest achievable error rate for any classifier on a given classification problem, determined by the irreducible noise in the data. It represents the theoretical performance limit set by the overlap between class distributions.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bayes-theorem",
      "term": "Bayes' Theorem",
      "definition": "A fundamental rule of probability that relates the conditional probability of a hypothesis given evidence to the prior probability of the hypothesis, the likelihood of the evidence, and the marginal probability of the evidence.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bayesian-inference",
      "term": "Bayesian Inference",
      "definition": "A statistical framework that updates probability estimates for hypotheses as additional evidence is acquired, using Bayes' theorem to compute posterior distributions from prior distributions and observed data.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bayesian-information-criterion",
      "term": "Bayesian Information Criterion",
      "definition": "A model selection criterion similar to AIC but with a larger penalty for the number of parameters that depends on sample size. It tends to favor simpler models than AIC, especially with large datasets.",
      "tags": [
        "Statistics",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bayesian",
      "term": "Bayesian Methods",
      "definition": "Statistical approaches that incorporate prior knowledge and update beliefs based on evidence. Used for uncertainty quantification, hyperparameter optimization, and probabilistic modeling.",
      "tags": [
        "Statistics",
        "Theory"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bayesian-model-averaging",
      "term": "Bayesian Model Averaging",
      "definition": "A technique that accounts for model uncertainty by averaging predictions across multiple models weighted by their posterior probabilities, rather than selecting a single best model.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bayesian-network",
      "term": "Bayesian Network",
      "definition": "A directed acyclic graph that represents a set of random variables and their conditional dependencies. Each node has a conditional probability table specifying the probability of the node given its parents.",
      "tags": [
        "Machine Learning",
        "Bayesian Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-bayesian-network-history",
      "term": "Bayesian Network History",
      "definition": "The development of Bayesian networks by Judea Pearl and others in the 1980s, providing a graphical framework for representing and reasoning under uncertainty that became central to AI and machine learning.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-bayesian-networks",
      "term": "Bayesian Networks",
      "definition": "Probabilistic graphical models that represent a set of variables and their conditional dependencies via directed acyclic graphs. Pioneered by Judea Pearl in the 1980s Bayesian networks enable reasoning under uncertainty and have applications in diagnosis prediction and decision making.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-bayesian-optimization",
      "term": "Bayesian Optimization",
      "definition": "A sequential strategy for optimizing expensive black-box functions that builds a probabilistic surrogate model (typically a Gaussian process) and uses an acquisition function to determine the most promising points to evaluate next.",
      "tags": [
        "Machine Learning",
        "Bayesian Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-beam-search",
      "term": "Beam Search",
      "definition": "A search algorithm used in text generation that maintains multiple candidate sequences at each step, selecting the most promising ones. Balances quality and computational cost compared to exhaustive search.",
      "tags": [
        "Generation",
        "Algorithm"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-beam-search-decoding",
      "term": "Beam Search Decoding",
      "definition": "A search algorithm for sequence generation that maintains the top-K partial sequences at each step. Explores multiple hypotheses simultaneously and selects the highest-scoring complete sequence. Widely used in machine translation and speech recognition.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-behavior-cloning",
      "term": "Behavior Cloning",
      "definition": "Learning to imitate expert behavior from demonstrations. The model learns to map observations to actions by copying what experts do in similar situations.",
      "tags": [
        "Training",
        "Imitation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-behavior-based-robotics",
      "term": "Behavior-Based Robotics",
      "definition": "An approach to robotics that generates complex behavior from the interaction of simple reactive behaviors rather than centralized planning. Pioneered by Rodney Brooks in the late 1980s this approach challenged traditional AI and proved effective for real-world robot control.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-behavioral-cloning-safety",
      "term": "Behavioral Cloning Safety",
      "definition": "Safety concerns arising from training AI agents to imitate human behavior. Includes distribution shift compounding errors and the risk of learning unsafe human behaviors along with desired ones.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-beijing-ai-principles",
      "term": "Beijing AI Principles",
      "definition": "A set of AI governance principles released in 2019 by the Beijing Academy of AI, emphasizing harmony, fairness, safety, shared benefits, and responsible development in the Chinese AI governance context.",
      "tags": [
        "Governance",
        "Regulation"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-beit",
      "term": "BEiT",
      "definition": "Bidirectional Encoder representation from Image Transformers applies masked image modeling as a pretraining task for vision transformers. Tokenizes image patches into visual tokens and predicts masked tokens similar to BERT in NLP.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bell-labs-ai-research",
      "term": "Bell Labs AI Research",
      "definition": "AI and machine learning research conducted at Bell Laboratories (AT&T) where foundational work was done on information theory speech recognition and neural networks. Yann LeCun developed early convolutional neural networks at Bell Labs in the late 1980s and 1990s.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-bellman-equation",
      "term": "Bellman Equation",
      "definition": "A recursive equation relating the value of a state to the immediate reward plus the discounted value of successor states. The Bellman equation provides the foundation for dynamic programming and most value-based RL algorithms.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-benchmark",
      "term": "Benchmark",
      "definition": "A standardized test or dataset used to evaluate and compare AI model performance. Common LLM benchmarks include MMLU, HellaSwag, and HumanEval for measuring different capabilities.",
      "tags": [
        "Evaluation",
        "Research"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-benchmark-gaming",
      "term": "Benchmark Gaming",
      "definition": "The practice of optimizing a model specifically to achieve high scores on popular benchmarks without corresponding improvements in real-world capabilities, often through data contamination or overfitting.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-benefit-sharing-in-ai",
      "term": "Benefit Sharing in AI",
      "definition": "The principle that the economic and social benefits generated by AI should be distributed broadly across society rather than concentrated among a small number of developers, companies, or nations.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-benefit-risk-analysis-for-ai",
      "term": "Benefit-Risk Analysis for AI",
      "definition": "A systematic comparison of the potential benefits and risks of an AI system to determine whether deployment is justified. Required by many regulatory frameworks for high-risk AI applications.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-berkeley-ai-research-lab",
      "term": "Berkeley AI Research Lab",
      "definition": "The Berkeley Artificial Intelligence Research Laboratory (BAIR) at UC Berkeley conducting research across computer vision NLP robotics and machine learning. BAIR has produced influential work on reinforcement learning robotic manipulation and open-source AI tools.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-bernoulli-distribution",
      "term": "Bernoulli Distribution",
      "definition": "The simplest discrete probability distribution, modeling a single trial with two outcomes (success with probability p, failure with probability 1-p). It is the building block of the binomial distribution.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bert",
      "term": "BERT (Bidirectional Encoder Representations from Transformers)",
      "definition": "A influential language model from Google (2018) that processes text bidirectionally, understanding context from both left and right. Revolutionized NLP and inspired many subsequent models.",
      "tags": [
        "Model",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bert-release",
      "term": "BERT Release",
      "definition": "Google's Bidirectional Encoder Representations from Transformers model, released in October 2018, which achieved state-of-the-art results across numerous NLP tasks through bidirectional pre-training and established a new paradigm for language understanding.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-bertscore",
      "term": "BERTScore",
      "definition": "An evaluation metric that computes the similarity between generated and reference texts using contextual BERT embeddings with greedy token matching, capturing semantic equivalence beyond exact surface-form overlap.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-best-of-n-sampling",
      "term": "Best-of-N Sampling",
      "definition": "An inference strategy that generates N candidate completions and returns the one scoring highest on a reward model, trading increased compute for better output quality without model modification.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-beta-distribution",
      "term": "Beta Distribution",
      "definition": "A continuous probability distribution defined on the interval [0, 1], parametrized by two shape parameters. It is commonly used as a prior distribution for probabilities in Bayesian inference.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-beta-vae",
      "term": "Beta-VAE",
      "definition": "A modification of the variational autoencoder that introduces a hyperparameter beta to weight the KL divergence term, promoting disentangled latent representations when beta is greater than one.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bev-perception",
      "term": "BEV Perception",
      "definition": "Bird's Eye View perception, a paradigm in autonomous driving that transforms multi-camera or LiDAR data into a unified top-down representation for joint 3D detection, segmentation, and prediction.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-bf16",
      "term": "BF16 (Brain Floating Point)",
      "definition": "A 16-bit floating-point format with 8 exponent bits (same as FP32) and 7 mantissa bits, developed by Google Brain. BF16 maintains FP32's dynamic range while halving memory, eliminating the need for loss scaling required by FP16.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bfloat16",
      "term": "bfloat16",
      "definition": "A 16-bit floating-point format optimized for neural network training. Sacrifices precision for range compared to float16, offering better training stability.",
      "tags": [
        "Technical",
        "Precision"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-bge",
      "term": "BGE",
      "definition": "BAAI General Embedding is a family of embedding models that achieve strong performance on text retrieval tasks. Trained by Beijing Academy of Artificial Intelligence with support for multiple languages and both short and long texts.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bi-encoder",
      "term": "Bi-Encoder",
      "definition": "A neural retrieval architecture that independently encodes queries and documents into fixed-size vectors using separate or shared encoders, enabling pre-computation of document embeddings and fast similarity search through vector comparison.",
      "tags": [
        "Retrieval",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-estimation-bias",
      "term": "Bias",
      "definition": "In statistical estimation, the difference between the expected value of an estimator and the true value of the parameter being estimated. An unbiased estimator has zero bias.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bias-amplification",
      "term": "Bias Amplification",
      "definition": "The phenomenon where machine learning models amplify biases present in training data producing outputs that are more biased than the data itself. Can create feedback loops that worsen inequality over time.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-bias-bounty",
      "term": "Bias Bounty",
      "definition": "A program that rewards individuals for identifying and reporting biases in AI systems. Similar to bug bounties in cybersecurity and aimed at crowdsourcing the discovery of unfair model behaviors.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-bias-in-ai",
      "term": "Bias in AI",
      "definition": "Systematic errors in AI system outputs that arise from prejudiced assumptions in training data algorithm design or deployment context. Can lead to unfair discriminatory or inaccurate outcomes for affected groups.",
      "tags": [
        "Safety",
        "Fundamentals"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-bias-mitigation",
      "term": "Bias Mitigation",
      "definition": "Techniques and practices for reducing unfair bias in AI systems. Includes pre-processing methods like resampling in-processing methods like adversarial debiasing and post-processing methods like threshold adjustment.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-bias-score",
      "term": "Bias Score",
      "definition": "A metric that measures the degree of systematic prejudice or unfair treatment in model outputs across demographic groups, assessed through differential response analysis, stereotype association tests, or fairness benchmarks.",
      "tags": [
        "Evaluation",
        "Safety"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-bias-testing",
      "term": "Bias Testing",
      "definition": "The systematic evaluation of AI systems for unfair biases across demographic groups and use cases. Includes statistical parity testing disparate impact analysis and intersectional bias assessment.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-bias-variance-decomposition",
      "term": "Bias-Variance Decomposition",
      "definition": "A mathematical decomposition of expected prediction error into three components: irreducible noise, squared bias (systematic error), and variance (sensitivity to training data), providing insight into sources of model error.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bias-variance-tradeoff",
      "term": "Bias-Variance Tradeoff",
      "definition": "The fundamental tension in supervised learning between a model's ability to minimize bias (error from overly simplistic assumptions) and variance (error from sensitivity to small fluctuations in the training set). Reducing one typically increases the other.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-biden-executive-order-on-ai",
      "term": "Biden Executive Order on AI",
      "definition": "Executive Order 14110, issued by President Biden in October 2023, establishing requirements for AI safety and security including red-teaming standards, reporting of large training runs, and federal agency AI governance.",
      "tags": [
        "Governance",
        "Regulation"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-bidirectional",
      "term": "Bidirectional",
      "definition": "Processing sequences in both directions (left-to-right and right-to-left). BERT processes bidirectionally for understanding; GPT processes unidirectionally for generation.",
      "tags": [
        "Architecture",
        "Processing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bidirectional-attention",
      "term": "Bidirectional Attention",
      "definition": "An attention pattern that allows each token to attend to all other tokens in both directions. Used in encoder models like BERT for building contextual representations. Contrasts with causal attention used in decoder models.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bidirectional-rnn",
      "term": "Bidirectional RNN",
      "definition": "A recurrent architecture that processes input sequences in both forward and backward directions simultaneously, combining both context directions to produce richer representations at each time step.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bigbench",
      "term": "BigBench",
      "definition": "Beyond the Imitation Game Benchmark, a large collaborative benchmark containing over 200 diverse tasks contributed by researchers, designed to probe language model capabilities including reasoning, translation, and social understanding that go beyond standard benchmarks.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-bigbird",
      "term": "BigBird",
      "definition": "A sparse attention transformer that combines random attention, window attention, and global attention patterns to achieve linear complexity while provably maintaining the expressive power of full attention.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-biggan",
      "term": "BigGAN",
      "definition": "A large-scale GAN that generates high-fidelity images by scaling up batch size model size and applying class-conditional generation with truncation. Demonstrated that GANs benefit significantly from scale.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bigram",
      "term": "Bigram / N-gram",
      "definition": "Sequences of N consecutive tokens used in language modeling. Bigrams are pairs; trigrams are triples. N-gram models were dominant before neural approaches but remain useful baselines.",
      "tags": [
        "NLP",
        "Historical"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-binary-classification",
      "term": "Binary Classification",
      "definition": "A classification task with exactly two possible outcomes (yes/no, spam/not spam). The simplest classification problem, often a building block for more complex tasks.",
      "tags": [
        "ML Task",
        "Classification"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-binary-cross-entropy",
      "term": "Binary Cross-Entropy",
      "definition": "A loss function for binary classification that measures the divergence between predicted probabilities and binary labels. Defined as -[y*log(p) + (1-y)*log(1-p)]. Equivalent to the negative log-likelihood of a Bernoulli distribution.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-binary-quantization",
      "term": "Binary Quantization",
      "definition": "An aggressive vector compression technique that reduces each vector dimension to a single bit based on its sign, enabling 32x compression from float32 and extremely fast Hamming distance comparisons at the cost of reduced recall accuracy.",
      "tags": [
        "Vector Database",
        "Quantization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-bing-chat",
      "term": "Bing Chat / Copilot",
      "definition": "Microsoft's AI-powered search assistant, integrating GPT-4 with web search. Can answer questions with citations, create content, and access current information.",
      "tags": [
        "Product",
        "Microsoft"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-binomial-distribution",
      "term": "Binomial Distribution",
      "definition": "A discrete probability distribution modeling the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. It is parametrized by n (trials) and p (success probability).",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bio-tagging",
      "term": "BIO Tagging",
      "definition": "An alternative name for IOB tagging format using Begin, Inside, Outside labels for sequence labeling tasks, where B marks the start of an entity and I continues it.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-bioes-tagging",
      "term": "BIOES Tagging",
      "definition": "An extended sequence labeling scheme that adds End and Single tags to the BIO format, providing more precise boundary information for named entities and improving recognition accuracy.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-biogpt",
      "term": "BioGPT",
      "definition": "A domain-specific generative language model pretrained on large-scale biomedical literature. Achieves strong performance on biomedical text generation question answering and relation extraction tasks.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-biometric-ai-regulation",
      "term": "Biometric AI Regulation",
      "definition": "Legal restrictions on AI systems that process biometric data such as facial features, fingerprints, or gait patterns, including bans on real-time biometric surveillance in public spaces under the EU AI Act.",
      "tags": [
        "Privacy",
        "Regulation"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-birch",
      "term": "BIRCH",
      "definition": "Balanced Iterative Reducing and Clustering using Hierarchies is a clustering algorithm designed for large datasets. Builds a compact summary called a CF-tree in a single pass through the data then applies standard clustering to the summary.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bisimulation-metric",
      "term": "Bisimulation Metric",
      "definition": "A distance metric on states that groups together states with similar reward and transition dynamics, providing a principled basis for state abstraction in RL. Deep bisimulation methods learn representations that preserve behaviorally relevant information.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-bit-precision",
      "term": "Bit Precision",
      "definition": "The number of bits used to represent model weights and activations. Lower precision (8-bit, 4-bit) reduces memory and increases speed but may affect accuracy.",
      "tags": [
        "Technical",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bitter-lesson",
      "term": "Bitter Lesson",
      "definition": "An influential 2019 essay by Rich Sutton arguing that the history of AI shows general methods leveraging computation (search and learning) ultimately outperform approaches that encode human domain knowledge.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-black-box",
      "term": "Black Box",
      "definition": "A system whose internal workings are not visible or understandable to users. Many AI models are considered black boxes because their decision-making processes are difficult to interpret.",
      "tags": [
        "Interpretability",
        "Concept"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-black-box-problem",
      "term": "Black Box Problem",
      "definition": "The challenge that many AI systems, particularly deep neural networks, operate in ways that are opaque to human understanding, making it difficult to explain, audit, or trust their decisions.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-blackboard-system",
      "term": "Blackboard System",
      "definition": "An AI architecture where multiple knowledge sources cooperate to solve a problem by reading from and writing to a shared data structure called the blackboard. Developed in the 1970s for speech understanding the architecture influenced multi-agent systems.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-bletchley-declaration-on-ai",
      "term": "Bletchley Declaration on AI",
      "definition": "A declaration signed by 28 countries at the November 2023 AI Safety Summit at Bletchley Park, acknowledging the potential for serious harm from frontier AI and committing to international cooperation on safety.",
      "tags": [
        "Governance",
        "Regulation"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-bletchley-park-ai-safety-summit",
      "term": "Bletchley Park AI Safety Summit",
      "definition": "The first major international AI Safety Summit held at Bletchley Park, UK, in November 2023, bringing together governments and AI companies to discuss frontier AI risks and establish international cooperation frameworks.",
      "tags": [
        "History",
        "Governance"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-bletchley-park-codebreaking",
      "term": "Bletchley Park Codebreaking",
      "definition": "The World War II British codebreaking operation where Alan Turing and colleagues developed the Bombe and Colossus machines to decrypt Axis communications, advancing computational methods that influenced early AI development.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-bleu-score",
      "term": "BLEU Score",
      "definition": "Bilingual Evaluation Understudy is a metric for evaluating machine translation quality by comparing n-gram overlap between candidate and reference translations. Introduced by Papineni et al. in 2002. Ranges from 0 to 1 with higher scores indicating better quality.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bleurt",
      "term": "BLEURT",
      "definition": "A learned evaluation metric that fine-tunes BERT on synthetic and human-rated data to predict text quality scores, providing robust assessments that correlate with human judgments even for paraphrases and semantically equivalent but lexically different outputs.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-bloom",
      "term": "BLOOM",
      "definition": "A large multilingual open-source language model created by BigScience, trained on 46 languages. Demonstrated the viability of collaborative, open AI development.",
      "tags": [
        "Model",
        "Open Source"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bloomberggpt",
      "term": "BloombergGPT",
      "definition": "A 50 billion parameter language model by Bloomberg trained on a mix of financial data and general text. Designed for financial NLP tasks while maintaining strong general language understanding capabilities.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-blue-team-ai",
      "term": "Blue Team (AI)",
      "definition": "A team responsible for defending AI systems against adversarial attacks and identifying vulnerabilities. Works in opposition to red teams to improve system security and robustness.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-bm25",
      "term": "BM25",
      "definition": "Best Matching 25, a probabilistic information retrieval ranking function that extends TF-IDF with document length normalization and term frequency saturation for more effective document scoring.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-bm25-in-rag",
      "term": "BM25 in RAG",
      "definition": "The application of the Best Matching 25 probabilistic ranking function within retrieval-augmented generation pipelines, providing strong lexical baseline retrieval that complements dense embedding search for finding documents with exact term matches.",
      "tags": [
        "Retrieval",
        "Search"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-boltzmann-exploration",
      "term": "Boltzmann Exploration",
      "definition": "An exploration strategy that selects actions with probability proportional to exponentiated Q-values divided by a temperature parameter. Higher temperatures increase randomness while lower temperatures converge toward greedy selection.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-boltzmann-machine",
      "term": "Boltzmann Machine",
      "definition": "A stochastic neural network model invented by Geoffrey Hinton and Terry Sejnowski in 1985 that uses simulated annealing to learn internal representations, representing an important step toward deep learning architectures.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-bonferroni-correction",
      "term": "Bonferroni Correction",
      "definition": "A multiple comparison adjustment that divides the significance level by the number of tests performed, controlling the family-wise error rate. It is conservative but simple to apply.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-boolean-retrieval",
      "term": "Boolean Retrieval",
      "definition": "Search using AND, OR, NOT operators to combine terms. Simple but limited compared to semantic search. Still used in specialized databases and advanced search interfaces.",
      "tags": [
        "Search",
        "Traditional"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-boosting",
      "term": "Boosting",
      "definition": "An ensemble technique that trains models sequentially, with each new model focusing on examples the previous ones got wrong. Powers XGBoost and LightGBM, popular for tabular data.",
      "tags": [
        "Technique",
        "ML"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-boosting-history",
      "term": "Boosting History",
      "definition": "The development of boosting algorithms from the theoretical work of Michael Kearns and Leslie Valiant (1988) through AdaBoost (Freund and Schapire 1995) to gradient boosting (Friedman 2001). Boosting transformed weak learners into strong learners through sequential error correction.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-bootstrap",
      "term": "Bootstrap",
      "definition": "A resampling technique that estimates the sampling distribution of a statistic by repeatedly drawing samples with replacement from the observed data. It provides standard errors, confidence intervals, and bias estimates.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bootstrap-aggregating",
      "term": "Bootstrap Aggregating",
      "definition": "An ensemble method (also called bagging) that trains multiple models on different bootstrap samples of the training data and combines their predictions by averaging (regression) or voting (classification) to reduce variance.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bottleneck",
      "term": "Bottleneck",
      "definition": "A narrow layer in a neural network that forces compression of information. Used in autoencoders and some architectures to learn efficient representations.",
      "tags": [
        "Architecture",
        "Design"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bottleneck-layer",
      "term": "Bottleneck Layer",
      "definition": "A narrow hidden layer that compresses representations to a lower dimension before expanding them, used in autoencoders and residual blocks to reduce computation and encourage abstraction.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-boundary-testing",
      "term": "Boundary Testing",
      "definition": "The practice of probing AI systems at the edges of their intended operating conditions to identify failure modes and safety limitations. Essential for understanding system behavior under stress.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-box-cox-transformation",
      "term": "Box-Cox Transformation",
      "definition": "A family of power transformations parametrized by lambda that aims to stabilize variance and make data more normally distributed. Special cases include the logarithmic transformation (lambda=0) and no transformation (lambda=1).",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bpe",
      "term": "BPE (Byte Pair Encoding)",
      "definition": "A tokenization algorithm that breaks text into subword units. Starts with individual characters and iteratively merges frequent pairs, balancing vocabulary size with the ability to handle rare words.",
      "tags": [
        "Tokenization",
        "NLP"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-brain-computer",
      "term": "Brain-Computer Interface (BCI)",
      "definition": "Technology connecting brain signals directly to computers. AI helps interpret neural signals for prosthetics, communication devices, and research applications.",
      "tags": [
        "Application",
        "Neuroscience"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-branch-and-bound",
      "term": "Branch and Bound",
      "definition": "An algorithmic paradigm for solving combinatorial optimization problems that systematically enumerates candidates while pruning large portions of the search space using bounds on the optimal solution. Used in integer programming and scheduling.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-brier-score",
      "term": "Brier Score",
      "definition": "A scoring metric that measures the accuracy of probabilistic predictions by computing the mean squared difference between predicted probabilities and actual binary outcomes. Lower values indicate better calibrated predictions.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-browse-mode",
      "term": "Browse Mode",
      "definition": "AI capability to access and retrieve current web information during conversations. Addresses knowledge cutoff limitations by fetching real-time data.",
      "tags": [
        "Feature",
        "Capability"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-bruce-buchanan",
      "term": "Bruce Buchanan",
      "definition": "American computer scientist who co-developed DENDRAL the first expert system with Edward Feigenbaum and Joshua Lederberg at Stanford. Buchanan's work on knowledge-based systems helped establish expert systems as a practical AI technology.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-buffer",
      "term": "Buffer (Memory)",
      "definition": "Temporary storage for data being processed. In AI agents, conversation buffers store recent exchanges. In training, data buffers optimize GPU utilization.",
      "tags": [
        "Technical",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-bundle-adjustment",
      "term": "Bundle Adjustment",
      "definition": "An optimization procedure that jointly refines 3D point positions and camera parameters by minimizing the reprojection error across all views, forming the core refinement step in 3D reconstruction pipelines.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-burst",
      "term": "Burst (API)",
      "definition": "Short periods of high API usage that may exceed normal rate limits. Many providers allow bursting with gradual throttling rather than hard cutoffs.",
      "tags": [
        "API",
        "Usage"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-burstiness",
      "term": "Burstiness",
      "definition": "A statistical property measuring the variability of sentence length and structure in text, often used in AI-generated text detection where machine-generated content tends to show lower burstiness (more uniform patterns) than human writing.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-byte-fallback",
      "term": "Byte Fallback",
      "definition": "A tokenization strategy that encodes unknown characters as individual bytes when they cannot be represented by the learned vocabulary, ensuring all possible inputs can be tokenized.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-byte-pair-encoding",
      "term": "Byte Pair Encoding",
      "definition": "A subword tokenization algorithm that iteratively merges the most frequent pair of consecutive bytes or characters. Originally a data compression technique adapted for NLP by Sennrich et al. in 2016. Used in GPT models for vocabulary construction.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-bpe-tokenizer",
      "term": "Byte Pair Encoding Tokenizer",
      "definition": "A subword tokenization algorithm that iteratively merges the most frequent pair of adjacent bytes or characters in the training corpus to build a vocabulary of variable-length subword units.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-byte-level-tokenization",
      "term": "Byte-Level Tokenization",
      "definition": "A tokenization approach that operates on raw bytes rather than characters, ensuring complete coverage of any text input without unknown tokens, used in models like GPT-2 with byte-level BPE.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    }
  ]
}