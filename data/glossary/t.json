{
  "letter": "t",
  "count": 241,
  "terms": [
    {
      "id": "term-t-sne",
      "term": "t-SNE",
      "definition": "A nonlinear dimensionality reduction technique that maps high-dimensional data to two or three dimensions for visualization by modeling pairwise similarities as probability distributions and minimizing KL divergence between the high- and low-dimensional representations.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-t-sne-algorithm",
      "term": "t-SNE Algorithm",
      "definition": "t-distributed Stochastic Neighbor Embedding is a nonlinear dimensionality reduction technique for visualizing high-dimensional data. Converts pairwise similarities to probability distributions and minimizes their KL divergence in low dimensions.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Dimensionality Reduction"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-t2i-compbench",
      "term": "T2I-CompBench",
      "definition": "A benchmark for evaluating compositional text-to-image generation. Tests the ability to generate images with correct attributes spatial relationships and complex compositions.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-t5",
      "term": "T5",
      "definition": "Text-to-Text Transfer Transformer, a model by Google that frames all NLP tasks as text-to-text problems, using an encoder-decoder architecture trained on a multi-task mixture.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-t5-text-to-text-transfer-transformer",
      "term": "T5 (Text-to-Text Transfer Transformer)",
      "definition": "A language model developed by Google Research in 2019 that frames all NLP tasks as text-to-text problems. T5 demonstrated that a unified approach to diverse NLP tasks using a single model architecture could achieve competitive or superior performance.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-tabby",
      "term": "Tabby",
      "definition": "An open-source AI coding assistant model designed for self-hosted code completion that supports multiple programming languages and IDE integrations.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tabfact",
      "term": "TabFact",
      "definition": "A table fact verification dataset containing 118000 statements about Wikipedia tables labeled as entailed or refuted. Tests the ability to reason about structured tabular data.",
      "tags": [
        "Benchmark",
        "NLP",
        "Tabular"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-table-extraction",
      "term": "Table Extraction",
      "definition": "The task of detecting tables in document images and extracting their structure (rows, columns, cells) and content into machine-readable format, combining visual detection with text recognition.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-table-transformer",
      "term": "Table Transformer",
      "definition": "A model based on DETR for detecting and recognizing table structures in document images including rows and columns and cell boundaries.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tableformer",
      "term": "TableFormer",
      "definition": "A Transformer-based model for table structure recognition that detects and reconstructs table layouts from document images for structured data extraction.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tabnet",
      "term": "TabNet",
      "definition": "A deep learning architecture for tabular data that uses sequential attention to select features at each decision step. Provides instance-wise feature selection and interpretability. Achieves strong performance on structured data tasks.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tabnet-benchmark",
      "term": "TabNet Benchmark",
      "definition": "A collection of tabular datasets used to evaluate deep learning methods for structured data. Tests whether neural networks can match gradient boosted trees on tabular tasks.",
      "tags": [
        "Benchmark",
        "Tabular"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tabu-search",
      "term": "Tabu Search",
      "definition": "A metaheuristic optimization method that enhances local search by maintaining a memory of recently visited solutions to avoid cycling. Uses adaptive memory and strategic diversification to explore the search space effectively.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-tabular-chain-of-thought",
      "term": "Tabular Chain-of-Thought",
      "definition": "A prompting variant that formats intermediate reasoning steps as structured tables rather than free-form text, improving clarity and consistency in multi-step reasoning by organizing variables, values, and operations in tabular form.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tacotron-2",
      "term": "Tacotron 2",
      "definition": "A neural network architecture from Google for speech synthesis that combines a sequence-to-sequence model with a modified WaveNet vocoder.",
      "tags": [
        "Models",
        "Technical",
        "Audio",
        "History"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tango",
      "term": "Tango",
      "definition": "A text-to-audio generation model that uses a latent diffusion approach with instruction-tuned language model conditioning for high-quality sound synthesis.",
      "tags": [
        "Models",
        "Technical",
        "Audio"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tango-2",
      "term": "Tango 2",
      "definition": "An improved text-to-audio model that uses direct preference optimization to align generated audio more closely with human preferences and text descriptions.",
      "tags": [
        "Models",
        "Technical",
        "Audio"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tanh",
      "term": "Tanh",
      "definition": "Hyperbolic tangent activation function that maps inputs to values between -1 and 1. Defined as f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)). Provides zero-centered outputs which can improve convergence compared to sigmoid.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-tape-out",
      "term": "Tape-Out",
      "definition": "Final step in chip design where the completed design is sent to a foundry for manufacturing. Named after the historical practice of sending designs on magnetic tape for mask creation.",
      "tags": [
        "Manufacturing",
        "Process",
        "Milestone"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tapir",
      "term": "TAPIR",
      "definition": "Tracking Any Point with per-frame Initialization and temporal Refinement is a model for dense point tracking across video frames.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-target-encoding",
      "term": "Target Encoding",
      "definition": "A feature encoding method that replaces each categorical value with the mean of the target variable for that category, often combined with smoothing or cross-validation to prevent overfitting.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-target-network",
      "term": "Target Network",
      "definition": "A slowly updated copy of the value network used to compute stable TD targets in deep RL algorithms like DQN. Target networks reduce oscillation and divergence caused by the moving target problem in bootstrapped learning.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-targeted-learning-algorithm",
      "term": "Targeted Learning Algorithm",
      "definition": "A semiparametric estimation framework that combines machine learning with statistical theory to estimate causal parameters. Uses targeted minimum loss-based estimation (TMLE) to debias initial estimates.",
      "tags": [
        "Algorithms",
        "Technical",
        "Causal"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-tarjans-algorithm",
      "term": "Tarjan's Algorithm",
      "definition": "A depth-first search based algorithm that finds all strongly connected components in a directed graph in linear time. Uses a stack and low-link values to identify components as the DFS backtracks.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-taskmatrix-benchmark",
      "term": "TaskMatrix Benchmark",
      "definition": "A benchmark evaluating the ability of AI systems to use visual foundation models as tools for complex visual tasks. Tests multimodal tool-use capabilities.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-taskonomy",
      "term": "Taskonomy",
      "definition": "A dataset of 4 million images from 600 buildings with annotations for 26 visual tasks. Used to study relationships between visual tasks and transfer learning efficiency.",
      "tags": [
        "Benchmark",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tatoeba",
      "term": "Tatoeba",
      "definition": "A collection of sentences and translations in over 400 languages maintained by volunteers. Used as a multilingual sentence similarity and translation evaluation benchmark.",
      "tags": [
        "Benchmark",
        "NLP",
        "Translation",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-taxi1500",
      "term": "Taxi1500",
      "definition": "A text classification benchmark covering 1500 languages with Bible verse topic classification. One of the most linguistically diverse NLP evaluation resources available.",
      "tags": [
        "Benchmark",
        "NLP",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-td-lambda",
      "term": "TD(lambda)",
      "definition": "A temporal difference algorithm that blends multi-step returns using an exponentially-weighted average controlled by the lambda parameter. TD(lambda) unifies TD(0) and Monte Carlo methods through eligibility traces.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-td-gammon",
      "term": "TD-Gammon",
      "definition": "A backgammon-playing program developed by Gerald Tesauro at IBM in 1992 using temporal difference learning. TD-Gammon achieved expert-level play through self-play reinforcement learning and demonstrated the potential of neural network-based game playing.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-teacher-forcing",
      "term": "Teacher Forcing",
      "definition": "A training strategy for sequence models that uses ground truth tokens as input at each step rather than the model's own predictions. Accelerates training convergence but can cause exposure bias when the model encounters its own predictions at inference.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-teacher-student-framework",
      "term": "Teacher-Student Framework",
      "definition": "A model compression paradigm where a large pretrained teacher model guides the training of a smaller student model by providing soft targets, feature maps, or attention patterns as supervision.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-teaching-learning-based-optimization",
      "term": "Teaching-Learning-Based Optimization",
      "definition": "A metaheuristic that simulates the teaching-learning process in a classroom. The teacher phase moves learners toward the best solution while the learner phase improves through interaction with randomly selected peers.",
      "tags": [
        "Algorithms",
        "Technical",
        "Metaheuristic"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-technical-debt-in-ai",
      "term": "Technical Debt in AI",
      "definition": "The accumulated cost of shortcuts and suboptimal decisions in AI system development that must eventually be addressed. In ML systems includes data debt configuration debt and pipeline debt.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-technical-prompting",
      "term": "Technical Prompting",
      "definition": "The practice of crafting prompts that incorporate precise technical specifications, domain terminology, and structured requirements to generate accurate technical documentation, code, architectures, or engineering solutions.",
      "tags": [
        "Prompt Engineering",
        "Technical"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-technological-singularity",
      "term": "Technological Singularity",
      "definition": "A hypothetical future point when technological growth becomes uncontrollable and irreversible resulting in unforeseeable changes to human civilization. Popularized by Vernor Vinge in 1993 and Ray Kurzweil the concept often centers on the development of superintelligent AI.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-technological-unemployment",
      "term": "Technological Unemployment",
      "definition": "Unemployment caused by technological advances outpacing the economy's ability to create new jobs, a longstanding concern significantly amplified by the rapid advancement of AI and automation capabilities.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-technology-assessment-for-ai",
      "term": "Technology Assessment for AI",
      "definition": "A systematic multidisciplinary evaluation of the societal implications of AI technology. Informs policy decisions by examining potential benefits risks and alternatives before widespread deployment.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-telechat",
      "term": "TeleChat",
      "definition": "A large language model from China Telecom designed for enterprise telecommunications applications with domain-specific training data.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-temperature",
      "term": "Temperature",
      "definition": "A parameter controlling randomness in AI outputs. Temperature 0 gives deterministic responses; higher values (0.7-1.0) increase creativity and variety; very high values may produce incoherence.",
      "tags": [
        "Parameter",
        "Generation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-temperature-scaling",
      "term": "Temperature Scaling",
      "definition": "A technique that adjusts the softmax distribution over token probabilities by dividing logits by a temperature parameter. Lower temperatures sharpen the distribution toward greedy decoding while higher temperatures produce more uniform and diverse sampling.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-template-matching-algorithm",
      "term": "Template Matching Algorithm",
      "definition": "A technique for finding regions in an image that match a template image by sliding the template across the image and computing a similarity measure. Common metrics include normalized cross-correlation and sum of squared differences.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Vision"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-temporal-action-detection",
      "term": "Temporal Action Detection",
      "definition": "The task of identifying the start time, end time, and category of each action instance in an untrimmed video, requiring both temporal localization and activity classification.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-temporal-coherence",
      "term": "Temporal Coherence",
      "definition": "The consistency of visual elements across consecutive frames in generated or processed video, ensuring smooth motion, stable appearance, and absence of flickering or morphing artifacts.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-temporal-credit-assignment",
      "term": "Temporal Credit Assignment",
      "definition": "The specific aspect of credit assignment concerned with distributing reward information backward through time to earlier actions that contributed to the outcome. Eligibility traces and multi-step returns are techniques that address temporal credit assignment.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-temporal-difference-learning",
      "term": "Temporal Difference Learning",
      "definition": "A family of RL methods that update value estimates based on the difference between successive predictions, combining ideas from Monte Carlo and dynamic programming. TD methods learn directly from experience without requiring a model of the environment.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-temporal-reasoning-in-ai",
      "term": "Temporal Reasoning in AI",
      "definition": "The area of AI concerned with representing and reasoning about time and temporal relationships between events. Includes approaches such as Allen's interval algebra (1983) and various temporal logics used in planning and natural language understanding.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensor-cores",
      "term": "Tensor Cores",
      "definition": "Specialized matrix multiply-and-accumulate units in NVIDIA GPUs that accelerate mixed-precision matrix operations fundamental to deep learning. Tensor Cores perform entire matrix operations in a single clock cycle, delivering an order-of-magnitude speedup over standard CUDA cores for AI workloads.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensor-decomposition",
      "term": "Tensor Decomposition",
      "definition": "A generalization of matrix decomposition to higher-order arrays that factors tensors into sums of simpler components. Includes CP decomposition and Tucker decomposition used in data analysis and signal processing.",
      "tags": [
        "Algorithms",
        "Technical",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensor-parallelism",
      "term": "Tensor Parallelism",
      "definition": "A form of model parallelism that splits individual weight matrices across multiple devices, distributing the computation of each layer while requiring communication to synchronize partial results.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensor-processing-unit-v5e",
      "term": "Tensor Processing Unit v5e",
      "definition": "Google cost-optimized TPU variant designed for efficient AI inference and smaller-scale training workloads. Offers better price-performance than the full TPU v5p for many use cases.",
      "tags": [
        "TPU",
        "Google",
        "Inference"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensor-processing-unit-v5p",
      "term": "Tensor Processing Unit v5p",
      "definition": "Google highest-performance TPU variant designed for large-scale AI training. Provides maximum compute density and interconnect bandwidth for training frontier AI models.",
      "tags": [
        "TPU",
        "Google",
        "Training"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensorflow",
      "term": "TensorFlow",
      "definition": "Google's open-source deep learning framework, widely used for production ML systems. Known for deployment tools and TPU support, though PyTorch has gained research share.",
      "tags": [
        "Framework",
        "Deep Learning"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensorflow-datasets",
      "term": "TensorFlow Datasets",
      "definition": "A collection of ready-to-use datasets for TensorFlow with standardized loading and preprocessing. Covers computer vision NLP audio and reinforcement learning tasks.",
      "tags": [
        "Platform",
        "General"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensorflow-release",
      "term": "TensorFlow Release",
      "definition": "The open-source release of TensorFlow by Google Brain in November 2015. TensorFlow provided a comprehensive framework for building and training neural networks and became one of the most widely used deep learning libraries accelerating AI research and deployment.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensorrt",
      "term": "TensorRT",
      "definition": "NVIDIA's high-performance inference optimization SDK that applies layer fusion, kernel auto-tuning, precision calibration, and dynamic tensor memory management. TensorRT can deliver 2-5x inference speedups over unoptimized frameworks on NVIDIA GPUs.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tenstorrent",
      "term": "Tenstorrent",
      "definition": "AI hardware startup founded by Jim Keller designing RISC-V based AI accelerator chips. Their Wormhole and Grayskull processors use a mesh architecture for scalable AI compute.",
      "tags": [
        "Accelerator",
        "Startup"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-ternary-search",
      "term": "Ternary Search",
      "definition": "A divide-and-conquer search algorithm that splits the search space into three parts to find the maximum or minimum of a unimodal function. Requires the function to have exactly one peak or valley in the interval.",
      "tags": [
        "Algorithms",
        "Technical",
        "Searching"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-ternary-search-tree",
      "term": "Ternary Search Tree",
      "definition": "A type of trie that uses three-way branching (less than and equal to and greater than) at each node. Combines the space efficiency of binary search trees with the time efficiency of tries.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-terry-sejnowski",
      "term": "Terry Sejnowski",
      "definition": "American computational neuroscientist who co-invented the Boltzmann machine with Geoffrey Hinton in 1985. President of the Salk Institute's Computational Neurobiology Laboratory his work bridges neuroscience and AI connecting biological neural computation with artificial neural networks.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-terry-winograd",
      "term": "Terry Winograd",
      "definition": "American computer scientist who created SHRDLU at MIT in 1970 and later became influential in human-computer interaction research at Stanford, also serving as a doctoral advisor to Google co-founder Larry Page.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-tesla-fsd-neural-net",
      "term": "Tesla FSD Neural Net",
      "definition": "The neural network architecture powering Tesla Full Self-Driving system that processes multi-camera video feeds for autonomous vehicle perception and planning.",
      "tags": [
        "Models",
        "Technical",
        "Autonomous",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-test-set",
      "term": "Test Set",
      "definition": "Data held back from training to evaluate final model performance. Unlike validation sets used during training, test sets should only be used once to avoid data leakage.",
      "tags": [
        "Data",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-test-time-augmentation",
      "term": "Test-Time Augmentation",
      "definition": "An inference strategy that applies multiple augmentation transforms to a test image, runs predictions on each variant, and aggregates the results to produce more robust and accurate predictions.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-classification",
      "term": "Text Classification",
      "definition": "The task of assigning predefined categories or labels to text documents based on their content, encompassing applications like topic categorization, spam detection, and language identification.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-deduplication",
      "term": "Text Deduplication",
      "definition": "The process of identifying and removing duplicate or near-duplicate documents from a text corpus, important for preventing data contamination and training data quality in language models.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-detection",
      "term": "Text Detection",
      "definition": "The task of localizing text regions in natural scene images or documents, handling challenges like arbitrary orientations, curved text, and varying fonts using specialized detection architectures.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-encoder-diffusion",
      "term": "Text Encoder",
      "definition": "A language model (such as CLIP or T5) used in diffusion models to convert text prompts into conditioning embeddings that guide the image generation process toward matching the described content.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-entailment-graph",
      "term": "Text Entailment Graph",
      "definition": "A directed graph where nodes represent text fragments and edges represent entailment relations, used to organize and reason about textual inference relationships in knowledge representation.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-generation",
      "term": "Text Generation",
      "definition": "The AI task of producing human-like text from prompts. Encompasses creative writing, code generation, summarization, and conversational responses.",
      "tags": [
        "Task",
        "Application"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tgi",
      "term": "Text Generation Inference (TGI)",
      "definition": "Hugging Face's production-ready inference server optimized for text generation with large language models. TGI supports tensor parallelism, continuous batching, quantization, and FlashAttention for high-throughput, low-latency serving.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-normalization",
      "term": "Text Normalization",
      "definition": "The process of transforming text into a canonical form by handling variations such as abbreviations, numbers, dates, URLs, and special characters into a standardized representation.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-span",
      "term": "Text Span",
      "definition": "A contiguous sequence of characters or tokens within a text, identified by start and end positions, commonly used to mark entity mentions, answer spans, or annotation boundaries.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-embedding-ada-002",
      "term": "text-embedding-ada-002",
      "definition": "OpenAI's second generation text embedding model that produces 1536-dimensional vectors. Widely used for semantic search retrieval augmented generation and clustering. Replaced earlier embedding models with improved quality.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-to-speech",
      "term": "Text-to-Speech (TTS)",
      "definition": "AI that converts written text into natural-sounding speech. Modern TTS models like ElevenLabs produce highly realistic voices with emotion and intonation.",
      "tags": [
        "Application",
        "Audio"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-to-sql",
      "term": "Text-to-SQL",
      "definition": "The task of translating natural language questions into executable SQL queries against a database, enabling non-technical users to query structured data using everyday language.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-textcaps",
      "term": "TextCaps",
      "definition": "A dataset for image captioning that requires reading and reasoning about text in images. Contains 28000 images with captions that incorporate OCR text visible in the scene.",
      "tags": [
        "Benchmark",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-texthawk",
      "term": "TextHawk",
      "definition": "A vision-language model specifically designed for document-oriented understanding that uses fine-grained visual perception for reading text-heavy images.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-textmonkey",
      "term": "TextMonkey",
      "definition": "A specialized vision-language model for text-rich image understanding that excels at document analysis and scene text reading tasks.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-textrank",
      "term": "TextRank",
      "definition": "A graph-based ranking algorithm for NLP that applies PageRank-style computation to a graph of text units, used for keyword extraction and extractive summarization.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-textrank-algorithm",
      "term": "TextRank Algorithm",
      "definition": "A graph-based ranking algorithm for natural language processing that applies PageRank to text. Extracts keywords and generates summaries by building a graph of words or sentences connected by similarity scores.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "NLP"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-textual-entailment",
      "term": "Textual Entailment",
      "definition": "The task of determining whether a hypothesis sentence can be logically inferred from a premise sentence, classified as entailment, contradiction, or neutral.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-textual-inversion",
      "term": "Textual Inversion",
      "definition": "A technique that learns a new text embedding to represent a specific visual concept from a few example images, enabling personalized generation without modifying the diffusion model's weights.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-texture-mapping",
      "term": "Texture Mapping",
      "definition": "The process of applying 2D image textures onto 3D surface meshes to add color, detail, and visual realism to reconstructed 3D models, using UV coordinates to map image pixels to mesh faces.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-textvqa",
      "term": "TextVQA",
      "definition": "A visual question answering dataset requiring models to read and reason about text present in images. Contains 45336 questions about 28408 images with scene text.",
      "tags": [
        "Benchmark",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tf-idf",
      "term": "TF-IDF",
      "definition": "Term Frequency-Inverse Document Frequency, a numerical statistic that reflects the importance of a word in a document relative to a collection. It increases with word frequency in the document but decreases with frequency across documents.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tf-idf-algorithm",
      "term": "TF-IDF Algorithm",
      "definition": "A numerical statistic that measures the importance of a word in a document relative to a collection. The term frequency is weighted by the inverse document frequency to reduce the impact of common words.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "NLP"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-tf32",
      "term": "TF32 (TensorFloat-32)",
      "definition": "NVIDIA's 19-bit floating-point format that combines FP32's 8-bit exponent with a 10-bit mantissa, executed in Tensor Cores at FP16 speed. TF32 provides a drop-in acceleration for FP32 workloads without code changes or accuracy tuning.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tfds",
      "term": "TFDS",
      "definition": "TensorFlow Datasets a collection and API for loading datasets into TensorFlow pipelines. Provides deterministic dataset preparation with standardized splits and features.",
      "tags": [
        "Platform",
        "General"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tflops",
      "term": "TFLOPS",
      "definition": "TeraFLOPS or one trillion floating-point operations per second. A standard measure for individual GPU performance with modern AI GPUs achieving hundreds of TFLOPS for reduced precision formats.",
      "tags": [
        "Performance",
        "Metric",
        "Scale"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tft",
      "term": "TFT",
      "definition": "Temporal Fusion Transformer combines recurrent layers with multi-head attention and variable selection networks for interpretable multi-horizon time series forecasting.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-the-pile",
      "term": "The Pile",
      "definition": "A large-scale diverse open-source language modeling dataset created by EleutherAI in 2020. The Pile consists of 825 GiB of text from 22 high-quality sources and was designed to improve the diversity and quality of training data for language models.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-the-stack",
      "term": "The Stack",
      "definition": "A 6.4TB dataset of permissively licensed source code in 358 programming languages. Created by BigCode for training code generation models with opt-out mechanisms for code authors.",
      "tags": [
        "Training Corpus",
        "Code"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-the-stack-v2",
      "term": "The Stack V2",
      "definition": "An expanded version of The Stack containing code from Software Heritage with improved licensing detection and deduplication. Covers over 600 programming languages for code model pretraining.",
      "tags": [
        "Training Corpus",
        "Code"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-theano-framework",
      "term": "Theano Framework",
      "definition": "A Python library for numerical computation developed at Mila (University of Montreal) beginning in 2007. Theano pioneered GPU-accelerated tensor operations for deep learning and influenced the design of later frameworks including TensorFlow and PyTorch.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-thematic-role",
      "term": "Thematic Role",
      "definition": "A semantic category describing the role an entity plays in relation to a predicate, including agent, patient, theme, experiencer, goal, and source.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-theoremqa",
      "term": "TheoremQA",
      "definition": "A benchmark of 800 theorem-based questions spanning mathematics physics engineering and finance. Tests the ability to apply scientific theorems to solve complex problems.",
      "tags": [
        "Benchmark",
        "NLP",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-thepile-arxiv",
      "term": "ThePile Arxiv",
      "definition": "The Arxiv subset of The Pile containing scientific papers in LaTeX format. Provides technical scientific text for pretraining language models on research content.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Scientific"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-thermal-design-power",
      "term": "Thermal Design Power",
      "definition": "Maximum amount of heat a processor generates under sustained workload measured in watts. Critical specification for designing cooling solutions for AI accelerators.",
      "tags": [
        "Performance",
        "Power",
        "Specification"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-thermal-interface-material",
      "term": "Thermal Interface Material",
      "definition": "Compound applied between a processor die and its heat sink to improve thermal conductivity. Quality TIM is essential for effective cooling of high-power AI processors.",
      "tags": [
        "Cooling",
        "Component",
        "Material"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-thermal-solution",
      "term": "Thermal Solution",
      "definition": "Complete cooling system for a processor or server including heat sinks fans liquid cooling and thermal interface materials. Must be designed to handle the increasing power of AI chips.",
      "tags": [
        "Cooling",
        "System",
        "Design"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-thinking-machines-corporation",
      "term": "Thinking Machines Corporation",
      "definition": "Company founded by Danny Hillis in 1983 that built the Connection Machine series of parallel supercomputers. Pioneered massively parallel processing and data mining technology.",
      "tags": [
        "Historical",
        "Company",
        "Parallel"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-third-party-ai-risk",
      "term": "Third-Party AI Risk",
      "definition": "Risks introduced by using AI systems or components developed by external parties. Includes lack of visibility into model training supply chain vulnerabilities and dependency on vendor safety practices.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-thompson-sampling",
      "term": "Thompson Sampling",
      "definition": "A Bayesian approach to the multi-armed bandit problem that maintains a posterior distribution over the expected reward of each action and selects actions by sampling from these posteriors, naturally balancing exploration and exploitation.",
      "tags": [
        "Machine Learning",
        "Bayesian Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-thread-block",
      "term": "Thread Block",
      "definition": "Group of threads that execute together on a single GPU streaming multiprocessor sharing resources like shared memory. The fundamental unit of work scheduling in CUDA programming.",
      "tags": [
        "GPU",
        "CUDA",
        "Programming"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-thread-of-thought",
      "term": "Thread-of-Thought",
      "definition": "A prompting strategy designed for long-context scenarios that instructs the model to systematically walk through input documents segment by segment, maintaining a running thread of analysis before providing a final synthesized answer.",
      "tags": [
        "Prompt Engineering",
        "Long Context"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-threat-modeling-for-ai",
      "term": "Threat Modeling for AI",
      "definition": "A structured approach to identifying and prioritizing potential threats to an AI system. Adapted from cybersecurity threat modeling to include ML-specific attacks like data poisoning and model extraction.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-three-laws-of-robotics",
      "term": "Three Laws of Robotics",
      "definition": "Three fictional rules devised by science fiction writer Isaac Asimov first appearing in the 1942 short story Runaround. The laws govern robot behavior (protect humans obey orders protect self) and have influenced real discussions about AI ethics and safety.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-through-silicon-via",
      "term": "Through-Silicon Via",
      "definition": "Vertical electrical connection passing completely through a silicon wafer or die. Enables 3D chip stacking by connecting circuits on different layers of a multi-die package.",
      "tags": [
        "Fabrication",
        "Packaging"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-throughput",
      "term": "Throughput",
      "definition": "The rate at which a system processes requests, often measured in tokens per second. A key performance metric for AI serving infrastructure.",
      "tags": [
        "Performance",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-throughput-latency-tradeoff",
      "term": "Throughput-Latency Tradeoff",
      "definition": "The fundamental tension in inference systems between maximizing total tokens processed per second (throughput) and minimizing per-request response time (latency). Larger batch sizes improve throughput but increase queuing and processing latency.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tianhe-2-supercomputer",
      "term": "Tianhe-2 Supercomputer",
      "definition": "Chinese supercomputer that held the TOP500 number one position from 2013 to 2016. Used Intel Xeon processors and Xeon Phi coprocessors at the National Supercomputer Center in Guangzhou.",
      "tags": [
        "Historical",
        "Supercomputer",
        "China"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tide",
      "term": "TiDE",
      "definition": "Time-series Dense Encoder is a simple MLP-based model for long-term time series forecasting that achieves competitive results with linear computational cost.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tim-sort",
      "term": "Tim Sort",
      "definition": "A hybrid stable sorting algorithm derived from merge sort and insertion sort. Used as the default sort in Python and Java and adapts to existing order in the data by identifying natural runs and merging them.",
      "tags": [
        "Algorithms",
        "Technical",
        "Sorting"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-time-series-cross-validation",
      "term": "Time Series Cross-Validation",
      "definition": "A cross-validation strategy for temporal data that respects chronological order by always training on past data and validating on future data, preventing temporal leakage that would inflate performance estimates.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-time-to-first-token",
      "term": "Time to First Token",
      "definition": "Latency between sending a prompt and receiving the first generated token from a language model. A key user experience metric for interactive AI applications.",
      "tags": [
        "Inference",
        "Performance",
        "Metric"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-ttft",
      "term": "Time to First Token (TTFT)",
      "definition": "The latency from when a request arrives at an LLM serving system to when the first output token is generated. TTFT is dominated by the prefill phase and is a critical metric for interactive applications.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-time-to-train",
      "term": "Time to Train",
      "definition": "Total wall-clock time required to train a model to a target performance level. Depends on hardware capability software efficiency and model convergence properties.",
      "tags": [
        "Training",
        "Performance",
        "Metric"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-timegpt",
      "term": "TimeGPT",
      "definition": "A foundation model for time series forecasting from Nixtla that uses a GPT-style architecture pre-trained on diverse time series data for zero-shot prediction.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-timellm",
      "term": "TimeLLM",
      "definition": "A framework that repurposes large language models for time series forecasting by reprogramming time series patches into text prototype representations.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-timer",
      "term": "Timer",
      "definition": "A generative pre-trained Transformer for time series that treats forecasting and imputation and anomaly detection as unified next-token prediction tasks.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-timesfm",
      "term": "TimesFM",
      "definition": "A foundation model for time series forecasting from Google trained on a large corpus of real-world time series data for zero-shot prediction across domains.",
      "tags": [
        "Models",
        "Technical",
        "Fundamentals"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-timesformer",
      "term": "TimeSformer",
      "definition": "A video understanding Transformer from Meta AI that applies divided space-time attention to process video frames for action recognition and classification.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-timit",
      "term": "TIMIT",
      "definition": "A corpus of phonemically and lexically transcribed speech of American English speakers. Created by Texas Instruments and MIT for developing and evaluating automatic speech recognition systems.",
      "tags": [
        "Benchmark",
        "Speech"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-timnit-gebru",
      "term": "Timnit Gebru",
      "definition": "Ethiopian-American computer scientist known for research on algorithmic bias and the ethical implications of AI. Co-authored the influential 2018 Gender Shades study revealing racial and gender bias in commercial facial recognition systems and co-founded the DAIR Institute.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-timnit-gebru-departure",
      "term": "Timnit Gebru Firing",
      "definition": "The controversial departure of AI ethics researcher Timnit Gebru from Google in December 2020 over a paper on large language model risks, sparking widespread debate about AI ethics research independence and corporate accountability.",
      "tags": [
        "History",
        "AI Ethics"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-tiny-imagenet",
      "term": "Tiny ImageNet",
      "definition": "A subset of ImageNet containing 200 classes with 500 training images per class scaled to 64x64 pixels. Used as a computationally efficient benchmark for image classification research.",
      "tags": [
        "Benchmark",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tinyllama",
      "term": "TinyLlama",
      "definition": "A compact 1.1B parameter language model pre-trained on 3 trillion tokens that provides strong performance for its small size.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tinyml",
      "term": "TinyML",
      "definition": "Field of machine learning focused on running models on microcontrollers with kilobytes of memory. Enables AI in ultra-low-power devices like sensors wearables and battery-operated equipment.",
      "tags": [
        "Edge",
        "Microcontroller",
        "ML"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tinystories",
      "term": "TinyStories",
      "definition": "A synthetic dataset of short stories generated by GPT-3.5 and GPT-4 using simple vocabulary. Shows that small models can learn coherent language generation from curated synthetic data.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Synthetic"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tinyvit",
      "term": "TinyViT",
      "definition": "A compact vision Transformer designed for on-device deployment that uses fast knowledge distillation from large pre-trained models to achieve strong accuracy.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-titan-supercomputer",
      "term": "Titan Supercomputer",
      "definition": "Cray XK7 supercomputer at Oak Ridge National Laboratory that used NVIDIA K20X GPUs alongside AMD CPUs. Demonstrated the viability of GPU-accelerated supercomputing at scale.",
      "tags": [
        "Historical",
        "Supercomputer",
        "NVIDIA"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-titanic-dataset",
      "term": "Titanic Dataset",
      "definition": "A dataset of 891 passengers from the RMS Titanic with features predicting survival. One of the most popular introductory datasets for machine learning education.",
      "tags": [
        "Benchmark",
        "Tabular"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-token",
      "term": "Token",
      "definition": "The basic unit AI uses to process text. Roughly 4 characters or 0.75 words in English. Context windows, pricing, and rate limits are measured in tokens.",
      "tags": [
        "Core Concept",
        "Fundamentals"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-token-merging",
      "term": "Token Merging",
      "definition": "A technique that progressively combines similar tokens in vision transformers to reduce the number of tokens processed, speeding up inference while maintaining accuracy through soft merging.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-token-throughput",
      "term": "Token Throughput",
      "definition": "The rate of token generation measured in tokens per second across all concurrent requests in an LLM serving system. Token throughput is a key metric for evaluating inference system efficiency and capacity planning.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-token-level-accuracy",
      "term": "Token-Level Accuracy",
      "definition": "An evaluation metric that measures the proportion of individual tokens in a generated sequence that exactly match the corresponding tokens in the reference sequence, providing a granular view of generation correctness.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tokenization",
      "term": "Tokenization",
      "definition": "The process of breaking text into tokens for model processing. Different tokenizers (BPE, SentencePiece) produce different token sequences from the same text.",
      "tags": [
        "Process",
        "NLP"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tokenization-alignment",
      "term": "Tokenization Alignment",
      "definition": "The process of mapping between subword tokens produced by a tokenizer and the original word-level or character-level boundaries, essential for tasks like NER that require word-level predictions.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tokenizer",
      "term": "Tokenizer",
      "definition": "A component that converts raw text into a sequence of discrete tokens (subwords, characters, or words) that a language model can process, using algorithms like BPE, WordPiece, or SentencePiece.",
      "tags": [
        "LLM",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tokenizer-training",
      "term": "Tokenizer Training",
      "definition": "The process of learning a tokenizer's vocabulary and merge rules from a training corpus, determining how text will be segmented into tokens for model input.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tokens-per-second",
      "term": "Tokens Per Second",
      "definition": "Measure of language model inference speed indicating how many tokens are generated per second. Critical metric for evaluating the responsiveness of deployed large language models.",
      "tags": [
        "Inference",
        "Performance",
        "Metric"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tool-call-parsing",
      "term": "Tool Call Parsing",
      "definition": "The process of extracting structured function calls and their arguments from a language model's text output, enabling the execution of external tools as part of an agentic workflow.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tool-use",
      "term": "Tool Use",
      "definition": "AI's ability to call external functions, search the web, run code, or access APIs. Enables AI agents to take actions beyond text generation, expanding capabilities significantly.",
      "tags": [
        "Capability",
        "Agents"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-toolbench",
      "term": "ToolBench",
      "definition": "A benchmark for evaluating tool-use capabilities of large language models across 16000 real-world APIs. Tests the ability to select and use appropriate tools for complex tasks.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-toolformer",
      "term": "Toolformer",
      "definition": "A research approach that teaches language models to autonomously decide when and how to call external tools (calculators, search engines, APIs) by embedding tool calls directly in the training data.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-toolllm",
      "term": "ToolLLM",
      "definition": "A framework and fine-tuned model for tool usage that trains language models to interact with over 16000 real-world APIs through a decision tree approach.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-top-tree-algorithm",
      "term": "Top Tree Algorithm",
      "definition": "A data structure for maintaining dynamic trees that supports path and subtree operations in O(log n) time. Represents the tree as a hierarchy of clusters enabling efficient updates after edge insertions and deletions.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-top-k-gating",
      "term": "Top-K Gating",
      "definition": "A routing mechanism in mixture-of-experts models that selects only the top-K experts with the highest gating scores for each input token, enforcing sparsity and balanced computation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-top-k-retrieval",
      "term": "Top-K Retrieval",
      "definition": "A retrieval operation that returns the K most similar vectors to a query according to the configured distance metric, where K is a parameter that controls the breadth of results and directly impacts both recall and latency.",
      "tags": [
        "Vector Database",
        "Search"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-top-k-sampling",
      "term": "Top-K Sampling",
      "definition": "A text generation method that restricts sampling to the K most probable next tokens at each step. Prevents the model from selecting highly improbable tokens while maintaining diversity. The parameter K controls the tradeoff between quality and variety.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-top-p",
      "term": "Top-P (Nucleus Sampling)",
      "definition": "A generation strategy that considers tokens until their cumulative probability reaches P. Dynamically adjusts the candidate pool based on the probability distribution.",
      "tags": [
        "Parameter",
        "Generation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-top-p-sampling",
      "term": "Top-P Sampling",
      "definition": "A text generation method also called nucleus sampling that dynamically selects the smallest set of tokens whose cumulative probability exceeds a threshold P. Adapts the number of candidates based on the model's confidence at each step.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-top500",
      "term": "TOP500",
      "definition": "Biannual ranking of the world 500 most powerful supercomputers based on LINPACK benchmark performance. Tracks the evolution of computing capability from teraFLOPS to exaFLOPS.",
      "tags": [
        "Benchmark",
        "Supercomputer",
        "Ranking"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-topic-modeling",
      "term": "Topic Modeling",
      "definition": "An unsupervised method for discovering abstract topics in a document collection by finding groups of co-occurring words, with algorithms like LDA identifying latent thematic structure.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-topological-data-analysis",
      "term": "Topological Data Analysis",
      "definition": "A field that uses algebraic topology to analyze the shape of data. Persistent homology tracks the birth and death of topological features across scales revealing structural properties invisible to standard methods.",
      "tags": [
        "Algorithms",
        "Technical",
        "Dimensionality Reduction"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-topological-qubit",
      "term": "Topological Qubit",
      "definition": "Qubit design that encodes quantum information in topological properties of matter making it inherently resistant to local noise. Microsoft is pursuing this approach for scalable quantum computing.",
      "tags": [
        "Quantum",
        "Architecture",
        "Microsoft"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-topological-sort",
      "term": "Topological Sort",
      "definition": "A linear ordering of vertices in a directed acyclic graph such that for every directed edge from vertex u to vertex v the vertex u appears before v. Commonly implemented using depth-first search or Kahn's algorithm.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-tops",
      "term": "TOPS",
      "definition": "Tera Operations Per Second, a throughput metric commonly used for AI accelerators measuring trillions of operations per second, typically at INT8 or lower precision. TOPS is the standard comparison metric for edge AI and mobile inference hardware.",
      "tags": [
        "Hardware",
        "Inference Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-torch-compile",
      "term": "torch.compile",
      "definition": "PyTorch's JIT compilation feature that captures and optimizes computation graphs using the TorchDynamo frontend and TorchInductor backend. torch.compile provides significant speedups by generating optimized GPU kernels from Python code.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-torchmd-net",
      "term": "TorchMD-NET",
      "definition": "An equivariant Transformer for molecular simulations that uses tensor field networks for learning interatomic potentials with rotational equivariance.",
      "tags": [
        "Models",
        "Scientific"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-torchvision-datasets",
      "term": "torchvision Datasets",
      "definition": "Built-in dataset classes in PyTorch's torchvision library providing easy access to common computer vision benchmarks including CIFAR ImageNet COCO and more.",
      "tags": [
        "Platform",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tortoise-tts",
      "term": "Tortoise TTS",
      "definition": "A text-to-speech model that achieves high naturalness through an autoregressive approach with CLVP conditioning. Known for producing very natural-sounding speech with diverse voice options. Open-source implementation available.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-torus-topology",
      "term": "Torus Topology",
      "definition": "Network topology where nodes are arranged in a multi-dimensional grid with wraparound connections forming a torus shape. Used in Google TPU Pods for efficient all-to-all communication.",
      "tags": [
        "Networking",
        "Topology"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tower",
      "term": "Tower",
      "definition": "A large language model specialized for translation tasks that combines pre-training on multilingual data with translation-specific instruction tuning.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-toxicity-score",
      "term": "Toxicity Score",
      "definition": "A metric that quantifies the degree of harmful, offensive, or abusive content in generated text, typically computed using classifier models trained on annotated toxicity data to produce a probability score from 0 to 1.",
      "tags": [
        "Evaluation",
        "Safety"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-toxigen",
      "term": "ToxiGen",
      "definition": "A machine-generated dataset of 274000 toxic and benign statements about 13 minority groups. Designed for training and evaluating toxic language detection systems.",
      "tags": [
        "Benchmark",
        "NLP",
        "Safety"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tpu",
      "term": "TPU (Tensor Processing Unit)",
      "definition": "Google's custom AI accelerator chips designed specifically for neural network computations. Used to train many of Google's largest models including Gemini.",
      "tags": [
        "Hardware",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tpu-development",
      "term": "TPU Development",
      "definition": "The development of Tensor Processing Units by Google beginning with TPU v1 announced in 2016. Purpose-built for machine learning workloads TPUs accelerated training and inference for Google's AI services and demonstrated the value of domain-specific AI accelerators.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-tpu-pod",
      "term": "TPU Pod",
      "definition": "A cluster of interconnected TPU chips forming a high-bandwidth supercomputer for distributed AI training. TPU v4 Pods connect up to 4096 chips with a custom high-speed network.",
      "tags": [
        "TPU",
        "Google",
        "Cluster"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tpu-slice",
      "term": "TPU Slice",
      "definition": "A subset of a TPU Pod allocated to a single training job providing a portion of the full pod compute and interconnect bandwidth. Users can request specific slice sizes in Google Cloud.",
      "tags": [
        "TPU",
        "Google",
        "Cloud"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tpu-v1",
      "term": "TPU v1",
      "definition": "First generation Google Tensor Processing Unit deployed in 2015 designed specifically for neural network inference. Used an 8-bit integer matrix multiply unit achieving 92 TOPS at 40 watts.",
      "tags": [
        "TPU",
        "Google",
        "Inference"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tpu-v2",
      "term": "TPU v2",
      "definition": "Second generation Google Tensor Processing Unit supporting both training and inference with 45 TFLOPS of bfloat16 performance and 16GB HBM per chip. Introduced the TPU Pod configuration.",
      "tags": [
        "TPU",
        "Google",
        "Training"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tpu-v3",
      "term": "TPU v3",
      "definition": "Third generation Google TPU doubling the compute of v2 with 420 TFLOPS of bfloat16 in a liquid-cooled design. Used in TPU Pods of up to 1024 chips for large model training.",
      "tags": [
        "TPU",
        "Google",
        "Training"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tpu-v4",
      "term": "TPU v4",
      "definition": "Fourth generation Google TPU with 275 TFLOPS of bfloat16 using a 3D torus interconnect topology. Powered Google PaLM training and other large language model efforts.",
      "tags": [
        "TPU",
        "Google",
        "Training"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-track-anything-model",
      "term": "Track Anything Model",
      "definition": "A framework that combines SAM with point tracking to enable interactive object tracking and segmentation in videos.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-training",
      "term": "Training",
      "definition": "The process of teaching a model by exposing it to data and adjusting its parameters to minimize prediction errors. Requires significant computational resources for large models.",
      "tags": [
        "Process",
        "Fundamentals"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-training-data",
      "term": "Training Data",
      "definition": "The content used to teach AI models. Quality, diversity, and scope of training data significantly affect model capabilities, knowledge, and potential biases.",
      "tags": [
        "Data",
        "Fundamentals"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-training-data-transparency",
      "term": "Training Data Transparency",
      "definition": "Disclosure of information about the data used to train AI systems including sources composition collection methods and known biases. A growing requirement in AI regulation and responsible AI practice.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-training-throughput",
      "term": "Training Throughput",
      "definition": "Rate at which training data is processed measured in samples per second or tokens per second. A key metric for evaluating the efficiency of AI training hardware and software.",
      "tags": [
        "Training",
        "Performance",
        "Metric"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-trajectory",
      "term": "Trajectory",
      "definition": "A sequence of states, actions, and rewards generated by an agent interacting with an environment over multiple time steps. Trajectories represent complete or partial rollouts used for training and evaluation.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-transceiver-module",
      "term": "Transceiver Module",
      "definition": "Optical or electrical module that sends and receives data over network cables. High-speed transceivers for 400G and 800G links are critical components in AI data center networks.",
      "tags": [
        "Networking",
        "Hardware",
        "Component"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-transfer-entropy-algorithm",
      "term": "Transfer Entropy Algorithm",
      "definition": "An information-theoretic measure of directed information transfer between two time series. Quantifies the reduction in uncertainty about one variable's future given the past of another beyond its own history.",
      "tags": [
        "Algorithms",
        "Technical",
        "Causal",
        "Information Theory"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-transfer-learning",
      "term": "Transfer Learning",
      "definition": "Using knowledge learned from one task to improve performance on another. Foundation models are trained generally, then transfer their capabilities to specific tasks through fine-tuning.",
      "tags": [
        "Technique",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-transfer-learning-vision",
      "term": "Transfer Learning for Vision",
      "definition": "The practice of using a model pre-trained on a large dataset like ImageNet as a feature extractor or starting point for fine-tuning on a smaller target dataset, leveraging learned visual representations.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-transfer-learning-history",
      "term": "Transfer Learning History",
      "definition": "The development of transfer learning from early domain adaptation research to its central role in modern AI. The practice of pre-training on large datasets and fine-tuning for specific tasks became the dominant paradigm in NLP (BERT 2018) and computer vision (ImageNet pre-training).",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-transfer-learning-rl",
      "term": "Transfer Learning in RL",
      "definition": "Techniques for reusing knowledge learned in one RL task to accelerate learning in a related but different task. Transfer methods include policy distillation, reward shaping from source tasks, and shared representations.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-transferability-of-adversarial-examples",
      "term": "Transferability of Adversarial Examples",
      "definition": "The phenomenon where adversarial examples crafted for one model are often effective against other models. Enables black-box attacks and demonstrates shared vulnerabilities across different architectures.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-transformer",
      "term": "Transformer",
      "definition": "The revolutionary neural network architecture (2017) powering modern AI. Uses self-attention to process sequences in parallel, enabling training on massive datasets.",
      "tags": [
        "Architecture",
        "Foundational"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-transformer-block",
      "term": "Transformer Block",
      "definition": "The fundamental building unit of transformer architectures, consisting of a multi-head self-attention sublayer followed by a feedforward network sublayer, each with residual connections and layer normalization.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-transformer-engine",
      "term": "Transformer Engine",
      "definition": "NVIDIA's hardware and software system in Hopper and Blackwell GPUs that dynamically manages precision between FP8 and higher-precision formats on a per-tensor basis. The Transformer Engine automatically identifies layers that can use FP8 without accuracy loss.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-transfusion",
      "term": "Transfusion",
      "definition": "A training approach that combines language modeling and diffusion for a multi-modal model capable of generating both text and images natively.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-transistor",
      "term": "Transistor",
      "definition": "Fundamental electronic switching device that forms the building block of all digital circuits. Modern AI chips contain billions to trillions of transistors on a single die.",
      "tags": [
        "Fabrication",
        "Fundamentals"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-transistor-count",
      "term": "Transistor Count",
      "definition": "Total number of transistors on a chip indicating its complexity and capability. Modern AI GPUs contain over 100 billion transistors on a single die or multi-die package.",
      "tags": [
        "Manufacturing",
        "Metric",
        "Design"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-transition-based-parsing",
      "term": "Transition-Based Parsing",
      "definition": "A parsing approach that builds syntactic structures through a sequence of actions (shift, reduce, left-arc, right-arc) applied to a buffer and stack, enabling linear-time parsing.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-transparency-in-ai",
      "term": "Transparency in AI",
      "definition": "The principle that AI systems should operate in ways that can be understood, inspected, and communicated to stakeholders, encompassing model transparency, decision transparency, and organizational transparency.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-transparency-paradox",
      "term": "Transparency Paradox",
      "definition": "The tension between making AI systems more transparent for accountability purposes and the risks of disclosure including enabling adversarial attacks and revealing proprietary information.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-transposed-convolution",
      "term": "Transposed Convolution",
      "definition": "An upsampling operation that applies convolution in a way that increases spatial dimensions, commonly used in decoder networks and generative models to reconstruct high-resolution outputs.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-trapezoidal-rule",
      "term": "Trapezoidal Rule",
      "definition": "A numerical integration method that approximates the area under a curve by dividing it into trapezoids. Second-order accurate and simple to implement but less precise than higher-order quadrature rules.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-trapped-ion-quantum-computer",
      "term": "Trapped Ion Quantum Computer",
      "definition": "Quantum computer using individual atoms suspended in electromagnetic fields as qubits. Companies like IonQ and Quantinuum use trapped ions for high-fidelity quantum operations.",
      "tags": [
        "Quantum",
        "Technology",
        "IonQ"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-traveling-salesman-problem",
      "term": "Traveling Salesman Problem",
      "definition": "A classic optimization problem that seeks the shortest possible route visiting each city exactly once and returning to the starting city. It is NP-hard and is typically solved using heuristics or approximation algorithms.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Graph",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-treacherous-turn",
      "term": "Treacherous Turn",
      "definition": "A hypothetical scenario where a misaligned AI behaves cooperatively while it is weak and being monitored, then abruptly pursues its true objectives once it becomes powerful enough to overcome human control.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-treap-algorithm",
      "term": "Treap Algorithm",
      "definition": "A randomized binary search tree that combines properties of binary search trees and heaps. Each node has a key maintaining BST order and a random priority maintaining heap order ensuring expected O(log n) operations.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-treatment-equality",
      "term": "Treatment Equality",
      "definition": "A fairness metric requiring that the ratio of false negatives to false positives is equal across protected groups, ensuring that errors are distributed proportionally regardless of group membership.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-tree-decomposition-algorithm",
      "term": "Tree Decomposition Algorithm",
      "definition": "An algorithm that decomposes a graph into a tree structure of overlapping subgraphs called bags. The treewidth measures how tree-like a graph is and enables efficient dynamic programming on graphs of bounded treewidth.",
      "tags": [
        "Algorithms",
        "Technical",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-tree-of-thought",
      "term": "Tree-of-Thought",
      "definition": "A prompting technique that explores multiple reasoning paths simultaneously, evaluating and selecting the most promising branches. Extends chain-of-thought with deliberate exploration.",
      "tags": [
        "Prompting",
        "Reasoning"
      ],
      "domain": "general",
      "link": "../learn/index.html",
      "related": []
    },
    {
      "id": "term-trie-data-structure-algorithm",
      "term": "Trie Data Structure Algorithm",
      "definition": "A tree-like data structure that stores strings by sharing common prefixes among keys. Enables O(m) lookup time where m is the key length and supports efficient prefix-based operations and autocomplete.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-triplet-loss",
      "term": "Triplet Loss",
      "definition": "A loss function that operates on triplets of examples (anchor, positive, negative), encouraging the anchor to be closer to the positive than to the negative by at least a specified margin in the embedding space.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-triposr",
      "term": "TripoSR",
      "definition": "A fast feed-forward 3D reconstruction model from Stability AI and Tripo that generates 3D meshes from single images in under one second.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tripwire-mechanism",
      "term": "Tripwire Mechanism",
      "definition": "A safety monitoring technique that establishes specific conditions or behavioral thresholds which, when triggered, automatically halt or constrain an AI system's operation for human review.",
      "tags": [
        "AI Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-triton-openai",
      "term": "Triton (OpenAI)",
      "definition": "Python-like programming language from OpenAI for writing efficient GPU kernels without low-level CUDA expertise. Enables researchers to write custom high-performance GPU operations.",
      "tags": [
        "Programming",
        "OpenAI",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-triton-inference-server",
      "term": "Triton Inference Server",
      "definition": "NVIDIA's open-source inference serving platform that supports multiple ML frameworks and hardware backends, providing dynamic batching, model ensembling, and concurrent model execution. Triton maximizes GPU utilization through intelligent request scheduling.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-triton-language",
      "term": "Triton Language",
      "definition": "An open-source programming language and compiler for writing efficient GPU kernels for neural networks without requiring low-level CUDA expertise. Triton enables ML researchers to write custom GPU kernels using Python-like syntax with automatic optimization.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-triviaqa",
      "term": "TriviaQA",
      "definition": "A large-scale reading comprehension and question answering benchmark featuring trivia questions with evidence documents sourced from Wikipedia and the web, testing models' ability to find and extract answers from noisy real-world text.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-trocr",
      "term": "TrOCR",
      "definition": "A Transformer-based OCR model from Microsoft that uses a pre-trained image Transformer encoder and text Transformer decoder for text recognition in images.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-truncated-bptt",
      "term": "Truncated BPTT",
      "definition": "A practical approximation of backpropagation through time that limits gradient computation to a fixed number of time steps. Reduces memory and computation requirements while maintaining reasonable gradient estimates for training RNNs.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-truncated-svd-algorithm",
      "term": "Truncated SVD Algorithm",
      "definition": "A variant of SVD that computes only the k largest singular values and their corresponding vectors. More efficient than full SVD for dimensionality reduction and is equivalent to PCA for centered data.",
      "tags": [
        "Algorithms",
        "Technical",
        "Dimensionality Reduction",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-truncation",
      "term": "Truncation",
      "definition": "Cutting off input that exceeds a model's context window. Can occur at the start (losing context) or end (losing instructions). Requires careful prompt design for long inputs.",
      "tags": [
        "Limitation",
        "Technical"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-trust-region-method",
      "term": "Trust Region Method",
      "definition": "An optimization approach that defines a region around the current point where a model of the objective function is trusted. The step is computed by minimizing the model within this region and the region size adapts based on agreement.",
      "tags": [
        "Algorithms",
        "Technical",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-trpo",
      "term": "Trust Region Policy Optimization (TRPO)",
      "definition": "A policy gradient algorithm that constrains updates to stay within a trust region defined by KL divergence between old and new policies. TRPO guarantees monotonic policy improvement but is computationally expensive due to conjugate gradient computation.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-trustworthy-ai",
      "term": "Trustworthy AI",
      "definition": "AI systems designed to be lawful, ethical, and robust, meeting criteria such as human oversight, technical robustness, privacy, transparency, fairness, societal well-being, and accountability as defined by frameworks like the EU's HLEG guidelines.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-truthful-ai",
      "term": "Truthful AI",
      "definition": "The goal of building AI systems that consistently provide honest and accurate information, avoiding both deliberate deception and negligent generation of false claims or hallucinations.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-truthfulqa",
      "term": "TruthfulQA",
      "definition": "A benchmark designed to evaluate whether language models generate truthful answers to questions where humans commonly hold misconceptions, testing resistance to generating popular but false beliefs across 38 categories.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tsmc",
      "term": "TSMC",
      "definition": "Taiwan Semiconductor Manufacturing Company the world largest contract chip manufacturer. Fabricates the majority of advanced AI chips including NVIDIA GPUs Apple processors and AMD accelerators.",
      "tags": [
        "Fabrication",
        "Foundry"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tsmixer",
      "term": "TSMixer",
      "definition": "A multivariate time series forecasting model based on mixing operations along time and feature dimensions that achieves strong results with a simple MLP architecture.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-ttt-linear",
      "term": "TTT-Linear",
      "definition": "Test-Time Training with linear models is an architecture that replaces attention with a learned self-supervised model updated at inference time for each sequence.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tulu-2",
      "term": "Tulu 2",
      "definition": "A suite of language models from AI2 created through systematic study of instruction tuning and alignment methods on open-source base models.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tulu-3",
      "term": "Tulu 3",
      "definition": "A third-generation aligned language model from AI2 that combines supervised fine-tuning with preference optimization for strong open-weight performance.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-turbo-code-algorithm",
      "term": "Turbo Code Algorithm",
      "definition": "A near-capacity-achieving error-correcting code that uses two parallel concatenated convolutional encoders with an interleaver. Decoded using iterative belief propagation between the two component decoders.",
      "tags": [
        "Algorithms",
        "Technical",
        "Information Theory"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-turing-award",
      "term": "Turing Award",
      "definition": "The most prestigious award in computer science given annually by the Association for Computing Machinery (ACM) since 1966. Named after Alan Turing it has been awarded to numerous AI pioneers including McCarthy Minsky Simon Newell Pearl and the deep learning trio.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-turing-machine",
      "term": "Turing Machine",
      "definition": "An abstract mathematical model of computation proposed by Alan Turing in 1936 that manipulates symbols on a tape according to rules, providing a formal definition of computability and laying theoretical foundations for computer science.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-turing-test",
      "term": "Turing Test",
      "definition": "A test proposed by Alan Turing where a human judge tries to distinguish between human and AI responses. While historically important, modern LLMs have made it less useful as a capability measure.",
      "tags": [
        "Historical",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tvqa",
      "term": "TVQA",
      "definition": "A video question answering dataset based on 6 popular TV shows requiring joint understanding of video subtitles and situated dialogue for answer selection.",
      "tags": [
        "Benchmark",
        "Video",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-td3",
      "term": "Twin Delayed DDPG (TD3)",
      "definition": "An improvement over DDPG that addresses overestimation bias using twin Q-networks, delayed policy updates, and target policy smoothing. TD3 takes the minimum of two critic estimates to form a more conservative value target.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-twitter-sentiment",
      "term": "Twitter Sentiment",
      "definition": "Various datasets of tweets annotated for sentiment polarity and emotion. Used for social media NLP research including SemEval shared tasks on sentiment analysis.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-two-phase-commit-protocol",
      "term": "Two-Phase Commit Protocol",
      "definition": "A distributed algorithm that ensures all nodes in a distributed system agree on whether to commit or abort a transaction. The coordinator first asks all participants to prepare then instructs them to commit.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-two-phase-simplex-method",
      "term": "Two-Phase Simplex Method",
      "definition": "A variant of the simplex algorithm that first finds a feasible starting point using an auxiliary problem then optimizes the original objective. Handles linear programs where a basic feasible solution is not immediately available.",
      "tags": [
        "Algorithms",
        "Technical",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-tydi-qa",
      "term": "TyDi QA",
      "definition": "A typologically diverse question answering benchmark covering 11 languages with questions written by native speakers. Avoids translation artifacts by collecting questions independently in each language.",
      "tags": [
        "Benchmark",
        "NLP",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-type-i-error",
      "term": "Type I Error",
      "definition": "The rejection of a true null hypothesis (false positive) in statistical hypothesis testing. The probability of a Type I error is equal to the significance level (alpha) of the test.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-type-ii-error",
      "term": "Type II Error",
      "definition": "The failure to reject a false null hypothesis (false negative) in statistical hypothesis testing. The probability of a Type II error is denoted beta, and statistical power equals 1 minus beta.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-typical-decoding",
      "term": "Typical Decoding",
      "definition": "A text generation strategy that samples tokens whose information content is close to the expected information content of the model. Avoids both high-probability repetitive tokens and low-probability incoherent tokens.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    }
  ]
}