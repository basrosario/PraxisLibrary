{
  "letter": "t",
  "count": 104,
  "terms": [
    {
      "id": "term-t-sne",
      "term": "t-SNE",
      "definition": "A nonlinear dimensionality reduction technique that maps high-dimensional data to two or three dimensions for visualization by modeling pairwise similarities as probability distributions and minimizing KL divergence between the high- and low-dimensional representations.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-t5",
      "term": "T5",
      "definition": "Text-to-Text Transfer Transformer, a model by Google that frames all NLP tasks as text-to-text problems, using an encoder-decoder architecture trained on a multi-task mixture.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-table-extraction",
      "term": "Table Extraction",
      "definition": "The task of detecting tables in document images and extracting their structure (rows, columns, cells) and content into machine-readable format, combining visual detection with text recognition.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tabu-search",
      "term": "Tabu Search",
      "definition": "A metaheuristic optimization method that enhances local search by maintaining a memory of recently visited solutions to avoid cycling. Uses adaptive memory and strategic diversification to explore the search space effectively.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-tabular-chain-of-thought",
      "term": "Tabular Chain-of-Thought",
      "definition": "A prompting variant that formats intermediate reasoning steps as structured tables rather than free-form text, improving clarity and consistency in multi-step reasoning by organizing variables, values, and operations in tabular form.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tanh",
      "term": "Tanh",
      "definition": "Hyperbolic tangent activation function that maps inputs to values between -1 and 1. Defined as f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)). Provides zero-centered outputs which can improve convergence compared to sigmoid.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-target-encoding",
      "term": "Target Encoding",
      "definition": "A feature encoding method that replaces each categorical value with the mean of the target variable for that category, often combined with smoothing or cross-validation to prevent overfitting.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-target-network",
      "term": "Target Network",
      "definition": "A slowly updated copy of the value network used to compute stable TD targets in deep RL algorithms like DQN. Target networks reduce oscillation and divergence caused by the moving target problem in bootstrapped learning.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-td-lambda",
      "term": "TD(lambda)",
      "definition": "A temporal difference algorithm that blends multi-step returns using an exponentially-weighted average controlled by the lambda parameter. TD(lambda) unifies TD(0) and Monte Carlo methods through eligibility traces.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-teacher-forcing",
      "term": "Teacher Forcing",
      "definition": "A training strategy for sequence models that uses ground truth tokens as input at each step rather than the model's own predictions. Accelerates training convergence but can cause exposure bias when the model encounters its own predictions at inference.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-teacher-student-framework",
      "term": "Teacher-Student Framework",
      "definition": "A model compression paradigm where a large pretrained teacher model guides the training of a smaller student model by providing soft targets, feature maps, or attention patterns as supervision.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-technical-prompting",
      "term": "Technical Prompting",
      "definition": "The practice of crafting prompts that incorporate precise technical specifications, domain terminology, and structured requirements to generate accurate technical documentation, code, architectures, or engineering solutions.",
      "tags": [
        "Prompt Engineering",
        "Technical"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-technological-unemployment",
      "term": "Technological Unemployment",
      "definition": "Unemployment caused by technological advances outpacing the economy's ability to create new jobs, a longstanding concern significantly amplified by the rapid advancement of AI and automation capabilities.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-temperature",
      "term": "Temperature",
      "definition": "A parameter controlling randomness in AI outputs. Temperature 0 gives deterministic responses; higher values (0.7-1.0) increase creativity and variety; very high values may produce incoherence.",
      "tags": [
        "Parameter",
        "Generation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-temperature-scaling",
      "term": "Temperature Scaling",
      "definition": "A technique that adjusts the softmax distribution over token probabilities by dividing logits by a temperature parameter. Lower temperatures sharpen the distribution toward greedy decoding while higher temperatures produce more uniform and diverse sampling.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-temporal-action-detection",
      "term": "Temporal Action Detection",
      "definition": "The task of identifying the start time, end time, and category of each action instance in an untrimmed video, requiring both temporal localization and activity classification.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-temporal-coherence",
      "term": "Temporal Coherence",
      "definition": "The consistency of visual elements across consecutive frames in generated or processed video, ensuring smooth motion, stable appearance, and absence of flickering or morphing artifacts.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-temporal-credit-assignment",
      "term": "Temporal Credit Assignment",
      "definition": "The specific aspect of credit assignment concerned with distributing reward information backward through time to earlier actions that contributed to the outcome. Eligibility traces and multi-step returns are techniques that address temporal credit assignment.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-temporal-difference-learning",
      "term": "Temporal Difference Learning",
      "definition": "A family of RL methods that update value estimates based on the difference between successive predictions, combining ideas from Monte Carlo and dynamic programming. TD methods learn directly from experience without requiring a model of the environment.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensor-cores",
      "term": "Tensor Cores",
      "definition": "Specialized matrix multiply-and-accumulate units in NVIDIA GPUs that accelerate mixed-precision matrix operations fundamental to deep learning. Tensor Cores perform entire matrix operations in a single clock cycle, delivering an order-of-magnitude speedup over standard CUDA cores for AI workloads.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensor-parallelism",
      "term": "Tensor Parallelism",
      "definition": "A form of model parallelism that splits individual weight matrices across multiple devices, distributing the computation of each layer while requiring communication to synchronize partial results.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensorflow",
      "term": "TensorFlow",
      "definition": "Google's open-source deep learning framework, widely used for production ML systems. Known for deployment tools and TPU support, though PyTorch has gained research share.",
      "tags": [
        "Framework",
        "Deep Learning"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tensorrt",
      "term": "TensorRT",
      "definition": "NVIDIA's high-performance inference optimization SDK that applies layer fusion, kernel auto-tuning, precision calibration, and dynamic tensor memory management. TensorRT can deliver 2-5x inference speedups over unoptimized frameworks on NVIDIA GPUs.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-terry-winograd",
      "term": "Terry Winograd",
      "definition": "American computer scientist who created SHRDLU at MIT in 1970 and later became influential in human-computer interaction research at Stanford, also serving as a doctoral advisor to Google co-founder Larry Page.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-test-set",
      "term": "Test Set",
      "definition": "Data held back from training to evaluate final model performance. Unlike validation sets used during training, test sets should only be used once to avoid data leakage.",
      "tags": [
        "Data",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-test-time-augmentation",
      "term": "Test-Time Augmentation",
      "definition": "An inference strategy that applies multiple augmentation transforms to a test image, runs predictions on each variant, and aggregates the results to produce more robust and accurate predictions.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-classification",
      "term": "Text Classification",
      "definition": "The task of assigning predefined categories or labels to text documents based on their content, encompassing applications like topic categorization, spam detection, and language identification.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-deduplication",
      "term": "Text Deduplication",
      "definition": "The process of identifying and removing duplicate or near-duplicate documents from a text corpus, important for preventing data contamination and training data quality in language models.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-detection",
      "term": "Text Detection",
      "definition": "The task of localizing text regions in natural scene images or documents, handling challenges like arbitrary orientations, curved text, and varying fonts using specialized detection architectures.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-encoder-diffusion",
      "term": "Text Encoder",
      "definition": "A language model (such as CLIP or T5) used in diffusion models to convert text prompts into conditioning embeddings that guide the image generation process toward matching the described content.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-entailment-graph",
      "term": "Text Entailment Graph",
      "definition": "A directed graph where nodes represent text fragments and edges represent entailment relations, used to organize and reason about textual inference relationships in knowledge representation.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-generation",
      "term": "Text Generation",
      "definition": "The AI task of producing human-like text from prompts. Encompasses creative writing, code generation, summarization, and conversational responses.",
      "tags": [
        "Task",
        "Application"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tgi",
      "term": "Text Generation Inference (TGI)",
      "definition": "Hugging Face's production-ready inference server optimized for text generation with large language models. TGI supports tensor parallelism, continuous batching, quantization, and FlashAttention for high-throughput, low-latency serving.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-normalization",
      "term": "Text Normalization",
      "definition": "The process of transforming text into a canonical form by handling variations such as abbreviations, numbers, dates, URLs, and special characters into a standardized representation.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-span",
      "term": "Text Span",
      "definition": "A contiguous sequence of characters or tokens within a text, identified by start and end positions, commonly used to mark entity mentions, answer spans, or annotation boundaries.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-to-speech",
      "term": "Text-to-Speech (TTS)",
      "definition": "AI that converts written text into natural-sounding speech. Modern TTS models like ElevenLabs produce highly realistic voices with emotion and intonation.",
      "tags": [
        "Application",
        "Audio"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-text-to-sql",
      "term": "Text-to-SQL",
      "definition": "The task of translating natural language questions into executable SQL queries against a database, enabling non-technical users to query structured data using everyday language.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-textrank",
      "term": "TextRank",
      "definition": "A graph-based ranking algorithm for NLP that applies PageRank-style computation to a graph of text units, used for keyword extraction and extractive summarization.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-textual-entailment",
      "term": "Textual Entailment",
      "definition": "The task of determining whether a hypothesis sentence can be logically inferred from a premise sentence, classified as entailment, contradiction, or neutral.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-textual-inversion",
      "term": "Textual Inversion",
      "definition": "A technique that learns a new text embedding to represent a specific visual concept from a few example images, enabling personalized generation without modifying the diffusion model's weights.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-texture-mapping",
      "term": "Texture Mapping",
      "definition": "The process of applying 2D image textures onto 3D surface meshes to add color, detail, and visual realism to reconstructed 3D models, using UV coordinates to map image pixels to mesh faces.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-tf-idf",
      "term": "TF-IDF",
      "definition": "Term Frequency-Inverse Document Frequency, a numerical statistic that reflects the importance of a word in a document relative to a collection. It increases with word frequency in the document but decreases with frequency across documents.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tf32",
      "term": "TF32 (TensorFloat-32)",
      "definition": "NVIDIA's 19-bit floating-point format that combines FP32's 8-bit exponent with a 10-bit mantissa, executed in Tensor Cores at FP16 speed. TF32 provides a drop-in acceleration for FP32 workloads without code changes or accuracy tuning.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-thematic-role",
      "term": "Thematic Role",
      "definition": "A semantic category describing the role an entity plays in relation to a predicate, including agent, patient, theme, experiencer, goal, and source.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-thompson-sampling",
      "term": "Thompson Sampling",
      "definition": "A Bayesian approach to the multi-armed bandit problem that maintains a posterior distribution over the expected reward of each action and selects actions by sampling from these posteriors, naturally balancing exploration and exploitation.",
      "tags": [
        "Machine Learning",
        "Bayesian Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-thread-of-thought",
      "term": "Thread-of-Thought",
      "definition": "A prompting strategy designed for long-context scenarios that instructs the model to systematically walk through input documents segment by segment, maintaining a running thread of analysis before providing a final synthesized answer.",
      "tags": [
        "Prompt Engineering",
        "Long Context"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-throughput",
      "term": "Throughput",
      "definition": "The rate at which a system processes requests, often measured in tokens per second. A key performance metric for AI serving infrastructure.",
      "tags": [
        "Performance",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-throughput-latency-tradeoff",
      "term": "Throughput-Latency Tradeoff",
      "definition": "The fundamental tension in inference systems between maximizing total tokens processed per second (throughput) and minimizing per-request response time (latency). Larger batch sizes improve throughput but increase queuing and processing latency.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-time-series-cross-validation",
      "term": "Time Series Cross-Validation",
      "definition": "A cross-validation strategy for temporal data that respects chronological order by always training on past data and validating on future data, preventing temporal leakage that would inflate performance estimates.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-ttft",
      "term": "Time to First Token (TTFT)",
      "definition": "The latency from when a request arrives at an LLM serving system to when the first output token is generated. TTFT is dominated by the prefill phase and is a critical metric for interactive applications.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-timnit-gebru-departure",
      "term": "Timnit Gebru Firing",
      "definition": "The controversial departure of AI ethics researcher Timnit Gebru from Google in December 2020 over a paper on large language model risks, sparking widespread debate about AI ethics research independence and corporate accountability.",
      "tags": [
        "History",
        "AI Ethics"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-token",
      "term": "Token",
      "definition": "The basic unit AI uses to process text. Roughly 4 characters or 0.75 words in English. Context windows, pricing, and rate limits are measured in tokens.",
      "tags": [
        "Core Concept",
        "Fundamentals"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-token-merging",
      "term": "Token Merging",
      "definition": "A technique that progressively combines similar tokens in vision transformers to reduce the number of tokens processed, speeding up inference while maintaining accuracy through soft merging.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-token-throughput",
      "term": "Token Throughput",
      "definition": "The rate of token generation measured in tokens per second across all concurrent requests in an LLM serving system. Token throughput is a key metric for evaluating inference system efficiency and capacity planning.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-token-level-accuracy",
      "term": "Token-Level Accuracy",
      "definition": "An evaluation metric that measures the proportion of individual tokens in a generated sequence that exactly match the corresponding tokens in the reference sequence, providing a granular view of generation correctness.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tokenization",
      "term": "Tokenization",
      "definition": "The process of breaking text into tokens for model processing. Different tokenizers (BPE, SentencePiece) produce different token sequences from the same text.",
      "tags": [
        "Process",
        "NLP"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tokenization-alignment",
      "term": "Tokenization Alignment",
      "definition": "The process of mapping between subword tokens produced by a tokenizer and the original word-level or character-level boundaries, essential for tasks like NER that require word-level predictions.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tokenizer",
      "term": "Tokenizer",
      "definition": "A component that converts raw text into a sequence of discrete tokens (subwords, characters, or words) that a language model can process, using algorithms like BPE, WordPiece, or SentencePiece.",
      "tags": [
        "LLM",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tokenizer-training",
      "term": "Tokenizer Training",
      "definition": "The process of learning a tokenizer's vocabulary and merge rules from a training corpus, determining how text will be segmented into tokens for model input.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tool-call-parsing",
      "term": "Tool Call Parsing",
      "definition": "The process of extracting structured function calls and their arguments from a language model's text output, enabling the execution of external tools as part of an agentic workflow.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-tool-use",
      "term": "Tool Use",
      "definition": "AI's ability to call external functions, search the web, run code, or access APIs. Enables AI agents to take actions beyond text generation, expanding capabilities significantly.",
      "tags": [
        "Capability",
        "Agents"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-toolformer",
      "term": "Toolformer",
      "definition": "A research approach that teaches language models to autonomously decide when and how to call external tools (calculators, search engines, APIs) by embedding tool calls directly in the training data.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-top-k-gating",
      "term": "Top-K Gating",
      "definition": "A routing mechanism in mixture-of-experts models that selects only the top-K experts with the highest gating scores for each input token, enforcing sparsity and balanced computation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-top-k-retrieval",
      "term": "Top-K Retrieval",
      "definition": "A retrieval operation that returns the K most similar vectors to a query according to the configured distance metric, where K is a parameter that controls the breadth of results and directly impacts both recall and latency.",
      "tags": [
        "Vector Database",
        "Search"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-top-k-sampling",
      "term": "Top-K Sampling",
      "definition": "A text generation method that restricts sampling to the K most probable next tokens at each step. Prevents the model from selecting highly improbable tokens while maintaining diversity. The parameter K controls the tradeoff between quality and variety.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-top-p",
      "term": "Top-P (Nucleus Sampling)",
      "definition": "A generation strategy that considers tokens until their cumulative probability reaches P. Dynamically adjusts the candidate pool based on the probability distribution.",
      "tags": [
        "Parameter",
        "Generation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-top-p-sampling",
      "term": "Top-P Sampling",
      "definition": "A text generation method also called nucleus sampling that dynamically selects the smallest set of tokens whose cumulative probability exceeds a threshold P. Adapts the number of candidates based on the model's confidence at each step.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-topic-modeling",
      "term": "Topic Modeling",
      "definition": "An unsupervised method for discovering abstract topics in a document collection by finding groups of co-occurring words, with algorithms like LDA identifying latent thematic structure.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-tops",
      "term": "TOPS",
      "definition": "Tera Operations Per Second, a throughput metric commonly used for AI accelerators measuring trillions of operations per second, typically at INT8 or lower precision. TOPS is the standard comparison metric for edge AI and mobile inference hardware.",
      "tags": [
        "Hardware",
        "Inference Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-torch-compile",
      "term": "torch.compile",
      "definition": "PyTorch's JIT compilation feature that captures and optimizes computation graphs using the TorchDynamo frontend and TorchInductor backend. torch.compile provides significant speedups by generating optimized GPU kernels from Python code.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-toxicity-score",
      "term": "Toxicity Score",
      "definition": "A metric that quantifies the degree of harmful, offensive, or abusive content in generated text, typically computed using classifier models trained on annotated toxicity data to produce a probability score from 0 to 1.",
      "tags": [
        "Evaluation",
        "Safety"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-tpu",
      "term": "TPU (Tensor Processing Unit)",
      "definition": "Google's custom AI accelerator chips designed specifically for neural network computations. Used to train many of Google's largest models including Gemini.",
      "tags": [
        "Hardware",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-training",
      "term": "Training",
      "definition": "The process of teaching a model by exposing it to data and adjusting its parameters to minimize prediction errors. Requires significant computational resources for large models.",
      "tags": [
        "Process",
        "Fundamentals"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-training-data",
      "term": "Training Data",
      "definition": "The content used to teach AI models. Quality, diversity, and scope of training data significantly affect model capabilities, knowledge, and potential biases.",
      "tags": [
        "Data",
        "Fundamentals"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-trajectory",
      "term": "Trajectory",
      "definition": "A sequence of states, actions, and rewards generated by an agent interacting with an environment over multiple time steps. Trajectories represent complete or partial rollouts used for training and evaluation.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-transfer-learning",
      "term": "Transfer Learning",
      "definition": "Using knowledge learned from one task to improve performance on another. Foundation models are trained generally, then transfer their capabilities to specific tasks through fine-tuning.",
      "tags": [
        "Technique",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-transfer-learning-vision",
      "term": "Transfer Learning for Vision",
      "definition": "The practice of using a model pre-trained on a large dataset like ImageNet as a feature extractor or starting point for fine-tuning on a smaller target dataset, leveraging learned visual representations.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-transfer-learning-rl",
      "term": "Transfer Learning in RL",
      "definition": "Techniques for reusing knowledge learned in one RL task to accelerate learning in a related but different task. Transfer methods include policy distillation, reward shaping from source tasks, and shared representations.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-transformer",
      "term": "Transformer",
      "definition": "The revolutionary neural network architecture (2017) powering modern AI. Uses self-attention to process sequences in parallel, enabling training on massive datasets.",
      "tags": [
        "Architecture",
        "Foundational"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-transformer-block",
      "term": "Transformer Block",
      "definition": "The fundamental building unit of transformer architectures, consisting of a multi-head self-attention sublayer followed by a feedforward network sublayer, each with residual connections and layer normalization.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-transformer-engine",
      "term": "Transformer Engine",
      "definition": "NVIDIA's hardware and software system in Hopper and Blackwell GPUs that dynamically manages precision between FP8 and higher-precision formats on a per-tensor basis. The Transformer Engine automatically identifies layers that can use FP8 without accuracy loss.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-transition-based-parsing",
      "term": "Transition-Based Parsing",
      "definition": "A parsing approach that builds syntactic structures through a sequence of actions (shift, reduce, left-arc, right-arc) applied to a buffer and stack, enabling linear-time parsing.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-transparency-in-ai",
      "term": "Transparency in AI",
      "definition": "The principle that AI systems should operate in ways that can be understood, inspected, and communicated to stakeholders, encompassing model transparency, decision transparency, and organizational transparency.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-transposed-convolution",
      "term": "Transposed Convolution",
      "definition": "An upsampling operation that applies convolution in a way that increases spatial dimensions, commonly used in decoder networks and generative models to reconstruct high-resolution outputs.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-treacherous-turn",
      "term": "Treacherous Turn",
      "definition": "A hypothetical scenario where a misaligned AI behaves cooperatively while it is weak and being monitored, then abruptly pursues its true objectives once it becomes powerful enough to overcome human control.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-treatment-equality",
      "term": "Treatment Equality",
      "definition": "A fairness metric requiring that the ratio of false negatives to false positives is equal across protected groups, ensuring that errors are distributed proportionally regardless of group membership.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-tree-of-thought",
      "term": "Tree-of-Thought",
      "definition": "A prompting technique that explores multiple reasoning paths simultaneously, evaluating and selecting the most promising branches. Extends chain-of-thought with deliberate exploration.",
      "tags": [
        "Prompting",
        "Reasoning"
      ],
      "domain": "general",
      "link": "../learn/index.html",
      "related": []
    },
    {
      "id": "term-triplet-loss",
      "term": "Triplet Loss",
      "definition": "A loss function that operates on triplets of examples (anchor, positive, negative), encouraging the anchor to be closer to the positive than to the negative by at least a specified margin in the embedding space.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-tripwire-mechanism",
      "term": "Tripwire Mechanism",
      "definition": "A safety monitoring technique that establishes specific conditions or behavioral thresholds which, when triggered, automatically halt or constrain an AI system's operation for human review.",
      "tags": [
        "AI Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-triton-inference-server",
      "term": "Triton Inference Server",
      "definition": "NVIDIA's open-source inference serving platform that supports multiple ML frameworks and hardware backends, providing dynamic batching, model ensembling, and concurrent model execution. Triton maximizes GPU utilization through intelligent request scheduling.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-triton-language",
      "term": "Triton Language",
      "definition": "An open-source programming language and compiler for writing efficient GPU kernels for neural networks without requiring low-level CUDA expertise. Triton enables ML researchers to write custom GPU kernels using Python-like syntax with automatic optimization.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-triviaqa",
      "term": "TriviaQA",
      "definition": "A large-scale reading comprehension and question answering benchmark featuring trivia questions with evidence documents sourced from Wikipedia and the web, testing models' ability to find and extract answers from noisy real-world text.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-truncated-bptt",
      "term": "Truncated BPTT",
      "definition": "A practical approximation of backpropagation through time that limits gradient computation to a fixed number of time steps. Reduces memory and computation requirements while maintaining reasonable gradient estimates for training RNNs.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-truncation",
      "term": "Truncation",
      "definition": "Cutting off input that exceeds a model's context window. Can occur at the start (losing context) or end (losing instructions). Requires careful prompt design for long inputs.",
      "tags": [
        "Limitation",
        "Technical"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-trpo",
      "term": "Trust Region Policy Optimization (TRPO)",
      "definition": "A policy gradient algorithm that constrains updates to stay within a trust region defined by KL divergence between old and new policies. TRPO guarantees monotonic policy improvement but is computationally expensive due to conjugate gradient computation.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-trustworthy-ai",
      "term": "Trustworthy AI",
      "definition": "AI systems designed to be lawful, ethical, and robust, meeting criteria such as human oversight, technical robustness, privacy, transparency, fairness, societal well-being, and accountability as defined by frameworks like the EU's HLEG guidelines.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-truthful-ai",
      "term": "Truthful AI",
      "definition": "The goal of building AI systems that consistently provide honest and accurate information, avoiding both deliberate deception and negligent generation of false claims or hallucinations.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-truthfulqa",
      "term": "TruthfulQA",
      "definition": "A benchmark designed to evaluate whether language models generate truthful answers to questions where humans commonly hold misconceptions, testing resistance to generating popular but false beliefs across 38 categories.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-turing-machine",
      "term": "Turing Machine",
      "definition": "An abstract mathematical model of computation proposed by Alan Turing in 1936 that manipulates symbols on a tape according to rules, providing a formal definition of computability and laying theoretical foundations for computer science.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-turing-test",
      "term": "Turing Test",
      "definition": "A test proposed by Alan Turing where a human judge tries to distinguish between human and AI responses. While historically important, modern LLMs have made it less useful as a capability measure.",
      "tags": [
        "Historical",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-td3",
      "term": "Twin Delayed DDPG (TD3)",
      "definition": "An improvement over DDPG that addresses overestimation bias using twin Q-networks, delayed policy updates, and target policy smoothing. TD3 takes the minimum of two critic estimates to form a more conservative value target.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-type-i-error",
      "term": "Type I Error",
      "definition": "The rejection of a true null hypothesis (false positive) in statistical hypothesis testing. The probability of a Type I error is equal to the significance level (alpha) of the test.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-type-ii-error",
      "term": "Type II Error",
      "definition": "The failure to reject a false null hypothesis (false negative) in statistical hypothesis testing. The probability of a Type II error is denoted beta, and statistical power equals 1 minus beta.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-typical-decoding",
      "term": "Typical Decoding",
      "definition": "A text generation strategy that samples tokens whose information content is close to the expected information content of the model. Avoids both high-probability repetitive tokens and low-probability incoherent tokens.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    }
  ]
}