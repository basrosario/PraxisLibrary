{
  "letter": "w",
  "count": 115,
  "terms": [
    {
      "id": "term-wafer-fabrication-equipment",
      "term": "Wafer Fabrication Equipment",
      "definition": "Specialized machinery used in semiconductor manufacturing including lithography etching deposition and inspection systems. The most advanced equipment costs hundreds of millions per unit.",
      "tags": [
        "Manufacturing",
        "Equipment",
        "Category"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-wafer-yield",
      "term": "Wafer Yield",
      "definition": "Percentage of functional dies produced from a semiconductor wafer. Higher yields reduce per-chip costs and are a critical metric for manufacturing efficiency of AI processors.",
      "tags": [
        "Fabrication",
        "Manufacturing",
        "Metric"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-wafer-level-testing",
      "term": "Wafer-Level Testing",
      "definition": "Process of testing semiconductor dies while still on the wafer before they are cut and packaged. Identifies defective dies early saving the cost of packaging non-functional chips.",
      "tags": [
        "Manufacturing",
        "Testing",
        "Process"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-wafer-scale-computing",
      "term": "Wafer-Scale Computing",
      "definition": "An approach to AI hardware that uses an entire silicon wafer as a single chip rather than cutting it into individual dies. Wafer-scale processors like the Cerebras WSE provide massive on-chip memory and compute density with high-bandwidth on-die interconnects.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-wafer-scale-engine",
      "term": "Wafer-Scale Engine",
      "definition": "Chip design approach using an entire silicon wafer as a single massive die rather than cutting it into individual chips. Pioneered by Cerebras for AI training acceleration.",
      "tags": [
        "Architecture",
        "Wafer-Scale",
        "Innovation"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-walter-pitts",
      "term": "Walter Pitts",
      "definition": "American logician (1923-1969) who, with Warren McCulloch, developed the McCulloch-Pitts neuron model in 1943, demonstrating that networks of simple logical units could compute any computable function.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-wards-method",
      "term": "Ward's Method",
      "definition": "An agglomerative clustering linkage criterion that merges the pair of clusters that results in the smallest increase in total within-cluster variance. Tends to produce compact spherical clusters of similar size.",
      "tags": [
        "Algorithms",
        "Technical",
        "Clustering"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-warm-restarts",
      "term": "Warm Restarts",
      "definition": "A training technique that periodically resets the learning rate to a high value during training. Helps the optimizer escape local minima and explore different regions of the loss landscape. Often combined with cosine annealing schedules.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-warm-starting",
      "term": "Warm Starting",
      "definition": "A training strategy that initializes model parameters from a previously trained model rather than random values. Accelerates convergence and can improve performance especially when training data changes incrementally or for transfer learning.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-warm-up-cosine-decay",
      "term": "Warm-up Cosine Decay",
      "definition": "A learning rate schedule that combines a linear warmup phase with cosine decay. Starts with a low learning rate gradually increases to peak then smoothly decreases following a cosine curve. Standard schedule for transformer training.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-warmup",
      "term": "Warmup (Learning Rate)",
      "definition": "Gradually increasing learning rate at the start of training before decay. Helps stabilize early training when gradients might be unreliable with random weights.",
      "tags": [
        "Training",
        "Technique"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-warp",
      "term": "Warp",
      "definition": "Group of 32 threads in NVIDIA GPU architecture that execute the same instruction simultaneously. The basic unit of SIMT execution on NVIDIA streaming multiprocessors.",
      "tags": [
        "GPU",
        "NVIDIA",
        "Architecture"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-warren-mcculloch",
      "term": "Warren McCulloch",
      "definition": "American neurophysiologist (1898-1969) who, with Walter Pitts, created the first mathematical model of an artificial neuron in 1943, laying the theoretical foundation for neural networks and computational neuroscience.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-wasserstein-distance",
      "term": "Wasserstein Distance",
      "definition": "Also known as the Earth Mover's Distance, a metric measuring the minimum cost of transforming one probability distribution into another, where cost is the amount of probability mass moved times the distance it travels.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-watermark",
      "term": "Watermark (AI)",
      "definition": "Hidden patterns in AI-generated content that allow detection of synthetic origins. Proposed for identifying AI text, images, and audio to combat misinformation.",
      "tags": [
        "Safety",
        "Detection"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-watermark-detection",
      "term": "Watermark Detection",
      "definition": "Algorithms that identify statistical patterns embedded in AI-generated text or images during the generation process, enabling attribution of content to specific AI systems.",
      "tags": [
        "Generative AI",
        "LLM"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-watershed-algorithm",
      "term": "Watershed Algorithm",
      "definition": "An image segmentation method that treats the grayscale image as a topographic surface and simulates flooding from markers. The boundaries where different flood basins meet define the segmentation.",
      "tags": [
        "Algorithms",
        "Technical",
        "Vision"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-watson-ibm",
      "term": "Watson (IBM)",
      "definition": "An AI system developed by IBM that defeated champions Brad Rutter and Ken Jennings on the game show Jeopardy! in 2011. Watson used natural language processing information retrieval and machine learning to understand and answer questions posed in natural language.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-wav2vec-20",
      "term": "Wav2Vec 2.0",
      "definition": "A self-supervised speech representation learning framework by Meta AI that learns from raw audio. Pretrained on unlabeled speech data then fine-tuned with minimal labeled data for speech recognition achieving strong results with limited supervision.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wav2vec-u",
      "term": "Wav2Vec-U",
      "definition": "An unsupervised speech recognition model that learns to transcribe speech without any labeled data using self-supervised representations and adversarial training.",
      "tags": [
        "Models",
        "Technical",
        "Audio",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wavcraft",
      "term": "WavCraft",
      "definition": "An LLM-powered framework that edits and creates audio content through natural language instructions by orchestrating specialized audio processing modules.",
      "tags": [
        "Models",
        "Technical",
        "Audio",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-waveform",
      "term": "WaveForM",
      "definition": "A Wavelet-based Forecasting Model that uses wavelet decomposition within a Transformer framework for capturing multi-scale temporal patterns in time series.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wavefront",
      "term": "Wavefront",
      "definition": "Group of 64 threads in AMD GPU architecture analogous to NVIDIA warps. The fundamental unit of SIMT execution on AMD compute units.",
      "tags": [
        "GPU",
        "AMD",
        "Architecture"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-wavelet-transform",
      "term": "Wavelet Transform",
      "definition": "A transform that represents signals using wavelets which are localized oscillating functions. Unlike Fourier transforms wavelets provide both time and frequency information simultaneously. Used in signal compression denoising and feature extraction.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-wavelet-tree-algorithm",
      "term": "Wavelet Tree Algorithm",
      "definition": "A data structure that provides efficient access and rank and select operations on sequences over arbitrary alphabets. Represents the sequence using a balanced binary tree of bitvectors enabling O(log sigma) operations.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-wavenet",
      "term": "WaveNet",
      "definition": "A deep generative model for raw audio waveforms that uses dilated causal convolutions to capture long-range temporal dependencies while maintaining the autoregressive property.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wavlm",
      "term": "WavLM",
      "definition": "A large-scale self-supervised speech model that learns universal representations through masked speech prediction and denoising. Excels on both speech recognition and non-ASR tasks like speaker verification and separation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wavtokenizer",
      "term": "WavTokenizer",
      "definition": "An audio tokenizer that converts continuous audio waveforms into discrete tokens for use in language-model-based speech and audio generation systems.",
      "tags": [
        "Models",
        "Technical",
        "Audio"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wayformer",
      "term": "Wayformer",
      "definition": "A Transformer-based model from Waymo for motion forecasting that predicts future trajectories of vehicles and pedestrians in autonomous driving scenes.",
      "tags": [
        "Models",
        "Technical",
        "Autonomous"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-waymo-history",
      "term": "Waymo History",
      "definition": "The evolution of Google's self-driving car project, started in 2009 by Sebastian Thrun, into Waymo as a subsidiary of Alphabet in 2016, becoming the first commercial autonomous ride-hailing service.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-waymo-multipath",
      "term": "Waymo MultiPath++",
      "definition": "An improved multi-agent trajectory prediction model from Waymo that forecasts future motions of all scene participants simultaneously.",
      "tags": [
        "Models",
        "Technical",
        "Autonomous"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-waymo-open-dataset",
      "term": "Waymo Open Dataset",
      "definition": "A large-scale autonomous driving dataset from Waymo containing lidar and camera data with 3D bounding box annotations. One of the largest and most diverse self-driving perception datasets.",
      "tags": [
        "Benchmark",
        "Autonomous Driving"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-waymo-perception-model",
      "term": "Waymo Perception Model",
      "definition": "The neural network stack used by Waymo autonomous vehicles for detecting and tracking objects using lidar and camera sensor fusion.",
      "tags": [
        "Models",
        "Technical",
        "Autonomous",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wbb",
      "term": "WBB",
      "definition": "The WebBodies Benchmark a dataset of 3D human body models and poses for evaluating human shape estimation algorithms. Tests reconstruction of human body geometry from images.",
      "tags": [
        "Benchmark",
        "3D",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-weak-supervision",
      "term": "Weak Supervision",
      "definition": "Training with noisy, imprecise, or automatically generated labels instead of perfect human annotations. Can dramatically reduce labeling costs while achieving good results.",
      "tags": [
        "Training",
        "Technique"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-weaponization-of-ai",
      "term": "Weaponization of AI",
      "definition": "The development or adaptation of AI systems for military offensive or harmful purposes. Includes autonomous weapons cyberweapons and AI-enhanced surveillance tools used for oppression.",
      "tags": [
        "Safety",
        "Policy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-weaviate",
      "term": "Weaviate",
      "definition": "An open-source vector database that combines vector search with structured filtering and supports multiple vectorization modules, offering hybrid search capabilities and a GraphQL API with built-in support for generative AI integrations.",
      "tags": [
        "Vector Database",
        "Open Source"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-webarena",
      "term": "WebArena",
      "definition": "A benchmark for evaluating autonomous web agents on realistic web tasks across self-hosted websites. Tests the ability of AI agents to complete complex multi-step web interactions.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-webdataset",
      "term": "WebDataset",
      "definition": "A format and library for efficiently storing and loading large-scale datasets using tar archives. Designed for distributed training with sequential access patterns.",
      "tags": [
        "Platform",
        "General"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-webquestions",
      "term": "WebQuestions",
      "definition": "A question answering dataset of 5810 questions collected using Google Suggest. Tests factoid question answering using knowledge bases like Freebase.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-webtext",
      "term": "WebText",
      "definition": "The training corpus for GPT-2 containing 8 million web pages selected through Reddit link quality filtering. Approximately 40GB of text demonstrating the effectiveness of internet-scale pretraining.",
      "tags": [
        "Training Corpus",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-webvid-10m",
      "term": "WebVid-10M",
      "definition": "A large-scale dataset of 10 million video-text pairs scraped from the web. Used for pretraining video-language models and text-to-video generation systems.",
      "tags": [
        "Training Corpus",
        "Video",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-weibull-distribution",
      "term": "Weibull Distribution",
      "definition": "A continuous probability distribution used in reliability analysis and survival modeling. Its shape parameter allows it to model increasing, constant, or decreasing failure rates over time.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight",
      "term": "Weight",
      "definition": "The numerical parameters in neural networks that are learned during training. Weights determine how inputs are transformed into outputs; large models have billions of weights.",
      "tags": [
        "Core Concept",
        "Neural Networks"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-decay",
      "term": "Weight Decay",
      "definition": "A regularization technique that adds a fraction of the current weight values to the weight update rule during training, effectively penalizing large weights. In SGD, it is equivalent to L2 regularization.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-initialization",
      "term": "Weight Initialization",
      "definition": "The strategy for setting initial parameter values in neural networks, with methods like Xavier and He initialization designed to maintain signal variance across layers and prevent training instability.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-normalization",
      "term": "Weight Normalization",
      "definition": "A reparameterization of weight vectors that decouples the magnitude and direction of weight vectors. Proposed by Salimans and Kingma in 2016 as a simpler alternative to batch normalization that does not introduce dependencies between examples.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-sharing",
      "term": "Weight Sharing",
      "definition": "A compression technique where multiple connections in a neural network share the same weight value, reducing the number of unique parameters that must be stored. Weight sharing is used in embedding layers, attention mechanisms, and compressed architectures.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-tying",
      "term": "Weight Tying",
      "definition": "A technique that shares the weight matrix between the input embedding layer and the output projection layer in language models, reducing parameters and often improving performance.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-balanced-tree",
      "term": "Weight-Balanced Tree",
      "definition": "A self-balancing binary search tree that maintains the invariant that the weight (number of nodes) of each child subtree is a bounded fraction of the parent's weight. Supports efficient split and join operations.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-only-quantization",
      "term": "Weight-Only Quantization",
      "definition": "A quantization strategy that compresses only the model weights to low precision while performing computation in higher precision after dequantization. Weight-only quantization reduces memory footprint for memory-bound inference without quantizing activations.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-weights-and-biases",
      "term": "Weights and Biases",
      "definition": "A machine learning operations (MLOps) platform founded in 2017 that provides experiment tracking model management and dataset versioning tools. W&B became widely adopted in AI research and industry for organizing and reproducing machine learning experiments.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-welchs-method",
      "term": "Welch's Method",
      "definition": "A technique for estimating the power spectral density of a signal by averaging modified periodograms of overlapping windowed segments. Reduces variance of the spectral estimate compared to a single periodogram.",
      "tags": [
        "Algorithms",
        "Technical",
        "Signal Processing"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-welsh-powell-algorithm",
      "term": "Welsh-Powell Algorithm",
      "definition": "A greedy graph coloring algorithm that sorts vertices by degree in descending order and assigns the smallest available color to each vertex. Provides an upper bound on the chromatic number but may not find the optimal coloring.",
      "tags": [
        "Algorithms",
        "Technical",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-wenetspeech",
      "term": "WenetSpeech",
      "definition": "A large-scale Mandarin speech corpus of 10000 hours from YouTube and podcasts with multi-domain coverage. One of the largest open Chinese ASR training datasets.",
      "tags": [
        "Training Corpus",
        "Speech",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-whale-optimization-algorithm",
      "term": "Whale Optimization Algorithm",
      "definition": "A metaheuristic inspired by the bubble-net hunting strategy of humpback whales. Simulates encircling prey and spiral bubble-net attacking and random search to explore and exploit the solution space.",
      "tags": [
        "Algorithms",
        "Technical",
        "Metaheuristic"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-whisper",
      "term": "Whisper",
      "definition": "OpenAI's speech recognition model that transcribes audio to text with high accuracy across many languages. Open-sourced, enabling widespread use in transcription applications.",
      "tags": [
        "Model",
        "Speech"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-whisper-architecture",
      "term": "Whisper Architecture",
      "definition": "An encoder-decoder transformer architecture trained on 680,000 hours of multilingual speech data for automatic speech recognition, using log-mel spectrogram features as input.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-whisper-large-v3",
      "term": "Whisper Large V3",
      "definition": "The latest and most accurate version of OpenAI Whisper speech recognition model with improved multilingual transcription and translation capabilities.",
      "tags": [
        "Models",
        "Technical",
        "Audio",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-whistleblower-protection-for-ai",
      "term": "Whistleblower Protection for AI",
      "definition": "Legal and organizational safeguards for individuals who report safety concerns ethical violations or illegal activities related to AI systems. Essential for maintaining accountability in AI development.",
      "tags": [
        "Safety",
        "Policy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-white-noise",
      "term": "White Noise",
      "definition": "A time series of uncorrelated random variables with zero mean and constant variance. It represents purely random variation with no exploitable patterns, serving as the residual target for good time series models.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-white-box-attack",
      "term": "White-Box Attack",
      "definition": "An adversarial attack where the attacker has full knowledge of and access to the target model including its architecture weights and training data. Represents the strongest threat model in adversarial ML.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-wide-and-deep-model",
      "term": "Wide and Deep Model",
      "definition": "A recommendation architecture from Google combining a wide linear model for memorization with a deep neural network for generalization.",
      "tags": [
        "Models",
        "Technical",
        "Recommendation"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-widerface",
      "term": "WiderFace",
      "definition": "A face detection benchmark containing 32203 images with 393703 annotated faces showing large variations in scale pose occlusion expression and lighting conditions.",
      "tags": [
        "Benchmark",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wiener-filter",
      "term": "Wiener Filter",
      "definition": "An optimal linear filter that minimizes the mean squared error between the desired signal and the filter output. Operates in the frequency domain using the power spectral densities of the signal and noise.",
      "tags": [
        "Algorithms",
        "Technical",
        "Signal Processing"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-wikidata",
      "term": "Wikidata",
      "definition": "A free and open knowledge base maintained by the Wikimedia Foundation containing structured data about millions of entities. Used as a training resource for knowledge-intensive NLP tasks.",
      "tags": [
        "Knowledge",
        "Graph"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wikilingua",
      "term": "WikiLingua",
      "definition": "A cross-lingual summarization dataset of how-to guides from WikiHow covering 18 languages. Tests the ability to produce cross-lingual summaries of procedural text.",
      "tags": [
        "Benchmark",
        "NLP",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wikipedia-dumps",
      "term": "Wikipedia Dumps",
      "definition": "Regular snapshots of all Wikipedia articles available in multiple languages. A standard high-quality pretraining data source for language models providing factual encyclopedic knowledge.",
      "tags": [
        "Training Corpus",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wikisql",
      "term": "WikiSQL",
      "definition": "A dataset of 80654 hand-annotated SQL queries and natural language questions on 24241 Wikipedia tables. One of the earliest large-scale text-to-SQL benchmarks.",
      "tags": [
        "Benchmark",
        "NLP",
        "Code"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wikitablequestions",
      "term": "WikiTableQuestions",
      "definition": "A question answering dataset of complex questions over Wikipedia tables requiring operations like comparison aggregation and arithmetic. Tests compositional reasoning over tables.",
      "tags": [
        "Benchmark",
        "NLP",
        "Tabular"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wikitext-103",
      "term": "WikiText-103",
      "definition": "A language modeling dataset containing over 103 million tokens from verified Good and Featured Wikipedia articles. Provides long-range dependency context for language model evaluation.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wildbench",
      "term": "WildBench",
      "definition": "A benchmark of challenging real-world user queries for evaluating LLMs on complex instruction-following tasks. Collected from actual user interactions with AI assistants.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wildchat",
      "term": "WildChat",
      "definition": "A dataset of one million conversations between users and ChatGPT with demographic and toxicity annotations. Provides realistic user interaction data for studying LLM usage patterns.",
      "tags": [
        "Training Corpus",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wildguard",
      "term": "WildGuard",
      "definition": "A dataset for evaluating safety classifiers on in-the-wild user prompts and model responses. Tests the ability to detect harmful content in realistic conversational settings.",
      "tags": [
        "Benchmark",
        "NLP",
        "Safety"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-win-rate",
      "term": "Win Rate",
      "definition": "An evaluation metric that measures the percentage of pairwise comparisons in which one model's output is preferred over another's by human judges or automated evaluators, providing a simple relative quality assessment.",
      "tags": [
        "Evaluation",
        "Ranking"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-windfall-clause",
      "term": "Windfall Clause",
      "definition": "A proposed commitment by AI developers to share the economic benefits of transformative AI widely, ensuring that a small number of companies or nations do not capture disproportionate gains from advanced AI.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-window-attention",
      "term": "Window Attention",
      "definition": "A variant of attention that only looks at nearby tokens rather than the full context. Reduces computational cost for long sequences, used in models like Longformer.",
      "tags": [
        "Architecture",
        "Efficiency"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-window-based-chunking",
      "term": "Window-Based Chunking",
      "definition": "A document splitting method that uses a fixed-size sliding window measured in tokens or characters to create overlapping chunks, providing simple and predictable chunk boundaries regardless of document structure or content semantics.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-windowed-attention",
      "term": "Windowed Attention",
      "definition": "An attention mechanism that restricts each token to attend only to tokens within a fixed window around its position. Reduces computational complexity from quadratic to linear in sequence length. Used in efficient transformer variants.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-wine-quality",
      "term": "Wine Quality",
      "definition": "A dataset of physicochemical properties and quality ratings for red and white Portuguese wines. Used for regression and classification benchmarking in machine learning.",
      "tags": [
        "Benchmark",
        "Tabular"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-winobias",
      "term": "WinoBias",
      "definition": "A Winograd-schema dataset for evaluating gender bias in coreference resolution systems. Contains sentence pairs that test whether models rely on gender stereotypes about occupations.",
      "tags": [
        "Benchmark",
        "NLP",
        "Fairness"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-winograd-schema",
      "term": "Winograd Schema",
      "definition": "A coreference resolution challenge requiring commonsense reasoning to determine what a pronoun refers to, designed as an alternative to the Turing test with pairs of sentences differing by one word.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-winograd-schema-challenge",
      "term": "Winograd Schema Challenge",
      "definition": "A test of machine intelligence proposed by Hector Levesque in 2012 as an alternative to the Turing Test. It presents sentences with ambiguous pronouns that require commonsense knowledge to resolve correctly. Named after Terry Winograd's work on language understanding.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-winogrande",
      "term": "WinoGrande",
      "definition": "A large-scale benchmark of Winograd schema-style coreference resolution problems that tests commonsense reasoning by requiring models to identify the correct referent of ambiguous pronouns using world knowledge.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-winoground",
      "term": "Winoground",
      "definition": "A benchmark of 400 image-caption pairs testing compositional understanding in vision-language models. Each example has two captions differing only in word order paired with matching images.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-winsorization",
      "term": "Winsorization",
      "definition": "A technique for handling outliers by replacing extreme values with specified percentile values rather than removing them. For example, values below the 5th percentile might be set to the 5th percentile value.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-wit",
      "term": "WIT",
      "definition": "Wikipedia-based Image Text a dataset of 37 million image-text pairs from Wikipedia in 108 languages. Provides high-quality curated multimodal data for multilingual vision-language research.",
      "tags": [
        "Training Corpus",
        "Multimodal",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wizard-of-wikipedia",
      "term": "Wizard of Wikipedia",
      "definition": "A dataset of conversations grounded in Wikipedia knowledge where one participant has access to relevant articles. Tests knowledge-grounded dialogue generation.",
      "tags": [
        "Benchmark",
        "NLP",
        "Dialogue"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wizardcoder",
      "term": "WizardCoder",
      "definition": "A code-generation language model fine-tuned using the Evol-Instruct method to progressively increase coding instruction complexity for stronger programming ability.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wizardlm",
      "term": "WizardLM",
      "definition": "A language model fine-tuned using Evol-Instruct a method that progressively creates complex instructions from simple seed instructions. Achieves strong instruction-following capabilities through evolutionary instruction generation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wizardmath",
      "term": "WizardMath",
      "definition": "A mathematical reasoning model fine-tuned using Reinforced Evol-Instruct to progressively enhance mathematical problem-solving capabilities.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wmt-metrics",
      "term": "WMT Metrics",
      "definition": "Shared task datasets for evaluating machine translation evaluation metrics. Tests how well automatic metrics correlate with human judgments of translation quality.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation",
        "Translation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wmt-translation",
      "term": "WMT Translation",
      "definition": "The Workshop on Machine Translation (formerly Workshop on Statistical Machine Translation) held annually since 2006. WMT provides standardized translation tasks and evaluation campaigns that have driven progress in machine translation research.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-wmt-translation-datasets",
      "term": "WMT Translation Datasets",
      "definition": "Annual shared task datasets from the Conference on Machine Translation covering dozens of language pairs. The standard benchmark for evaluating machine translation systems.",
      "tags": [
        "Benchmark",
        "NLP",
        "Translation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wn18rr",
      "term": "WN18RR",
      "definition": "A knowledge graph completion benchmark derived from WordNet with 40943 entities and 11 relation types. Removes inverse relations from WN18 to provide a more challenging evaluation.",
      "tags": [
        "Benchmark",
        "Knowledge",
        "Graph"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wnli",
      "term": "WNLI",
      "definition": "Winograd NLI a natural language inference dataset derived from the Winograd Schema Challenge. Tests pronoun coreference resolution reframed as a textual entailment problem within the GLUE benchmark.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wonder3d",
      "term": "Wonder3D",
      "definition": "A model for single-image 3D reconstruction that generates multi-view normal maps and color images using cross-domain diffusion for consistent geometry.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-word-embedding",
      "term": "Word Embedding",
      "definition": "Dense vector representations of words where similar words have similar vectors. Classic examples include Word2Vec and GloVe; modern LLMs use contextual embeddings that vary by context.",
      "tags": [
        "Representation",
        "NLP"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-word-error-rate",
      "term": "Word Error Rate",
      "definition": "A metric that measures the edit distance between predicted and reference transcriptions at the word level, calculated as the number of word substitutions, insertions, and deletions divided by the total reference words, widely used in speech recognition evaluation.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-word-movers-distance",
      "term": "Word Mover's Distance",
      "definition": "A distance metric between text documents that uses word embeddings to measure the minimum cumulative distance words must travel to transform one document into another. Based on the earth mover's distance formulation.",
      "tags": [
        "Algorithms",
        "Technical",
        "NLP"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-word-segmentation",
      "term": "Word Segmentation",
      "definition": "The task of identifying word boundaries in languages that do not use whitespace to separate words, such as Chinese, Japanese, and Thai, essential for subsequent NLP processing.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-word-sense-disambiguation",
      "term": "Word Sense Disambiguation",
      "definition": "The task of determining which meaning of a polysemous word is used in a given context, selecting from a predefined sense inventory based on surrounding words and discourse.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-word2vec",
      "term": "Word2Vec",
      "definition": "A family of neural network models (Skip-gram and CBOW) that learn dense vector representations of words from large text corpora, capturing semantic relationships so that similar words have nearby embeddings.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-word2vec-algorithm",
      "term": "Word2Vec Algorithm",
      "definition": "A neural network-based method for learning word embeddings from large text corpora. The skip-gram model predicts context words given a target word while CBOW predicts a target word from context words.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "NLP",
        "History"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-word2vec-model",
      "term": "Word2Vec Model",
      "definition": "A shallow neural network model that produces word embeddings by learning to predict context words given a target word or vice versa. The Skip-gram variant predicts context from center words while CBOW predicts center from context.",
      "tags": [
        "Models",
        "Fundamentals"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wordnet",
      "term": "WordNet",
      "definition": "A large lexical database of English where nouns, verbs, adjectives, and adverbs are grouped into cognitive synonym sets (synsets) linked by semantic and lexical relations.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-wordpiece",
      "term": "WordPiece",
      "definition": "A subword tokenization algorithm that greedily selects merges maximizing the likelihood of the training data, used by BERT and related models, splitting unknown words into known subword units.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-wordpiece-tokenization",
      "term": "WordPiece Tokenization",
      "definition": "A subword tokenization algorithm used by BERT and related models that greedily builds a vocabulary of subword units. Selects merges that maximize the likelihood of the training data under a unigram language model.",
      "tags": [
        "Algorithms",
        "Technical",
        "NLP"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-workload-scheduling",
      "term": "Workload Scheduling",
      "definition": "Process of assigning and managing computational tasks across available processing resources. Efficient scheduling maximizes GPU utilization and throughput in AI training clusters.",
      "tags": [
        "Infrastructure",
        "Management"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-world-model",
      "term": "World Model",
      "definition": "An AI's internal representation of how the world works. Used to simulate outcomes and plan actions. A key concept in AI safety discussions about model capabilities.",
      "tags": [
        "Concept",
        "Research"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-world-models",
      "term": "World Models",
      "definition": "Learned neural network representations of environment dynamics that allow an agent to simulate and plan in a latent space. World models compress observations and predict future states, enabling sample-efficient learning through imagined rollouts.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-world-models-algorithm",
      "term": "World Models Algorithm",
      "definition": "A reinforcement learning approach that trains a variational autoencoder and recurrent network to model the environment dynamics. The agent can learn policies entirely within the learned model through imagination.",
      "tags": [
        "Algorithms",
        "Technical",
        "RL"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-worlddreamer",
      "term": "WorldDreamer",
      "definition": "A world model that generates future frames and actions in complex environments using a diffusion-based architecture for open-ended world simulation.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wsj-corpus",
      "term": "WSJ Corpus",
      "definition": "The Wall Street Journal speech corpus containing read speech from WSJ articles. A classic benchmark for large-vocabulary continuous speech recognition systems.",
      "tags": [
        "Benchmark",
        "Speech"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-wuerstchen",
      "term": "Wuerstchen",
      "definition": "A three-stage image generation architecture that compresses images to a very compact latent space for extremely efficient diffusion-based generation.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    }
  ]
}