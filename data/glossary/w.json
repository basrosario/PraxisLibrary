{
  "letter": "w",
  "count": 51,
  "terms": [
    {
      "id": "term-wafer-scale-computing",
      "term": "Wafer-Scale Computing",
      "definition": "An approach to AI hardware that uses an entire silicon wafer as a single chip rather than cutting it into individual dies. Wafer-scale processors like the Cerebras WSE provide massive on-chip memory and compute density with high-bandwidth on-die interconnects.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-walter-pitts",
      "term": "Walter Pitts",
      "definition": "American logician (1923-1969) who, with Warren McCulloch, developed the McCulloch-Pitts neuron model in 1943, demonstrating that networks of simple logical units could compute any computable function.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-warm-restarts",
      "term": "Warm Restarts",
      "definition": "A training technique that periodically resets the learning rate to a high value during training. Helps the optimizer escape local minima and explore different regions of the loss landscape. Often combined with cosine annealing schedules.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-warm-starting",
      "term": "Warm Starting",
      "definition": "A training strategy that initializes model parameters from a previously trained model rather than random values. Accelerates convergence and can improve performance especially when training data changes incrementally or for transfer learning.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-warm-up-cosine-decay",
      "term": "Warm-up Cosine Decay",
      "definition": "A learning rate schedule that combines a linear warmup phase with cosine decay. Starts with a low learning rate gradually increases to peak then smoothly decreases following a cosine curve. Standard schedule for transformer training.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-warmup",
      "term": "Warmup (Learning Rate)",
      "definition": "Gradually increasing learning rate at the start of training before decay. Helps stabilize early training when gradients might be unreliable with random weights.",
      "tags": [
        "Training",
        "Technique"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-warren-mcculloch",
      "term": "Warren McCulloch",
      "definition": "American neurophysiologist (1898-1969) who, with Walter Pitts, created the first mathematical model of an artificial neuron in 1943, laying the theoretical foundation for neural networks and computational neuroscience.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-wasserstein-distance",
      "term": "Wasserstein Distance",
      "definition": "Also known as the Earth Mover's Distance, a metric measuring the minimum cost of transforming one probability distribution into another, where cost is the amount of probability mass moved times the distance it travels.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-watermark",
      "term": "Watermark (AI)",
      "definition": "Hidden patterns in AI-generated content that allow detection of synthetic origins. Proposed for identifying AI text, images, and audio to combat misinformation.",
      "tags": [
        "Safety",
        "Detection"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-watermark-detection",
      "term": "Watermark Detection",
      "definition": "Algorithms that identify statistical patterns embedded in AI-generated text or images during the generation process, enabling attribution of content to specific AI systems.",
      "tags": [
        "Generative AI",
        "LLM"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-watson-ibm",
      "term": "Watson (IBM)",
      "definition": "An AI system developed by IBM that defeated champions Brad Rutter and Ken Jennings on the game show Jeopardy! in 2011. Watson used natural language processing information retrieval and machine learning to understand and answer questions posed in natural language.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-wav2vec-20",
      "term": "Wav2Vec 2.0",
      "definition": "A self-supervised speech representation learning framework by Meta AI that learns from raw audio. Pretrained on unlabeled speech data then fine-tuned with minimal labeled data for speech recognition achieving strong results with limited supervision.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wavelet-transform",
      "term": "Wavelet Transform",
      "definition": "A transform that represents signals using wavelets which are localized oscillating functions. Unlike Fourier transforms wavelets provide both time and frequency information simultaneously. Used in signal compression denoising and feature extraction.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-wavenet",
      "term": "WaveNet",
      "definition": "A deep generative model for raw audio waveforms that uses dilated causal convolutions to capture long-range temporal dependencies while maintaining the autoregressive property.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wavlm",
      "term": "WavLM",
      "definition": "A large-scale self-supervised speech model that learns universal representations through masked speech prediction and denoising. Excels on both speech recognition and non-ASR tasks like speaker verification and separation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-waymo-history",
      "term": "Waymo History",
      "definition": "The evolution of Google's self-driving car project, started in 2009 by Sebastian Thrun, into Waymo as a subsidiary of Alphabet in 2016, becoming the first commercial autonomous ride-hailing service.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-weak-supervision",
      "term": "Weak Supervision",
      "definition": "Training with noisy, imprecise, or automatically generated labels instead of perfect human annotations. Can dramatically reduce labeling costs while achieving good results.",
      "tags": [
        "Training",
        "Technique"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-weaviate",
      "term": "Weaviate",
      "definition": "An open-source vector database that combines vector search with structured filtering and supports multiple vectorization modules, offering hybrid search capabilities and a GraphQL API with built-in support for generative AI integrations.",
      "tags": [
        "Vector Database",
        "Open Source"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-weibull-distribution",
      "term": "Weibull Distribution",
      "definition": "A continuous probability distribution used in reliability analysis and survival modeling. Its shape parameter allows it to model increasing, constant, or decreasing failure rates over time.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight",
      "term": "Weight",
      "definition": "The numerical parameters in neural networks that are learned during training. Weights determine how inputs are transformed into outputs; large models have billions of weights.",
      "tags": [
        "Core Concept",
        "Neural Networks"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-decay",
      "term": "Weight Decay",
      "definition": "A regularization technique that adds a fraction of the current weight values to the weight update rule during training, effectively penalizing large weights. In SGD, it is equivalent to L2 regularization.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-initialization",
      "term": "Weight Initialization",
      "definition": "The strategy for setting initial parameter values in neural networks, with methods like Xavier and He initialization designed to maintain signal variance across layers and prevent training instability.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-normalization",
      "term": "Weight Normalization",
      "definition": "A reparameterization of weight vectors that decouples the magnitude and direction of weight vectors. Proposed by Salimans and Kingma in 2016 as a simpler alternative to batch normalization that does not introduce dependencies between examples.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-sharing",
      "term": "Weight Sharing",
      "definition": "A compression technique where multiple connections in a neural network share the same weight value, reducing the number of unique parameters that must be stored. Weight sharing is used in embedding layers, attention mechanisms, and compressed architectures.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-tying",
      "term": "Weight Tying",
      "definition": "A technique that shares the weight matrix between the input embedding layer and the output projection layer in language models, reducing parameters and often improving performance.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-weight-only-quantization",
      "term": "Weight-Only Quantization",
      "definition": "A quantization strategy that compresses only the model weights to low precision while performing computation in higher precision after dequantization. Weight-only quantization reduces memory footprint for memory-bound inference without quantizing activations.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-weights-and-biases",
      "term": "Weights and Biases",
      "definition": "A machine learning operations (MLOps) platform founded in 2017 that provides experiment tracking model management and dataset versioning tools. W&B became widely adopted in AI research and industry for organizing and reproducing machine learning experiments.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-whisper",
      "term": "Whisper",
      "definition": "OpenAI's speech recognition model that transcribes audio to text with high accuracy across many languages. Open-sourced, enabling widespread use in transcription applications.",
      "tags": [
        "Model",
        "Speech"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-whisper-architecture",
      "term": "Whisper Architecture",
      "definition": "An encoder-decoder transformer architecture trained on 680,000 hours of multilingual speech data for automatic speech recognition, using log-mel spectrogram features as input.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-white-noise",
      "term": "White Noise",
      "definition": "A time series of uncorrelated random variables with zero mean and constant variance. It represents purely random variation with no exploitable patterns, serving as the residual target for good time series models.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-win-rate",
      "term": "Win Rate",
      "definition": "An evaluation metric that measures the percentage of pairwise comparisons in which one model's output is preferred over another's by human judges or automated evaluators, providing a simple relative quality assessment.",
      "tags": [
        "Evaluation",
        "Ranking"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-windfall-clause",
      "term": "Windfall Clause",
      "definition": "A proposed commitment by AI developers to share the economic benefits of transformative AI widely, ensuring that a small number of companies or nations do not capture disproportionate gains from advanced AI.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-window-attention",
      "term": "Window Attention",
      "definition": "A variant of attention that only looks at nearby tokens rather than the full context. Reduces computational cost for long sequences, used in models like Longformer.",
      "tags": [
        "Architecture",
        "Efficiency"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-window-based-chunking",
      "term": "Window-Based Chunking",
      "definition": "A document splitting method that uses a fixed-size sliding window measured in tokens or characters to create overlapping chunks, providing simple and predictable chunk boundaries regardless of document structure or content semantics.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-windowed-attention",
      "term": "Windowed Attention",
      "definition": "An attention mechanism that restricts each token to attend only to tokens within a fixed window around its position. Reduces computational complexity from quadratic to linear in sequence length. Used in efficient transformer variants.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-winograd-schema",
      "term": "Winograd Schema",
      "definition": "A coreference resolution challenge requiring commonsense reasoning to determine what a pronoun refers to, designed as an alternative to the Turing test with pairs of sentences differing by one word.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-winograd-schema-challenge",
      "term": "Winograd Schema Challenge",
      "definition": "A test of machine intelligence proposed by Hector Levesque in 2012 as an alternative to the Turing Test. It presents sentences with ambiguous pronouns that require commonsense knowledge to resolve correctly. Named after Terry Winograd's work on language understanding.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-winogrande",
      "term": "WinoGrande",
      "definition": "A large-scale benchmark of Winograd schema-style coreference resolution problems that tests commonsense reasoning by requiring models to identify the correct referent of ambiguous pronouns using world knowledge.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-winsorization",
      "term": "Winsorization",
      "definition": "A technique for handling outliers by replacing extreme values with specified percentile values rather than removing them. For example, values below the 5th percentile might be set to the 5th percentile value.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-wizardlm",
      "term": "WizardLM",
      "definition": "A language model fine-tuned using Evol-Instruct a method that progressively creates complex instructions from simple seed instructions. Achieves strong instruction-following capabilities through evolutionary instruction generation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wmt-translation",
      "term": "WMT Translation",
      "definition": "The Workshop on Machine Translation (formerly Workshop on Statistical Machine Translation) held annually since 2006. WMT provides standardized translation tasks and evaluation campaigns that have driven progress in machine translation research.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-word-embedding",
      "term": "Word Embedding",
      "definition": "Dense vector representations of words where similar words have similar vectors. Classic examples include Word2Vec and GloVe; modern LLMs use contextual embeddings that vary by context.",
      "tags": [
        "Representation",
        "NLP"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-word-error-rate",
      "term": "Word Error Rate",
      "definition": "A metric that measures the edit distance between predicted and reference transcriptions at the word level, calculated as the number of word substitutions, insertions, and deletions divided by the total reference words, widely used in speech recognition evaluation.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-word-segmentation",
      "term": "Word Segmentation",
      "definition": "The task of identifying word boundaries in languages that do not use whitespace to separate words, such as Chinese, Japanese, and Thai, essential for subsequent NLP processing.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-word-sense-disambiguation",
      "term": "Word Sense Disambiguation",
      "definition": "The task of determining which meaning of a polysemous word is used in a given context, selecting from a predefined sense inventory based on surrounding words and discourse.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-word2vec",
      "term": "Word2Vec",
      "definition": "A family of neural network models (Skip-gram and CBOW) that learn dense vector representations of words from large text corpora, capturing semantic relationships so that similar words have nearby embeddings.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-word2vec-model",
      "term": "Word2Vec Model",
      "definition": "A shallow neural network model that produces word embeddings by learning to predict context words given a target word or vice versa. The Skip-gram variant predicts context from center words while CBOW predicts center from context.",
      "tags": [
        "Models",
        "Fundamentals"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-wordnet",
      "term": "WordNet",
      "definition": "A large lexical database of English where nouns, verbs, adjectives, and adverbs are grouped into cognitive synonym sets (synsets) linked by semantic and lexical relations.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-wordpiece",
      "term": "WordPiece",
      "definition": "A subword tokenization algorithm that greedily selects merges maximizing the likelihood of the training data, used by BERT and related models, splitting unknown words into known subword units.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-world-model",
      "term": "World Model",
      "definition": "An AI's internal representation of how the world works. Used to simulate outcomes and plan actions. A key concept in AI safety discussions about model capabilities.",
      "tags": [
        "Concept",
        "Research"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-world-models",
      "term": "World Models",
      "definition": "Learned neural network representations of environment dynamics that allow an agent to simulate and plan in a latent space. World models compress observations and predict future states, enabling sample-efficient learning through imagined rollouts.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    }
  ]
}