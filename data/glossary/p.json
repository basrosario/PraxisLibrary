{
  "letter": "p",
  "count": 316,
  "terms": [
    {
      "id": "term-p-value",
      "term": "P-Value",
      "definition": "The probability of observing a test statistic at least as extreme as the one computed from the data, assuming the null hypothesis is true. Smaller p-values provide stronger evidence against the null hypothesis.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-p3",
      "term": "P3",
      "definition": "The Public Pool of Prompts a collection of prompted templates for over 170 NLP datasets. Used to train T0 demonstrating the effectiveness of multitask prompted training.",
      "tags": [
        "Training Corpus",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-pac-learning",
      "term": "PAC Learning",
      "definition": "Probably Approximately Correct learning, a theoretical framework that defines the conditions under which a learning algorithm can, with high probability, produce a hypothesis that is approximately correct, given sufficient training data.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-padchest",
      "term": "PadChest",
      "definition": "A large multilabeled chest X-ray dataset from Spain containing over 160000 images with 174 radiographic findings. Provides one of the most extensively labeled medical imaging datasets.",
      "tags": [
        "Benchmark",
        "Medical",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-padding",
      "term": "Padding",
      "definition": "The addition of extra values (typically zeros) around the borders of an input image or feature map before convolution, controlling the spatial dimensions of the output and preserving edge information.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-paddleocr-pp-ocrv4",
      "term": "PaddleOCR PP-OCRv4",
      "definition": "The fourth generation of the PaddleOCR text recognition system from Baidu with improved detection and recognition accuracy for diverse document formats.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pade-approximation",
      "term": "Pade Approximation",
      "definition": "A rational function approximation technique that matches the Taylor series of a function to a specified order. Often provides better approximations than truncated power series especially near singularities.",
      "tags": [
        "Algorithms",
        "Technical",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-paged-attention",
      "term": "Paged Attention",
      "definition": "A memory management technique for KV caches during LLM serving that stores attention keys and values in non-contiguous memory pages, reducing waste and enabling efficient batched inference.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pagedattention",
      "term": "PagedAttention",
      "definition": "Memory management technique for KV caches that allocates memory in fixed-size pages rather than contiguous blocks. Eliminates memory waste from fragmentation in language model serving.",
      "tags": [
        "Inference",
        "Memory",
        "Optimization"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pagerank-algorithm",
      "term": "PageRank Algorithm",
      "definition": "An algorithm developed by Larry Page and Sergey Brin that ranks web pages by analyzing the link structure of the web. Models a random surfer who follows hyperlinks and occasionally jumps to a random page with a damping factor.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Graph",
        "History"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-painn",
      "term": "PaiNN",
      "definition": "Polarizable Atom Interaction Neural Network is an equivariant message passing model for molecular simulations that uses vector-valued features for directional interactions.",
      "tags": [
        "Models",
        "Scientific"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pairing-heap-algorithm",
      "term": "Pairing Heap Algorithm",
      "definition": "A self-adjusting heap data structure with simple implementation and excellent practical performance. Supports insert in O(1) and decrease-key in O(log log n) amortized time making it useful in graph algorithms.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pali",
      "term": "PaLI",
      "definition": "Pathways Language and Image model jointly scales a vision transformer and language model. Trained on WebLI a multilingual image-text dataset. Achieves state-of-the-art on numerous vision-language benchmarks with a unified architecture.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pali-3",
      "term": "PaLI-3",
      "definition": "A third-generation vision-language model from Google that uses a SigLIP visual encoder with a language model for strong multimodal understanding at smaller scales.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pali-x",
      "term": "PaLI-X",
      "definition": "A scaled-up version of the PaLI vision-language model with 55 billion parameters that achieves strong performance across visual understanding benchmarks.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-palm",
      "term": "PaLM",
      "definition": "Pathways Language Model, a 540-billion parameter dense transformer by Google that demonstrated breakthrough performance on reasoning tasks using the Pathways system for efficient training.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-palm-2",
      "term": "PaLM 2",
      "definition": "Google's second generation language model with improved multilingual reasoning and coding capabilities compared to PaLM. Uses a more compute-optimal approach with better data mixture. Powers Bard and various Google products.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-palm-e",
      "term": "PaLM-E",
      "definition": "An embodied multimodal language model that integrates real-world continuous sensor data with language. Can reason about the physical world plan robot actions and answer visual questions from embodied observations.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-panda-70m",
      "term": "Panda-70M",
      "definition": "A large-scale video dataset of 70 million high-quality video clips with captions. Provides massive video-text data for pretraining video understanding and generation models.",
      "tags": [
        "Training Corpus",
        "Video",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-pandemonium-architecture",
      "term": "Pandemonium Architecture",
      "definition": "The hierarchical pattern recognition system proposed by Oliver Selfridge in 1959 at the National Physical Laboratory. Multiple levels of demons (computational units) process sensory input through feature detection cognitive evaluation and decision-making in parallel.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-pandemonium-model",
      "term": "Pandemonium Model",
      "definition": "A pattern recognition model proposed by Oliver Selfridge in 1959 using hierarchical layers of feature-detecting demons that compete to identify patterns, anticipating key ideas in modern deep learning architectures.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-pangu-weather",
      "term": "Pangu-Weather",
      "definition": "A 3D Earth-specific Transformer model from Huawei for medium-range weather forecasting that achieves competitive accuracy with traditional numerical weather prediction.",
      "tags": [
        "Models",
        "Scientific"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-panoptic-segmentation",
      "term": "Panoptic Segmentation",
      "definition": "A unified image segmentation task that assigns both a class label and an instance ID to every pixel, combining semantic segmentation of stuff (sky, road) with instance segmentation of things (cars, people).",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-paperclip-maximizer",
      "term": "Paperclip Maximizer",
      "definition": "A thought experiment by Nick Bostrom illustrating the dangers of misaligned AI, in which an AI with the sole objective of maximizing paperclip production converts all available matter into paperclips, including harmful outcomes.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-papers-with-code",
      "term": "Papers With Code",
      "definition": "A platform linking machine learning papers with their code implementations and benchmark results. Tracks state-of-the-art results across thousands of benchmarks and tasks.",
      "tags": [
        "Platform",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-paracrawl",
      "term": "ParaCrawl",
      "definition": "A large web-crawled parallel corpus covering 42 language pairs with billions of sentence pairs. Used for training neural machine translation systems especially for lower-resource languages.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Translation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-parakeet",
      "term": "Parakeet",
      "definition": "A family of automatic speech recognition models from NVIDIA optimized for English transcription with state-of-the-art accuracy and fast inference speed.",
      "tags": [
        "Models",
        "Technical",
        "Audio",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-parallel-distributed-processing",
      "term": "Parallel Distributed Processing",
      "definition": "A two-volume work published in 1986 by David Rumelhart James McClelland and the PDP Research Group. The books provided computational models of cognition using neural networks and helped spark the connectionist revival in AI and cognitive science.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-parallel-processing",
      "term": "Parallel Processing",
      "definition": "Simultaneous execution of multiple computations to solve problems faster. Encompasses data parallelism model parallelism pipeline parallelism and other strategies for distributed AI training.",
      "tags": [
        "Architecture",
        "Fundamentals",
        "Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-parameter-server",
      "term": "Parameter Server",
      "definition": "Distributed architecture where dedicated server nodes store and synchronize model parameters while worker nodes compute gradients. An alternative to all-reduce for distributed training.",
      "tags": [
        "Networking",
        "Distributed Training",
        "Architecture"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-parameters",
      "term": "Parameters",
      "definition": "In prompting: constraints and specifications that shape AI output. In models: the learned weights (billions in LLMs) that determine behavior. Parameter count indicates model scale.",
      "tags": [
        "Core Concept",
        "Dual Meaning"
      ],
      "domain": "general",
      "link": "../learn/crisp.html",
      "related": []
    },
    {
      "id": "term-parametric-relu",
      "term": "Parametric ReLU",
      "definition": "An activation function that generalizes Leaky ReLU by making the negative slope a learnable parameter. Introduced by He et al. in 2015 as part of the work on deep residual networks allowing the model to adaptively learn the appropriate slope for negative inputs.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-paraphrase-detection",
      "term": "Paraphrase Detection",
      "definition": "The task of determining whether two text passages convey the same meaning using different words or structures, requiring understanding of semantic equivalence beyond surface form.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-parent-document-retrieval",
      "term": "Parent Document Retrieval",
      "definition": "A RAG strategy that indexes small child chunks for precise matching but returns their larger parent documents to the LLM, providing sufficient surrounding context for accurate generation.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-parent-child-chunking",
      "term": "Parent-Child Chunking",
      "definition": "A hierarchical chunking strategy that creates small child chunks for precise embedding-based retrieval while linking them to larger parent chunks that provide extended context, returning the parent context when a child chunk is matched.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-parler-tts",
      "term": "Parler-TTS",
      "definition": "An open-source text-to-speech model that generates high-quality speech from natural language descriptions of the desired voice characteristics.",
      "tags": [
        "Models",
        "Technical",
        "Audio"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-parse-tree",
      "term": "Parse Tree",
      "definition": "A hierarchical tree structure representing the syntactic structure of a sentence according to a formal grammar, with internal nodes as phrase categories and leaves as words.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pos-tagging",
      "term": "Part-of-Speech Tagging",
      "definition": "The task of assigning grammatical categories such as noun, verb, adjective, or adverb to each word in a sentence based on its context and morphological form.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-parti",
      "term": "Parti",
      "definition": "Pathways Autoregressive Text-to-Image model by Google that treats image generation as a sequence-to-sequence problem using an autoregressive transformer. Scales well and handles complex prompts with multiple objects and relationships.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-partial-autocorrelation",
      "term": "Partial Autocorrelation",
      "definition": "The correlation between a time series observation and a lagged observation after removing the effects of intermediate lags. It helps determine the order of the autoregressive component in ARIMA models.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-partial-dependence-plot",
      "term": "Partial Dependence Plot",
      "definition": "A visualization showing the marginal effect of one or two features on the predicted outcome of a model, averaging over the values of all other features. It reveals the relationship learned by the model between features and predictions.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-partial-least-squares",
      "term": "Partial Least Squares",
      "definition": "A regression method that simultaneously reduces the dimensionality of predictors and response variables by finding latent components that maximize the covariance between them, useful when predictors outnumber observations.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-partially-observable-mdp",
      "term": "Partially Observable MDP (POMDP)",
      "definition": "An extension of the MDP framework where the agent cannot directly observe the full state and instead receives partial observations. POMDPs require the agent to maintain a belief state or use memory to handle uncertainty about the true state.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-participatory-ai-design",
      "term": "Participatory AI Design",
      "definition": "An approach to AI development that involves affected communities and stakeholders in the design, development, and evaluation process, ensuring that diverse perspectives shape the system's goals and constraints.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-particle-filter-algorithm",
      "term": "Particle Filter Algorithm",
      "definition": "A sequential Monte Carlo method that approximates the posterior distribution of a system state using a set of weighted particles. Effective for nonlinear and non-Gaussian state estimation problems.",
      "tags": [
        "Algorithms",
        "Technical",
        "Signal Processing"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-particle-swarm-optimization",
      "term": "Particle Swarm Optimization",
      "definition": "A population-based optimization algorithm inspired by the social behavior of bird flocking and fish schooling. Particles move through the search space guided by their own best-known position and the swarm's best-known position.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-partiprompts",
      "term": "PartiPrompts",
      "definition": "A collection of 1600 text prompts for evaluating text-to-image generation models created by Google for the Parti model. Covers diverse categories of increasing complexity.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-partnership-on-ai",
      "term": "Partnership on AI",
      "definition": "A multi-stakeholder organization founded in 2016 by major technology companies to study and formulate best practices on AI technologies, advancing understanding of AI's impact on people and society.",
      "tags": [
        "Governance",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-pascal-voc",
      "term": "Pascal VOC",
      "definition": "The PASCAL Visual Object Classes challenge and dataset running from 2005 to 2012. Pascal VOC provided standardized evaluation for object detection image classification and segmentation and was a precursor to larger benchmarks like ImageNet and COCO.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-pass-at-k",
      "term": "Pass@k",
      "definition": "A code generation evaluation metric that measures the probability that at least one of k generated code samples passes all test cases, computed using an unbiased estimator that accounts for the total number of samples generated.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-passage-retrieval",
      "term": "Passage Retrieval",
      "definition": "The task of identifying and retrieving the most relevant text passages from a large corpus in response to a query, operating at a finer granularity than full-document retrieval to provide more precise context for downstream tasks.",
      "tags": [
        "Retrieval",
        "Search"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-patchcore",
      "term": "PatchCore",
      "definition": "An anomaly detection method that maintains a coreset of nominal patch features from a pre-trained network and detects anomalies by measuring distance to nearest patches.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-patchtst",
      "term": "PatchTST",
      "definition": "A Transformer model for time series forecasting that segments input sequences into subseries-level patches and uses channel-independent processing for efficiency.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pathai",
      "term": "PathAI",
      "definition": "An AI platform and model suite for computational pathology that analyzes digitized tissue slides for disease diagnosis and biomarker detection.",
      "tags": [
        "Models",
        "Technical",
        "Medical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pathvqa",
      "term": "PathVQA",
      "definition": "A visual question answering dataset for pathology containing over 32000 questions about 4998 pathology images. Tests multimodal reasoning in the medical domain.",
      "tags": [
        "Benchmark",
        "Medical",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-patience-sort",
      "term": "Patience Sort",
      "definition": "A sorting algorithm inspired by the card game patience that creates sorted piles and merges them. Has connections to the longest increasing subsequence problem and achieves O(n log n) worst-case time.",
      "tags": [
        "Algorithms",
        "Technical",
        "Sorting"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-patrick-winston",
      "term": "Patrick Winston",
      "definition": "American computer scientist (1943-2019) who directed the MIT AI Lab from 1972 to 1997 and authored the influential AI textbook, making significant contributions to learning theory and knowledge representation.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-pattern-recognition-and-machine-learning",
      "term": "Pattern Recognition and Machine Learning",
      "definition": "A textbook by Christopher Bishop published in 2006 that covers Bayesian approaches to pattern recognition and machine learning. The book became influential for presenting machine learning from a probabilistic perspective.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-paws",
      "term": "PAWS",
      "definition": "Paraphrase Adversaries from Word Scrambling a dataset of sentence pairs with high lexical overlap that tests whether models rely on superficial cues for paraphrase detection.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-paxos-algorithm",
      "term": "Paxos Algorithm",
      "definition": "A consensus algorithm for distributed systems that enables a group of processes to agree on a single value despite failures. Guarantees safety (agreement) even when some processes crash or messages are delayed.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pbs-portable-batch-system",
      "term": "PBS (Portable Batch System)",
      "definition": "Job scheduling system for managing workload distribution across compute clusters. Used in some HPC and AI environments for allocating GPU resources to training jobs.",
      "tags": [
        "Infrastructure",
        "Scheduling",
        "System"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pc-algorithm",
      "term": "PC Algorithm",
      "definition": "A constraint-based causal discovery algorithm that starts with a complete graph and removes edges based on conditional independence tests. Named after its creators Peter Spirtes and Clark Glymour.",
      "tags": [
        "Algorithms",
        "Technical",
        "Causal"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pca-algorithm",
      "term": "PCA Algorithm",
      "definition": "Principal Component Analysis finds orthogonal directions of maximum variance in data and projects onto a lower-dimensional subspace. Computed via eigendecomposition of the covariance matrix or SVD of the data matrix.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Dimensionality Reduction"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pca-for-embeddings",
      "term": "PCA for Embeddings",
      "definition": "The application of Principal Component Analysis to reduce embedding dimensionality by projecting vectors onto the directions of maximum variance, commonly used to compress embeddings for faster search with controllable information loss.",
      "tags": [
        "Vector Database",
        "Dimensionality Reduction"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pcie",
      "term": "PCIe for AI",
      "definition": "Peripheral Component Interconnect Express, the standard high-speed serial interface connecting GPUs and accelerators to the host system. PCIe Gen 5 provides up to 64 GB/s bidirectional bandwidth per x16 slot, used for host-device and inter-device communication.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pcie-gen5",
      "term": "PCIe Gen5",
      "definition": "Fifth generation Peripheral Component Interconnect Express standard doubling bandwidth to 32 GT/s per lane or 128 GB/s for x16 slots. Enables faster GPU-to-CPU communication in AI servers.",
      "tags": [
        "Interconnect",
        "Standard"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pcie-gen6",
      "term": "PCIe Gen6",
      "definition": "Sixth generation PCIe standard doubling bandwidth again to 64 GT/s per lane using PAM4 signaling. Will further reduce communication bottlenecks between AI accelerators and host systems.",
      "tags": [
        "Interconnect",
        "Standard"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pdbbind",
      "term": "PDBBind",
      "definition": "A database of experimentally measured binding affinity data for biomolecular complexes from the Protein Data Bank. Used for training molecular docking and drug discovery models.",
      "tags": [
        "Benchmark",
        "Scientific"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-pdp-11",
      "term": "PDP-11",
      "definition": "Digital Equipment Corporation minicomputer from 1970 with an influential instruction set architecture. Ran early Unix systems and influenced the design of many subsequent processors.",
      "tags": [
        "Historical",
        "DEC",
        "Minicomputer"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pdp-8",
      "term": "PDP-8",
      "definition": "Digital Equipment Corporation minicomputer from 1965 considered the first successful commercial minicomputer. Its low price made computing accessible to smaller organizations and laboratories.",
      "tags": [
        "Historical",
        "DEC",
        "Minicomputer"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-peft",
      "term": "PEFT (Parameter-Efficient Fine-Tuning)",
      "definition": "Techniques that fine-tune models by training only a small subset of parameters. Includes LoRA, prefix tuning, and adapters. Dramatically reduces compute and memory needs.",
      "tags": [
        "Training",
        "Efficiency"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-penalty-method",
      "term": "Penalty Method",
      "definition": "An optimization approach that converts constrained problems into unconstrained ones by adding a penalty term for constraint violations. The penalty parameter is increased iteratively to enforce constraint satisfaction.",
      "tags": [
        "Algorithms",
        "Technical",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pengi",
      "term": "Pengi",
      "definition": "An audio language model that frames all audio tasks as text-generation tasks by combining an audio encoder with a pre-trained language model.",
      "tags": [
        "Models",
        "Technical",
        "Audio",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-penn-treebank",
      "term": "Penn Treebank",
      "definition": "A large annotated corpus of English text with part-of-speech tags and syntactic parse trees, widely used as a benchmark for training and evaluating NLP parsers and language models.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-peoplespeech",
      "term": "PeopleSpeech",
      "definition": "A dataset of 30000 hours of English speech from diverse public domain sources for training speech recognition systems. Provides large-scale open ASR training data.",
      "tags": [
        "Training Corpus",
        "Speech"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-perceiver",
      "term": "Perceiver",
      "definition": "A general-purpose architecture that uses cross-attention to map arbitrary high-dimensional inputs to a fixed-size latent array, followed by self-attention in the latent space, handling any input modality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-perceiver-io",
      "term": "Perceiver IO",
      "definition": "An extension of the Perceiver that adds flexible output generation through output queries and cross-attention. Handles diverse input and output structures enabling multi-task learning across different modalities and formats.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-perceptron",
      "term": "Perceptron",
      "definition": "A single-layer neural network model introduced by Frank Rosenblatt in 1957 that could learn to classify linearly separable patterns. Its limitations, demonstrated by Minsky and Papert in 1969, contributed to the first AI winter.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-perceptrons-book",
      "term": "Perceptrons Book",
      "definition": "A 1969 book by Marvin Minsky and Seymour Papert that mathematically analyzed the limitations of single-layer perceptrons particularly their inability to solve the XOR problem. The book's conclusions were widely interpreted as damning neural network research and contributed to the first AI winter.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-perceptual-loss",
      "term": "Perceptual Loss",
      "definition": "A loss function for image generation that compares feature representations from a pre-trained network rather than raw pixel values, encouraging outputs that are perceptually similar to targets.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-performance-per-watt",
      "term": "Performance Per Watt",
      "definition": "Measure of computing efficiency calculated as useful computation divided by power consumed. A key metric for evaluating AI accelerator efficiency especially in power-constrained environments.",
      "tags": [
        "Performance",
        "Efficiency",
        "Metric"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-performer",
      "term": "Performer",
      "definition": "A transformer variant that uses random feature-based approximation of softmax attention through the FAVOR+ mechanism, achieving linear time and space complexity for attention computation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-permutation-importance",
      "term": "Permutation Importance",
      "definition": "A model-agnostic method for estimating feature importance by measuring the increase in prediction error when a single feature's values are randomly shuffled, breaking its relationship with the target.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-perplexity",
      "term": "Perplexity",
      "definition": "A metric measuring how \"surprised\" a language model is by text. Lower perplexity indicates better prediction. Also the name of an AI search engine combining LLMs with web search.",
      "tags": [
        "Metric",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-perplexity-metric",
      "term": "Perplexity Metric",
      "definition": "An intrinsic evaluation metric for language models defined as the exponentiated average negative log-likelihood per token, measuring how well the model predicts a held-out test set.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-persimmon",
      "term": "Persimmon",
      "definition": "An 8.1B parameter language model from Adept AI that uses a novel architecture with squared ReLU activations and rotary position embeddings.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-persistent-data-structure",
      "term": "Persistent Data Structure",
      "definition": "A data structure that preserves all previous versions of itself when modified. Fat nodes and path copying and lazy propagation are techniques for achieving persistence with bounded overhead.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-persistent-memory",
      "term": "Persistent Memory",
      "definition": "Non-volatile memory technology like Intel Optane that sits between DRAM and storage in the memory hierarchy. Offers byte-addressable access with data persistence for AI checkpointing.",
      "tags": [
        "Memory",
        "Storage"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-persona",
      "term": "Persona",
      "definition": "A specific character or role assigned to an AI through prompting. Personas can include expertise, communication style, and behavioral guidelines to shape responses.",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "domain": "general",
      "link": "../learn/crisp.html",
      "related": []
    },
    {
      "id": "term-persona-prompting",
      "term": "Persona Prompting",
      "definition": "A technique that defines a detailed character profile including background, expertise, communication style, and behavioral traits for the model to embody, producing responses that consistently reflect the specified persona throughout a conversation.",
      "tags": [
        "Prompt Engineering",
        "Persona"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-personachat",
      "term": "PersonaChat",
      "definition": "A dialogue dataset where conversations are conditioned on assigned persona descriptions. Contains over 160000 utterances testing the ability to maintain consistent personality in dialogue.",
      "tags": [
        "Benchmark",
        "NLP",
        "Dialogue"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-personalitychat",
      "term": "PersonalityChat",
      "definition": "A dataset of conversational responses exhibiting different personality traits for training dialogue systems with consistent and controllable personalities.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Dialogue"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-perspective-n-point-algorithm",
      "term": "Perspective-n-Point Algorithm",
      "definition": "An algorithm that estimates the pose of a calibrated camera given a set of n 3D-to-2D point correspondences. P3P solves the minimal case with three points and RANSAC is used to handle outliers.",
      "tags": [
        "Algorithms",
        "Technical",
        "Vision"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pes2o",
      "term": "PeS2o",
      "definition": "A preprocessed version of S2ORC designed for efficient pretraining containing cleaned text from millions of academic papers. Used as a component in several open pretraining datasets.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Scientific"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-peter-norvig",
      "term": "Peter Norvig",
      "definition": "American computer scientist and co-author of Artificial Intelligence: A Modern Approach the most widely used AI textbook. Former Director of Research at Google and NASA computational sciences director. Known for contributions to natural language processing and AI education.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-pflops",
      "term": "PFLOPS",
      "definition": "PetaFLOPS or one quadrillion floating-point operations per second. Modern AI GPU clusters deliver hundreds to thousands of PFLOPS of AI compute for training large models.",
      "tags": [
        "Performance",
        "Metric",
        "Scale"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-phase-correlation-algorithm",
      "term": "Phase Correlation Algorithm",
      "definition": "A frequency-domain method for estimating the translational shift between two images. Computes the normalized cross-power spectrum and finds the peak in the inverse Fourier transform to determine displacement.",
      "tags": [
        "Algorithms",
        "Technical",
        "Vision"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-phase-change-memory",
      "term": "Phase-Change Memory",
      "definition": "Non-volatile memory technology that stores data by switching material between crystalline and amorphous states. Offers faster access than flash with higher density than DRAM.",
      "tags": [
        "Memory",
        "Emerging",
        "Non-Volatile"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-phenaki",
      "term": "Phenaki",
      "definition": "A model for generating variable-length videos from open-domain text descriptions by using a bidirectional masked Transformer on compressed video tokens.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-phi",
      "term": "Phi",
      "definition": "A family of small language models by Microsoft Research that achieve surprisingly strong performance through careful data curation. Phi models demonstrate that data quality can compensate for smaller model sizes.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-phi-architecture",
      "term": "Phi Architecture",
      "definition": "A family of small language models by Microsoft that achieve strong performance through carefully curated high-quality training data, demonstrating that data quality can compensate for model size.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-phi-training-data",
      "term": "Phi Training Data",
      "definition": "Carefully curated datasets used to train Microsoft Phi models combining filtered web data with synthetic textbook-quality content. Demonstrates the importance of data quality over quantity.",
      "tags": [
        "Training Corpus",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-phi-2",
      "term": "Phi-2",
      "definition": "A 2.7B parameter small language model from Microsoft Research trained on high-quality textbook data that rivals much larger models on reasoning benchmarks.",
      "tags": [
        "Models",
        "Technical",
        "NLP",
        "Fundamentals"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-phi-3",
      "term": "Phi-3",
      "definition": "A family of small language models from Microsoft available in Mini and Small and Medium sizes that achieve strong performance through curated data selection.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-phi-3-vision",
      "term": "Phi-3 Vision",
      "definition": "A multimodal variant of Microsoft Phi-3 that can process both images and text for visual question answering and image understanding tasks.",
      "tags": [
        "Models",
        "Technical",
        "NLP",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-phi-35",
      "term": "Phi-3.5",
      "definition": "A mid-generation update to Microsoft Phi-3 that adds improved multilingual support and longer context windows while maintaining the compact model footprint.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-phind-codellama",
      "term": "Phind CodeLlama",
      "definition": "A code-specialized model fine-tuned from CodeLlama by Phind that excels at programming tasks and technical problem solving with enhanced context handling.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-phoneme",
      "term": "Phoneme",
      "definition": "The smallest unit of sound in a language that can distinguish one word from another, used in speech recognition systems to map acoustic signals to linguistic representations.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-phonetics-in-ai",
      "term": "Phonetics in AI",
      "definition": "The application of phonetic knowledge to AI systems for speech processing, including modeling the acoustic properties of speech sounds for recognition and synthesis tasks.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-photolithography",
      "term": "Photolithography",
      "definition": "Semiconductor manufacturing process using light to transfer patterns from a photomask onto a wafer coated with photoresist. The foundational patterning technique for all modern chip production.",
      "tags": [
        "Fabrication",
        "Manufacturing",
        "Process"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-photomaker",
      "term": "PhotoMaker",
      "definition": "A personalized text-to-image generation model that creates customized portraits by encoding identity information from reference photos into the generation process.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-photomask",
      "term": "Photomask",
      "definition": "Glass plate with precisely defined opaque patterns used to project circuit layouts onto wafers during lithography. Each chip layer requires a separate photomask costing millions for advanced designs.",
      "tags": [
        "Fabrication",
        "Manufacturing",
        "Tool"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-photometric-augmentation",
      "term": "Photometric Augmentation",
      "definition": "Image augmentation techniques that modify pixel values without changing spatial layout, including brightness, contrast, saturation, hue adjustments, and color jittering to improve model robustness.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-photonic-integrated-circuit",
      "term": "Photonic Integrated Circuit",
      "definition": "Optical circuit integrating multiple photonic functions on a single chip similar to electronic integrated circuits. Enables compact high-bandwidth optical computing and communication for AI.",
      "tags": [
        "Emerging",
        "Photonic",
        "Integration"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-photonic-processor",
      "term": "Photonic Processor",
      "definition": "Processor that uses light-based circuits to perform computations particularly matrix multiplications. Companies like Lightmatter and Luminous Computing are developing photonic AI accelerators.",
      "tags": [
        "Emerging",
        "Photonic",
        "Accelerator"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-physbench",
      "term": "PhysBench",
      "definition": "A physics reasoning benchmark testing language models on problems from undergraduate physics. Evaluates quantitative scientific reasoning capabilities.",
      "tags": [
        "Benchmark",
        "NLP",
        "Reasoning",
        "Scientific"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-physical-symbol-system-hypothesis",
      "term": "Physical Symbol System Hypothesis",
      "definition": "The 1976 hypothesis by Newell and Simon that a physical symbol system has the necessary and sufficient means for intelligent action, providing the theoretical foundation for symbolic AI approaches.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-pi0",
      "term": "Pi0",
      "definition": "A flow-matching-based robot foundation model from Physical Intelligence that generates dexterous robot actions for complex real-world manipulation tasks.",
      "tags": [
        "Models",
        "Technical",
        "Robotics"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pick-a-pic",
      "term": "Pick-a-Pic",
      "definition": "A dataset of text-to-image generation preferences where users chose between pairs of generated images. Used for training reward models for image generation quality.",
      "tags": [
        "Training Corpus",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-pieter-abbeel",
      "term": "Pieter Abbeel",
      "definition": "Belgian-American computer scientist at UC Berkeley known for work on robot learning from demonstration deep reinforcement learning and dexterous robotic manipulation. Co-founder of Covariant which applies AI to robotic warehouse automation.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-pigeonhole-sort",
      "term": "Pigeonhole Sort",
      "definition": "A sorting algorithm suitable for sorting lists where the number of elements and the range of key values are approximately the same. Operates by placing each element into its corresponding pigeonhole and collecting them in order.",
      "tags": [
        "Algorithms",
        "Technical",
        "Sorting"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pii-detection",
      "term": "PII Detection",
      "definition": "Automated identification of personally identifiable information in datasets used for AI training and in AI system outputs. Critical for privacy compliance and preventing unintended disclosure of sensitive information.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-pile-of-law",
      "term": "Pile of Law",
      "definition": "A 256GB corpus of legal and administrative text for pretraining and studying legal language models. Includes court opinions legislation contracts and regulatory filings from US and international sources.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Legal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-pinecone",
      "term": "Pinecone",
      "definition": "A fully managed cloud-native vector database service designed for production machine learning applications, providing serverless and pod-based architectures with built-in filtering, real-time updates, and horizontal scaling for similarity search.",
      "tags": [
        "Vector Database",
        "Managed Service"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pinsage",
      "term": "PinSage",
      "definition": "A graph convolutional network from Pinterest that generates embeddings for billions of items using random-walk-based neighborhood sampling on a pin-board graph.",
      "tags": [
        "Models",
        "Technical",
        "Recommendation"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pipeline",
      "term": "Pipeline (ML)",
      "definition": "A sequence of data processing and modeling steps chained together. Includes preprocessing, feature extraction, model inference, and post-processing. Ensures reproducible workflows.",
      "tags": [
        "Architecture",
        "MLOps"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pipeline-parallelism",
      "term": "Pipeline Parallelism",
      "definition": "A distributed training strategy that partitions model layers into stages across devices, processing different micro-batches simultaneously in a pipeline fashion to improve device utilization.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pipelining",
      "term": "Pipelining",
      "definition": "Processor design technique that overlaps execution of multiple instructions by dividing them into stages. Increases instruction throughput and is fundamental to all modern processor designs.",
      "tags": [
        "Architecture",
        "Fundamentals",
        "Design"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-piqa",
      "term": "PIQA",
      "definition": "Physical Intuition QA a benchmark of 20000 everyday physical commonsense reasoning questions. Tests understanding of how the physical world works through goal-oriented question answering.",
      "tags": [
        "Benchmark",
        "NLP",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-pitch-detection-algorithm",
      "term": "Pitch Detection Algorithm",
      "definition": "An algorithm that estimates the fundamental frequency of a periodic or quasi-periodic signal. Methods include autocorrelation and cepstrum analysis and YIN which uses the difference function for robust estimation.",
      "tags": [
        "Algorithms",
        "Technical",
        "Signal Processing"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pix2pix",
      "term": "Pix2Pix",
      "definition": "A conditional GAN framework for paired image-to-image translation that uses a U-Net generator and PatchGAN discriminator to learn mappings between aligned image pairs.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pix2struct",
      "term": "Pix2Struct",
      "definition": "A pre-trained image-to-text model from Google that learns to parse visual structure from screenshots for document understanding and chart analysis.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pixart-alpha",
      "term": "PixArt-alpha",
      "definition": "A training-efficient text-to-image diffusion Transformer that achieves photorealistic image generation quality comparable to larger models with reduced compute costs.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pixart-delta",
      "term": "PixArt-Delta",
      "definition": "A refined variant of PixArt featuring improved training efficiency and output quality through enhanced attention mechanisms and training data curation.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pixart-sigma",
      "term": "PixArt-Sigma",
      "definition": "An improved version of PixArt that generates higher-resolution images with better text alignment using a more advanced Transformer architecture.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pixtral",
      "term": "Pixtral",
      "definition": "A vision-language model from Mistral AI that natively processes images at their original resolution without resizing for accurate visual understanding.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pku-saferlhf",
      "term": "PKU-SafeRLHF",
      "definition": "A safety alignment dataset from Peking University containing human preference labels for helpfulness and harmlessness. Supports safety-focused RLHF training.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Safety"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-place-and-route",
      "term": "Place and Route",
      "definition": "Chip design step that determines the physical locations of logic cells and the routing of interconnections between them. Directly impacts chip performance power and area.",
      "tags": [
        "Manufacturing",
        "Design",
        "Process"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-places365",
      "term": "Places365",
      "definition": "A scene recognition dataset containing over 1.8 million images across 365 scene categories. Developed at MIT to advance visual understanding of scenes and environments.",
      "tags": [
        "Benchmark",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-plan-and-execute-agent",
      "term": "Plan-and-Execute Agent",
      "definition": "An agentic architecture that separates high-level planning from step-by-step execution, with a planner LLM creating task decompositions and an executor LLM carrying out individual steps.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-plan-and-solve-plus",
      "term": "Plan-and-Solve Plus",
      "definition": "An enhanced version of plan-and-solve prompting that adds detailed instructions to extract relevant variables, calculate intermediate results, and pay attention to calculation and commonsense reasoning during plan execution.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-planarity-testing-algorithm",
      "term": "Planarity Testing Algorithm",
      "definition": "An algorithm that determines whether a graph can be drawn in the plane without edge crossings. Linear-time algorithms by Hopcroft-Tarjan and Boyer-Myrvold solve this using path addition or edge addition strategies.",
      "tags": [
        "Algorithms",
        "Technical",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-planning-in-ai",
      "term": "Planning in AI",
      "definition": "The area of AI concerned with the realization of strategies or action sequences typically for autonomous agents robots and drones. AI planning encompasses classical planning (STRIPS) partial-order planning hierarchical task networks and modern approaches using neural networks.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-planning-rl",
      "term": "Planning in RL",
      "definition": "The process of using a model of the environment to compute or improve a policy before or during interaction. Planning methods like Dyna integrate model-based simulation with model-free learning to accelerate convergence.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-plant",
      "term": "PlanT",
      "definition": "A Transformer-based planning model for autonomous driving that directly predicts future waypoints from processed sensor features and route information.",
      "tags": [
        "Models",
        "Technical",
        "Autonomous"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-platt-scaling",
      "term": "Platt Scaling",
      "definition": "A post-hoc calibration method that fits a logistic regression model to the raw output scores of a classifier using a held-out validation set, transforming the scores into well-calibrated probabilities.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-platypus",
      "term": "Platypus",
      "definition": "A family of fine-tuned language models that achieve strong performance through curated STEM and logic datasets with focused parameter-efficient training.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-playground",
      "term": "Playground (AI)",
      "definition": "An interactive interface for experimenting with AI models without coding. Most AI providers offer playgrounds to test prompts, adjust parameters, and explore capabilities.",
      "tags": [
        "Tools",
        "Interface"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-playground-v2",
      "term": "Playground v2",
      "definition": "An open-source text-to-image model optimized for aesthetic quality that uses curated training data and improved sampling to produce visually appealing outputs.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-playground-v25",
      "term": "Playground v2.5",
      "definition": "An updated text-to-image model with improved color and contrast handling and enhanced ability to generate images across multiple aspect ratios.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-plip",
      "term": "PLIP",
      "definition": "Pathology Language and Image Pre-Training is a CLIP-based model fine-tuned on pathology image-text pairs for zero-shot pathology image analysis.",
      "tags": [
        "Models",
        "Technical",
        "Medical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-plotqa",
      "term": "PlotQA",
      "definition": "A question answering dataset about scientific plots containing over 8 million questions about 224000 plots. Tests structural data reasoning and numerical understanding of visualizations.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Scientific"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-pmc-llama",
      "term": "PMC-LLaMA",
      "definition": "A language model adapted for biomedical applications through continued pre-training on 4.8 million PubMed Central scientific articles.",
      "tags": [
        "Models",
        "Technical",
        "NLP",
        "Medical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pmc-oa",
      "term": "PMC-OA",
      "definition": "PubMed Central Open Access a large collection of biomedical literature with full text available for text mining. Provides millions of scientific articles for biomedical NLP research.",
      "tags": [
        "Training Corpus",
        "Medical",
        "Scientific"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-point-cloud",
      "term": "Point Cloud",
      "definition": "A 3D data representation consisting of a set of points in three-dimensional space, typically acquired by LiDAR or depth sensors, used for 3D object detection, segmentation, and scene reconstruction.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-point-e",
      "term": "Point-E",
      "definition": "A system by OpenAI for generating 3D point clouds from text prompts using a two-stage process. First generates a synthetic view using a text-to-image model then produces a 3D point cloud conditioned on the image.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pointnet",
      "term": "PointNet",
      "definition": "A pioneering deep learning architecture that directly processes unordered 3D point cloud data using shared MLPs and symmetric pooling functions to perform classification and segmentation.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pointnet-plus-plus",
      "term": "PointNet++",
      "definition": "An extension of PointNet that introduces hierarchical feature learning by applying PointNet recursively on nested partitions of the point set, capturing local geometric structures at multiple scales.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pointpillars",
      "term": "PointPillars",
      "definition": "A fast 3D object detection model for lidar point clouds that converts point clouds into vertical columns (pillars) for efficient processing.",
      "tags": [
        "Models",
        "Technical",
        "Autonomous",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pointwise-convolution",
      "term": "Pointwise Convolution",
      "definition": "A 1x1 convolution that linearly combines features across channels at each spatial position without considering spatial context, commonly used to change the number of channels.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pmi",
      "term": "Pointwise Mutual Information",
      "definition": "A statistical measure of association between two events that compares their joint probability with their expected co-occurrence under independence, used to identify collocations and build word representations.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-poisson-distribution",
      "term": "Poisson Distribution",
      "definition": "A discrete probability distribution expressing the probability of a given number of events occurring in a fixed interval, given a known average rate and independent occurrences. It is parametrized by the rate lambda.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-poisson-regression",
      "term": "Poisson Regression",
      "definition": "A generalized linear model for count data that assumes the response follows a Poisson distribution and uses a log link function. It models the log of the expected count as a linear combination of predictors.",
      "tags": [
        "Statistics",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-poisson-surface-reconstruction",
      "term": "Poisson Surface Reconstruction",
      "definition": "A technique that reconstructs a smooth watertight surface from an oriented point cloud by solving a Poisson equation. Produces high-quality surfaces from noisy and incomplete 3D scan data.",
      "tags": [
        "Algorithms",
        "Technical",
        "Vision"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-policy",
      "term": "Policy",
      "definition": "A mapping from states to actions (or probability distributions over actions) that defines the agent's behavior. Policies can be deterministic or stochastic and are the central object optimized in RL.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-policy-distillation",
      "term": "Policy Distillation",
      "definition": "A transfer learning technique that trains a student policy to replicate the behavior of one or more teacher policies. Policy distillation can compress multiple task-specific policies into a single multi-task policy or reduce model size for deployment.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-policy-entropy",
      "term": "Policy Entropy",
      "definition": "A measure of randomness in the agent's policy, used as a regularizer in RL to encourage exploration and prevent premature convergence. Entropy bonuses are added to the objective in algorithms like SAC and A3C.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-policy-gradient",
      "term": "Policy Gradient",
      "definition": "A class of RL algorithms that directly optimize the policy by computing gradients of expected return with respect to policy parameters. Policy gradient methods can handle continuous action spaces and stochastic policies naturally.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-policy-iteration",
      "term": "Policy Iteration",
      "definition": "A dynamic programming algorithm for solving Markov Decision Processes that alternates between policy evaluation and policy improvement until convergence. Guaranteed to find the optimal policy in a finite number of iterations.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-polyak-averaging",
      "term": "Polyak Averaging",
      "definition": "A technique that maintains a running average of model parameters during optimization and uses the averaged parameters for final predictions. Proven to achieve optimal convergence rates for convex problems and widely used as exponential moving average in practice.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-polynomial-kernel",
      "term": "Polynomial Kernel",
      "definition": "A kernel function that computes the inner product of feature vectors raised to a specified power, enabling SVMs and other kernel methods to learn polynomial decision boundaries of a given degree.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-polynomial-regression",
      "term": "Polynomial Regression",
      "definition": "A form of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial, capturing non-linear relationships within a linear model framework.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-polysemy",
      "term": "Polysemy",
      "definition": "The property of a word having multiple related meanings, such as 'bank' meaning a financial institution or a river bank, posing challenges for word sense disambiguation.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pooling-operation",
      "term": "Pooling Operation",
      "definition": "A downsampling operation in neural networks that reduces the spatial dimensions of feature maps by aggregating values within local regions, typically using maximum or average functions.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pope",
      "term": "POPE",
      "definition": "Polling-based Object Probing Evaluation a benchmark for measuring object hallucination in large vision-language models. Tests whether models generate descriptions of objects not present in images.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-popqa",
      "term": "PopQA",
      "definition": "A long-tail knowledge QA benchmark testing language model knowledge of entities with varying popularity. Reveals how model performance degrades for less common entities.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-population-based-training",
      "term": "Population-Based Training (PBT)",
      "definition": "A hyperparameter optimization method that trains a population of agents in parallel, periodically replacing poorly performing agents with mutated copies of better ones. PBT adapts hyperparameters during training rather than searching beforehand.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pose-estimation",
      "term": "Pose Estimation",
      "definition": "A computer vision task that detects the positions of body joints or keypoints in images or video, producing a skeletal representation of human body posture used in activity analysis and motion capture.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-positional-encoding",
      "term": "Positional Encoding",
      "definition": "A technique to inject position information into transformers, which otherwise process tokens without order awareness. Can be absolute, relative, or learned (RoPE).",
      "tags": [
        "Architecture",
        "Transformers"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-positional-interpolation",
      "term": "Positional Interpolation",
      "definition": "A method for extending the context length of pretrained language models by linearly interpolating position encodings to fit longer sequences. Requires minimal fine-tuning and preserves model quality on the original context length.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-post-deployment-monitoring",
      "term": "Post-Deployment Monitoring",
      "definition": "The ongoing assessment of AI system behavior performance and impact after release to production. Includes tracking metrics detecting drift and responding to emerging safety concerns.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-post-market-surveillance-for-ai",
      "term": "Post-Market Surveillance for AI",
      "definition": "Regulatory requirements for monitoring AI system safety and performance after deployment modeled on post-market surveillance in medical devices and pharmaceuticals.",
      "tags": [
        "Safety",
        "Policy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-post-norm-transformer",
      "term": "Post-Norm Transformer",
      "definition": "The original transformer configuration where layer normalization is applied after the residual connection in each sublayer, requiring careful learning rate warmup but sometimes yielding better final performance.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-post-training-quantization",
      "term": "Post-Training Quantization (PTQ)",
      "definition": "Quantization applied to an already-trained model without further training, using calibration data to determine optimal scaling factors. PTQ is simpler and faster than QAT but may result in greater accuracy degradation, especially at very low bit-widths.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-posterior-distribution",
      "term": "Posterior Distribution",
      "definition": "The probability distribution of a parameter after updating the prior distribution with observed data via Bayes' theorem. It combines prior beliefs with the likelihood of the data to form updated beliefs.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-posterior-predictive-distribution",
      "term": "Posterior Predictive Distribution",
      "definition": "The distribution of future observations given the observed data, obtained by integrating the likelihood of new data over the posterior distribution of model parameters, naturally incorporating parameter uncertainty.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-potential-based-reward-shaping",
      "term": "Potential-Based Reward Shaping",
      "definition": "A reward shaping method using a potential function over states where the shaping reward equals the discounted difference in potentials between successor and current states. This form guarantees that the optimal policy is preserved.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-powells-method",
      "term": "Powell's Method",
      "definition": "A derivative-free optimization method that minimizes a function by performing sequential one-dimensional optimizations along a set of direction vectors. Updates the direction set to improve convergence without computing gradients.",
      "tags": [
        "Algorithms",
        "Technical",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-power-analysis",
      "term": "Power Analysis",
      "definition": "A statistical method for determining the minimum sample size required to detect an effect of a specified size with a given level of confidence and power, or the power of a test given a fixed sample size.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-power-asymmetry-in-ai",
      "term": "Power Asymmetry in AI",
      "definition": "The imbalance of power between AI developers who control technology and the individuals and communities affected by it. Raises concerns about consent accountability and democratic governance.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-power-distribution-unit",
      "term": "Power Distribution Unit",
      "definition": "Device that distributes electrical power to equipment in a server rack. Modern PDUs for AI racks must handle dramatically higher power loads than traditional server deployments.",
      "tags": [
        "Data Center",
        "Power",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-power-iteration",
      "term": "Power Iteration",
      "definition": "An iterative algorithm that computes the dominant eigenvalue and corresponding eigenvector of a matrix. Starts with a random vector and repeatedly multiplies by the matrix and normalizes until convergence.",
      "tags": [
        "Algorithms",
        "Technical",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-power-supply-unit",
      "term": "Power Supply Unit",
      "definition": "Component that converts AC mains electricity to the DC voltages required by computing equipment. AI servers require high-wattage power supplies to feed power-hungry GPU accelerators.",
      "tags": [
        "Power",
        "Component",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-power-transform",
      "term": "Power Transform",
      "definition": "A family of parametric transformations (including Box-Cox and Yeo-Johnson) applied to make data more Gaussian-like, stabilize variance, and minimize skewness, improving the performance of models that assume normality.",
      "tags": [
        "Data Science",
        "Feature Engineering"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-power-usage-effectiveness",
      "term": "Power Usage Effectiveness",
      "definition": "Data center energy efficiency metric calculated as total facility power divided by IT equipment power. A PUE of 1.0 is perfect and modern AI data centers target values below 1.2.",
      "tags": [
        "Data Center",
        "Efficiency",
        "Metric"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-powerpaint",
      "term": "PowerPaint",
      "definition": "A versatile image inpainting model that handles context-aware filling and text-guided object insertion and shape-guided outpainting through learnable task prompts.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-ppi",
      "term": "PPI",
      "definition": "Protein-Protein Interaction dataset containing graphs representing interactions between proteins in different human tissues. Used for benchmarking graph neural networks on biological networks.",
      "tags": [
        "Benchmark",
        "Graph",
        "Scientific"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-ppo",
      "term": "PPO (Proximal Policy Optimization)",
      "definition": "A reinforcement learning algorithm commonly used in RLHF to train language models. Balances exploration with stable learning, making it practical for large model training.",
      "tags": [
        "Training",
        "Algorithm"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-ppo-clip",
      "term": "PPO-Clip",
      "definition": "The clipped surrogate objective variant of Proximal Policy Optimization that limits policy updates by clipping the probability ratio. Simpler than the KL-penalty variant and the dominant algorithm for RLHF training of language models.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pragmatics",
      "term": "Pragmatics",
      "definition": "The branch of linguistics studying how context, speaker intention, and shared knowledge influence the interpretation of language beyond its literal semantic meaning.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pre-deployment-testing",
      "term": "Pre-Deployment Testing",
      "definition": "Systematic evaluation of AI systems before deployment to identify potential safety issues biases and failure modes. Includes stress testing adversarial evaluation and demographic disaggregation of performance metrics.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-pre-norm",
      "term": "Pre-Norm",
      "definition": "A transformer architecture variant that applies layer normalization before rather than after each sub-layer, improving training stability and enabling the training of very deep models without warm-up.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pre-norm-transformer",
      "term": "Pre-Norm Transformer",
      "definition": "A transformer variant where layer normalization is applied before the attention and feedforward sublayers rather than after, improving training stability and enabling the removal of learning rate warmup.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pre-tokenization",
      "term": "Pre-Tokenization",
      "definition": "The initial splitting of raw text into preliminary units before applying subword tokenization, typically based on whitespace, punctuation, or language-specific rules.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pre-training",
      "term": "Pre-Training",
      "definition": "The initial training phase where models learn general language understanding from vast text data. Creates a foundation that can be fine-tuned for specific tasks.",
      "tags": [
        "Training",
        "Phase"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-precautionary-principle-in-ai",
      "term": "Precautionary Principle in AI",
      "definition": "The application of the precautionary principle to AI development, arguing that when potential harms are severe or irreversible, lack of scientific certainty should not delay protective measures.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-precision",
      "term": "Precision",
      "definition": "In metrics: the proportion of positive predictions that are correct. In computing: the numerical format for model weights (FP32, FP16, INT8), affecting model size and speed.",
      "tags": [
        "Metrics",
        "Technical"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-precision-at-k",
      "term": "Precision at K",
      "definition": "A retrieval evaluation metric that measures the proportion of relevant documents among the top K retrieved results, providing a cutoff-based assessment of how many returned items are actually useful to the user.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-precision-recall-curve-cv",
      "term": "Precision-Recall Curve",
      "definition": "A plot showing the trade-off between precision and recall at different confidence thresholds for an object detector, with the area under the curve corresponding to Average Precision.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-preconditioned-conjugate-gradient",
      "term": "Preconditioned Conjugate Gradient",
      "definition": "An enhanced version of the conjugate gradient method that applies a preconditioner to improve convergence for solving symmetric positive-definite linear systems. The preconditioner approximates the inverse of the coefficient matrix.",
      "tags": [
        "Algorithms",
        "Technical",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-predictive-parity",
      "term": "Predictive Parity",
      "definition": "A fairness metric requiring that the positive predictive value of a classifier is equal across all protected groups, meaning that among individuals predicted positive, the proportion of true positives is the same.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-predictive-policing-ethics",
      "term": "Predictive Policing Ethics",
      "definition": "The ethical concerns surrounding AI systems used to forecast criminal activity, including risks of reinforcing racial biases, violating civil liberties, and creating feedback loops that entrench discriminatory patterns.",
      "tags": [
        "AI Ethics",
        "Fairness"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-preemptible-vms",
      "term": "Preemptible VMs",
      "definition": "Google Cloud's discounted virtual machine instances that last up to 24 hours and can be terminated when resources are needed elsewhere. Preemptible VMs provide cost-effective compute for AI training workloads that implement checkpointing and fault tolerance.",
      "tags": [
        "Distributed Computing",
        "Inference Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-preference-learning",
      "term": "Preference Learning",
      "definition": "A family of techniques that train models using human preference data (rankings or comparisons between outputs) rather than explicit labels, including methods like RLHF, DPO, and IPO.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prefill-phase",
      "term": "Prefill Phase",
      "definition": "The initial phase of LLM inference that processes the entire input prompt in parallel to populate the KV cache. The prefill phase is compute-bound and its duration scales with input sequence length.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prefill-decode-disaggregation",
      "term": "Prefill-Decode Disaggregation",
      "definition": "An inference architecture that separates the compute-bound prefill and memory-bound decode phases onto different hardware optimized for each workload. Disaggregation improves overall throughput by eliminating resource contention between the two phases.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prefix-attention",
      "term": "Prefix Attention",
      "definition": "An attention pattern used in prefix language models where a set of prefix tokens can attend to each other bidirectionally while subsequent tokens use causal attention. Combines encoder-like and decoder-like attention in a single model.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-prefix-caching",
      "term": "Prefix Caching",
      "definition": "An inference optimization that reuses the computed KV cache of shared prompt prefixes across multiple requests, avoiding redundant computation for system prompts or common instruction templates.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prefix-language-model",
      "term": "Prefix Language Model",
      "definition": "A language model architecture where a prefix portion of the input uses bidirectional attention while the remaining portion uses causal attention, combining understanding and generation capabilities.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prefix-tuning",
      "term": "Prefix Tuning",
      "definition": "A parameter-efficient fine-tuning method that prepends trainable vectors to inputs. Only these prefixes are updated during training, keeping the base model frozen.",
      "tags": [
        "Training",
        "Efficiency"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-presence-penalty",
      "term": "Presence Penalty",
      "definition": "A parameter that applies a fixed penalty to any token that has appeared at least once in the output, encouraging the model to introduce new topics and vocabulary.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prewitt-operator",
      "term": "Prewitt Operator",
      "definition": "An edge detection operator similar to the Sobel operator that uses simpler convolution kernels without weighting the center pixel. Computes the gradient magnitude and direction for edge detection in images.",
      "tags": [
        "Algorithms",
        "Technical",
        "Vision"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-prims-algorithm",
      "term": "Prim's Algorithm",
      "definition": "A greedy algorithm that builds a minimum spanning tree by starting from an arbitrary vertex and repeatedly adding the cheapest edge that connects a vertex in the tree to a vertex outside the tree.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-primal-dual-method",
      "term": "Primal-Dual Method",
      "definition": "An optimization framework that simultaneously updates primal and dual variables to find saddle points of the Lagrangian. Forms the basis of interior point methods and many modern convex optimization algorithms.",
      "tags": [
        "Algorithms",
        "Technical",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-principal-component-analysis",
      "term": "Principal Component Analysis",
      "definition": "An unsupervised linear dimensionality reduction technique that projects data onto orthogonal axes (principal components) that maximize variance. The first components capture the most significant patterns in the data.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-principal-component-regression",
      "term": "Principal Component Regression",
      "definition": "A regression technique that first reduces the dimensionality of predictor variables using PCA and then regresses the response on the retained principal components, addressing multicollinearity and high-dimensionality.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-principal-variation-search",
      "term": "Principal Variation Search",
      "definition": "An enhancement to alpha-beta pruning that searches the first move with a full window and subsequent moves with a null window. Re-searches with a full window only when the null window search suggests a better move.",
      "tags": [
        "Algorithms",
        "Technical",
        "RL",
        "Searching"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-prior-distribution",
      "term": "Prior Distribution",
      "definition": "In Bayesian statistics, the probability distribution representing beliefs about a parameter before observing data. It encodes prior knowledge or assumptions and is updated by the likelihood to form the posterior.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-prioritized-experience-replay",
      "term": "Prioritized Experience Replay",
      "definition": "An experience replay strategy that samples transitions with probability proportional to their TD error magnitude, allowing the agent to learn more frequently from surprising or informative experiences. Importance sampling weights correct for the non-uniform sampling.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-prioritized-level-replay",
      "term": "Prioritized Level Replay (PLR)",
      "definition": "An unsupervised environment design method that tracks learning progress on procedurally generated levels and replays those where the agent has the highest regret. PLR creates adaptive curricula that automatically target the frontier of the agent's capabilities.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-priority-queue-algorithm",
      "term": "Priority Queue Algorithm",
      "definition": "An abstract data structure that supports insertion and extraction of the minimum (or maximum) element. Commonly implemented using binary heaps and provides the foundation for Dijkstra's and Prim's algorithms.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-privacy-amplification-by-subsampling",
      "term": "Privacy Amplification by Subsampling",
      "definition": "A technique that amplifies differential privacy guarantees by applying a mechanism only to a random subsample of the dataset. The privacy guarantee improves proportionally to the sampling rate.",
      "tags": [
        "Algorithms",
        "Technical",
        "Privacy"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-privacy-attack",
      "term": "Privacy Attack",
      "definition": "Any technique that extracts private information from a machine learning model or its training process. Includes membership inference model inversion attribute inference and training data extraction attacks.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-privacy-by-design",
      "term": "Privacy by Design",
      "definition": "An approach to system development that embeds privacy protections throughout the entire engineering process rather than adding them as an afterthought. Adopted as a principle in GDPR and AI governance frameworks.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-privacy-preserving",
      "term": "Privacy-Preserving AI",
      "definition": "Techniques to use AI without exposing sensitive data. Includes federated learning, differential privacy, and secure multi-party computation. Critical for healthcare and finance.",
      "tags": [
        "Privacy",
        "Ethics"
      ],
      "domain": "safety",
      "link": "ai-safety.html",
      "related": []
    },
    {
      "id": "term-privacy-preserving-machine-learning",
      "term": "Privacy-Preserving Machine Learning",
      "definition": "Techniques that enable machine learning while protecting the privacy of training data. Includes federated learning differential privacy secure computation and synthetic data generation.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-private-aggregation-of-teacher-ensembles",
      "term": "Private Aggregation of Teacher Ensembles",
      "definition": "A privacy-preserving knowledge transfer framework that trains an ensemble of teacher models on disjoint private data. The student model learns from noisy aggregated teacher predictions with formal differential privacy guarantees.",
      "tags": [
        "Algorithms",
        "Technical",
        "Privacy"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-proactive-risk-management",
      "term": "Proactive Risk Management",
      "definition": "An approach to AI safety that anticipates and mitigates risks before they materialize rather than reacting to incidents after they occur. Includes threat modeling scenario planning and safety by design.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-pcfg",
      "term": "Probabilistic Context-Free Grammar",
      "definition": "A context-free grammar augmented with probabilities for each production rule, enabling statistical parsing by selecting the most probable parse tree for an input sentence.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-probabilistic-graphical-models",
      "term": "Probabilistic Graphical Models",
      "definition": "A framework for representing complex probability distributions using graph structures. Encompassing Bayesian networks and Markov random fields the framework was systematized by Daphne Koller and Nir Friedman in their 2009 textbook and underpins many AI reasoning systems.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-probabilistic-pca-algorithm",
      "term": "Probabilistic PCA Algorithm",
      "definition": "A probabilistic formulation of PCA that models data as generated from a latent variable model with Gaussian noise. Enables handling of missing data and provides a likelihood framework for model selection.",
      "tags": [
        "Algorithms",
        "Technical",
        "Dimensionality Reduction"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-probabilistic-safety",
      "term": "Probabilistic Safety",
      "definition": "An approach to AI safety that provides statistical guarantees about system behavior rather than absolute guarantees. Useful when formal verification is intractable for complex neural network models.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-probing",
      "term": "Probing",
      "definition": "An interpretability technique that trains simple classifiers on the internal representations of a neural network to test what linguistic or semantic information is encoded. Reveals how different layers capture different levels of abstraction.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-probit-model",
      "term": "Probit Model",
      "definition": "A regression model for binary outcomes that uses the cumulative distribution function of the standard normal distribution as the link function, relating the linear predictor to the probability of the positive class.",
      "tags": [
        "Statistics",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-procedural-environment-generation",
      "term": "Procedural Environment Generation",
      "definition": "The automatic creation of diverse training environments through algorithmic variation of level layouts, object positions, and task parameters. Procedural generation improves generalization by exposing agents to a wide distribution of scenarios.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-procedural-fairness",
      "term": "Procedural Fairness",
      "definition": "Fairness in the processes and methods used to develop and deploy AI systems regardless of outcomes. Includes transparency participation and consistent application of rules and criteria.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-process-node",
      "term": "Process Node",
      "definition": "Manufacturing technology generation for semiconductor chips measured in nanometers. Smaller nodes like 3nm and 5nm pack more transistors per area improving performance and efficiency.",
      "tags": [
        "Fabrication",
        "Manufacturing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-process-reward-model",
      "term": "Process Reward Model",
      "definition": "A reward model that scores each intermediate reasoning step rather than only the final answer, enabling more fine-grained feedback for training models to perform multi-step reasoning.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-processing-in-memory",
      "term": "Processing-In-Memory",
      "definition": "Computing paradigm that embeds processing logic directly within or near memory arrays to reduce data movement. Addresses the memory wall bottleneck for data-intensive AI workloads.",
      "tags": [
        "Memory",
        "Architecture",
        "Emerging"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-procgen",
      "term": "ProcGen",
      "definition": "Procedurally generated game environments for reinforcement learning research testing generalization across different levels. Contains 16 unique game environments with controlled difficulty.",
      "tags": [
        "Benchmark",
        "Reinforcement Learning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-procrustes-analysis-algorithm",
      "term": "Procrustes Analysis Algorithm",
      "definition": "A method for comparing two sets of points by finding the optimal rigid transformation (rotation and translation and scaling) that minimizes the sum of squared distances between corresponding points.",
      "tags": [
        "Algorithms",
        "Technical",
        "Dimensionality Reduction"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-product-quantization",
      "term": "Product Quantization",
      "definition": "A vector compression technique that splits high-dimensional vectors into sub-vectors and quantizes each independently using a learned codebook, enabling dramatic memory reduction while supporting fast approximate distance computation via lookup tables.",
      "tags": [
        "Vector Database",
        "Quantization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-product-quantization-algorithm",
      "term": "Product Quantization Algorithm",
      "definition": "A vector compression technique that splits high-dimensional vectors into subvectors and quantizes each independently. Enables efficient approximate nearest-neighbor search with compact codes stored in memory.",
      "tags": [
        "Algorithms",
        "Technical",
        "Searching"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-production-rules",
      "term": "Production Rules",
      "definition": "A knowledge representation formalism consisting of condition-action pairs (if-then rules) used extensively in expert systems. Production systems like OPS5 and CLIPS provided the foundation for rule-based AI systems throughout the 1980s.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-progan",
      "term": "ProGAN",
      "definition": "Progressive Growing of GANs trains the generator and discriminator progressively starting from low resolution and adding layers for higher resolutions. Enables stable training of high-resolution image generation up to 1024x1024.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-progen2",
      "term": "Progen2",
      "definition": "A protein language model from Salesforce Research that generates functional protein sequences conditioned on protein family and function annotations.",
      "tags": [
        "Models",
        "Scientific"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-program-aided-language-model",
      "term": "Program-Aided Language Model",
      "definition": "A framework (PAL) that prompts a language model to generate executable program code as intermediate reasoning steps rather than natural language, offloading computation to a code interpreter for more accurate numerical and logical results.",
      "tags": [
        "Prompt Engineering",
        "Code-Augmented"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-progressive-training",
      "term": "Progressive Training",
      "definition": "A training strategy that gradually increases model or data complexity during training. Examples include progressive growing of GANs and curriculum learning. Helps stabilize training for complex models by building capabilities incrementally.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-project-euler-dataset",
      "term": "Project Euler Dataset",
      "definition": "Mathematical programming problems from the Project Euler platform testing the intersection of mathematical reasoning and programming ability.",
      "tags": [
        "Benchmark",
        "Code",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-project-gutenberg-corpus",
      "term": "Project Gutenberg Corpus",
      "definition": "A collection of over 70000 public domain books from Project Gutenberg used as a pretraining data source. Provides high-quality literary and non-fiction text spanning centuries.",
      "tags": [
        "Training Corpus",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-projected-gradient-descent-attack",
      "term": "Projected Gradient Descent Attack",
      "definition": "An iterative adversarial attack that applies FGSM multiple times with small step sizes projecting back onto the epsilon-ball after each step. Produces stronger adversarial examples than single-step FGSM. A standard benchmark for adversarial robustness.",
      "tags": [
        "Algorithms",
        "Safety"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-prolog",
      "term": "Prolog",
      "definition": "A logic programming language created by Alain Colmerauer and Robert Kowalski in 1972, widely used in AI research for natural language processing, expert systems, and knowledge representation throughout the 1980s.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt",
      "term": "Prompt",
      "definition": "The text input you send to an AI assistant. Can include context, instructions, examples, and constraints. Prompt quality directly influences response quality.",
      "tags": [
        "Core Concept",
        "Fundamentals"
      ],
      "domain": "general",
      "link": "../learn/prompt-basics.html",
      "related": []
    },
    {
      "id": "term-prompt-caching",
      "term": "Prompt Caching",
      "definition": "An optimization technique that stores and reuses the computed key-value representations of common prompt prefixes, reducing redundant computation for repeated or similar queries.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-chaining",
      "term": "Prompt Chaining",
      "definition": "Breaking complex tasks into multiple sequential prompts, where each builds on the previous output. Enables sophisticated workflows and better results on multi-step problems.",
      "tags": [
        "Technique",
        "Advanced"
      ],
      "domain": "general",
      "link": "../learn/index.html",
      "related": []
    },
    {
      "id": "term-prompt-chaining-architecture",
      "term": "Prompt Chaining Architecture",
      "definition": "A system design pattern where multiple prompts are connected in a pipeline or directed graph, with each prompt handling a specific subtask and passing structured outputs to downstream prompts for further processing.",
      "tags": [
        "Prompt Engineering",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-compression",
      "term": "Prompt Compression",
      "definition": "Techniques that reduce the token length of prompts without losing essential information, using methods like selective context, summarization, or learned compression to fit more content within context limits.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-engineering",
      "term": "Prompt Engineering",
      "definition": "The practice of crafting effective prompts to get better results from AI systems. Includes techniques, frameworks (CRISP, COSTAR), and iterative refinement.",
      "tags": [
        "Skill",
        "Practice"
      ],
      "domain": "general",
      "link": "../learn/index.html",
      "related": []
    },
    {
      "id": "term-prompt-engineering-emergence",
      "term": "Prompt Engineering Emergence",
      "definition": "The emergence of prompt engineering as a discipline for designing effective inputs to large language models. As LLMs became more capable the art and science of crafting prompts to elicit desired behaviors became a critical skill in AI application development.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-ensembling",
      "term": "Prompt Ensembling",
      "definition": "A strategy that runs multiple differently-phrased prompts for the same query and aggregates the outputs through voting, averaging, or selection to produce more robust and accurate final responses than any single prompt alone.",
      "tags": [
        "Prompt Engineering",
        "Ensemble"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-extraction-attack",
      "term": "Prompt Extraction Attack",
      "definition": "A targeted attack technique that attempts to reconstruct or extract a model's system prompt, proprietary instructions, or confidential context through systematic probing queries and analysis of model responses.",
      "tags": [
        "Prompt Engineering",
        "Security"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-injection",
      "term": "Prompt Injection",
      "definition": "A security vulnerability where malicious instructions hidden in content cause AI to behave unexpectedly. A significant concern for AI applications processing external data.",
      "tags": [
        "Security",
        "Risk"
      ],
      "domain": "safety",
      "link": "ai-safety.html",
      "related": []
    },
    {
      "id": "term-prompt-leaking",
      "term": "Prompt Leaking",
      "definition": "A security vulnerability where an attacker manipulates a language model into revealing its hidden system prompt or confidential instructions through carefully crafted queries that exploit the model's tendency to be helpful.",
      "tags": [
        "Prompt Engineering",
        "Security"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-optimization",
      "term": "Prompt Optimization",
      "definition": "The systematic process of refining prompt text, structure, and parameters to maximize model performance on a target task, employing techniques ranging from manual iteration to gradient-based or evolutionary search methods.",
      "tags": [
        "Prompt Engineering",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-robustness",
      "term": "Prompt Robustness",
      "definition": "The ability of a prompt to maintain consistent model performance across variations in input phrasing, perturbations, and edge cases, indicating how reliably the prompt produces correct outputs under diverse conditions.",
      "tags": [
        "Prompt Engineering",
        "Robustness"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-sensitivity",
      "term": "Prompt Sensitivity",
      "definition": "The degree to which a model's output quality and correctness varies in response to minor changes in prompt wording, formatting, or example ordering, representing a key challenge in achieving reliable and reproducible results.",
      "tags": [
        "Prompt Engineering",
        "Robustness"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-template",
      "term": "Prompt Template",
      "definition": "A reusable prompt structure with placeholders for variable content. Enables consistent, repeatable interactions and is essential for building AI-powered applications.",
      "tags": [
        "Pattern",
        "Reusable"
      ],
      "domain": "general",
      "link": "../patterns/index.html",
      "related": []
    },
    {
      "id": "term-prompt-templating",
      "term": "Prompt Templating",
      "definition": "The practice of creating reusable prompt structures with placeholder variables that can be dynamically filled with specific inputs at runtime, enabling consistent prompt formatting across multiple queries and use cases.",
      "tags": [
        "Prompt Engineering",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-tuning",
      "term": "Prompt Tuning",
      "definition": "A parameter-efficient method that prepends learnable continuous embeddings (soft prompts) to the input while keeping all model parameters frozen, enabling task adaptation with minimal overhead.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-versioning",
      "term": "Prompt Versioning",
      "definition": "The practice of maintaining version-controlled prompt templates with change tracking, performance baselines, and rollback capabilities, treating prompts as critical software artifacts that require systematic lifecycle management.",
      "tags": [
        "Prompt Engineering",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-promptbench",
      "term": "PromptBench",
      "definition": "A benchmark for evaluating the robustness of large language models to adversarial prompts. Tests model performance under various prompt perturbations and attack strategies.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-pronoun-resolution",
      "term": "Pronoun Resolution",
      "definition": "The specific task of determining which entity a pronoun refers to in context, requiring understanding of gender, number, syntactic position, and semantic plausibility.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-proof-pile",
      "term": "Proof Pile",
      "definition": "A dataset of mathematical text including textbooks proofs and lecture notes curated for training language models on mathematical reasoning. Used for the Llemma math model.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-proof-pile-2",
      "term": "Proof Pile 2",
      "definition": "An expanded mathematical pretraining corpus combining arXiv papers open-source math textbooks and mathematical web pages. Provides over 55 billion tokens of mathematical content.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-proof-number-search",
      "term": "Proof-Number Search",
      "definition": "A best-first search algorithm for solving game trees that maintains proof and disproof numbers at each node. Efficiently determines whether a game position is a win or loss by focusing search on critical subtrees.",
      "tags": [
        "Algorithms",
        "Technical",
        "RL",
        "Searching"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-proofwriter",
      "term": "ProofWriter",
      "definition": "A dataset for evaluating deductive reasoning requiring models to prove or disprove statements given a set of rules and facts expressed in natural language.",
      "tags": [
        "Benchmark",
        "NLP",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-propbank",
      "term": "PropBank",
      "definition": "Proposition Bank, a corpus annotated with predicate-argument structures for verbs, providing semantic role labels that facilitate training and evaluation of semantic role labeling systems.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-propensity-score",
      "term": "Propensity Score",
      "definition": "The probability that a unit is assigned to a particular treatment given its observed covariates. It is used in causal inference to balance treatment and control groups by matching, stratification, or inverse weighting.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-propensity-score-matching-algorithm",
      "term": "Propensity Score Matching Algorithm",
      "definition": "A causal inference technique that matches treated and control units based on their estimated probability of receiving treatment. Reduces confounding bias in observational studies by balancing covariates between groups.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Causal"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-proportionality-assessment",
      "term": "Proportionality Assessment",
      "definition": "An evaluation of whether the benefits of an AI system justify its risks and intrusions on individual rights. A key requirement in European AI regulation and human rights impact assessments.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-prosocial-dialog",
      "term": "Prosocial Dialog",
      "definition": "A dataset of conversations about prosocial behavior containing dialogues that model constructive and empathetic responses to problematic situations.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Dialogue",
        "Safety"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-prospector",
      "term": "PROSPECTOR",
      "definition": "An expert system developed at SRI International in the late 1970s for mineral exploration. PROSPECTOR used Bayesian probability networks to evaluate geological data and notably predicted the location of a molybdenum deposit in Washington state.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-prost",
      "term": "PROST",
      "definition": "Physical Reasoning about Objects Through Space and Time a benchmark testing physical commonsense reasoning about object properties and spatial relationships.",
      "tags": [
        "Benchmark",
        "NLP",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-protected-attributes",
      "term": "Protected Attributes",
      "definition": "Characteristics such as race, gender, age, religion, and disability status that are legally or ethically designated as bases upon which differential treatment by AI systems is prohibited or restricted.",
      "tags": [
        "Fairness",
        "Regulation"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-protected-group",
      "term": "Protected Group",
      "definition": "A demographic group defined by characteristics such as race gender age disability or religion that is legally protected from discrimination. AI fairness evaluation typically disaggregates performance across protected groups.",
      "tags": [
        "Safety",
        "Policy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-proteinmpnn",
      "term": "ProteinMPNN",
      "definition": "A message passing neural network for protein sequence design that predicts amino acid sequences most likely to fold into a given protein backbone structure.",
      "tags": [
        "Models",
        "Scientific"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-protgpt2",
      "term": "ProtGPT2",
      "definition": "A language model trained on the protein sequence universe that generates de novo protein sequences with natural-like properties and structural features.",
      "tags": [
        "Models",
        "Scientific"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prottrans",
      "term": "ProtTrans",
      "definition": "A collection of protein language models that apply NLP Transformer architectures (including BERT and GPT variants) to protein sequence data for structure and function prediction.",
      "tags": [
        "Models",
        "Scientific"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-provenance-tracking",
      "term": "Provenance Tracking",
      "definition": "The recording and verification of the origin and history of data models and AI system components throughout their lifecycle. Essential for accountability reproducibility and supply chain security.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-proximal-gradient-method",
      "term": "Proximal Gradient Method",
      "definition": "An optimization algorithm for minimizing composite objective functions consisting of a smooth term and a non-smooth regularizer. Combines gradient descent on the smooth part with a proximal operator on the regularizer. Underlies algorithms like ISTA and FISTA.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-proximal-point-algorithm",
      "term": "Proximal Point Algorithm",
      "definition": "An optimization method that iteratively minimizes the objective plus a quadratic penalty centered at the current iterate. Forms the theoretical basis for many modern optimization algorithms including proximal gradient methods.",
      "tags": [
        "Algorithms",
        "Technical",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-proxy-discrimination",
      "term": "Proxy Discrimination",
      "definition": "Discrimination that occurs when an AI system uses features that are correlated with protected attributes as proxies, achieving discriminatory outcomes even when protected attributes are explicitly excluded.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-pruning",
      "term": "Pruning",
      "definition": "Removing unnecessary weights or neurons from neural networks to reduce size and increase speed. Can dramatically decrease model size with minimal performance loss.",
      "tags": [
        "Optimization",
        "Efficiency"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pruning-at-initialization",
      "term": "Pruning-at-Initialization",
      "definition": "Techniques that identify and remove redundant weights before any training occurs, based on signal propagation or gradient flow analysis. Methods like SNIP and GraSP aim to find sparse architectures that train as well as dense networks.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pseudo-labeling",
      "term": "Pseudo-Labeling",
      "definition": "A semi-supervised learning technique where a model trained on labeled data generates predictions for unlabeled data and uses high-confidence predictions as additional training labels. Simple yet effective when the initial model has reasonable accuracy.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-psnr",
      "term": "PSNR",
      "definition": "Peak Signal-to-Noise Ratio measures image quality by comparing the maximum possible pixel value to the mean squared error between original and reconstructed images. Expressed in decibels with higher values indicating better quality. Standard metric in image compression.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-public-interest-ai",
      "term": "Public Interest AI",
      "definition": "AI development and deployment focused on serving broad public interests rather than purely commercial objectives. Includes applications in healthcare education environmental protection and public safety.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-pubmed-abstracts",
      "term": "PubMed Abstracts",
      "definition": "A collection of over 35 million biomedical literature abstracts from the PubMed database. Used for biomedical NLP research including information extraction and question answering.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Medical"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-pubmedbert",
      "term": "PubMedBERT",
      "definition": "A BERT model pre-trained from scratch on PubMed biomedical literature abstracts for improved performance on biomedical natural language processing tasks.",
      "tags": [
        "Models",
        "Technical",
        "Medical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pubmedqa",
      "term": "PubMedQA",
      "definition": "A biomedical question answering dataset where questions are derived from PubMed article titles and answers must be yes/no/maybe with supporting reasoning from abstracts.",
      "tags": [
        "Benchmark",
        "NLP",
        "Medical"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-punched-card",
      "term": "Punched Card",
      "definition": "Paper card with holes representing data used for input and programming in early computers. IBM punched cards were the primary data entry method for computers from the 1890s through the 1970s.",
      "tags": [
        "Historical",
        "Storage",
        "Input"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-purposeful-limitation",
      "term": "Purposeful Limitation",
      "definition": "The deliberate restriction of an AI system's capabilities to reduce risk even when greater capability is technically achievable. A precautionary approach to deploying powerful AI systems.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-push-relabel-algorithm",
      "term": "Push-Relabel Algorithm",
      "definition": "A maximum flow algorithm that uses preflow instead of augmenting paths. Pushes excess flow toward the sink and relabels vertices to establish valid residual paths achieving O(V^2 * E) time complexity.",
      "tags": [
        "Algorithms",
        "Technical",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pythia",
      "term": "Pythia",
      "definition": "A suite of language models by EleutherAI ranging from 70M to 12B parameters all trained on exactly the same data in the same order. Designed for studying language model training dynamics and scaling properties.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pytorch",
      "term": "PyTorch",
      "definition": "A popular open-source deep learning framework from Meta, known for its flexibility and Pythonic design. The dominant framework for AI research and increasingly for production.",
      "tags": [
        "Framework",
        "Deep Learning"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pytorch-release",
      "term": "PyTorch Release",
      "definition": "The release of PyTorch by Facebook AI Research in October 2016. With its dynamic computational graphs and Pythonic design PyTorch became the preferred framework for AI research. Its ease of use and flexibility helped democratize deep learning experimentation.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    }
  ]
}