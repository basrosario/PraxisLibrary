{
  "letter": "p",
  "count": 146,
  "terms": [
    {
      "id": "term-p-value",
      "term": "P-Value",
      "definition": "The probability of observing a test statistic at least as extreme as the one computed from the data, assuming the null hypothesis is true. Smaller p-values provide stronger evidence against the null hypothesis.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pac-learning",
      "term": "PAC Learning",
      "definition": "Probably Approximately Correct learning, a theoretical framework that defines the conditions under which a learning algorithm can, with high probability, produce a hypothesis that is approximately correct, given sufficient training data.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-padding",
      "term": "Padding",
      "definition": "The addition of extra values (typically zeros) around the borders of an input image or feature map before convolution, controlling the spatial dimensions of the output and preserving edge information.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-paged-attention",
      "term": "Paged Attention",
      "definition": "A memory management technique for KV caches during LLM serving that stores attention keys and values in non-contiguous memory pages, reducing waste and enabling efficient batched inference.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-palm",
      "term": "PaLM",
      "definition": "Pathways Language Model, a 540-billion parameter dense transformer by Google that demonstrated breakthrough performance on reasoning tasks using the Pathways system for efficient training.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pandemonium-model",
      "term": "Pandemonium Model",
      "definition": "A pattern recognition model proposed by Oliver Selfridge in 1959 using hierarchical layers of feature-detecting demons that compete to identify patterns, anticipating key ideas in modern deep learning architectures.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-panoptic-segmentation",
      "term": "Panoptic Segmentation",
      "definition": "A unified image segmentation task that assigns both a class label and an instance ID to every pixel, combining semantic segmentation of stuff (sky, road) with instance segmentation of things (cars, people).",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-paperclip-maximizer",
      "term": "Paperclip Maximizer",
      "definition": "A thought experiment by Nick Bostrom illustrating the dangers of misaligned AI, in which an AI with the sole objective of maximizing paperclip production converts all available matter into paperclips, including harmful outcomes.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-parameters",
      "term": "Parameters",
      "definition": "In prompting: constraints and specifications that shape AI output. In models: the learned weights (billions in LLMs) that determine behavior. Parameter count indicates model scale.",
      "tags": [
        "Core Concept",
        "Dual Meaning"
      ],
      "domain": "general",
      "link": "../learn/crisp.html",
      "related": []
    },
    {
      "id": "term-parametric-relu",
      "term": "Parametric ReLU",
      "definition": "An activation function that generalizes Leaky ReLU by making the negative slope a learnable parameter. Introduced by He et al. in 2015 as part of the work on deep residual networks allowing the model to adaptively learn the appropriate slope for negative inputs.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-paraphrase-detection",
      "term": "Paraphrase Detection",
      "definition": "The task of determining whether two text passages convey the same meaning using different words or structures, requiring understanding of semantic equivalence beyond surface form.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-parent-document-retrieval",
      "term": "Parent Document Retrieval",
      "definition": "A RAG strategy that indexes small child chunks for precise matching but returns their larger parent documents to the LLM, providing sufficient surrounding context for accurate generation.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-parent-child-chunking",
      "term": "Parent-Child Chunking",
      "definition": "A hierarchical chunking strategy that creates small child chunks for precise embedding-based retrieval while linking them to larger parent chunks that provide extended context, returning the parent context when a child chunk is matched.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-parse-tree",
      "term": "Parse Tree",
      "definition": "A hierarchical tree structure representing the syntactic structure of a sentence according to a formal grammar, with internal nodes as phrase categories and leaves as words.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pos-tagging",
      "term": "Part-of-Speech Tagging",
      "definition": "The task of assigning grammatical categories such as noun, verb, adjective, or adverb to each word in a sentence based on its context and morphological form.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-partial-autocorrelation",
      "term": "Partial Autocorrelation",
      "definition": "The correlation between a time series observation and a lagged observation after removing the effects of intermediate lags. It helps determine the order of the autoregressive component in ARIMA models.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-partial-dependence-plot",
      "term": "Partial Dependence Plot",
      "definition": "A visualization showing the marginal effect of one or two features on the predicted outcome of a model, averaging over the values of all other features. It reveals the relationship learned by the model between features and predictions.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-partial-least-squares",
      "term": "Partial Least Squares",
      "definition": "A regression method that simultaneously reduces the dimensionality of predictors and response variables by finding latent components that maximize the covariance between them, useful when predictors outnumber observations.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-partially-observable-mdp",
      "term": "Partially Observable MDP (POMDP)",
      "definition": "An extension of the MDP framework where the agent cannot directly observe the full state and instead receives partial observations. POMDPs require the agent to maintain a belief state or use memory to handle uncertainty about the true state.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-participatory-ai-design",
      "term": "Participatory AI Design",
      "definition": "An approach to AI development that involves affected communities and stakeholders in the design, development, and evaluation process, ensuring that diverse perspectives shape the system's goals and constraints.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-particle-swarm-optimization",
      "term": "Particle Swarm Optimization",
      "definition": "A population-based optimization algorithm inspired by the social behavior of bird flocking and fish schooling. Particles move through the search space guided by their own best-known position and the swarm's best-known position.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-partnership-on-ai",
      "term": "Partnership on AI",
      "definition": "A multi-stakeholder organization founded in 2016 by major technology companies to study and formulate best practices on AI technologies, advancing understanding of AI's impact on people and society.",
      "tags": [
        "Governance",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-pass-at-k",
      "term": "Pass@k",
      "definition": "A code generation evaluation metric that measures the probability that at least one of k generated code samples passes all test cases, computed using an unbiased estimator that accounts for the total number of samples generated.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-passage-retrieval",
      "term": "Passage Retrieval",
      "definition": "The task of identifying and retrieving the most relevant text passages from a large corpus in response to a query, operating at a finer granularity than full-document retrieval to provide more precise context for downstream tasks.",
      "tags": [
        "Retrieval",
        "Search"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-patrick-winston",
      "term": "Patrick Winston",
      "definition": "American computer scientist (1943-2019) who directed the MIT AI Lab from 1972 to 1997 and authored the influential AI textbook, making significant contributions to learning theory and knowledge representation.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-pca-for-embeddings",
      "term": "PCA for Embeddings",
      "definition": "The application of Principal Component Analysis to reduce embedding dimensionality by projecting vectors onto the directions of maximum variance, commonly used to compress embeddings for faster search with controllable information loss.",
      "tags": [
        "Vector Database",
        "Dimensionality Reduction"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pcie",
      "term": "PCIe for AI",
      "definition": "Peripheral Component Interconnect Express, the standard high-speed serial interface connecting GPUs and accelerators to the host system. PCIe Gen 5 provides up to 64 GB/s bidirectional bandwidth per x16 slot, used for host-device and inter-device communication.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-peft",
      "term": "PEFT (Parameter-Efficient Fine-Tuning)",
      "definition": "Techniques that fine-tune models by training only a small subset of parameters. Includes LoRA, prefix tuning, and adapters. Dramatically reduces compute and memory needs.",
      "tags": [
        "Training",
        "Efficiency"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-penn-treebank",
      "term": "Penn Treebank",
      "definition": "A large annotated corpus of English text with part-of-speech tags and syntactic parse trees, widely used as a benchmark for training and evaluating NLP parsers and language models.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-perceiver",
      "term": "Perceiver",
      "definition": "A general-purpose architecture that uses cross-attention to map arbitrary high-dimensional inputs to a fixed-size latent array, followed by self-attention in the latent space, handling any input modality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-perceptron",
      "term": "Perceptron",
      "definition": "A single-layer neural network model introduced by Frank Rosenblatt in 1957 that could learn to classify linearly separable patterns. Its limitations, demonstrated by Minsky and Papert in 1969, contributed to the first AI winter.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-perceptual-loss",
      "term": "Perceptual Loss",
      "definition": "A loss function for image generation that compares feature representations from a pre-trained network rather than raw pixel values, encouraging outputs that are perceptually similar to targets.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-performer",
      "term": "Performer",
      "definition": "A transformer variant that uses random feature-based approximation of softmax attention through the FAVOR+ mechanism, achieving linear time and space complexity for attention computation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-permutation-importance",
      "term": "Permutation Importance",
      "definition": "A model-agnostic method for estimating feature importance by measuring the increase in prediction error when a single feature's values are randomly shuffled, breaking its relationship with the target.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-perplexity",
      "term": "Perplexity",
      "definition": "A metric measuring how \"surprised\" a language model is by text. Lower perplexity indicates better prediction. Also the name of an AI search engine combining LLMs with web search.",
      "tags": [
        "Metric",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-perplexity-metric",
      "term": "Perplexity Metric",
      "definition": "An intrinsic evaluation metric for language models defined as the exponentiated average negative log-likelihood per token, measuring how well the model predicts a held-out test set.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-persona",
      "term": "Persona",
      "definition": "A specific character or role assigned to an AI through prompting. Personas can include expertise, communication style, and behavioral guidelines to shape responses.",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "domain": "general",
      "link": "../learn/crisp.html",
      "related": []
    },
    {
      "id": "term-persona-prompting",
      "term": "Persona Prompting",
      "definition": "A technique that defines a detailed character profile including background, expertise, communication style, and behavioral traits for the model to embody, producing responses that consistently reflect the specified persona throughout a conversation.",
      "tags": [
        "Prompt Engineering",
        "Persona"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-phi-architecture",
      "term": "Phi Architecture",
      "definition": "A family of small language models by Microsoft that achieve strong performance through carefully curated high-quality training data, demonstrating that data quality can compensate for model size.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-phoneme",
      "term": "Phoneme",
      "definition": "The smallest unit of sound in a language that can distinguish one word from another, used in speech recognition systems to map acoustic signals to linguistic representations.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-phonetics-in-ai",
      "term": "Phonetics in AI",
      "definition": "The application of phonetic knowledge to AI systems for speech processing, including modeling the acoustic properties of speech sounds for recognition and synthesis tasks.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-photometric-augmentation",
      "term": "Photometric Augmentation",
      "definition": "Image augmentation techniques that modify pixel values without changing spatial layout, including brightness, contrast, saturation, hue adjustments, and color jittering to improve model robustness.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-physical-symbol-system-hypothesis",
      "term": "Physical Symbol System Hypothesis",
      "definition": "The 1976 hypothesis by Newell and Simon that a physical symbol system has the necessary and sufficient means for intelligent action, providing the theoretical foundation for symbolic AI approaches.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-pinecone",
      "term": "Pinecone",
      "definition": "A fully managed cloud-native vector database service designed for production machine learning applications, providing serverless and pod-based architectures with built-in filtering, real-time updates, and horizontal scaling for similarity search.",
      "tags": [
        "Vector Database",
        "Managed Service"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pipeline",
      "term": "Pipeline (ML)",
      "definition": "A sequence of data processing and modeling steps chained together. Includes preprocessing, feature extraction, model inference, and post-processing. Ensures reproducible workflows.",
      "tags": [
        "Architecture",
        "MLOps"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pipeline-parallelism",
      "term": "Pipeline Parallelism",
      "definition": "A distributed training strategy that partitions model layers into stages across devices, processing different micro-batches simultaneously in a pipeline fashion to improve device utilization.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pix2pix",
      "term": "Pix2Pix",
      "definition": "A conditional GAN framework for paired image-to-image translation that uses a U-Net generator and PatchGAN discriminator to learn mappings between aligned image pairs.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-plan-and-execute-agent",
      "term": "Plan-and-Execute Agent",
      "definition": "An agentic architecture that separates high-level planning from step-by-step execution, with a planner LLM creating task decompositions and an executor LLM carrying out individual steps.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-plan-and-solve-plus",
      "term": "Plan-and-Solve Plus",
      "definition": "An enhanced version of plan-and-solve prompting that adds detailed instructions to extract relevant variables, calculate intermediate results, and pay attention to calculation and commonsense reasoning during plan execution.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-planning-rl",
      "term": "Planning in RL",
      "definition": "The process of using a model of the environment to compute or improve a policy before or during interaction. Planning methods like Dyna integrate model-based simulation with model-free learning to accelerate convergence.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-platt-scaling",
      "term": "Platt Scaling",
      "definition": "A post-hoc calibration method that fits a logistic regression model to the raw output scores of a classifier using a held-out validation set, transforming the scores into well-calibrated probabilities.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-playground",
      "term": "Playground (AI)",
      "definition": "An interactive interface for experimenting with AI models without coding. Most AI providers offer playgrounds to test prompts, adjust parameters, and explore capabilities.",
      "tags": [
        "Tools",
        "Interface"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-point-cloud",
      "term": "Point Cloud",
      "definition": "A 3D data representation consisting of a set of points in three-dimensional space, typically acquired by LiDAR or depth sensors, used for 3D object detection, segmentation, and scene reconstruction.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pointnet",
      "term": "PointNet",
      "definition": "A pioneering deep learning architecture that directly processes unordered 3D point cloud data using shared MLPs and symmetric pooling functions to perform classification and segmentation.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pointnet-plus-plus",
      "term": "PointNet++",
      "definition": "An extension of PointNet that introduces hierarchical feature learning by applying PointNet recursively on nested partitions of the point set, capturing local geometric structures at multiple scales.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pointwise-convolution",
      "term": "Pointwise Convolution",
      "definition": "A 1x1 convolution that linearly combines features across channels at each spatial position without considering spatial context, commonly used to change the number of channels.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pmi",
      "term": "Pointwise Mutual Information",
      "definition": "A statistical measure of association between two events that compares their joint probability with their expected co-occurrence under independence, used to identify collocations and build word representations.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-poisson-distribution",
      "term": "Poisson Distribution",
      "definition": "A discrete probability distribution expressing the probability of a given number of events occurring in a fixed interval, given a known average rate and independent occurrences. It is parametrized by the rate lambda.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-poisson-regression",
      "term": "Poisson Regression",
      "definition": "A generalized linear model for count data that assumes the response follows a Poisson distribution and uses a log link function. It models the log of the expected count as a linear combination of predictors.",
      "tags": [
        "Statistics",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-policy",
      "term": "Policy",
      "definition": "A mapping from states to actions (or probability distributions over actions) that defines the agent's behavior. Policies can be deterministic or stochastic and are the central object optimized in RL.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-policy-distillation",
      "term": "Policy Distillation",
      "definition": "A transfer learning technique that trains a student policy to replicate the behavior of one or more teacher policies. Policy distillation can compress multiple task-specific policies into a single multi-task policy or reduce model size for deployment.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-policy-entropy",
      "term": "Policy Entropy",
      "definition": "A measure of randomness in the agent's policy, used as a regularizer in RL to encourage exploration and prevent premature convergence. Entropy bonuses are added to the objective in algorithms like SAC and A3C.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-policy-gradient",
      "term": "Policy Gradient",
      "definition": "A class of RL algorithms that directly optimize the policy by computing gradients of expected return with respect to policy parameters. Policy gradient methods can handle continuous action spaces and stochastic policies naturally.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-policy-iteration",
      "term": "Policy Iteration",
      "definition": "A dynamic programming algorithm for solving Markov Decision Processes that alternates between policy evaluation and policy improvement until convergence. Guaranteed to find the optimal policy in a finite number of iterations.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-polyak-averaging",
      "term": "Polyak Averaging",
      "definition": "A technique that maintains a running average of model parameters during optimization and uses the averaged parameters for final predictions. Proven to achieve optimal convergence rates for convex problems and widely used as exponential moving average in practice.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-polynomial-kernel",
      "term": "Polynomial Kernel",
      "definition": "A kernel function that computes the inner product of feature vectors raised to a specified power, enabling SVMs and other kernel methods to learn polynomial decision boundaries of a given degree.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-polynomial-regression",
      "term": "Polynomial Regression",
      "definition": "A form of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial, capturing non-linear relationships within a linear model framework.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-polysemy",
      "term": "Polysemy",
      "definition": "The property of a word having multiple related meanings, such as 'bank' meaning a financial institution or a river bank, posing challenges for word sense disambiguation.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pooling-operation",
      "term": "Pooling Operation",
      "definition": "A downsampling operation in neural networks that reduces the spatial dimensions of feature maps by aggregating values within local regions, typically using maximum or average functions.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-population-based-training",
      "term": "Population-Based Training (PBT)",
      "definition": "A hyperparameter optimization method that trains a population of agents in parallel, periodically replacing poorly performing agents with mutated copies of better ones. PBT adapts hyperparameters during training rather than searching beforehand.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pose-estimation",
      "term": "Pose Estimation",
      "definition": "A computer vision task that detects the positions of body joints or keypoints in images or video, producing a skeletal representation of human body posture used in activity analysis and motion capture.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-positional-encoding",
      "term": "Positional Encoding",
      "definition": "A technique to inject position information into transformers, which otherwise process tokens without order awareness. Can be absolute, relative, or learned (RoPE).",
      "tags": [
        "Architecture",
        "Transformers"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-positional-interpolation",
      "term": "Positional Interpolation",
      "definition": "A method for extending the context length of pretrained language models by linearly interpolating position encodings to fit longer sequences. Requires minimal fine-tuning and preserves model quality on the original context length.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-post-norm-transformer",
      "term": "Post-Norm Transformer",
      "definition": "The original transformer configuration where layer normalization is applied after the residual connection in each sublayer, requiring careful learning rate warmup but sometimes yielding better final performance.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-post-training-quantization",
      "term": "Post-Training Quantization (PTQ)",
      "definition": "Quantization applied to an already-trained model without further training, using calibration data to determine optimal scaling factors. PTQ is simpler and faster than QAT but may result in greater accuracy degradation, especially at very low bit-widths.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-posterior-distribution",
      "term": "Posterior Distribution",
      "definition": "The probability distribution of a parameter after updating the prior distribution with observed data via Bayes' theorem. It combines prior beliefs with the likelihood of the data to form updated beliefs.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-posterior-predictive-distribution",
      "term": "Posterior Predictive Distribution",
      "definition": "The distribution of future observations given the observed data, obtained by integrating the likelihood of new data over the posterior distribution of model parameters, naturally incorporating parameter uncertainty.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-potential-based-reward-shaping",
      "term": "Potential-Based Reward Shaping",
      "definition": "A reward shaping method using a potential function over states where the shaping reward equals the discounted difference in potentials between successor and current states. This form guarantees that the optimal policy is preserved.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-power-analysis",
      "term": "Power Analysis",
      "definition": "A statistical method for determining the minimum sample size required to detect an effect of a specified size with a given level of confidence and power, or the power of a test given a fixed sample size.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-power-transform",
      "term": "Power Transform",
      "definition": "A family of parametric transformations (including Box-Cox and Yeo-Johnson) applied to make data more Gaussian-like, stabilize variance, and minimize skewness, improving the performance of models that assume normality.",
      "tags": [
        "Data Science",
        "Feature Engineering"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-ppo",
      "term": "PPO (Proximal Policy Optimization)",
      "definition": "A reinforcement learning algorithm commonly used in RLHF to train language models. Balances exploration with stable learning, making it practical for large model training.",
      "tags": [
        "Training",
        "Algorithm"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-ppo-clip",
      "term": "PPO-Clip",
      "definition": "The clipped surrogate objective variant of Proximal Policy Optimization that limits policy updates by clipping the probability ratio. Simpler than the KL-penalty variant and the dominant algorithm for RLHF training of language models.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pragmatics",
      "term": "Pragmatics",
      "definition": "The branch of linguistics studying how context, speaker intention, and shared knowledge influence the interpretation of language beyond its literal semantic meaning.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pre-norm",
      "term": "Pre-Norm",
      "definition": "A transformer architecture variant that applies layer normalization before rather than after each sub-layer, improving training stability and enabling the training of very deep models without warm-up.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pre-norm-transformer",
      "term": "Pre-Norm Transformer",
      "definition": "A transformer variant where layer normalization is applied before the attention and feedforward sublayers rather than after, improving training stability and enabling the removal of learning rate warmup.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pre-tokenization",
      "term": "Pre-Tokenization",
      "definition": "The initial splitting of raw text into preliminary units before applying subword tokenization, typically based on whitespace, punctuation, or language-specific rules.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-pre-training",
      "term": "Pre-Training",
      "definition": "The initial training phase where models learn general language understanding from vast text data. Creates a foundation that can be fine-tuned for specific tasks.",
      "tags": [
        "Training",
        "Phase"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-precautionary-principle-in-ai",
      "term": "Precautionary Principle in AI",
      "definition": "The application of the precautionary principle to AI development, arguing that when potential harms are severe or irreversible, lack of scientific certainty should not delay protective measures.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-precision",
      "term": "Precision",
      "definition": "In metrics: the proportion of positive predictions that are correct. In computing: the numerical format for model weights (FP32, FP16, INT8), affecting model size and speed.",
      "tags": [
        "Metrics",
        "Technical"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-precision-at-k",
      "term": "Precision at K",
      "definition": "A retrieval evaluation metric that measures the proportion of relevant documents among the top K retrieved results, providing a cutoff-based assessment of how many returned items are actually useful to the user.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-precision-recall-curve-cv",
      "term": "Precision-Recall Curve",
      "definition": "A plot showing the trade-off between precision and recall at different confidence thresholds for an object detector, with the area under the curve corresponding to Average Precision.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-predictive-parity",
      "term": "Predictive Parity",
      "definition": "A fairness metric requiring that the positive predictive value of a classifier is equal across all protected groups, meaning that among individuals predicted positive, the proportion of true positives is the same.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-predictive-policing-ethics",
      "term": "Predictive Policing Ethics",
      "definition": "The ethical concerns surrounding AI systems used to forecast criminal activity, including risks of reinforcing racial biases, violating civil liberties, and creating feedback loops that entrench discriminatory patterns.",
      "tags": [
        "AI Ethics",
        "Fairness"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-preemptible-vms",
      "term": "Preemptible VMs",
      "definition": "Google Cloud's discounted virtual machine instances that last up to 24 hours and can be terminated when resources are needed elsewhere. Preemptible VMs provide cost-effective compute for AI training workloads that implement checkpointing and fault tolerance.",
      "tags": [
        "Distributed Computing",
        "Inference Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-preference-learning",
      "term": "Preference Learning",
      "definition": "A family of techniques that train models using human preference data (rankings or comparisons between outputs) rather than explicit labels, including methods like RLHF, DPO, and IPO.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prefill-phase",
      "term": "Prefill Phase",
      "definition": "The initial phase of LLM inference that processes the entire input prompt in parallel to populate the KV cache. The prefill phase is compute-bound and its duration scales with input sequence length.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prefill-decode-disaggregation",
      "term": "Prefill-Decode Disaggregation",
      "definition": "An inference architecture that separates the compute-bound prefill and memory-bound decode phases onto different hardware optimized for each workload. Disaggregation improves overall throughput by eliminating resource contention between the two phases.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prefix-attention",
      "term": "Prefix Attention",
      "definition": "An attention pattern used in prefix language models where a set of prefix tokens can attend to each other bidirectionally while subsequent tokens use causal attention. Combines encoder-like and decoder-like attention in a single model.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-prefix-caching",
      "term": "Prefix Caching",
      "definition": "An inference optimization that reuses the computed KV cache of shared prompt prefixes across multiple requests, avoiding redundant computation for system prompts or common instruction templates.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prefix-language-model",
      "term": "Prefix Language Model",
      "definition": "A language model architecture where a prefix portion of the input uses bidirectional attention while the remaining portion uses causal attention, combining understanding and generation capabilities.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prefix-tuning",
      "term": "Prefix Tuning",
      "definition": "A parameter-efficient fine-tuning method that prepends trainable vectors to inputs. Only these prefixes are updated during training, keeping the base model frozen.",
      "tags": [
        "Training",
        "Efficiency"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-presence-penalty",
      "term": "Presence Penalty",
      "definition": "A parameter that applies a fixed penalty to any token that has appeared at least once in the output, encouraging the model to introduce new topics and vocabulary.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-principal-component-analysis",
      "term": "Principal Component Analysis",
      "definition": "An unsupervised linear dimensionality reduction technique that projects data onto orthogonal axes (principal components) that maximize variance. The first components capture the most significant patterns in the data.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-principal-component-regression",
      "term": "Principal Component Regression",
      "definition": "A regression technique that first reduces the dimensionality of predictor variables using PCA and then regresses the response on the retained principal components, addressing multicollinearity and high-dimensionality.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prior-distribution",
      "term": "Prior Distribution",
      "definition": "In Bayesian statistics, the probability distribution representing beliefs about a parameter before observing data. It encodes prior knowledge or assumptions and is updated by the likelihood to form the posterior.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-prioritized-experience-replay",
      "term": "Prioritized Experience Replay",
      "definition": "An experience replay strategy that samples transitions with probability proportional to their TD error magnitude, allowing the agent to learn more frequently from surprising or informative experiences. Importance sampling weights correct for the non-uniform sampling.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-prioritized-level-replay",
      "term": "Prioritized Level Replay (PLR)",
      "definition": "An unsupervised environment design method that tracks learning progress on procedurally generated levels and replays those where the agent has the highest regret. PLR creates adaptive curricula that automatically target the frontier of the agent's capabilities.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-privacy-preserving",
      "term": "Privacy-Preserving AI",
      "definition": "Techniques to use AI without exposing sensitive data. Includes federated learning, differential privacy, and secure multi-party computation. Critical for healthcare and finance.",
      "tags": [
        "Privacy",
        "Ethics"
      ],
      "domain": "safety",
      "link": "ai-safety.html",
      "related": []
    },
    {
      "id": "term-pcfg",
      "term": "Probabilistic Context-Free Grammar",
      "definition": "A context-free grammar augmented with probabilities for each production rule, enabling statistical parsing by selecting the most probable parse tree for an input sentence.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-probing",
      "term": "Probing",
      "definition": "An interpretability technique that trains simple classifiers on the internal representations of a neural network to test what linguistic or semantic information is encoded. Reveals how different layers capture different levels of abstraction.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-probit-model",
      "term": "Probit Model",
      "definition": "A regression model for binary outcomes that uses the cumulative distribution function of the standard normal distribution as the link function, relating the linear predictor to the probability of the positive class.",
      "tags": [
        "Statistics",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-procedural-environment-generation",
      "term": "Procedural Environment Generation",
      "definition": "The automatic creation of diverse training environments through algorithmic variation of level layouts, object positions, and task parameters. Procedural generation improves generalization by exposing agents to a wide distribution of scenarios.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-process-reward-model",
      "term": "Process Reward Model",
      "definition": "A reward model that scores each intermediate reasoning step rather than only the final answer, enabling more fine-grained feedback for training models to perform multi-step reasoning.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-product-quantization",
      "term": "Product Quantization",
      "definition": "A vector compression technique that splits high-dimensional vectors into sub-vectors and quantizes each independently using a learned codebook, enabling dramatic memory reduction while supporting fast approximate distance computation via lookup tables.",
      "tags": [
        "Vector Database",
        "Quantization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-program-aided-language-model",
      "term": "Program-Aided Language Model",
      "definition": "A framework (PAL) that prompts a language model to generate executable program code as intermediate reasoning steps rather than natural language, offloading computation to a code interpreter for more accurate numerical and logical results.",
      "tags": [
        "Prompt Engineering",
        "Code-Augmented"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-progressive-training",
      "term": "Progressive Training",
      "definition": "A training strategy that gradually increases model or data complexity during training. Examples include progressive growing of GANs and curriculum learning. Helps stabilize training for complex models by building capabilities incrementally.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-projected-gradient-descent-attack",
      "term": "Projected Gradient Descent Attack",
      "definition": "An iterative adversarial attack that applies FGSM multiple times with small step sizes projecting back onto the epsilon-ball after each step. Produces stronger adversarial examples than single-step FGSM. A standard benchmark for adversarial robustness.",
      "tags": [
        "Algorithms",
        "Safety"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-prolog",
      "term": "Prolog",
      "definition": "A logic programming language created by Alain Colmerauer and Robert Kowalski in 1972, widely used in AI research for natural language processing, expert systems, and knowledge representation throughout the 1980s.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt",
      "term": "Prompt",
      "definition": "The text input you send to an AI assistant. Can include context, instructions, examples, and constraints. Prompt quality directly influences response quality.",
      "tags": [
        "Core Concept",
        "Fundamentals"
      ],
      "domain": "general",
      "link": "../learn/prompt-basics.html",
      "related": []
    },
    {
      "id": "term-prompt-caching",
      "term": "Prompt Caching",
      "definition": "An optimization technique that stores and reuses the computed key-value representations of common prompt prefixes, reducing redundant computation for repeated or similar queries.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-chaining",
      "term": "Prompt Chaining",
      "definition": "Breaking complex tasks into multiple sequential prompts, where each builds on the previous output. Enables sophisticated workflows and better results on multi-step problems.",
      "tags": [
        "Technique",
        "Advanced"
      ],
      "domain": "general",
      "link": "../learn/index.html",
      "related": []
    },
    {
      "id": "term-prompt-chaining-architecture",
      "term": "Prompt Chaining Architecture",
      "definition": "A system design pattern where multiple prompts are connected in a pipeline or directed graph, with each prompt handling a specific subtask and passing structured outputs to downstream prompts for further processing.",
      "tags": [
        "Prompt Engineering",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-compression",
      "term": "Prompt Compression",
      "definition": "Techniques that reduce the token length of prompts without losing essential information, using methods like selective context, summarization, or learned compression to fit more content within context limits.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-engineering",
      "term": "Prompt Engineering",
      "definition": "The practice of crafting effective prompts to get better results from AI systems. Includes techniques, frameworks (CRISP, COSTAR), and iterative refinement.",
      "tags": [
        "Skill",
        "Practice"
      ],
      "domain": "general",
      "link": "../learn/index.html",
      "related": []
    },
    {
      "id": "term-prompt-ensembling",
      "term": "Prompt Ensembling",
      "definition": "A strategy that runs multiple differently-phrased prompts for the same query and aggregates the outputs through voting, averaging, or selection to produce more robust and accurate final responses than any single prompt alone.",
      "tags": [
        "Prompt Engineering",
        "Ensemble"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-extraction-attack",
      "term": "Prompt Extraction Attack",
      "definition": "A targeted attack technique that attempts to reconstruct or extract a model's system prompt, proprietary instructions, or confidential context through systematic probing queries and analysis of model responses.",
      "tags": [
        "Prompt Engineering",
        "Security"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-injection",
      "term": "Prompt Injection",
      "definition": "A security vulnerability where malicious instructions hidden in content cause AI to behave unexpectedly. A significant concern for AI applications processing external data.",
      "tags": [
        "Security",
        "Risk"
      ],
      "domain": "safety",
      "link": "ai-safety.html",
      "related": []
    },
    {
      "id": "term-prompt-leaking",
      "term": "Prompt Leaking",
      "definition": "A security vulnerability where an attacker manipulates a language model into revealing its hidden system prompt or confidential instructions through carefully crafted queries that exploit the model's tendency to be helpful.",
      "tags": [
        "Prompt Engineering",
        "Security"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-optimization",
      "term": "Prompt Optimization",
      "definition": "The systematic process of refining prompt text, structure, and parameters to maximize model performance on a target task, employing techniques ranging from manual iteration to gradient-based or evolutionary search methods.",
      "tags": [
        "Prompt Engineering",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-robustness",
      "term": "Prompt Robustness",
      "definition": "The ability of a prompt to maintain consistent model performance across variations in input phrasing, perturbations, and edge cases, indicating how reliably the prompt produces correct outputs under diverse conditions.",
      "tags": [
        "Prompt Engineering",
        "Robustness"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-sensitivity",
      "term": "Prompt Sensitivity",
      "definition": "The degree to which a model's output quality and correctness varies in response to minor changes in prompt wording, formatting, or example ordering, representing a key challenge in achieving reliable and reproducible results.",
      "tags": [
        "Prompt Engineering",
        "Robustness"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-template",
      "term": "Prompt Template",
      "definition": "A reusable prompt structure with placeholders for variable content. Enables consistent, repeatable interactions and is essential for building AI-powered applications.",
      "tags": [
        "Pattern",
        "Reusable"
      ],
      "domain": "general",
      "link": "../patterns/index.html",
      "related": []
    },
    {
      "id": "term-prompt-templating",
      "term": "Prompt Templating",
      "definition": "The practice of creating reusable prompt structures with placeholder variables that can be dynamically filled with specific inputs at runtime, enabling consistent prompt formatting across multiple queries and use cases.",
      "tags": [
        "Prompt Engineering",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-tuning",
      "term": "Prompt Tuning",
      "definition": "A parameter-efficient method that prepends learnable continuous embeddings (soft prompts) to the input while keeping all model parameters frozen, enabling task adaptation with minimal overhead.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-prompt-versioning",
      "term": "Prompt Versioning",
      "definition": "The practice of maintaining version-controlled prompt templates with change tracking, performance baselines, and rollback capabilities, treating prompts as critical software artifacts that require systematic lifecycle management.",
      "tags": [
        "Prompt Engineering",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-pronoun-resolution",
      "term": "Pronoun Resolution",
      "definition": "The specific task of determining which entity a pronoun refers to in context, requiring understanding of gender, number, syntactic position, and semantic plausibility.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-propbank",
      "term": "PropBank",
      "definition": "Proposition Bank, a corpus annotated with predicate-argument structures for verbs, providing semantic role labels that facilitate training and evaluation of semantic role labeling systems.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-propensity-score",
      "term": "Propensity Score",
      "definition": "The probability that a unit is assigned to a particular treatment given its observed covariates. It is used in causal inference to balance treatment and control groups by matching, stratification, or inverse weighting.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-protected-attributes",
      "term": "Protected Attributes",
      "definition": "Characteristics such as race, gender, age, religion, and disability status that are legally or ethically designated as bases upon which differential treatment by AI systems is prohibited or restricted.",
      "tags": [
        "Fairness",
        "Regulation"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-proximal-gradient-method",
      "term": "Proximal Gradient Method",
      "definition": "An optimization algorithm for minimizing composite objective functions consisting of a smooth term and a non-smooth regularizer. Combines gradient descent on the smooth part with a proximal operator on the regularizer. Underlies algorithms like ISTA and FISTA.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-proxy-discrimination",
      "term": "Proxy Discrimination",
      "definition": "Discrimination that occurs when an AI system uses features that are correlated with protected attributes as proxies, achieving discriminatory outcomes even when protected attributes are explicitly excluded.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-pruning",
      "term": "Pruning",
      "definition": "Removing unnecessary weights or neurons from neural networks to reduce size and increase speed. Can dramatically decrease model size with minimal performance loss.",
      "tags": [
        "Optimization",
        "Efficiency"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pruning-at-initialization",
      "term": "Pruning-at-Initialization",
      "definition": "Techniques that identify and remove redundant weights before any training occurs, based on signal propagation or gradient flow analysis. Methods like SNIP and GraSP aim to find sparse architectures that train as well as dense networks.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-pseudo-labeling",
      "term": "Pseudo-Labeling",
      "definition": "A semi-supervised learning technique where a model trained on labeled data generates predictions for unlabeled data and uses high-confidence predictions as additional training labels. Simple yet effective when the initial model has reasonable accuracy.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-psnr",
      "term": "PSNR",
      "definition": "Peak Signal-to-Noise Ratio measures image quality by comparing the maximum possible pixel value to the mean squared error between original and reconstructed images. Expressed in decibels with higher values indicating better quality. Standard metric in image compression.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-pytorch",
      "term": "PyTorch",
      "definition": "A popular open-source deep learning framework from Meta, known for its flexibility and Pythonic design. The dominant framework for AI research and increasingly for production.",
      "tags": [
        "Framework",
        "Deep Learning"
      ],
      "domain": "models",
      "link": null,
      "related": []
    }
  ]
}