{
  "letter": "g",
  "count": 127,
  "terms": [
    {
      "id": "term-g-eval",
      "term": "G-Eval",
      "definition": "A framework that uses large language models with chain-of-thought prompting to evaluate natural language generation quality, achieving high correlation with human judgments by having the LLM score outputs on dimensions like coherence, fluency, and relevance.",
      "tags": [
        "Evaluation",
        "LLM-Based"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-galactica",
      "term": "Galactica",
      "definition": "A large language model trained by Meta AI on 48 million scientific papers citations and other academic content. Designed for scientific knowledge tasks including citation prediction and mathematical reasoning.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gamma-distribution",
      "term": "Gamma Distribution",
      "definition": "A two-parameter family of continuous probability distributions that generalizes the exponential distribution. It models the waiting time for a specified number of events and serves as a conjugate prior for the Poisson rate.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-gan",
      "term": "GAN (Generative Adversarial Network)",
      "definition": "A neural network architecture with two competing networks: a generator creating content and a discriminator evaluating it. Pioneered realistic image generation before diffusion models.",
      "tags": [
        "Architecture",
        "Generative"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gan-discriminator",
      "term": "GAN Discriminator",
      "definition": "The component of a generative adversarial network that learns to distinguish real data from generated samples, providing training signal to the generator through adversarial competition.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gan-generator",
      "term": "GAN Generator",
      "definition": "The component of a generative adversarial network that transforms random noise into synthetic data samples, trained to fool the discriminator into classifying its outputs as real.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gan-invention",
      "term": "GAN Invention",
      "definition": "The invention of Generative Adversarial Networks by Ian Goodfellow and colleagues in 2014. GANs train two neural networks (generator and discriminator) in competition enabling the generation of realistic synthetic data including images audio and text.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-gan-inversion",
      "term": "GAN Inversion",
      "definition": "The process of finding the latent code in a pre-trained GAN's latent space that best reconstructs a given real image, enabling GAN-based editing and manipulation of real photographs.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-gated-linear-unit",
      "term": "Gated Linear Unit",
      "definition": "An activation mechanism that multiplies a linear projection of the input by a sigmoid-gated linear projection, allowing the network to control information flow and commonly used in transformer feedforward layers.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gaussian-kernel",
      "term": "Gaussian Kernel",
      "definition": "A kernel function based on the Gaussian (normal) distribution, commonly used in kernel density estimation, smoothing, and SVMs. Its bandwidth parameter controls the width of the weighting function around each data point.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-gaussian-mixture-model",
      "term": "Gaussian Mixture Model",
      "definition": "A probabilistic model that represents data as generated from a mixture of a finite number of Gaussian distributions with unknown parameters, typically estimated via the EM algorithm. It supports soft cluster assignments.",
      "tags": [
        "Machine Learning",
        "Clustering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-gaussian-process",
      "term": "Gaussian Process",
      "definition": "A non-parametric Bayesian model that defines a probability distribution over functions, where any finite collection of function values follows a multivariate Gaussian distribution. It provides uncertainty estimates alongside predictions.",
      "tags": [
        "Machine Learning",
        "Bayesian Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-gaussian-splatting",
      "term": "Gaussian Splatting",
      "definition": "A 3D scene representation that models scenes as collections of 3D Gaussian primitives, enabling real-time rendering of photorealistic novel views through efficient rasterization rather than ray marching.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-gaze-estimation",
      "term": "Gaze Estimation",
      "definition": "The task of predicting where a person is looking based on their eye appearance and head pose in images or video, used in attention analysis, driver monitoring, and human-computer interaction.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-gazetteer",
      "term": "Gazetteer",
      "definition": "A curated list of entity names organized by type, such as person names, locations, or organizations, used as a feature or lookup resource in named entity recognition systems.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-gdpr-ai-provisions",
      "term": "GDPR AI Provisions",
      "definition": "Provisions within the EU General Data Protection Regulation relevant to AI, including the right not to be subject to purely automated decisions, data protection impact assessments, and requirements for transparency.",
      "tags": [
        "Governance",
        "Regulation"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-gelu",
      "term": "GELU (Gaussian Error Linear Unit)",
      "definition": "An activation function commonly used in transformers that applies a smooth, probabilistic non-linearity. Outperforms ReLU in many language models.",
      "tags": [
        "Architecture",
        "Function"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gemini",
      "term": "Gemini",
      "definition": "Google's family of multimodal AI models that can process text, images, audio, and video. Powers Google's AI features including Bard and Workspace integrations.",
      "tags": [
        "Model",
        "Google"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gemini-ai",
      "term": "Gemini (AI)",
      "definition": "A multimodal AI model family developed by Google DeepMind announced in December 2023. Gemini was designed from the ground up to be natively multimodal understanding and generating text code images audio and video.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-gemini-flash",
      "term": "Gemini Flash",
      "definition": "A lightweight model in the Gemini family optimized for speed and efficiency. Designed for high-volume low-latency applications where fast responses are prioritized while maintaining good quality.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gemini-launch",
      "term": "Gemini Launch",
      "definition": "Google DeepMind's release of Gemini in December 2023, a family of multimodal large language models designed to process text, images, audio, and video, positioned as Google's most capable AI model.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-gemini-pro",
      "term": "Gemini Pro",
      "definition": "A mid-tier model in Google's Gemini family designed for a wide range of tasks. Powers many Google AI products and is available through the Gemini API. Balances capability and efficiency.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gemma",
      "term": "Gemma",
      "definition": "A family of lightweight open models built by Google DeepMind from the same technology used to create Gemini. Available in 2B and 7B parameter sizes optimized for responsible AI development.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gen-2",
      "term": "Gen-2",
      "definition": "A multimodal AI model by Runway for generating and editing video from text images or existing video. One of the first commercially available AI video generation tools. Supports text-to-video image-to-video and video-to-video tasks.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-general-problem-solver",
      "term": "General Problem Solver",
      "definition": "An AI program developed by Newell and Simon in 1957 that used means-ends analysis to solve a wide range of formalized problems, representing an early attempt at creating a general-purpose reasoning system.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-generalization",
      "term": "Generalization",
      "definition": "A model's ability to perform well on new, unseen data rather than just memorizing training examples. The fundamental goal of machine learning.",
      "tags": [
        "Concept",
        "Quality"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-generalized-advantage-estimation",
      "term": "Generalized Advantage Estimation (GAE)",
      "definition": "A technique that computes advantage function estimates using an exponentially-weighted average of multi-step TD errors, controlled by a lambda parameter. GAE provides a smooth tradeoff between bias and variance in advantage estimation.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-generalized-linear-model",
      "term": "Generalized Linear Model",
      "definition": "A flexible generalization of ordinary linear regression that allows the response variable to follow any distribution from the exponential family, using a link function to relate the linear predictor to the mean of the distribution.",
      "tags": [
        "Statistics",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-generated-knowledge-prompting",
      "term": "Generated Knowledge Prompting",
      "definition": "A technique where the model first generates relevant knowledge or facts about a topic, then uses that self-generated knowledge as additional context to answer a downstream question more accurately.",
      "tags": [
        "Prompt Engineering",
        "Knowledge Augmentation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-generation",
      "term": "Generation",
      "definition": "The process of producing new content from an AI model. Text generation works by predicting one token at a time; image generation uses diffusion or similar processes.",
      "tags": [
        "Process",
        "Core Concept"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-gail",
      "term": "Generative Adversarial Imitation Learning (GAIL)",
      "definition": "An imitation learning algorithm that uses a GAN-like framework where a discriminator distinguishes between agent and expert trajectories while the policy learns to fool the discriminator. GAIL avoids explicit reward function recovery.",
      "tags": [
        "Reinforcement Learning",
        "Imitation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-gan-history",
      "term": "Generative Adversarial Network History",
      "definition": "The development of GANs from Ian Goodfellow's 2014 invention through progressive improvements including DCGAN, StyleGAN, and BigGAN, which dominated image generation before being supplanted by diffusion models.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-generative-ai",
      "term": "Generative AI",
      "definition": "AI systems that can create new content (text, images, code, music, video) rather than just analyzing existing data. Includes chatbots, image generators, and coding assistants.",
      "tags": [
        "Field",
        "Category"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-generative-ai-era",
      "term": "Generative AI Era",
      "definition": "The period beginning approximately in 2022 with the release of ChatGPT DALL-E 2 and Stable Diffusion when generative AI models capable of creating text images code and other content became widely accessible to the public transforming creative industries and knowledge work.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-generative-question-answering",
      "term": "Generative Question Answering",
      "definition": "A QA approach where the model generates a free-form answer in natural language rather than extracting a span, enabling responses that synthesize information or require reasoning.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-genetic-algorithm",
      "term": "Genetic Algorithm",
      "definition": "An evolutionary optimization algorithm inspired by natural selection. Maintains a population of candidate solutions that undergo selection crossover and mutation over generations. Effective for black-box optimization and combinatorial problems.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-genetic-algorithms",
      "term": "Genetic Algorithms",
      "definition": "Optimization algorithms inspired by natural selection, developed by John Holland in the 1960s-1970s, that evolve candidate solutions through selection, crossover, and mutation operators to find optimal or near-optimal solutions.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-geoffrey-hinton",
      "term": "Geoffrey Hinton",
      "definition": "British-Canadian computer scientist known as a godfather of deep learning, who co-developed backpropagation, Boltzmann machines, and deep belief networks. He won the 2024 Nobel Prize in Physics for foundational work on neural networks.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-geometric-augmentation",
      "term": "Geometric Augmentation",
      "definition": "Image augmentation techniques that modify the spatial arrangement of pixels including rotation, translation, scaling, shearing, flipping, and perspective transformations to improve model invariance.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-georgetown-ibm-experiment",
      "term": "Georgetown-IBM Experiment",
      "definition": "A 1954 demonstration of automatic translation of Russian sentences into English using an IBM 701 computer. Though limited to 250 words and six grammar rules the experiment generated optimism about machine translation and increased government funding for NLP research.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-gguf",
      "term": "GGUF",
      "definition": "A binary file format designed for storing quantized language models optimized for CPU and hybrid CPU/GPU inference, commonly used with the llama.cpp ecosystem.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gh200",
      "term": "GH200 Grace Hopper Superchip",
      "definition": "NVIDIA's integrated CPU-GPU superchip combining a Grace ARM CPU with a Hopper H200 GPU connected by a high-bandwidth NVLink-C2C interconnect. The GH200 provides unified memory addressing and eliminates PCIe bottlenecks for AI workloads.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-ghost-work",
      "term": "Ghost Work",
      "definition": "The often invisible human labor that powers AI systems, including data labeling, content moderation, and quality assurance, frequently performed by low-paid workers in developing countries under precarious conditions.",
      "tags": [
        "AI Ethics",
        "Fairness"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-gibbs-sampling",
      "term": "Gibbs Sampling",
      "definition": "An MCMC method that samples each variable in turn from its conditional distribution given the current values of all other variables. It is efficient when conditional distributions are easy to sample from.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-gini-impurity",
      "term": "Gini Impurity",
      "definition": "A measure of the probability that a randomly chosen sample would be misclassified if labeled according to the class distribution at a node. It is commonly used as a splitting criterion in decision tree algorithms.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-github-copilot",
      "term": "GitHub Copilot",
      "definition": "An AI pair programmer integrated into code editors. Uses LLMs to suggest code completions, write functions, and explain code. One of the most successful AI developer tools.",
      "tags": [
        "Product",
        "Development"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-github-copilot-launch",
      "term": "GitHub Copilot Launch",
      "definition": "The public release of GitHub Copilot in June 2022 an AI pair programming tool powered by OpenAI Codex. Copilot generates code suggestions in real-time within code editors marking a milestone in AI-assisted software development.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-global-average-pooling",
      "term": "Global Average Pooling",
      "definition": "A pooling operation that computes the mean of each feature map across all spatial dimensions, reducing the feature map to a single value per channel and often replacing fully connected layers.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-glove",
      "term": "GloVe",
      "definition": "Global Vectors for Word Representation, a word embedding method that trains on aggregated word co-occurrence statistics from a corpus, combining global matrix factorization with local context window methods.",
      "tags": [
        "NLP",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-glu",
      "term": "GLU",
      "definition": "Gated Linear Unit that splits input into two halves and applies a sigmoid gate to one half then multiplies element-wise. Introduced for language modeling by Dauphin et al. in 2017. Enables the network to control information flow through learned gating.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-glue-benchmark",
      "term": "GLUE Benchmark",
      "definition": "The General Language Understanding Evaluation benchmark introduced in 2018 consisting of nine natural language understanding tasks. GLUE provided a standardized way to evaluate language models and was quickly surpassed by models like BERT leading to the harder SuperGLUE benchmark.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-goal-conditioned-rl",
      "term": "Goal-Conditioned RL",
      "definition": "An RL formulation where the agent's policy and value function are conditioned on a goal specifying what the agent should achieve. Goal-conditioned policies enable multi-task learning and generalization across different objectives.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-goedel-escher-bach",
      "term": "Goedel Escher Bach",
      "definition": "A 1979 book by Douglas Hofstadter exploring common themes in the works of mathematician Kurt Goedel artist M.C. Escher and composer Johann Sebastian Bach. The book examines self-reference recursion and the nature of intelligence winning the Pulitzer Prize.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-goedels-incompleteness-theorems",
      "term": "Goedel's Incompleteness Theorems",
      "definition": "Two theorems published by Kurt Goedel in 1931 showing that any consistent formal system capable of expressing basic arithmetic contains statements that are true but unprovable within the system. These results have philosophical implications for the limits of AI reasoning.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-gofai-good-old-fashioned-ai",
      "term": "GOFAI (Good Old-Fashioned AI)",
      "definition": "A term coined by philosopher John Haugeland in 1985 referring to classical symbolic AI approaches based on physical symbol systems. GOFAI relies on explicit knowledge representation logical reasoning and search as opposed to the statistical learning methods that later became dominant.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-goodharts-law-in-ai",
      "term": "Goodhart's Law in AI",
      "definition": "The principle that when a proxy measure becomes the target for optimization, it ceases to be a good measure. In AI, this manifests when models over-optimize a proxy reward, diverging from the true intended goal.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-google-brain",
      "term": "Google Brain",
      "definition": "A deep learning research project founded at Google in 2011 by Andrew Ng and Jeff Dean that demonstrated unsupervised learning on YouTube videos and became a major center for neural network research before merging into Google DeepMind in 2023.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-google-brain-founded",
      "term": "Google Brain Founded",
      "definition": "The founding of Google Brain as a deep learning research project within Google in 2011 by Andrew Ng and Jeff Dean. The team achieved a breakthrough when their neural network autonomously learned to recognize cats in YouTube videos demonstrating unsupervised feature learning at scale.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-google-deepmind",
      "term": "Google DeepMind",
      "definition": "An AI research laboratory formed in April 2023 by merging Google Brain and DeepMind, created from the original DeepMind Technologies founded by Demis Hassabis, Shane Legg, and Mustafa Suleyman in 2010.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-tpu-v5",
      "term": "Google TPU v5",
      "definition": "The fifth generation of Google's Tensor Processing Unit featuring improved matrix multiply units, increased memory bandwidth, and better inter-chip interconnect. TPU v5 pods scale to thousands of chips for training the largest foundation models.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-google-translate-neural-mt",
      "term": "Google Translate Neural MT",
      "definition": "Google's 2016 transition from statistical to neural machine translation using sequence-to-sequence models with attention, dramatically improving translation quality and bringing neural networks to a product used by billions.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-googlenet",
      "term": "GoogLeNet",
      "definition": "The winner of the 2014 ImageNet competition also known as Inception v1. Introduced the inception module that processes input through multiple filter sizes in parallel. Achieved state-of-the-art accuracy with significantly fewer parameters than VGG.",
      "tags": [
        "Models",
        "History"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gopher",
      "term": "Gopher",
      "definition": "A 280 billion parameter language model by DeepMind that advanced understanding of language model scaling. Provided comprehensive analysis of model behavior across 152 diverse tasks.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpqa",
      "term": "GPQA",
      "definition": "Graduate-Level Google-Proof Question Answering, an extremely difficult benchmark of expert-crafted questions across biology, physics, and chemistry that even domain experts struggle with, designed to evaluate advanced reasoning beyond what search engines can resolve.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpt",
      "term": "GPT (Generative Pre-trained Transformer)",
      "definition": "OpenAI's series of large language models. GPT-4 is the latest major version, known for strong reasoning, multimodal capabilities, and broad knowledge.",
      "tags": [
        "Model",
        "OpenAI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpt-scaling-laws",
      "term": "GPT Scaling Laws",
      "definition": "Empirical power-law relationships discovered by Kaplan et al. at OpenAI in 2020 showing that language model performance improves predictably with increases in model size, dataset size, and training compute.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpt-1",
      "term": "GPT-1",
      "definition": "The first Generative Pre-trained Transformer model released by OpenAI in June 2018, demonstrating that unsupervised pre-training on large text corpora followed by task-specific fine-tuning could achieve strong NLP performance.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpt-2",
      "term": "GPT-2",
      "definition": "A 1.5-billion parameter autoregressive language model by OpenAI that demonstrated strong text generation capabilities and was initially withheld from full release due to concerns about misuse.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpt-3",
      "term": "GPT-3",
      "definition": "A 175-billion parameter autoregressive transformer model by OpenAI that popularized few-shot and zero-shot learning through in-context prompting without fine-tuning.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpt-35",
      "term": "GPT-3.5",
      "definition": "An improved version of GPT-3 fine-tuned with reinforcement learning from human feedback. Powers the initial release of ChatGPT. Demonstrates significant improvements in following instructions and producing helpful safe responses.",
      "tags": [
        "Models",
        "Fundamentals"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpt-4",
      "term": "GPT-4",
      "definition": "OpenAI's multimodal large language model released in March 2023, capable of processing both text and images, demonstrating human-level performance on many professional and academic benchmarks.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpt-4o",
      "term": "GPT-4o",
      "definition": "A multimodal model from OpenAI that natively processes text audio and images. Features faster response times and improved capabilities across modalities compared to previous GPT-4 variants. Designed for real-time conversational applications.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpt-4v",
      "term": "GPT-4V",
      "definition": "GPT-4 with Vision extends GPT-4 with the ability to understand and reason about images. Accepts interleaved text and image inputs enabling visual question answering image analysis and multimodal reasoning tasks.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpt-j",
      "term": "GPT-J",
      "definition": "A 6-billion parameter open-source autoregressive language model created by EleutherAI, notable for being one of the first performant open-source alternatives to GPT-3.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpt-neox",
      "term": "GPT-NeoX",
      "definition": "A 20-billion parameter autoregressive language model by EleutherAI that uses rotary positional embeddings and parallel attention-feedforward computation for improved efficiency.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpt-score",
      "term": "GPT-Score",
      "definition": "An evaluation framework that leverages generative pre-trained models to score text quality by computing conditional generation probabilities, assessing how likely a model would generate the candidate text given a quality-indicating prompt template.",
      "tags": [
        "Evaluation",
        "LLM-Based"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gptq",
      "term": "GPTQ",
      "definition": "A post-training quantization method for large language models that uses approximate second-order information to compress weights to lower bit precision (typically 4-bit) with minimal accuracy loss.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpu",
      "term": "GPU (Graphics Processing Unit)",
      "definition": "Hardware originally designed for graphics that excels at the parallel computations needed for AI. NVIDIA GPUs are the dominant hardware for training and running large AI models.",
      "tags": [
        "Hardware",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpu-architecture",
      "term": "GPU Architecture for AI",
      "definition": "The parallel processing design of graphics processing units optimized for AI workloads, featuring thousands of cores organized in streaming multiprocessors with shared memory hierarchies. Modern GPU architectures include specialized tensor processing units alongside general-purpose cores.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpu-computing-revolution",
      "term": "GPU Computing Revolution",
      "definition": "The adoption of graphics processing units for general-purpose computing and machine learning beginning around 2007-2012. NVIDIA CUDA (2007) and cuDNN (2014) enabled massive parallelization of neural network training catalyzing the deep learning revolution.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpu-direct",
      "term": "GPU Direct",
      "definition": "NVIDIA's technology suite enabling direct data transfers between GPUs and network adapters or storage without staging through CPU memory. GPU Direct RDMA eliminates extra copy operations, reducing communication latency in distributed training.",
      "tags": [
        "Distributed Computing",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-gpu-memory-hierarchy",
      "term": "GPU Memory Hierarchy",
      "definition": "The layered memory system in GPUs consisting of registers, shared memory (SRAM), L2 cache, and global memory (HBM/GDDR). Understanding and optimizing data placement across this hierarchy is critical for AI workload performance.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-grace-hopper",
      "term": "Grace Hopper",
      "definition": "American computer scientist and United States Navy rear admiral who developed the first compiler (A-0 System) in 1952. Pioneered the concept of machine-independent programming languages and contributed to the development of COBOL. Popularized the term debugging.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-grad-cam",
      "term": "Grad-CAM",
      "definition": "Gradient-weighted Class Activation Mapping, a visualization method that uses gradients flowing into the final convolutional layer to produce a heatmap highlighting important regions for any CNN prediction.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-gradient",
      "term": "Gradient",
      "definition": "A vector indicating the direction and magnitude of change needed to reduce a model's error. The foundation of gradient descent optimization used in training neural networks.",
      "tags": [
        "Training",
        "Math"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-gradient-accumulation",
      "term": "Gradient Accumulation",
      "definition": "A technique that simulates larger batch sizes by accumulating gradients over multiple forward-backward passes before performing a parameter update. Gradient accumulation enables training with effective batch sizes larger than GPU memory allows.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gradient-boosting",
      "term": "Gradient Boosting",
      "definition": "An ensemble technique that builds models sequentially, with each new model trained to correct the residual errors of the combined ensemble so far, using gradient descent in function space to minimize a specified loss.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gradient-boosting-history",
      "term": "Gradient Boosting History",
      "definition": "The development of gradient boosting from the work of Jerome Friedman (2001) through XGBoost (Chen and Guestrin 2016) LightGBM (Ke et al. 2017) and CatBoost. Gradient boosted trees became dominant in structured data competitions and production ML systems.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-gradient-checkpointing",
      "term": "Gradient Checkpointing",
      "definition": "A memory optimization technique that trades computation for memory by only storing activations at selected checkpoints during the forward pass and recomputing intermediate activations during backpropagation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gradient-clipping",
      "term": "Gradient Clipping",
      "definition": "A technique that rescales or truncates gradients when their norm exceeds a specified threshold, preventing the exploding gradient problem that can destabilize training in deep networks and recurrent architectures.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-gradient-descent",
      "term": "Gradient Descent",
      "definition": "The optimization algorithm that trains neural networks by iteratively adjusting weights in the direction that reduces error. Variants include SGD, Adam, and AdaGrad.",
      "tags": [
        "Algorithm",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-gradient-surgery",
      "term": "Gradient Surgery",
      "definition": "A technique for multi-task learning that modifies conflicting gradients from different tasks to reduce interference. Projects conflicting gradients so they do not oppose each other enabling better optimization across all tasks simultaneously.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-gradient-synchronization",
      "term": "Gradient Synchronization",
      "definition": "The process of aggregating gradients across multiple GPUs or nodes in distributed training, typically via all-reduce. Synchronous methods wait for all workers while asynchronous methods allow stale gradients for faster iteration.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-grammar-constrained-decoding",
      "term": "Grammar-Constrained Decoding",
      "definition": "A decoding approach that restricts token generation to sequences valid under a formal grammar (such as BNF or regex), ensuring outputs always conform to specified structural formats.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-granger-causality",
      "term": "Granger Causality",
      "definition": "A statistical concept where a time series X is said to Granger-cause Y if past values of X provide statistically significant information about future values of Y beyond what past values of Y alone provide.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-graph-attention",
      "term": "Graph Attention",
      "definition": "An attention mechanism applied to graph-structured data that learns to weight the importance of different neighbor nodes. Introduced in Graph Attention Networks allowing nodes to attend to their neighbors with different importance weights.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-gat",
      "term": "Graph Attention Network",
      "definition": "A graph neural network that uses attention mechanisms to weight the importance of neighboring nodes' features during aggregation, learning to focus on the most relevant connections.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-graph-convolution",
      "term": "Graph Convolution",
      "definition": "An operation that extends convolution to graph-structured data by aggregating features from neighboring nodes. Spectral approaches use graph Fourier transforms while spatial approaches directly aggregate neighbor features. Core operation in GCNs.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-gcn",
      "term": "Graph Convolutional Network",
      "definition": "A neural network that operates on graph-structured data by aggregating features from neighboring nodes through learnable convolutional operations defined on the graph topology.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gin",
      "term": "Graph Isomorphism Network",
      "definition": "A graph neural network provably as powerful as the Weisfeiler-Lehman graph isomorphism test, using a sum aggregator and MLP update function to maximize discriminative power on graph structures.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-graph-neural-network",
      "term": "Graph Neural Network",
      "definition": "A class of neural networks designed to operate on graph-structured data. Learns node edge or graph-level representations by aggregating information from local neighborhoods. Applied to social networks molecules and knowledge graphs.",
      "tags": [
        "Models",
        "Fundamentals"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-graph-neural-network-history",
      "term": "Graph Neural Network History",
      "definition": "The development of neural networks for graph-structured data from early spectral approaches (Bruna et al. 2013) through Graph Convolutional Networks (Kipf and Welling 2017) to modern message-passing frameworks and their applications in chemistry and social networks.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-graph-rag",
      "term": "Graph RAG",
      "definition": "A retrieval-augmented generation approach that builds a knowledge graph from source documents and uses graph traversal to retrieve structured, interconnected context for more coherent multi-hop reasoning.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-graph-based-index",
      "term": "Graph-Based Index",
      "definition": "A vector index structure that organizes vectors as nodes in a proximity graph where edges connect similar vectors, enabling efficient nearest neighbor search by navigating the graph from entry points toward the query's neighborhood.",
      "tags": [
        "Vector Database",
        "Index Structure"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-graph-based-parsing",
      "term": "Graph-Based Parsing",
      "definition": "A parsing approach that scores all possible dependency edges simultaneously and finds the highest-scoring tree using algorithms like maximum spanning tree, typically more accurate but slower than transition-based methods.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-graphcore",
      "term": "Graphcore",
      "definition": "A semiconductor company that developed the Intelligence Processing Unit (IPU), featuring a massive number of independent processor cores with large distributed on-chip SRAM. Graphcore's bulk synchronous parallel programming model targets both training and inference workloads.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-graphsage",
      "term": "GraphSAGE",
      "definition": "A framework for inductive representation learning on graphs that samples and aggregates features from a node's local neighborhood, enabling generalization to unseen nodes without retraining.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-greedy-decoding",
      "term": "Greedy Decoding",
      "definition": "A deterministic text generation strategy that always selects the token with the highest probability at each step, producing the most likely sequence but often lacking diversity.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-grid-search",
      "term": "Grid Search",
      "definition": "A hyperparameter tuning method that exhaustively evaluates all combinations of specified parameter values, typically combined with cross-validation to select the combination yielding the best performance.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-griffin",
      "term": "Griffin",
      "definition": "A hybrid model by Google DeepMind that combines recurrent layers based on linear recurrences with local attention. Achieves strong performance with efficient inference for long sequences while using less compute than pure transformers.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-grok",
      "term": "Grok",
      "definition": "An AI assistant developed by xAI (Elon Musk's AI company). Integrated with X (Twitter) and known for real-time information access and less restrictive conversation style.",
      "tags": [
        "Product",
        "Model"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-groq",
      "term": "Groq",
      "definition": "An AI hardware company that developed the Language Processing Unit (LPU), a deterministic architecture using software-defined scheduling to achieve extremely low-latency LLM inference. Groq's architecture eliminates dynamic scheduling overhead for predictable, high-speed token generation.",
      "tags": [
        "Hardware",
        "Inference Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-ground-truth",
      "term": "Ground Truth",
      "definition": "The correct answer or label used to evaluate model predictions. Obtained through human annotation, measurement, or other authoritative sources.",
      "tags": [
        "Data",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-grounding",
      "term": "Grounding",
      "definition": "Connecting AI outputs to verified information sources to reduce hallucinations and increase accuracy. Often involves retrieval-augmented generation (RAG) or real-time search.",
      "tags": [
        "Technique",
        "Accuracy"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-grounding-dino",
      "term": "Grounding DINO",
      "definition": "An open-set object detection model that combines a DINO-based detector with grounded pre-training, enabling detection of arbitrary objects specified by text descriptions without category-specific training.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-group-fairness",
      "term": "Group Fairness",
      "definition": "Fairness criteria that require statistical parity of outcomes or error rates across demographic groups defined by protected attributes, including demographic parity, equalized odds, and predictive parity.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-group-normalization",
      "term": "Group Normalization",
      "definition": "A normalization method that divides channels into groups and normalizes within each group independently, providing stable performance regardless of batch size unlike batch normalization.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-group-relative-policy-optimization",
      "term": "Group Relative Policy Optimization",
      "definition": "A reinforcement learning algorithm for language model alignment that uses group-based normalization of advantages rather than a separate critic model. Simplifies the RLHF pipeline while maintaining alignment quality. Used in DeepSeek models.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-grouped-query-attention",
      "term": "Grouped Query Attention",
      "definition": "An attention mechanism that groups multiple query heads to share a single key-value head, interpolating between multi-head and multi-query attention to balance quality and inference speed.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gru",
      "term": "GRU",
      "definition": "Gated Recurrent Unit, a recurrent neural network variant that uses reset and update gates to control information flow, offering similar performance to LSTM with fewer parameters and simpler computation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gshard",
      "term": "GShard",
      "definition": "A framework for scaling giant models across thousands of devices using conditional computation. Uses top-2 gating in mixture-of-experts layers with auxiliary load balancing loss. Demonstrated 600 billion parameter translation models.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gsm8k",
      "term": "GSM8K",
      "definition": "Grade School Math 8K, a benchmark of 8,500 linguistically diverse grade-school-level math word problems requiring multi-step arithmetic reasoning, widely used to evaluate mathematical problem-solving capabilities of language models.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-guardrails",
      "term": "Guardrails",
      "definition": "Safety mechanisms that constrain AI behavior to prevent harmful outputs. Include content filters, output validators, and behavioral restrictions built into AI systems.",
      "tags": [
        "Safety",
        "Constraint"
      ],
      "domain": "safety",
      "link": "ai-safety.html",
      "related": []
    },
    {
      "id": "term-guided-generation",
      "term": "Guided Generation",
      "definition": "Techniques that constrain language model output to conform to a specified format (such as JSON schema or grammar rules) by masking invalid tokens during the decoding process.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-gumbel-max-trick",
      "term": "Gumbel-Max Trick",
      "definition": "A method for sampling from categorical distributions using Gumbel noise. Adds independent Gumbel noise to log-probabilities and takes the argmax. Enables efficient sampling and is the basis for the Gumbel-Softmax relaxation.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-gumbel-softmax",
      "term": "Gumbel-Softmax",
      "definition": "A continuous relaxation of categorical sampling that allows gradient-based optimization of discrete variables. Uses the Gumbel-Max trick with a temperature-controlled softmax to produce approximately one-hot vectors that are differentiable.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-gym-environment",
      "term": "Gym Environment",
      "definition": "An interface standard and collection of benchmark environments originally developed by OpenAI for RL research. The Gym API defines a common protocol for environment interaction including reset, step, and observation/action spaces.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    }
  ]
}