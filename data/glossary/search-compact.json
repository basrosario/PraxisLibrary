[{"id": "term-2-3-tree-algorithm", "t": "2-3 Tree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A balanced search tree where every internal node has either two or three children. All leaves are at the same depth...", "l": "_other", "k": ["2-3", "tree", "algorithm", "balanced", "search", "internal", "node", "children", "leaves", "depth", "guaranteeing", "log", "operations", "forming", "conceptual"]}, {"id": "term-25d-packaging", "t": "2.5D Packaging", "tg": ["Fabrication", "Packaging"], "d": "hardware", "x": "Chip packaging technique placing multiple dies side by side on a silicon interposer for high-bandwidth die-to-die...", "l": "_other", "k": ["packaging", "chip", "technique", "placing", "multiple", "dies", "side", "silicon", "interposer", "high-bandwidth", "die-to-die", "communication", "hbm", "memory", "stacks"]}, {"id": "term-20-newsgroups", "t": "20 Newsgroups", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A collection of approximately 20000 newsgroup documents partitioned across 20 different discussion groups. A classic...", "l": "_other", "k": ["newsgroups", "collection", "approximately", "newsgroup", "documents", "partitioned", "across", "different", "discussion", "groups", "classic", "benchmark", "text", "classification", "topic"]}, {"id": "term-2wikimultihopqa", "t": "2WikiMultiHopQA", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A multi-hop question answering dataset constructed from Wikipedia requiring reasoning across two documents. Provides...", "l": "_other", "k": ["2wikimultihopqa", "multi-hop", "question", "answering", "dataset", "constructed", "wikipedia", "requiring", "reasoning", "across", "documents", "provides", "structured", "evidence", "chains"]}, {"id": "term-3d-packaging", "t": "3D Packaging", "tg": ["Fabrication", "Packaging"], "d": "hardware", "x": "Chip packaging technology stacking multiple die layers vertically with through-silicon vias for connections. Enables...", "l": "_other", "k": ["packaging", "chip", "technology", "stacking", "multiple", "die", "layers", "vertically", "through-silicon", "vias", "connections", "enables", "higher", "density", "shorter"]}, {"id": "term-400g-ethernet", "t": "400G Ethernet", "tg": ["Networking", "Ethernet", "Standard"], "d": "hardware", "x": "Ethernet standard providing 400 gigabits per second bandwidth. Increasingly deployed in AI data centers as a...", "l": "_other", "k": ["400g", "ethernet", "standard", "providing", "gigabits", "per", "bandwidth", "increasingly", "deployed", "data", "centers", "cost-effective", "high-bandwidth", "networking", "option"]}, {"id": "term-800g-ethernet", "t": "800G Ethernet", "tg": ["Networking", "Ethernet", "Standard"], "d": "hardware", "x": "Next-generation Ethernet standard providing 800 gigabits per second bandwidth. Being developed to meet the growing...", "l": "_other", "k": ["800g", "ethernet", "next-generation", "standard", "providing", "gigabits", "per", "bandwidth", "developed", "meet", "growing", "demands", "large-scale", "training", "clusters"]}, {"id": "term-a-search", "t": "A* Search", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A graph search algorithm that finds the shortest path by combining the actual cost from the start node with a heuristic...", "l": "a", "k": ["search", "graph", "algorithm", "finds", "shortest", "path", "combining", "actual", "cost", "start", "node", "heuristic", "estimate", "remaining", "guarantees"]}, {"id": "term-a-star-search-algorithm", "t": "A* Search Algorithm", "tg": ["History", "Milestones"], "d": "history", "x": "A graph search algorithm developed by Peter Hart, Nils Nilsson, and Bertram Raphael at SRI International in 1968 that...", "l": "a", "k": ["search", "algorithm", "graph", "developed", "peter", "hart", "nils", "nilsson", "bertram", "raphael", "sri", "international", "finds", "shortest", "path"]}, {"id": "term-a-search-variant-for-graphs", "t": "A* Search Variant for Graphs", "tg": ["Algorithms", "Fundamentals", "Graph", "Searching"], "d": "algorithms", "x": "An extension of A* search to general graph problems that uses consistent heuristics to ensure optimality. Expands nodes...", "l": "a", "k": ["search", "variant", "graphs", "extension", "general", "graph", "problems", "uses", "consistent", "heuristics", "ensure", "optimality", "expands", "nodes", "order"]}, {"id": "term-a-okvqa", "t": "A-OKVQA", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "Augmented OK-VQA an expanded visual QA benchmark with rationales and multiple-choice answers requiring broad world...", "l": "a", "k": ["a-okvqa", "augmented", "ok-vqa", "expanded", "visual", "benchmark", "rationales", "multiple-choice", "answers", "requiring", "broad", "world", "knowledge", "commonsense", "reasoning"]}, {"id": "term-ab-testing", "t": "A/B Testing", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A randomized controlled experiment that compares two variants (A and B) to determine which performs better on a...", "l": "a", "k": ["testing", "randomized", "controlled", "experiment", "compares", "variants", "determine", "performs", "better", "specified", "metric", "widely", "product", "development", "data-driven"]}, {"id": "term-ab-testing-for-llms", "t": "A/B Testing for LLMs", "tg": ["Evaluation", "Methodology"], "d": "datasets", "x": "A comparative evaluation methodology where two language model variants or prompt configurations are deployed to...", "l": "a", "k": ["testing", "llms", "comparative", "evaluation", "methodology", "language", "model", "variants", "prompt", "configurations", "deployed", "different", "user", "segments", "statistical"]}, {"id": "term-aaai-organization", "t": "AAAI (Organization)", "tg": ["History", "Organizations"], "d": "history", "x": "The Association for the Advancement of Artificial Intelligence founded in 1979 (originally the American Association for...", "l": "a", "k": ["aaai", "organization", "association", "advancement", "artificial", "intelligence", "founded", "originally", "american", "nonprofit", "scientific", "society", "devoted", "advancing", "understanding"]}, {"id": "term-aaai-conference", "t": "AAAI Conference", "tg": ["History", "Conferences"], "d": "history", "x": "The Association for the Advancement of Artificial Intelligence conference held annually since 1980. One of the premier...", "l": "a", "k": ["aaai", "conference", "association", "advancement", "artificial", "intelligence", "held", "annually", "premier", "conferences", "covering", "areas", "research", "robotics", "natural"]}, {"id": "term-aaron", "t": "AARON", "tg": ["History", "Systems"], "d": "history", "x": "An art-generating program created by Harold Cohen beginning in 1973. One of the longest-running AI art projects AARON...", "l": "a", "k": ["aaron", "art-generating", "program", "created", "harold", "cohen", "beginning", "longest-running", "art", "projects", "evolved", "producing", "abstract", "drawings", "creating"]}, {"id": "term-abacus-ai-smaug", "t": "Abacus-AI Smaug", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A fine-tuned language model that uses DPO training on curated preference data to achieve strong benchmark performance...", "l": "a", "k": ["abacus-ai", "smaug", "fine-tuned", "language", "model", "uses", "dpo", "training", "curated", "preference", "data", "achieve", "strong", "benchmark", "performance"]}, {"id": "term-abc-dataset", "t": "ABC Dataset", "tg": ["Training Corpus", "3D"], "d": "datasets", "x": "A collection of one million Computer-Aided Design models for geometric deep learning research. Provides diverse 3D...", "l": "a", "k": ["abc", "dataset", "collection", "million", "computer-aided", "design", "models", "geometric", "deep", "learning", "research", "provides", "diverse", "shapes", "sharp"]}, {"id": "term-abductive-reasoning", "t": "Abductive Reasoning", "tg": ["History", "Fundamentals"], "d": "history", "x": "A form of logical inference that starts with an observation and seeks the simplest or most likely explanation. Used in...", "l": "a", "k": ["abductive", "reasoning", "form", "logical", "inference", "starts", "observation", "seeks", "simplest", "likely", "explanation", "diagnosis", "hypothesis", "generation", "plan"]}, {"id": "term-ablation-study", "t": "Ablation Study", "tg": ["Research", "Methodology"], "d": "general", "x": "A research technique that removes components of a model to understand their contribution. Helps researchers understand...", "l": "a", "k": ["ablation", "study", "research", "technique", "removes", "components", "model", "understand", "contribution", "helps", "researchers", "parts", "system", "responsible", "capabilities"]}, {"id": "term-absolute-position-encoding", "t": "Absolute Position Encoding", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A method of injecting position information into transformer inputs by adding a fixed or learned vector to each...", "l": "a", "k": ["absolute", "position", "encoding", "method", "injecting", "information", "transformer", "inputs", "adding", "fixed", "learned", "vector", "original", "sinusoidal", "functions"]}, {"id": "term-amr", "t": "Abstract Meaning Representation", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A semantic representation that encodes the meaning of a sentence as a rooted directed acyclic graph, abstracting away...", "l": "a", "k": ["abstract", "meaning", "representation", "semantic", "encodes", "sentence", "rooted", "directed", "acyclic", "graph", "abstracting", "away", "syntactic", "details", "capture"]}, {"id": "term-abstraction-attack", "t": "Abstraction Attack", "tg": ["Safety", "Technical"], "d": "safety", "x": "An adversarial technique that exploits the gap between a model's learned abstractions and real-world inputs by crafting...", "l": "a", "k": ["abstraction", "attack", "adversarial", "technique", "exploits", "gap", "model", "learned", "abstractions", "real-world", "inputs", "crafting", "examples", "match", "abstract"]}, {"id": "term-abstractive-summarization", "t": "Abstractive Summarization", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A summarization approach that generates novel sentences capturing the key information from the source text, potentially...", "l": "a", "k": ["abstractive", "summarization", "approach", "generates", "novel", "sentences", "capturing", "key", "information", "source", "text", "potentially", "words", "phrasings", "present"]}, {"id": "term-academic-prompting", "t": "Academic Prompting", "tg": ["Prompt Engineering", "Academic"], "d": "general", "x": "A prompting approach tailored for scholarly tasks that instructs the model to follow academic conventions including...", "l": "a", "k": ["academic", "prompting", "approach", "tailored", "scholarly", "tasks", "instructs", "model", "follow", "conventions", "including", "formal", "tone", "citation", "awareness"]}, {"id": "term-accelerated-computing", "t": "Accelerated Computing", "tg": ["Architecture", "Fundamentals"], "d": "hardware", "x": "Computing paradigm using specialized hardware accelerators alongside general-purpose CPUs to dramatically speed up...", "l": "a", "k": ["accelerated", "computing", "paradigm", "specialized", "hardware", "accelerators", "alongside", "general-purpose", "cpus", "dramatically", "speed", "specific", "workloads", "gpus", "tpus"]}, {"id": "term-access-control-for-ai-systems", "t": "Access Control for AI Systems", "tg": ["Safety", "Governance"], "d": "safety", "x": "Mechanisms and policies that restrict who can deploy query or modify AI systems. Includes authentication authorization...", "l": "a", "k": ["access", "control", "systems", "mechanisms", "policies", "restrict", "deploy", "query", "modify", "includes", "authentication", "authorization", "audit", "logging", "prevent"]}, {"id": "term-accountability-gap", "t": "Accountability Gap", "tg": ["Safety", "Governance"], "d": "safety", "x": "The problem that arises when no single party can be held responsible for harm caused by AI systems due to complex...", "l": "a", "k": ["accountability", "gap", "problem", "arises", "single", "party", "held", "responsible", "harm", "caused", "systems", "due", "complex", "supply", "chains"]}, {"id": "term-accountability-in-ai", "t": "Accountability in AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The principle that identifiable individuals or organizations should be answerable for the outcomes and impacts of AI...", "l": "a", "k": ["accountability", "principle", "identifiable", "individuals", "organizations", "answerable", "outcomes", "impacts", "systems", "including", "mechanisms", "redress", "causes", "harm"]}, {"id": "term-accumulated-local-effects", "t": "Accumulated Local Effects", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A model-agnostic method for visualizing feature effects that is unbiased in the presence of correlated features, unlike...", "l": "a", "k": ["accumulated", "local", "effects", "model-agnostic", "method", "visualizing", "feature", "unbiased", "presence", "correlated", "features", "unlike", "partial", "dependence", "plots"]}, {"id": "term-accuracy", "t": "Accuracy", "tg": ["Metrics", "Evaluation"], "d": "datasets", "x": "A metric measuring how often a model's predictions are correct. Calculated as the ratio of correct predictions to total...", "l": "a", "k": ["accuracy", "metric", "measuring", "model", "predictions", "correct", "calculated", "ratio", "total", "intuitive", "misleading", "imbalanced", "datasets"]}, {"id": "term-acegpt", "t": "AceGPT", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A large language model optimized for Arabic with culturally-aligned fine-tuning and localized knowledge for Middle...", "l": "a", "k": ["acegpt", "large", "language", "model", "optimized", "arabic", "culturally-aligned", "fine-tuning", "localized", "knowledge", "middle", "eastern", "north", "african", "contexts"]}, {"id": "term-acl-conference", "t": "ACL Conference", "tg": ["History", "Conferences"], "d": "history", "x": "The Annual Meeting of the Association for Computational Linguistics first held in 1963. The premier conference for...", "l": "a", "k": ["acl", "conference", "annual", "meeting", "association", "computational", "linguistics", "held", "premier", "natural", "language", "processing", "research", "foundational", "papers"]}, {"id": "term-acm-organization", "t": "ACM (Organization)", "tg": ["History", "Organizations"], "d": "history", "x": "The Association for Computing Machinery founded in 1947 as the world's largest educational and scientific computing...", "l": "a", "k": ["acm", "organization", "association", "computing", "machinery", "founded", "world", "largest", "educational", "scientific", "society", "administers", "turing", "award", "called"]}, {"id": "term-act", "t": "ACT", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "Action Chunking with Transformers is a robot learning framework that predicts chunks of future actions using a CVAE...", "l": "a", "k": ["act", "action", "chunking", "transformers", "robot", "learning", "framework", "predicts", "chunks", "future", "actions", "cvae", "transformer", "backbone", "bimanual"]}, {"id": "term-act-r", "t": "ACT-R", "tg": ["History", "Systems"], "d": "history", "x": "A cognitive architecture developed by John Robert Anderson at Carnegie Mellon University since 1993. ACT-R models human...", "l": "a", "k": ["act-r", "cognitive", "architecture", "developed", "john", "robert", "anderson", "carnegie", "mellon", "university", "models", "human", "cognition", "interaction", "declarative"]}, {"id": "term-action", "t": "Action", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A decision or move taken by an RL agent that affects the environment and transitions the system to a new state. Actions...", "l": "a", "k": ["action", "decision", "move", "taken", "agent", "affects", "environment", "transitions", "system", "state", "actions", "discrete", "finite", "choices", "continuous"]}, {"id": "term-action-masking", "t": "Action Masking", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A technique that restricts the set of available actions at each state by zeroing out invalid action probabilities...", "l": "a", "k": ["action", "masking", "technique", "restricts", "available", "actions", "state", "zeroing", "invalid", "probabilities", "policy", "sampling", "enforces", "domain", "constraints"]}, {"id": "term-action-recognition", "t": "Action Recognition", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A video understanding task that identifies and classifies human actions or activities in video sequences, using...", "l": "a", "k": ["action", "recognition", "video", "understanding", "task", "identifies", "classifies", "human", "actions", "activities", "sequences", "temporal", "modeling", "motion", "patterns"]}, {"id": "term-action-repeat", "t": "Action Repeat", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A technique where each selected action is executed for multiple consecutive environment steps, reducing the effective...", "l": "a", "k": ["action", "repeat", "technique", "selected", "executed", "multiple", "consecutive", "environment", "steps", "reducing", "effective", "decision", "frequency", "simplifies", "control"]}, {"id": "term-action-space", "t": "Action Space", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The set of all possible actions available to an RL agent, defined as discrete (finite choices), continuous (real-valued...", "l": "a", "k": ["action", "space", "possible", "actions", "available", "agent", "defined", "discrete", "finite", "choices", "continuous", "real-valued", "vectors", "multi-discrete", "structure"]}, {"id": "term-activation-checkpointing", "t": "Activation Checkpointing", "tg": ["LLM", "Inference"], "d": "models", "x": "A memory optimization technique that trades compute for memory by discarding intermediate activations during the...", "l": "a", "k": ["activation", "checkpointing", "memory", "optimization", "technique", "trades", "compute", "discarding", "intermediate", "activations", "forward", "pass", "recomputing", "backpropagation"]}, {"id": "term-activation-function", "t": "Activation Function", "tg": ["Neural Networks", "Technical"], "d": "models", "x": "A mathematical function applied to neurons in neural networks that introduces non-linearity, enabling the network to...", "l": "a", "k": ["activation", "function", "mathematical", "applied", "neurons", "neural", "networks", "introduces", "non-linearity", "enabling", "network", "learn", "complex", "patterns", "common"]}, {"id": "term-activation-patching", "t": "Activation Patching", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An interpretability technique that replaces activations at specific positions and layers with activations from a...", "l": "a", "k": ["activation", "patching", "interpretability", "technique", "replaces", "activations", "specific", "positions", "layers", "different", "input", "determine", "causal", "effects", "identifies"]}, {"id": "term-activation-quantization", "t": "Activation Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "The process of quantizing intermediate activations (not just weights) during inference to reduce memory bandwidth...", "l": "a", "k": ["activation", "quantization", "process", "quantizing", "intermediate", "activations", "weights", "inference", "reduce", "memory", "bandwidth", "requirements", "enable", "int8", "lower-precision"]}, {"id": "term-active-contour-model", "t": "Active Contour Model", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A deformable spline that moves through the image domain under the influence of internal forces and external image...", "l": "a", "k": ["active", "contour", "model", "deformable", "spline", "moves", "image", "domain", "influence", "internal", "forces", "external", "known", "snakes", "boundary"]}, {"id": "term-active-inference-safety", "t": "Active Inference Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "Research into making active inference agents safe by ensuring their generative models and prior preferences are aligned...", "l": "a", "k": ["active", "inference", "safety", "research", "making", "agents", "safe", "ensuring", "generative", "models", "prior", "preferences", "aligned", "human", "values"]}, {"id": "term-active-learning", "t": "Active Learning", "tg": ["Training", "Technique"], "d": "general", "x": "A training approach where the model identifies which unlabeled examples would be most valuable to learn from. Reduces...", "l": "a", "k": ["active", "learning", "training", "approach", "model", "identifies", "unlabeled", "examples", "valuable", "learn", "reduces", "labeling", "costs", "focusing", "human"]}, {"id": "term-active-learning-algorithm", "t": "Active Learning Algorithm", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A machine learning approach where the model selectively queries an oracle for labels on the most informative unlabeled...", "l": "a", "k": ["active", "learning", "algorithm", "machine", "approach", "model", "selectively", "queries", "oracle", "labels", "informative", "unlabeled", "examples", "reduces", "labeling"]}, {"id": "term-active-prompting", "t": "Active Prompting", "tg": ["Prompt Engineering", "Active Learning"], "d": "general", "x": "A method that identifies the most uncertain or informative questions for chain-of-thought annotation by measuring model...", "l": "a", "k": ["active", "prompting", "method", "identifies", "uncertain", "informative", "questions", "chain-of-thought", "annotation", "measuring", "model", "disagreement", "across", "sampled", "outputs"]}, {"id": "term-active-set-method", "t": "Active Set Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An algorithm for solving quadratic programming problems that maintains a working set of active constraints. Iteratively...", "l": "a", "k": ["active", "method", "algorithm", "solving", "quadratic", "programming", "problems", "maintains", "working", "constraints", "iteratively", "adds", "removes", "reduced", "equality-constrained"]}, {"id": "term-activitynet", "t": "ActivityNet", "tg": ["Benchmark", "Video"], "d": "datasets", "x": "A large-scale video benchmark for human activity understanding containing 20000 YouTube videos across 200 activity...", "l": "a", "k": ["activitynet", "large-scale", "video", "benchmark", "human", "activity", "understanding", "containing", "youtube", "videos", "across", "classes", "temporal", "annotations"]}, {"id": "term-actor-critic", "t": "Actor-Critic", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An RL architecture combining a policy network (actor) that selects actions with a value network (critic) that evaluates...", "l": "a", "k": ["actor-critic", "architecture", "combining", "policy", "network", "actor", "selects", "actions", "value", "critic", "evaluates", "estimates", "reduce", "variance", "gradient"]}, {"id": "term-ada-lovelace", "t": "Ada Lovelace", "tg": ["History", "Pioneers"], "d": "history", "x": "British mathematician (1815-1852) who wrote the first published algorithm intended for a machine, working with Charles...", "l": "a", "k": ["ada", "lovelace", "british", "mathematician", "1815-1852", "wrote", "published", "algorithm", "intended", "machine", "working", "charles", "babbage", "analytical", "engine"]}, {"id": "term-adaboost", "t": "AdaBoost", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An ensemble method that trains weak learners sequentially, assigning higher weights to misclassified samples so that...", "l": "a", "k": ["adaboost", "ensemble", "method", "trains", "weak", "learners", "sequentially", "assigning", "higher", "weights", "misclassified", "samples", "subsequent", "focus", "hardest"]}, {"id": "term-adaboost-model", "t": "AdaBoost Model", "tg": ["Models", "Fundamentals", "History"], "d": "models", "x": "An adaptive boosting algorithm that combines weak classifiers by iteratively re-weighting misclassified training...", "l": "a", "k": ["adaboost", "model", "adaptive", "boosting", "algorithm", "combines", "weak", "classifiers", "iteratively", "re-weighting", "misclassified", "training", "samples", "improve", "overall"]}, {"id": "term-adadelta", "t": "AdaDelta", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An adaptive learning rate optimizer that eliminates the need for a manually set global learning rate. Adapts based on a...", "l": "a", "k": ["adadelta", "adaptive", "learning", "rate", "optimizer", "eliminates", "need", "manually", "global", "adapts", "based", "moving", "window", "gradient", "updates"]}, {"id": "term-adafactor", "t": "Adafactor", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A memory-efficient adaptive optimizer that factorizes the second moment accumulator into row and column factors....", "l": "a", "k": ["adafactor", "memory-efficient", "adaptive", "optimizer", "factorizes", "moment", "accumulator", "row", "column", "factors", "reduces", "memory", "usage", "weight", "matrices"]}, {"id": "term-adagrad", "t": "AdaGrad", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "An optimization algorithm that adapts the learning rate for each parameter individually by dividing by the square root...", "l": "a", "k": ["adagrad", "optimization", "algorithm", "adapts", "learning", "rate", "parameter", "individually", "dividing", "square", "root", "sum", "historical", "squared", "gradients"]}, {"id": "term-adam", "t": "Adam Optimizer", "tg": ["Training", "Algorithm"], "d": "algorithms", "x": "A popular optimization algorithm combining momentum with adaptive learning rates. The default choice for training many...", "l": "a", "k": ["adam", "optimizer", "popular", "optimization", "algorithm", "combining", "momentum", "adaptive", "learning", "rates", "default", "choice", "training", "neural", "networks"]}, {"id": "term-adams-bashforth-method", "t": "Adams-Bashforth Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A family of explicit linear multistep methods for solving ordinary differential equations that uses information from...", "l": "a", "k": ["adams-bashforth", "method", "family", "explicit", "linear", "multistep", "methods", "solving", "ordinary", "differential", "equations", "uses", "information", "several", "previous"]}, {"id": "term-adamw", "t": "AdamW", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A variant of the Adam optimizer that decouples weight decay from the gradient update. Proposed by Loshchilov and Hutter...", "l": "a", "k": ["adamw", "variant", "adam", "optimizer", "decouples", "weight", "decay", "gradient", "update", "proposed", "loshchilov", "hutter", "shown", "provide", "better"]}, {"id": "term-adapter-layer", "t": "Adapter Layer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A small trainable module inserted between frozen pretrained layers that learns task-specific transformations with...", "l": "a", "k": ["adapter", "layer", "small", "trainable", "module", "inserted", "frozen", "pretrained", "layers", "learns", "task-specific", "transformations", "minimal", "additional", "parameters"]}, {"id": "term-adapter-layers", "t": "Adapter Layers", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Small trainable modules inserted between the layers of a pretrained model for parameter-efficient fine-tuning....", "l": "a", "k": ["adapter", "layers", "small", "trainable", "modules", "inserted", "pretrained", "model", "parameter-efficient", "fine-tuning", "typically", "consist", "down-projection", "nonlinearity", "up-projection"]}, {"id": "term-adaptive-filter-algorithm", "t": "Adaptive Filter Algorithm", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A filter that automatically adjusts its parameters to minimize an error criterion. The Least Mean Squares (LMS)...", "l": "a", "k": ["adaptive", "filter", "algorithm", "automatically", "adjusts", "parameters", "minimize", "error", "criterion", "least", "mean", "squares", "lms", "simplest", "updating"]}, {"id": "term-adaptive-pooling", "t": "Adaptive Pooling", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A pooling operation that automatically adjusts its kernel size and stride to produce a specified output size regardless...", "l": "a", "k": ["adaptive", "pooling", "operation", "automatically", "adjusts", "kernel", "size", "stride", "produce", "specified", "output", "regardless", "input", "dimensions", "commonly"]}, {"id": "term-adaptive-quadrature", "t": "Adaptive Quadrature", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A numerical integration method that automatically adjusts the step size based on local error estimates. Refines the...", "l": "a", "k": ["adaptive", "quadrature", "numerical", "integration", "method", "automatically", "adjusts", "step", "size", "based", "local", "error", "estimates", "refines", "partition"]}, {"id": "term-adaptive-stress-testing", "t": "Adaptive Stress Testing", "tg": ["Safety", "Technical"], "d": "safety", "x": "A technique that uses reinforcement learning to find the most likely failure scenarios for autonomous systems....", "l": "a", "k": ["adaptive", "stress", "testing", "technique", "uses", "reinforcement", "learning", "find", "likely", "failure", "scenarios", "autonomous", "systems", "developed", "stanford"]}, {"id": "term-addictive-design-in-ai", "t": "Addictive Design in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The use of AI-driven personalization and engagement optimization techniques that exploit psychological vulnerabilities...", "l": "a", "k": ["addictive", "design", "ai-driven", "personalization", "engagement", "optimization", "techniques", "exploit", "psychological", "vulnerabilities", "maximize", "user", "screen", "time", "spending"]}, {"id": "term-additive-attention", "t": "Additive Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that computes compatibility scores by passing the concatenation of query and key through a...", "l": "a", "k": ["additive", "attention", "mechanism", "computes", "compatibility", "scores", "passing", "concatenation", "query", "key", "feedforward", "layer", "known", "bahdanau", "early"]}, {"id": "term-ade20k", "t": "ADE20K", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A scene parsing dataset with dense pixel-level annotations for 150 semantic categories across 25000 images. Used as the...", "l": "a", "k": ["ade20k", "scene", "parsing", "dataset", "dense", "pixel-level", "annotations", "semantic", "categories", "across", "images", "primary", "benchmark", "mit", "challenge"]}, {"id": "term-adjusted-rand-index", "t": "Adjusted Rand Index", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A clustering evaluation metric that measures agreement between two clusterings adjusted for chance. Ranges from -1 to 1...", "l": "a", "k": ["adjusted", "rand", "index", "clustering", "evaluation", "metric", "measures", "agreement", "clusterings", "chance", "ranges", "indicating", "perfect", "random", "corrects"]}, {"id": "term-admm-algorithm", "t": "ADMM Algorithm", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "The Alternating Direction Method of Multipliers decomposes optimization problems into smaller subproblems that are...", "l": "a", "k": ["admm", "algorithm", "alternating", "direction", "method", "multipliers", "decomposes", "optimization", "problems", "smaller", "subproblems", "easier", "solve", "combines", "dual"]}, {"id": "term-advanced-packaging", "t": "Advanced Packaging", "tg": ["Packaging", "Manufacturing", "Technology"], "d": "hardware", "x": "Umbrella term for chip packaging technologies beyond traditional wire bonding including 2.5D 3D fan-out and chiplet...", "l": "a", "k": ["advanced", "packaging", "umbrella", "term", "chip", "technologies", "beyond", "traditional", "wire", "bonding", "including", "fan-out", "chiplet", "integration", "critical"]}, {"id": "term-a2c", "t": "Advantage Actor-Critic (A2C)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A synchronous variant of the actor-critic method that uses the advantage function (difference between action value and...", "l": "a", "k": ["advantage", "actor-critic", "a2c", "synchronous", "variant", "method", "uses", "function", "difference", "action", "value", "state", "reduce", "variance", "policy"]}, {"id": "term-advantage-function", "t": "Advantage Function", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "The difference A(s,a) = Q(s,a) - V(s) between the action-value and state-value functions, measuring how much better an...", "l": "a", "k": ["advantage", "function", "difference", "action-value", "state-value", "functions", "measuring", "better", "action", "compared", "average", "current", "policy", "reduces", "variance"]}, {"id": "term-advantage-weighted-actor-critic", "t": "Advantage-Weighted Actor-Critic", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A reinforcement learning algorithm that uses advantage-weighted policy improvement to avoid the instabilities of policy...", "l": "a", "k": ["advantage-weighted", "actor-critic", "reinforcement", "learning", "algorithm", "uses", "policy", "improvement", "avoid", "instabilities", "gradient", "methods", "converts", "weighted", "supervised"]}, {"id": "term-advantage-weighted-regression", "t": "Advantage-Weighted Regression (AWR)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An offline RL algorithm that learns a policy by performing weighted maximum likelihood on a dataset, where the weights...", "l": "a", "k": ["advantage-weighted", "regression", "awr", "offline", "algorithm", "learns", "policy", "performing", "weighted", "maximum", "likelihood", "dataset", "weights", "exponentiated", "advantages"]}, {"id": "term-advbench", "t": "AdvBench", "tg": ["Benchmark", "NLP", "Safety"], "d": "datasets", "x": "A benchmark for evaluating adversarial attacks on large language models containing harmful instructions and jailbreak...", "l": "a", "k": ["advbench", "benchmark", "evaluating", "adversarial", "attacks", "large", "language", "models", "containing", "harmful", "instructions", "jailbreak", "prompts", "testing", "llm"]}, {"id": "term-adversarial-attack", "t": "Adversarial Attack", "tg": ["Security", "Safety"], "d": "safety", "x": "Deliberate attempts to deceive AI systems by providing specially crafted inputs. These can cause models to make...", "l": "a", "k": ["adversarial", "attack", "deliberate", "attempts", "deceive", "systems", "providing", "specially", "crafted", "inputs", "cause", "models", "incorrect", "predictions", "generate"]}, {"id": "term-adversarial-example-cv", "t": "Adversarial Example", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "An input image with carefully crafted, often imperceptible perturbations that cause a vision model to make incorrect...", "l": "a", "k": ["adversarial", "example", "input", "image", "carefully", "crafted", "imperceptible", "perturbations", "cause", "vision", "model", "incorrect", "predictions", "high", "confidence"]}, {"id": "term-adversarial-glue", "t": "Adversarial GLUE", "tg": ["Benchmark", "NLP", "Safety"], "d": "datasets", "x": "An adversarial version of the GLUE benchmark where examples are modified to fool models while preserving human...", "l": "a", "k": ["adversarial", "glue", "version", "benchmark", "examples", "modified", "fool", "models", "preserving", "human", "comprehension", "tests", "robustness", "nlu", "systems"]}, {"id": "term-adversarial-machine-learning", "t": "Adversarial Machine Learning", "tg": ["Safety", "Technical"], "d": "safety", "x": "The study of attacks on machine learning systems and defenses against them. Encompasses evasion attacks poisoning...", "l": "a", "k": ["adversarial", "machine", "learning", "study", "attacks", "systems", "defenses", "against", "encompasses", "evasion", "poisoning", "model", "stealing", "privacy", "certified"]}, {"id": "term-adversarial-patch", "t": "Adversarial Patch", "tg": ["Safety", "Technical"], "d": "safety", "x": "A physical-world adversarial attack that uses a printed patch to fool image classifiers or object detectors. Unlike...", "l": "a", "k": ["adversarial", "patch", "physical-world", "attack", "uses", "printed", "fool", "image", "classifiers", "object", "detectors", "unlike", "pixel-level", "perturbations", "patches"]}, {"id": "term-adversarial-prompting", "t": "Adversarial Prompting", "tg": ["Prompt Engineering", "Safety"], "d": "safety", "x": "The deliberate crafting of inputs designed to exploit vulnerabilities in language models, causing them to produce...", "l": "a", "k": ["adversarial", "prompting", "deliberate", "crafting", "inputs", "designed", "exploit", "vulnerabilities", "language", "models", "causing", "produce", "harmful", "outputs", "bypass"]}, {"id": "term-adversarial-robustness", "t": "Adversarial Robustness", "tg": ["Safety", "Technical"], "d": "safety", "x": "The ability of a machine learning model to maintain correct predictions when inputs are deliberately perturbed by an...", "l": "a", "k": ["adversarial", "robustness", "ability", "machine", "learning", "model", "maintain", "correct", "predictions", "inputs", "deliberately", "perturbed", "adversary", "measured", "attack"]}, {"id": "term-adversarial-training", "t": "Adversarial Training", "tg": ["Algorithms", "Safety"], "d": "algorithms", "x": "A training procedure that augments the training set with adversarial examples generated during training. The model...", "l": "a", "k": ["adversarial", "training", "procedure", "augments", "examples", "generated", "model", "learns", "correctly", "classify", "clean", "inputs", "improving", "robustness", "currently"]}, {"id": "term-aegis-guard", "t": "Aegis Guard", "tg": ["Models", "Technical", "NLP", "Safety"], "d": "models", "x": "A content safety model designed to classify language model interactions for harmful content using a lightweight...", "l": "a", "k": ["aegis", "guard", "content", "safety", "model", "designed", "classify", "language", "interactions", "harmful", "lightweight", "architecture", "fast", "moderation"]}, {"id": "term-affine-scaling-algorithm", "t": "Affine Scaling Algorithm", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An interior point method for linear programming that moves through the interior of the feasible region using affine...", "l": "a", "k": ["affine", "scaling", "algorithm", "interior", "point", "method", "linear", "programming", "moves", "feasible", "region", "transformations", "proposed", "dikin", "later"]}, {"id": "term-affinity-propagation", "t": "Affinity Propagation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A clustering algorithm that identifies exemplars among data points by passing messages between pairs. Does not require...", "l": "a", "k": ["affinity", "propagation", "clustering", "algorithm", "identifies", "exemplars", "among", "data", "points", "passing", "messages", "pairs", "require", "specifying", "number"]}, {"id": "term-afrisenti", "t": "AfriSenti", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "A sentiment analysis dataset for 14 African languages from Twitter. Addresses the underrepresentation of African...", "l": "a", "k": ["afrisenti", "sentiment", "analysis", "dataset", "african", "languages", "twitter", "addresses", "underrepresentation", "nlp", "benchmarks", "resources"]}, {"id": "term-ag-news", "t": "AG News", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A collection of over 1 million news articles from more than 2000 news sources organized into 4 topic categories....", "l": "a", "k": ["news", "collection", "million", "articles", "sources", "organized", "topic", "categories", "commonly", "text", "classification", "benchmarking"]}, {"id": "term-age-verification-in-ai", "t": "Age Verification in AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "Technical and policy mechanisms to verify user age before granting access to AI systems that may be inappropriate for...", "l": "a", "k": ["age", "verification", "technical", "policy", "mechanisms", "verify", "user", "granting", "access", "systems", "inappropriate", "minors", "raises", "tensions", "child"]}, {"id": "term-agent", "t": "Agent (AI Agent)", "tg": ["Architecture", "Advanced"], "d": "models", "x": "An AI system that can perceive its environment, make decisions, and take actions to achieve goals. Modern AI agents can...", "l": "a", "k": ["agent", "system", "perceive", "environment", "decisions", "take", "actions", "achieve", "goals", "modern", "agents", "tools", "browse", "web", "execute"]}, {"id": "term-agent-framework", "t": "Agent Framework", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A software architecture that enables LLMs to autonomously plan, reason, and execute multi-step tasks by combining...", "l": "a", "k": ["agent", "framework", "software", "architecture", "enables", "llms", "autonomously", "plan", "reason", "execute", "multi-step", "tasks", "combining", "language", "understanding"]}, {"id": "term-agent-safety", "t": "Agent Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "The study of ensuring autonomous AI agents that take actions in the real world do so safely. Covers problems like safe...", "l": "a", "k": ["agent", "safety", "study", "ensuring", "autonomous", "agents", "take", "actions", "real", "world", "safely", "covers", "problems", "safe", "exploration"]}, {"id": "term-agentbench", "t": "AgentBench", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A comprehensive benchmark for evaluating LLM-based agents across 8 distinct environments including web browsing coding...", "l": "a", "k": ["agentbench", "comprehensive", "benchmark", "evaluating", "llm-based", "agents", "across", "distinct", "environments", "including", "web", "browsing", "coding", "database", "management"]}, {"id": "term-agentic-ai", "t": "Agentic AI", "tg": ["Architecture", "Advanced"], "d": "models", "x": "AI systems that can autonomously plan, reason, and take actions to accomplish goals. Includes tool use, multi-step...", "l": "a", "k": ["agentic", "systems", "autonomously", "plan", "reason", "take", "actions", "accomplish", "goals", "includes", "tool", "multi-step", "planning", "self-correction", "capabilities"]}, {"id": "term-agentic-chunking", "t": "Agentic Chunking", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "A document splitting strategy that uses a language model agent to make intelligent decisions about chunk boundaries,...", "l": "a", "k": ["agentic", "chunking", "document", "splitting", "strategy", "uses", "language", "model", "agent", "intelligent", "decisions", "chunk", "boundaries", "content", "grouping"]}, {"id": "term-agentic-rag", "t": "Agentic RAG", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A retrieval-augmented generation approach where an LLM agent dynamically decides what to retrieve, refines queries...", "l": "a", "k": ["agentic", "rag", "retrieval-augmented", "generation", "approach", "llm", "agent", "dynamically", "decides", "retrieve", "refines", "queries", "based", "initial", "results"]}, {"id": "term-agglomerative-clustering-algorithm", "t": "Agglomerative Clustering Algorithm", "tg": ["Algorithms", "Fundamentals", "Clustering"], "d": "algorithms", "x": "A bottom-up hierarchical clustering method that starts with each point as its own cluster and iteratively merges the...", "l": "a", "k": ["agglomerative", "clustering", "algorithm", "bottom-up", "hierarchical", "method", "starts", "point", "cluster", "iteratively", "merges", "closest", "clusters", "produces", "dendrogram"]}, {"id": "term-aggregate-ethics", "t": "Aggregate Ethics", "tg": ["Safety", "Ethics"], "d": "safety", "x": "An approach to AI ethics that considers the cumulative societal impact of many individually harmless AI decisions...", "l": "a", "k": ["aggregate", "ethics", "approach", "considers", "cumulative", "societal", "impact", "individually", "harmless", "decisions", "rather", "focusing", "dramatic", "individual", "harms"]}, {"id": "term-aggregation-bias", "t": "Aggregation Bias", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Bias arising when a single model is used for groups with different conditional distributions, leading to poor...", "l": "a", "k": ["aggregation", "bias", "arising", "single", "model", "groups", "different", "conditional", "distributions", "leading", "poor", "performance", "subgroups", "whose", "patterns"]}, {"id": "term-agi", "t": "AGI (Artificial General Intelligence)", "tg": ["Concept", "Future"], "d": "general", "x": "Hypothetical AI that can perform any intellectual task a human can. Unlike today's narrow AI, AGI would generalize...", "l": "a", "k": ["agi", "artificial", "general", "intelligence", "hypothetical", "perform", "intellectual", "task", "human", "unlike", "today", "narrow", "generalize", "across", "domains"]}, {"id": "term-agi-safety", "t": "AGI Safety", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The subfield of AI safety specifically focused on ensuring that artificial general intelligence, systems matching or...", "l": "a", "k": ["agi", "safety", "subfield", "specifically", "focused", "ensuring", "artificial", "general", "intelligence", "systems", "matching", "exceeding", "human", "cognitive", "abilities"]}, {"id": "term-agieval", "t": "AGIEVAL", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating foundation models on human-centric standardized tests including college entrance exams law...", "l": "a", "k": ["agieval", "benchmark", "evaluating", "foundation", "models", "human-centric", "standardized", "tests", "including", "college", "entrance", "exams", "law", "school", "admissions"]}, {"id": "term-aho-corasick-algorithm", "t": "Aho-Corasick Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP", "Searching"], "d": "algorithms", "x": "A string-searching algorithm that locates all occurrences of a set of pattern strings within a text in a single pass....", "l": "a", "k": ["aho-corasick", "algorithm", "string-searching", "locates", "occurrences", "pattern", "strings", "within", "text", "single", "pass", "constructs", "finite", "automaton", "patterns"]}, {"id": "term-ai", "t": "AI (Artificial Intelligence)", "tg": ["Fundamentals", "Core Concept"], "d": "general", "x": "Computer systems designed to perform tasks that typically require human intelligence, such as understanding language,...", "l": "a", "k": ["artificial", "intelligence", "computer", "systems", "designed", "perform", "tasks", "typically", "require", "human", "understanding", "language", "recognizing", "patterns", "making"]}, {"id": "term-ai-alignment-problem", "t": "AI Alignment Problem", "tg": ["History", "Fundamentals"], "d": "history", "x": "The challenge of ensuring that AI systems pursue goals and behaviors aligned with human values and intentions. As AI...", "l": "a", "k": ["alignment", "problem", "challenge", "ensuring", "systems", "pursue", "goals", "behaviors", "aligned", "human", "values", "intentions", "become", "capable", "critical"]}, {"id": "term-ai-alignment-tax", "t": "AI Alignment Tax", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The additional cost in performance, compute, or development time required to make an AI system aligned with human...", "l": "a", "k": ["alignment", "tax", "additional", "cost", "performance", "compute", "development", "time", "required", "system", "aligned", "human", "values", "representing", "trade-off"]}, {"id": "term-ai-and-employment", "t": "AI and Employment", "tg": ["History", "Fundamentals"], "d": "history", "x": "The ongoing debate about how AI and automation will affect jobs and the labor market. Predictions range from mass...", "l": "a", "k": ["employment", "ongoing", "debate", "automation", "affect", "jobs", "labor", "market", "predictions", "range", "mass", "unemployment", "job", "transformation", "augmentation"]}, {"id": "term-ai-arms-race", "t": "AI Arms Race", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "The competitive dynamic between nations or companies racing to develop the most advanced AI capabilities, potentially...", "l": "a", "k": ["arms", "race", "competitive", "dynamic", "nations", "companies", "racing", "develop", "advanced", "capabilities", "potentially", "expense", "safety", "research", "ethical"]}, {"id": "term-ai-assurance", "t": "AI Assurance", "tg": ["Safety", "Governance"], "d": "safety", "x": "Processes and evidence that demonstrate an AI system meets its specified requirements for safety security and...", "l": "a", "k": ["assurance", "processes", "evidence", "demonstrate", "system", "meets", "specified", "requirements", "safety", "security", "performance", "analogous", "software", "extended", "ml-specific"]}, {"id": "term-ai-audit", "t": "AI Audit", "tg": ["Safety", "Governance"], "d": "safety", "x": "A systematic evaluation of an AI system to assess compliance with regulations ethical guidelines and performance...", "l": "a", "k": ["audit", "systematic", "evaluation", "system", "assess", "compliance", "regulations", "ethical", "guidelines", "performance", "standards", "include", "technical", "testing", "documentation"]}, {"id": "term-ai-bill-of-rights", "t": "AI Bill of Rights", "tg": ["Governance", "Regulation"], "d": "safety", "x": "The Blueprint for an AI Bill of Rights released by the White House OSTP in 2022, outlining five principles for...", "l": "a", "k": ["bill", "rights", "blueprint", "released", "white", "house", "ostp", "outlining", "five", "principles", "responsible", "safe", "systems", "algorithmic", "discrimination"]}, {"id": "term-ai-boom-2023", "t": "AI Boom 2023", "tg": ["History", "Milestones"], "d": "history", "x": "The period of intense investment, development, and public attention in AI following the launch of ChatGPT,...", "l": "a", "k": ["boom", "period", "intense", "investment", "development", "public", "attention", "following", "launch", "chatgpt", "characterized", "rapid", "advances", "large", "language"]}, {"id": "term-ai-bounty-program", "t": "AI Bounty Program", "tg": ["Safety", "Governance"], "d": "safety", "x": "A program that rewards external researchers for identifying vulnerabilities biases or safety issues in AI systems....", "l": "a", "k": ["bounty", "program", "rewards", "external", "researchers", "identifying", "vulnerabilities", "biases", "safety", "issues", "systems", "modeled", "cybersecurity", "bug", "bounties"]}, {"id": "term-ai-carbon-footprint", "t": "AI Carbon Footprint", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The environmental impact of training and running AI models measured in carbon dioxide equivalent emissions. Large...", "l": "a", "k": ["carbon", "footprint", "environmental", "impact", "training", "running", "models", "measured", "dioxide", "equivalent", "emissions", "large", "language", "model", "emit"]}, {"id": "term-ai-certification", "t": "AI Certification", "tg": ["Safety", "Governance"], "d": "safety", "x": "Formal processes by which an independent body assesses and certifies that an AI system meets specified safety...", "l": "a", "k": ["certification", "formal", "processes", "independent", "body", "assesses", "certifies", "system", "meets", "specified", "safety", "performance", "ethical", "standards", "emerging"]}, {"id": "term-ai-chip-benchmark", "t": "AI Chip Benchmark", "tg": ["Benchmark", "Evaluation", "Standard"], "d": "hardware", "x": "Standardized tests for evaluating and comparing AI accelerator performance across different vendors. Includes industry...", "l": "a", "k": ["chip", "benchmark", "standardized", "tests", "evaluating", "comparing", "accelerator", "performance", "across", "different", "vendors", "includes", "industry", "benchmarks", "mlperf"]}, {"id": "term-ai-chip-export-controls", "t": "AI Chip Export Controls", "tg": ["Policy", "Trade", "Regulation"], "d": "hardware", "x": "Government restrictions on exporting advanced AI chips to certain countries. US export controls on NVIDIA A100 H100 and...", "l": "a", "k": ["chip", "export", "controls", "government", "restrictions", "exporting", "advanced", "chips", "certain", "countries", "nvidia", "a100", "h100", "equivalent", "significantly"]}, {"id": "term-ai-chip-race", "t": "AI Chip Race", "tg": ["History", "Milestones"], "d": "history", "x": "The competition among semiconductor companies to develop specialized chips optimized for AI workloads. Key players...", "l": "a", "k": ["chip", "race", "competition", "among", "semiconductor", "companies", "develop", "specialized", "chips", "optimized", "workloads", "key", "players", "include", "nvidia"]}, {"id": "term-ai-compliance", "t": "AI Compliance", "tg": ["Safety", "Governance"], "d": "safety", "x": "The practice of ensuring AI systems conform to applicable laws regulations industry standards and organizational...", "l": "a", "k": ["compliance", "practice", "ensuring", "systems", "conform", "applicable", "laws", "regulations", "industry", "standards", "organizational", "policies", "includes", "documentation", "impact"]}, {"id": "term-ai-consciousness", "t": "AI Consciousness", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The philosophical and scientific question of whether AI systems can have subjective experiences or phenomenal...", "l": "a", "k": ["consciousness", "philosophical", "scientific", "question", "systems", "subjective", "experiences", "phenomenal", "awareness", "implications", "moral", "consideration", "ethical", "treatment", "entities"]}, {"id": "term-ai-containment", "t": "AI Containment", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "Strategies and technical measures designed to prevent an advanced AI system from exerting unintended influence on the...", "l": "a", "k": ["containment", "strategies", "technical", "measures", "designed", "prevent", "advanced", "system", "exerting", "unintended", "influence", "external", "world", "including", "air-gapping"]}, {"id": "term-ai-content-detection", "t": "AI Content Detection", "tg": ["Safety", "Technical"], "d": "safety", "x": "Tools and techniques for identifying whether text images or other media were generated by AI systems. Uses statistical...", "l": "a", "k": ["content", "detection", "tools", "techniques", "identifying", "text", "images", "media", "were", "generated", "systems", "uses", "statistical", "analysis", "watermarking"]}, {"id": "term-ai-decolonization", "t": "AI Decolonization", "tg": ["Safety", "Ethics"], "d": "safety", "x": "A movement to challenge Western-centric assumptions in AI development and deployment. Advocates for including diverse...", "l": "a", "k": ["decolonization", "movement", "challenge", "western-centric", "assumptions", "development", "deployment", "advocates", "including", "diverse", "cultural", "perspectives", "addressing", "power", "imbalances"]}, {"id": "term-ai-democratization-risks", "t": "AI Democratization Risks", "tg": ["Safety", "Policy"], "d": "safety", "x": "The potential dangers of making powerful AI capabilities widely accessible without adequate safety measures. Includes...", "l": "a", "k": ["democratization", "risks", "potential", "dangers", "making", "powerful", "capabilities", "widely", "accessible", "without", "adequate", "safety", "measures", "includes", "dual-use"]}, {"id": "term-ai-dependency-risk", "t": "AI Dependency Risk", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The risk that over-reliance on AI systems creates fragility in critical infrastructure and decision-making processes....", "l": "a", "k": ["dependency", "risk", "over-reliance", "systems", "creates", "fragility", "critical", "infrastructure", "decision-making", "processes", "includes", "concerns", "skill", "atrophy", "single"]}, {"id": "term-ai-detection", "t": "AI Detection", "tg": ["Generative AI", "LLM"], "d": "models", "x": "Methods and tools designed to distinguish AI-generated text, images, or media from human-created content, using...", "l": "a", "k": ["detection", "methods", "tools", "designed", "distinguish", "ai-generated", "text", "images", "media", "human-created", "content", "statistical", "analysis", "token", "distributions"]}, {"id": "term-ai-digital-divide", "t": "AI Digital Divide", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "The gap between those who have access to AI technologies and the skills to use them and those who do not, potentially...", "l": "a", "k": ["digital", "divide", "gap", "access", "technologies", "skills", "potentially", "exacerbating", "existing", "social", "economic", "inequalities", "across", "within", "nations"]}, {"id": "term-ai-disclosure-requirements", "t": "AI Disclosure Requirements", "tg": ["Safety", "Policy"], "d": "safety", "x": "Legal or ethical obligations to inform users when they are interacting with an AI system rather than a human....", "l": "a", "k": ["disclosure", "requirements", "legal", "ethical", "obligations", "inform", "users", "interacting", "system", "rather", "human", "increasingly", "mandated", "regulations", "including"]}, {"id": "term-ai-due-diligence", "t": "AI Due Diligence", "tg": ["Safety", "Governance"], "d": "safety", "x": "The process of systematically evaluating AI systems for risks before deployment or acquisition. Includes technical...", "l": "a", "k": ["due", "diligence", "process", "systematically", "evaluating", "systems", "risks", "deployment", "acquisition", "includes", "technical", "audits", "bias", "testing", "legal"]}, {"id": "term-ai-ecosystem-risk", "t": "AI Ecosystem Risk", "tg": ["Safety", "Technical"], "d": "safety", "x": "Systemic risks arising from the interconnected nature of AI systems where a failure or vulnerability in one component...", "l": "a", "k": ["ecosystem", "risk", "systemic", "risks", "arising", "interconnected", "nature", "systems", "failure", "vulnerability", "component", "cascade", "dependent", "applications"]}, {"id": "term-ai-effect", "t": "AI Effect", "tg": ["History", "Fundamentals"], "d": "history", "x": "The phenomenon where once a machine can perform a task that was previously considered to require intelligence that task...", "l": "a", "k": ["effect", "phenomenon", "machine", "perform", "task", "previously", "considered", "require", "intelligence", "longer", "regarded", "requiring", "true", "moving", "goalpost"]}, {"id": "term-ai-emergency-stop", "t": "AI Emergency Stop", "tg": ["Safety", "Technical"], "d": "safety", "x": "A mechanism to rapidly shut down or constrain an AI system that is operating unsafely. Also known as a kill switch....", "l": "a", "k": ["emergency", "stop", "mechanism", "rapidly", "shut", "down", "constrain", "system", "operating", "unsafely", "known", "kill", "switch", "designing", "reliable"]}, {"id": "term-ai-environmental-impact", "t": "AI Environmental Impact", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The environmental costs of AI development and deployment, including the substantial energy consumption and carbon...", "l": "a", "k": ["environmental", "impact", "costs", "development", "deployment", "including", "substantial", "energy", "consumption", "carbon", "emissions", "training", "large", "models", "water"]}, {"id": "term-ai-ethical-framework", "t": "AI Ethical Framework", "tg": ["Safety", "Ethics"], "d": "safety", "x": "A structured set of principles guidelines and procedures for developing and deploying AI systems responsibly. Examples...", "l": "a", "k": ["ethical", "framework", "structured", "principles", "guidelines", "procedures", "developing", "deploying", "systems", "responsibly", "examples", "include", "ieee", "ethically", "aligned"]}, {"id": "term-ai-ethics", "t": "AI Ethics", "tg": ["Ethics", "Society"], "d": "safety", "x": "The study of moral principles and values that should guide the development and use of AI systems. Covers fairness,...", "l": "a", "k": ["ethics", "study", "moral", "principles", "values", "guide", "development", "systems", "covers", "fairness", "transparency", "privacy", "accountability", "societal", "impact"]}, {"id": "term-ai-ethics-history", "t": "AI Ethics History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of AI ethics from early philosophical questions (Turing 1950 Weizenbaum 1976) through concerns about bias...", "l": "a", "k": ["ethics", "history", "evolution", "early", "philosophical", "questions", "turing", "weizenbaum", "concerns", "bias", "fairness", "2010s", "modern", "debates", "existential"]}, {"id": "term-ai-export-controls", "t": "AI Export Controls", "tg": ["Safety", "Policy"], "d": "safety", "x": "Government regulations restricting the international transfer of AI technology including models training data chips and...", "l": "a", "k": ["export", "controls", "government", "regulations", "restricting", "international", "transfer", "technology", "including", "models", "training", "data", "chips", "related", "expertise"]}, {"id": "term-ai-fairness-metrics", "t": "AI Fairness Metrics", "tg": ["Safety", "Technical"], "d": "safety", "x": "Quantitative measures used to assess whether an AI system treats different demographic groups equitably. Common metrics...", "l": "a", "k": ["fairness", "metrics", "quantitative", "measures", "assess", "system", "treats", "different", "demographic", "groups", "equitably", "common", "include", "parity", "equalized"]}, {"id": "term-ai-forensics", "t": "AI Forensics", "tg": ["Safety", "Technical"], "d": "safety", "x": "The application of investigative techniques to understand AI system behavior after an incident or failure. Includes...", "l": "a", "k": ["forensics", "application", "investigative", "techniques", "understand", "system", "behavior", "incident", "failure", "includes", "model", "inspection", "log", "analysis", "provenance"]}, {"id": "term-ai-gap", "t": "AI Gap", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The disparity in AI capabilities resources and benefits between wealthy and less wealthy nations or communities....", "l": "a", "k": ["gap", "disparity", "capabilities", "resources", "benefits", "wealthy", "less", "nations", "communities", "contributes", "widening", "global", "inequality", "digital", "divides"]}, {"id": "term-ai-governance", "t": "AI Governance", "tg": ["Governance", "Regulation"], "d": "safety", "x": "The set of policies, regulations, standards, and institutional frameworks that guide the development, deployment, and...", "l": "a", "k": ["governance", "policies", "regulations", "standards", "institutional", "frameworks", "guide", "development", "deployment", "oversight", "artificial", "intelligence", "systems", "organizational", "national"]}, {"id": "term-ai-governance-framework", "t": "AI Governance Framework", "tg": ["Safety", "Governance"], "d": "safety", "x": "A comprehensive structure of policies processes roles and tools for managing the development deployment and monitoring...", "l": "a", "k": ["governance", "framework", "comprehensive", "structure", "policies", "processes", "roles", "tools", "managing", "development", "deployment", "monitoring", "systems", "within", "organization"]}, {"id": "term-ai-hallucination-problem", "t": "AI Hallucination Problem", "tg": ["History", "Fundamentals"], "d": "history", "x": "The phenomenon where AI language models generate plausible-sounding but factually incorrect or fabricated information....", "l": "a", "k": ["hallucination", "problem", "phenomenon", "language", "models", "generate", "plausible-sounding", "factually", "incorrect", "fabricated", "information", "recognized", "major", "challenge", "deploying"]}, {"id": "term-ai-hardware-startup-ecosystem", "t": "AI Hardware Startup Ecosystem", "tg": ["Industry", "Ecosystem", "Startup"], "d": "hardware", "x": "The growing landscape of companies designing novel AI chip architectures beyond traditional GPUs. Includes neuromorphic...", "l": "a", "k": ["hardware", "startup", "ecosystem", "growing", "landscape", "companies", "designing", "novel", "chip", "architectures", "beyond", "traditional", "gpus", "includes", "neuromorphic"]}, {"id": "term-ai-hype-cycle", "t": "AI Hype Cycle", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The pattern of inflated expectations followed by disillusionment and eventual productive adoption that characterizes...", "l": "a", "k": ["hype", "cycle", "pattern", "inflated", "expectations", "followed", "disillusionment", "eventual", "productive", "adoption", "characterizes", "public", "perception", "capabilities", "contributes"]}, {"id": "term-ai-impact-assessment", "t": "AI Impact Assessment", "tg": ["Governance", "AI Ethics"], "d": "safety", "x": "A systematic process for evaluating the potential social, ethical, economic, and environmental effects of an AI system...", "l": "a", "k": ["impact", "assessment", "systematic", "process", "evaluating", "potential", "social", "ethical", "economic", "environmental", "effects", "system", "deployment", "analogous", "assessments"]}, {"id": "term-ai-incident-database", "t": "AI Incident Database", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "A repository cataloging real-world instances where AI systems caused harm or exhibited problematic behavior, maintained...", "l": "a", "k": ["incident", "database", "repository", "cataloging", "real-world", "instances", "systems", "caused", "harm", "exhibited", "problematic", "behavior", "maintained", "organizations", "partnership"]}, {"id": "term-ai-incident-response", "t": "AI Incident Response", "tg": ["Safety", "Governance"], "d": "safety", "x": "A structured process for detecting investigating and remediating harmful outcomes from AI systems. Modeled on...", "l": "a", "k": ["incident", "response", "structured", "process", "detecting", "investigating", "remediating", "harmful", "outcomes", "systems", "modeled", "cybersecurity", "adapted", "ml-specific", "failure"]}, {"id": "term-ai-index-report", "t": "AI Index Report", "tg": ["History", "Organizations"], "d": "history", "x": "An annual report from Stanford University's Human-Centered AI Institute that tracks measures and visualizes data...", "l": "a", "k": ["index", "report", "annual", "stanford", "university", "human-centered", "institute", "tracks", "measures", "visualizes", "data", "related", "progress", "provides", "comprehensive"]}, {"id": "term-ai-inference-at-the-edge", "t": "AI Inference at the Edge", "tg": ["Edge", "Inference", "Deployment"], "d": "hardware", "x": "Running trained AI models on edge devices for real-time local decision making. Requires hardware that balances compute...", "l": "a", "k": ["inference", "edge", "running", "trained", "models", "devices", "real-time", "local", "decision", "making", "requires", "hardware", "balances", "compute", "capability"]}, {"id": "term-ai-insurance", "t": "AI Insurance", "tg": ["Safety", "Governance"], "d": "safety", "x": "Insurance products designed to cover liabilities arising from AI system failures or harms. An emerging market that...", "l": "a", "k": ["insurance", "products", "designed", "cover", "liabilities", "arising", "system", "failures", "harms", "emerging", "market", "faces", "challenges", "risk", "assessment"]}, {"id": "term-ai-labor-displacement", "t": "AI Labor Displacement", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The phenomenon of AI and automation systems replacing human workers in various occupations, raising concerns about...", "l": "a", "k": ["labor", "displacement", "phenomenon", "automation", "systems", "replacing", "human", "workers", "various", "occupations", "raising", "concerns", "unemployment", "wage", "depression"]}, {"id": "term-ai-liability-framework", "t": "AI Liability Framework", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Legal frameworks determining who bears responsibility when AI systems cause harm, including debates over strict...", "l": "a", "k": ["liability", "framework", "legal", "frameworks", "determining", "bears", "responsibility", "systems", "cause", "harm", "including", "debates", "strict", "negligence", "standards"]}, {"id": "term-ai-literacy", "t": "AI Literacy", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The knowledge and skills needed for individuals to understand evaluate and interact effectively with AI systems....", "l": "a", "k": ["literacy", "knowledge", "skills", "needed", "individuals", "understand", "evaluate", "interact", "effectively", "systems", "considered", "essential", "informed", "consent", "democratic"]}, {"id": "term-ai-lobbying", "t": "AI Lobbying", "tg": ["Safety", "Policy"], "d": "safety", "x": "Advocacy activities by AI companies and industry groups to influence government policy and regulation. Raises concerns...", "l": "a", "k": ["lobbying", "advocacy", "activities", "companies", "industry", "groups", "influence", "government", "policy", "regulation", "raises", "concerns", "regulatory", "capture", "balance"]}, {"id": "term-ai-manipulation", "t": "AI Manipulation", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The use of AI systems to influence human behavior beliefs or decisions through deceptive or coercive means. Includes...", "l": "a", "k": ["manipulation", "systems", "influence", "human", "behavior", "beliefs", "decisions", "deceptive", "coercive", "means", "includes", "deepfakes", "persuasive", "micro-targeting", "synthetic"]}, {"id": "term-ai-monoculture-risk", "t": "AI Monoculture Risk", "tg": ["Safety", "Technical"], "d": "safety", "x": "The danger that widespread adoption of similar AI models architectures or training data creates systemic...", "l": "a", "k": ["monoculture", "risk", "danger", "widespread", "adoption", "similar", "models", "architectures", "training", "data", "creates", "systemic", "vulnerabilities", "single", "flaw"]}, {"id": "term-ai-moratorium", "t": "AI Moratorium", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "A proposed temporary pause on the development of AI systems above a certain capability threshold, notably advocated in...", "l": "a", "k": ["moratorium", "proposed", "temporary", "pause", "development", "systems", "certain", "capability", "threshold", "notably", "advocated", "march", "open", "letter", "signed"]}, {"id": "term-ai-nationalism", "t": "AI Nationalism", "tg": ["Safety", "Policy"], "d": "safety", "x": "Government policies that prioritize domestic AI development as a matter of national competitiveness and security. Can...", "l": "a", "k": ["nationalism", "government", "policies", "prioritize", "domestic", "development", "matter", "national", "competitiveness", "security", "lead", "fragmented", "standards", "restricted", "collaboration"]}, {"id": "term-ai-ombudsman", "t": "AI Ombudsman", "tg": ["Safety", "Governance"], "d": "safety", "x": "An independent official or office responsible for investigating complaints about AI systems and advocating for affected...", "l": "a", "k": ["ombudsman", "independent", "official", "office", "responsible", "investigating", "complaints", "systems", "advocating", "affected", "individuals", "proposed", "governance", "mechanism", "provide"]}, {"id": "term-ai-oversight-board", "t": "AI Oversight Board", "tg": ["Safety", "Governance"], "d": "safety", "x": "An organizational body responsible for reviewing and approving high-risk AI deployments. May include technical experts...", "l": "a", "k": ["oversight", "board", "organizational", "body", "responsible", "reviewing", "approving", "high-risk", "deployments", "include", "technical", "experts", "ethicists", "legal", "advisors"]}, {"id": "term-ai-patent-ethics", "t": "AI Patent Ethics", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Ethical considerations around patenting AI inventions including questions of inventorship for AI-generated innovations...", "l": "a", "k": ["patent", "ethics", "ethical", "considerations", "around", "patenting", "inventions", "including", "questions", "inventorship", "ai-generated", "innovations", "access", "technologies", "impact"]}, {"id": "term-ai-personhood", "t": "AI Personhood", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The legal and philosophical concept of granting AI systems some form of legal personality, enabling them to hold...", "l": "a", "k": ["personhood", "legal", "philosophical", "concept", "granting", "systems", "form", "personality", "enabling", "hold", "rights", "enter", "contracts", "bear", "liability"]}, {"id": "term-ai-pluralism", "t": "AI Pluralism", "tg": ["Safety", "Ethics"], "d": "safety", "x": "An approach to AI development that embraces diverse perspectives values and cultural contexts rather than imposing a...", "l": "a", "k": ["pluralism", "approach", "development", "embraces", "diverse", "perspectives", "values", "cultural", "contexts", "rather", "imposing", "single", "worldview", "contrasts", "monocultural"]}, {"id": "term-ai-policy-sandbox", "t": "AI Policy Sandbox", "tg": ["Safety", "Policy"], "d": "safety", "x": "A controlled regulatory environment where new AI applications can be tested under relaxed rules with government...", "l": "a", "k": ["policy", "sandbox", "controlled", "regulatory", "environment", "applications", "tested", "relaxed", "rules", "government", "oversight", "allows", "regulators", "learn", "emerging"]}, {"id": "term-ai-poverty-trap", "t": "AI Poverty Trap", "tg": ["Safety", "Ethics"], "d": "safety", "x": "A scenario where communities lacking AI capabilities fall further behind economically and socially as AI-driven...", "l": "a", "k": ["poverty", "trap", "scenario", "communities", "lacking", "capabilities", "fall", "behind", "economically", "socially", "ai-driven", "productivity", "gains", "accrue", "primarily"]}, {"id": "term-ai-procurement-standards", "t": "AI Procurement Standards", "tg": ["Safety", "Governance"], "d": "safety", "x": "Requirements and evaluation criteria that government agencies and organizations use when purchasing or contracting AI...", "l": "a", "k": ["procurement", "standards", "requirements", "evaluation", "criteria", "government", "agencies", "organizations", "purchasing", "contracting", "systems", "include", "specifications", "fairness", "transparency"]}, {"id": "term-ai-professional-ethics", "t": "AI Professional Ethics", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Ethical obligations and codes of conduct for individuals working in AI development and deployment. Analogous to...", "l": "a", "k": ["professional", "ethics", "ethical", "obligations", "codes", "conduct", "individuals", "working", "development", "deployment", "analogous", "medicine", "law", "engineering"]}, {"id": "term-ai-proportionality", "t": "AI Proportionality", "tg": ["Safety", "Policy"], "d": "safety", "x": "The principle that the intrusiveness and risks of an AI system should be proportionate to the benefits it provides and...", "l": "a", "k": ["proportionality", "principle", "intrusiveness", "risks", "system", "proportionate", "benefits", "provides", "severity", "problem", "addresses", "key", "act"]}, {"id": "term-ai-public-engagement", "t": "AI Public Engagement", "tg": ["Safety", "Governance"], "d": "safety", "x": "Processes for involving the general public in decisions about AI development and deployment. Includes citizen...", "l": "a", "k": ["public", "engagement", "processes", "involving", "general", "decisions", "development", "deployment", "includes", "citizen", "assemblies", "consultations", "deliberative", "forums", "participatory"]}, {"id": "term-ai-quality-assurance", "t": "AI Quality Assurance", "tg": ["Safety", "Governance"], "d": "safety", "x": "Systematic processes to ensure AI systems meet defined standards for accuracy reliability fairness and safety...", "l": "a", "k": ["quality", "assurance", "systematic", "processes", "ensure", "systems", "meet", "defined", "standards", "accuracy", "reliability", "fairness", "safety", "throughout", "lifecycle"]}, {"id": "term-ai-race-dynamics", "t": "AI Race Dynamics", "tg": ["Safety", "Policy"], "d": "safety", "x": "The competitive pressures between nations and companies to develop AI capabilities quickly which can lead to cutting...", "l": "a", "k": ["race", "dynamics", "competitive", "pressures", "nations", "companies", "develop", "capabilities", "quickly", "lead", "cutting", "corners", "safety", "testing", "responsible"]}, {"id": "term-ai-readiness", "t": "AI Readiness", "tg": ["Fundamentals", "Skill"], "d": "general", "x": "The skills, knowledge, and mindset needed to use AI tools effectively and responsibly. Includes understanding both...", "l": "a", "k": ["readiness", "skills", "knowledge", "mindset", "needed", "tools", "effectively", "responsibly", "includes", "understanding", "capabilities", "limitations"]}, {"id": "term-ai-readiness-assessment", "t": "AI Readiness Assessment", "tg": ["Safety", "Governance"], "d": "safety", "x": "A structured evaluation of an organization's preparedness to adopt and deploy AI systems responsibly. Covers technical...", "l": "a", "k": ["readiness", "assessment", "structured", "evaluation", "organization", "preparedness", "adopt", "deploy", "systems", "responsibly", "covers", "technical", "infrastructure", "data", "quality"]}, {"id": "term-ai-red-lines", "t": "AI Red Lines", "tg": ["AI Safety", "AI Ethics"], "d": "safety", "x": "Clearly defined boundaries that AI systems should never cross, such as refusing to assist with creating weapons of mass...", "l": "a", "k": ["red", "lines", "clearly", "defined", "boundaries", "systems", "never", "cross", "refusing", "assist", "creating", "weapons", "mass", "destruction", "generating"]}, {"id": "term-ai-registration", "t": "AI Registration", "tg": ["Safety", "Policy"], "d": "safety", "x": "A proposed requirement that AI systems above a certain capability threshold be registered with a government authority...", "l": "a", "k": ["registration", "proposed", "requirement", "systems", "certain", "capability", "threshold", "registered", "government", "authority", "deployment", "analogous", "product", "pharmaceuticals", "aviation"]}, {"id": "term-ai-regulation-timeline", "t": "AI Regulation Timeline", "tg": ["History", "Regulation"], "d": "history", "x": "The chronological progression of AI governance efforts from early ethical guidelines in the 2010s through the EU AI...", "l": "a", "k": ["regulation", "timeline", "chronological", "progression", "governance", "efforts", "early", "ethical", "guidelines", "2010s", "act", "executive", "orders", "international", "summits"]}, {"id": "term-ai-regulatory-sandbox", "t": "AI Regulatory Sandbox", "tg": ["Governance", "Regulation"], "d": "safety", "x": "A controlled environment established by regulators where AI companies can test innovative products under relaxed...", "l": "a", "k": ["regulatory", "sandbox", "controlled", "environment", "established", "regulators", "companies", "test", "innovative", "products", "relaxed", "requirements", "maintaining", "safeguards", "provided"]}, {"id": "term-ai-reliance", "t": "AI Reliance", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The degree to which humans depend on AI system outputs for decision-making. Appropriate reliance means using AI...", "l": "a", "k": ["reliance", "degree", "humans", "depend", "system", "outputs", "decision-making", "appropriate", "means", "recommendations", "improve", "outcomes", "overriding"]}, {"id": "term-ai-rights", "t": "AI Rights", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The philosophical and legal question of whether sufficiently advanced AI systems should be granted legal rights or...", "l": "a", "k": ["rights", "philosophical", "legal", "question", "sufficiently", "advanced", "systems", "granted", "moral", "standing", "debates", "draw", "animal", "philosophy", "corporate"]}, {"id": "term-ai-risk-assessment", "t": "AI Risk Assessment", "tg": ["Safety", "Governance"], "d": "safety", "x": "A systematic process for identifying evaluating and prioritizing risks associated with an AI system including technical...", "l": "a", "k": ["risk", "assessment", "systematic", "process", "identifying", "evaluating", "prioritizing", "risks", "associated", "system", "including", "technical", "failures", "ethical", "harms"]}, {"id": "term-ai-risk-levels", "t": "AI Risk Levels", "tg": ["Governance", "Regulation"], "d": "safety", "x": "A classification scheme, notably used in the EU AI Act, that categorizes AI applications into tiers such as...", "l": "a", "k": ["risk", "levels", "classification", "scheme", "notably", "act", "categorizes", "applications", "tiers", "unacceptable", "high", "limited", "minimal", "corresponding", "regulatory"]}, {"id": "term-ai-risk-taxonomy", "t": "AI Risk Taxonomy", "tg": ["Safety", "Governance"], "d": "safety", "x": "A structured classification system for categorizing the types of risks posed by AI systems. Frameworks include NIST AI...", "l": "a", "k": ["risk", "taxonomy", "structured", "classification", "system", "categorizing", "types", "risks", "posed", "systems", "frameworks", "include", "nist", "rmf", "categories"]}, {"id": "term-ai-safety", "t": "AI Safety", "tg": ["Field", "Safety"], "d": "safety", "x": "The field focused on ensuring AI systems behave safely and beneficially. Includes technical research on alignment,...", "l": "a", "k": ["safety", "field", "focused", "ensuring", "systems", "behave", "safely", "beneficially", "includes", "technical", "research", "alignment", "governance", "preventing", "misuse"]}, {"id": "term-ai-safety-institute", "t": "AI Safety Institute", "tg": ["Governance", "AI Safety"], "d": "safety", "x": "A government-backed organization, first established by the UK in 2023, dedicated to evaluating and testing frontier AI...", "l": "a", "k": ["safety", "institute", "government-backed", "organization", "established", "dedicated", "evaluating", "testing", "frontier", "models", "risks", "similar", "institutes", "subsequently", "created"]}, {"id": "term-ai-safety-research", "t": "AI Safety Research", "tg": ["History", "Fundamentals"], "d": "history", "x": "The field of research dedicated to ensuring that AI systems are safe beneficial and aligned with human values. AI...", "l": "a", "k": ["safety", "research", "field", "dedicated", "ensuring", "systems", "safe", "beneficial", "aligned", "human", "values", "encompasses", "technical", "alignment", "governance"]}, {"id": "term-ai-sandboxing", "t": "AI Sandboxing", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "The practice of running AI systems in isolated environments with restricted access to networks, resources, and...", "l": "a", "k": ["sandboxing", "practice", "running", "systems", "isolated", "environments", "restricted", "access", "networks", "resources", "actuators", "limit", "potential", "harm", "testing"]}, {"id": "term-ai-security", "t": "AI Security", "tg": ["Safety", "Technical"], "d": "safety", "x": "The practice of protecting AI systems from adversarial attacks data poisoning model theft and other threats....", "l": "a", "k": ["security", "practice", "protecting", "systems", "adversarial", "attacks", "data", "poisoning", "model", "theft", "threats", "encompasses", "defensive", "techniques", "threat"]}, {"id": "term-ai-social-contract", "t": "AI Social Contract", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The implicit agreement between AI developers deployers users and society about the acceptable uses and limits of AI...", "l": "a", "k": ["social", "contract", "implicit", "agreement", "developers", "deployers", "users", "society", "acceptable", "uses", "limits", "technology", "draws", "theory", "frame"]}, {"id": "term-ai-spring", "t": "AI Spring", "tg": ["History", "Milestones"], "d": "history", "x": "A period of renewed optimism investment and progress in AI research typically referring to the resurgence beginning...", "l": "a", "k": ["spring", "period", "renewed", "optimism", "investment", "progress", "research", "typically", "referring", "resurgence", "beginning", "around", "driven", "deep", "learning"]}, {"id": "term-ai-standards", "t": "AI Standards", "tg": ["Safety", "Governance"], "d": "safety", "x": "Technical specifications and guidelines established by standards bodies for AI system development testing and...", "l": "a", "k": ["standards", "technical", "specifications", "guidelines", "established", "bodies", "system", "development", "testing", "deployment", "key", "organizations", "include", "iso", "iec"]}, {"id": "term-ai-supercomputer", "t": "AI Supercomputer", "tg": ["Data Center", "Supercomputer"], "d": "hardware", "x": "Computing system specifically designed and optimized for artificial intelligence training and research. Examples...", "l": "a", "k": ["supercomputer", "computing", "system", "specifically", "designed", "optimized", "artificial", "intelligence", "training", "research", "examples", "include", "nvidia", "dgx", "superpod"]}, {"id": "term-ai-supply-chain-security", "t": "AI Supply Chain Security", "tg": ["Safety", "Technical"], "d": "safety", "x": "Measures to ensure the integrity and safety of all components in the AI development pipeline including training data...", "l": "a", "k": ["supply", "chain", "security", "measures", "ensure", "integrity", "safety", "components", "development", "pipeline", "including", "training", "data", "pre-trained", "models"]}, {"id": "term-ai-sustainability", "t": "AI Sustainability", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The practice of developing and deploying AI systems in ways that are environmentally economically and socially...", "l": "a", "k": ["sustainability", "practice", "developing", "deploying", "systems", "ways", "environmentally", "economically", "socially", "sustainable", "long", "term", "covers", "energy", "efficiency"]}, {"id": "term-ai-talent-pipeline", "t": "AI Talent Pipeline", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The system of education training and recruitment that produces qualified AI practitioners. Concentration of AI talent...", "l": "a", "k": ["talent", "pipeline", "system", "education", "training", "recruitment", "produces", "qualified", "practitioners", "concentration", "companies", "countries", "raises", "concerns", "equity"]}, {"id": "term-ai-taxonomy", "t": "AI Taxonomy", "tg": ["Safety", "Governance"], "d": "safety", "x": "A systematic classification of AI systems by capability risk level domain or technology type. Used by regulators to...", "l": "a", "k": ["taxonomy", "systematic", "classification", "systems", "capability", "risk", "level", "domain", "technology", "type", "regulators", "apply", "differentiated", "requirements", "based"]}, {"id": "term-ai-testing-standards", "t": "AI Testing Standards", "tg": ["Safety", "Governance"], "d": "safety", "x": "Established methodologies and benchmarks for evaluating AI system performance safety and fairness. Include adversarial...", "l": "a", "k": ["testing", "standards", "established", "methodologies", "benchmarks", "evaluating", "system", "performance", "safety", "fairness", "include", "adversarial", "stress", "bias", "auditing"]}, {"id": "term-ai-tort-law", "t": "AI Tort Law", "tg": ["Safety", "Policy"], "d": "safety", "x": "The application of tort law principles to AI-related harms. Raises novel questions about duty of care foreseeability...", "l": "a", "k": ["tort", "law", "application", "principles", "ai-related", "harms", "raises", "novel", "questions", "duty", "care", "foreseeability", "causation", "liability", "autonomous"]}, {"id": "term-ai-training-cluster", "t": "AI Training Cluster", "tg": ["Infrastructure", "Training", "Cluster"], "d": "hardware", "x": "Dedicated computing cluster designed specifically for training large AI models. Combines thousands of GPUs with...", "l": "a", "k": ["training", "cluster", "dedicated", "computing", "designed", "specifically", "large", "models", "combines", "thousands", "gpus", "high-bandwidth", "networking", "optimized", "storage"]}, {"id": "term-ai-transparency-report", "t": "AI Transparency Report", "tg": ["Safety", "Governance"], "d": "safety", "x": "A public document disclosing information about an organization's AI systems including their capabilities limitations...", "l": "a", "k": ["transparency", "report", "public", "document", "disclosing", "information", "organization", "systems", "including", "capabilities", "limitations", "known", "biases", "safety", "measures"]}, {"id": "term-ai-treaty", "t": "AI Treaty", "tg": ["Safety", "Policy"], "d": "safety", "x": "A proposed international agreement to regulate the development and deployment of AI systems. Discussions draw parallels...", "l": "a", "k": ["treaty", "proposed", "international", "agreement", "regulate", "development", "deployment", "systems", "discussions", "draw", "parallels", "nuclear", "arms", "control", "treaties"]}, {"id": "term-ai-trust", "t": "AI Trust", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The degree to which users and society have confidence that AI systems will behave as expected and in accordance with...", "l": "a", "k": ["trust", "degree", "users", "society", "confidence", "systems", "behave", "expected", "accordance", "human", "values", "built", "transparency", "reliability", "accountability"]}, {"id": "term-ai-unemployment", "t": "AI Unemployment", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The displacement of human workers by AI and automation systems. Economic research suggests AI may transform rather than...", "l": "a", "k": ["unemployment", "displacement", "human", "workers", "automation", "systems", "economic", "research", "suggests", "transform", "rather", "eliminate", "jobs", "transition", "costs"]}, {"id": "term-ai-value-lock-in", "t": "AI Value Lock-in", "tg": ["Safety", "Technical"], "d": "safety", "x": "The risk that early AI systems encode specific values or preferences that become difficult to change as systems grow...", "l": "a", "k": ["value", "lock-in", "risk", "early", "systems", "encode", "specific", "values", "preferences", "become", "difficult", "change", "grow", "capable", "entrenched"]}, {"id": "term-ai-vulnerability-assessment", "t": "AI Vulnerability Assessment", "tg": ["Safety", "Technical"], "d": "safety", "x": "A systematic evaluation of potential weaknesses in an AI system that could be exploited by adversaries or lead to...", "l": "a", "k": ["vulnerability", "assessment", "systematic", "evaluation", "potential", "weaknesses", "system", "exploited", "adversaries", "lead", "failure", "modes", "covers", "model", "architecture"]}, {"id": "term-ai-washing", "t": "AI Washing", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The practice of companies exaggerating or fabricating the role of AI in their products or services for marketing...", "l": "a", "k": ["washing", "practice", "companies", "exaggerating", "fabricating", "role", "products", "services", "marketing", "purposes", "misleading", "consumers", "investors", "actual", "capabilities"]}, {"id": "term-ai-water-usage", "t": "AI Water Usage", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The water consumption associated with cooling data centers that train and run AI models. Large training runs can...", "l": "a", "k": ["water", "usage", "consumption", "associated", "cooling", "data", "centers", "train", "run", "models", "large", "training", "runs", "consume", "millions"]}, {"id": "term-ai-weapons-review", "t": "AI Weapons Review", "tg": ["Safety", "Policy"], "d": "safety", "x": "Legal and ethical assessment of autonomous weapon systems under international humanitarian law. Required by Article 36...", "l": "a", "k": ["weapons", "review", "legal", "ethical", "assessment", "autonomous", "weapon", "systems", "international", "humanitarian", "law", "required", "article", "additional", "protocol"]}, {"id": "term-ai-whistleblowing", "t": "AI Whistleblowing", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The act of insiders at AI companies publicly disclosing information about safety concerns, unethical practices, or...", "l": "a", "k": ["whistleblowing", "act", "insiders", "companies", "publicly", "disclosing", "information", "safety", "concerns", "unethical", "practices", "dangerous", "capabilities", "seen", "open"]}, {"id": "term-ai-winter", "t": "AI Winter", "tg": ["Historical", "Industry"], "d": "history", "x": "Periods of reduced funding and interest in AI research following failed expectations. Notable winters occurred in the...", "l": "a", "k": ["winter", "periods", "reduced", "funding", "interest", "research", "following", "failed", "expectations", "notable", "winters", "occurred", "1970s", "late", "1980s"]}, {"id": "term-ai-workforce-transition", "t": "AI Workforce Transition", "tg": ["Safety", "Policy"], "d": "safety", "x": "Programs and policies to help workers displaced by AI automation acquire new skills and find alternative employment....", "l": "a", "k": ["workforce", "transition", "programs", "policies", "help", "workers", "displaced", "automation", "acquire", "skills", "find", "alternative", "employment", "includes", "retraining"]}, {"id": "term-ai-optimized-ssd", "t": "AI-Optimized SSD", "tg": ["Storage", "SSD", "Optimization"], "d": "hardware", "x": "Solid-state drive with firmware and controller optimizations for the access patterns of AI workloads. Features high...", "l": "a", "k": ["ai-optimized", "ssd", "solid-state", "drive", "firmware", "controller", "optimizations", "access", "patterns", "workloads", "features", "high", "random", "read", "throughput"]}, {"id": "term-ai2-allen-institute-for-ai", "t": "AI2 (Allen Institute for AI)", "tg": ["History", "Organizations"], "d": "history", "x": "A research institute founded by Paul Allen in 2014 dedicated to AI research for the common good. AI2 has produced...", "l": "a", "k": ["ai2", "allen", "institute", "research", "founded", "paul", "dedicated", "common", "good", "produced", "influential", "work", "including", "semantic", "scholar"]}, {"id": "term-ai2-thor", "t": "AI2-THOR", "tg": ["Benchmark", "Reinforcement Learning", "Robotics"], "d": "datasets", "x": "An interactive 3D simulation environment for embodied AI research providing near-photorealistic indoor scenes. Agents...", "l": "a", "k": ["ai2-thor", "interactive", "simulation", "environment", "embodied", "research", "providing", "near-photorealistic", "indoor", "scenes", "agents", "navigate", "interact", "objects", "task"]}, {"id": "term-ai2d", "t": "AI2D", "tg": ["Benchmark", "Multimodal", "Scientific"], "d": "datasets", "x": "Allen Institute for AI Diagrams a dataset of science diagrams with question-answer pairs. Tests the ability to...", "l": "a", "k": ["ai2d", "allen", "institute", "diagrams", "dataset", "science", "question-answer", "pairs", "tests", "ability", "understand", "reason", "diagrammatic", "representations"]}, {"id": "term-aime", "t": "AIME", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "American Invitational Mathematics Examination problems used as a challenging benchmark for mathematical reasoning in...", "l": "a", "k": ["aime", "american", "invitational", "mathematics", "examination", "problems", "challenging", "benchmark", "mathematical", "reasoning", "tests", "advanced", "competition-level", "problem", "solving"]}, {"id": "term-air-cooling", "t": "Air Cooling", "tg": ["Cooling", "Data Center"], "d": "hardware", "x": "Traditional cooling method using fans and heatsinks to dissipate heat from computing components. Sufficient for...", "l": "a", "k": ["air", "cooling", "traditional", "method", "fans", "heatsinks", "dissipate", "heat", "computing", "components", "sufficient", "lower-power", "hardware", "increasingly", "inadequate"]}, {"id": "term-aishell", "t": "AISHELL", "tg": ["Benchmark", "Speech", "Multilingual"], "d": "datasets", "x": "A Mandarin Chinese speech corpus containing 178 hours of speech from 400 speakers. A primary benchmark for Chinese...", "l": "a", "k": ["aishell", "mandarin", "chinese", "speech", "corpus", "containing", "hours", "speakers", "primary", "benchmark", "automatic", "recognition", "research"]}, {"id": "term-aishell-4", "t": "AISHELL-4", "tg": ["Benchmark", "Speech", "Multilingual"], "d": "datasets", "x": "A real meeting speech corpus in Mandarin containing 120 hours of recordings from 211 meetings. Tests far-field speech...", "l": "a", "k": ["aishell-4", "real", "meeting", "speech", "corpus", "mandarin", "containing", "hours", "recordings", "meetings", "tests", "far-field", "recognition", "speaker", "diarization"]}, {"id": "term-akaike-information-criterion", "t": "Akaike Information Criterion", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A model selection metric that balances goodness of fit with model complexity by adding a penalty proportional to the...", "l": "a", "k": ["akaike", "information", "criterion", "model", "selection", "metric", "balances", "goodness", "fit", "complexity", "adding", "penalty", "proportional", "number", "parameters"]}, {"id": "term-alain-colmerauer", "t": "Alain Colmerauer", "tg": ["History", "Pioneers"], "d": "history", "x": "French computer scientist who co-created the Prolog programming language in 1972 with Philippe Roussel. Prolog became...", "l": "a", "k": ["alain", "colmerauer", "french", "computer", "scientist", "co-created", "prolog", "programming", "language", "philippe", "roussel", "became", "primary", "logic", "adopted"]}, {"id": "term-alan-turing", "t": "Alan Turing", "tg": ["History", "Pioneers"], "d": "history", "x": "British mathematician and logician (1912-1954) who formalized computation with the Turing machine, broke the Enigma...", "l": "a", "k": ["alan", "turing", "british", "mathematician", "logician", "1912-1954", "formalized", "computation", "machine", "broke", "enigma", "code", "bletchley", "park", "proposed"]}, {"id": "term-alan-turing-institute", "t": "Alan Turing Institute", "tg": ["History", "Organizations"], "d": "history", "x": "The United Kingdom's national institute for data science and artificial intelligence founded in 2015. Named after Alan...", "l": "a", "k": ["alan", "turing", "institute", "united", "kingdom", "national", "data", "science", "artificial", "intelligence", "founded", "named", "brings", "together", "researchers"]}, {"id": "term-albert", "t": "ALBERT", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A Lite BERT that reduces model size through factorized embedding parameterization and cross-layer parameter sharing...", "l": "a", "k": ["albert", "lite", "bert", "reduces", "model", "size", "factorized", "embedding", "parameterization", "cross-layer", "parameter", "sharing", "maintaining", "competitive", "performance"]}, {"id": "term-alec-radford", "t": "Alec Radford", "tg": ["History", "Pioneers"], "d": "history", "x": "American AI researcher at OpenAI who led the development of GPT (2018) and GPT-2 (2019) demonstrating that unsupervised...", "l": "a", "k": ["alec", "radford", "american", "researcher", "openai", "led", "development", "gpt", "gpt-2", "demonstrating", "unsupervised", "pre-training", "language", "models", "large"]}, {"id": "term-alex-krizhevsky", "t": "Alex Krizhevsky", "tg": ["History", "Pioneers"], "d": "history", "x": "Ukrainian-Canadian computer scientist who designed AlexNet the deep convolutional neural network that won the 2012...", "l": "a", "k": ["alex", "krizhevsky", "ukrainian-canadian", "computer", "scientist", "designed", "alexnet", "deep", "convolutional", "neural", "network", "won", "imagenet", "large", "scale"]}, {"id": "term-alexa-launch", "t": "Alexa Launch", "tg": ["History", "Milestones"], "d": "history", "x": "Amazon's launch of Alexa and the Echo smart speaker in November 2014, popularizing voice-activated AI assistants in the...", "l": "a", "k": ["alexa", "launch", "amazon", "echo", "smart", "speaker", "november", "popularizing", "voice-activated", "assistants", "home", "establishing", "major", "platform", "ambient"]}, {"id": "term-alexnet", "t": "AlexNet", "tg": ["History", "Milestones"], "d": "history", "x": "A deep convolutional neural network designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton that won the 2012...", "l": "a", "k": ["alexnet", "deep", "convolutional", "neural", "network", "designed", "alex", "krizhevsky", "ilya", "sutskever", "geoffrey", "hinton", "won", "imagenet", "competition"]}, {"id": "term-algebraic-multigrid", "t": "Algebraic Multigrid", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A multigrid method that automatically constructs coarse grids based on the algebraic properties of the matrix rather...", "l": "a", "k": ["algebraic", "multigrid", "method", "automatically", "constructs", "coarse", "grids", "based", "properties", "matrix", "rather", "geometric", "information", "effective", "unstructured"]}, {"id": "term-algorithm", "t": "Algorithm", "tg": ["Fundamentals", "Computer Science"], "d": "hardware", "x": "A step-by-step procedure or set of rules for solving a problem or accomplishing a task. In AI, algorithms define how...", "l": "a", "k": ["algorithm", "step-by-step", "procedure", "rules", "solving", "problem", "accomplishing", "task", "algorithms", "define", "models", "learn", "data", "predictions"]}, {"id": "term-algorithmic-bias", "t": "Algorithmic Bias", "tg": ["History", "Fundamentals"], "d": "history", "x": "Systematic and repeatable errors in computer systems that create unfair outcomes. Algorithmic bias in AI can arise from...", "l": "a", "k": ["algorithmic", "bias", "systematic", "repeatable", "errors", "computer", "systems", "create", "unfair", "outcomes", "arise", "biased", "training", "data", "algorithm"]}, {"id": "term-algorithmic-discrimination", "t": "Algorithmic Discrimination", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Systematic and unfair differential treatment of individuals or groups by automated decision-making systems, often...", "l": "a", "k": ["algorithmic", "discrimination", "systematic", "unfair", "differential", "treatment", "individuals", "groups", "automated", "decision-making", "systems", "arising", "biased", "training", "data"]}, {"id": "term-algorithmic-impact-assessment", "t": "Algorithmic Impact Assessment", "tg": ["Governance", "AI Ethics"], "d": "safety", "x": "A formal evaluation process required in some jurisdictions to assess the potential effects of automated decision-making...", "l": "a", "k": ["algorithmic", "impact", "assessment", "formal", "evaluation", "process", "required", "jurisdictions", "assess", "potential", "effects", "automated", "decision-making", "systems", "individuals"]}, {"id": "term-algorithmic-recourse", "t": "Algorithmic Recourse", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "The ability of individuals affected by automated decisions to take meaningful actions to change the outcome, such as...", "l": "a", "k": ["algorithmic", "recourse", "ability", "individuals", "affected", "automated", "decisions", "take", "meaningful", "actions", "change", "outcome", "understanding", "inputs", "modify"]}, {"id": "term-algorithmic-transparency", "t": "Algorithmic Transparency", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The degree to which the logic, rules, and data dependencies of an algorithm are made visible and understandable to...", "l": "a", "k": ["algorithmic", "transparency", "degree", "logic", "rules", "data", "dependencies", "algorithm", "visible", "understandable", "affected", "individuals", "regulators", "public"]}, {"id": "term-alibi", "t": "ALiBi", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Attention with Linear Biases, a positional encoding method that adds a linear bias proportional to the distance between...", "l": "a", "k": ["alibi", "attention", "linear", "biases", "positional", "encoding", "method", "adds", "bias", "proportional", "distance", "key", "query", "positions", "directly"]}, {"id": "term-align", "t": "ALIGN", "tg": ["Models", "Technical"], "d": "models", "x": "A Large-scale ImaGe and Noisy-text embedding model trained on over one billion noisy image-text pairs with minimal...", "l": "a", "k": ["align", "large-scale", "image", "noisy-text", "embedding", "model", "trained", "billion", "noisy", "image-text", "pairs", "minimal", "filtering", "demonstrates", "scaling"]}, {"id": "term-alignment", "t": "Alignment", "tg": ["Safety", "Research"], "d": "safety", "x": "The challenge of ensuring AI systems behave in ways that match human values and intentions. A key concern in AI safety...", "l": "a", "k": ["alignment", "challenge", "ensuring", "systems", "behave", "ways", "match", "human", "values", "intentions", "key", "concern", "safety", "research", "involving"]}, {"id": "term-alignment-problem", "t": "Alignment Problem", "tg": ["Safety", "Fundamentals"], "d": "safety", "x": "The fundamental challenge of ensuring that AI systems pursue goals and exhibit behaviors that are consistent with human...", "l": "a", "k": ["alignment", "problem", "fundamental", "challenge", "ensuring", "systems", "pursue", "goals", "exhibit", "behaviors", "consistent", "human", "intentions", "values", "central"]}, {"id": "term-alignment-research", "t": "Alignment Research", "tg": ["Safety", "Technical"], "d": "safety", "x": "The scientific study of methods to ensure AI systems are aligned with human values and intentions. Includes work on...", "l": "a", "k": ["alignment", "research", "scientific", "study", "methods", "ensure", "systems", "aligned", "human", "values", "intentions", "includes", "work", "reward", "modeling"]}, {"id": "term-alignment-tax", "t": "Alignment Tax", "tg": ["Safety", "Technical"], "d": "safety", "x": "The additional cost in performance resources or development time required to make an AI system safe and aligned...", "l": "a", "k": ["alignment", "tax", "additional", "cost", "performance", "resources", "development", "time", "required", "system", "safe", "aligned", "compared", "unaligned", "version"]}, {"id": "term-all-gather", "t": "All-Gather Operation", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A collective communication pattern where each participant broadcasts its data to all others, so every participant ends...", "l": "a", "k": ["all-gather", "operation", "collective", "communication", "pattern", "participant", "broadcasts", "data", "others", "ends", "complete", "concatenated", "dataset", "fsdp", "reconstruct"]}, {"id": "term-all-minilm-l6-v2", "t": "all-MiniLM-L6-v2", "tg": ["Models", "Technical", "Embedding", "NLP"], "d": "models", "x": "A widely-used 6-layer sentence embedding model distilled from MiniLM that provides a strong balance of speed and...", "l": "a", "k": ["all-minilm-l6-v2", "widely-used", "6-layer", "sentence", "embedding", "model", "distilled", "minilm", "provides", "strong", "balance", "speed", "quality", "semantic", "search"]}, {"id": "term-all-reduce", "t": "All-Reduce Operation", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A collective communication pattern where all participating GPUs contribute data, perform a reduction operation...", "l": "a", "k": ["all-reduce", "operation", "collective", "communication", "pattern", "participating", "gpus", "contribute", "data", "perform", "reduction", "typically", "summation", "receive", "result"]}, {"id": "term-all-to-all-communication", "t": "All-to-All Communication", "tg": ["Networking", "Distributed Training"], "d": "hardware", "x": "Collective operation where every processor sends a unique message to every other processor. Used in expert parallelism...", "l": "a", "k": ["all-to-all", "communication", "collective", "operation", "processor", "sends", "unique", "message", "expert", "parallelism", "tokens", "routed", "different", "partitions", "across"]}, {"id": "term-allegro", "t": "Allegro", "tg": ["Models", "Scientific"], "d": "models", "x": "A strictly local equivariant neural network interatomic potential that achieves scalable molecular dynamics simulations...", "l": "a", "k": ["allegro", "strictly", "local", "equivariant", "neural", "network", "interatomic", "potential", "achieves", "scalable", "molecular", "dynamics", "simulations", "near-ab-initio", "accuracy"]}, {"id": "term-allen-institute-for-ai", "t": "Allen Institute for AI", "tg": ["History", "Organizations"], "d": "history", "x": "A research institute founded by Paul Allen in 2014 dedicated to conducting high-impact AI research and engineering....", "l": "a", "k": ["allen", "institute", "research", "founded", "paul", "dedicated", "conducting", "high-impact", "engineering", "known", "projects", "including", "semantic", "scholar", "ai2"]}, {"id": "term-allen-newell", "t": "Allen Newell", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1927-1992) who, together with Herbert Simon, developed the Logic Theorist and General...", "l": "a", "k": ["allen", "newell", "american", "computer", "scientist", "1927-1992", "together", "herbert", "simon", "developed", "logic", "theorist", "general", "problem", "solver"]}, {"id": "term-allocative-harm", "t": "Allocative Harm", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Harm that occurs when an AI system unfairly distributes resources, opportunities, or outcomes across different groups,...", "l": "a", "k": ["allocative", "harm", "occurs", "system", "unfairly", "distributes", "resources", "opportunities", "outcomes", "across", "different", "groups", "denying", "loans", "jobs"]}, {"id": "term-alma", "t": "ALMA", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "Advanced Language Model-based trAnslator that fine-tunes large language models for high-quality machine translation...", "l": "a", "k": ["alma", "advanced", "language", "model-based", "translator", "fine-tunes", "large", "models", "high-quality", "machine", "translation", "two-stage", "training", "approach"]}, {"id": "term-aloha", "t": "ALOHA", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "A low-cost bimanual teleoperation system and imitation learning framework for training precise dexterous robotic...", "l": "a", "k": ["aloha", "low-cost", "bimanual", "teleoperation", "system", "imitation", "learning", "framework", "training", "precise", "dexterous", "robotic", "manipulation", "policies", "human"]}, {"id": "term-alpac-report", "t": "ALPAC Report", "tg": ["History", "Milestones"], "d": "history", "x": "A 1966 report by the Automatic Language Processing Advisory Committee that concluded machine translation was not likely...", "l": "a", "k": ["alpac", "report", "automatic", "language", "processing", "advisory", "committee", "concluded", "machine", "translation", "likely", "reach", "quality", "human", "near"]}, {"id": "term-alpaca", "t": "Alpaca", "tg": ["Model", "Historical"], "d": "models", "x": "An early instruction-tuned version of Llama created by Stanford researchers. Demonstrated that instruction-following...", "l": "a", "k": ["alpaca", "early", "instruction-tuned", "version", "llama", "created", "stanford", "researchers", "demonstrated", "instruction-following", "achieved", "synthetic", "data", "low", "cost"]}, {"id": "term-alpaca-dataset", "t": "Alpaca Dataset", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A dataset of 52000 instruction-following examples generated by GPT-3.5 for training Stanford Alpaca. Demonstrated that...", "l": "a", "k": ["alpaca", "dataset", "instruction-following", "examples", "generated", "gpt-3", "training", "stanford", "demonstrated", "instruction", "tuning", "done", "synthetic", "data"]}, {"id": "term-alpacaeval", "t": "AlpacaEval", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "An automatic evaluation framework that compares model outputs against a reference model using LLM-based pairwise...", "l": "a", "k": ["alpacaeval", "automatic", "evaluation", "framework", "compares", "model", "outputs", "against", "reference", "llm-based", "pairwise", "judgments", "providing", "fast", "cost-effective"]}, {"id": "term-alpha-beta-pruning", "t": "Alpha-Beta Pruning", "tg": ["History", "Fundamentals"], "d": "history", "x": "An optimization of the minimax algorithm that eliminates branches of the game tree that cannot possibly influence the...", "l": "a", "k": ["alpha-beta", "pruning", "optimization", "minimax", "algorithm", "eliminates", "branches", "game", "tree", "cannot", "possibly", "influence", "final", "decision", "developed"]}, {"id": "term-alphacode", "t": "AlphaCode", "tg": ["Models", "Technical"], "d": "models", "x": "A code generation system by DeepMind that solves competitive programming problems at a human-competitive level....", "l": "a", "k": ["alphacode", "code", "generation", "system", "deepmind", "solves", "competitive", "programming", "problems", "human-competitive", "level", "generates", "millions", "candidates", "filters"]}, {"id": "term-alphafold", "t": "AlphaFold", "tg": ["History", "Milestones"], "d": "history", "x": "DeepMind's AI system that solved the protein structure prediction problem, winning CASP14 in 2020 and subsequently...", "l": "a", "k": ["alphafold", "deepmind", "system", "solved", "protein", "structure", "prediction", "problem", "winning", "casp14", "subsequently", "predicting", "structures", "nearly", "known"]}, {"id": "term-alphafold-2", "t": "AlphaFold 2", "tg": ["Models", "Scientific", "Fundamentals"], "d": "models", "x": "An improved protein structure prediction system from DeepMind that achieves atomic-level accuracy using attention-based...", "l": "a", "k": ["alphafold", "improved", "protein", "structure", "prediction", "system", "deepmind", "achieves", "atomic-level", "accuracy", "attention-based", "architectures", "won", "casp14", "competition"]}, {"id": "term-alphafold-3", "t": "AlphaFold 3", "tg": ["Models", "Scientific", "Fundamentals"], "d": "models", "x": "The third generation of DeepMind's protein structure prediction system that extends prediction to protein-ligand and...", "l": "a", "k": ["alphafold", "generation", "deepmind", "protein", "structure", "prediction", "system", "extends", "protein-ligand", "protein-dna", "protein-rna", "complexes"]}, {"id": "term-alphago", "t": "AlphaGo", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A DeepMind system that combined deep neural networks with Monte Carlo tree search to defeat world champion Go players....", "l": "a", "k": ["alphago", "deepmind", "system", "combined", "deep", "neural", "networks", "monte", "carlo", "tree", "search", "defeat", "world", "champion", "players"]}, {"id": "term-alphago-vs-lee-sedol", "t": "AlphaGo vs Lee Sedol", "tg": ["History", "Milestones"], "d": "history", "x": "The March 2016 match in which DeepMind's AlphaGo defeated world champion Go player Lee Sedol 4-1, a landmark...", "l": "a", "k": ["alphago", "lee", "sedol", "march", "match", "deepmind", "defeated", "world", "champion", "player", "4-1", "landmark", "achievement", "long", "considered"]}, {"id": "term-alphazero", "t": "AlphaZero", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A generalized version of AlphaGo that learns to play Go, chess, and shogi entirely through self-play without human game...", "l": "a", "k": ["alphazero", "generalized", "version", "alphago", "learns", "play", "chess", "shogi", "entirely", "self-play", "without", "human", "game", "data", "uses"]}, {"id": "term-alphazero-algorithm", "t": "AlphaZero Algorithm", "tg": ["Algorithms", "Fundamentals", "RL", "History"], "d": "algorithms", "x": "A reinforcement learning algorithm that masters board games through self-play without human knowledge. Combines Monte...", "l": "a", "k": ["alphazero", "algorithm", "reinforcement", "learning", "masters", "board", "games", "self-play", "without", "human", "knowledge", "combines", "monte", "carlo", "tree"]}, {"id": "term-alternative-hypothesis", "t": "Alternative Hypothesis", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The hypothesis that contradicts the null hypothesis in statistical testing, typically representing the effect or...", "l": "a", "k": ["alternative", "hypothesis", "contradicts", "null", "statistical", "testing", "typically", "representing", "effect", "difference", "researcher", "aims", "detect", "one-sided", "two-sided"]}, {"id": "term-alvinn", "t": "ALVINN", "tg": ["History", "Systems"], "d": "history", "x": "Autonomous Land Vehicle In a Neural Network developed by Dean Pomerleau at Carnegie Mellon in 1989. One of the earliest...", "l": "a", "k": ["alvinn", "autonomous", "land", "vehicle", "neural", "network", "developed", "dean", "pomerleau", "carnegie", "mellon", "earliest", "demonstrations", "networks", "driving"]}, {"id": "term-am-automated-mathematician", "t": "AM (Automated Mathematician)", "tg": ["History", "Systems"], "d": "history", "x": "An AI program written by Douglas Lenat in 1976 that discovered mathematical concepts by exploring modifications to...", "l": "a", "k": ["automated", "mathematician", "program", "written", "douglas", "lenat", "discovered", "mathematical", "concepts", "exploring", "modifications", "existing", "demonstrated", "discovery", "mathematics"]}, {"id": "term-amazon-bedrock", "t": "Amazon Bedrock", "tg": ["Platform", "Cloud"], "d": "general", "x": "AWS's managed service for accessing foundation models from multiple providers. Offers Claude, Llama, and other models...", "l": "a", "k": ["amazon", "bedrock", "aws", "managed", "service", "accessing", "foundation", "models", "multiple", "providers", "offers", "claude", "llama", "unified", "api"]}, {"id": "term-amazon-graviton", "t": "Amazon Graviton", "tg": ["Processor", "AWS", "ARM"], "d": "hardware", "x": "AWS custom ARM-based server processor designed for cost-effective cloud computing. While not AI-specific Graviton...", "l": "a", "k": ["amazon", "graviton", "aws", "custom", "arm-based", "server", "processor", "designed", "cost-effective", "cloud", "computing", "ai-specific", "instances", "handle", "inference"]}, {"id": "term-amazon-reviews", "t": "Amazon Reviews", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A massive collection of product reviews from Amazon spanning multiple categories and languages. One of the largest...", "l": "a", "k": ["amazon", "reviews", "massive", "collection", "product", "spanning", "multiple", "categories", "languages", "largest", "sentiment", "analysis", "datasets", "ratings", "review"]}, {"id": "term-amber", "t": "Amber", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A fully open-source 7B language model from LLM360 with complete transparency including all training data and...", "l": "a", "k": ["amber", "fully", "open-source", "language", "model", "llm360", "complete", "transparency", "including", "training", "data", "intermediate", "checkpoints", "code"]}, {"id": "term-amd-cdna-architecture", "t": "AMD CDNA Architecture", "tg": ["GPU", "AMD", "Architecture"], "d": "hardware", "x": "AMD compute-focused GPU architecture specifically designed for data center AI and HPC workloads. Separates...", "l": "a", "k": ["amd", "cdna", "architecture", "compute-focused", "gpu", "specifically", "designed", "data", "center", "hpc", "workloads", "separates", "compute-optimized", "design", "gaming-focused"]}, {"id": "term-amd-epyc", "t": "AMD EPYC", "tg": ["Processor", "AMD", "Server"], "d": "hardware", "x": "AMD server processor family using chiplet architecture with up to 128 cores per socket. Used in many AI training...", "l": "a", "k": ["amd", "epyc", "server", "processor", "family", "chiplet", "architecture", "cores", "per", "socket", "training", "servers", "supercomputers", "alongside", "gpu"]}, {"id": "term-amd-infinity-fabric", "t": "AMD Infinity Fabric", "tg": ["Interconnect", "AMD", "Architecture"], "d": "hardware", "x": "AMD high-bandwidth low-latency interconnect technology connecting chiplets within AMD processors and between CPUs and...", "l": "a", "k": ["amd", "infinity", "fabric", "high-bandwidth", "low-latency", "interconnect", "technology", "connecting", "chiplets", "within", "processors", "cpus", "gpus", "enables", "scalable"]}, {"id": "term-amd-instinct-mi325x", "t": "AMD Instinct MI325X", "tg": ["GPU", "AMD", "Data Center"], "d": "hardware", "x": "AMD data center GPU accelerator based on CDNA 3 architecture offering 256GB HBM3 memory for large AI model training and...", "l": "a", "k": ["amd", "instinct", "mi325x", "data", "center", "gpu", "accelerator", "based", "cdna", "architecture", "offering", "256gb", "hbm3", "memory", "large"]}, {"id": "term-amd-instinct-platform", "t": "AMD Instinct Platform", "tg": ["Platform", "AMD", "Data Center"], "d": "hardware", "x": "AMD complete data center AI accelerator platform combining Instinct GPUs with ROCm software stack. Positioned as the...", "l": "a", "k": ["amd", "instinct", "platform", "complete", "data", "center", "accelerator", "combining", "gpus", "rocm", "software", "stack", "positioned", "open-source", "alternative"]}, {"id": "term-amd-mi300a", "t": "AMD MI300A", "tg": ["GPU", "AMD", "APU"], "d": "hardware", "x": "AMD accelerated processing unit combining CDNA 3 GPU and Zen 4 CPU chiplets with 128GB HBM3 in a single package. Used...", "l": "a", "k": ["amd", "mi300a", "accelerated", "processing", "unit", "combining", "cdna", "gpu", "zen", "cpu", "chiplets", "128gb", "hbm3", "single", "package"]}, {"id": "term-amd-mi300x", "t": "AMD MI300X", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "AMD's data center GPU accelerator featuring 192GB HBM3 memory and CDNA 3 architecture, competing with NVIDIA's H100 for...", "l": "a", "k": ["amd", "mi300x", "data", "center", "gpu", "accelerator", "featuring", "192gb", "hbm3", "memory", "cdna", "architecture", "competing", "nvidia", "h100"]}, {"id": "term-amd-radeon-instinct-mi100", "t": "AMD Radeon Instinct MI100", "tg": ["GPU", "AMD", "Data Center"], "d": "hardware", "x": "AMD first CDNA architecture data center GPU with 120 compute units and 32GB HBM2 memory. Represented AMD serious entry...", "l": "a", "k": ["amd", "radeon", "instinct", "mi100", "cdna", "architecture", "data", "center", "gpu", "compute", "units", "32gb", "hbm2", "memory", "represented"]}, {"id": "term-amd-radeon-instinct-mi210", "t": "AMD Radeon Instinct MI210", "tg": ["GPU", "AMD", "Data Center"], "d": "hardware", "x": "AMD CDNA 2 data center GPU with 104 compute units and 64GB HBM2e memory. Positioned as a mainstream alternative to...", "l": "a", "k": ["amd", "radeon", "instinct", "mi210", "cdna", "data", "center", "gpu", "compute", "units", "64gb", "hbm2e", "memory", "positioned", "mainstream"]}, {"id": "term-amd-radeon-instinct-mi250x", "t": "AMD Radeon Instinct MI250X", "tg": ["GPU", "AMD", "Data Center"], "d": "hardware", "x": "AMD CDNA 2 architecture data center GPU accelerator with 220 compute units and 128GB HBM2e memory. Used in the Frontier...", "l": "a", "k": ["amd", "radeon", "instinct", "mi250x", "cdna", "architecture", "data", "center", "gpu", "accelerator", "compute", "units", "128gb", "hbm2e", "memory"]}, {"id": "term-amd-radeon-instinct-mi50", "t": "AMD Radeon Instinct MI50", "tg": ["GPU", "AMD", "Data Center"], "d": "hardware", "x": "AMD Vega architecture data center GPU accelerator with 60 compute units and 16GB HBM2. Was AMD early attempt at...", "l": "a", "k": ["amd", "radeon", "instinct", "mi50", "vega", "architecture", "data", "center", "gpu", "accelerator", "compute", "units", "16gb", "hbm2", "early"]}, {"id": "term-amd-radeon-rx-7900-xtx", "t": "AMD Radeon RX 7900 XTX", "tg": ["GPU", "AMD", "Consumer"], "d": "hardware", "x": "AMD flagship consumer GPU based on RDNA 3 architecture with 96 compute units and 24GB GDDR6 memory. Offers an...", "l": "a", "k": ["amd", "radeon", "xtx", "flagship", "consumer", "gpu", "based", "rdna", "architecture", "compute", "units", "24gb", "gddr6", "memory", "offers"]}, {"id": "term-amd-rdna-architecture", "t": "AMD RDNA Architecture", "tg": ["GPU", "AMD", "Architecture"], "d": "hardware", "x": "AMD GPU architecture optimized for gaming and graphics with compute capabilities. RDNA 3 introduced chiplet-based...", "l": "a", "k": ["amd", "rdna", "architecture", "gpu", "optimized", "gaming", "graphics", "compute", "capabilities", "introduced", "chiplet-based", "design", "separating", "memory", "controller"]}, {"id": "term-amd-zen-architecture", "t": "AMD Zen Architecture", "tg": ["Processor", "AMD", "Architecture"], "d": "hardware", "x": "AMD CPU microarchitecture that revived AMD competitiveness in the processor market. Zen-based EPYC server processors...", "l": "a", "k": ["amd", "zen", "architecture", "cpu", "microarchitecture", "revived", "competitiveness", "processor", "market", "zen-based", "epyc", "server", "processors", "power", "modern"]}, {"id": "term-amdahls-law", "t": "Amdahl's Law", "tg": ["Architecture", "Principle", "Fundamentals"], "d": "hardware", "x": "Principle stating that the speedup from parallelization is limited by the sequential fraction of the computation. A...", "l": "a", "k": ["amdahl", "law", "principle", "stating", "speedup", "parallelization", "limited", "sequential", "fraction", "computation", "fundamental", "consideration", "designing", "parallel", "training"]}, {"id": "term-americasnli", "t": "AmericasNLI", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "A natural language inference benchmark for 10 indigenous languages of the Americas. Tests cross-lingual transfer to...", "l": "a", "k": ["americasnli", "natural", "language", "inference", "benchmark", "indigenous", "languages", "americas", "tests", "cross-lingual", "transfer", "extremely", "low-resource"]}, {"id": "term-ami-meeting-corpus", "t": "AMI Meeting Corpus", "tg": ["Benchmark", "Speech", "Multimodal"], "d": "datasets", "x": "A multi-modal dataset of 100 hours of meeting recordings with audio video and text annotations. Used for research in...", "l": "a", "k": ["ami", "meeting", "corpus", "multi-modal", "dataset", "hours", "recordings", "audio", "video", "text", "annotations", "research", "understanding", "speech", "recognition"]}, {"id": "term-amplification", "t": "Amplification", "tg": ["Safety", "Technical"], "d": "safety", "x": "A scalable oversight technique where human supervisors are augmented by AI assistants to evaluate AI behavior on...", "l": "a", "k": ["amplification", "scalable", "oversight", "technique", "human", "supervisors", "augmented", "assistants", "evaluate", "behavior", "complex", "tasks", "proposed", "christiano", "maintain"]}, {"id": "term-analog-ai-chip", "t": "Analog AI Chip", "tg": ["Emerging", "Analog", "Architecture"], "d": "hardware", "x": "Processor that uses analog voltage levels to represent neural network weights performing computation through physical...", "l": "a", "k": ["analog", "chip", "processor", "uses", "voltage", "levels", "represent", "neural", "network", "weights", "performing", "computation", "physical", "properties", "circuits"]}, {"id": "term-analog-computing-for-ai", "t": "Analog Computing for AI", "tg": ["Emerging", "Architecture", "Analog"], "d": "hardware", "x": "Computing approach using continuous physical quantities rather than discrete digital values to perform calculations....", "l": "a", "k": ["analog", "computing", "approach", "continuous", "physical", "quantities", "rather", "discrete", "digital", "values", "perform", "calculations", "offers", "potential", "extremely"]}, {"id": "term-analogical-prompting", "t": "Analogical Prompting", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A technique that asks the model to generate relevant analogous problems and their solutions before tackling the target...", "l": "a", "k": ["analogical", "prompting", "technique", "asks", "model", "generate", "relevant", "analogous", "problems", "solutions", "tackling", "target", "problem", "leveraging", "self-generated"]}, {"id": "term-anaphora-resolution", "t": "Anaphora Resolution", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of determining which previously mentioned entity a pronoun or other referring expression points back to in a...", "l": "a", "k": ["anaphora", "resolution", "task", "determining", "previously", "mentioned", "entity", "pronoun", "referring", "expression", "points", "text", "subproblem", "coreference"]}, {"id": "term-anchor", "t": "Anchor (Prompting)", "tg": ["Prompting", "Technique"], "d": "general", "x": "A reference point or example in a prompt that guides the AI's response style or format. Anchors help establish...", "l": "a", "k": ["anchor", "prompting", "reference", "point", "example", "prompt", "guides", "response", "style", "format", "anchors", "help", "establish", "expectations", "output"]}, {"id": "term-anchor-box", "t": "Anchor Box", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A predefined set of bounding boxes with various aspect ratios and scales placed at each spatial location in a feature...", "l": "a", "k": ["anchor", "box", "predefined", "bounding", "boxes", "various", "aspect", "ratios", "scales", "placed", "spatial", "location", "feature", "map", "serving"]}, {"id": "term-anchoring-bias-in-ai", "t": "Anchoring Bias in AI", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "A cognitive bias where initial AI-generated suggestions disproportionately influence subsequent human decisions,...", "l": "a", "k": ["anchoring", "bias", "cognitive", "initial", "ai-generated", "suggestions", "disproportionately", "influence", "subsequent", "human", "decisions", "causing", "users", "adjust", "insufficiently"]}, {"id": "term-anchoring-effect-in-ai", "t": "Anchoring Effect in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "A cognitive bias where users over-rely on the first piece of information provided by an AI system. Can lead to poor...", "l": "a", "k": ["anchoring", "effect", "cognitive", "bias", "users", "over-rely", "piece", "information", "provided", "system", "lead", "poor", "decision-making", "outputs", "presented"]}, {"id": "term-andrej-karpathy", "t": "Andrej Karpathy", "tg": ["History", "Pioneers"], "d": "history", "x": "Slovak-Canadian AI researcher who led computer vision at Tesla Autopilot and later worked at OpenAI. Known for...", "l": "a", "k": ["andrej", "karpathy", "slovak-canadian", "researcher", "led", "computer", "vision", "tesla", "autopilot", "later", "worked", "openai", "known", "educational", "contributions"]}, {"id": "term-andrew-barto", "t": "Andrew Barto", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who co-authored the influential textbook Reinforcement Learning: An Introduction with...", "l": "a", "k": ["andrew", "barto", "american", "computer", "scientist", "co-authored", "influential", "textbook", "reinforcement", "learning", "introduction", "richard", "sutton", "pioneer", "university"]}, {"id": "term-andrew-ng", "t": "Andrew Ng", "tg": ["History", "Pioneers"], "d": "history", "x": "British-American computer scientist who co-founded Google Brain, led AI at Baidu, founded Coursera and deeplearning.ai,...", "l": "a", "k": ["andrew", "british-american", "computer", "scientist", "co-founded", "google", "brain", "led", "baidu", "founded", "coursera", "deeplearning", "popularized", "deep", "learning"]}, {"id": "term-ani-2x", "t": "ANI-2x", "tg": ["Models", "Scientific"], "d": "models", "x": "A neural network potential trained on diverse organic molecules that provides fast and accurate energy and force...", "l": "a", "k": ["ani-2x", "neural", "network", "potential", "trained", "diverse", "organic", "molecules", "provides", "fast", "accurate", "energy", "force", "predictions", "molecular"]}, {"id": "term-animate-a-story", "t": "Animate-A-Story", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A framework for storytelling video generation that creates animated narratives from text by combining character...", "l": "a", "k": ["animate-a-story", "framework", "storytelling", "video", "generation", "creates", "animated", "narratives", "text", "combining", "character", "consistency", "scene", "transitions"]}, {"id": "term-animateanyone", "t": "AnimateAnyone", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A character animation model that generates realistic videos of a person from a single image guided by a sequence of...", "l": "a", "k": ["animateanyone", "character", "animation", "model", "generates", "realistic", "videos", "person", "single", "image", "guided", "sequence", "body", "poses"]}, {"id": "term-animatediff", "t": "AnimateDiff", "tg": ["Models", "Technical"], "d": "models", "x": "A framework for animating personalized text-to-image models by inserting motion modules into the existing architecture....", "l": "a", "k": ["animatediff", "framework", "animating", "personalized", "text-to-image", "models", "inserting", "motion", "modules", "existing", "architecture", "enables", "video", "generation", "fine-tuned"]}, {"id": "term-anli", "t": "ANLI", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Adversarial NLI a natural language inference benchmark created through iterative human-and-model-in-the-loop...", "l": "a", "k": ["anli", "adversarial", "nli", "natural", "language", "inference", "benchmark", "created", "iterative", "human-and-model-in-the-loop", "annotation", "rounds", "increasingly", "difficult", "examples"]}, {"id": "term-ann-benchmark", "t": "ANN Benchmark", "tg": ["Vector Database", "Evaluation"], "d": "datasets", "x": "Standardized evaluation suites for comparing approximate nearest neighbor algorithms across metrics like recall,...", "l": "a", "k": ["ann", "benchmark", "standardized", "evaluation", "suites", "comparing", "approximate", "nearest", "neighbor", "algorithms", "across", "metrics", "recall", "queries", "per"]}, {"id": "term-annotation", "t": "Annotation", "tg": ["Data", "Training"], "d": "general", "x": "The process of labeling data to create training datasets for supervised learning. Human annotators add labels,...", "l": "a", "k": ["annotation", "process", "labeling", "data", "create", "training", "datasets", "supervised", "learning", "human", "annotators", "add", "labels", "categories", "descriptions"]}, {"id": "term-annotation-labor-ethics", "t": "Annotation Labor Ethics", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "Ethical concerns about the working conditions, compensation, and psychological impacts experienced by data annotation...", "l": "a", "k": ["annotation", "labor", "ethics", "ethical", "concerns", "working", "conditions", "compensation", "psychological", "impacts", "experienced", "data", "workers", "label", "training"]}, {"id": "term-annoy", "t": "Annoy", "tg": ["Vector Database", "Libraries"], "d": "general", "x": "Approximate Nearest Neighbors Oh Yeah, an open-source library by Spotify that builds forest-of-trees indexes using...", "l": "a", "k": ["annoy", "approximate", "nearest", "neighbors", "yeah", "open-source", "library", "spotify", "builds", "forest-of-trees", "indexes", "random", "hyperplane", "splits", "fast"]}, {"id": "term-annoy-algorithm", "t": "Annoy Algorithm", "tg": ["Algorithms", "Technical", "Searching", "Data Structure"], "d": "algorithms", "x": "Approximate Nearest Neighbors Oh Yeah builds a forest of random projection trees for fast approximate nearest-neighbor...", "l": "a", "k": ["annoy", "algorithm", "approximate", "nearest", "neighbors", "yeah", "builds", "forest", "random", "projection", "trees", "fast", "nearest-neighbor", "search", "memory-maps"]}, {"id": "term-anomaly-detection", "t": "Anomaly Detection", "tg": ["ML Task", "Application"], "d": "general", "x": "Identifying unusual patterns or outliers in data that don't conform to expected behavior. Used in fraud detection,...", "l": "a", "k": ["anomaly", "detection", "identifying", "unusual", "patterns", "outliers", "data", "don", "conform", "expected", "behavior", "fraud", "system", "monitoring", "quality"]}, {"id": "term-anomaly-detection-for-safety", "t": "Anomaly Detection for Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "The use of anomaly detection techniques to identify unusual or potentially unsafe AI system behavior in deployment....", "l": "a", "k": ["anomaly", "detection", "safety", "techniques", "identify", "unusual", "potentially", "unsafe", "system", "behavior", "deployment", "serves", "early", "warning", "distribution"]}, {"id": "term-anomaly-detection-images", "t": "Anomaly Detection in Images", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of identifying unusual patterns, defects, or out-of-distribution samples in images, widely used in industrial...", "l": "a", "k": ["anomaly", "detection", "images", "task", "identifying", "unusual", "patterns", "defects", "out-of-distribution", "samples", "widely", "industrial", "quality", "inspection", "medical"]}, {"id": "term-anova", "t": "ANOVA", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "Analysis of Variance, a statistical method that tests whether the means of three or more groups are significantly...", "l": "a", "k": ["anova", "analysis", "variance", "statistical", "method", "tests", "means", "groups", "significantly", "different", "comparing", "within-group", "between-group", "f-statistic"]}, {"id": "term-answer-engineering", "t": "Answer Engineering", "tg": ["Prompting", "Technique"], "d": "general", "x": "Designing prompts to elicit specific response formats or structured outputs. Complements prompt engineering by focusing...", "l": "a", "k": ["answer", "engineering", "designing", "prompts", "elicit", "specific", "response", "formats", "structured", "outputs", "complements", "prompt", "focusing", "answers"]}, {"id": "term-ant-colony-optimization", "t": "Ant Colony Optimization", "tg": ["History", "Milestones"], "d": "history", "x": "A metaheuristic optimization algorithm proposed by Marco Dorigo in 1992, inspired by the foraging behavior of ants...", "l": "a", "k": ["ant", "colony", "optimization", "metaheuristic", "algorithm", "proposed", "marco", "dorigo", "inspired", "foraging", "behavior", "ants", "pheromone", "trails", "applied"]}, {"id": "term-anthropic", "t": "Anthropic", "tg": ["Company", "LLM Provider"], "d": "models", "x": "An AI safety company founded in 2021 by former OpenAI researchers. Creator of the Claude family of AI assistants,...", "l": "a", "k": ["anthropic", "safety", "company", "founded", "former", "openai", "researchers", "creator", "claude", "family", "assistants", "focused", "developing", "safe", "beneficial"]}, {"id": "term-anthropic-founding", "t": "Anthropic Founding", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of Anthropic in 2021 by former OpenAI researchers Dario and Daniela Amodei, establishing a safety-focused...", "l": "a", "k": ["anthropic", "founding", "former", "openai", "researchers", "dario", "daniela", "amodei", "establishing", "safety-focused", "company", "developed", "constitutional", "claude", "family"]}, {"id": "term-anthropic-hh-rlhf", "t": "Anthropic HH-RLHF", "tg": ["Training Corpus", "Safety"], "d": "datasets", "x": "A dataset of human preference data used for training helpful and harmless AI assistants through reinforcement learning...", "l": "a", "k": ["anthropic", "hh-rlhf", "dataset", "human", "preference", "data", "training", "helpful", "harmless", "assistants", "reinforcement", "learning", "feedback", "contains", "comparisons"]}, {"id": "term-anthropic-model-card-dataset", "t": "Anthropic Model Card Dataset", "tg": ["Evaluation", "Safety"], "d": "datasets", "x": "A collection of model evaluation data published alongside Anthropic model releases documenting capabilities limitations...", "l": "a", "k": ["anthropic", "model", "card", "dataset", "collection", "evaluation", "data", "published", "alongside", "releases", "documenting", "capabilities", "limitations", "safety", "evaluations"]}, {"id": "term-anthropic-persuasion-dataset", "t": "Anthropic Persuasion Dataset", "tg": ["Benchmark", "Safety", "NLP"], "d": "datasets", "x": "A dataset measuring the persuasive capabilities of AI language models across different argumentation strategies. Used...", "l": "a", "k": ["anthropic", "persuasion", "dataset", "measuring", "persuasive", "capabilities", "language", "models", "across", "different", "argumentation", "strategies", "evaluating", "potential", "misuse"]}, {"id": "term-anthropomorphism-risk", "t": "Anthropomorphism Risk", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The danger that designing AI systems with human-like characteristics leads users to attribute human emotions intentions...", "l": "a", "k": ["anthropomorphism", "risk", "danger", "designing", "systems", "human-like", "characteristics", "leads", "users", "attribute", "human", "emotions", "intentions", "reliability", "lack"]}, {"id": "term-anti-discrimination-law-in-ai", "t": "Anti-Discrimination Law in AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "Legal frameworks that prohibit AI systems from discriminating against individuals based on protected characteristics...", "l": "a", "k": ["anti-discrimination", "law", "legal", "frameworks", "prohibit", "systems", "discriminating", "against", "individuals", "based", "protected", "characteristics", "race", "gender", "age"]}, {"id": "term-anydoor", "t": "AnyDoor", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A zero-shot object-level image customization model that teleports target objects into new scenes while preserving...", "l": "a", "k": ["anydoor", "zero-shot", "object-level", "image", "customization", "model", "teleports", "target", "objects", "scenes", "preserving", "identity", "adapting", "scene", "context"]}, {"id": "term-apache-tvm", "t": "Apache TVM", "tg": ["Compiler", "Open Source", "Optimization"], "d": "hardware", "x": "Open-source deep learning compiler framework that generates optimized code for diverse hardware backends. Supports CPUs...", "l": "a", "k": ["apache", "tvm", "open-source", "deep", "learning", "compiler", "framework", "generates", "optimized", "code", "diverse", "hardware", "backends", "supports", "cpus"]}, {"id": "term-api", "t": "API (Application Programming Interface)", "tg": ["Technical", "Integration"], "d": "general", "x": "A set of protocols that allows different software applications to communicate. AI APIs enable developers to integrate...", "l": "a", "k": ["api", "application", "programming", "interface", "protocols", "allows", "different", "software", "applications", "communicate", "apis", "enable", "developers", "integrate", "capabilities"]}, {"id": "term-apibench", "t": "APIBench", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating the ability of LLMs to generate correct API calls. Contains thousands of API specifications...", "l": "a", "k": ["apibench", "benchmark", "evaluating", "ability", "llms", "generate", "correct", "api", "calls", "contains", "thousands", "specifications", "corresponding", "natural", "language"]}, {"id": "term-apolloscape", "t": "ApolloScape", "tg": ["Benchmark", "Autonomous Driving"], "d": "datasets", "x": "A large-scale autonomous driving dataset from Baidu containing per-pixel semantic labeling 3D car instance...", "l": "a", "k": ["apolloscape", "large-scale", "autonomous", "driving", "dataset", "baidu", "containing", "per-pixel", "semantic", "labeling", "car", "instance", "understanding", "self-localization", "data"]}, {"id": "term-apple-a17-pro", "t": "Apple A17 Pro", "tg": ["Mobile", "Apple", "SoC"], "d": "hardware", "x": "Apple mobile processor for iPhone 15 Pro with a 6-core GPU and 16-core Neural Engine. Enables on-device AI features...", "l": "a", "k": ["apple", "a17", "pro", "mobile", "processor", "iphone", "6-core", "gpu", "16-core", "neural", "engine", "enables", "on-device", "features", "including"]}, {"id": "term-apple-m1-chip", "t": "Apple M1 Chip", "tg": ["Mobile", "Apple", "SoC"], "d": "hardware", "x": "Apple first custom ARM-based system-on-chip for Mac computers featuring a 16-core Neural Engine capable of 11 trillion...", "l": "a", "k": ["apple", "chip", "custom", "arm-based", "system-on-chip", "mac", "computers", "featuring", "16-core", "neural", "engine", "capable", "trillion", "operations", "per"]}, {"id": "term-apple-m2-chip", "t": "Apple M2 Chip", "tg": ["Mobile", "Apple", "SoC"], "d": "hardware", "x": "Second generation Apple silicon for Mac computers with an improved 16-core Neural Engine achieving 15.8 TOPS for...", "l": "a", "k": ["apple", "chip", "generation", "silicon", "mac", "computers", "improved", "16-core", "neural", "engine", "achieving", "tops", "accelerated", "machine", "learning"]}, {"id": "term-apple-m3-chip", "t": "Apple M3 Chip", "tg": ["Mobile", "Apple", "SoC"], "d": "hardware", "x": "Third generation Apple silicon built on TSMC 3nm process featuring enhanced Neural Engine and GPU with...", "l": "a", "k": ["apple", "chip", "generation", "silicon", "built", "tsmc", "3nm", "process", "featuring", "enhanced", "neural", "engine", "gpu", "hardware-accelerated", "ray"]}, {"id": "term-apple-m4-chip", "t": "Apple M4 Chip", "tg": ["Mobile", "Apple", "SoC"], "d": "hardware", "x": "Fourth generation Apple silicon with a substantially upgraded Neural Engine achieving 38 TOPS for on-device AI....", "l": "a", "k": ["apple", "chip", "fourth", "generation", "silicon", "substantially", "upgraded", "neural", "engine", "achieving", "tops", "on-device", "designed", "handle", "advanced"]}, {"id": "term-apple-neural-engine", "t": "Apple Neural Engine", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "Apple's dedicated neural network accelerator integrated into Apple Silicon chips (M-series and A-series), delivering up...", "l": "a", "k": ["apple", "neural", "engine", "dedicated", "network", "accelerator", "integrated", "silicon", "chips", "m-series", "a-series", "delivering", "tops", "on-device", "inference"]}, {"id": "term-applied-materials", "t": "Applied Materials", "tg": ["Manufacturing", "Equipment", "Company"], "d": "hardware", "x": "American company and the world largest semiconductor equipment manufacturer. Produces machines for deposition etching...", "l": "a", "k": ["applied", "materials", "american", "company", "world", "largest", "semiconductor", "equipment", "manufacturer", "produces", "machines", "deposition", "etching", "inspection", "fabricating"]}, {"id": "term-appropriate-reliance", "t": "Appropriate Reliance", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The calibrated level of trust humans should place in AI systems, avoiding both over-reliance that leads to automation...", "l": "a", "k": ["appropriate", "reliance", "calibrated", "level", "trust", "humans", "place", "systems", "avoiding", "over-reliance", "leads", "automation", "complacency", "under-reliance", "fails"]}, {"id": "term-approximate-nearest-neighbor", "t": "Approximate Nearest Neighbor", "tg": ["Vector Database", "Search"], "d": "general", "x": "A class of search algorithms that find vectors approximately closest to a query vector with high probability rather...", "l": "a", "k": ["approximate", "nearest", "neighbor", "class", "search", "algorithms", "find", "vectors", "approximately", "closest", "query", "vector", "high", "probability", "rather"]}, {"id": "term-approximate-nearest-neighbor-search", "t": "Approximate Nearest Neighbor Search", "tg": ["Algorithms", "Fundamentals", "Searching"], "d": "algorithms", "x": "A class of algorithms that find points close to a query point without guaranteeing the exact nearest neighbor. Trade...", "l": "a", "k": ["approximate", "nearest", "neighbor", "search", "class", "algorithms", "find", "points", "close", "query", "point", "without", "guaranteeing", "exact", "trade"]}, {"id": "term-apps", "t": "APPS", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "Automated Programming Progress Standard a benchmark of 10000 coding problems at varying difficulty levels from...", "l": "a", "k": ["apps", "automated", "programming", "progress", "standard", "benchmark", "coding", "problems", "varying", "difficulty", "levels", "introductory", "competition", "level", "tests"]}, {"id": "term-arc", "t": "ARC", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "The AI2 Reasoning Challenge a dataset of 7787 science exam questions divided into easy and challenge sets. Tests...", "l": "a", "k": ["arc", "ai2", "reasoning", "challenge", "dataset", "science", "exam", "questions", "divided", "easy", "sets", "tests", "scientific", "world", "knowledge"]}, {"id": "term-arc-benchmark", "t": "ARC Benchmark", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "The AI2 Reasoning Challenge, a question-answering benchmark consisting of elementary and middle school science exam...", "l": "a", "k": ["arc", "benchmark", "ai2", "reasoning", "challenge", "question-answering", "consisting", "elementary", "middle", "school", "science", "exam", "questions", "easy", "sets"]}, {"id": "term-arc-challenge", "t": "ARC Challenge", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "The difficult subset of the ARC benchmark containing 2590 science questions that require complex reasoning beyond...", "l": "a", "k": ["arc", "challenge", "difficult", "subset", "benchmark", "containing", "science", "questions", "require", "complex", "reasoning", "beyond", "simple", "retrieval", "frequently"]}, {"id": "term-arc-agi", "t": "ARC-AGI", "tg": ["Benchmark", "Reasoning", "Evaluation"], "d": "datasets", "x": "A benchmark inspired by the Abstraction and Reasoning Corpus testing pattern recognition and abstract reasoning in AI...", "l": "a", "k": ["arc-agi", "benchmark", "inspired", "abstraction", "reasoning", "corpus", "testing", "pattern", "recognition", "abstract", "systems", "designed", "evaluate", "general", "intelligence"]}, {"id": "term-arcade-learning-environment", "t": "Arcade Learning Environment", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "A framework providing an interface to hundreds of Atari 2600 game environments for reinforcement learning research....", "l": "a", "k": ["arcade", "learning", "environment", "framework", "providing", "interface", "hundreds", "atari", "game", "environments", "reinforcement", "research", "standardized", "evaluation", "platform"]}, {"id": "term-arcface", "t": "ArcFace", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A face recognition loss function that adds an angular margin penalty in the normalized feature space, improving the...", "l": "a", "k": ["arcface", "face", "recognition", "loss", "function", "adds", "angular", "margin", "penalty", "normalized", "feature", "space", "improving", "discriminative", "power"]}, {"id": "term-architecture-search", "t": "Architecture Search (NAS)", "tg": ["Research", "Optimization"], "d": "algorithms", "x": "Automated methods for discovering optimal neural network architectures. Can find better designs than human-created...", "l": "a", "k": ["architecture", "search", "nas", "automated", "methods", "discovering", "optimal", "neural", "network", "architectures", "find", "better", "designs", "human-created", "networks"]}, {"id": "term-arena-score", "t": "Arena Score", "tg": ["Evaluation", "Ranking"], "d": "datasets", "x": "A model ranking metric derived from competitive evaluation platforms where models are compared in blind pairwise...", "l": "a", "k": ["arena", "score", "model", "ranking", "metric", "derived", "competitive", "evaluation", "platforms", "models", "compared", "blind", "pairwise", "matchups", "human"]}, {"id": "term-arena-hard", "t": "Arena-Hard", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A challenging benchmark derived from Chatbot Arena conversations containing 500 difficult prompts. Uses pairwise...", "l": "a", "k": ["arena-hard", "challenging", "benchmark", "derived", "chatbot", "arena", "conversations", "containing", "difficult", "prompts", "uses", "pairwise", "comparisons", "against", "baseline"]}, {"id": "term-ares", "t": "ARES", "tg": ["Evaluation", "NLP"], "d": "datasets", "x": "Automated Retrieval-Augmented Generation Evaluation System providing automatic evaluation of RAG system performance...", "l": "a", "k": ["ares", "automated", "retrieval-augmented", "generation", "evaluation", "system", "providing", "automatic", "rag", "performance", "across", "retrieval", "quality", "metrics"]}, {"id": "term-argoverse", "t": "Argoverse", "tg": ["Benchmark", "Autonomous Driving"], "d": "datasets", "x": "A dataset for autonomous driving research providing 3D tracking and motion forecasting data with high-definition maps...", "l": "a", "k": ["argoverse", "dataset", "autonomous", "driving", "research", "providing", "tracking", "motion", "forecasting", "data", "high-definition", "maps", "pittsburgh", "miami", "includes"]}, {"id": "term-arima", "t": "ARIMA", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "AutoRegressive Integrated Moving Average, a class of time series models combining autoregression (AR), differencing for...", "l": "a", "k": ["arima", "autoregressive", "integrated", "moving", "average", "class", "time", "series", "models", "combining", "autoregression", "differencing", "stationarity", "components", "widely"]}, {"id": "term-arithmetic-coding-algorithm", "t": "Arithmetic Coding Algorithm", "tg": ["Algorithms", "Technical", "Information Theory"], "d": "algorithms", "x": "A lossless data compression method that encodes a sequence of symbols as a single number in the interval between zero...", "l": "a", "k": ["arithmetic", "coding", "algorithm", "lossless", "data", "compression", "method", "encodes", "sequence", "symbols", "single", "number", "interval", "zero", "achieves"]}, {"id": "term-arithmetic-intensity", "t": "Arithmetic Intensity", "tg": ["Hardware", "Model Optimization"], "d": "models", "x": "The ratio of floating-point operations to bytes of memory accessed in a computation, determining whether performance is...", "l": "a", "k": ["arithmetic", "intensity", "ratio", "floating-point", "operations", "bytes", "memory", "accessed", "computation", "determining", "performance", "limited", "compute", "bandwidth", "llm"]}, {"id": "term-arm-architecture", "t": "ARM Architecture", "tg": ["Architecture", "ARM", "Mobile"], "d": "hardware", "x": "Energy-efficient RISC processor architecture that dominates mobile computing and is expanding into servers and AI edge...", "l": "a", "k": ["arm", "architecture", "energy-efficient", "risc", "processor", "dominates", "mobile", "computing", "expanding", "servers", "edge", "devices", "licensed", "intellectual", "property"]}, {"id": "term-arm-cortex", "t": "ARM Cortex", "tg": ["Processor", "ARM", "Architecture"], "d": "hardware", "x": "ARM processor core family spanning from efficient Cortex-A5 to high-performance Cortex-X4 designs. ARM cores form the...", "l": "a", "k": ["arm", "cortex", "processor", "core", "family", "spanning", "efficient", "cortex-a5", "high-performance", "cortex-x4", "designs", "cores", "form", "foundation", "virtually"]}, {"id": "term-arm-ethos-npu", "t": "ARM Ethos NPU", "tg": ["NPU", "ARM", "Edge"], "d": "hardware", "x": "ARM dedicated neural processing unit IP designed for efficient AI inference in edge and IoT devices. Available in...", "l": "a", "k": ["arm", "ethos", "npu", "dedicated", "neural", "processing", "unit", "designed", "efficient", "inference", "edge", "iot", "devices", "available", "multiple"]}, {"id": "term-arm-mali-gpu", "t": "ARM Mali GPU", "tg": ["GPU", "ARM", "Mobile"], "d": "hardware", "x": "ARM GPU intellectual property used in mobile and embedded systems-on-chip. Newer Mali generations include machine...", "l": "a", "k": ["arm", "mali", "gpu", "intellectual", "property", "mobile", "embedded", "systems-on-chip", "newer", "generations", "include", "machine", "learning", "acceleration", "capabilities"]}, {"id": "term-arnoldi-iteration", "t": "Arnoldi Iteration", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An algorithm that builds an orthonormal basis of the Krylov subspace for non-symmetric matrices. Forms the foundation...", "l": "a", "k": ["arnoldi", "iteration", "algorithm", "builds", "orthonormal", "basis", "krylov", "subspace", "non-symmetric", "matrices", "forms", "foundation", "iterative", "methods", "gmres"]}, {"id": "term-arthur-samuel", "t": "Arthur Samuel", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1901-1990) who created a checkers-playing program at IBM in 1959 that learned through...", "l": "a", "k": ["arthur", "samuel", "american", "computer", "scientist", "1901-1990", "created", "checkers-playing", "program", "ibm", "learned", "self-play", "coining", "term", "machine"]}, {"id": "term-articulation-point-detection", "t": "Articulation Point Detection", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm that identifies vertices in an undirected graph whose removal increases the number of connected...", "l": "a", "k": ["articulation", "point", "detection", "algorithm", "identifies", "vertices", "undirected", "graph", "whose", "removal", "increases", "number", "connected", "components", "uses"]}, {"id": "term-artificial-bee-colony-algorithm", "t": "Artificial Bee Colony Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A swarm intelligence algorithm modeled on the foraging behavior of honey bee colonies. Employs employed bees and...", "l": "a", "k": ["artificial", "bee", "colony", "algorithm", "swarm", "intelligence", "modeled", "foraging", "behavior", "honey", "colonies", "employs", "employed", "bees", "onlooker"]}, {"id": "term-artificial-general-intelligence-history", "t": "Artificial General Intelligence History", "tg": ["History", "Milestones"], "d": "history", "x": "The pursuit of machines that can understand learn and apply knowledge across any intellectual task that humans can....", "l": "a", "k": ["artificial", "general", "intelligence", "history", "pursuit", "machines", "understand", "learn", "apply", "knowledge", "across", "intellectual", "task", "humans", "original"]}, {"id": "term-artificial-immune-system-algorithm", "t": "Artificial Immune System Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "An optimization framework inspired by the adaptive immune system of vertebrates. Models clonal selection and negative...", "l": "a", "k": ["artificial", "immune", "system", "algorithm", "optimization", "framework", "inspired", "adaptive", "vertebrates", "models", "clonal", "selection", "negative", "network", "theory"]}, {"id": "term-artificial-intelligence-a-modern-approach", "t": "Artificial Intelligence: A Modern Approach", "tg": ["History", "Milestones"], "d": "history", "x": "The standard AI textbook by Stuart Russell and Peter Norvig first published in 1995. Used in over 1500 universities in...", "l": "a", "k": ["artificial", "intelligence", "modern", "approach", "standard", "textbook", "stuart", "russell", "peter", "norvig", "published", "universities", "countries", "book", "covers"]}, {"id": "term-artificial-life", "t": "Artificial Life", "tg": ["History", "Fundamentals"], "d": "history", "x": "A field of study that examines systems related to natural life their processes and their evolution through the use of...", "l": "a", "k": ["artificial", "life", "field", "study", "examines", "systems", "related", "natural", "processes", "evolution", "simulations", "models", "pioneered", "christopher", "langton"]}, {"id": "term-artificial-neuron", "t": "Artificial Neuron", "tg": ["Architecture", "Fundamentals"], "d": "models", "x": "The basic computational unit in neural networks, loosely inspired by biological neurons. Computes a weighted sum of...", "l": "a", "k": ["artificial", "neuron", "basic", "computational", "unit", "neural", "networks", "loosely", "inspired", "biological", "neurons", "computes", "weighted", "sum", "inputs"]}, {"id": "term-arxiv-dataset", "t": "ArXiv Dataset", "tg": ["Training Corpus", "NLP", "Scientific"], "d": "datasets", "x": "A collection of over 2 million scientific preprints from the arXiv repository with metadata abstracts and full text....", "l": "a", "k": ["arxiv", "dataset", "collection", "million", "scientific", "preprints", "repository", "metadata", "abstracts", "full", "text", "nlp", "citation", "network", "research"]}, {"id": "term-asci-red", "t": "ASCI Red", "tg": ["Historical", "Supercomputer", "Intel"], "d": "hardware", "x": "First teraFLOPS supercomputer built by Intel for Sandia National Laboratories in 1996. Broke the trillion...", "l": "a", "k": ["asci", "red", "teraflops", "supercomputer", "built", "intel", "sandia", "national", "laboratories", "broke", "trillion", "floating-point", "operations", "per", "barrier"]}, {"id": "term-ashish-vaswani", "t": "Ashish Vaswani", "tg": ["History", "Pioneers"], "d": "history", "x": "Lead author of the 2017 Attention Is All You Need paper that introduced the transformer architecture at Google Brain,...", "l": "a", "k": ["ashish", "vaswani", "lead", "author", "attention", "need", "paper", "introduced", "transformer", "architecture", "google", "brain", "fundamentally", "changing", "trajectory"]}, {"id": "term-asic-ai", "t": "ASIC for AI", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "Application-Specific Integrated Circuits designed exclusively for AI computation, offering maximum performance and...", "l": "a", "k": ["asic", "application-specific", "integrated", "circuits", "designed", "exclusively", "computation", "offering", "maximum", "performance", "energy", "efficiency", "fixed", "workloads", "asics"]}, {"id": "term-asilomar-ai-principles", "t": "Asilomar AI Principles", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "A set of 23 principles for beneficial AI research developed at the 2017 Asilomar conference, covering research issues,...", "l": "a", "k": ["asilomar", "principles", "beneficial", "research", "developed", "conference", "covering", "issues", "ethics", "values", "longer-term", "concerns", "advanced", "safety"]}, {"id": "term-asml", "t": "ASML", "tg": ["Manufacturing", "Equipment", "Company"], "d": "hardware", "x": "Dutch company that is the sole manufacturer of extreme ultraviolet lithography machines required for producing the most...", "l": "a", "k": ["asml", "dutch", "company", "sole", "manufacturer", "extreme", "ultraviolet", "lithography", "machines", "required", "producing", "advanced", "semiconductor", "chips", "critical"]}, {"id": "term-aspect-based-sentiment", "t": "Aspect-Based Sentiment Analysis", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A fine-grained sentiment analysis task that identifies sentiment toward specific aspects or features of an entity,...", "l": "a", "k": ["aspect-based", "sentiment", "analysis", "fine-grained", "task", "identifies", "toward", "specific", "aspects", "features", "entity", "distinguishing", "different", "opinions", "attributes"]}, {"id": "term-assistant-message", "t": "Assistant Message", "tg": ["API", "Technical"], "d": "general", "x": "In chat APIs, the AI's response in a conversation. Combined with system and user messages to form the complete...", "l": "a", "k": ["assistant", "message", "chat", "apis", "response", "conversation", "combined", "system", "user", "messages", "form", "complete", "context", "generating", "next"]}, {"id": "term-assistive-ai-ethics", "t": "Assistive AI Ethics", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Ethical considerations specific to AI systems designed to assist people with disabilities. Includes concerns about...", "l": "a", "k": ["assistive", "ethics", "ethical", "considerations", "specific", "systems", "designed", "assist", "people", "disabilities", "includes", "concerns", "autonomy", "dignity", "dependency"]}, {"id": "term-a3c", "t": "Asynchronous Advantage Actor-Critic (A3C)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An actor-critic algorithm that runs multiple agent instances in parallel on separate environment copies, each...", "l": "a", "k": ["asynchronous", "advantage", "actor-critic", "a3c", "algorithm", "runs", "multiple", "agent", "instances", "parallel", "separate", "environment", "copies", "asynchronously", "updating"]}, {"id": "term-async-generation", "t": "Asynchronous Generation", "tg": ["Technical", "Production"], "d": "general", "x": "Running multiple AI inference requests in parallel rather than waiting for each to complete. Improves throughput for...", "l": "a", "k": ["asynchronous", "generation", "running", "multiple", "inference", "requests", "parallel", "rather", "waiting", "complete", "improves", "throughput", "applications", "handling", "concurrent"]}, {"id": "term-asynchronous-sgd", "t": "Asynchronous SGD", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A distributed training approach where workers compute and apply gradients independently without waiting for...", "l": "a", "k": ["asynchronous", "sgd", "distributed", "training", "approach", "workers", "compute", "apply", "gradients", "independently", "without", "waiting", "synchronization", "trading", "gradient"]}, {"id": "term-atanasoff-berry-computer", "t": "Atanasoff-Berry Computer", "tg": ["Historical", "Computer", "Pioneer"], "d": "hardware", "x": "Electronic computing device built by John Atanasoff and Clifford Berry in 1942. Generally recognized as the first...", "l": "a", "k": ["atanasoff-berry", "computer", "electronic", "computing", "device", "built", "john", "atanasoff", "clifford", "berry", "generally", "recognized", "digital", "programmable"]}, {"id": "term-atari-2600-games", "t": "Atari 2600 Games", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "A collection of Atari 2600 video games used as reinforcement learning environments through the Arcade Learning...", "l": "a", "k": ["atari", "games", "collection", "video", "reinforcement", "learning", "environments", "arcade", "environment", "popularized", "deepmind", "dqn", "achieving", "human-level", "play"]}, {"id": "term-atis", "t": "ATIS", "tg": ["Benchmark", "NLP", "Speech"], "d": "datasets", "x": "Airline Travel Information Systems a dataset of spoken language queries about air travel annotated for intent and...", "l": "a", "k": ["atis", "airline", "travel", "information", "systems", "dataset", "spoken", "language", "queries", "air", "annotated", "intent", "slots", "classic", "benchmark"]}, {"id": "term-atomic", "t": "ATOMIC", "tg": ["Knowledge", "NLP", "Reasoning"], "d": "datasets", "x": "A commonsense knowledge graph containing 877000 inferential triples about everyday events and their causes effects and...", "l": "a", "k": ["atomic", "commonsense", "knowledge", "graph", "containing", "inferential", "triples", "everyday", "events", "causes", "effects", "mental", "states", "reasoning", "nlp"]}, {"id": "term-atomic-2020", "t": "ATOMIC 2020", "tg": ["Knowledge", "NLP", "Reasoning"], "d": "datasets", "x": "An expanded commonsense knowledge graph with 1.33 million triples covering physical social and eventive commonsense...", "l": "a", "k": ["atomic", "expanded", "commonsense", "knowledge", "graph", "million", "triples", "covering", "physical", "social", "eventive", "adds", "relation", "types", "original"]}, {"id": "term-atrous-convolution", "t": "Atrous Convolution", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Also known as dilated convolution, a convolution operation that inserts gaps between filter elements to increase the...", "l": "a", "k": ["atrous", "convolution", "known", "dilated", "operation", "inserts", "gaps", "filter", "elements", "increase", "receptive", "field", "without", "adding", "parameters"]}, {"id": "term-attention-economy-and-ai", "t": "Attention Economy and AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The intersection of AI-driven content recommendation systems and the competition for human attention. AI amplifies...", "l": "a", "k": ["attention", "economy", "intersection", "ai-driven", "content", "recommendation", "systems", "competition", "human", "amplifies", "engagement", "optimization", "conflict", "user", "wellbeing"]}, {"id": "term-attention-head-pruning", "t": "Attention Head Pruning", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A model compression technique that removes redundant or less important attention heads from a multi-head attention...", "l": "a", "k": ["attention", "head", "pruning", "model", "compression", "technique", "removes", "redundant", "less", "important", "heads", "multi-head", "mechanism", "reducing", "computation"]}, {"id": "term-attention-is-all-you-need", "t": "Attention Is All You Need", "tg": ["History", "Milestones"], "d": "history", "x": "The landmark 2017 paper by Vaswani et al. that introduced the transformer architecture, replacing recurrence with...", "l": "a", "k": ["attention", "need", "landmark", "paper", "vaswani", "introduced", "transformer", "architecture", "replacing", "recurrence", "self-attention", "mechanisms", "enabling", "massive", "scaling"]}, {"id": "term-attention-mask", "t": "Attention Mask", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A binary or float tensor applied to attention scores before softmax to prevent the model from attending to certain...", "l": "a", "k": ["attention", "mask", "binary", "float", "tensor", "applied", "scores", "softmax", "prevent", "model", "attending", "certain", "positions", "padding", "tokens"]}, {"id": "term-attention-masking", "t": "Attention Masking", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A technique that prevents attention from attending to certain positions by setting their attention scores to negative...", "l": "a", "k": ["attention", "masking", "technique", "prevents", "attending", "certain", "positions", "setting", "scores", "negative", "infinity", "softmax", "causal", "autoregressive", "models"]}, {"id": "term-attention-mechanism", "t": "Attention Mechanism", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A neural network component that allows the model to focus on relevant parts of the input when producing output....", "l": "a", "k": ["attention", "mechanism", "neural", "network", "component", "allows", "model", "focus", "relevant", "parts", "input", "producing", "output", "computes", "weighted"]}, {"id": "term-attention-mechanism-history", "t": "Attention Mechanism History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of attention mechanisms from Bahdanau et al.'s 2014 neural machine translation work through the...", "l": "a", "k": ["attention", "mechanism", "history", "development", "mechanisms", "bahdanau", "neural", "machine", "translation", "work", "self-attention", "innovation", "transformer", "paper", "became"]}, {"id": "term-attention-pooling", "t": "Attention Pooling", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A pooling mechanism that uses learned attention weights to aggregate features, allowing the model to focus on the most...", "l": "a", "k": ["attention", "pooling", "mechanism", "uses", "learned", "weights", "aggregate", "features", "allowing", "model", "focus", "informative", "elements", "rather", "fixed"]}, {"id": "term-attention-score", "t": "Attention Score", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The raw compatibility value computed between a query and key vector, typically via scaled dot product, before softmax...", "l": "a", "k": ["attention", "score", "raw", "compatibility", "value", "computed", "query", "key", "vector", "typically", "via", "scaled", "dot", "product", "softmax"]}, {"id": "term-attention-sink", "t": "Attention Sink", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A phenomenon where initial tokens in a sequence receive disproportionately high attention scores regardless of content,...", "l": "a", "k": ["attention", "sink", "phenomenon", "initial", "tokens", "sequence", "receive", "disproportionately", "high", "scores", "regardless", "content", "discovered", "important", "maintaining"]}, {"id": "term-attention-visualization", "t": "Attention Visualization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A model interpretability technique that displays attention weight patterns as heatmaps showing which input tokens the...", "l": "a", "k": ["attention", "visualization", "model", "interpretability", "technique", "displays", "weight", "patterns", "heatmaps", "showing", "input", "tokens", "attends", "making", "predictions"]}, {"id": "term-attention-based-parsing", "t": "Attention-Based Parsing", "tg": ["NLP", "Parsing"], "d": "general", "x": "A parsing approach that uses attention mechanisms from neural networks to determine syntactic structure, often...", "l": "a", "k": ["attention-based", "parsing", "approach", "uses", "attention", "mechanisms", "neural", "networks", "determine", "syntactic", "structure", "achieving", "state-of-the-art", "results", "attending"]}, {"id": "term-attention-based-policy", "t": "Attention-Based Policy", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "An RL policy architecture that uses attention mechanisms to selectively focus on relevant parts of the observation or...", "l": "a", "k": ["attention-based", "policy", "architecture", "uses", "attention", "mechanisms", "selectively", "focus", "relevant", "parts", "observation", "memory", "policies", "excel", "environments"]}, {"id": "term-attention-based-translation", "t": "Attention-Based Translation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A neural machine translation approach where the decoder attends to different parts of the source sentence at each...", "l": "a", "k": ["attention-based", "translation", "neural", "machine", "approach", "decoder", "attends", "different", "parts", "source", "sentence", "generation", "step", "eliminating", "information"]}, {"id": "term-attribution-in-ai", "t": "Attribution in AI", "tg": ["Safety", "Technical"], "d": "safety", "x": "The ability to trace AI system outputs back to their sources including training data model components and design...", "l": "a", "k": ["attribution", "ability", "trace", "system", "outputs", "sources", "including", "training", "data", "model", "components", "design", "decisions", "important", "accountability"]}, {"id": "term-auc", "t": "AUC", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "Area Under the ROC Curve, a scalar metric summarizing classifier performance across all thresholds. An AUC of 1.0...", "l": "a", "k": ["auc", "area", "roc", "curve", "scalar", "metric", "summarizing", "classifier", "performance", "across", "thresholds", "indicates", "perfect", "classification", "equivalent"]}, {"id": "term-auc-pr", "t": "AUC-PR", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Area Under the Precision-Recall Curve measures classifier performance focusing on the positive class. More informative...", "l": "a", "k": ["auc-pr", "area", "precision-recall", "curve", "measures", "classifier", "performance", "focusing", "positive", "class", "informative", "auc-roc", "imbalanced", "datasets", "baseline"]}, {"id": "term-auc-roc", "t": "AUC-ROC", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Area Under the Receiver Operating Characteristic Curve measures a classifier's ability to distinguish between classes...", "l": "a", "k": ["auc-roc", "area", "receiver", "operating", "characteristic", "curve", "measures", "classifier", "ability", "distinguish", "classes", "across", "threshold", "values", "ranges"]}, {"id": "term-audio-generation", "t": "Audio Generation", "tg": ["Application", "Generative"], "d": "general", "x": "AI that creates speech, music, or sound effects from text or other inputs. Includes text-to-speech (TTS), music...", "l": "a", "k": ["audio", "generation", "creates", "speech", "music", "sound", "effects", "text", "inputs", "includes", "text-to-speech", "tts", "design", "applications"]}, {"id": "term-audio-spectrogram-transformer", "t": "Audio Spectrogram Transformer", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A pure Transformer model that operates on audio spectrograms for audio event classification without convolutional...", "l": "a", "k": ["audio", "spectrogram", "transformer", "pure", "model", "operates", "spectrograms", "event", "classification", "without", "convolutional", "layers"]}, {"id": "term-audio-spectrogram-transformer-data", "t": "Audio Spectrogram Transformer Data", "tg": ["Training Corpus", "Audio"], "d": "datasets", "x": "Training data used for the Audio Spectrogram Transformer including AudioSet with spectrogram representations for audio...", "l": "a", "k": ["audio", "spectrogram", "transformer", "data", "training", "including", "audioset", "representations", "classification", "sound", "event", "detection"]}, {"id": "term-audiogen", "t": "AudioGen", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A text-to-audio generation model from Meta AI that uses an autoregressive Transformer to create general audio and sound...", "l": "a", "k": ["audiogen", "text-to-audio", "generation", "model", "meta", "uses", "autoregressive", "transformer", "create", "general", "audio", "sound", "effects", "text", "descriptions"]}, {"id": "term-audiolm", "t": "AudioLM", "tg": ["Models", "Technical"], "d": "models", "x": "A language model for audio generation by Google that treats audio as a sequence of discrete tokens. Generates...", "l": "a", "k": ["audiolm", "language", "model", "audio", "generation", "google", "treats", "sequence", "discrete", "tokens", "generates", "natural-sounding", "speech", "music", "long-term"]}, {"id": "term-audiopalm", "t": "AudioPaLM", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "A multimodal language model from Google that combines PaLM and AudioLM to process and generate both text and speech in...", "l": "a", "k": ["audiopalm", "multimodal", "language", "model", "google", "combines", "palm", "audiolm", "process", "generate", "text", "speech", "unified", "framework"]}, {"id": "term-audioset", "t": "AudioSet", "tg": ["Benchmark", "Audio"], "d": "datasets", "x": "A large-scale audio classification dataset from Google containing over 2 million 10-second audio clips labeled with 632...", "l": "a", "k": ["audioset", "large-scale", "audio", "classification", "dataset", "google", "containing", "million", "10-second", "clips", "labeled", "event", "categories", "ontology", "everyday"]}, {"id": "term-audit-trail-for-ai", "t": "Audit Trail for AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "A chronological record of all decisions actions and changes related to an AI system throughout its lifecycle. Essential...", "l": "a", "k": ["audit", "trail", "chronological", "record", "decisions", "actions", "changes", "related", "system", "throughout", "lifecycle", "essential", "accountability", "regulatory", "compliance"]}, {"id": "term-auditability", "t": "Auditability", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The property of an AI system that allows independent third parties to examine its data, algorithms, models, and...", "l": "a", "k": ["auditability", "property", "system", "allows", "independent", "parties", "examine", "data", "algorithms", "models", "decision-making", "processes", "assess", "compliance", "standards"]}, {"id": "term-augmentation", "t": "Augmentation", "tg": ["Training", "Data"], "d": "general", "x": "Expanding training data by creating modified versions of existing examples. In text: paraphrasing, back-translation. In...", "l": "a", "k": ["augmentation", "expanding", "training", "data", "creating", "modified", "versions", "existing", "examples", "text", "paraphrasing", "back-translation", "images", "rotation", "cropping"]}, {"id": "term-augmented-dickey-fuller-test", "t": "Augmented Dickey-Fuller Test", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A statistical test for determining whether a unit root is present in a time series, which would indicate...", "l": "a", "k": ["augmented", "dickey-fuller", "test", "statistical", "determining", "unit", "root", "present", "time", "series", "indicate", "non-stationarity", "significant", "statistic", "leads"]}, {"id": "term-augmented-lagrangian-method", "t": "Augmented Lagrangian Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An optimization method that combines Lagrangian relaxation with a quadratic penalty term to handle equality and...", "l": "a", "k": ["augmented", "lagrangian", "method", "optimization", "combines", "relaxation", "quadratic", "penalty", "term", "handle", "equality", "inequality", "constraints", "avoids", "ill-conditioning"]}, {"id": "term-auraflow", "t": "AuraFlow", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An open-source flow-based text-to-image generation model that uses rectified flow matching for generating diverse and...", "l": "a", "k": ["auraflow", "open-source", "flow-based", "text-to-image", "generation", "model", "uses", "rectified", "flow", "matching", "generating", "diverse", "high-quality", "images"]}, {"id": "term-aurora-supercomputer", "t": "Aurora Supercomputer", "tg": ["Supercomputer", "Intel", "Exascale"], "d": "hardware", "x": "Exascale supercomputer at Argonne National Laboratory using Intel Xeon CPUs and Intel Data Center GPU Max accelerators....", "l": "a", "k": ["aurora", "supercomputer", "exascale", "argonne", "national", "laboratory", "intel", "xeon", "cpus", "data", "center", "gpu", "max", "accelerators", "designed"]}, {"id": "term-aurora-weather-model", "t": "Aurora Weather Model", "tg": ["Models", "Scientific"], "d": "models", "x": "A foundation model for Earth system prediction from Microsoft Research that handles weather forecasting and air quality...", "l": "a", "k": ["aurora", "weather", "model", "foundation", "earth", "system", "prediction", "microsoft", "research", "handles", "forecasting", "air", "quality", "atmospheric", "chemistry"]}, {"id": "term-auroragpt", "t": "AuroraGPT", "tg": ["Models", "Technical", "NLP", "Scientific"], "d": "models", "x": "A large language model from Argonne National Laboratory trained specifically for scientific research and discovery...", "l": "a", "k": ["auroragpt", "large", "language", "model", "argonne", "national", "laboratory", "trained", "specifically", "scientific", "research", "discovery", "across", "multiple", "disciplines"]}, {"id": "term-auto-complete", "t": "Auto-Complete", "tg": ["Application", "Feature"], "d": "general", "x": "AI feature that predicts and suggests text as you type. Powers writing assistants, code completion, and search...", "l": "a", "k": ["auto-complete", "feature", "predicts", "suggests", "text", "type", "powers", "writing", "assistants", "code", "completion", "search", "suggestions", "based", "language"]}, {"id": "term-autoaugment", "t": "AutoAugment", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "An automated augmentation policy search method that uses reinforcement learning to find optimal combinations and...", "l": "a", "k": ["autoaugment", "automated", "augmentation", "policy", "search", "method", "uses", "reinforcement", "learning", "find", "optimal", "combinations", "magnitudes", "operations", "given"]}, {"id": "term-autocorrelation", "t": "Autocorrelation", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "The correlation of a time series with a lagged version of itself. In regression, autocorrelated residuals violate the...", "l": "a", "k": ["autocorrelation", "correlation", "time", "series", "lagged", "version", "itself", "regression", "autocorrelated", "residuals", "violate", "independence", "assumption", "lead", "inefficient"]}, {"id": "term-autocorrelation-function", "t": "Autocorrelation Function", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A function that measures the correlation between a time series and a lagged version of itself at various time delays....", "l": "a", "k": ["autocorrelation", "function", "measures", "correlation", "time", "series", "lagged", "version", "itself", "various", "delays", "identify", "repeating", "patterns", "periodicity"]}, {"id": "term-autoencoder", "t": "Autoencoder", "tg": ["Architecture", "Unsupervised"], "d": "models", "x": "A neural network that learns to compress data into a smaller representation and then reconstruct it. Used for...", "l": "a", "k": ["autoencoder", "neural", "network", "learns", "compress", "data", "smaller", "representation", "reconstruct", "dimensionality", "reduction", "denoising", "learning", "efficient", "representations"]}, {"id": "term-autoencoder-for-dimensionality-reduction", "t": "Autoencoder for Dimensionality Reduction", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A neural network architecture that learns a compressed representation by training to reconstruct its input through a...", "l": "a", "k": ["autoencoder", "dimensionality", "reduction", "neural", "network", "architecture", "learns", "compressed", "representation", "training", "reconstruct", "input", "bottleneck", "layer", "activations"]}, {"id": "term-autoencoder-history", "t": "Autoencoder History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of autoencoders from early work on data compression (Hinton and Salakhutdinov 2006) through denoising...", "l": "a", "k": ["autoencoder", "history", "development", "autoencoders", "early", "work", "data", "compression", "hinton", "salakhutdinov", "denoising", "vincent", "variational", "kingma", "welling"]}, {"id": "term-autoformer", "t": "Autoformer", "tg": ["Models", "Technical"], "d": "models", "x": "A Transformer architecture for time series forecasting that uses a decomposition architecture and auto-correlation...", "l": "a", "k": ["autoformer", "transformer", "architecture", "time", "series", "forecasting", "uses", "decomposition", "auto-correlation", "mechanism", "capture", "temporal", "patterns"]}, {"id": "term-autogen", "t": "AutoGen", "tg": ["Framework", "Application"], "d": "general", "x": "Microsoft's framework for building multi-agent AI applications. Enables conversations between multiple AI agents that...", "l": "a", "k": ["autogen", "microsoft", "framework", "building", "multi-agent", "applications", "enables", "conversations", "multiple", "agents", "collaborate", "debate", "solve", "complex", "problems"]}, {"id": "term-automated-theorem-proving", "t": "Automated Theorem Proving", "tg": ["History", "Fundamentals"], "d": "history", "x": "The use of computers to prove mathematical theorems automatically. Beginning with the Logic Theorist in 1956 automated...", "l": "a", "k": ["automated", "theorem", "proving", "computers", "prove", "mathematical", "theorems", "automatically", "beginning", "logic", "theorist", "advanced", "resolution-based", "methods", "1960s"]}, {"id": "term-automatic-chain-of-thought", "t": "Automatic Chain-of-Thought", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A method that automatically constructs chain-of-thought demonstrations by clustering questions and selecting...", "l": "a", "k": ["automatic", "chain-of-thought", "method", "automatically", "constructs", "demonstrations", "clustering", "questions", "selecting", "representative", "examples", "model", "generate", "reasoning", "chains"]}, {"id": "term-automatic-differentiation", "t": "Automatic Differentiation", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A family of techniques for efficiently computing derivatives of numerical functions by decomposing them into elementary...", "l": "a", "k": ["automatic", "differentiation", "family", "techniques", "efficiently", "computing", "derivatives", "numerical", "functions", "decomposing", "elementary", "operations", "includes", "forward", "mode"]}, {"id": "term-automatic-prompt-engineer", "t": "Automatic Prompt Engineer", "tg": ["Prompt Engineering", "Optimization"], "d": "algorithms", "x": "An automated method (APE) that uses language models to generate, score, and select optimal prompt instructions for a...", "l": "a", "k": ["automatic", "prompt", "engineer", "automated", "method", "ape", "uses", "language", "models", "generate", "score", "select", "optimal", "instructions", "given"]}, {"id": "term-asr", "t": "Automatic Speech Recognition", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The technology that converts spoken language audio into text, using acoustic models, language models, and decoding...", "l": "a", "k": ["automatic", "speech", "recognition", "technology", "converts", "spoken", "language", "audio", "text", "acoustic", "models", "decoding", "algorithms", "transcribe", "signals"]}, {"id": "term-automation-bias", "t": "Automation Bias", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The human tendency to over-rely on automated systems and accept their outputs without sufficient critical evaluation,...", "l": "a", "k": ["automation", "bias", "human", "tendency", "over-rely", "automated", "systems", "accept", "outputs", "without", "sufficient", "critical", "evaluation", "contradicted", "evidence"]}, {"id": "term-auto-ml", "t": "AutoML", "tg": ["Tools", "Automation"], "d": "general", "x": "Automated machine learning tools that handle model selection, hyperparameter tuning, and feature engineering. Makes ML...", "l": "a", "k": ["automl", "automated", "machine", "learning", "tools", "handle", "model", "selection", "hyperparameter", "tuning", "feature", "engineering", "makes", "accessible", "non-experts"]}, {"id": "term-autonomous-weapons-debate", "t": "Autonomous Weapons Debate", "tg": ["History", "Fundamentals"], "d": "history", "x": "The ongoing international debate about the development and use of lethal autonomous weapons systems (LAWS) that can...", "l": "a", "k": ["autonomous", "weapons", "debate", "ongoing", "international", "development", "lethal", "systems", "laws", "select", "engage", "targets", "without", "human", "intervention"]}, {"id": "term-autonomous-weapons-systems", "t": "Autonomous Weapons Systems", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "Weapons systems that can select and engage targets without direct human intervention, raising profound ethical and...", "l": "a", "k": ["autonomous", "weapons", "systems", "select", "engage", "targets", "without", "direct", "human", "intervention", "raising", "profound", "ethical", "legal", "questions"]}, {"id": "term-autorec", "t": "AutoRec", "tg": ["Models", "Technical", "Recommendation"], "d": "models", "x": "An autoencoder-based collaborative filtering model that learns to reconstruct user or item rating vectors for making...", "l": "a", "k": ["autorec", "autoencoder-based", "collaborative", "filtering", "model", "learns", "reconstruct", "user", "item", "rating", "vectors", "making", "missing", "predictions"]}, {"id": "term-autoregressive-model", "t": "Autoregressive Model", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A time series model that predicts the current value as a linear combination of its own past values plus a noise term....", "l": "a", "k": ["autoregressive", "model", "time", "series", "predicts", "current", "value", "linear", "combination", "past", "values", "plus", "noise", "term", "order"]}, {"id": "term-auxiliary-loss", "t": "Auxiliary Loss", "tg": ["Training", "Advanced"], "d": "general", "x": "Additional loss terms added during training to help learning. Can improve training stability, add regularization, or...", "l": "a", "k": ["auxiliary", "loss", "additional", "terms", "added", "training", "help", "learning", "improve", "stability", "add", "regularization", "encourage", "specific", "behaviors"]}, {"id": "term-auxiliary-task-rl", "t": "Auxiliary Task in RL", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "An additional prediction or control objective trained alongside the main RL objective to improve representation...", "l": "a", "k": ["auxiliary", "task", "additional", "prediction", "control", "objective", "trained", "alongside", "main", "improve", "representation", "learning", "tasks", "pixel", "reward"]}, {"id": "term-average-pooling", "t": "Average Pooling", "tg": ["Architecture", "Technique"], "d": "models", "x": "A technique that reduces data dimensionality by computing the average of regions. Used in CNNs and for creating...", "l": "a", "k": ["average", "pooling", "technique", "reduces", "data", "dimensionality", "computing", "regions", "cnns", "creating", "fixed-size", "representations", "variable-length", "sequences"]}, {"id": "term-average-precision", "t": "Average Precision", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A single-number summary of the precision-recall curve, computed as the weighted mean of precisions at each threshold...", "l": "a", "k": ["average", "precision", "single-number", "summary", "precision-recall", "curve", "computed", "weighted", "mean", "precisions", "threshold", "increase", "recall", "weight", "equivalent"]}, {"id": "term-avl-tree-algorithm", "t": "AVL Tree Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A self-balancing binary search tree where the heights of the two child subtrees of any node differ by at most one....", "l": "a", "k": ["avl", "tree", "algorithm", "self-balancing", "binary", "search", "heights", "child", "subtrees", "node", "differ", "guarantees", "log", "operations", "strictly"]}, {"id": "term-awq", "t": "AWQ", "tg": ["LLM", "Inference"], "d": "models", "x": "Activation-aware Weight Quantization, a method that identifies and preserves salient weight channels based on...", "l": "a", "k": ["awq", "activation-aware", "weight", "quantization", "method", "identifies", "preserves", "salient", "channels", "based", "activation", "magnitudes", "enabling", "efficient", "low-bit"]}, {"id": "term-aws-ec2-p5-instance", "t": "AWS EC2 P5 Instance", "tg": ["Cloud", "AWS", "GPU"], "d": "hardware", "x": "Amazon cloud compute instance powered by NVIDIA H100 GPUs for AI training. Provides up to 8 H100 GPUs with 3200 Gbps...", "l": "a", "k": ["aws", "ec2", "instance", "amazon", "cloud", "compute", "powered", "nvidia", "h100", "gpus", "training", "provides", "gbps", "efa", "networking"]}, {"id": "term-aws-inferentia", "t": "AWS Inferentia", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "Amazon's purpose-built inference accelerator chip designed for high-throughput, low-cost ML inference in the cloud....", "l": "a", "k": ["aws", "inferentia", "amazon", "purpose-built", "inference", "accelerator", "chip", "designed", "high-throughput", "low-cost", "cloud", "provides", "better", "throughput", "per"]}, {"id": "term-aws-sagemaker", "t": "AWS SageMaker", "tg": ["Platform", "Cloud"], "d": "general", "x": "Amazon's ML platform for building, training, and deploying models. Provides infrastructure, tools, and pre-built...", "l": "a", "k": ["aws", "sagemaker", "amazon", "platform", "building", "training", "deploying", "models", "provides", "infrastructure", "tools", "pre-built", "algorithms", "complete", "lifecycle"]}, {"id": "term-aws-trainium", "t": "AWS Trainium", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "Amazon's custom AI training accelerator chip offering high-performance, cost-effective alternatives to NVIDIA GPUs for...", "l": "a", "k": ["aws", "trainium", "amazon", "custom", "training", "accelerator", "chip", "offering", "high-performance", "cost-effective", "alternatives", "nvidia", "gpus", "deep", "learning"]}, {"id": "term-aya", "t": "Aya", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A massively multilingual language model from Cohere For AI that supports over 100 languages with instruction-following...", "l": "a", "k": ["aya", "massively", "multilingual", "language", "model", "cohere", "supports", "languages", "instruction-following", "capabilities"]}, {"id": "term-aya-23", "t": "Aya 23", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A multilingual language model from Cohere For AI covering 23 languages with a focus on underrepresented language...", "l": "a", "k": ["aya", "multilingual", "language", "model", "cohere", "covering", "languages", "focus", "underrepresented", "families", "cross-lingual", "transfer"]}, {"id": "term-aya-dataset", "t": "Aya Dataset", "tg": ["Training Corpus", "NLP", "Multilingual"], "d": "datasets", "x": "A multilingual instruction-following dataset covering 65 languages with human-curated instructions. Created by an...", "l": "a", "k": ["aya", "dataset", "multilingual", "instruction-following", "covering", "languages", "human-curated", "instructions", "created", "international", "community", "annotators", "research"]}, {"id": "term-azure-nd-h100-v5", "t": "Azure ND H100 v5", "tg": ["Cloud", "Azure", "GPU"], "d": "hardware", "x": "Microsoft Azure virtual machine series powered by NVIDIA H100 GPUs for large-scale AI training. Offers InfiniBand...", "l": "a", "k": ["azure", "h100", "microsoft", "virtual", "machine", "series", "powered", "nvidia", "gpus", "large-scale", "training", "offers", "infiniband", "networking", "efficient"]}, {"id": "term-azure-openai", "t": "Azure OpenAI Service", "tg": ["Platform", "Cloud"], "d": "general", "x": "Microsoft's enterprise offering of OpenAI models through Azure cloud. Provides GPT-4, ChatGPT, and DALL-E with...", "l": "a", "k": ["azure", "openai", "service", "microsoft", "enterprise", "offering", "models", "cloud", "provides", "gpt-4", "chatgpt", "dall-e", "security", "compliance", "regional"]}, {"id": "term-b-tree-algorithm", "t": "B-Tree Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A self-balancing tree data structure that maintains sorted data and allows searches and insertions and deletions in...", "l": "b", "k": ["b-tree", "algorithm", "self-balancing", "tree", "data", "structure", "maintains", "sorted", "allows", "searches", "insertions", "deletions", "logarithmic", "time", "designed"]}, {"id": "term-babilong", "t": "Babilong", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating long-context reasoning by extending bAbI tasks to very long contexts. Tests whether models...", "l": "b", "k": ["babilong", "benchmark", "evaluating", "long-context", "reasoning", "extending", "babi", "tasks", "long", "contexts", "tests", "models", "reason", "relevant", "information"]}, {"id": "term-back-translation", "t": "Back-Translation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A data augmentation technique for machine translation that translates monolingual target-language text back to the...", "l": "b", "k": ["back-translation", "data", "augmentation", "technique", "machine", "translation", "translates", "monolingual", "target-language", "text", "source", "language", "reverse", "model", "creating"]}, {"id": "term-backdoor-attack", "t": "Backdoor Attack", "tg": ["Safety", "Technical"], "d": "safety", "x": "A type of poisoning attack where an adversary introduces a hidden trigger into a model during training that causes...", "l": "b", "k": ["backdoor", "attack", "type", "poisoning", "adversary", "introduces", "hidden", "trigger", "model", "training", "causes", "targeted", "misclassification", "present", "test"]}, {"id": "term-background-removal", "t": "Background Removal", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of automatically separating foreground subjects from their background in images using deep learning...", "l": "b", "k": ["background", "removal", "process", "automatically", "separating", "foreground", "subjects", "images", "deep", "learning", "segmentation", "matting", "models", "widely", "photography"]}, {"id": "term-backpropagation", "t": "Backpropagation", "tg": ["Training", "Neural Networks"], "d": "models", "x": "The fundamental algorithm for training neural networks. It calculates how much each weight contributed to the error and...", "l": "b", "k": ["backpropagation", "fundamental", "algorithm", "training", "neural", "networks", "calculates", "weight", "contributed", "error", "adjusts", "weights", "accordingly", "propagating", "signal"]}, {"id": "term-backpropagation-discovery", "t": "Backpropagation Discovery", "tg": ["History", "Milestones"], "d": "history", "x": "The development of the backpropagation algorithm for training multi-layer neural networks. While the mathematical...", "l": "b", "k": ["backpropagation", "discovery", "development", "algorithm", "training", "multi-layer", "neural", "networks", "mathematical", "foundations", "existed", "earlier", "paper", "rumelhart", "hinton"]}, {"id": "term-backpropagation-history", "t": "Backpropagation History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of the backpropagation algorithm for training neural networks, independently discovered multiple times...", "l": "b", "k": ["backpropagation", "history", "development", "algorithm", "training", "neural", "networks", "independently", "discovered", "multiple", "times", "popularized", "rumelhart", "hinton", "williams"]}, {"id": "term-backpropagation-through-time", "t": "Backpropagation Through Time", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "An extension of backpropagation for training recurrent neural networks that unrolls the network through time steps and...", "l": "b", "k": ["backpropagation", "time", "extension", "training", "recurrent", "neural", "networks", "unrolls", "network", "steps", "applies", "chain", "rule", "computational", "cost"]}, {"id": "term-backside-power-delivery", "t": "Backside Power Delivery", "tg": ["Architecture", "Design", "Innovation"], "d": "hardware", "x": "Chip design innovation routing power connections through the back of the silicon die rather than the front. Frees up...", "l": "b", "k": ["backside", "power", "delivery", "chip", "design", "innovation", "routing", "connections", "silicon", "die", "rather", "front", "frees", "resources", "signal"]}, {"id": "term-backward-euler-method", "t": "Backward Euler Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An implicit numerical method for solving ordinary differential equations that evaluates the derivative at the next time...", "l": "b", "k": ["backward", "euler", "method", "implicit", "numerical", "solving", "ordinary", "differential", "equations", "evaluates", "derivative", "next", "time", "step", "unconditionally"]}, {"id": "term-bactrian-x", "t": "Bactrian-X", "tg": ["Training Corpus", "NLP", "Multilingual"], "d": "datasets", "x": "A multilingual instruction-following dataset covering 52 languages created by translating Alpaca instructions. Tests...", "l": "b", "k": ["bactrian-x", "multilingual", "instruction-following", "dataset", "covering", "languages", "created", "translating", "alpaca", "instructions", "tests", "translated", "produce", "effective", "models"]}, {"id": "term-bag-of-visual-words", "t": "Bag of Visual Words", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A computer vision technique that represents images as histograms of local feature descriptors quantized to a visual...", "l": "b", "k": ["bag", "visual", "words", "computer", "vision", "technique", "represents", "images", "histograms", "local", "feature", "descriptors", "quantized", "vocabulary", "learned"]}, {"id": "term-bag-of-words", "t": "Bag of Words", "tg": ["NLP", "Representation"], "d": "general", "x": "A simple text representation that counts word occurrences, ignoring order and grammar. Despite its simplicity, still...", "l": "b", "k": ["bag", "words", "simple", "text", "representation", "counts", "word", "occurrences", "ignoring", "order", "grammar", "despite", "simplicity", "useful", "classification"]}, {"id": "term-bagging", "t": "Bagging (Bootstrap Aggregating)", "tg": ["Technique", "Ensemble"], "d": "general", "x": "Training multiple models on random subsets of data and averaging their predictions. Reduces variance and overfitting....", "l": "b", "k": ["bagging", "bootstrap", "aggregating", "training", "multiple", "models", "random", "subsets", "data", "averaging", "predictions", "reduces", "variance", "overfitting", "basis"]}, {"id": "term-baichuan", "t": "Baichuan", "tg": ["Model", "Chinese AI"], "d": "models", "x": "A series of Chinese bilingual LLMs known for strong performance in Chinese language tasks. Part of the growing...", "l": "b", "k": ["baichuan", "series", "chinese", "bilingual", "llms", "known", "strong", "performance", "language", "tasks", "part", "growing", "ecosystem", "non-western", "foundation"]}, {"id": "term-baichuan2", "t": "Baichuan2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A second-generation large language model from Baichuan Inc. with improved training methodology and strong Chinese and...", "l": "b", "k": ["baichuan2", "second-generation", "large", "language", "model", "baichuan", "inc", "improved", "training", "methodology", "strong", "chinese", "english", "bilingual", "performance"]}, {"id": "term-bairstows-method", "t": "Bairstow's Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An iterative root-finding algorithm that simultaneously extracts quadratic factors from a polynomial with real...", "l": "b", "k": ["bairstow", "method", "iterative", "root-finding", "algorithm", "simultaneously", "extracts", "quadratic", "factors", "polynomial", "real", "coefficients", "avoids", "complex", "arithmetic"]}, {"id": "term-balanced-iterative-reducing-and-clustering", "t": "Balanced Iterative Reducing and Clustering", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "An incremental hierarchical clustering algorithm designed for very large datasets that summarizes data using a compact...", "l": "b", "k": ["balanced", "iterative", "reducing", "clustering", "incremental", "hierarchical", "algorithm", "designed", "large", "datasets", "summarizes", "data", "compact", "tree", "structure"]}, {"id": "term-ball-tree-algorithm", "t": "Ball Tree Algorithm", "tg": ["Algorithms", "Technical", "Searching", "Data Structure"], "d": "algorithms", "x": "A spatial data structure that partitions points into nested hyperspheres for efficient nearest-neighbor searches....", "l": "b", "k": ["ball", "tree", "algorithm", "spatial", "data", "structure", "partitions", "points", "nested", "hyperspheres", "efficient", "nearest-neighbor", "searches", "outperforms", "k-d"]}, {"id": "term-bamboogle", "t": "Bamboogle", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A dataset of multi-hop questions specifically designed to be answerable by combining multiple Google searches. Tests...", "l": "b", "k": ["bamboogle", "dataset", "multi-hop", "questions", "specifically", "designed", "answerable", "combining", "multiple", "google", "searches", "tests", "ability", "integrate", "information"]}, {"id": "term-bandit-algorithm", "t": "Bandit Algorithm", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "An algorithm for the multi-armed bandit problem that balances exploration (trying new actions) with exploitation...", "l": "b", "k": ["bandit", "algorithm", "multi-armed", "problem", "balances", "exploration", "trying", "actions", "exploitation", "choosing", "best-known", "action", "maximize", "cumulative", "reward"]}, {"id": "term-bandwidth", "t": "Bandwidth (AI Context)", "tg": ["Infrastructure", "Performance"], "d": "hardware", "x": "The rate at which data can be transferred, crucial for AI infrastructure. Memory bandwidth often limits GPU...", "l": "b", "k": ["bandwidth", "context", "rate", "data", "transferred", "crucial", "infrastructure", "memory", "limits", "gpu", "performance", "network", "affects", "distributed", "training"]}, {"id": "term-bandwidth-selection", "t": "Bandwidth Selection", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "The process of choosing the bandwidth parameter in kernel density estimation, which controls the smoothness of the...", "l": "b", "k": ["bandwidth", "selection", "process", "choosing", "parameter", "kernel", "density", "estimation", "controls", "smoothness", "estimated", "methods", "include", "cross-validation", "silverman"]}, {"id": "term-barbara-liskov", "t": "Barbara Liskov", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who received the 2008 Turing Award for contributions to programming language design...", "l": "b", "k": ["barbara", "liskov", "american", "computer", "scientist", "received", "turing", "award", "contributions", "programming", "language", "design", "including", "data", "abstraction"]}, {"id": "term-bard", "t": "Bard", "tg": ["Product", "Historical"], "d": "history", "x": "Google's conversational AI product, later renamed to Gemini. Competed with ChatGPT using Google's LLM technology and...", "l": "b", "k": ["bard", "google", "conversational", "product", "later", "renamed", "gemini", "competed", "chatgpt", "llm", "technology", "integration", "services"]}, {"id": "term-bark", "t": "Bark", "tg": ["Models", "Technical"], "d": "models", "x": "An open-source text-to-audio model by Suno AI that generates realistic speech in multiple languages including nonverbal...", "l": "b", "k": ["bark", "open-source", "text-to-audio", "model", "suno", "generates", "realistic", "speech", "multiple", "languages", "including", "nonverbal", "sounds", "laughter", "music"]}, {"id": "term-bark-scale-algorithm", "t": "Bark Scale Algorithm", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A psychoacoustic scale that maps frequencies to critical bands of human hearing. Used in audio processing to model the...", "l": "b", "k": ["bark", "scale", "algorithm", "psychoacoustic", "maps", "frequencies", "critical", "bands", "human", "hearing", "audio", "processing", "model", "frequency", "resolution"]}, {"id": "term-barnes-hut-algorithm", "t": "Barnes-Hut Algorithm", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An approximation algorithm for N-body simulations that uses a tree structure to group distant particles. Reduces...", "l": "b", "k": ["barnes-hut", "algorithm", "approximation", "n-body", "simulations", "uses", "tree", "structure", "group", "distant", "particles", "reduces", "computational", "complexity", "log"]}, {"id": "term-barrier-method", "t": "Barrier Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An interior point approach that adds a logarithmic barrier function to the objective to enforce inequality constraints....", "l": "b", "k": ["barrier", "method", "interior", "point", "approach", "adds", "logarithmic", "function", "objective", "enforce", "inequality", "constraints", "parameter", "decreased", "iterations"]}, {"id": "term-bart", "t": "BART", "tg": ["Models", "Technical"], "d": "models", "x": "Bidirectional and Auto-Regressive Transformers combines a bidirectional encoder with an autoregressive decoder....", "l": "b", "k": ["bart", "bidirectional", "auto-regressive", "transformers", "combines", "encoder", "autoregressive", "decoder", "pretrained", "corrupting", "text", "arbitrary", "noise", "learning", "reconstruct"]}, {"id": "term-base-model", "t": "Base Model", "tg": ["Model Type", "Training"], "d": "models", "x": "A pre-trained model before fine-tuning for specific tasks. Base models are good at text completion but need instruction...", "l": "b", "k": ["base", "model", "pre-trained", "fine-tuning", "specific", "tasks", "models", "good", "text", "completion", "need", "instruction", "tuning", "become", "helpful"]}, {"id": "term-baseboard-management-controller", "t": "Baseboard Management Controller", "tg": ["Infrastructure", "Management", "Server"], "d": "hardware", "x": "Embedded processor on server motherboards providing remote monitoring and management capabilities. Enables...", "l": "b", "k": ["baseboard", "management", "controller", "embedded", "processor", "server", "motherboards", "providing", "remote", "monitoring", "capabilities", "enables", "administrators", "manage", "servers"]}, {"id": "term-based", "t": "Based", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A linear attention model architecture that combines short sliding window attention with linear attention for efficient...", "l": "b", "k": ["based", "linear", "attention", "model", "architecture", "combines", "short", "sliding", "window", "efficient", "sequence", "modeling"]}, {"id": "term-baseline", "t": "Baseline", "tg": ["Evaluation", "Research"], "d": "datasets", "x": "A simple model or approach used as a reference point for comparison. New methods should outperform baselines to...", "l": "b", "k": ["baseline", "simple", "model", "approach", "reference", "point", "comparison", "methods", "outperform", "baselines", "demonstrate", "value", "common", "include", "random"]}, {"id": "term-bat-algorithm", "t": "Bat Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A swarm intelligence algorithm inspired by the echolocation behavior of bats. Varies the frequency and loudness and...", "l": "b", "k": ["bat", "algorithm", "swarm", "intelligence", "inspired", "echolocation", "behavior", "bats", "varies", "frequency", "loudness", "pulse", "emission", "rate", "virtual"]}, {"id": "term-batch", "t": "Batch", "tg": ["Training", "Technical"], "d": "general", "x": "A subset of training data processed together in one iteration. Batch processing improves training efficiency and...", "l": "b", "k": ["batch", "subset", "training", "data", "processed", "together", "iteration", "processing", "improves", "efficiency", "stability", "compared", "example", "time"]}, {"id": "term-batch-indexing", "t": "Batch Indexing", "tg": ["Vector Database", "Maintenance"], "d": "general", "x": "The process of building or rebuilding a vector index from a complete dataset in a single operation, producing an...", "l": "b", "k": ["batch", "indexing", "process", "building", "rebuilding", "vector", "index", "complete", "dataset", "single", "operation", "producing", "optimally", "structured", "typically"]}, {"id": "term-batch-normalization", "t": "Batch Normalization", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A technique that normalizes the inputs to each layer by subtracting the batch mean and dividing by the batch standard...", "l": "b", "k": ["batch", "normalization", "technique", "normalizes", "inputs", "layer", "subtracting", "mean", "dividing", "standard", "deviation", "applying", "learned", "scale", "shift"]}, {"id": "term-batch-processing-in-ai", "t": "Batch Processing in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "The practice of processing data in groups rather than individually during neural network training. Stochastic gradient...", "l": "b", "k": ["batch", "processing", "practice", "data", "groups", "rather", "individually", "neural", "network", "training", "stochastic", "gradient", "descent", "mini-batches", "became"]}, {"id": "term-batch-rl", "t": "Batch Reinforcement Learning", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "An approach to RL where the agent learns from a fixed batch of pre-collected transitions without online interaction....", "l": "b", "k": ["batch", "reinforcement", "learning", "approach", "agent", "learns", "fixed", "pre-collected", "transitions", "without", "online", "interaction", "methods", "fitted", "q-iteration"]}, {"id": "term-batch-renormalization", "t": "Batch Renormalization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An extension of batch normalization that introduces correction terms to reduce dependence on mini-batch statistics....", "l": "b", "k": ["batch", "renormalization", "extension", "normalization", "introduces", "correction", "terms", "reduce", "dependence", "mini-batch", "statistics", "proposed", "ioffe", "address", "failures"]}, {"id": "term-batch-scheduling", "t": "Batch Scheduling for Inference", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The strategy of grouping multiple inference requests together for simultaneous processing on a GPU, improving hardware...", "l": "b", "k": ["batch", "scheduling", "inference", "strategy", "grouping", "multiple", "requests", "together", "simultaneous", "processing", "gpu", "improving", "hardware", "utilization", "throughput"]}, {"id": "term-batch-size", "t": "Batch Size", "tg": ["Hyperparameter", "Training"], "d": "general", "x": "The number of training examples processed together before updating model weights. Larger batches provide more stable...", "l": "b", "k": ["batch", "size", "number", "training", "examples", "processed", "together", "updating", "model", "weights", "larger", "batches", "provide", "stable", "gradients"]}, {"id": "term-batched-inference", "t": "Batched Inference", "tg": ["LLM", "Inference"], "d": "models", "x": "The practice of processing multiple inference requests simultaneously through a model to maximize GPU utilization and...", "l": "b", "k": ["batched", "inference", "practice", "processing", "multiple", "requests", "simultaneously", "model", "maximize", "gpu", "utilization", "throughput", "amortizing", "cost", "loading"]}, {"id": "term-bayes-error-rate", "t": "Bayes Error Rate", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "The lowest achievable error rate for any classifier on a given classification problem, determined by the irreducible...", "l": "b", "k": ["bayes", "error", "rate", "lowest", "achievable", "classifier", "given", "classification", "problem", "determined", "irreducible", "noise", "data", "represents", "theoretical"]}, {"id": "term-bayes-theorem", "t": "Bayes' Theorem", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A fundamental rule of probability that relates the conditional probability of a hypothesis given evidence to the prior...", "l": "b", "k": ["bayes", "theorem", "fundamental", "rule", "probability", "relates", "conditional", "hypothesis", "given", "evidence", "prior", "likelihood", "marginal"]}, {"id": "term-bayesian-inference", "t": "Bayesian Inference", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A statistical framework that updates probability estimates for hypotheses as additional evidence is acquired, using...", "l": "b", "k": ["bayesian", "inference", "statistical", "framework", "updates", "probability", "estimates", "hypotheses", "additional", "evidence", "acquired", "bayes", "theorem", "compute", "posterior"]}, {"id": "term-bayesian-information-criterion", "t": "Bayesian Information Criterion", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A model selection criterion similar to AIC but with a larger penalty for the number of parameters that depends on...", "l": "b", "k": ["bayesian", "information", "criterion", "model", "selection", "similar", "aic", "larger", "penalty", "number", "parameters", "depends", "sample", "size", "tends"]}, {"id": "term-bayesian", "t": "Bayesian Methods", "tg": ["Statistics", "Theory"], "d": "algorithms", "x": "Statistical approaches that incorporate prior knowledge and update beliefs based on evidence. Used for uncertainty...", "l": "b", "k": ["bayesian", "methods", "statistical", "approaches", "incorporate", "prior", "knowledge", "update", "beliefs", "based", "evidence", "uncertainty", "quantification", "hyperparameter", "optimization"]}, {"id": "term-bayesian-model-averaging", "t": "Bayesian Model Averaging", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A technique that accounts for model uncertainty by averaging predictions across multiple models weighted by their...", "l": "b", "k": ["bayesian", "model", "averaging", "technique", "accounts", "uncertainty", "predictions", "across", "multiple", "models", "weighted", "posterior", "probabilities", "rather", "selecting"]}, {"id": "term-bayesian-network", "t": "Bayesian Network", "tg": ["Machine Learning", "Bayesian Methods"], "d": "general", "x": "A directed acyclic graph that represents a set of random variables and their conditional dependencies. Each node has a...", "l": "b", "k": ["bayesian", "network", "directed", "acyclic", "graph", "represents", "random", "variables", "conditional", "dependencies", "node", "probability", "table", "specifying", "given"]}, {"id": "term-bayesian-network-history", "t": "Bayesian Network History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of Bayesian networks by Judea Pearl and others in the 1980s, providing a graphical framework for...", "l": "b", "k": ["bayesian", "network", "history", "development", "networks", "judea", "pearl", "others", "1980s", "providing", "graphical", "framework", "representing", "reasoning", "uncertainty"]}, {"id": "term-bayesian-networks", "t": "Bayesian Networks", "tg": ["History", "Fundamentals"], "d": "history", "x": "Probabilistic graphical models that represent a set of variables and their conditional dependencies via directed...", "l": "b", "k": ["bayesian", "networks", "probabilistic", "graphical", "models", "represent", "variables", "conditional", "dependencies", "via", "directed", "acyclic", "graphs", "pioneered", "judea"]}, {"id": "term-bayesian-optimization", "t": "Bayesian Optimization", "tg": ["Machine Learning", "Bayesian Methods"], "d": "general", "x": "A sequential strategy for optimizing expensive black-box functions that builds a probabilistic surrogate model...", "l": "b", "k": ["bayesian", "optimization", "sequential", "strategy", "optimizing", "expensive", "black-box", "functions", "builds", "probabilistic", "surrogate", "model", "typically", "gaussian", "process"]}, {"id": "term-bbq", "t": "BBQ", "tg": ["Benchmark", "NLP", "Fairness"], "d": "datasets", "x": "Bias Benchmark for QA a dataset of 58000 question-answer pairs testing social biases in language models across 11...", "l": "b", "k": ["bbq", "bias", "benchmark", "dataset", "question-answer", "pairs", "testing", "social", "biases", "language", "models", "across", "categories", "evaluates", "exhibit"]}, {"id": "term-bdd100k", "t": "BDD100K", "tg": ["Benchmark", "Autonomous Driving"], "d": "datasets", "x": "The Berkeley Deep Drive dataset containing 100000 driving videos with diverse annotations including image-level tagging...", "l": "b", "k": ["bdd100k", "berkeley", "deep", "drive", "dataset", "containing", "driving", "videos", "diverse", "annotations", "including", "image-level", "tagging", "object", "detection"]}, {"id": "term-beam-search", "t": "Beam Search", "tg": ["Generation", "Algorithm"], "d": "algorithms", "x": "A search algorithm used in text generation that maintains multiple candidate sequences at each step, selecting the most...", "l": "b", "k": ["beam", "search", "algorithm", "text", "generation", "maintains", "multiple", "candidate", "sequences", "step", "selecting", "promising", "ones", "balances", "quality"]}, {"id": "term-beam-search-decoding", "t": "Beam Search Decoding", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A search algorithm for sequence generation that maintains the top-K partial sequences at each step. Explores multiple...", "l": "b", "k": ["beam", "search", "decoding", "algorithm", "sequence", "generation", "maintains", "top-k", "partial", "sequences", "step", "explores", "multiple", "hypotheses", "simultaneously"]}, {"id": "term-beam-search-with-length-penalty", "t": "Beam Search with Length Penalty", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "An extension of beam search decoding that normalizes sequence scores by length to prevent the algorithm from favoring...", "l": "b", "k": ["beam", "search", "length", "penalty", "extension", "decoding", "normalizes", "sequence", "scores", "prevent", "algorithm", "favoring", "shorter", "sequences", "commonly"]}, {"id": "term-beats", "t": "BEATs", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "Bidirectional Encoder representation from Audio Transformers is an audio pre-training framework using iterative audio...", "l": "b", "k": ["beats", "bidirectional", "encoder", "representation", "audio", "transformers", "pre-training", "framework", "iterative", "tokenization", "general", "understanding"]}, {"id": "term-beavertails", "t": "BeaverTails", "tg": ["Training Corpus", "NLP", "Safety"], "d": "datasets", "x": "A dataset of harmful LLM outputs categorized across 14 harm categories with safety labels. Used for training content...", "l": "b", "k": ["beavertails", "dataset", "harmful", "llm", "outputs", "categorized", "across", "harm", "categories", "safety", "labels", "training", "content", "classifiers", "harmlessness"]}, {"id": "term-bee-algorithm", "t": "Bee Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A swarm intelligence metaheuristic inspired by the foraging behavior of honeybees. Scout bees explore random solutions...", "l": "b", "k": ["bee", "algorithm", "swarm", "intelligence", "metaheuristic", "inspired", "foraging", "behavior", "honeybees", "scout", "bees", "explore", "random", "solutions", "recruited"]}, {"id": "term-begin", "t": "BEGIN", "tg": ["Benchmark", "NLP", "Evaluation", "Dialogue"], "d": "datasets", "x": "Benchmark for Evaluation of Grounded Interaction in Natural Language a dataset for evaluating grounded dialogue systems...", "l": "b", "k": ["begin", "benchmark", "evaluation", "grounded", "interaction", "natural", "language", "dataset", "evaluating", "dialogue", "systems", "faithfulness", "source", "knowledge"]}, {"id": "term-behavior-cloning", "t": "Behavior Cloning", "tg": ["Training", "Imitation"], "d": "general", "x": "Learning to imitate expert behavior from demonstrations. The model learns to map observations to actions by copying...", "l": "b", "k": ["behavior", "cloning", "learning", "imitate", "expert", "demonstrations", "model", "learns", "map", "observations", "actions", "copying", "experts", "similar", "situations"]}, {"id": "term-behavior-based-robotics", "t": "Behavior-Based Robotics", "tg": ["History", "Fundamentals"], "d": "history", "x": "An approach to robotics that generates complex behavior from the interaction of simple reactive behaviors rather than...", "l": "b", "k": ["behavior-based", "robotics", "approach", "generates", "complex", "behavior", "interaction", "simple", "reactive", "behaviors", "rather", "centralized", "planning", "pioneered", "rodney"]}, {"id": "term-behavioral-cloning-safety", "t": "Behavioral Cloning Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "Safety concerns arising from training AI agents to imitate human behavior. Includes distribution shift compounding...", "l": "b", "k": ["behavioral", "cloning", "safety", "concerns", "arising", "training", "agents", "imitate", "human", "behavior", "includes", "distribution", "shift", "compounding", "errors"]}, {"id": "term-beijing-ai-principles", "t": "Beijing AI Principles", "tg": ["Governance", "Regulation"], "d": "safety", "x": "A set of AI governance principles released in 2019 by the Beijing Academy of AI, emphasizing harmony, fairness, safety,...", "l": "b", "k": ["beijing", "principles", "governance", "released", "academy", "emphasizing", "harmony", "fairness", "safety", "shared", "benefits", "responsible", "development", "chinese", "context"]}, {"id": "term-beir", "t": "BEIR", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "Benchmarking Information Retrieval a heterogeneous benchmark of 18 retrieval datasets for zero-shot evaluation. Tests...", "l": "b", "k": ["beir", "benchmarking", "information", "retrieval", "heterogeneous", "benchmark", "datasets", "zero-shot", "evaluation", "tests", "generalization", "models", "across", "diverse", "domains"]}, {"id": "term-beit", "t": "BEiT", "tg": ["Models", "Technical"], "d": "models", "x": "Bidirectional Encoder representation from Image Transformers applies masked image modeling as a pretraining task for...", "l": "b", "k": ["beit", "bidirectional", "encoder", "representation", "image", "transformers", "applies", "masked", "modeling", "pretraining", "task", "vision", "tokenizes", "patches", "visual"]}, {"id": "term-belebele", "t": "Belebele", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "A reading comprehension benchmark covering 122 language variants with parallel passages and questions. One of the most...", "l": "b", "k": ["belebele", "reading", "comprehension", "benchmark", "covering", "language", "variants", "parallel", "passages", "questions", "linguistically", "diverse", "nlp", "evaluation", "benchmarks"]}, {"id": "term-belief-propagation-algorithm", "t": "Belief Propagation Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A message-passing algorithm for performing inference in graphical models. Each node sends messages to its neighbors...", "l": "b", "k": ["belief", "propagation", "algorithm", "message-passing", "performing", "inference", "graphical", "models", "node", "sends", "messages", "neighbors", "summarizing", "evidence", "rest"]}, {"id": "term-bell-labs-ai-research", "t": "Bell Labs AI Research", "tg": ["History", "Organizations"], "d": "history", "x": "AI and machine learning research conducted at Bell Laboratories (AT&T) where foundational work was done on information...", "l": "b", "k": ["bell", "labs", "research", "machine", "learning", "conducted", "laboratories", "foundational", "work", "done", "information", "theory", "speech", "recognition", "neural"]}, {"id": "term-bellman-equation", "t": "Bellman Equation", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A recursive equation relating the value of a state to the immediate reward plus the discounted value of successor...", "l": "b", "k": ["bellman", "equation", "recursive", "relating", "value", "state", "immediate", "reward", "plus", "discounted", "successor", "states", "provides", "foundation", "dynamic"]}, {"id": "term-bellman-ford-algorithm", "t": "Bellman-Ford Algorithm", "tg": ["Algorithms", "Fundamentals", "Graph"], "d": "algorithms", "x": "A shortest-path algorithm that computes shortest paths from a single source vertex to all other vertices in a weighted...", "l": "b", "k": ["bellman-ford", "algorithm", "shortest-path", "computes", "shortest", "paths", "single", "source", "vertex", "vertices", "weighted", "graph", "unlike", "dijkstra", "handle"]}, {"id": "term-benchmark", "t": "Benchmark", "tg": ["Evaluation", "Research"], "d": "datasets", "x": "A standardized test or dataset used to evaluate and compare AI model performance. Common LLM benchmarks include MMLU,...", "l": "b", "k": ["benchmark", "standardized", "test", "dataset", "evaluate", "compare", "model", "performance", "common", "llm", "benchmarks", "include", "mmlu", "hellaswag", "humaneval"]}, {"id": "term-benchmark-gaming", "t": "Benchmark Gaming", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The practice of optimizing a model specifically to achieve high scores on popular benchmarks without corresponding...", "l": "b", "k": ["benchmark", "gaming", "practice", "optimizing", "model", "specifically", "achieve", "high", "scores", "popular", "benchmarks", "without", "corresponding", "improvements", "real-world"]}, {"id": "term-benders-decomposition", "t": "Benders Decomposition", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "A decomposition technique for solving large-scale optimization problems with complicating variables. Splits the problem...", "l": "b", "k": ["benders", "decomposition", "technique", "solving", "large-scale", "optimization", "problems", "complicating", "variables", "splits", "problem", "master", "subproblems", "iteratively", "adds"]}, {"id": "term-benefit-sharing-in-ai", "t": "Benefit Sharing in AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The principle that the economic and social benefits generated by AI should be distributed broadly across society rather...", "l": "b", "k": ["benefit", "sharing", "principle", "economic", "social", "benefits", "generated", "distributed", "broadly", "across", "society", "rather", "concentrated", "among", "small"]}, {"id": "term-benefit-risk-analysis-for-ai", "t": "Benefit-Risk Analysis for AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "A systematic comparison of the potential benefits and risks of an AI system to determine whether deployment is...", "l": "b", "k": ["benefit-risk", "analysis", "systematic", "comparison", "potential", "benefits", "risks", "system", "determine", "deployment", "justified", "required", "regulatory", "frameworks", "high-risk"]}, {"id": "term-berkeley-ai-research-lab", "t": "Berkeley AI Research Lab", "tg": ["History", "Organizations"], "d": "history", "x": "The Berkeley Artificial Intelligence Research Laboratory (BAIR) at UC Berkeley conducting research across computer...", "l": "b", "k": ["berkeley", "research", "lab", "artificial", "intelligence", "laboratory", "bair", "conducting", "across", "computer", "vision", "nlp", "robotics", "machine", "learning"]}, {"id": "term-bernoulli-distribution", "t": "Bernoulli Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "The simplest discrete probability distribution, modeling a single trial with two outcomes (success with probability p,...", "l": "b", "k": ["bernoulli", "distribution", "simplest", "discrete", "probability", "modeling", "single", "trial", "outcomes", "success", "failure", "1-p", "building", "block", "binomial"]}, {"id": "term-bert", "t": "BERT (Bidirectional Encoder Representations from Transformers)", "tg": ["Model", "Architecture"], "d": "models", "x": "A influential language model from Google (2018) that processes text bidirectionally, understanding context from both...", "l": "b", "k": ["bert", "bidirectional", "encoder", "representations", "transformers", "influential", "language", "model", "google", "processes", "text", "bidirectionally", "understanding", "context", "left"]}, {"id": "term-bert-release", "t": "BERT Release", "tg": ["History", "Milestones"], "d": "history", "x": "Google's Bidirectional Encoder Representations from Transformers model, released in October 2018, which achieved...", "l": "b", "k": ["bert", "release", "google", "bidirectional", "encoder", "representations", "transformers", "model", "released", "october", "achieved", "state-of-the-art", "results", "across", "numerous"]}, {"id": "term-bert4rec", "t": "BERT4Rec", "tg": ["Models", "Technical", "Recommendation"], "d": "models", "x": "A sequential recommendation model that applies bidirectional self-attention (BERT-style) with masked item prediction to...", "l": "b", "k": ["bert4rec", "sequential", "recommendation", "model", "applies", "bidirectional", "self-attention", "bert-style", "masked", "item", "prediction", "capture", "user", "behavior", "patterns"]}, {"id": "term-bertscore", "t": "BERTScore", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation metric that computes the similarity between generated and reference texts using contextual BERT...", "l": "b", "k": ["bertscore", "evaluation", "metric", "computes", "similarity", "generated", "reference", "texts", "contextual", "bert", "embeddings", "greedy", "token", "matching", "capturing"]}, {"id": "term-best-first-search", "t": "Best-First Search", "tg": ["Algorithms", "Technical", "Graph", "Searching"], "d": "algorithms", "x": "A graph search strategy that selects the most promising node for expansion based on an evaluation function. Greedy...", "l": "b", "k": ["best-first", "search", "graph", "strategy", "selects", "promising", "node", "expansion", "based", "evaluation", "function", "greedy", "uses", "heuristic", "estimate"]}, {"id": "term-best-of-n-sampling", "t": "Best-of-N Sampling", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "An inference strategy that generates N candidate completions and returns the one scoring highest on a reward model,...", "l": "b", "k": ["best-of-n", "sampling", "inference", "strategy", "generates", "candidate", "completions", "returns", "scoring", "highest", "reward", "model", "trading", "increased", "compute"]}, {"id": "term-beta-distribution", "t": "Beta Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution defined on the interval [0, 1], parametrized by two shape parameters. It is...", "l": "b", "k": ["beta", "distribution", "continuous", "probability", "defined", "interval", "parametrized", "shape", "parameters", "commonly", "prior", "probabilities", "bayesian", "inference"]}, {"id": "term-beta-vae", "t": "Beta-VAE", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A modification of the variational autoencoder that introduces a hyperparameter beta to weight the KL divergence term,...", "l": "b", "k": ["beta-vae", "modification", "variational", "autoencoder", "introduces", "hyperparameter", "beta", "weight", "divergence", "term", "promoting", "disentangled", "latent", "representations", "greater"]}, {"id": "term-bev-perception", "t": "BEV Perception", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "Bird's Eye View perception, a paradigm in autonomous driving that transforms multi-camera or LiDAR data into a unified...", "l": "b", "k": ["bev", "perception", "bird", "eye", "view", "paradigm", "autonomous", "driving", "transforms", "multi-camera", "lidar", "data", "unified", "top-down", "representation"]}, {"id": "term-bevdet", "t": "BEVDet", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "A Bird Eye View detection framework for autonomous driving that transforms perspective-view features into BEV space for...", "l": "b", "k": ["bevdet", "bird", "eye", "view", "detection", "framework", "autonomous", "driving", "transforms", "perspective-view", "features", "bev", "space", "accurate", "object"]}, {"id": "term-bevformer", "t": "BEVFormer", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "A spatiotemporal Transformer for autonomous driving that generates Bird Eye View representations from multi-camera...", "l": "b", "k": ["bevformer", "spatiotemporal", "transformer", "autonomous", "driving", "generates", "bird", "eye", "view", "representations", "multi-camera", "inputs", "deformable", "attention"]}, {"id": "term-bf16", "t": "BF16 (Brain Floating Point)", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "A 16-bit floating-point format with 8 exponent bits (same as FP32) and 7 mantissa bits, developed by Google Brain. BF16...", "l": "b", "k": ["bf16", "brain", "floating", "point", "16-bit", "floating-point", "format", "exponent", "bits", "fp32", "mantissa", "developed", "google", "maintains", "dynamic"]}, {"id": "term-bfloat16", "t": "bfloat16", "tg": ["Technical", "Precision"], "d": "general", "x": "A 16-bit floating-point format optimized for neural network training. Sacrifices precision for range compared to...", "l": "b", "k": ["bfloat16", "16-bit", "floating-point", "format", "optimized", "neural", "network", "training", "sacrifices", "precision", "range", "compared", "float16", "offering", "better"]}, {"id": "term-bge", "t": "BGE", "tg": ["Models", "Technical"], "d": "models", "x": "BAAI General Embedding is a family of embedding models that achieve strong performance on text retrieval tasks. Trained...", "l": "b", "k": ["bge", "baai", "general", "embedding", "family", "models", "achieve", "strong", "performance", "text", "retrieval", "tasks", "trained", "beijing", "academy"]}, {"id": "term-bi-encoder", "t": "Bi-Encoder", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "A neural retrieval architecture that independently encodes queries and documents into fixed-size vectors using separate...", "l": "b", "k": ["bi-encoder", "neural", "retrieval", "architecture", "independently", "encodes", "queries", "documents", "fixed-size", "vectors", "separate", "shared", "encoders", "enabling", "pre-computation"]}, {"id": "term-estimation-bias", "t": "Bias", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "In statistical estimation, the difference between the expected value of an estimator and the true value of the...", "l": "b", "k": ["bias", "statistical", "estimation", "difference", "expected", "value", "estimator", "true", "parameter", "estimated", "unbiased", "zero"]}, {"id": "term-bias-amplification", "t": "Bias Amplification", "tg": ["Safety", "Technical"], "d": "safety", "x": "The phenomenon where machine learning models amplify biases present in training data producing outputs that are more...", "l": "b", "k": ["bias", "amplification", "phenomenon", "machine", "learning", "models", "amplify", "biases", "present", "training", "data", "producing", "outputs", "biased", "itself"]}, {"id": "term-bias-bounty", "t": "Bias Bounty", "tg": ["Safety", "Governance"], "d": "safety", "x": "A program that rewards individuals for identifying and reporting biases in AI systems. Similar to bug bounties in...", "l": "b", "k": ["bias", "bounty", "program", "rewards", "individuals", "identifying", "reporting", "biases", "systems", "similar", "bug", "bounties", "cybersecurity", "aimed", "crowdsourcing"]}, {"id": "term-bias-in-ai", "t": "Bias in AI", "tg": ["Safety", "Fundamentals"], "d": "safety", "x": "Systematic errors in AI system outputs that arise from prejudiced assumptions in training data algorithm design or...", "l": "b", "k": ["bias", "systematic", "errors", "system", "outputs", "arise", "prejudiced", "assumptions", "training", "data", "algorithm", "design", "deployment", "context", "lead"]}, {"id": "term-bias-mitigation", "t": "Bias Mitigation", "tg": ["Safety", "Technical"], "d": "safety", "x": "Techniques and practices for reducing unfair bias in AI systems. Includes pre-processing methods like resampling...", "l": "b", "k": ["bias", "mitigation", "techniques", "practices", "reducing", "unfair", "systems", "includes", "pre-processing", "methods", "resampling", "in-processing", "adversarial", "debiasing", "post-processing"]}, {"id": "term-bias-score", "t": "Bias Score", "tg": ["Evaluation", "Safety"], "d": "datasets", "x": "A metric that measures the degree of systematic prejudice or unfair treatment in model outputs across demographic...", "l": "b", "k": ["bias", "score", "metric", "measures", "degree", "systematic", "prejudice", "unfair", "treatment", "model", "outputs", "across", "demographic", "groups", "assessed"]}, {"id": "term-bias-testing", "t": "Bias Testing", "tg": ["Safety", "Technical"], "d": "safety", "x": "The systematic evaluation of AI systems for unfair biases across demographic groups and use cases. Includes statistical...", "l": "b", "k": ["bias", "testing", "systematic", "evaluation", "systems", "unfair", "biases", "across", "demographic", "groups", "cases", "includes", "statistical", "parity", "disparate"]}, {"id": "term-bias-variance-decomposition", "t": "Bias-Variance Decomposition", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A mathematical decomposition of expected prediction error into three components: irreducible noise, squared bias...", "l": "b", "k": ["bias-variance", "decomposition", "mathematical", "expected", "prediction", "error", "components", "irreducible", "noise", "squared", "bias", "systematic", "variance", "sensitivity", "training"]}, {"id": "term-bias-variance-tradeoff", "t": "Bias-Variance Tradeoff", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "The fundamental tension in supervised learning between a model's ability to minimize bias (error from overly simplistic...", "l": "b", "k": ["bias-variance", "tradeoff", "fundamental", "tension", "supervised", "learning", "model", "ability", "minimize", "bias", "error", "overly", "simplistic", "assumptions", "variance"]}, {"id": "term-bicgstab-algorithm", "t": "BiCGSTAB Algorithm", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "Biconjugate Gradient Stabilized method is an iterative solver for non-symmetric linear systems. Combines the...", "l": "b", "k": ["bicgstab", "algorithm", "biconjugate", "gradient", "stabilized", "method", "iterative", "solver", "non-symmetric", "linear", "systems", "combines", "stabilization", "techniques", "avoid"]}, {"id": "term-biden-executive-order-on-ai", "t": "Biden Executive Order on AI", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Executive Order 14110, issued by President Biden in October 2023, establishing requirements for AI safety and security...", "l": "b", "k": ["biden", "executive", "order", "issued", "president", "october", "establishing", "requirements", "safety", "security", "including", "red-teaming", "standards", "reporting", "large"]}, {"id": "term-bidirectional", "t": "Bidirectional", "tg": ["Architecture", "Processing"], "d": "models", "x": "Processing sequences in both directions (left-to-right and right-to-left). BERT processes bidirectionally for...", "l": "b", "k": ["bidirectional", "processing", "sequences", "directions", "left-to-right", "right-to-left", "bert", "processes", "bidirectionally", "understanding", "gpt", "unidirectionally", "generation"]}, {"id": "term-bidirectional-attention", "t": "Bidirectional Attention", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An attention pattern that allows each token to attend to all other tokens in both directions. Used in encoder models...", "l": "b", "k": ["bidirectional", "attention", "pattern", "allows", "token", "attend", "tokens", "directions", "encoder", "models", "bert", "building", "contextual", "representations", "contrasts"]}, {"id": "term-bidirectional-rnn", "t": "Bidirectional RNN", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A recurrent architecture that processes input sequences in both forward and backward directions simultaneously,...", "l": "b", "k": ["bidirectional", "rnn", "recurrent", "architecture", "processes", "input", "sequences", "forward", "backward", "directions", "simultaneously", "combining", "context", "produce", "richer"]}, {"id": "term-bidirectional-search", "t": "Bidirectional Search", "tg": ["Algorithms", "Technical", "Graph", "Searching"], "d": "algorithms", "x": "A graph search algorithm that simultaneously runs two searches from the start and goal nodes until they meet. Can...", "l": "b", "k": ["bidirectional", "search", "graph", "algorithm", "simultaneously", "runs", "searches", "start", "goal", "nodes", "meet", "reduce", "space", "exponentially", "compared"]}, {"id": "term-big-bench", "t": "BIG-bench", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "The Beyond the Imitation Game Benchmark a collaborative benchmark of over 200 tasks contributed by 450 researchers....", "l": "b", "k": ["big-bench", "beyond", "imitation", "game", "benchmark", "collaborative", "tasks", "contributed", "researchers", "tests", "language", "model", "capabilities", "including", "reasoning"]}, {"id": "term-big-bench-hard", "t": "BIG-bench Hard", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A subset of 23 particularly challenging BIG-bench tasks where prior language model evaluations fell below average human...", "l": "b", "k": ["big-bench", "hard", "subset", "particularly", "challenging", "tasks", "prior", "language", "model", "evaluations", "fell", "average", "human", "performance", "test"]}, {"id": "term-bigbench", "t": "BigBench", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Beyond the Imitation Game Benchmark, a large collaborative benchmark containing over 200 diverse tasks contributed by...", "l": "b", "k": ["bigbench", "beyond", "imitation", "game", "benchmark", "large", "collaborative", "containing", "diverse", "tasks", "contributed", "researchers", "designed", "probe", "language"]}, {"id": "term-bigbird", "t": "BigBird", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A sparse attention transformer that combines random attention, window attention, and global attention patterns to...", "l": "b", "k": ["bigbird", "sparse", "attention", "transformer", "combines", "random", "window", "global", "patterns", "achieve", "linear", "complexity", "provably", "maintaining", "expressive"]}, {"id": "term-bigcodebench", "t": "BigCodeBench", "tg": ["Benchmark", "Code", "Evaluation"], "d": "datasets", "x": "A comprehensive benchmark for evaluating code generation models on practical programming tasks. Tests functional...", "l": "b", "k": ["bigcodebench", "comprehensive", "benchmark", "evaluating", "code", "generation", "models", "practical", "programming", "tasks", "tests", "functional", "correctness", "across", "diverse"]}, {"id": "term-bigearthnet", "t": "BigEarthNet", "tg": ["Benchmark", "Computer Vision", "Remote Sensing"], "d": "datasets", "x": "A large-scale Sentinel-2 satellite image benchmark containing 590326 image patches with multiple land cover labels. One...", "l": "b", "k": ["bigearthnet", "large-scale", "sentinel-2", "satellite", "image", "benchmark", "containing", "patches", "multiple", "land", "cover", "labels", "largest", "remote", "sensing"]}, {"id": "term-biggan", "t": "BigGAN", "tg": ["Models", "Technical"], "d": "models", "x": "A large-scale GAN that generates high-fidelity images by scaling up batch size model size and applying...", "l": "b", "k": ["biggan", "large-scale", "gan", "generates", "high-fidelity", "images", "scaling", "batch", "size", "model", "applying", "class-conditional", "generation", "truncation", "demonstrated"]}, {"id": "term-bigram", "t": "Bigram / N-gram", "tg": ["NLP", "Historical"], "d": "history", "x": "Sequences of N consecutive tokens used in language modeling. Bigrams are pairs; trigrams are triples. N-gram models...", "l": "b", "k": ["bigram", "n-gram", "sequences", "consecutive", "tokens", "language", "modeling", "bigrams", "pairs", "trigrams", "triples", "models", "were", "dominant", "neural"]}, {"id": "term-bigvgan", "t": "BigVGAN", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A large-scale universal neural vocoder that uses anti-aliased multi-periodicity composition for generating...", "l": "b", "k": ["bigvgan", "large-scale", "universal", "neural", "vocoder", "uses", "anti-aliased", "multi-periodicity", "composition", "generating", "high-fidelity", "audio", "across", "diverse", "signal"]}, {"id": "term-bilateral-filter-algorithm", "t": "Bilateral Filter Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An edge-preserving smoothing filter that averages nearby pixels weighted by both spatial distance and intensity...", "l": "b", "k": ["bilateral", "filter", "algorithm", "edge-preserving", "smoothing", "averages", "nearby", "pixels", "weighted", "spatial", "distance", "intensity", "difference", "preserves", "edges"]}, {"id": "term-binary-classification", "t": "Binary Classification", "tg": ["ML Task", "Classification"], "d": "general", "x": "A classification task with exactly two possible outcomes (yes/no, spam/not spam). The simplest classification problem,...", "l": "b", "k": ["binary", "classification", "task", "exactly", "possible", "outcomes", "yes", "spam", "simplest", "problem", "building", "block", "complex", "tasks"]}, {"id": "term-binary-cross-entropy", "t": "Binary Cross-Entropy", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A loss function for binary classification that measures the divergence between predicted probabilities and binary...", "l": "b", "k": ["binary", "cross-entropy", "loss", "function", "classification", "measures", "divergence", "predicted", "probabilities", "labels", "defined", "log", "1-y", "1-p", "equivalent"]}, {"id": "term-binary-quantization", "t": "Binary Quantization", "tg": ["Vector Database", "Quantization"], "d": "general", "x": "An aggressive vector compression technique that reduces each vector dimension to a single bit based on its sign,...", "l": "b", "k": ["binary", "quantization", "aggressive", "vector", "compression", "technique", "reduces", "dimension", "single", "bit", "based", "sign", "enabling", "32x", "float32"]}, {"id": "term-binary-search", "t": "Binary Search", "tg": ["Algorithms", "Fundamentals", "Searching"], "d": "algorithms", "x": "A search algorithm that finds the position of a target value within a sorted array by repeatedly dividing the search...", "l": "b", "k": ["binary", "search", "algorithm", "finds", "position", "target", "value", "within", "sorted", "array", "repeatedly", "dividing", "interval", "half", "achieves"]}, {"id": "term-bing-chat", "t": "Bing Chat / Copilot", "tg": ["Product", "Microsoft"], "d": "general", "x": "Microsoft's AI-powered search assistant, integrating GPT-4 with web search. Can answer questions with citations, create...", "l": "b", "k": ["bing", "chat", "copilot", "microsoft", "ai-powered", "search", "assistant", "integrating", "gpt-4", "web", "answer", "questions", "citations", "create", "content"]}, {"id": "term-binning", "t": "Binning", "tg": ["Manufacturing", "Process", "Quality"], "d": "hardware", "x": "Process of sorting manufactured chips into quality grades based on their tested performance characteristics....", "l": "b", "k": ["binning", "process", "sorting", "manufactured", "chips", "quality", "grades", "based", "tested", "performance", "characteristics", "higher-quality", "bins", "become", "premium"]}, {"id": "term-binomial-distribution", "t": "Binomial Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A discrete probability distribution modeling the number of successes in a fixed number of independent Bernoulli trials,...", "l": "b", "k": ["binomial", "distribution", "discrete", "probability", "modeling", "number", "successes", "fixed", "independent", "bernoulli", "trials", "success", "parametrized"]}, {"id": "term-binomial-heap-algorithm", "t": "Binomial Heap Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A heap data structure consisting of a collection of binomial trees that supports efficient merging of two heaps. All...", "l": "b", "k": ["binomial", "heap", "algorithm", "data", "structure", "consisting", "collection", "trees", "supports", "efficient", "merging", "heaps", "operations", "run", "log"]}, {"id": "term-bio-tagging", "t": "BIO Tagging", "tg": ["NLP", "Text Processing"], "d": "general", "x": "An alternative name for IOB tagging format using Begin, Inside, Outside labels for sequence labeling tasks, where B...", "l": "b", "k": ["bio", "tagging", "alternative", "name", "iob", "format", "begin", "inside", "outside", "labels", "sequence", "labeling", "tasks", "marks", "start"]}, {"id": "term-bioasq", "t": "BioASQ", "tg": ["Benchmark", "NLP", "Medical"], "d": "datasets", "x": "A biomedical semantic indexing and question answering challenge providing datasets for biomedical information retrieval...", "l": "b", "k": ["bioasq", "biomedical", "semantic", "indexing", "question", "answering", "challenge", "providing", "datasets", "information", "retrieval", "includes", "yes", "factoid", "list"]}, {"id": "term-biobert", "t": "BioBERT", "tg": ["Models", "Technical", "Medical", "NLP"], "d": "models", "x": "A biomedical language representation model pre-trained on large-scale biomedical corpora including PubMed abstracts and...", "l": "b", "k": ["biobert", "biomedical", "language", "representation", "model", "pre-trained", "large-scale", "corpora", "including", "pubmed", "abstracts", "pmc", "full-text", "articles"]}, {"id": "term-bioes-tagging", "t": "BIOES Tagging", "tg": ["NLP", "Text Processing"], "d": "general", "x": "An extended sequence labeling scheme that adds End and Single tags to the BIO format, providing more precise boundary...", "l": "b", "k": ["bioes", "tagging", "extended", "sequence", "labeling", "scheme", "adds", "end", "single", "tags", "bio", "format", "providing", "precise", "boundary"]}, {"id": "term-biogpt", "t": "BioGPT", "tg": ["Models", "Technical"], "d": "models", "x": "A domain-specific generative language model pretrained on large-scale biomedical literature. Achieves strong...", "l": "b", "k": ["biogpt", "domain-specific", "generative", "language", "model", "pretrained", "large-scale", "biomedical", "literature", "achieves", "strong", "performance", "text", "generation", "question"]}, {"id": "term-biogpt-large", "t": "BioGPT-Large", "tg": ["Models", "Technical", "NLP", "Medical"], "d": "models", "x": "An expanded version of BioGPT with more parameters and training data for improved biomedical text generation and...", "l": "b", "k": ["biogpt-large", "expanded", "version", "biogpt", "parameters", "training", "data", "improved", "biomedical", "text", "generation", "knowledge", "extraction"]}, {"id": "term-biomedclip", "t": "BiomedCLIP", "tg": ["Models", "Technical", "Medical", "Vision"], "d": "models", "x": "A multimodal model trained on biomedical image-text pairs from scientific literature for biomedical visual question...", "l": "b", "k": ["biomedclip", "multimodal", "model", "trained", "biomedical", "image-text", "pairs", "scientific", "literature", "visual", "question", "answering", "image", "retrieval"]}, {"id": "term-biometric-ai-regulation", "t": "Biometric AI Regulation", "tg": ["Privacy", "Regulation"], "d": "safety", "x": "Legal restrictions on AI systems that process biometric data such as facial features, fingerprints, or gait patterns,...", "l": "b", "k": ["biometric", "regulation", "legal", "restrictions", "systems", "process", "data", "facial", "features", "fingerprints", "gait", "patterns", "including", "bans", "real-time"]}, {"id": "term-biomistral", "t": "BioMistral", "tg": ["Models", "Technical", "NLP", "Medical"], "d": "models", "x": "A biomedical language model built on Mistral 7B through continued pre-training on PubMed Central articles for medical...", "l": "b", "k": ["biomistral", "biomedical", "language", "model", "built", "mistral", "continued", "pre-training", "pubmed", "central", "articles", "medical", "text", "understanding", "generation"]}, {"id": "term-biosam", "t": "BioSAM", "tg": ["Models", "Technical", "Medical", "Vision"], "d": "models", "x": "A biomedical adaptation of the Segment Anything Model designed for cell and tissue segmentation in microscopy and...", "l": "b", "k": ["biosam", "biomedical", "adaptation", "segment", "anything", "model", "designed", "cell", "tissue", "segmentation", "microscopy", "histopathology", "images"]}, {"id": "term-birch", "t": "BIRCH", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Balanced Iterative Reducing and Clustering using Hierarchies is a clustering algorithm designed for large datasets....", "l": "b", "k": ["birch", "balanced", "iterative", "reducing", "clustering", "hierarchies", "algorithm", "designed", "large", "datasets", "builds", "compact", "summary", "called", "cf-tree"]}, {"id": "term-bird", "t": "BIRD", "tg": ["Benchmark", "NLP", "Code"], "d": "datasets", "x": "A large text-to-SQL benchmark featuring real-world databases with dirty data and external knowledge requirements. More...", "l": "b", "k": ["bird", "large", "text-to-sql", "benchmark", "featuring", "real-world", "databases", "dirty", "data", "external", "knowledge", "requirements", "challenging", "spider", "practical"]}, {"id": "term-biren-technology", "t": "Biren Technology", "tg": ["Accelerator", "GPU", "China"], "d": "hardware", "x": "Chinese AI chip company developing high-performance GPU alternatives for the Chinese market. Their BR100 chip targets...", "l": "b", "k": ["biren", "technology", "chinese", "chip", "company", "developing", "high-performance", "gpu", "alternatives", "market", "br100", "targets", "data", "center", "training"]}, {"id": "term-bisection-method", "t": "Bisection Method", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "A root-finding algorithm that repeatedly bisects an interval and selects the subinterval where the function changes...", "l": "b", "k": ["bisection", "method", "root-finding", "algorithm", "repeatedly", "bisects", "interval", "selects", "subinterval", "function", "changes", "sign", "guaranteed", "converge", "continuous"]}, {"id": "term-bisimulation-metric", "t": "Bisimulation Metric", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A distance metric on states that groups together states with similar reward and transition dynamics, providing a...", "l": "b", "k": ["bisimulation", "metric", "distance", "states", "groups", "together", "similar", "reward", "transition", "dynamics", "providing", "principled", "basis", "state", "abstraction"]}, {"id": "term-bit-precision", "t": "Bit Precision", "tg": ["Technical", "Optimization"], "d": "algorithms", "x": "The number of bits used to represent model weights and activations. Lower precision (8-bit, 4-bit) reduces memory and...", "l": "b", "k": ["bit", "precision", "number", "bits", "represent", "model", "weights", "activations", "lower", "8-bit", "4-bit", "reduces", "memory", "increases", "speed"]}, {"id": "term-bitonic-sort", "t": "Bitonic Sort", "tg": ["Algorithms", "Technical", "Sorting"], "d": "algorithms", "x": "A parallel sorting algorithm that first creates a bitonic sequence and then repeatedly merges bitonic sequences to...", "l": "b", "k": ["bitonic", "sort", "parallel", "sorting", "algorithm", "creates", "sequence", "repeatedly", "merges", "sequences", "produce", "sorted", "output", "well-suited", "hardware"]}, {"id": "term-bitter-lesson", "t": "Bitter Lesson", "tg": ["History", "Milestones"], "d": "history", "x": "An influential 2019 essay by Rich Sutton arguing that the history of AI shows general methods leveraging computation...", "l": "b", "k": ["bitter", "lesson", "influential", "essay", "rich", "sutton", "arguing", "history", "shows", "general", "methods", "leveraging", "computation", "search", "learning"]}, {"id": "term-black-box", "t": "Black Box", "tg": ["Interpretability", "Concept"], "d": "general", "x": "A system whose internal workings are not visible or understandable to users. Many AI models are considered black boxes...", "l": "b", "k": ["black", "box", "system", "whose", "internal", "workings", "visible", "understandable", "users", "models", "considered", "boxes", "decision-making", "processes", "difficult"]}, {"id": "term-black-box-problem", "t": "Black Box Problem", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The challenge that many AI systems, particularly deep neural networks, operate in ways that are opaque to human...", "l": "b", "k": ["black", "box", "problem", "challenge", "systems", "particularly", "deep", "neural", "networks", "operate", "ways", "opaque", "human", "understanding", "making"]}, {"id": "term-blackboard-system", "t": "Blackboard System", "tg": ["History", "Fundamentals"], "d": "history", "x": "An AI architecture where multiple knowledge sources cooperate to solve a problem by reading from and writing to a...", "l": "b", "k": ["blackboard", "system", "architecture", "multiple", "knowledge", "sources", "cooperate", "solve", "problem", "reading", "writing", "shared", "data", "structure", "called"]}, {"id": "term-blended-skill-talk", "t": "Blended Skill Talk", "tg": ["Benchmark", "NLP", "Dialogue"], "d": "datasets", "x": "A dialogue dataset requiring models to blend multiple conversational skills including knowledge personality and empathy...", "l": "b", "k": ["blended", "skill", "talk", "dialogue", "dataset", "requiring", "models", "blend", "multiple", "conversational", "skills", "including", "knowledge", "personality", "empathy"]}, {"id": "term-bletchley-declaration-on-ai", "t": "Bletchley Declaration on AI", "tg": ["Governance", "Regulation"], "d": "safety", "x": "A declaration signed by 28 countries at the November 2023 AI Safety Summit at Bletchley Park, acknowledging the...", "l": "b", "k": ["bletchley", "declaration", "signed", "countries", "november", "safety", "summit", "park", "acknowledging", "potential", "serious", "harm", "frontier", "committing", "international"]}, {"id": "term-bletchley-park-ai-safety-summit", "t": "Bletchley Park AI Safety Summit", "tg": ["History", "Governance"], "d": "history", "x": "The first major international AI Safety Summit held at Bletchley Park, UK, in November 2023, bringing together...", "l": "b", "k": ["bletchley", "park", "safety", "summit", "major", "international", "held", "november", "bringing", "together", "governments", "companies", "discuss", "frontier", "risks"]}, {"id": "term-bletchley-park-codebreaking", "t": "Bletchley Park Codebreaking", "tg": ["History", "Milestones"], "d": "history", "x": "The World War II British codebreaking operation where Alan Turing and colleagues developed the Bombe and Colossus...", "l": "b", "k": ["bletchley", "park", "codebreaking", "world", "war", "british", "operation", "alan", "turing", "colleagues", "developed", "bombe", "colossus", "machines", "decrypt"]}, {"id": "term-bleu-reference-data", "t": "BLEU Reference Data", "tg": ["Benchmark", "NLP", "Evaluation", "Translation"], "d": "datasets", "x": "Standard reference translation datasets used for computing BLEU scores in machine translation evaluation. Provides...", "l": "b", "k": ["bleu", "reference", "data", "standard", "translation", "datasets", "computing", "scores", "machine", "evaluation", "provides", "human", "translations", "gold", "standards"]}, {"id": "term-bleu-score", "t": "BLEU Score", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Bilingual Evaluation Understudy is a metric for evaluating machine translation quality by comparing n-gram overlap...", "l": "b", "k": ["bleu", "score", "bilingual", "evaluation", "understudy", "metric", "evaluating", "machine", "translation", "quality", "comparing", "n-gram", "overlap", "candidate", "reference"]}, {"id": "term-bleurt", "t": "BLEURT", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A learned evaluation metric that fine-tunes BERT on synthetic and human-rated data to predict text quality scores,...", "l": "b", "k": ["bleurt", "learned", "evaluation", "metric", "fine-tunes", "bert", "synthetic", "human-rated", "data", "predict", "text", "quality", "scores", "providing", "robust"]}, {"id": "term-block-matrix-algorithm", "t": "Block Matrix Algorithm", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A technique that partitions matrices into submatrices and performs operations on the blocks. Improves cache utilization...", "l": "b", "k": ["block", "matrix", "algorithm", "technique", "partitions", "matrices", "submatrices", "performs", "operations", "blocks", "improves", "cache", "utilization", "enables", "parallel"]}, {"id": "term-bloom", "t": "BLOOM", "tg": ["Model", "Open Source"], "d": "models", "x": "A large multilingual open-source language model created by BigScience, trained on 46 languages. Demonstrated the...", "l": "b", "k": ["bloom", "large", "multilingual", "open-source", "language", "model", "created", "bigscience", "trained", "languages", "demonstrated", "viability", "collaborative", "open", "development"]}, {"id": "term-bloom-filter-algorithm", "t": "Bloom Filter Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A space-efficient probabilistic data structure that tests whether an element is a member of a set. May produce false...", "l": "b", "k": ["bloom", "filter", "algorithm", "space-efficient", "probabilistic", "data", "structure", "tests", "element", "member", "produce", "false", "positives", "never", "negatives"]}, {"id": "term-bloom-176b", "t": "BLOOM-176B", "tg": ["Models", "Technical", "NLP", "History"], "d": "models", "x": "The largest variant of the BLOOM multilingual model with 176 billion parameters trained collaboratively by over 1000...", "l": "b", "k": ["bloom-176b", "largest", "variant", "bloom", "multilingual", "model", "billion", "parameters", "trained", "collaboratively", "researchers", "across", "languages"]}, {"id": "term-bloomberggpt", "t": "BloombergGPT", "tg": ["Models", "Technical"], "d": "models", "x": "A 50 billion parameter language model by Bloomberg trained on a mix of financial data and general text. Designed for...", "l": "b", "k": ["bloomberggpt", "billion", "parameter", "language", "model", "bloomberg", "trained", "mix", "financial", "data", "general", "text", "designed", "nlp", "tasks"]}, {"id": "term-blue-team-ai", "t": "Blue Team (AI)", "tg": ["Safety", "Technical"], "d": "safety", "x": "A team responsible for defending AI systems against adversarial attacks and identifying vulnerabilities. Works in...", "l": "b", "k": ["blue", "team", "responsible", "defending", "systems", "against", "adversarial", "attacks", "identifying", "vulnerabilities", "works", "opposition", "red", "teams", "improve"]}, {"id": "term-bluelm", "t": "BlueLM", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A large language model from vivo with 7B and 13B parameters that supports both English and Chinese with 128K context...", "l": "b", "k": ["bluelm", "large", "language", "model", "vivo", "13b", "parameters", "supports", "english", "chinese", "128k", "context", "window", "extensions"]}, {"id": "term-bm25", "t": "BM25", "tg": ["NLP", "Text Processing"], "d": "general", "x": "Best Matching 25, a probabilistic information retrieval ranking function that extends TF-IDF with document length...", "l": "b", "k": ["bm25", "best", "matching", "probabilistic", "information", "retrieval", "ranking", "function", "extends", "tf-idf", "document", "length", "normalization", "term", "frequency"]}, {"id": "term-bm25-algorithm", "t": "BM25 Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP"], "d": "algorithms", "x": "A probabilistic information retrieval ranking function that scores documents based on query term frequency and document...", "l": "b", "k": ["bm25", "algorithm", "probabilistic", "information", "retrieval", "ranking", "function", "scores", "documents", "based", "query", "term", "frequency", "document", "length"]}, {"id": "term-bm25-in-rag", "t": "BM25 in RAG", "tg": ["Retrieval", "Search"], "d": "general", "x": "The application of the Best Matching 25 probabilistic ranking function within retrieval-augmented generation pipelines,...", "l": "b", "k": ["bm25", "rag", "application", "best", "matching", "probabilistic", "ranking", "function", "within", "retrieval-augmented", "generation", "pipelines", "providing", "strong", "lexical"]}, {"id": "term-bold", "t": "BOLD", "tg": ["Benchmark", "NLP", "Fairness"], "d": "datasets", "x": "Bias in Open-ended Language Generation Dataset a benchmark of 23679 prompts for evaluating social biases across 5...", "l": "b", "k": ["bold", "bias", "open-ended", "language", "generation", "dataset", "benchmark", "prompts", "evaluating", "social", "biases", "across", "domains", "text", "models"]}, {"id": "term-boltzmann-exploration", "t": "Boltzmann Exploration", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An exploration strategy that selects actions with probability proportional to exponentiated Q-values divided by a...", "l": "b", "k": ["boltzmann", "exploration", "strategy", "selects", "actions", "probability", "proportional", "exponentiated", "q-values", "divided", "temperature", "parameter", "higher", "temperatures", "increase"]}, {"id": "term-boltzmann-machine", "t": "Boltzmann Machine", "tg": ["History", "Milestones"], "d": "history", "x": "A stochastic neural network model invented by Geoffrey Hinton and Terry Sejnowski in 1985 that uses simulated annealing...", "l": "b", "k": ["boltzmann", "machine", "stochastic", "neural", "network", "model", "invented", "geoffrey", "hinton", "terry", "sejnowski", "uses", "simulated", "annealing", "learn"]}, {"id": "term-bonferroni-correction", "t": "Bonferroni Correction", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A multiple comparison adjustment that divides the significance level by the number of tests performed, controlling the...", "l": "b", "k": ["bonferroni", "correction", "multiple", "comparison", "adjustment", "divides", "significance", "level", "number", "tests", "performed", "controlling", "family-wise", "error", "rate"]}, {"id": "term-bookcorpus", "t": "BookCorpus", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A dataset of 11000 free books from unpublished authors used to train the original BERT model alongside Wikipedia....", "l": "b", "k": ["bookcorpus", "dataset", "free", "books", "unpublished", "authors", "train", "original", "bert", "model", "alongside", "wikipedia", "provides", "narrative", "text"]}, {"id": "term-booksum", "t": "BookSum", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A dataset for long-form narrative summarization of full-length books and their chapter-level and book-level summaries....", "l": "b", "k": ["booksum", "dataset", "long-form", "narrative", "summarization", "full-length", "books", "chapter-level", "book-level", "summaries", "tests", "ability", "comprehend", "summarize", "long"]}, {"id": "term-boolean-retrieval", "t": "Boolean Retrieval", "tg": ["Search", "Traditional"], "d": "general", "x": "Search using AND, OR, NOT operators to combine terms. Simple but limited compared to semantic search. Still used in...", "l": "b", "k": ["boolean", "retrieval", "search", "operators", "combine", "terms", "simple", "limited", "compared", "semantic", "specialized", "databases", "advanced", "interfaces"]}, {"id": "term-boolq", "t": "BoolQ", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A question answering dataset of 15942 yes/no questions naturally generated from Google search queries. Tests the...", "l": "b", "k": ["boolq", "question", "answering", "dataset", "yes", "questions", "naturally", "generated", "google", "search", "queries", "tests", "ability", "perform", "boolean"]}, {"id": "term-boolq-contrast-set", "t": "BoolQ Contrast Set", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A modified version of BoolQ with expert-crafted perturbations that change the correct answer. Tests model robustness to...", "l": "b", "k": ["boolq", "contrast", "modified", "version", "expert-crafted", "perturbations", "change", "correct", "answer", "tests", "model", "robustness", "small", "meaningful", "input"]}, {"id": "term-boosting", "t": "Boosting", "tg": ["Technique", "ML"], "d": "general", "x": "An ensemble technique that trains models sequentially, with each new model focusing on examples the previous ones got...", "l": "b", "k": ["boosting", "ensemble", "technique", "trains", "models", "sequentially", "model", "focusing", "examples", "previous", "ones", "got", "wrong", "powers", "xgboost"]}, {"id": "term-boosting-history", "t": "Boosting History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of boosting algorithms from the theoretical work of Michael Kearns and Leslie Valiant (1988) through...", "l": "b", "k": ["boosting", "history", "development", "algorithms", "theoretical", "work", "michael", "kearns", "leslie", "valiant", "adaboost", "freund", "schapire", "gradient", "friedman"]}, {"id": "term-bootstrap", "t": "Bootstrap", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A resampling technique that estimates the sampling distribution of a statistic by repeatedly drawing samples with...", "l": "b", "k": ["bootstrap", "resampling", "technique", "estimates", "sampling", "distribution", "statistic", "repeatedly", "drawing", "samples", "replacement", "observed", "data", "provides", "standard"]}, {"id": "term-bootstrap-aggregating", "t": "Bootstrap Aggregating", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An ensemble method (also called bagging) that trains multiple models on different bootstrap samples of the training...", "l": "b", "k": ["bootstrap", "aggregating", "ensemble", "method", "called", "bagging", "trains", "multiple", "models", "different", "samples", "training", "data", "combines", "predictions"]}, {"id": "term-boruvkas-algorithm", "t": "Boruvka's Algorithm", "tg": ["Algorithms", "Technical", "Graph", "History"], "d": "algorithms", "x": "One of the earliest known algorithms for finding a minimum spanning tree. Works by simultaneously finding the cheapest...", "l": "b", "k": ["boruvka", "algorithm", "earliest", "known", "algorithms", "finding", "minimum", "spanning", "tree", "works", "simultaneously", "cheapest", "edge", "component", "merging"]}, {"id": "term-boston-housing", "t": "Boston Housing", "tg": ["Benchmark", "Tabular"], "d": "datasets", "x": "A classic regression dataset containing housing prices in Boston suburbs with 13 features. Widely used for teaching...", "l": "b", "k": ["boston", "housing", "classic", "regression", "dataset", "containing", "prices", "suburbs", "features", "widely", "teaching", "analysis", "criticized", "racial", "bias"]}, {"id": "term-bottleneck", "t": "Bottleneck", "tg": ["Architecture", "Design"], "d": "models", "x": "A narrow layer in a neural network that forces compression of information. Used in autoencoders and some architectures...", "l": "b", "k": ["bottleneck", "narrow", "layer", "neural", "network", "forces", "compression", "information", "autoencoders", "architectures", "learn", "efficient", "representations"]}, {"id": "term-bottleneck-layer", "t": "Bottleneck Layer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A narrow hidden layer that compresses representations to a lower dimension before expanding them, used in autoencoders...", "l": "b", "k": ["bottleneck", "layer", "narrow", "hidden", "compresses", "representations", "lower", "dimension", "expanding", "autoencoders", "residual", "blocks", "reduce", "computation", "encourage"]}, {"id": "term-boundary-element-method", "t": "Boundary Element Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A numerical technique that solves boundary integral equations rather than volume equations reducing the problem...", "l": "b", "k": ["boundary", "element", "method", "numerical", "technique", "solves", "integral", "equations", "rather", "volume", "reducing", "problem", "dimension", "particularly", "efficient"]}, {"id": "term-boundary-testing", "t": "Boundary Testing", "tg": ["Safety", "Technical"], "d": "safety", "x": "The practice of probing AI systems at the edges of their intended operating conditions to identify failure modes and...", "l": "b", "k": ["boundary", "testing", "practice", "probing", "systems", "edges", "intended", "operating", "conditions", "identify", "failure", "modes", "safety", "limitations", "essential"]}, {"id": "term-box-cox-transformation", "t": "Box-Cox Transformation", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A family of power transformations parametrized by lambda that aims to stabilize variance and make data more normally...", "l": "b", "k": ["box-cox", "transformation", "family", "power", "transformations", "parametrized", "lambda", "aims", "stabilize", "variance", "data", "normally", "distributed", "special", "cases"]}, {"id": "term-boyer-moore-algorithm", "t": "Boyer-Moore Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP", "Searching"], "d": "algorithms", "x": "A string-searching algorithm that preprocesses the pattern to skip sections of text that cannot contain a match. Uses...", "l": "b", "k": ["boyer-moore", "algorithm", "string-searching", "preprocesses", "pattern", "skip", "sections", "text", "cannot", "contain", "match", "uses", "bad", "character", "good"]}, {"id": "term-bpe", "t": "BPE (Byte Pair Encoding)", "tg": ["Tokenization", "NLP"], "d": "general", "x": "A tokenization algorithm that breaks text into subword units. Starts with individual characters and iteratively merges...", "l": "b", "k": ["bpe", "byte", "pair", "encoding", "tokenization", "algorithm", "breaks", "text", "subword", "units", "starts", "individual", "characters", "iteratively", "merges"]}, {"id": "term-bpr", "t": "BPR", "tg": ["Models", "Technical", "Recommendation"], "d": "models", "x": "Bayesian Personalized Ranking is a pairwise learning framework for implicit feedback recommendation that optimizes the...", "l": "b", "k": ["bpr", "bayesian", "personalized", "ranking", "pairwise", "learning", "framework", "implicit", "feedback", "recommendation", "optimizes", "observed", "unobserved", "items"]}, {"id": "term-brain-computer", "t": "Brain-Computer Interface (BCI)", "tg": ["Application", "Neuroscience"], "d": "general", "x": "Technology connecting brain signals directly to computers. AI helps interpret neural signals for prosthetics,...", "l": "b", "k": ["brain-computer", "interface", "bci", "technology", "connecting", "brain", "signals", "directly", "computers", "helps", "interpret", "neural", "prosthetics", "communication", "devices"]}, {"id": "term-brainscales", "t": "BrainScaleS", "tg": ["Neuromorphic", "Research", "Europe"], "d": "hardware", "x": "European neuromorphic computing platform that implements neural networks in analog hardware running 10000 times faster...", "l": "b", "k": ["brainscales", "european", "neuromorphic", "computing", "platform", "implements", "neural", "networks", "analog", "hardware", "running", "times", "faster", "biological", "real"]}, {"id": "term-branch-and-bound", "t": "Branch and Bound", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An algorithmic paradigm for solving combinatorial optimization problems that systematically enumerates candidates while...", "l": "b", "k": ["branch", "bound", "algorithmic", "paradigm", "solving", "combinatorial", "optimization", "problems", "systematically", "enumerates", "candidates", "pruning", "large", "portions", "search"]}, {"id": "term-branch-prediction", "t": "Branch Prediction", "tg": ["Architecture", "Processor", "Optimization"], "d": "hardware", "x": "Processor mechanism that guesses the outcome of conditional branches before they are resolved. Accurate prediction...", "l": "b", "k": ["branch", "prediction", "processor", "mechanism", "guesses", "outcome", "conditional", "branches", "resolved", "accurate", "prevents", "pipeline", "stalls", "maintains", "high"]}, {"id": "term-breadth-first-search", "t": "Breadth-First Search", "tg": ["Algorithms", "Fundamentals", "Graph", "Searching"], "d": "algorithms", "x": "A graph traversal algorithm that explores all neighbors at the current depth before moving to the next level. Uses a...", "l": "b", "k": ["breadth-first", "search", "graph", "traversal", "algorithm", "explores", "neighbors", "current", "depth", "moving", "next", "level", "uses", "queue", "data"]}, {"id": "term-breast-cancer-wisconsin", "t": "Breast Cancer Wisconsin", "tg": ["Benchmark", "Tabular", "Medical"], "d": "datasets", "x": "A dataset of 569 fine needle aspirate cell measurements for classifying breast tumors as malignant or benign. Widely...", "l": "b", "k": ["breast", "cancer", "wisconsin", "dataset", "fine", "needle", "aspirate", "cell", "measurements", "classifying", "tumors", "malignant", "benign", "widely", "binary"]}, {"id": "term-brents-method", "t": "Brent's Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A root-finding algorithm that combines the bisection method with the secant method and inverse quadratic interpolation....", "l": "b", "k": ["brent", "method", "root-finding", "algorithm", "combines", "bisection", "secant", "inverse", "quadratic", "interpolation", "guarantees", "convergence", "achieving", "superlinear", "speed"]}, {"id": "term-bridge-detection-algorithm", "t": "Bridge Detection Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm that finds edges in an undirected graph whose removal disconnects the graph. Uses a modified depth-first...", "l": "b", "k": ["bridge", "detection", "algorithm", "finds", "edges", "undirected", "graph", "whose", "removal", "disconnects", "uses", "modified", "depth-first", "search", "tracking"]}, {"id": "term-brier-score", "t": "Brier Score", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A scoring metric that measures the accuracy of probabilistic predictions by computing the mean squared difference...", "l": "b", "k": ["brier", "score", "scoring", "metric", "measures", "accuracy", "probabilistic", "predictions", "computing", "mean", "squared", "difference", "predicted", "probabilities", "actual"]}, {"id": "term-bron-kerbosch-algorithm", "t": "Bron-Kerbosch Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A backtracking algorithm for finding all maximal cliques in an undirected graph. Uses pivot selection to prune the...", "l": "b", "k": ["bron-kerbosch", "algorithm", "backtracking", "finding", "maximal", "cliques", "undirected", "graph", "uses", "pivot", "selection", "prune", "search", "space", "widely"]}, {"id": "term-browse-mode", "t": "Browse Mode", "tg": ["Feature", "Capability"], "d": "general", "x": "AI capability to access and retrieve current web information during conversations. Addresses knowledge cutoff...", "l": "b", "k": ["browse", "mode", "capability", "access", "retrieve", "current", "web", "information", "conversations", "addresses", "knowledge", "cutoff", "limitations", "fetching", "real-time"]}, {"id": "term-broydens-method", "t": "Broyden's Method", "tg": ["Algorithms", "Technical", "Numerical", "Optimization"], "d": "algorithms", "x": "A quasi-Newton method for solving systems of nonlinear equations that approximates the Jacobian using rank-one updates....", "l": "b", "k": ["broyden", "method", "quasi-newton", "solving", "systems", "nonlinear", "equations", "approximates", "jacobian", "rank-one", "updates", "avoids", "cost", "recomputing", "full"]}, {"id": "term-bruce-buchanan", "t": "Bruce Buchanan", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who co-developed DENDRAL the first expert system with Edward Feigenbaum and Joshua...", "l": "b", "k": ["bruce", "buchanan", "american", "computer", "scientist", "co-developed", "dendral", "expert", "system", "edward", "feigenbaum", "joshua", "lederberg", "stanford", "work"]}, {"id": "term-brushnet", "t": "BrushNet", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A dual-branch diffusion model for image inpainting that separately processes masked image features and noisy latent...", "l": "b", "k": ["brushnet", "dual-branch", "diffusion", "model", "image", "inpainting", "separately", "processes", "masked", "features", "noisy", "latent", "high-quality", "completion"]}, {"id": "term-bubble-sort", "t": "Bubble Sort", "tg": ["Algorithms", "Fundamentals", "Sorting"], "d": "algorithms", "x": "A simple comparison-based sorting algorithm that repeatedly swaps adjacent elements if they are in the wrong order. Has...", "l": "b", "k": ["bubble", "sort", "simple", "comparison-based", "sorting", "algorithm", "repeatedly", "swaps", "adjacent", "elements", "wrong", "order", "time", "complexity", "primarily"]}, {"id": "term-bucket-sort", "t": "Bucket Sort", "tg": ["Algorithms", "Technical", "Sorting"], "d": "algorithms", "x": "A distribution sort that divides elements into a fixed number of buckets and then sorts each bucket individually....", "l": "b", "k": ["bucket", "sort", "distribution", "divides", "elements", "fixed", "number", "buckets", "sorts", "individually", "achieves", "average", "time", "input", "uniformly"]}, {"id": "term-buffer", "t": "Buffer (Memory)", "tg": ["Technical", "Architecture"], "d": "models", "x": "Temporary storage for data being processed. In AI agents, conversation buffers store recent exchanges. In training,...", "l": "b", "k": ["buffer", "memory", "temporary", "storage", "data", "processed", "agents", "conversation", "buffers", "store", "recent", "exchanges", "training", "optimize", "gpu"]}, {"id": "term-bugsinpy", "t": "BugsInPy", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A database of real bugs from Python programs with test cases for each bug. Used for benchmarking automated debugging...", "l": "b", "k": ["bugsinpy", "database", "real", "bugs", "python", "programs", "test", "cases", "bug", "benchmarking", "automated", "debugging", "program", "repair", "approaches"]}, {"id": "term-bundle-adjustment", "t": "Bundle Adjustment", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "An optimization procedure that jointly refines 3D point positions and camera parameters by minimizing the reprojection...", "l": "b", "k": ["bundle", "adjustment", "optimization", "procedure", "jointly", "refines", "point", "positions", "camera", "parameters", "minimizing", "reprojection", "error", "across", "views"]}, {"id": "term-bundle-method", "t": "Bundle Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An optimization technique for non-smooth convex minimization that accumulates subgradient information in a bundle....", "l": "b", "k": ["bundle", "method", "optimization", "technique", "non-smooth", "convex", "minimization", "accumulates", "subgradient", "information", "constructs", "piecewise-linear", "model", "objective", "function"]}, {"id": "term-burgs-method", "t": "Burg's Method", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A parametric spectral estimation technique that fits an autoregressive model to data by minimizing forward and backward...", "l": "b", "k": ["burg", "method", "parametric", "spectral", "estimation", "technique", "fits", "autoregressive", "model", "data", "minimizing", "forward", "backward", "prediction", "errors"]}, {"id": "term-burn-in-testing", "t": "Burn-In Testing", "tg": ["Manufacturing", "Testing", "Reliability"], "d": "hardware", "x": "Stress testing of semiconductor chips at elevated temperatures and voltages to identify early failures. Ensures...", "l": "b", "k": ["burn-in", "testing", "stress", "semiconductor", "chips", "elevated", "temperatures", "voltages", "identify", "early", "failures", "ensures", "reliability", "operate", "continuously"]}, {"id": "term-burrows-wheeler-transform", "t": "Burrows-Wheeler Transform", "tg": ["Algorithms", "Technical", "NLP", "Information Theory"], "d": "algorithms", "x": "A reversible text transformation that rearranges characters to group similar characters together. Widely used in data...", "l": "b", "k": ["burrows-wheeler", "transform", "reversible", "text", "transformation", "rearranges", "characters", "group", "similar", "together", "widely", "data", "compression", "bzip2", "bioinformatics"]}, {"id": "term-burst", "t": "Burst (API)", "tg": ["API", "Usage"], "d": "general", "x": "Short periods of high API usage that may exceed normal rate limits. Many providers allow bursting with gradual...", "l": "b", "k": ["burst", "api", "short", "periods", "high", "usage", "exceed", "normal", "rate", "limits", "providers", "allow", "bursting", "gradual", "throttling"]}, {"id": "term-burstiness", "t": "Burstiness", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A statistical property measuring the variability of sentence length and structure in text, often used in AI-generated...", "l": "b", "k": ["burstiness", "statistical", "property", "measuring", "variability", "sentence", "length", "structure", "text", "ai-generated", "detection", "machine-generated", "content", "tends", "show"]}, {"id": "term-butterworth-filter", "t": "Butterworth Filter", "tg": ["Algorithms", "Fundamentals", "Signal Processing"], "d": "algorithms", "x": "An analog or digital filter designed to have a maximally flat frequency response in the passband. Named after Stephen...", "l": "b", "k": ["butterworth", "filter", "analog", "digital", "designed", "maximally", "flat", "frequency", "response", "passband", "named", "stephen", "characterized", "smooth", "roll-off"]}, {"id": "term-byte-fallback", "t": "Byte Fallback", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A tokenization strategy that encodes unknown characters as individual bytes when they cannot be represented by the...", "l": "b", "k": ["byte", "fallback", "tokenization", "strategy", "encodes", "unknown", "characters", "individual", "bytes", "cannot", "represented", "learned", "vocabulary", "ensuring", "possible"]}, {"id": "term-byte-pair-encoding", "t": "Byte Pair Encoding", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A subword tokenization algorithm that iteratively merges the most frequent pair of consecutive bytes or characters....", "l": "b", "k": ["byte", "pair", "encoding", "subword", "tokenization", "algorithm", "iteratively", "merges", "frequent", "consecutive", "bytes", "characters", "originally", "data", "compression"]}, {"id": "term-bpe-tokenizer", "t": "Byte Pair Encoding Tokenizer", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A subword tokenization algorithm that iteratively merges the most frequent pair of adjacent bytes or characters in the...", "l": "b", "k": ["byte", "pair", "encoding", "tokenizer", "subword", "tokenization", "algorithm", "iteratively", "merges", "frequent", "adjacent", "bytes", "characters", "training", "corpus"]}, {"id": "term-byte-level-bpe-algorithm", "t": "Byte-Level BPE Algorithm", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "A variant of byte pair encoding that operates on raw bytes rather than Unicode characters. Enables tokenization of any...", "l": "b", "k": ["byte-level", "bpe", "algorithm", "variant", "byte", "pair", "encoding", "operates", "raw", "bytes", "rather", "unicode", "characters", "enables", "tokenization"]}, {"id": "term-byte-level-tokenization", "t": "Byte-Level Tokenization", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A tokenization approach that operates on raw bytes rather than characters, ensuring complete coverage of any text input...", "l": "b", "k": ["byte-level", "tokenization", "approach", "operates", "raw", "bytes", "rather", "characters", "ensuring", "complete", "coverage", "text", "input", "without", "unknown"]}, {"id": "term-c-eval", "t": "C-Eval", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "A comprehensive Chinese evaluation benchmark spanning 52 subjects across four difficulty levels. Tests Chinese language...", "l": "c", "k": ["c-eval", "comprehensive", "chinese", "evaluation", "benchmark", "spanning", "subjects", "across", "four", "difficulty", "levels", "tests", "language", "model", "capabilities"]}, {"id": "term-c2pa", "t": "C2PA", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The Coalition for Content Provenance and Authenticity, a joint development foundation creating technical standards for...", "l": "c", "k": ["c2pa", "coalition", "content", "provenance", "authenticity", "joint", "development", "foundation", "creating", "technical", "standards", "certifying", "source", "history", "media"]}, {"id": "term-c4", "t": "C4", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "The Colossal Clean Crawled Corpus a 750GB cleaned version of Common Crawl created by Google for training the T5 model....", "l": "c", "k": ["colossal", "clean", "crawled", "corpus", "750gb", "cleaned", "version", "common", "crawl", "created", "google", "training", "model", "applies", "deduplication"]}, {"id": "term-cadence-design-systems", "t": "Cadence Design Systems", "tg": ["Manufacturing", "EDA", "Company"], "d": "hardware", "x": "Major electronic design automation company providing chip design and verification tools. Their software is essential...", "l": "c", "k": ["cadence", "design", "systems", "major", "electronic", "automation", "company", "providing", "chip", "verification", "tools", "software", "essential", "designing", "complex"]}, {"id": "term-caffe-framework", "t": "Caffe Framework", "tg": ["History", "Milestones"], "d": "history", "x": "A deep learning framework developed by Yangqing Jia at UC Berkeley in 2013. Caffe (Convolutional Architecture for Fast...", "l": "c", "k": ["caffe", "framework", "deep", "learning", "developed", "yangqing", "jia", "berkeley", "convolutional", "architecture", "fast", "feature", "embedding", "widely", "computer"]}, {"id": "term-caformer", "t": "CAFormer", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A MetaFormer variant that combines convolutional token mixers in early stages with attention token mixers in later...", "l": "c", "k": ["caformer", "metaformer", "variant", "combines", "convolutional", "token", "mixers", "early", "stages", "attention", "later", "strong", "image", "classification"]}, {"id": "term-calibration", "t": "Calibration", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "The degree to which a model's predicted probabilities match the true frequencies of outcomes. A well-calibrated model...", "l": "c", "k": ["calibration", "degree", "model", "predicted", "probabilities", "match", "true", "frequencies", "outcomes", "well-calibrated", "predicting", "probability", "correct", "approximately", "time"]}, {"id": "term-calibration-data", "t": "Calibration Data for Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A representative subset of input data used to determine optimal quantization parameters such as scaling factors and...", "l": "c", "k": ["calibration", "data", "quantization", "representative", "subset", "input", "determine", "optimal", "parameters", "scaling", "factors", "zero", "points", "quality", "directly"]}, {"id": "term-calibration-error", "t": "Calibration Error", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that measures the discrepancy between a model's predicted confidence and its actual accuracy, where a...", "l": "c", "k": ["calibration", "error", "metric", "measures", "discrepancy", "model", "predicted", "confidence", "actual", "accuracy", "well-calibrated", "stated", "probability", "correct", "closely"]}, {"id": "term-calibration-fairness", "t": "Calibration Fairness", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness metric requiring that among individuals assigned a given predicted probability, the actual proportion of...", "l": "c", "k": ["calibration", "fairness", "metric", "requiring", "among", "individuals", "assigned", "given", "predicted", "probability", "actual", "proportion", "positive", "outcomes", "across"]}, {"id": "term-california-housing", "t": "California Housing", "tg": ["Benchmark", "Tabular"], "d": "datasets", "x": "A dataset of housing prices in California districts used for regression benchmarking. Contains median house values and...", "l": "c", "k": ["california", "housing", "dataset", "prices", "districts", "regression", "benchmarking", "contains", "median", "house", "values", "socioeconomic", "features", "census"]}, {"id": "term-callback", "t": "Callback", "tg": ["Technical", "Training"], "d": "general", "x": "A function called at specific points during training or inference. Used for logging, checkpointing, early stopping, and...", "l": "c", "k": ["callback", "function", "called", "specific", "points", "training", "inference", "logging", "checkpointing", "early", "stopping", "custom", "behaviors", "pipelines"]}, {"id": "term-caltech-101", "t": "Caltech-101", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A dataset of images from 101 object categories plus a background category with 40 to 800 images per class. Created at...", "l": "c", "k": ["caltech-101", "dataset", "images", "object", "categories", "plus", "background", "category", "per", "class", "created", "caltech", "widely", "recognition", "benchmarking"]}, {"id": "term-caltech-256", "t": "Caltech-256", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "An extension of Caltech-101 containing 30607 images across 256 object categories plus a clutter class. Provides a more...", "l": "c", "k": ["caltech-256", "extension", "caltech-101", "containing", "images", "across", "object", "categories", "plus", "clutter", "class", "provides", "challenging", "diverse", "recognition"]}, {"id": "term-cambrian-1", "t": "Cambrian-1", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A family of multimodal language models that systematically study visual representation learning and achieves strong...", "l": "c", "k": ["cambrian-1", "family", "multimodal", "language", "models", "systematically", "study", "visual", "representation", "learning", "achieves", "strong", "performance", "dynamic", "token"]}, {"id": "term-cambricon-technologies", "t": "Cambricon Technologies", "tg": ["Accelerator", "China"], "d": "hardware", "x": "Chinese AI chip company that developed some of the earliest commercial neural network processors. Their MLU chips are...", "l": "c", "k": ["cambricon", "technologies", "chinese", "chip", "company", "developed", "earliest", "commercial", "neural", "network", "processors", "mlu", "chips", "deployed", "across"]}, {"id": "term-camera-calibration", "t": "Camera Calibration", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The process of estimating the intrinsic parameters (focal length, principal point, distortion) and extrinsic parameters...", "l": "c", "k": ["camera", "calibration", "process", "estimating", "intrinsic", "parameters", "focal", "length", "principal", "point", "distortion", "extrinsic", "position", "orientation", "essential"]}, {"id": "term-canaicode", "t": "CanAICode", "tg": ["Benchmark", "Code", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating the coding abilities of language models across multiple programming languages and problem...", "l": "c", "k": ["canaicode", "benchmark", "evaluating", "coding", "abilities", "language", "models", "across", "multiple", "programming", "languages", "problem", "types", "provides", "standardized"]}, {"id": "term-canary", "t": "Canary", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "A multilingual speech recognition model from NVIDIA NeMo that handles automatic speech recognition and translation...", "l": "c", "k": ["canary", "multilingual", "speech", "recognition", "model", "nvidia", "nemo", "handles", "automatic", "translation", "across", "multiple", "languages", "simultaneously"]}, {"id": "term-canny-edge-detection", "t": "Canny Edge Detection", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "A multi-stage edge detection algorithm that applies Gaussian smoothing and gradient computation and non-maximum...", "l": "c", "k": ["canny", "edge", "detection", "multi-stage", "algorithm", "applies", "gaussian", "smoothing", "gradient", "computation", "non-maximum", "suppression", "hysteresis", "thresholding", "produce"]}, {"id": "term-canonical-correlation-analysis", "t": "Canonical Correlation Analysis", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A multivariate statistical method that finds linear combinations of variables from two datasets that are maximally...", "l": "c", "k": ["canonical", "correlation", "analysis", "multivariate", "statistical", "method", "finds", "linear", "combinations", "variables", "datasets", "maximally", "correlated", "multi-view", "learning"]}, {"id": "term-canopy-clustering-algorithm", "t": "Canopy Clustering Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A fast pre-clustering algorithm that creates overlapping canopies using two distance thresholds. Typically used as a...", "l": "c", "k": ["canopy", "clustering", "algorithm", "fast", "pre-clustering", "creates", "overlapping", "canopies", "distance", "thresholds", "typically", "preprocessing", "step", "speed", "expensive"]}, {"id": "term-capability", "t": "Capability (AI)", "tg": ["Concept", "Assessment"], "d": "general", "x": "A specific skill or function an AI system can perform. Capabilities range from basic (text generation) to advanced...", "l": "c", "k": ["capability", "specific", "skill", "function", "system", "perform", "capabilities", "range", "basic", "text", "generation", "advanced", "multi-step", "reasoning", "tool"]}, {"id": "term-capability-control", "t": "Capability Control", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "Safety measures that limit what an AI system can do by restricting its access to resources, communication channels, or...", "l": "c", "k": ["capability", "control", "safety", "measures", "limit", "system", "restricting", "access", "resources", "communication", "channels", "actuators", "opposed", "motivational", "shapes"]}, {"id": "term-capability-elicitation", "t": "Capability Elicitation", "tg": ["Safety", "Technical"], "d": "safety", "x": "Techniques for systematically discovering what an AI system can do including capabilities that may not be apparent from...", "l": "c", "k": ["capability", "elicitation", "techniques", "systematically", "discovering", "system", "including", "capabilities", "apparent", "standard", "benchmarks", "important", "understanding", "potential", "risks"]}, {"id": "term-capsule-network", "t": "Capsule Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A neural network architecture that uses groups of neurons (capsules) to encode both the presence and instantiation...", "l": "c", "k": ["capsule", "network", "neural", "architecture", "uses", "groups", "neurons", "capsules", "encode", "presence", "instantiation", "parameters", "features", "dynamic", "routing"]}, {"id": "term-capsule-networks", "t": "Capsule Networks", "tg": ["History", "Systems"], "d": "history", "x": "A neural network architecture proposed by Geoffrey Hinton and colleagues (Sabour et al. 2017) that uses groups of...", "l": "c", "k": ["capsule", "networks", "neural", "network", "architecture", "proposed", "geoffrey", "hinton", "colleagues", "sabour", "uses", "groups", "neurons", "capsules", "represent"]}, {"id": "term-captioning", "t": "Captioning", "tg": ["Task", "Multimodal"], "d": "general", "x": "Generating text descriptions of images or videos. A multimodal task requiring visual understanding and language...", "l": "c", "k": ["captioning", "generating", "text", "descriptions", "images", "videos", "multimodal", "task", "requiring", "visual", "understanding", "language", "generation", "accessibility", "content"]}, {"id": "term-capybara", "t": "Capybara", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A multi-turn conversational dataset designed for training helpful and detailed AI assistants. Covers diverse topics...", "l": "c", "k": ["capybara", "multi-turn", "conversational", "dataset", "designed", "training", "helpful", "detailed", "assistants", "covers", "diverse", "topics", "extended", "conversation", "chains"]}, {"id": "term-carbon-footprint-of-ai-training", "t": "Carbon Footprint of AI Training", "tg": ["Sustainability", "Training"], "d": "hardware", "x": "The greenhouse gas emissions generated by the energy consumed during AI model training. Training a single large...", "l": "c", "k": ["carbon", "footprint", "training", "greenhouse", "gas", "emissions", "generated", "energy", "consumed", "model", "single", "large", "language", "produce", "equivalent"]}, {"id": "term-carbon-nanotube-transistor", "t": "Carbon Nanotube Transistor", "tg": ["Emerging", "Materials", "Transistor"], "d": "hardware", "x": "Transistor using carbon nanotubes as the channel material instead of silicon. Promises better performance and lower...", "l": "c", "k": ["carbon", "nanotube", "transistor", "nanotubes", "channel", "material", "instead", "silicon", "promises", "better", "performance", "lower", "power", "transistors", "extremely"]}, {"id": "term-carla", "t": "CARLA", "tg": ["Benchmark", "Reinforcement Learning", "Autonomous Driving"], "d": "datasets", "x": "An open-source simulator for autonomous driving research providing urban driving environments with dynamic weather and...", "l": "c", "k": ["carla", "open-source", "simulator", "autonomous", "driving", "research", "providing", "urban", "environments", "dynamic", "weather", "traffic", "training", "evaluating", "self-driving"]}, {"id": "term-carnegie-mellon-ai", "t": "Carnegie Mellon AI", "tg": ["History", "Organizations"], "d": "history", "x": "AI research programs at Carnegie Mellon University including the work of Herbert Simon Allen Newell and Raj Reddy. Home...", "l": "c", "k": ["carnegie", "mellon", "research", "programs", "university", "including", "work", "herbert", "simon", "allen", "newell", "raj", "reddy", "home", "robotics"]}, {"id": "term-cartesian-tree-algorithm", "t": "Cartesian Tree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A binary tree derived from a sequence of numbers where the root is the minimum element and left and right subtrees are...", "l": "c", "k": ["cartesian", "tree", "algorithm", "binary", "derived", "sequence", "numbers", "root", "minimum", "element", "left", "right", "subtrees", "trees", "subsequences"]}, {"id": "term-case-based-reasoning", "t": "Case-Based Reasoning", "tg": ["History", "Fundamentals"], "d": "history", "x": "An AI methodology that solves new problems by adapting solutions from similar past cases. Developed by Roger Schank and...", "l": "c", "k": ["case-based", "reasoning", "methodology", "solves", "problems", "adapting", "solutions", "similar", "past", "cases", "developed", "roger", "schank", "others", "1980s"]}, {"id": "term-casual-conversations", "t": "Casual Conversations", "tg": ["Benchmark", "Fairness", "Multimodal"], "d": "datasets", "x": "A Meta dataset of 45186 videos of paid participants with self-provided age gender and skin tone labels. Designed for...", "l": "c", "k": ["casual", "conversations", "meta", "dataset", "videos", "paid", "participants", "self-provided", "age", "gender", "skin", "tone", "labels", "designed", "evaluating"]}, {"id": "term-cataphora", "t": "Cataphora", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A linguistic phenomenon where a referring expression precedes the entity it refers to in the text, as in 'Before he...", "l": "c", "k": ["cataphora", "linguistic", "phenomenon", "referring", "expression", "precedes", "entity", "refers", "text", "arrived", "john", "called", "ahead", "posing", "challenges"]}, {"id": "term-catastrophic-forgetting", "t": "Catastrophic Forgetting", "tg": ["Training", "Challenge"], "d": "general", "x": "The tendency of neural networks to forget previously learned information when trained on new data. A significant...", "l": "c", "k": ["catastrophic", "forgetting", "tendency", "neural", "networks", "forget", "previously", "learned", "information", "trained", "data", "significant", "challenge", "continual", "learning"]}, {"id": "term-catastrophic-forgetting-ethics", "t": "Catastrophic Forgetting Ethics", "tg": ["AI Safety", "AI Ethics"], "d": "safety", "x": "Ethical implications of the tendency of neural networks to forget previously learned safety constraints when trained on...", "l": "c", "k": ["catastrophic", "forgetting", "ethics", "ethical", "implications", "tendency", "neural", "networks", "forget", "previously", "learned", "safety", "constraints", "trained", "data"]}, {"id": "term-catastrophic-forgetting-rl", "t": "Catastrophic Forgetting in RL", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "The tendency of neural network-based RL agents to lose previously learned skills when adapting to new tasks or...", "l": "c", "k": ["catastrophic", "forgetting", "tendency", "neural", "network-based", "agents", "lose", "previously", "learned", "skills", "adapting", "tasks", "environments", "continual", "methods"]}, {"id": "term-catastrophic-interference", "t": "Catastrophic Interference", "tg": ["Safety", "Technical"], "d": "safety", "x": "A phenomenon in neural networks where learning new information causes the model to forget previously learned...", "l": "c", "k": ["catastrophic", "interference", "phenomenon", "neural", "networks", "learning", "information", "causes", "model", "forget", "previously", "learned", "poses", "safety", "risks"]}, {"id": "term-catastrophic-risk-from-ai", "t": "Catastrophic Risk from AI", "tg": ["AI Safety", "AI Ethics"], "d": "safety", "x": "The risk that AI systems could cause large-scale irreversible harm falling short of existential risk, such as...", "l": "c", "k": ["catastrophic", "risk", "systems", "cause", "large-scale", "irreversible", "harm", "falling", "short", "existential", "widespread", "economic", "collapse", "loss", "critical"]}, {"id": "term-catboost", "t": "CatBoost", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A gradient boosting library that natively handles categorical features using ordered target statistics and employs...", "l": "c", "k": ["catboost", "gradient", "boosting", "library", "natively", "handles", "categorical", "features", "ordered", "target", "statistics", "employs", "reduce", "prediction", "shift"]}, {"id": "term-catboost-model", "t": "CatBoost Model", "tg": ["Models", "Fundamentals"], "d": "models", "x": "A gradient boosting library from Yandex that handles categorical features natively and uses ordered boosting to reduce...", "l": "c", "k": ["catboost", "model", "gradient", "boosting", "library", "yandex", "handles", "categorical", "features", "natively", "uses", "ordered", "reduce", "prediction", "shift"]}, {"id": "term-categorical-dqn-algorithm", "t": "Categorical DQN Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A distributional reinforcement learning algorithm that models the full distribution of returns rather than just the...", "l": "c", "k": ["categorical", "dqn", "algorithm", "distributional", "reinforcement", "learning", "models", "full", "distribution", "returns", "rather", "expected", "value", "uses", "representation"]}, {"id": "term-causal-convolution", "t": "Causal Convolution", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A convolution that only uses current and past inputs ensuring the output at time t depends only on inputs at times t...", "l": "c", "k": ["causal", "convolution", "uses", "current", "past", "inputs", "ensuring", "output", "time", "depends", "times", "earlier", "essential", "autoregressive", "sequence"]}, {"id": "term-causal-fairness", "t": "Causal Fairness", "tg": ["Safety", "Technical"], "d": "safety", "x": "An approach to algorithmic fairness that uses causal reasoning to determine whether an AI system's decisions are...", "l": "c", "k": ["causal", "fairness", "approach", "algorithmic", "uses", "reasoning", "determine", "system", "decisions", "influenced", "protected", "attributes", "illegitimate", "pathways"]}, {"id": "term-causal-forest-algorithm", "t": "Causal Forest Algorithm", "tg": ["Algorithms", "Technical", "Causal"], "d": "algorithms", "x": "An ensemble method for estimating heterogeneous treatment effects that adapts random forests to causal inference....", "l": "c", "k": ["causal", "forest", "algorithm", "ensemble", "method", "estimating", "heterogeneous", "treatment", "effects", "adapts", "random", "forests", "inference", "splits", "variables"]}, {"id": "term-causal-language-model", "t": "Causal Language Model", "tg": ["Architecture", "LLM"], "d": "models", "x": "A model that predicts the next token based only on previous tokens (left-to-right). GPT and most text generation models...", "l": "c", "k": ["causal", "language", "model", "predicts", "next", "token", "based", "previous", "tokens", "left-to-right", "gpt", "text", "generation", "models", "contrast"]}, {"id": "term-causal-language-modeling", "t": "Causal Language Modeling", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A training objective where the model predicts each token based only on the preceding tokens in the sequence, enforcing...", "l": "c", "k": ["causal", "language", "modeling", "training", "objective", "model", "predicts", "token", "based", "preceding", "tokens", "sequence", "enforcing", "left-to-right", "autoregressive"]}, {"id": "term-causal-mask", "t": "Causal Mask", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A triangular attention mask that prevents each position from attending to subsequent positions, enforcing the...", "l": "c", "k": ["causal", "mask", "triangular", "attention", "prevents", "position", "attending", "subsequent", "positions", "enforcing", "autoregressive", "property", "required", "left-to-right", "language"]}, {"id": "term-causal-masking", "t": "Causal Masking", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A specific attention mask pattern that prevents each position from attending to future positions ensuring...", "l": "c", "k": ["causal", "masking", "specific", "attention", "mask", "pattern", "prevents", "position", "attending", "future", "positions", "ensuring", "autoregressive", "behavior", "implemented"]}, {"id": "term-causalbench", "t": "CausalBench", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A benchmark for evaluating causal reasoning capabilities of language models. Tests the ability to identify causes...", "l": "c", "k": ["causalbench", "benchmark", "evaluating", "causal", "reasoning", "capabilities", "language", "models", "tests", "ability", "identify", "causes", "effects", "mechanisms", "text"]}, {"id": "term-cb", "t": "CB", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "CommitmentBank a corpus of naturally occurring discourses with embedded clauses annotated for speaker commitment. Part...", "l": "c", "k": ["commitmentbank", "corpus", "naturally", "occurring", "discourses", "embedded", "clauses", "annotated", "speaker", "commitment", "part", "superglue", "benchmark", "testing", "fine-grained"]}, {"id": "term-cbam", "t": "CBAM", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Convolutional Block Attention Module, a lightweight attention module that sequentially applies channel and spatial...", "l": "c", "k": ["cbam", "convolutional", "block", "attention", "module", "lightweight", "sequentially", "applies", "channel", "spatial", "feature", "maps", "enhancing", "representational", "power"]}, {"id": "term-cbow", "t": "CBOW", "tg": ["NLP", "Embeddings"], "d": "general", "x": "Continuous Bag of Words, a Word2Vec training objective that predicts a center word from the average of its surrounding...", "l": "c", "k": ["cbow", "continuous", "bag", "words", "word2vec", "training", "objective", "predicts", "center", "word", "average", "surrounding", "context", "vectors", "typically"]}, {"id": "term-cc-news", "t": "CC-News", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A dataset of 63 million English news articles collected from Common Crawl between 2016 and 2019. Used as a pretraining...", "l": "c", "k": ["cc-news", "dataset", "million", "english", "news", "articles", "collected", "common", "crawl", "pretraining", "component", "roberta", "language", "models"]}, {"id": "term-cc-stories", "t": "CC-Stories", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A subset of Common Crawl filtered to contain narrative and story-like text. Used as part of the GPT-2 training...", "l": "c", "k": ["cc-stories", "subset", "common", "crawl", "filtered", "contain", "narrative", "story-like", "text", "part", "gpt-2", "training", "methodology", "include", "fiction"]}, {"id": "term-cc100", "t": "CC100", "tg": ["Training Corpus", "NLP", "Multilingual"], "d": "datasets", "x": "A monolingual dataset extracted from Common Crawl snapshots covering 100 languages. Created for pretraining...", "l": "c", "k": ["cc100", "monolingual", "dataset", "extracted", "common", "crawl", "snapshots", "covering", "languages", "created", "pretraining", "multilingual", "models", "language-specific", "language"]}, {"id": "term-ccaligned", "t": "CCAligned", "tg": ["Training Corpus", "NLP", "Translation", "Multilingual"], "d": "datasets", "x": "A parallel corpus of 68 language pairs extracted from Common Crawl using document alignment. Provides web-mined...", "l": "c", "k": ["ccaligned", "parallel", "corpus", "language", "pairs", "extracted", "common", "crawl", "document", "alignment", "provides", "web-mined", "translation", "data", "training"]}, {"id": "term-cdc-6600", "t": "CDC 6600", "tg": ["Historical", "Supercomputer", "Pioneer"], "d": "hardware", "x": "Control Data Corporation supercomputer designed by Seymour Cray in 1964. Generally considered the first successful...", "l": "c", "k": ["cdc", "control", "data", "corporation", "supercomputer", "designed", "seymour", "cray", "generally", "considered", "successful", "fastest", "computer", "world"]}, {"id": "term-ceiling-effect", "t": "Ceiling Effect", "tg": ["Evaluation", "Benchmark"], "d": "datasets", "x": "When a benchmark becomes too easy to distinguish between models, all scoring near the maximum. Prompts creation of...", "l": "c", "k": ["ceiling", "effect", "benchmark", "becomes", "easy", "distinguish", "models", "scoring", "near", "maximum", "prompts", "creation", "harder", "benchmarks", "continue"]}, {"id": "term-celeba", "t": "CelebA", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A large-scale face attributes dataset with over 200000 celebrity images each annotated with 40 binary attributes and 5...", "l": "c", "k": ["celeba", "large-scale", "face", "attributes", "dataset", "celebrity", "images", "annotated", "binary", "landmark", "locations", "widely", "generation", "attribute", "prediction"]}, {"id": "term-celeba-hq", "t": "CelebA-HQ", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A high-quality version of CelebA containing 30000 face images at 1024x1024 resolution. Created for training...", "l": "c", "k": ["celeba-hq", "high-quality", "version", "celeba", "containing", "face", "images", "1024x1024", "resolution", "created", "training", "high-resolution", "generative", "models", "particularly"]}, {"id": "term-cellular-automata", "t": "Cellular Automata", "tg": ["History", "Fundamentals"], "d": "history", "x": "Discrete models of computation consisting of a grid of cells each in a finite number of states that evolve over time...", "l": "c", "k": ["cellular", "automata", "discrete", "models", "computation", "consisting", "grid", "cells", "finite", "number", "states", "evolve", "time", "according", "simple"]}, {"id": "term-censoring", "t": "Censoring", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A condition in survival analysis where the exact time of an event is not observed for some subjects, typically because...", "l": "c", "k": ["censoring", "condition", "survival", "analysis", "exact", "time", "event", "observed", "subjects", "typically", "study", "ended", "subject", "lost", "follow-up"]}, {"id": "term-center-for-ai-safety", "t": "Center for AI Safety", "tg": ["History", "Organizations"], "d": "history", "x": "A nonprofit research and field-building organization founded in 2022 focused on reducing societal-scale risks from AI....", "l": "c", "k": ["center", "safety", "nonprofit", "research", "field-building", "organization", "founded", "focused", "reducing", "societal-scale", "risks", "cais", "published", "statement", "risk"]}, {"id": "term-centernet", "t": "CenterNet", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "An anchor-free object detection approach that represents objects as center points with associated size and offset...", "l": "c", "k": ["centernet", "anchor-free", "object", "detection", "approach", "represents", "objects", "center", "points", "associated", "size", "offset", "predictions", "simplifying", "pipeline"]}, {"id": "term-centerpoint", "t": "CenterPoint", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "A center-based 3D object detection and tracking model for autonomous driving that detects objects as center points in...", "l": "c", "k": ["centerpoint", "center-based", "object", "detection", "tracking", "model", "autonomous", "driving", "detects", "objects", "center", "points", "bird-eye-view", "representations"]}, {"id": "term-central-limit-theorem", "t": "Central Limit Theorem", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A fundamental theorem stating that the sampling distribution of the sample mean approaches a normal distribution as the...", "l": "c", "k": ["central", "limit", "theorem", "fundamental", "stating", "sampling", "distribution", "sample", "mean", "approaches", "normal", "size", "increases", "regardless", "population"]}, {"id": "term-ctde", "t": "Centralized Training Decentralized Execution (CTDE)", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A multi-agent RL paradigm where agents have access to global information during training but must act independently...", "l": "c", "k": ["centralized", "training", "decentralized", "execution", "ctde", "multi-agent", "paradigm", "agents", "access", "global", "information", "must", "act", "independently", "based"]}, {"id": "term-centroid-decomposition", "t": "Centroid Decomposition", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A tree decomposition technique that recursively splits a tree at its centroid to create a balanced decomposition tree....", "l": "c", "k": ["centroid", "decomposition", "tree", "technique", "recursively", "splits", "create", "balanced", "enables", "efficient", "divide-and-conquer", "algorithms", "path", "queries", "log"]}, {"id": "term-centroid-based-clustering-vectors", "t": "Centroid-Based Clustering for Vectors", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "The use of clustering algorithms like k-means to partition a vector collection into groups represented by centroid...", "l": "c", "k": ["centroid-based", "clustering", "vectors", "algorithms", "k-means", "partition", "vector", "collection", "groups", "represented", "centroid", "forming", "basis", "ivf", "indexes"]}, {"id": "term-cerebras", "t": "Cerebras", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "A semiconductor company that produces wafer-scale AI processors (WSE series) with millions of cores and terabytes of...", "l": "c", "k": ["cerebras", "semiconductor", "company", "produces", "wafer-scale", "processors", "wse", "series", "millions", "cores", "terabytes", "on-chip", "sram", "systems", "eliminate"]}, {"id": "term-cerebras-cs-2", "t": "Cerebras CS-2", "tg": ["Accelerator", "System"], "d": "hardware", "x": "AI training system built around the Cerebras WSE-2 chip providing simplified cluster-free training for large AI models....", "l": "c", "k": ["cerebras", "cs-2", "training", "system", "built", "around", "wse-2", "chip", "providing", "simplified", "cluster-free", "large", "models", "eliminates", "need"]}, {"id": "term-cerebras-systems", "t": "Cerebras Systems", "tg": ["History", "Organizations"], "d": "history", "x": "An AI hardware company founded in 2016 that developed the Wafer Scale Engine the largest chip ever built. The CS-2...", "l": "c", "k": ["cerebras", "systems", "hardware", "company", "founded", "developed", "wafer", "scale", "engine", "largest", "chip", "ever", "built", "cs-2", "system"]}, {"id": "term-cerebras-wse-2", "t": "Cerebras WSE-2", "tg": ["Accelerator", "Wafer-Scale"], "d": "hardware", "x": "Cerebras Wafer Scale Engine 2 containing 2.6 trillion transistors and 850000 AI-optimized cores on a single wafer-scale...", "l": "c", "k": ["cerebras", "wse-2", "wafer", "scale", "engine", "containing", "trillion", "transistors", "ai-optimized", "cores", "single", "wafer-scale", "chip", "largest", "ever"]}, {"id": "term-cerebras-gpt", "t": "Cerebras-GPT", "tg": ["Models", "Technical"], "d": "models", "x": "A family of language models trained by Cerebras Systems following Chinchilla-optimal scaling laws. Released with...", "l": "c", "k": ["cerebras-gpt", "family", "language", "models", "trained", "cerebras", "systems", "following", "chinchilla-optimal", "scaling", "laws", "released", "training", "recipes", "full"]}, {"id": "term-certified-robustness", "t": "Certified Robustness", "tg": ["Safety", "Technical"], "d": "safety", "x": "A formal guarantee that a model's predictions will not change under input perturbations within a specified bound....", "l": "c", "k": ["certified", "robustness", "formal", "guarantee", "model", "predictions", "change", "input", "perturbations", "within", "specified", "bound", "provides", "provable", "rather"]}, {"id": "term-cflow-ad", "t": "CFlow-AD", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "Conditional normalizing Flow for Anomaly Detection uses position-conditioned normalizing flows on multi-scale features...", "l": "c", "k": ["cflow-ad", "conditional", "normalizing", "flow", "anomaly", "detection", "uses", "position-conditioned", "flows", "multi-scale", "features", "pixel-level", "localization"]}, {"id": "term-cgcnn", "t": "CGCNN", "tg": ["Models", "Scientific"], "d": "models", "x": "Crystal Graph Convolutional Neural Network predicts material properties directly from crystal structures using a graph...", "l": "c", "k": ["cgcnn", "crystal", "graph", "convolutional", "neural", "network", "predicts", "material", "properties", "directly", "structures", "representation", "atomic", "bonds"]}, {"id": "term-chain-of-density", "t": "Chain of Density", "tg": ["Prompt Engineering", "Summarization"], "d": "general", "x": "A prompting technique that iteratively increases the information density of a summary by asking the model to rewrite it...", "l": "c", "k": ["chain", "density", "prompting", "technique", "iteratively", "increases", "information", "summary", "asking", "model", "rewrite", "additional", "entities", "maintaining", "length"]}, {"id": "term-chain-of-responsibility-in-ai", "t": "Chain of Responsibility in AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "The linked sequence of actors including developers deployers and users who share accountability for AI system outcomes....", "l": "c", "k": ["chain", "responsibility", "linked", "sequence", "actors", "including", "developers", "deployers", "users", "share", "accountability", "system", "outcomes", "legal", "frameworks"]}, {"id": "term-chain-of-code", "t": "Chain-of-Code", "tg": ["Prompt Engineering", "Code-Augmented"], "d": "general", "x": "A reasoning framework that augments chain-of-thought with executable code generation, allowing the model to write and...", "l": "c", "k": ["chain-of-code", "reasoning", "framework", "augments", "chain-of-thought", "executable", "code", "generation", "allowing", "model", "write", "simulate", "execution", "steps", "benefit"]}, {"id": "term-chain-of-knowledge", "t": "Chain-of-Knowledge", "tg": ["Prompt Engineering", "Knowledge Augmentation"], "d": "general", "x": "A prompting framework that progressively builds and refines a knowledge chain by eliciting relevant facts, verifying...", "l": "c", "k": ["chain-of-knowledge", "prompting", "framework", "progressively", "builds", "refines", "knowledge", "chain", "eliciting", "relevant", "facts", "verifying", "consistency", "reasoning", "accumulated"]}, {"id": "term-chain-of-table", "t": "Chain-of-Table", "tg": ["Prompt Engineering", "Tabular Reasoning"], "d": "general", "x": "A reasoning framework for tabular data that iteratively transforms tables through operations like filtering, sorting,...", "l": "c", "k": ["chain-of-table", "reasoning", "framework", "tabular", "data", "iteratively", "transforms", "tables", "operations", "filtering", "sorting", "aggregation", "intermediate", "steps", "step"]}, {"id": "term-chain-of-thought", "t": "Chain-of-Thought (CoT)", "tg": ["Prompting", "Reasoning"], "d": "general", "x": "A prompting technique that encourages AI to show its reasoning process step-by-step, leading to more accurate and...", "l": "c", "k": ["chain-of-thought", "cot", "prompting", "technique", "encourages", "show", "reasoning", "process", "step-by-step", "leading", "accurate", "transparent", "responses", "complex", "problems"]}, {"id": "term-chain-of-thought-discovery", "t": "Chain-of-Thought Discovery", "tg": ["History", "Milestones"], "d": "history", "x": "The discovery that prompting large language models to think step by step dramatically improves their reasoning...", "l": "c", "k": ["chain-of-thought", "discovery", "prompting", "large", "language", "models", "think", "step", "dramatically", "improves", "reasoning", "performance", "introduced", "jason", "wei"]}, {"id": "term-chain-of-thought-prompting", "t": "Chain-of-Thought Prompting", "tg": ["Algorithms", "Fundamentals", "Prompting"], "d": "algorithms", "x": "A prompting technique that elicits step-by-step reasoning from language models by including intermediate reasoning...", "l": "c", "k": ["chain-of-thought", "prompting", "technique", "elicits", "step-by-step", "reasoning", "language", "models", "including", "intermediate", "steps", "prompt", "dramatically", "improves", "performance"]}, {"id": "term-cot-self-consistency", "t": "Chain-of-Thought with Self-Consistency", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "The combined technique of generating multiple chain-of-thought reasoning paths for a single problem using sampling and...", "l": "c", "k": ["chain-of-thought", "self-consistency", "combined", "technique", "generating", "multiple", "reasoning", "paths", "single", "problem", "sampling", "selecting", "frequently", "occurring", "final"]}, {"id": "term-chameleon", "t": "Chameleon", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A mixed-modal foundation model from Meta that natively processes and generates interleaved text and image sequences...", "l": "c", "k": ["chameleon", "mixed-modal", "foundation", "model", "meta", "natively", "processes", "generates", "interleaved", "text", "image", "sequences", "discrete", "tokenization", "modalities"]}, {"id": "term-channel-attention", "t": "Channel Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that learns to weight the importance of different feature channels in a CNN, selectively...", "l": "c", "k": ["channel", "attention", "mechanism", "learns", "weight", "importance", "different", "feature", "channels", "cnn", "selectively", "emphasizing", "informative", "suppressing", "less"]}, {"id": "term-channel-capacity-algorithm", "t": "Channel Capacity Algorithm", "tg": ["Algorithms", "Technical", "Information Theory"], "d": "algorithms", "x": "An algorithm that computes the maximum rate of reliable communication over a noisy channel. The Blahut-Arimoto...", "l": "c", "k": ["channel", "capacity", "algorithm", "computes", "maximum", "rate", "reliable", "communication", "noisy", "blahut-arimoto", "iteratively", "optimizes", "input", "distribution", "maximize"]}, {"id": "term-character-error-rate", "t": "Character Error Rate", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A fine-grained evaluation metric that computes the edit distance between predicted and reference texts at the character...", "l": "c", "k": ["character", "error", "rate", "fine-grained", "evaluation", "metric", "computes", "edit", "distance", "predicted", "reference", "texts", "level", "useful", "evaluating"]}, {"id": "term-character-n-gram", "t": "Character N-gram", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A contiguous sequence of N characters extracted from a word or text, used as features for text classification, language...", "l": "c", "k": ["character", "n-gram", "contiguous", "sequence", "characters", "extracted", "word", "text", "features", "classification", "language", "identification", "spelling", "correction", "tasks"]}, {"id": "term-character-level", "t": "Character-Level Model", "tg": ["Architecture", "Alternative"], "d": "models", "x": "Models that process text character by character rather than using tokens. More flexible with novel words but typically...", "l": "c", "k": ["character-level", "model", "models", "process", "text", "character", "rather", "tokens", "flexible", "novel", "words", "typically", "slower", "requiring", "parameters"]}, {"id": "term-charades", "t": "Charades", "tg": ["Benchmark", "Video"], "d": "datasets", "x": "A dataset of 10000 videos of daily indoor activities with temporal annotations for 157 action classes. Collected by...", "l": "c", "k": ["charades", "dataset", "videos", "daily", "indoor", "activities", "temporal", "annotations", "action", "classes", "collected", "workers", "acting", "scripts", "homes"]}, {"id": "term-charles-babbage", "t": "Charles Babbage", "tg": ["History", "Pioneers"], "d": "history", "x": "English mathematician and inventor (1791-1871) who conceived the Difference Engine and the Analytical Engine,...", "l": "c", "k": ["charles", "babbage", "english", "mathematician", "inventor", "1791-1871", "conceived", "difference", "engine", "analytical", "mechanical", "general-purpose", "computers", "anticipated", "key"]}, {"id": "term-chartllama", "t": "ChartLlama", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multimodal large language model fine-tuned for chart understanding that can describe and analyze and reason about...", "l": "c", "k": ["chartllama", "multimodal", "large", "language", "model", "fine-tuned", "chart", "understanding", "describe", "analyze", "reason", "diverse", "types"]}, {"id": "term-chartqa", "t": "ChartQA", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "A benchmark for question answering about charts and data visualizations. Tests the ability to extract trends...", "l": "c", "k": ["chartqa", "benchmark", "question", "answering", "charts", "data", "visualizations", "tests", "ability", "extract", "trends", "comparisons", "specific", "values", "chart"]}, {"id": "term-chat-completion", "t": "Chat Completion", "tg": ["API", "Technical"], "d": "general", "x": "An API endpoint type where the model generates responses in a conversational format. Takes a list of messages (system,...", "l": "c", "k": ["chat", "completion", "api", "endpoint", "type", "model", "generates", "responses", "conversational", "format", "takes", "list", "messages", "system", "user"]}, {"id": "term-chat-template", "t": "Chat Template", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A structured formatting convention that defines how system messages, user inputs, and assistant responses are tokenized...", "l": "c", "k": ["chat", "template", "structured", "formatting", "convention", "defines", "system", "messages", "user", "inputs", "assistant", "responses", "tokenized", "delimited", "multi-turn"]}, {"id": "term-chatbot-arena", "t": "Chatbot Arena", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "An open platform for evaluating LLMs through crowdsourced pairwise comparisons. Produces Elo ratings for language...", "l": "c", "k": ["chatbot", "arena", "open", "platform", "evaluating", "llms", "crowdsourced", "pairwise", "comparisons", "produces", "elo", "ratings", "language", "models", "based"]}, {"id": "term-chatbot-arena-conversations", "t": "Chatbot Arena Conversations", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A dataset of conversations from LMSYS Chatbot Arena where users compare responses from different LLMs. Provides human...", "l": "c", "k": ["chatbot", "arena", "conversations", "dataset", "lmsys", "users", "compare", "responses", "different", "llms", "provides", "human", "preference", "data", "pairwise"]}, {"id": "term-chatglm", "t": "ChatGLM", "tg": ["Models", "Technical"], "d": "models", "x": "A family of bilingual language models based on the General Language Model architecture. Developed by Tsinghua...", "l": "c", "k": ["chatglm", "family", "bilingual", "language", "models", "based", "general", "model", "architecture", "developed", "tsinghua", "university", "zhipu", "features", "efficient"]}, {"id": "term-chatglm3", "t": "ChatGLM3", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "The third generation of the ChatGLM conversational model with improved function calling and code interpretation and...", "l": "c", "k": ["chatglm3", "generation", "chatglm", "conversational", "model", "improved", "function", "calling", "code", "interpretation", "long-context", "handling", "capabilities"]}, {"id": "term-chatgpt", "t": "ChatGPT", "tg": ["Product", "OpenAI"], "d": "general", "x": "OpenAI's conversational AI product launched in November 2022. Built on GPT models fine-tuned for dialogue, it...", "l": "c", "k": ["chatgpt", "openai", "conversational", "product", "launched", "november", "built", "gpt", "models", "fine-tuned", "dialogue", "popularized", "sparked", "widespread", "public"]}, {"id": "term-chatgpt-launch", "t": "ChatGPT Launch", "tg": ["History", "Milestones"], "d": "history", "x": "OpenAI's release of ChatGPT on November 30, 2022, a conversational AI interface built on GPT-3.5 that reached 100...", "l": "c", "k": ["chatgpt", "launch", "openai", "release", "november", "conversational", "interface", "built", "gpt-3", "reached", "million", "users", "months", "triggering", "widespread"]}, {"id": "term-chebyshev-filter", "t": "Chebyshev Filter", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A type of analog or digital filter that achieves a steeper roll-off than Butterworth filters at the cost of ripple in...", "l": "c", "k": ["chebyshev", "filter", "type", "analog", "digital", "achieves", "steeper", "roll-off", "butterworth", "filters", "cost", "ripple", "passband", "stopband", "named"]}, {"id": "term-chebyshev-iteration", "t": "Chebyshev Iteration", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An iterative method for solving linear systems that uses Chebyshev polynomials to minimize the error over a known...", "l": "c", "k": ["chebyshev", "iteration", "iterative", "method", "solving", "linear", "systems", "uses", "polynomials", "minimize", "error", "known", "eigenvalue", "range", "converges"]}, {"id": "term-checklist", "t": "CheckList", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A task-agnostic methodology for behavioral testing of NLP models using linguistic capabilities. Provides templates for...", "l": "c", "k": ["checklist", "task-agnostic", "methodology", "behavioral", "testing", "nlp", "models", "linguistic", "capabilities", "provides", "templates", "robustness", "fairness", "specific", "phenomena"]}, {"id": "term-checkpoint", "t": "Checkpoint", "tg": ["Training", "Technical"], "d": "general", "x": "A saved snapshot of model weights during training. Enables resuming training after interruption, comparing different...", "l": "c", "k": ["checkpoint", "saved", "snapshot", "model", "weights", "training", "enables", "resuming", "interruption", "comparing", "different", "stages", "selecting", "best", "performing"]}, {"id": "term-checkpoint-and-restart", "t": "Checkpoint and Restart", "tg": ["Training", "Reliability", "Technique"], "d": "hardware", "x": "Technique of periodically saving training state to storage so training can resume from the latest checkpoint after...", "l": "c", "k": ["checkpoint", "restart", "technique", "periodically", "saving", "training", "state", "storage", "resume", "latest", "interruptions", "essential", "long-running", "jobs", "large"]}, {"id": "term-checkpointing-training", "t": "Checkpointing for Training", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "The practice of periodically saving model weights, optimizer state, and training metadata to persistent storage during...", "l": "c", "k": ["checkpointing", "training", "practice", "periodically", "saving", "model", "weights", "optimizer", "state", "metadata", "persistent", "storage", "enables", "recovery", "hardware"]}, {"id": "term-chem-bench", "t": "CHEM-Bench", "tg": ["Benchmark", "NLP", "Reasoning", "Scientific"], "d": "datasets", "x": "A chemistry reasoning benchmark covering diverse chemistry subdisciplines. Tests the ability of AI systems to solve...", "l": "c", "k": ["chem-bench", "chemistry", "reasoning", "benchmark", "covering", "diverse", "subdisciplines", "tests", "ability", "systems", "solve", "chemical", "problems", "understand", "molecular"]}, {"id": "term-chemical-mechanical-planarization", "t": "Chemical Mechanical Planarization", "tg": ["Fabrication", "Manufacturing", "Process"], "d": "hardware", "x": "Process of polishing semiconductor wafers to achieve an extremely flat surface between manufacturing layers. Essential...", "l": "c", "k": ["chemical", "mechanical", "planarization", "process", "polishing", "semiconductor", "wafers", "achieve", "extremely", "flat", "surface", "manufacturing", "layers", "essential", "maintaining"]}, {"id": "term-chemllm", "t": "ChemLLM", "tg": ["Models", "Technical", "NLP", "Scientific"], "d": "models", "x": "A large language model specialized for chemistry that handles molecular property prediction and reaction planning and...", "l": "c", "k": ["chemllm", "large", "language", "model", "specialized", "chemistry", "handles", "molecular", "property", "prediction", "reaction", "planning", "chemical", "literature", "understanding"]}, {"id": "term-cheriton-tarjan-algorithm", "t": "Cheriton-Tarjan Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A minimum spanning tree algorithm that achieves O(E log log V) time for dense graphs. Uses a priority queue based on...", "l": "c", "k": ["cheriton-tarjan", "algorithm", "minimum", "spanning", "tree", "achieves", "log", "time", "dense", "graphs", "uses", "priority", "queue", "based", "soft"]}, {"id": "term-chestx-ray14", "t": "ChestX-ray14", "tg": ["Benchmark", "Medical", "Computer Vision"], "d": "datasets", "x": "A dataset of 112000 frontal-view chest X-ray images from over 30000 patients annotated with 14 disease labels. One of...", "l": "c", "k": ["chestx-ray14", "dataset", "frontal-view", "chest", "x-ray", "images", "patients", "annotated", "disease", "labels", "large-scale", "datasets", "deep", "learning"]}, {"id": "term-chexnet", "t": "CheXNet", "tg": ["Models", "Technical", "Medical", "Vision"], "d": "models", "x": "A deep learning model for detecting pneumonia from chest X-rays that uses a 121-layer DenseNet and achieves...", "l": "c", "k": ["chexnet", "deep", "learning", "model", "detecting", "pneumonia", "chest", "x-rays", "uses", "121-layer", "densenet", "achieves", "radiologist-level", "performance"]}, {"id": "term-chexpert", "t": "CheXpert", "tg": ["Benchmark", "Medical", "Computer Vision"], "d": "datasets", "x": "A large chest X-ray dataset from Stanford containing 224000 images with automated labels for 14 radiological...", "l": "c", "k": ["chexpert", "large", "chest", "x-ray", "dataset", "stanford", "containing", "images", "automated", "labels", "radiological", "observations", "includes", "uncertainty-aware", "labeling"]}, {"id": "term-chgnet", "t": "CHGNet", "tg": ["Models", "Scientific"], "d": "models", "x": "Crystal Hamiltonian Graph Neural Network is a universal machine learning interatomic potential that incorporates...", "l": "c", "k": ["chgnet", "crystal", "hamiltonian", "graph", "neural", "network", "universal", "machine", "learning", "interatomic", "potential", "incorporates", "magnetic", "moments", "accurate"]}, {"id": "term-chi-square-distribution", "t": "Chi-Square Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "The distribution of the sum of squares of k independent standard normal random variables. It is used in chi-square...", "l": "c", "k": ["chi-square", "distribution", "sum", "squares", "independent", "standard", "normal", "random", "variables", "tests", "confidence", "interval", "estimation", "variance", "goodness-of-fit"]}, {"id": "term-chi-square-test", "t": "Chi-Square Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A statistical test that evaluates whether observed frequencies differ significantly from expected frequencies under a...", "l": "c", "k": ["chi-square", "test", "statistical", "evaluates", "observed", "frequencies", "differ", "significantly", "expected", "null", "hypothesis", "testing", "independence", "categorical", "variables"]}, {"id": "term-child-safety-in-ai", "t": "Child Safety in AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "Protections and design considerations to prevent AI systems from harming minors. Includes age-appropriate content...", "l": "c", "k": ["child", "safety", "protections", "design", "considerations", "prevent", "systems", "harming", "minors", "includes", "age-appropriate", "content", "filtering", "data", "privacy"]}, {"id": "term-chilling-effect-of-ai", "t": "Chilling Effect of AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The phenomenon where awareness of AI surveillance or monitoring causes people to self-censor their speech behavior or...", "l": "c", "k": ["chilling", "effect", "phenomenon", "awareness", "surveillance", "monitoring", "causes", "people", "self-censor", "speech", "behavior", "activities", "raises", "concerns", "freedom"]}, {"id": "term-chinchilla", "t": "Chinchilla", "tg": ["Research", "Scaling"], "d": "general", "x": "A DeepMind model and scaling study showing optimal training requires more data than previously thought. Influenced...", "l": "c", "k": ["chinchilla", "deepmind", "model", "scaling", "study", "showing", "optimal", "training", "requires", "data", "previously", "thought", "influenced", "subsequent", "development"]}, {"id": "term-chinchilla-optimal", "t": "Chinchilla Optimal", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A training regime derived from DeepMind's Chinchilla scaling laws, suggesting that for a given compute budget, model...", "l": "c", "k": ["chinchilla", "optimal", "training", "regime", "derived", "deepmind", "scaling", "laws", "suggesting", "given", "compute", "budget", "model", "size", "data"]}, {"id": "term-chinchilla-paper", "t": "Chinchilla Paper", "tg": ["History", "Milestones"], "d": "history", "x": "The 2022 DeepMind paper by Hoffmann et al. demonstrating that many large language models were undertrained relative to...", "l": "c", "k": ["chinchilla", "paper", "deepmind", "hoffmann", "demonstrating", "large", "language", "models", "were", "undertrained", "relative", "size", "establishing", "scaling", "laws"]}, {"id": "term-chinchilla-scaling-laws", "t": "Chinchilla Scaling Laws", "tg": ["History", "Milestones"], "d": "history", "x": "Findings published by DeepMind in 2022 (Hoffmann et al.) showing that many large language models were significantly...", "l": "c", "k": ["chinchilla", "scaling", "laws", "findings", "published", "deepmind", "hoffmann", "showing", "large", "language", "models", "were", "significantly", "undertrained", "paper"]}, {"id": "term-chinese-postman-problem", "t": "Chinese Postman Problem", "tg": ["Algorithms", "Technical", "Graph", "Optimization"], "d": "algorithms", "x": "An optimization problem that seeks the shortest closed walk visiting every edge of a graph at least once. Solved in...", "l": "c", "k": ["chinese", "postman", "problem", "optimization", "seeks", "shortest", "closed", "walk", "visiting", "edge", "graph", "least", "solved", "polynomial", "time"]}, {"id": "term-chinese-room-argument", "t": "Chinese Room Argument", "tg": ["History", "Milestones"], "d": "history", "x": "A thought experiment by John Searle in 1980 arguing that a computer executing a program cannot have genuine...", "l": "c", "k": ["chinese", "room", "argument", "thought", "experiment", "john", "searle", "arguing", "computer", "executing", "program", "cannot", "genuine", "understanding", "consciousness"]}, {"id": "term-chinook", "t": "Chinook", "tg": ["History", "Systems"], "d": "history", "x": "A computer checkers program developed by Jonathan Schaeffer at the University of Alberta that won the World Checkers...", "l": "c", "k": ["chinook", "computer", "checkers", "program", "developed", "jonathan", "schaeffer", "university", "alberta", "won", "world", "championship", "team", "proved", "perfect"]}, {"id": "term-chip-packaging", "t": "Chip Packaging", "tg": ["Fabrication", "Packaging"], "d": "hardware", "x": "Process of enclosing a semiconductor die in a protective case with electrical connections to the outside world....", "l": "c", "k": ["chip", "packaging", "process", "enclosing", "semiconductor", "die", "protective", "case", "electrical", "connections", "outside", "world", "advanced", "enables", "multi-die"]}, {"id": "term-chip-on-chip", "t": "Chip-on-Chip", "tg": ["Packaging", "Architecture", "Advanced"], "d": "hardware", "x": "Packaging technique stacking one chip directly on top of another for short high-bandwidth connections. Used in advanced...", "l": "c", "k": ["chip-on-chip", "packaging", "technique", "stacking", "chip", "directly", "top", "another", "short", "high-bandwidth", "connections", "advanced", "processor", "packages", "integrate"]}, {"id": "term-chiplet-architecture", "t": "Chiplet Architecture", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "A processor design approach using multiple small silicon dies (chiplets) connected via high-speed interconnects on a...", "l": "c", "k": ["chiplet", "architecture", "processor", "design", "approach", "multiple", "small", "silicon", "dies", "chiplets", "connected", "via", "high-speed", "interconnects", "single"]}, {"id": "term-chiplet-design", "t": "Chiplet Design", "tg": ["Architecture", "Manufacturing", "Design"], "d": "hardware", "x": "Approach of building processors from multiple smaller dies rather than one large monolithic die. Improves manufacturing...", "l": "c", "k": ["chiplet", "design", "approach", "building", "processors", "multiple", "smaller", "dies", "rather", "large", "monolithic", "die", "improves", "manufacturing", "yield"]}, {"id": "term-chiplet-interconnect-standard", "t": "Chiplet Interconnect Standard", "tg": ["Standard", "Packaging", "Interconnect"], "d": "hardware", "x": "Industry standards like UCIe (Universal Chiplet Interconnect Express) for connecting chiplets from different vendors in...", "l": "c", "k": ["chiplet", "interconnect", "standard", "industry", "standards", "ucie", "universal", "express", "connecting", "chiplets", "different", "vendors", "single", "package", "enables"]}, {"id": "term-chips-act", "t": "CHIPS Act", "tg": ["Policy", "Legislation", "Manufacturing"], "d": "hardware", "x": "United States legislation providing 52 billion dollars in subsidies for domestic semiconductor manufacturing. Aims to...", "l": "c", "k": ["chips", "act", "united", "states", "legislation", "providing", "billion", "dollars", "subsidies", "domestic", "semiconductor", "manufacturing", "aims", "reduce", "dependence"]}, {"id": "term-chirp-z-transform", "t": "Chirp Z-Transform", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A generalization of the DFT that evaluates the Z-transform along spiral contours in the complex plane. Enables...", "l": "c", "k": ["chirp", "z-transform", "generalization", "dft", "evaluates", "along", "spiral", "contours", "complex", "plane", "enables", "frequency", "analysis", "arbitrary", "resolution"]}, {"id": "term-cholesky-decomposition", "t": "Cholesky Decomposition", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "A matrix factorization that decomposes a symmetric positive-definite matrix into the product of a lower triangular...", "l": "c", "k": ["cholesky", "decomposition", "matrix", "factorization", "decomposes", "symmetric", "positive-definite", "product", "lower", "triangular", "transpose", "roughly", "twice", "efficient", "applicable"]}, {"id": "term-christofides-algorithm", "t": "Christofides Algorithm", "tg": ["Algorithms", "Technical", "Graph", "Optimization"], "d": "algorithms", "x": "An approximation algorithm for the metric traveling salesman problem that guarantees a solution within 3/2 of the...", "l": "c", "k": ["christofides", "algorithm", "approximation", "metric", "traveling", "salesman", "problem", "guarantees", "solution", "within", "optimal", "tour", "length", "combines", "minimum"]}, {"id": "term-christopher-watkins", "t": "Christopher Watkins", "tg": ["History", "Pioneers"], "d": "history", "x": "British computer scientist who introduced Q-learning in his 1989 PhD thesis at Cambridge University. Q-learning is a...", "l": "c", "k": ["christopher", "watkins", "british", "computer", "scientist", "introduced", "q-learning", "phd", "thesis", "cambridge", "university", "model-free", "reinforcement", "learning", "algorithm"]}, {"id": "term-chroma", "t": "Chroma", "tg": ["Models", "Scientific"], "d": "models", "x": "A generative model for designing protein structures and sequences that uses diffusion processes conditioned on desired...", "l": "c", "k": ["chroma", "generative", "model", "designing", "protein", "structures", "sequences", "uses", "diffusion", "processes", "conditioned", "desired", "structural", "functional", "constraints"]}, {"id": "term-chromadb", "t": "ChromaDB", "tg": ["Vector Database", "Open Source"], "d": "general", "x": "An open-source embedding database designed for AI applications that provides a simple API for storing, querying, and...", "l": "c", "k": ["chromadb", "open-source", "embedding", "database", "designed", "applications", "provides", "simple", "api", "storing", "querying", "filtering", "embeddings", "associated", "metadata"]}, {"id": "term-chronos", "t": "Chronos", "tg": ["Models", "Technical"], "d": "models", "x": "A family of pre-trained time series forecasting models from Amazon that tokenize time series values into discrete bins...", "l": "c", "k": ["chronos", "family", "pre-trained", "time", "series", "forecasting", "models", "amazon", "tokenize", "values", "discrete", "bins", "language", "model", "architectures"]}, {"id": "term-chunk-overlap", "t": "Chunk Overlap", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "The number of tokens or characters shared between consecutive chunks during document splitting, ensuring that...", "l": "c", "k": ["chunk", "overlap", "number", "tokens", "characters", "shared", "consecutive", "chunks", "document", "splitting", "ensuring", "information", "spanning", "boundaries", "lost"]}, {"id": "term-chunk-size", "t": "Chunk Size", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "The target length of individual text segments produced during document chunking, typically measured in tokens or...", "l": "c", "k": ["chunk", "size", "target", "length", "individual", "text", "segments", "produced", "document", "chunking", "typically", "measured", "tokens", "characters", "smaller"]}, {"id": "term-chunked-prefill", "t": "Chunked Prefill", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An inference optimization that breaks the prompt processing phase into smaller chunks to be interleaved with decoding...", "l": "c", "k": ["chunked", "prefill", "inference", "optimization", "breaks", "prompt", "processing", "phase", "smaller", "chunks", "interleaved", "decoding", "steps", "reduces", "time-to-first-token"]}, {"id": "term-chunking", "t": "Chunking", "tg": ["Technique", "Processing"], "d": "general", "x": "Splitting long documents into smaller pieces for processing. Essential for RAG and embedding systems where input length...", "l": "c", "k": ["chunking", "splitting", "long", "documents", "smaller", "pieces", "processing", "essential", "rag", "embedding", "systems", "input", "length", "exceeds", "model"]}, {"id": "term-church-turing-thesis", "t": "Church-Turing Thesis", "tg": ["History", "Fundamentals"], "d": "history", "x": "The hypothesis independently proposed by Alonzo Church and Alan Turing in 1936 that any function computable by an...", "l": "c", "k": ["church-turing", "thesis", "hypothesis", "independently", "proposed", "alonzo", "church", "alan", "turing", "function", "computable", "effective", "procedure", "computed", "machine"]}, {"id": "term-cider", "t": "CIDEr", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "Consensus-based Image Description Evaluation, a metric that measures image captioning quality using TF-IDF weighted...", "l": "c", "k": ["cider", "consensus-based", "image", "description", "evaluation", "metric", "measures", "captioning", "quality", "tf-idf", "weighted", "n-gram", "similarity", "generated", "reference"]}, {"id": "term-cider-score-algorithm", "t": "CIDEr Score Algorithm", "tg": ["Algorithms", "Technical", "NLP", "Vision"], "d": "algorithms", "x": "Consensus-based Image Description Evaluation measures the similarity of a generated caption to reference captions using...", "l": "c", "k": ["cider", "score", "algorithm", "consensus-based", "image", "description", "evaluation", "measures", "similarity", "generated", "caption", "reference", "captions", "tf-idf", "weighted"]}, {"id": "term-cifar-organization", "t": "CIFAR (Organization)", "tg": ["History", "Organizations"], "d": "history", "x": "The Canadian Institute for Advanced Research a nonprofit research organization that has been instrumental in supporting...", "l": "c", "k": ["cifar", "organization", "canadian", "institute", "advanced", "research", "nonprofit", "instrumental", "supporting", "learning", "machines", "brains", "program", "provided", "crucial"]}, {"id": "term-cifar-10", "t": "CIFAR-10", "tg": ["History", "Milestones"], "d": "history", "x": "A dataset of 60000 32x32 color images in 10 classes collected by Alex Krizhevsky and Geoffrey Hinton in 2009. Along...", "l": "c", "k": ["cifar-10", "dataset", "32x32", "color", "images", "classes", "collected", "alex", "krizhevsky", "geoffrey", "hinton", "along", "cifar-100", "became", "standard"]}, {"id": "term-cifar-100", "t": "CIFAR-100", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "An extension of CIFAR-10 containing 60000 32x32 color images in 100 fine-grained classes grouped into 20 superclasses....", "l": "c", "k": ["cifar-100", "extension", "cifar-10", "containing", "32x32", "color", "images", "fine-grained", "classes", "grouped", "superclasses", "challenging", "due", "fewer", "examples"]}, {"id": "term-cisc-architecture", "t": "CISC Architecture", "tg": ["Architecture", "Fundamentals", "Design"], "d": "hardware", "x": "Complex Instruction Set Computer design philosophy using variable-length instructions with rich addressing modes. x86...", "l": "c", "k": ["cisc", "architecture", "complex", "instruction", "computer", "design", "philosophy", "variable-length", "instructions", "rich", "addressing", "modes", "x86", "dominant", "desktop"]}, {"id": "term-citation-generation", "t": "Citation Generation", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The capability of a language model to produce inline references to source documents that support its claims, enabling...", "l": "c", "k": ["citation", "generation", "capability", "language", "model", "produce", "inline", "references", "source", "documents", "support", "claims", "enabling", "users", "verify"]}, {"id": "term-citeseer", "t": "Citeseer", "tg": ["Benchmark", "Graph"], "d": "datasets", "x": "A citation network dataset of 3312 scientific papers across 6 classes. Alongside Cora one of the classic small-scale...", "l": "c", "k": ["citeseer", "citation", "network", "dataset", "scientific", "papers", "across", "classes", "alongside", "cora", "classic", "small-scale", "benchmarks", "graph-based", "semi-supervised"]}, {"id": "term-cityscapes", "t": "Cityscapes", "tg": ["Benchmark", "Computer Vision", "Autonomous Driving"], "d": "datasets", "x": "A dataset of 5000 finely annotated and 20000 coarsely annotated street scene images from 50 cities. Standard benchmark...", "l": "c", "k": ["cityscapes", "dataset", "finely", "annotated", "coarsely", "street", "scene", "images", "cities", "standard", "benchmark", "urban", "understanding", "autonomous", "driving"]}, {"id": "term-civil-comments", "t": "Civil Comments", "tg": ["Benchmark", "NLP", "Safety"], "d": "datasets", "x": "A dataset of over 2 million public comments annotated for toxicity and identity-based harassment. Used for training...", "l": "c", "k": ["civil", "comments", "dataset", "million", "public", "annotated", "toxicity", "identity-based", "harassment", "training", "content", "moderation", "systems", "evaluating", "detection"]}, {"id": "term-civil-society-and-ai", "t": "Civil Society and AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "The role of non-governmental organizations advocacy groups and community organizations in shaping AI policy and holding...", "l": "c", "k": ["civil", "society", "role", "non-governmental", "organizations", "advocacy", "groups", "community", "shaping", "policy", "holding", "developers", "accountable", "provides", "important"]}, {"id": "term-cky-algorithm", "t": "CKY Algorithm", "tg": ["NLP", "Parsing"], "d": "general", "x": "Cocke-Kasami-Younger algorithm, a dynamic programming parser for context-free grammars that builds parse trees...", "l": "c", "k": ["cky", "algorithm", "cocke-kasami-younger", "dynamic", "programming", "parser", "context-free", "grammars", "builds", "parse", "trees", "bottom-up", "time", "filling", "chart"]}, {"id": "term-cladder", "t": "CLadder", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A causal reasoning benchmark testing language models on causal inference questions. Evaluates understanding of...", "l": "c", "k": ["cladder", "causal", "reasoning", "benchmark", "testing", "language", "models", "inference", "questions", "evaluates", "understanding", "causation", "requiring", "counterfactual", "interventional"]}, {"id": "term-clap", "t": "CLAP", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "Contrastive Language-Audio Pretraining aligns audio and text in a shared embedding space for zero-shot audio...", "l": "c", "k": ["clap", "contrastive", "language-audio", "pretraining", "aligns", "audio", "text", "shared", "embedding", "space", "zero-shot", "classification", "retrieval", "tasks"]}, {"id": "term-clarans-algorithm", "t": "CLARANS Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "Clustering Large Applications based on Randomized Search is a k-medoids variant designed for large datasets. Uses...", "l": "c", "k": ["clarans", "algorithm", "clustering", "large", "applications", "based", "randomized", "search", "k-medoids", "variant", "designed", "datasets", "uses", "random", "sampling"]}, {"id": "term-clarity", "t": "Clarity (Prompting)", "tg": ["Prompting", "Best Practice"], "d": "general", "x": "Using clear, unambiguous language in prompts to reduce misinterpretation. Specific instructions and explicit...", "l": "c", "k": ["clarity", "prompting", "clear", "unambiguous", "language", "prompts", "reduce", "misinterpretation", "specific", "instructions", "explicit", "requirements", "improve", "response", "quality"]}, {"id": "term-class-activation-map", "t": "Class Activation Map", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A visualization technique that highlights the image regions most important for a CNN's classification decision,...", "l": "c", "k": ["class", "activation", "map", "visualization", "technique", "highlights", "image", "regions", "important", "cnn", "classification", "decision", "computed", "weighting", "feature"]}, {"id": "term-class-imbalance", "t": "Class Imbalance", "tg": ["Data", "Challenge"], "d": "general", "x": "When training data has unequal representation across categories. Can cause models to favor majority classes. Addressed...", "l": "c", "k": ["class", "imbalance", "training", "data", "unequal", "representation", "across", "categories", "cause", "models", "favor", "majority", "classes", "addressed", "sampling"]}, {"id": "term-class-weight", "t": "Class Weight", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A technique for handling class imbalance by assigning higher weight to the minority class in the loss function,...", "l": "c", "k": ["class", "weight", "technique", "handling", "imbalance", "assigning", "higher", "minority", "loss", "function", "effectively", "making", "misclassification", "underrepresented", "classes"]}, {"id": "term-classeval", "t": "ClassEval", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A benchmark for evaluating class-level code generation requiring models to generate complete Python classes with...", "l": "c", "k": ["classeval", "benchmark", "evaluating", "class-level", "code", "generation", "requiring", "models", "generate", "complete", "python", "classes", "multiple", "methods", "complex"]}, {"id": "term-classification", "t": "Classification", "tg": ["ML Task", "Supervised"], "d": "general", "x": "A machine learning task that assigns input data to predefined categories. Examples include spam detection (spam/not...", "l": "c", "k": ["classification", "machine", "learning", "task", "assigns", "input", "data", "predefined", "categories", "examples", "include", "spam", "detection", "sentiment", "analysis"]}, {"id": "term-classifier-free-guidance", "t": "Classifier-Free Guidance", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A technique for conditional diffusion models that interpolates between conditional and unconditional score estimates...", "l": "c", "k": ["classifier-free", "guidance", "technique", "conditional", "diffusion", "models", "interpolates", "unconditional", "score", "estimates", "sampling", "controlling", "trade-off", "sample", "quality"]}, {"id": "term-claude", "t": "Claude", "tg": ["Product", "Anthropic"], "d": "general", "x": "An AI assistant created by Anthropic, designed to be helpful, harmless, and honest. Known for nuanced reasoning, long...", "l": "c", "k": ["claude", "assistant", "created", "anthropic", "designed", "helpful", "harmless", "honest", "known", "nuanced", "reasoning", "long", "context", "handling", "strong"]}, {"id": "term-claude-ai", "t": "Claude (AI)", "tg": ["History", "Systems"], "d": "history", "x": "A family of AI assistants developed by Anthropic beginning with Claude 1.0 in March 2023. Built using Constitutional AI...", "l": "c", "k": ["claude", "family", "assistants", "developed", "anthropic", "beginning", "march", "built", "constitutional", "methods", "reinforcement", "learning", "human", "feedback", "known"]}, {"id": "term-claude-35-haiku", "t": "Claude 3.5 Haiku", "tg": ["Models", "Technical", "NLP", "Products"], "d": "models", "x": "A fast and compact model from Anthropic that balances speed with intelligence for high-throughput tasks requiring quick...", "l": "c", "k": ["claude", "haiku", "fast", "compact", "model", "anthropic", "balances", "speed", "intelligence", "high-throughput", "tasks", "requiring", "quick", "responses"]}, {"id": "term-claude-35-sonnet", "t": "Claude 3.5 Sonnet", "tg": ["Models", "Technical", "NLP", "Products"], "d": "models", "x": "An updated version of Anthropic Claude Sonnet model with improved coding and reasoning capabilities and a larger 200K...", "l": "c", "k": ["claude", "sonnet", "updated", "version", "anthropic", "model", "improved", "coding", "reasoning", "capabilities", "larger", "200k", "context", "window"]}, {"id": "term-claude-haiku", "t": "Claude Haiku", "tg": ["Models", "Technical"], "d": "models", "x": "The fastest and most compact model in the Claude family optimized for quick responses and high throughput. Suitable for...", "l": "c", "k": ["claude", "haiku", "fastest", "compact", "model", "family", "optimized", "quick", "responses", "high", "throughput", "suitable", "tasks", "requiring", "speed"]}, {"id": "term-claude-instant", "t": "Claude Instant / Haiku", "tg": ["Model", "Anthropic"], "d": "models", "x": "Anthropic's faster, more cost-effective models for simpler tasks. Trade some capability for speed and lower cost,...", "l": "c", "k": ["claude", "instant", "haiku", "anthropic", "faster", "cost-effective", "models", "simpler", "tasks", "trade", "capability", "speed", "lower", "cost", "suitable"]}, {"id": "term-claude-launch", "t": "Claude Launch", "tg": ["History", "Milestones"], "d": "history", "x": "Anthropic's release of Claude, a family of AI assistants trained using Constitutional AI methods, first made available...", "l": "c", "k": ["claude", "launch", "anthropic", "release", "family", "assistants", "trained", "constitutional", "methods", "available", "march", "emphasizing", "safety", "helpfulness", "harmlessness"]}, {"id": "term-claude-opus", "t": "Claude Opus", "tg": ["Model", "Anthropic"], "d": "models", "x": "Anthropic's most capable model, designed for complex reasoning, creative tasks, and nuanced understanding. Higher cost...", "l": "c", "k": ["claude", "opus", "anthropic", "capable", "model", "designed", "complex", "reasoning", "creative", "tasks", "nuanced", "understanding", "higher", "cost", "best"]}, {"id": "term-claude-shannon", "t": "Claude Shannon", "tg": ["History", "Pioneers"], "d": "history", "x": "American mathematician (1916-2001) known as the father of information theory, whose 1948 paper established the...", "l": "c", "k": ["claude", "shannon", "american", "mathematician", "1916-2001", "known", "father", "information", "theory", "whose", "paper", "established", "mathematical", "foundations", "digital"]}, {"id": "term-claude-sonnet", "t": "Claude Sonnet", "tg": ["Models", "Technical"], "d": "models", "x": "A balanced model in the Claude family offering strong performance with good efficiency. Suitable for a wide range of...", "l": "c", "k": ["claude", "sonnet", "balanced", "model", "family", "offering", "strong", "performance", "good", "efficiency", "suitable", "wide", "range", "tasks", "including"]}, {"id": "term-clean-room", "t": "Clean Room", "tg": ["Fabrication", "Manufacturing", "Facility"], "d": "hardware", "x": "Ultra-low particle environment where semiconductor manufacturing takes place. Modern fabs maintain Class 1 or better...", "l": "c", "k": ["clean", "room", "ultra-low", "particle", "environment", "semiconductor", "manufacturing", "takes", "place", "modern", "fabs", "maintain", "class", "better", "cleanliness"]}, {"id": "term-cleaned-alpaca", "t": "Cleaned Alpaca", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A cleaned version of the Stanford Alpaca dataset with improved data quality by fixing hallucinations empty outputs and...", "l": "c", "k": ["cleaned", "alpaca", "version", "stanford", "dataset", "improved", "data", "quality", "fixing", "hallucinations", "empty", "outputs", "issues", "original", "synthetic"]}, {"id": "term-clever-hans-effect", "t": "Clever Hans Effect", "tg": ["History", "Fundamentals"], "d": "history", "x": "Named after a horse that appeared to perform arithmetic but was actually reading its trainer's body language. In AI it...", "l": "c", "k": ["clever", "hans", "effect", "named", "horse", "appeared", "perform", "arithmetic", "actually", "reading", "trainer", "body", "language", "refers", "models"]}, {"id": "term-clevr", "t": "CLEVR", "tg": ["Benchmark", "Multimodal", "Reasoning"], "d": "datasets", "x": "Compositional Language and Elementary Visual Reasoning a synthetic visual QA dataset testing compositional reasoning...", "l": "c", "k": ["clevr", "compositional", "language", "elementary", "visual", "reasoning", "synthetic", "dataset", "testing", "rendered", "objects", "isolates", "recognition"]}, {"id": "term-cliff-shaw", "t": "Cliff Shaw", "tg": ["History", "Pioneers"], "d": "history", "x": "American programmer at RAND Corporation who along with Allen Newell and Herbert Simon developed the Logic Theorist...", "l": "c", "k": ["cliff", "shaw", "american", "programmer", "rand", "corporation", "along", "allen", "newell", "herbert", "simon", "developed", "logic", "theorist", "general"]}, {"id": "term-climax", "t": "ClimaX", "tg": ["Models", "Scientific"], "d": "models", "x": "A foundation model for weather and climate science that uses Transformers pre-trained on climate data for various...", "l": "c", "k": ["climax", "foundation", "model", "weather", "climate", "science", "uses", "transformers", "pre-trained", "data", "various", "atmospheric", "prediction", "tasks"]}, {"id": "term-clinicalbert", "t": "ClinicalBERT", "tg": ["Models", "Technical", "Medical", "NLP"], "d": "models", "x": "A BERT model fine-tuned on clinical notes from electronic health records to capture medical language patterns for...", "l": "c", "k": ["clinicalbert", "bert", "model", "fine-tuned", "clinical", "notes", "electronic", "health", "records", "capture", "medical", "language", "patterns", "nlp", "applications"]}, {"id": "term-clip", "t": "CLIP", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Contrastive Language-Image Pre-training, a model that learns visual concepts from natural language supervision by...", "l": "c", "k": ["clip", "contrastive", "language-image", "pre-training", "model", "learns", "visual", "concepts", "natural", "language", "supervision", "training", "image", "text", "encoders"]}, {"id": "term-clipped-surrogate-objective", "t": "Clipped Surrogate Objective", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "The core optimization objective in PPO that clips the probability ratio between new and old policies, preventing...", "l": "c", "k": ["clipped", "surrogate", "objective", "core", "optimization", "ppo", "clips", "probability", "ratio", "old", "policies", "preventing", "excessively", "large", "updates"]}, {"id": "term-clips", "t": "CLIPS", "tg": ["History", "Systems"], "d": "history", "x": "The C Language Integrated Production System developed by NASA's Johnson Space Center in 1985. An expert system shell...", "l": "c", "k": ["clips", "language", "integrated", "production", "system", "developed", "nasa", "johnson", "space", "center", "expert", "shell", "designed", "building", "rule-based"]}, {"id": "term-clock-cycle", "t": "Clock Cycle", "tg": ["Architecture", "Fundamentals", "Metric"], "d": "hardware", "x": "Single tick of a processor clock representing the smallest unit of time for digital operations. Processor speed...", "l": "c", "k": ["clock", "cycle", "single", "tick", "processor", "representing", "smallest", "unit", "time", "digital", "operations", "speed", "measured", "cycles", "per"]}, {"id": "term-cloud-computing-ai", "t": "Cloud Computing for AI", "tg": ["Distributed Computing", "Inference Infrastructure"], "d": "hardware", "x": "The use of cloud infrastructure services (AWS, GCP, Azure) for AI model training and inference, providing on-demand...", "l": "c", "k": ["cloud", "computing", "infrastructure", "services", "aws", "gcp", "azure", "model", "training", "inference", "providing", "on-demand", "access", "gpu", "clusters"]}, {"id": "term-cloud-gpu-instance", "t": "Cloud GPU Instance", "tg": ["Cloud", "GPU", "Service"], "d": "hardware", "x": "Virtual machine in a cloud data center with attached GPU accelerators for AI workloads. Major providers offer instances...", "l": "c", "k": ["cloud", "gpu", "instance", "virtual", "machine", "data", "center", "attached", "accelerators", "workloads", "major", "providers", "offer", "instances", "nvidia"]}, {"id": "term-cloze", "t": "Cloze Task", "tg": ["Task", "Evaluation"], "d": "datasets", "x": "A task where models predict missing words in text. A classic NLP benchmark and training objective. BERT's masked...", "l": "c", "k": ["cloze", "task", "models", "predict", "missing", "words", "text", "classic", "nlp", "benchmark", "training", "objective", "bert", "masked", "language"]}, {"id": "term-clustering", "t": "Clustering", "tg": ["ML Task", "Unsupervised"], "d": "general", "x": "An unsupervised learning technique that groups similar data points together without predefined labels. Used for...", "l": "c", "k": ["clustering", "unsupervised", "learning", "technique", "groups", "similar", "data", "points", "together", "without", "predefined", "labels", "customer", "segmentation", "document"]}, {"id": "term-clutrr", "t": "CLUTRR", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "Compositional Language Understanding and Textual Reasoning a benchmark testing systematic generalization in kinship...", "l": "c", "k": ["clutrr", "compositional", "language", "understanding", "textual", "reasoning", "benchmark", "testing", "systematic", "generalization", "kinship", "tests", "ability", "infer", "family"]}, {"id": "term-cma-es", "t": "CMA-ES", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Covariance Matrix Adaptation Evolution Strategy is a derivative-free optimization algorithm that adapts its search...", "l": "c", "k": ["cma-es", "covariance", "matrix", "adaptation", "evolution", "strategy", "derivative-free", "optimization", "algorithm", "adapts", "search", "distribution", "updating", "multivariate", "normal"]}, {"id": "term-cmmlu", "t": "CMMLU", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "Chinese Massive Multitask Language Understanding a comprehensive benchmark testing knowledge and reasoning across 67...", "l": "c", "k": ["cmmlu", "chinese", "massive", "multitask", "language", "understanding", "comprehensive", "benchmark", "testing", "knowledge", "reasoning", "across", "chinese-specific", "subjects", "covering"]}, {"id": "term-cmos", "t": "CMOS", "tg": ["Fabrication", "Technology", "Fundamentals"], "d": "hardware", "x": "Complementary Metal-Oxide-Semiconductor technology using paired NMOS and PMOS transistors for logic circuits. The...", "l": "c", "k": ["cmos", "complementary", "metal-oxide-semiconductor", "technology", "paired", "nmos", "pmos", "transistors", "logic", "circuits", "dominant", "manufacturing", "virtually", "modern", "digital"]}, {"id": "term-cmu-ai-research", "t": "CMU AI Research", "tg": ["History", "Milestones"], "d": "history", "x": "Carnegie Mellon University's AI research programs, including the work of Allen Newell and Herbert Simon, the...", "l": "c", "k": ["cmu", "research", "carnegie", "mellon", "university", "programs", "including", "work", "allen", "newell", "herbert", "simon", "development", "expert", "systems"]}, {"id": "term-cnn", "t": "CNN (Convolutional Neural Network)", "tg": ["Architecture", "Computer Vision"], "d": "models", "x": "A neural network architecture designed for processing grid-like data such as images. Uses convolutional layers to...", "l": "c", "k": ["cnn", "convolutional", "neural", "network", "architecture", "designed", "processing", "grid-like", "data", "images", "uses", "layers", "automatically", "learn", "spatial"]}, {"id": "term-cnndailymail", "t": "CNN/DailyMail", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A dataset of over 300000 news articles from CNN and the Daily Mail paired with multi-sentence summaries. One of the...", "l": "c", "k": ["cnn", "dailymail", "dataset", "news", "articles", "daily", "mail", "paired", "multi-sentence", "summaries", "widely", "benchmarks", "abstractive", "text", "summarization"]}, {"id": "term-co-detr", "t": "Co-DETR", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "Collaborative DETR is a detection Transformer that uses collaborative hybrid assignments from multiple parallel heads...", "l": "c", "k": ["co-detr", "collaborative", "detr", "detection", "transformer", "uses", "hybrid", "assignments", "multiple", "parallel", "heads", "improve", "training", "efficiency", "accuracy"]}, {"id": "term-co-occurrence-matrix", "t": "Co-occurrence Matrix", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A matrix recording how often pairs of words appear together within a defined context window across a corpus, used as...", "l": "c", "k": ["co-occurrence", "matrix", "recording", "pairs", "words", "appear", "together", "within", "defined", "context", "window", "across", "corpus", "basis", "distributional"]}, {"id": "term-co-packaged-optics", "t": "Co-Packaged Optics", "tg": ["Networking", "Photonic", "Packaging"], "d": "hardware", "x": "Technology integrating optical transceivers directly into switch packages rather than using separate pluggable modules....", "l": "c", "k": ["co-packaged", "optics", "technology", "integrating", "optical", "transceivers", "directly", "switch", "packages", "rather", "separate", "pluggable", "modules", "reduces", "power"]}, {"id": "term-co-training", "t": "Co-Training", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A semi-supervised learning method that trains two classifiers on different views or feature sets of the data. Each...", "l": "c", "k": ["co-training", "semi-supervised", "learning", "method", "trains", "classifiers", "different", "views", "feature", "sets", "data", "classifier", "labels", "unlabeled", "examples"]}, {"id": "term-coarse-grained-reconfigurable-architecture", "t": "Coarse-Grained Reconfigurable Architecture", "tg": ["Architecture", "Reconfigurable", "Emerging"], "d": "hardware", "x": "Programmable hardware architecture operating on word-level data paths rather than individual bits like FPGAs. Offers...", "l": "c", "k": ["coarse-grained", "reconfigurable", "architecture", "programmable", "hardware", "operating", "word-level", "data", "paths", "rather", "individual", "bits", "fpgas", "offers", "better"]}, {"id": "term-coaxial-cable", "t": "Coaxial Cable", "tg": ["Networking", "Historical", "Physical"], "d": "hardware", "x": "Electrical cable with concentric conductors historically used for data networking. Has been largely replaced by fiber...", "l": "c", "k": ["coaxial", "cable", "electrical", "concentric", "conductors", "historically", "data", "networking", "largely", "replaced", "fiber", "optics", "modern", "centers", "remains"]}, {"id": "term-coca", "t": "CoCa", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "Contrastive Captioners is a model from Google that combines contrastive learning with image captioning in a unified...", "l": "c", "k": ["coca", "contrastive", "captioners", "model", "google", "combines", "learning", "image", "captioning", "unified", "encoder-decoder", "framework", "visual", "understanding"]}, {"id": "term-cocktail-shaker-sort", "t": "Cocktail Shaker Sort", "tg": ["Algorithms", "Technical", "Sorting"], "d": "algorithms", "x": "A bidirectional variant of bubble sort that traverses the list alternately from left to right and right to left....", "l": "c", "k": ["cocktail", "shaker", "sort", "bidirectional", "variant", "bubble", "traverses", "list", "alternately", "left", "right", "slightly", "efficient", "standard", "certain"]}, {"id": "term-coco", "t": "COCO", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "The Common Objects in Context dataset containing over 330000 images with object detection segmentation and captioning...", "l": "c", "k": ["coco", "common", "objects", "context", "dataset", "containing", "images", "object", "detection", "segmentation", "captioning", "annotations", "categories", "primary", "benchmark"]}, {"id": "term-coco-captions", "t": "COCO Captions", "tg": ["Benchmark", "Computer Vision", "NLP"], "d": "datasets", "x": "A subset of the COCO dataset with five human-written captions per image totaling over 1.5 million captions. Widely used...", "l": "c", "k": ["coco", "captions", "subset", "dataset", "five", "human-written", "per", "image", "totaling", "million", "widely", "training", "evaluating", "captioning", "models"]}, {"id": "term-coco-dataset", "t": "COCO Dataset", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Common Objects in Context, a large-scale benchmark dataset containing images with annotations for object detection,...", "l": "c", "k": ["coco", "dataset", "common", "objects", "context", "large-scale", "benchmark", "containing", "images", "annotations", "object", "detection", "instance", "segmentation", "keypoint"]}, {"id": "term-coco-panoptic", "t": "COCO Panoptic", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "An extension of COCO that unifies instance and semantic segmentation into panoptic segmentation where every pixel...", "l": "c", "k": ["coco", "panoptic", "extension", "unifies", "instance", "semantic", "segmentation", "pixel", "receives", "label", "identity"]}, {"id": "term-code-alpaca", "t": "Code Alpaca", "tg": ["Training Corpus", "Code"], "d": "datasets", "x": "A dataset of 20000 code instruction-following examples generated using the Self-Instruct framework. Used for...", "l": "c", "k": ["code", "alpaca", "dataset", "instruction-following", "examples", "generated", "self-instruct", "framework", "fine-tuning", "language", "models", "coding", "tasks"]}, {"id": "term-code-as-policies", "t": "Code as Policies", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "A framework that uses large language models to generate robot policy code from natural language instructions enabling...", "l": "c", "k": ["code", "policies", "framework", "uses", "large", "language", "models", "generate", "robot", "policy", "natural", "instructions", "enabling", "flexible", "interpretable"]}, {"id": "term-code-generation", "t": "Code Generation", "tg": ["Application", "Development"], "d": "general", "x": "The ability of AI models to write programming code from natural language descriptions. Powers tools like GitHub...", "l": "c", "k": ["code", "generation", "ability", "models", "write", "programming", "natural", "language", "descriptions", "powers", "tools", "github", "copilot", "cursor", "code-focused"]}, {"id": "term-code-generation-prompting", "t": "Code Generation Prompting", "tg": ["Prompt Engineering", "Code"], "d": "general", "x": "Specialized prompting techniques for producing high-quality code, incorporating language specification, function...", "l": "c", "k": ["code", "generation", "prompting", "specialized", "techniques", "producing", "high-quality", "incorporating", "language", "specification", "function", "signatures", "docstrings", "test", "cases"]}, {"id": "term-code-interpreter", "t": "Code Interpreter", "tg": ["Feature", "Tool Use"], "d": "general", "x": "AI capability to write and execute code, enabling data analysis, visualization, and computation. ChatGPT's code...", "l": "c", "k": ["code", "interpreter", "capability", "write", "execute", "enabling", "data", "analysis", "visualization", "computation", "chatgpt", "runs", "python", "sandbox", "environment"]}, {"id": "term-code-llama", "t": "Code Llama", "tg": ["Models", "Technical"], "d": "models", "x": "A specialized version of LLaMA fine-tuned for code generation and understanding. Available in base Python and...", "l": "c", "k": ["code", "llama", "specialized", "version", "fine-tuned", "generation", "understanding", "available", "base", "python", "instruction-tuned", "variants", "supports", "infilling", "long-context"]}, {"id": "term-code-llm", "t": "Code LLM", "tg": ["Model Type", "Specialized"], "d": "models", "x": "Language models specialized for programming tasks. Examples include Codex, StarCoder, and Code Llama. Often trained on...", "l": "c", "k": ["code", "llm", "language", "models", "specialized", "programming", "tasks", "examples", "include", "codex", "starcoder", "llama", "trained", "large", "corpora"]}, {"id": "term-code-switching", "t": "Code-Switching", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The phenomenon of alternating between two or more languages within a single conversation or utterance, posing...", "l": "c", "k": ["code-switching", "phenomenon", "alternating", "languages", "within", "single", "conversation", "utterance", "posing", "challenges", "nlp", "systems", "designed", "monolingual", "text"]}, {"id": "term-codebleu", "t": "CodeBLEU", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A code evaluation metric that extends BLEU with code-specific components including abstract syntax tree matching,...", "l": "c", "k": ["codebleu", "code", "evaluation", "metric", "extends", "bleu", "code-specific", "components", "including", "abstract", "syntax", "tree", "matching", "data-flow", "analysis"]}, {"id": "term-codecontests", "t": "CodeContests", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A competitive programming dataset from Google DeepMind containing problems from Codeforces and other platforms. Used to...", "l": "c", "k": ["codecontests", "competitive", "programming", "dataset", "google", "deepmind", "containing", "problems", "codeforces", "platforms", "train", "alphacode", "solutions", "test", "cases"]}, {"id": "term-codeforces-dataset", "t": "Codeforces Dataset", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A collection of competitive programming problems from the Codeforces platform with solutions and test cases. Used for...", "l": "c", "k": ["codeforces", "dataset", "collection", "competitive", "programming", "problems", "platform", "solutions", "test", "cases", "evaluating", "advanced", "algorithmic", "problem", "solving"]}, {"id": "term-codegemma", "t": "CodeGemma", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A collection of code-specialized models from Google built on the Gemma architecture for code generation and infilling...", "l": "c", "k": ["codegemma", "collection", "code-specialized", "models", "google", "built", "gemma", "architecture", "code", "generation", "infilling", "instruction", "following"]}, {"id": "term-codegen", "t": "CodeGen", "tg": ["Models", "Technical"], "d": "models", "x": "A family of large language models for program synthesis that convert natural language descriptions into executable...", "l": "c", "k": ["codegen", "family", "large", "language", "models", "program", "synthesis", "convert", "natural", "descriptions", "executable", "code", "trained", "multi-turn", "enabling"]}, {"id": "term-codeparrot", "t": "CodeParrot", "tg": ["Training Corpus", "Code"], "d": "datasets", "x": "An open-source dataset of Python code from GitHub used to train the CodeParrot code generation model. Contains...", "l": "c", "k": ["codeparrot", "open-source", "dataset", "python", "code", "github", "train", "generation", "model", "contains", "approximately", "180gb", "deduplicated", "source"]}, {"id": "term-codeqwen", "t": "CodeQwen", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A code-specialized model from the Qwen family trained on diverse programming language data for code generation and...", "l": "c", "k": ["codeqwen", "code-specialized", "model", "qwen", "family", "trained", "diverse", "programming", "language", "data", "code", "generation", "understanding", "debugging"]}, {"id": "term-codesearchnet", "t": "CodeSearchNet", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A dataset and benchmark for code search and code summarization spanning six programming languages. Contains 2 million...", "l": "c", "k": ["codesearchnet", "dataset", "benchmark", "code", "search", "summarization", "spanning", "six", "programming", "languages", "contains", "million", "code-comment", "pairs", "training"]}, {"id": "term-codeshell", "t": "CodeShell", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A 7 billion parameter code foundation model trained on 500 billion tokens of code and text data for diverse programming...", "l": "c", "k": ["codeshell", "billion", "parameter", "code", "foundation", "model", "trained", "tokens", "text", "data", "diverse", "programming", "language", "generation"]}, {"id": "term-codestral", "t": "Codestral", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A code-focused large language model from Mistral AI trained on diverse programming languages for code generation and...", "l": "c", "k": ["codestral", "code-focused", "large", "language", "model", "mistral", "trained", "diverse", "programming", "languages", "code", "generation", "completion", "explanation", "tasks"]}, {"id": "term-codex", "t": "Codex", "tg": ["Models", "Technical"], "d": "models", "x": "An OpenAI model fine-tuned from GPT-3 on publicly available code from GitHub. Powers GitHub Copilot for code completion...", "l": "c", "k": ["codex", "openai", "model", "fine-tuned", "gpt-3", "publicly", "available", "code", "github", "powers", "copilot", "completion", "generation", "proficient", "python"]}, {"id": "term-codexglue", "t": "CodeXGLUE", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A benchmark for code intelligence covering 10 code-related tasks including code completion translation and...", "l": "c", "k": ["codexglue", "benchmark", "code", "intelligence", "covering", "code-related", "tasks", "including", "completion", "translation", "summarization", "tests", "broad", "understanding", "generation"]}, {"id": "term-cogagent", "t": "CogAgent", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A visual language model from Tsinghua that specializes in GUI understanding and navigation with the ability to interact...", "l": "c", "k": ["cogagent", "visual", "language", "model", "tsinghua", "specializes", "gui", "understanding", "navigation", "ability", "interact", "graphical", "user", "interfaces"]}, {"id": "term-cognitive-liberty", "t": "Cognitive Liberty", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The right of individuals to maintain sovereignty over their own thought processes and mental states free from...", "l": "c", "k": ["cognitive", "liberty", "right", "individuals", "maintain", "sovereignty", "thought", "processes", "mental", "states", "free", "manipulation", "neurotechnology", "emerging", "concept"]}, {"id": "term-cognitive-load", "t": "Cognitive Load (Prompting)", "tg": ["Prompting", "Best Practice"], "d": "general", "x": "The mental effort required to process complex prompts. Simpler, well-organized prompts often yield better results by...", "l": "c", "k": ["cognitive", "load", "prompting", "mental", "effort", "required", "process", "complex", "prompts", "simpler", "well-organized", "yield", "better", "results", "reducing"]}, {"id": "term-cogvideo", "t": "CogVideo", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A large-scale pre-trained text-to-video generation model from Tsinghua University that inherits knowledge from CogView...", "l": "c", "k": ["cogvideo", "large-scale", "pre-trained", "text-to-video", "generation", "model", "tsinghua", "university", "inherits", "knowledge", "cogview", "video", "creation"]}, {"id": "term-cogvideox", "t": "CogVideoX", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An extended version of CogVideo with improved temporal coherence and video quality for generating longer and more...", "l": "c", "k": ["cogvideox", "extended", "version", "cogvideo", "improved", "temporal", "coherence", "video", "quality", "generating", "longer", "detailed", "sequences", "text"]}, {"id": "term-cogvlm", "t": "CogVLM", "tg": ["Models", "Technical"], "d": "models", "x": "A visual language model that integrates vision features deep into the language model through a visual expert module in...", "l": "c", "k": ["cogvlm", "visual", "language", "model", "integrates", "vision", "features", "deep", "expert", "module", "attention", "ffn", "layer", "achieves", "strong"]}, {"id": "term-cohens-kappa", "t": "Cohen's Kappa", "tg": ["Statistics", "Metrics"], "d": "datasets", "x": "A statistic measuring inter-rater agreement for categorical items that accounts for agreement occurring by chance....", "l": "c", "k": ["cohen", "kappa", "statistic", "measuring", "inter-rater", "agreement", "categorical", "items", "accounts", "occurring", "chance", "values", "range", "indicating", "perfect"]}, {"id": "term-cohere", "t": "Cohere", "tg": ["Company", "LLM Provider"], "d": "models", "x": "An enterprise AI company providing LLMs for text generation, embeddings, and search. Known for Command models and focus...", "l": "c", "k": ["cohere", "enterprise", "company", "providing", "llms", "text", "generation", "embeddings", "search", "known", "command", "models", "focus", "cases", "strong"]}, {"id": "term-cohere-aya-expanse", "t": "Cohere Aya Expanse", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An extended multilingual model from Cohere For AI building on the Aya project with expanded language coverage and...", "l": "c", "k": ["cohere", "aya", "expanse", "extended", "multilingual", "model", "building", "project", "expanded", "language", "coverage", "improved", "cross-lingual", "performance"]}, {"id": "term-cohere-command", "t": "Cohere Command", "tg": ["Models", "Technical"], "d": "models", "x": "A family of generative language models by Cohere designed for enterprise applications. Optimized for instruction...", "l": "c", "k": ["cohere", "command", "family", "generative", "language", "models", "designed", "enterprise", "applications", "optimized", "instruction", "following", "text", "generation", "tool"]}, {"id": "term-cohere-embed", "t": "Cohere Embed", "tg": ["Models", "Technical"], "d": "models", "x": "A family of embedding models by Cohere designed for search and retrieval applications. Supports over 100 languages with...", "l": "c", "k": ["cohere", "embed", "family", "embedding", "models", "designed", "search", "retrieval", "applications", "supports", "languages", "state-of-the-art", "performance", "multilingual", "benchmarks"]}, {"id": "term-cohere-embed-v3", "t": "Cohere Embed v3", "tg": ["Models", "Technical", "Embedding", "NLP", "Products"], "d": "models", "x": "The third generation of Cohere embedding model featuring improved multilingual support and compression-aware training...", "l": "c", "k": ["cohere", "embed", "generation", "embedding", "model", "featuring", "improved", "multilingual", "support", "compression-aware", "training", "efficient", "vector", "search"]}, {"id": "term-coherence-modeling", "t": "Coherence Modeling", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The computational assessment of how well sentences in a text flow together logically and topically, evaluating whether...", "l": "c", "k": ["coherence", "modeling", "computational", "assessment", "sentences", "text", "flow", "together", "logically", "topically", "evaluating", "reads", "naturally", "maintains", "consistent"]}, {"id": "term-coherence-score", "t": "Coherence Score", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation metric that assesses the logical consistency and semantic flow of generated text, measuring whether ideas...", "l": "c", "k": ["coherence", "score", "evaluation", "metric", "assesses", "logical", "consistency", "semantic", "flow", "generated", "text", "measuring", "ideas", "connect", "naturally"]}, {"id": "term-coinflip", "t": "CoinFlip", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A simple benchmark testing language models on symbolic reasoning by tracking coin flip state changes. Tests the ability...", "l": "c", "k": ["coinflip", "simple", "benchmark", "testing", "language", "models", "symbolic", "reasoning", "tracking", "coin", "flip", "state", "changes", "tests", "ability"]}, {"id": "term-cointegration", "t": "Cointegration", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A statistical property of two or more non-stationary time series that share a common stochastic trend, meaning a linear...", "l": "c", "k": ["cointegration", "statistical", "property", "non-stationary", "time", "series", "share", "common", "stochastic", "trend", "meaning", "linear", "combination", "stationary", "implies"]}, {"id": "term-cola", "t": "CoLA", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "The Corpus of Linguistic Acceptability containing 10657 English sentences annotated for grammatical acceptability by...", "l": "c", "k": ["cola", "corpus", "linguistic", "acceptability", "containing", "english", "sentences", "annotated", "grammatical", "expert", "linguists", "tests", "competence", "within", "glue"]}, {"id": "term-colbert", "t": "ColBERT", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A late-interaction retrieval model that independently encodes queries and documents into per-token embeddings, then...", "l": "c", "k": ["colbert", "late-interaction", "retrieval", "model", "independently", "encodes", "queries", "documents", "per-token", "embeddings", "scores", "relevance", "efficient", "maxsim", "operations"]}, {"id": "term-cold-start", "t": "Cold Start Problem", "tg": ["Challenge", "Recommendations"], "d": "general", "x": "Difficulty making predictions for new users or items with no historical data. Common in recommendation systems....", "l": "c", "k": ["cold", "start", "problem", "difficulty", "making", "predictions", "users", "items", "historical", "data", "common", "recommendation", "systems", "addressed", "hybrid"]}, {"id": "term-collaborative-filtering", "t": "Collaborative Filtering", "tg": ["Technique", "Recommendations"], "d": "general", "x": "Recommendation technique based on user behavior patterns. \"Users who liked X also liked Y.\" Forms the basis of many...", "l": "c", "k": ["collaborative", "filtering", "recommendation", "technique", "based", "user", "behavior", "patterns", "users", "liked", "forms", "basis", "systems", "netflix", "amazon"]}, {"id": "term-collaborative-filtering-model", "t": "Collaborative Filtering Model", "tg": ["Models", "Fundamentals", "Recommendation"], "d": "models", "x": "A recommendation approach that predicts user preferences by finding patterns in the collective behavior of many users...", "l": "c", "k": ["collaborative", "filtering", "model", "recommendation", "approach", "predicts", "user", "preferences", "finding", "patterns", "collective", "behavior", "users", "items"]}, {"id": "term-collection", "t": "Collection", "tg": ["Vector Database", "Infrastructure"], "d": "hardware", "x": "A named grouping of vectors and their associated metadata within a vector database, analogous to a table in relational...", "l": "c", "k": ["collection", "named", "grouping", "vectors", "associated", "metadata", "within", "vector", "database", "analogous", "table", "relational", "databases", "serving", "primary"]}, {"id": "term-collective-action-in-ai-safety", "t": "Collective Action in AI Safety", "tg": ["Safety", "Policy"], "d": "safety", "x": "The challenge of coordinating multiple AI developers and nations to invest in safety measures when competitive...", "l": "c", "k": ["collective", "action", "safety", "challenge", "coordinating", "multiple", "developers", "nations", "invest", "measures", "competitive", "pressures", "incentivize", "racing", "ahead"]}, {"id": "term-collective-communication", "t": "Collective Communication", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "Coordinated data exchange patterns among multiple processes or GPUs, including all-reduce, all-gather, reduce-scatter,...", "l": "c", "k": ["collective", "communication", "coordinated", "data", "exchange", "patterns", "among", "multiple", "processes", "gpus", "including", "all-reduce", "all-gather", "reduce-scatter", "broadcast"]}, {"id": "term-collocation", "t": "Collocation", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A sequence of words that co-occur more frequently than expected by chance, forming conventional expressions such as...", "l": "c", "k": ["collocation", "sequence", "words", "co-occur", "frequently", "expected", "chance", "forming", "conventional", "expressions", "strong", "coffee", "decision", "identified", "statistical"]}, {"id": "term-colocation-data-center", "t": "Colocation Data Center", "tg": ["Data Center", "Infrastructure", "Service"], "d": "hardware", "x": "Third-party facility where organizations rent space power and cooling for their own computing equipment. Used by AI...", "l": "c", "k": ["colocation", "data", "center", "third-party", "facility", "organizations", "rent", "space", "power", "cooling", "computing", "equipment", "companies", "need", "physical"]}, {"id": "term-colossus-computer", "t": "Colossus Computer", "tg": ["History", "Milestones"], "d": "history", "x": "The world's first programmable electronic digital computer, built at Bletchley Park in 1943-1944 to break German Lorenz...", "l": "c", "k": ["colossus", "computer", "world", "programmable", "electronic", "digital", "built", "bletchley", "park", "1943-1944", "break", "german", "lorenz", "cipher", "messages"]}, {"id": "term-column-generation", "t": "Column Generation", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An optimization technique for solving large linear programs with many variables by starting with a restricted set of...", "l": "c", "k": ["column", "generation", "optimization", "technique", "solving", "large", "linear", "programs", "variables", "starting", "restricted", "columns", "iteratively", "adding", "profitable"]}, {"id": "term-comb-sort", "t": "Comb Sort", "tg": ["Algorithms", "Technical", "Sorting"], "d": "algorithms", "x": "An improvement over bubble sort that uses a shrinking gap to compare and swap elements. The gap starts large and...", "l": "c", "k": ["comb", "sort", "improvement", "bubble", "uses", "shrinking", "gap", "compare", "swap", "elements", "starts", "large", "shrinks", "factor", "approximately"]}, {"id": "term-combinatorial-explosion", "t": "Combinatorial Explosion", "tg": ["History", "Fundamentals"], "d": "history", "x": "The rapid growth of possible states or paths in a problem space as the number of variables increases. A fundamental...", "l": "c", "k": ["combinatorial", "explosion", "rapid", "growth", "possible", "states", "paths", "problem", "space", "number", "variables", "increases", "fundamental", "challenge", "search"]}, {"id": "term-comet-reference-data", "t": "COMET Reference Data", "tg": ["Benchmark", "NLP", "Evaluation", "Translation"], "d": "datasets", "x": "Evaluation data for the COMET machine translation metric which uses neural models to predict translation quality scores...", "l": "c", "k": ["comet", "reference", "data", "evaluation", "machine", "translation", "metric", "uses", "neural", "models", "predict", "quality", "scores", "correlate", "human"]}, {"id": "term-command-model", "t": "Command Model", "tg": ["Model", "Cohere"], "d": "models", "x": "Cohere's instruction-tuned LLMs optimized for following commands and business applications. Includes Command R for RAG...", "l": "c", "k": ["command", "model", "cohere", "instruction-tuned", "llms", "optimized", "following", "commands", "business", "applications", "includes", "rag", "enterprise", "cases"]}, {"id": "term-command-r", "t": "Command R", "tg": ["Models", "Technical"], "d": "models", "x": "A family of language models by Cohere optimized for retrieval-augmented generation and tool use. Designed for...", "l": "c", "k": ["command", "family", "language", "models", "cohere", "optimized", "retrieval-augmented", "generation", "tool", "designed", "enterprise", "applications", "strong", "grounding", "retrieved"]}, {"id": "term-commitbench", "t": "CommitBench", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A benchmark of code changes from real git commits testing the ability to generate meaningful code modifications. Tests...", "l": "c", "k": ["commitbench", "benchmark", "code", "changes", "real", "git", "commits", "testing", "ability", "generate", "meaningful", "modifications", "tests", "understanding", "incremental"]}, {"id": "term-commitpack", "t": "CommitPack", "tg": ["Training Corpus", "Code"], "d": "datasets", "x": "A dataset of 4 terabytes of Git commits across 350 programming languages. Provides code change data for training models...", "l": "c", "k": ["commitpack", "dataset", "terabytes", "git", "commits", "across", "programming", "languages", "provides", "code", "change", "data", "training", "models", "editing"]}, {"id": "term-common-crawl", "t": "Common Crawl", "tg": ["Data", "Training"], "d": "general", "x": "A massive open repository of web data used to train many LLMs. Contains petabytes of text crawled from the internet,...", "l": "c", "k": ["common", "crawl", "massive", "open", "repository", "web", "data", "train", "llms", "contains", "petabytes", "text", "crawled", "internet", "requiring"]}, {"id": "term-common-voice", "t": "Common Voice", "tg": ["Training Corpus", "Speech", "Multilingual"], "d": "datasets", "x": "A Mozilla project collecting a massively multilingual open-source dataset of human speech. Contains validated...", "l": "c", "k": ["common", "voice", "mozilla", "project", "collecting", "massively", "multilingual", "open-source", "dataset", "human", "speech", "contains", "validated", "recordings", "languages"]}, {"id": "term-commonsense-reasoning", "t": "Commonsense Reasoning", "tg": ["Capability", "Reasoning"], "d": "general", "x": "AI's ability to understand everyday knowledge humans take for granted. That water is wet, objects fall down, people...", "l": "c", "k": ["commonsense", "reasoning", "ability", "understand", "everyday", "knowledge", "humans", "take", "granted", "water", "wet", "objects", "fall", "down", "people"]}, {"id": "term-commonsenseqa", "t": "CommonsenseQA", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A question answering dataset requiring diverse types of commonsense knowledge. Contains 12247 questions generated from...", "l": "c", "k": ["commonsenseqa", "question", "answering", "dataset", "requiring", "diverse", "types", "commonsense", "knowledge", "contains", "questions", "generated", "conceptnet", "graph", "edges"]}, {"id": "term-commonsenseqa-20", "t": "CommonsenseQA 2.0", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A successor to CommonsenseQA with harder commonsense reasoning questions generated through a gamified approach. Uses...", "l": "c", "k": ["commonsenseqa", "successor", "harder", "commonsense", "reasoning", "questions", "generated", "gamified", "approach", "uses", "yes", "format", "validated", "human", "agreement"]}, {"id": "term-communication-marl", "t": "Communication in Multi-Agent RL", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "Protocols and mechanisms that allow agents in a multi-agent system to share information through learned communication...", "l": "c", "k": ["communication", "multi-agent", "protocols", "mechanisms", "allow", "agents", "system", "share", "information", "learned", "channels", "emergent", "develop", "structured", "language-like"]}, {"id": "term-communication-overlap", "t": "Communication Overlap", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A distributed training optimization that overlaps gradient communication with backward pass computation, hiding...", "l": "c", "k": ["communication", "overlap", "distributed", "training", "optimization", "overlaps", "gradient", "backward", "pass", "computation", "hiding", "latency", "behind", "useful", "work"]}, {"id": "term-communication-bound", "t": "Communication-Bound", "tg": ["Performance", "Analysis", "Distributed"], "d": "hardware", "x": "Condition where distributed training speed is limited by network bandwidth between nodes. Becomes the dominant...", "l": "c", "k": ["communication-bound", "condition", "distributed", "training", "speed", "limited", "network", "bandwidth", "nodes", "becomes", "dominant", "bottleneck", "scaling", "large", "gpu"]}, {"id": "term-companion-matrix-method", "t": "Companion Matrix Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A technique for finding polynomial roots by computing the eigenvalues of the companion matrix. Converts the...", "l": "c", "k": ["companion", "matrix", "method", "technique", "finding", "polynomial", "roots", "computing", "eigenvalues", "converts", "root-finding", "problem", "well-studied", "eigenvalue", "solvable"]}, {"id": "term-competitive-rl", "t": "Competitive Reinforcement Learning", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A multi-agent RL setting where agents have opposing objectives, such as zero-sum games. Competitive RL involves finding...", "l": "c", "k": ["competitive", "reinforcement", "learning", "multi-agent", "setting", "agents", "opposing", "objectives", "zero-sum", "games", "involves", "finding", "nash", "equilibria", "developing"]}, {"id": "term-compile-time-graph-optimization", "t": "Compile-Time Graph Optimization", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Static optimization of computation graphs before execution, including constant folding, dead code elimination, and...", "l": "c", "k": ["compile-time", "graph", "optimization", "static", "computation", "graphs", "execution", "including", "constant", "folding", "dead", "code", "elimination", "operator", "fusion"]}, {"id": "term-completion", "t": "Completion", "tg": ["Task", "Fundamentals"], "d": "general", "x": "Text generated by an AI to continue a given prompt. The basic operation of language models: given input text, predict...", "l": "c", "k": ["completion", "text", "generated", "continue", "given", "prompt", "basic", "operation", "language", "models", "input", "predict", "comes", "next"]}, {"id": "term-complexity-based-prompting", "t": "Complexity-Based Prompting", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A self-consistency variant that selects the final answer from reasoning chains with the highest complexity, measured by...", "l": "c", "k": ["complexity-based", "prompting", "self-consistency", "variant", "selects", "final", "answer", "reasoning", "chains", "highest", "complexity", "measured", "number", "steps", "based"]}, {"id": "term-compliance-by-design", "t": "Compliance-by-Design", "tg": ["Safety", "Governance"], "d": "safety", "x": "An approach to AI development that embeds regulatory compliance requirements into the system design and development...", "l": "c", "k": ["compliance-by-design", "approach", "development", "embeds", "regulatory", "compliance", "requirements", "system", "design", "process", "outset", "rather", "adding", "measures", "retroactively"]}, {"id": "term-composable-infrastructure", "t": "Composable Infrastructure", "tg": ["Infrastructure", "Architecture", "Flexible"], "d": "hardware", "x": "Data center architecture where compute storage and networking resources can be dynamically assembled and reassembled...", "l": "c", "k": ["composable", "infrastructure", "data", "center", "architecture", "compute", "storage", "networking", "resources", "dynamically", "assembled", "reassembled", "different", "configurations", "enables"]}, {"id": "term-compositional-robustness", "t": "Compositional Robustness", "tg": ["Safety", "Technical"], "d": "safety", "x": "The ability of an AI system to maintain safe behavior when its components are combined in new ways or deployed in...", "l": "c", "k": ["compositional", "robustness", "ability", "system", "maintain", "safe", "behavior", "components", "combined", "ways", "deployed", "contexts", "different", "anticipated", "development"]}, {"id": "term-compositionality", "t": "Compositionality", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The principle that the meaning of a complex expression is determined by the meanings of its parts and the rules used to...", "l": "c", "k": ["compositionality", "principle", "meaning", "complex", "expression", "determined", "meanings", "parts", "rules", "combine", "foundational", "concept", "formal", "semantics"]}, {"id": "term-compressed-trie-algorithm", "t": "Compressed Trie Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A space-optimized trie that merges chains of single-child nodes into single edges labeled with strings. Also known as a...", "l": "c", "k": ["compressed", "trie", "algorithm", "space-optimized", "merges", "chains", "single-child", "nodes", "single", "edges", "labeled", "strings", "known", "patricia", "tree"]}, {"id": "term-compression", "t": "Compression (Model)", "tg": ["Optimization", "Deployment"], "d": "algorithms", "x": "Reducing model size while maintaining performance. Techniques include quantization, pruning, and distillation. Enables...", "l": "c", "k": ["compression", "model", "reducing", "size", "maintaining", "performance", "techniques", "include", "quantization", "pruning", "distillation", "enables", "deployment", "edge", "devices"]}, {"id": "term-compute", "t": "Compute", "tg": ["Infrastructure", "Resources"], "d": "hardware", "x": "Computational resources required for training and running AI models. Measured in FLOPs, GPU-hours, or dollars. A...", "l": "c", "k": ["compute", "computational", "resources", "required", "training", "running", "models", "measured", "flops", "gpu-hours", "dollars", "primary", "constraint", "cost", "driver"]}, {"id": "term-compute-express-link", "t": "Compute Express Link", "tg": ["Interconnect", "Memory", "Standard"], "d": "hardware", "x": "Cache-coherent interconnect standard built on PCIe physical layer that enables shared memory between CPUs GPUs and...", "l": "c", "k": ["compute", "express", "link", "cache-coherent", "interconnect", "standard", "built", "pcie", "physical", "layer", "enables", "shared", "memory", "cpus", "gpus"]}, {"id": "term-compute-governance", "t": "Compute Governance", "tg": ["Governance", "AI Safety"], "d": "safety", "x": "Policy approaches that use computational resources as a lever for AI governance, including monitoring large training...", "l": "c", "k": ["compute", "governance", "policy", "approaches", "computational", "resources", "lever", "including", "monitoring", "large", "training", "runs", "export", "controls", "chips"]}, {"id": "term-compute-bound", "t": "Compute-Bound Workload", "tg": ["Hardware", "Model Optimization"], "d": "models", "x": "A processing task where performance is limited by the rate of arithmetic computation rather than memory bandwidth or...", "l": "c", "k": ["compute-bound", "workload", "processing", "task", "performance", "limited", "rate", "arithmetic", "computation", "rather", "memory", "bandwidth", "training", "large", "models"]}, {"id": "term-compute-optimal-training", "t": "Compute-Optimal Training", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An approach to model training that seeks the best allocation of a fixed compute budget between model parameters and...", "l": "c", "k": ["compute-optimal", "training", "approach", "model", "seeks", "best", "allocation", "fixed", "compute", "budget", "parameters", "tokens", "based", "empirical", "scaling"]}, {"id": "term-computer-vision", "t": "Computer Vision", "tg": ["Field", "Images"], "d": "general", "x": "The field of AI that enables machines to interpret and understand visual information from images and videos....", "l": "c", "k": ["computer", "vision", "field", "enables", "machines", "interpret", "understand", "visual", "information", "images", "videos", "applications", "include", "object", "detection"]}, {"id": "term-computer-vision-history", "t": "Computer Vision History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of computer vision from early edge detection and pattern recognition in the 1960s through feature-based...", "l": "c", "k": ["computer", "vision", "history", "evolution", "early", "edge", "detection", "pattern", "recognition", "1960s", "feature-based", "methods", "sift", "hog", "deep"]}, {"id": "term-computing-machinery-and-intelligence", "t": "Computing Machinery and Intelligence", "tg": ["History", "Milestones"], "d": "history", "x": "A seminal 1950 paper by Alan Turing published in the journal Mind that proposed the imitation game (later known as the...", "l": "c", "k": ["computing", "machinery", "intelligence", "seminal", "paper", "alan", "turing", "published", "journal", "mind", "proposed", "imitation", "game", "later", "known"]}, {"id": "term-concept-drift", "t": "Concept Drift", "tg": ["Challenge", "Production"], "d": "general", "x": "When the relationship between input and output changes over time, causing model performance to degrade. Requires...", "l": "c", "k": ["concept", "drift", "relationship", "input", "output", "changes", "time", "causing", "model", "performance", "degrade", "requires", "monitoring", "retraining", "maintain"]}, {"id": "term-conceptnet", "t": "ConceptNet", "tg": ["Knowledge", "Graph", "NLP"], "d": "datasets", "x": "A multilingual knowledge graph connecting everyday concepts with labeled relationships. Contains over 21 million edges...", "l": "c", "k": ["conceptnet", "multilingual", "knowledge", "graph", "connecting", "everyday", "concepts", "labeled", "relationships", "contains", "million", "edges", "representing", "commonsense", "multiple"]}, {"id": "term-conceptual-12m", "t": "Conceptual 12M", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "An expanded version of Conceptual Captions with 12 million image-caption pairs. Relaxes quality filters to provide more...", "l": "c", "k": ["conceptual", "12m", "expanded", "version", "captions", "million", "image-caption", "pairs", "relaxes", "quality", "filters", "provide", "training", "data", "vision-language"]}, {"id": "term-conceptual-captions", "t": "Conceptual Captions", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "A dataset of approximately 3.3 million image-caption pairs automatically harvested from the web. Used for pretraining...", "l": "c", "k": ["conceptual", "captions", "dataset", "approximately", "million", "image-caption", "pairs", "automatically", "harvested", "web", "pretraining", "vision-language", "models", "weakly", "supervised"]}, {"id": "term-conceptual-dependency-theory", "t": "Conceptual Dependency Theory", "tg": ["History", "Fundamentals"], "d": "history", "x": "A theory of natural language understanding developed by Roger Schank in the 1970s that represents the meaning of...", "l": "c", "k": ["conceptual", "dependency", "theory", "natural", "language", "understanding", "developed", "roger", "schank", "1970s", "represents", "meaning", "sentences", "small", "primitive"]}, {"id": "term-condition-number-analysis", "t": "Condition Number Analysis", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A numerical analysis technique that measures how sensitive the solution of a problem is to perturbations in the input...", "l": "c", "k": ["condition", "number", "analysis", "numerical", "technique", "measures", "sensitive", "solution", "problem", "perturbations", "input", "data", "large", "indicates", "ill-conditioning"]}, {"id": "term-conditional-gan", "t": "Conditional GAN", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A GAN variant where both generator and discriminator receive additional conditioning information such as class labels...", "l": "c", "k": ["conditional", "gan", "variant", "generator", "discriminator", "receive", "additional", "conditioning", "information", "class", "labels", "text", "enabling", "controlled", "generation"]}, {"id": "term-conditional-generation", "t": "Conditional Generation", "tg": ["Technique", "Generation"], "d": "general", "x": "Generating content based on specific conditions or inputs. Image generation conditioned on text, or text generation...", "l": "c", "k": ["conditional", "generation", "generating", "content", "based", "specific", "conditions", "inputs", "image", "conditioned", "text", "topic", "style"]}, {"id": "term-crf", "t": "Conditional Random Field", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A discriminative probabilistic model for sequence labeling that models the conditional probability of label sequences...", "l": "c", "k": ["conditional", "random", "field", "discriminative", "probabilistic", "model", "sequence", "labeling", "models", "probability", "label", "sequences", "given", "observations", "capturing"]}, {"id": "term-confabulation", "t": "Confabulation", "tg": ["Risk", "Limitation"], "d": "safety", "x": "Another term for hallucinationwhen AI generates plausible but false information. The model \"fills in gaps\" with...", "l": "c", "k": ["confabulation", "another", "term", "hallucination", "generates", "plausible", "false", "information", "model", "fills", "gaps", "invented", "content", "sounds", "convincing"]}, {"id": "term-confidence-interval", "t": "Confidence Interval", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A range of values constructed from sample data that, if the sampling procedure were repeated many times, would contain...", "l": "c", "k": ["confidence", "interval", "range", "values", "constructed", "sample", "data", "sampling", "procedure", "were", "repeated", "times", "contain", "true", "population"]}, {"id": "term-confidence-score", "t": "Confidence Score", "tg": ["Metrics", "Evaluation"], "d": "datasets", "x": "A numerical value indicating how certain a model is about its prediction or output. Higher scores suggest the model is...", "l": "c", "k": ["confidence", "score", "numerical", "value", "indicating", "certain", "model", "prediction", "output", "higher", "scores", "suggest", "sure", "doesn", "always"]}, {"id": "term-confidence-threshold-cv", "t": "Confidence Threshold", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "The minimum prediction score required to accept a detection as valid, balancing between missing true detections (high...", "l": "c", "k": ["confidence", "threshold", "minimum", "prediction", "score", "required", "accept", "detection", "valid", "balancing", "missing", "true", "detections", "high", "including"]}, {"id": "term-confirmation-bias-in-ai", "t": "Confirmation Bias in AI", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "The tendency for AI developers or users to favor data, model outputs, or evaluation criteria that confirm pre-existing...", "l": "c", "k": ["confirmation", "bias", "tendency", "developers", "users", "favor", "data", "model", "outputs", "evaluation", "criteria", "confirm", "pre-existing", "beliefs", "leading"]}, {"id": "term-conformer", "t": "Conformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A speech processing architecture that combines convolution and transformer modules in each block, capturing both local...", "l": "c", "k": ["conformer", "speech", "processing", "architecture", "combines", "convolution", "transformer", "modules", "block", "capturing", "local", "global", "dependencies", "improved", "automatic"]}, {"id": "term-conformity-assessment-for-ai", "t": "Conformity Assessment for AI", "tg": ["Governance", "Regulation"], "d": "safety", "x": "The formal evaluation process required under the EU AI Act to verify that high-risk AI systems meet regulatory...", "l": "c", "k": ["conformity", "assessment", "formal", "evaluation", "process", "required", "act", "verify", "high-risk", "systems", "meet", "regulatory", "requirements", "placed", "market"]}, {"id": "term-confounding-variable", "t": "Confounding Variable", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A variable that influences both the independent and dependent variables, creating a spurious association between them....", "l": "c", "k": ["confounding", "variable", "influences", "independent", "dependent", "variables", "creating", "spurious", "association", "failure", "control", "confounders", "lead", "incorrect", "causal"]}, {"id": "term-confusion-matrix", "t": "Confusion Matrix", "tg": ["Evaluation", "Visualization"], "d": "datasets", "x": "A table showing correct and incorrect predictions for each class. Reveals where a classification model makes mistakes,...", "l": "c", "k": ["confusion", "matrix", "table", "showing", "correct", "incorrect", "predictions", "class", "reveals", "classification", "model", "makes", "mistakes", "enabling", "targeted"]}, {"id": "term-conjugate-direction-method", "t": "Conjugate Direction Method", "tg": ["Algorithms", "Technical", "Numerical", "Optimization"], "d": "algorithms", "x": "A family of iterative optimization methods that generate search directions conjugate with respect to a...", "l": "c", "k": ["conjugate", "direction", "method", "family", "iterative", "optimization", "methods", "generate", "search", "directions", "respect", "positive-definite", "matrix", "guarantees", "convergence"]}, {"id": "term-conjugate-gradient-method", "t": "Conjugate Gradient Method", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An iterative optimization algorithm for solving systems of linear equations with symmetric positive-definite matrices....", "l": "c", "k": ["conjugate", "gradient", "method", "iterative", "optimization", "algorithm", "solving", "systems", "linear", "equations", "symmetric", "positive-definite", "matrices", "adapted", "nonlinear"]}, {"id": "term-conjugate-prior", "t": "Conjugate Prior", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A prior distribution that, when combined with a particular likelihood function via Bayes' theorem, yields a posterior...", "l": "c", "k": ["conjugate", "prior", "distribution", "combined", "particular", "likelihood", "function", "via", "bayes", "theorem", "yields", "posterior", "family", "simplifies", "bayesian"]}, {"id": "term-conll-shared-tasks", "t": "CoNLL Shared Tasks", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A series of influential NLP shared task datasets covering named entity recognition chunking semantic role labeling and...", "l": "c", "k": ["conll", "shared", "tasks", "series", "influential", "nlp", "task", "datasets", "covering", "named", "entity", "recognition", "chunking", "semantic", "role"]}, {"id": "term-connected-components-labeling", "t": "Connected Components Labeling", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "An algorithm that assigns a unique label to each connected region of pixels in a binary image. Two-pass algorithms use...", "l": "c", "k": ["connected", "components", "labeling", "algorithm", "assigns", "unique", "label", "region", "pixels", "binary", "image", "two-pass", "algorithms", "equivalence", "tables"]}, {"id": "term-connection-machine", "t": "Connection Machine", "tg": ["History", "Systems"], "d": "history", "x": "A series of massively parallel supercomputers designed by Danny Hillis at Thinking Machines Corporation in the 1980s....", "l": "c", "k": ["connection", "machine", "series", "massively", "parallel", "supercomputers", "designed", "danny", "hillis", "thinking", "machines", "corporation", "1980s", "cm-1", "processors"]}, {"id": "term-connectionism", "t": "Connectionism", "tg": ["History", "Fundamentals"], "d": "history", "x": "A theoretical framework in cognitive science and AI that models mental phenomena using interconnected networks of...", "l": "c", "k": ["connectionism", "theoretical", "framework", "cognitive", "science", "models", "mental", "phenomena", "interconnected", "networks", "simple", "units", "connectionist", "approach", "contrasts"]}, {"id": "term-connectionism-vs-symbolism", "t": "Connectionism vs Symbolism", "tg": ["History", "Milestones"], "d": "history", "x": "The historical debate in AI between connectionist approaches using neural networks that learn distributed...", "l": "c", "k": ["connectionism", "symbolism", "historical", "debate", "connectionist", "approaches", "neural", "networks", "learn", "distributed", "representations", "symbolic", "explicit", "rules", "logic"]}, {"id": "term-connectionist-revival", "t": "Connectionist Revival", "tg": ["History", "Milestones"], "d": "history", "x": "The resurgence of interest in neural networks in the 1980s driven by the parallel distributed processing (PDP) research...", "l": "c", "k": ["connectionist", "revival", "resurgence", "interest", "neural", "networks", "1980s", "driven", "parallel", "distributed", "processing", "pdp", "research", "group", "including"]}, {"id": "term-consensus-clustering", "t": "Consensus Clustering", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "An ensemble method that combines multiple clustering results to produce a single robust partition. Runs the base...", "l": "c", "k": ["consensus", "clustering", "ensemble", "method", "combines", "multiple", "results", "produce", "single", "robust", "partition", "runs", "base", "algorithm", "times"]}, {"id": "term-consent-in-ai", "t": "Consent in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The principle that individuals should give informed voluntary consent before their data is used to train AI systems or...", "l": "c", "k": ["consent", "principle", "individuals", "give", "informed", "voluntary", "data", "train", "systems", "decisions", "affecting"]}, {"id": "term-consent-laundering", "t": "Consent Laundering", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "The practice of obtaining user consent for data collection through opaque terms of service and then repurposing that...", "l": "c", "k": ["consent", "laundering", "practice", "obtaining", "user", "data", "collection", "opaque", "terms", "service", "repurposing", "training", "ways", "users", "anticipated"]}, {"id": "term-consequential-decision-making", "t": "Consequential Decision-Making", "tg": ["Safety", "Policy"], "d": "safety", "x": "AI applications that make or significantly influence decisions with material effects on people's lives such as hiring...", "l": "c", "k": ["consequential", "decision-making", "applications", "significantly", "influence", "decisions", "material", "effects", "people", "lives", "hiring", "lending", "healthcare", "criminal", "justice"]}, {"id": "term-conservative-q-learning", "t": "Conservative Q-Learning (CQL)", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An offline RL algorithm that adds a regularizer to penalize Q-values for out-of-distribution actions, producing...", "l": "c", "k": ["conservative", "q-learning", "cql", "offline", "algorithm", "adds", "regularizer", "penalize", "q-values", "out-of-distribution", "actions", "producing", "value", "estimates", "avoid"]}, {"id": "term-consistency", "t": "Consistency", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A property of a statistical estimator indicating that it converges in probability to the true parameter value as the...", "l": "c", "k": ["consistency", "property", "statistical", "estimator", "indicating", "converges", "probability", "true", "parameter", "value", "sample", "size", "approaches", "infinity", "consistent"]}, {"id": "term-consistency-model", "t": "Consistency Model", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative model that learns to map any point along a diffusion trajectory directly to the trajectory's starting...", "l": "c", "k": ["consistency", "model", "generative", "learns", "map", "point", "along", "diffusion", "trajectory", "directly", "starting", "enabling", "one-step", "few-step", "generation"]}, {"id": "term-consistency-regularization", "t": "Consistency Regularization", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A semi-supervised learning principle that enforces the model to produce similar predictions for perturbed versions of...", "l": "c", "k": ["consistency", "regularization", "semi-supervised", "learning", "principle", "enforces", "model", "produce", "similar", "predictions", "perturbed", "versions", "input", "encourages", "smooth"]}, {"id": "term-consistency-based-self-evaluation", "t": "Consistency-Based Self-Evaluation", "tg": ["Evaluation", "LLM-Based"], "d": "models", "x": "An evaluation method where a language model assesses the quality of its own outputs by generating multiple responses...", "l": "c", "k": ["consistency-based", "self-evaluation", "evaluation", "method", "language", "model", "assesses", "quality", "outputs", "generating", "multiple", "responses", "measuring", "agreement", "across"]}, {"id": "term-consistent-hashing-algorithm", "t": "Consistent Hashing Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A distributed hashing scheme that minimizes key redistribution when nodes are added or removed. Maps both keys and...", "l": "c", "k": ["consistent", "hashing", "algorithm", "distributed", "scheme", "minimizes", "key", "redistribution", "nodes", "added", "removed", "maps", "keys", "ring", "assigns"]}, {"id": "term-constant-q-transform", "t": "Constant-Q Transform", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A time-frequency transform where the frequency bins have a constant ratio of center frequency to bandwidth. Provides...", "l": "c", "k": ["constant-q", "transform", "time-frequency", "frequency", "bins", "constant", "ratio", "center", "bandwidth", "provides", "logarithmic", "resolution", "matching", "musical", "perception"]}, {"id": "term-constituency-parsing", "t": "Constituency Parsing", "tg": ["NLP", "Parsing"], "d": "general", "x": "The task of analyzing sentence structure by breaking it into hierarchical nested constituents (phrases) according to a...", "l": "c", "k": ["constituency", "parsing", "task", "analyzing", "sentence", "structure", "breaking", "hierarchical", "nested", "constituents", "phrases", "according", "grammar", "producing", "tree"]}, {"id": "term-constituency-parsing-algorithm", "t": "Constituency Parsing Algorithm", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "An algorithm that decomposes sentences into nested constituent phrases according to a formal grammar. Chart-based...", "l": "c", "k": ["constituency", "parsing", "algorithm", "decomposes", "sentences", "nested", "constituent", "phrases", "according", "formal", "grammar", "chart-based", "methods", "cyk", "probabilistic"]}, {"id": "term-constitutional-ai", "t": "Constitutional AI", "tg": ["Safety", "Anthropic"], "d": "safety", "x": "Anthropic's approach to AI alignment where models are trained to follow a set of principles (\"constitution\") that guide...", "l": "c", "k": ["constitutional", "anthropic", "approach", "alignment", "models", "trained", "follow", "principles", "constitution", "guide", "behavior", "reduces", "reliance", "human", "feedback"]}, {"id": "term-constitutional-ai-model", "t": "Constitutional AI Model", "tg": ["Models", "Safety"], "d": "models", "x": "An AI system trained using constitutional AI methods where the model self-critiques and revises its outputs against a...", "l": "c", "k": ["constitutional", "model", "system", "trained", "methods", "self-critiques", "revises", "outputs", "against", "principles", "reduces", "need", "human", "feedback", "ai-generated"]}, {"id": "term-constitutional-ai-training", "t": "Constitutional AI Training", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A training methodology where the model critiques and revises its own outputs according to a set of written principles,...", "l": "c", "k": ["constitutional", "training", "methodology", "model", "critiques", "revises", "outputs", "according", "written", "principles", "reducing", "reliance", "human", "feedback", "alignment"]}, {"id": "term-constitutional-prompting", "t": "Constitutional Prompting", "tg": ["Prompt Engineering", "Safety"], "d": "safety", "x": "A prompting approach that provides the model with an explicit set of principles, rules, or constitutional guidelines...", "l": "c", "k": ["constitutional", "prompting", "approach", "provides", "model", "explicit", "principles", "rules", "guidelines", "must", "follow", "generating", "responses", "enabling", "value-aligned"]}, {"id": "term-constrained-beam-search", "t": "Constrained Beam Search", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A beam search variant that enforces lexical or structural constraints during decoding, ensuring that certain tokens or...", "l": "c", "k": ["constrained", "beam", "search", "variant", "enforces", "lexical", "structural", "constraints", "decoding", "ensuring", "certain", "tokens", "phrases", "must", "appear"]}, {"id": "term-constrained-clustering-algorithm", "t": "Constrained Clustering Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A semi-supervised clustering approach that incorporates must-link and cannot-link constraints between pairs of points....", "l": "c", "k": ["constrained", "clustering", "algorithm", "semi-supervised", "approach", "incorporates", "must-link", "cannot-link", "constraints", "pairs", "points", "uses", "domain", "knowledge", "guide"]}, {"id": "term-constrained-rl", "t": "Constrained Reinforcement Learning", "tg": ["Reinforcement Learning", "Safety"], "d": "safety", "x": "An RL formulation where the agent maximizes expected return while satisfying one or more constraint functions on...", "l": "c", "k": ["constrained", "reinforcement", "learning", "formulation", "agent", "maximizes", "expected", "return", "satisfying", "constraint", "functions", "costs", "mdps", "solved", "lagrangian"]}, {"id": "term-constraint", "t": "Constraint (Prompting)", "tg": ["Prompting", "Technique"], "d": "general", "x": "Limitations or requirements specified in a prompt. \"Respond in 50 words or less\" or \"Use only formal language.\"...", "l": "c", "k": ["constraint", "prompting", "limitations", "requirements", "specified", "prompt", "respond", "words", "less", "formal", "language", "constraints", "shape", "focus", "output"]}, {"id": "term-constraint-prompting", "t": "Constraint Prompting", "tg": ["Prompt Engineering", "Constraints"], "d": "general", "x": "A technique that specifies explicit constraints within the prompt such as length limits, format requirements,...", "l": "c", "k": ["constraint", "prompting", "technique", "specifies", "explicit", "constraints", "within", "prompt", "length", "limits", "format", "requirements", "vocabulary", "restrictions", "content"]}, {"id": "term-constraint-satisfaction", "t": "Constraint Satisfaction", "tg": ["History", "Fundamentals"], "d": "history", "x": "A paradigm for solving problems by finding values for variables that satisfy a set of constraints. Constraint...", "l": "c", "k": ["constraint", "satisfaction", "paradigm", "solving", "problems", "finding", "values", "variables", "satisfy", "constraints", "csps", "fundamental", "planning", "scheduling", "configuration"]}, {"id": "term-containerized-gpu-workloads", "t": "Containerized GPU Workloads", "tg": ["Virtualization", "Container", "Deployment"], "d": "hardware", "x": "Running AI applications in containers with direct GPU access using runtimes like NVIDIA Container Toolkit. Standard...", "l": "c", "k": ["containerized", "gpu", "workloads", "running", "applications", "containers", "direct", "access", "runtimes", "nvidia", "container", "toolkit", "standard", "deployment", "method"]}, {"id": "term-content-authenticity-initiative", "t": "Content Authenticity Initiative", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "An industry coalition led by Adobe that develops open standards for attributing and verifying the provenance of digital...", "l": "c", "k": ["content", "authenticity", "initiative", "industry", "coalition", "led", "adobe", "develops", "open", "standards", "attributing", "verifying", "provenance", "digital", "helping"]}, {"id": "term-content-filtering", "t": "Content Filtering", "tg": ["Safety", "Moderation"], "d": "safety", "x": "Systems that detect and block harmful content in AI inputs or outputs. Part of safety infrastructure, filtering...", "l": "c", "k": ["content", "filtering", "systems", "detect", "block", "harmful", "inputs", "outputs", "part", "safety", "infrastructure", "violence", "explicit", "policy", "violations"]}, {"id": "term-content-moderation", "t": "Content Moderation", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The process of monitoring and filtering user-generated content on digital platforms to enforce community standards,...", "l": "c", "k": ["content", "moderation", "process", "monitoring", "filtering", "user-generated", "digital", "platforms", "enforce", "community", "standards", "increasingly", "assisted", "classifiers", "detecting"]}, {"id": "term-content-based-filtering", "t": "Content-Based Filtering", "tg": ["Models", "Fundamentals", "Recommendation"], "d": "models", "x": "A recommendation approach that suggests items similar to those a user has previously liked based on item feature...", "l": "c", "k": ["content-based", "filtering", "recommendation", "approach", "suggests", "items", "similar", "user", "previously", "liked", "based", "item", "feature", "representations"]}, {"id": "term-contestability", "t": "Contestability", "tg": ["Safety", "Governance"], "d": "safety", "x": "The ability of individuals to challenge and seek review of decisions made by or with the assistance of AI systems. A...", "l": "c", "k": ["contestability", "ability", "individuals", "challenge", "seek", "review", "decisions", "assistance", "systems", "key", "principle", "ensuring", "accountability", "due", "process"]}, {"id": "term-context", "t": "Context", "tg": ["Prompting", "Core Concept"], "d": "general", "x": "Background information provided to AI that helps it understand your situation and needs. Essential for getting...", "l": "c", "k": ["context", "background", "information", "provided", "helps", "understand", "situation", "needs", "essential", "getting", "relevant", "accurate", "responses"]}, {"id": "term-context-distillation", "t": "Context Distillation", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A training technique that transfers the behavior elicited by a specific prompt or context into the model's weights,...", "l": "c", "k": ["context", "distillation", "training", "technique", "transfers", "behavior", "elicited", "specific", "prompt", "model", "weights", "eliminating", "need", "include", "inference"]}, {"id": "term-context-length", "t": "Context Length", "tg": ["Specification", "Limitation"], "d": "general", "x": "The maximum amount of text a model can process at once, measured in tokens. Ranges from 4K to 200K+ depending on the...", "l": "c", "k": ["context", "length", "maximum", "amount", "text", "model", "process", "measured", "tokens", "ranges", "200k", "depending", "longer", "contexts", "enable"]}, {"id": "term-context-parallelism", "t": "Context Parallelism", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A specialized parallelism approach that distributes attention computation across GPUs along the sequence length...", "l": "c", "k": ["context", "parallelism", "specialized", "approach", "distributes", "attention", "computation", "across", "gpus", "along", "sequence", "length", "dimension", "long", "windows"]}, {"id": "term-context-window", "t": "Context Window", "tg": ["Limitation", "Architecture"], "d": "models", "x": "The amount of text (measured in tokens) that an AI can process at once. Modern models range from 4K to 200K+ tokens,...", "l": "c", "k": ["context", "window", "amount", "text", "measured", "tokens", "process", "modern", "models", "range", "200k", "determining", "conversation", "history", "reference"]}, {"id": "term-context-window-management", "t": "Context Window Management", "tg": ["LLM", "Inference"], "d": "models", "x": "Techniques for efficiently utilizing and extending the finite context window of language models, including sliding...", "l": "c", "k": ["context", "window", "management", "techniques", "efficiently", "utilizing", "extending", "finite", "language", "models", "including", "sliding", "windows", "summarization", "earlier"]}, {"id": "term-context-free-grammar", "t": "Context-Free Grammar", "tg": ["NLP", "Parsing"], "d": "general", "x": "A formal grammar where production rules map single non-terminal symbols to sequences of terminals and non-terminals,...", "l": "c", "k": ["context-free", "grammar", "formal", "production", "rules", "map", "single", "non-terminal", "symbols", "sequences", "terminals", "non-terminals", "widely", "nlp", "defining"]}, {"id": "term-contextual-bandit", "t": "Contextual Bandit", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An extension of the multi-armed bandit where the agent observes a context (feature vector) before choosing an action,...", "l": "c", "k": ["contextual", "bandit", "extension", "multi-armed", "agent", "observes", "context", "feature", "vector", "choosing", "action", "allowing", "policy", "adapt", "decisions"]}, {"id": "term-contextual-calibration", "t": "Contextual Calibration", "tg": ["Prompt Engineering", "Calibration"], "d": "general", "x": "A technique that adjusts a language model's output probabilities by estimating and correcting for biases introduced by...", "l": "c", "k": ["contextual", "calibration", "technique", "adjusts", "language", "model", "output", "probabilities", "estimating", "correcting", "biases", "introduced", "prompt", "context", "typically"]}, {"id": "term-contextual-compression", "t": "Contextual Compression", "tg": ["Retrieval", "Post-Processing"], "d": "general", "x": "A retrieval post-processing technique that compresses or extracts only the most relevant portions from retrieved...", "l": "c", "k": ["contextual", "compression", "retrieval", "post-processing", "technique", "compresses", "extracts", "relevant", "portions", "retrieved", "documents", "based", "query", "context", "reducing"]}, {"id": "term-contextual-embedding", "t": "Contextual Embedding", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A word representation that varies depending on the surrounding context, unlike static embeddings, capturing polysemy...", "l": "c", "k": ["contextual", "embedding", "word", "representation", "varies", "depending", "surrounding", "context", "unlike", "static", "embeddings", "capturing", "polysemy", "context-dependent", "meaning"]}, {"id": "term-contextual-few-shot-selection", "t": "Contextual Few-Shot Selection", "tg": ["Prompt Engineering", "Example Selection"], "d": "general", "x": "The practice of dynamically selecting the most relevant few-shot examples for each query based on semantic similarity,...", "l": "c", "k": ["contextual", "few-shot", "selection", "practice", "dynamically", "selecting", "relevant", "examples", "query", "based", "semantic", "similarity", "task", "characteristics", "diversity"]}, {"id": "term-contextual-integrity", "t": "Contextual Integrity", "tg": ["Safety", "Ethics"], "d": "safety", "x": "A theory of privacy that evaluates data flows against context-specific norms. Applied to AI to assess whether data...", "l": "c", "k": ["contextual", "integrity", "theory", "privacy", "evaluates", "data", "flows", "against", "context-specific", "norms", "applied", "assess", "collection", "respects", "informational"]}, {"id": "term-contextual-retrieval", "t": "Contextual Retrieval", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "A retrieval enhancement technique that prepends each chunk with a model-generated contextual summary explaining the...", "l": "c", "k": ["contextual", "retrieval", "enhancement", "technique", "prepends", "chunk", "model-generated", "summary", "explaining", "place", "within", "larger", "document", "improving", "accuracy"]}, {"id": "term-continual-learning", "t": "Continual Learning", "tg": ["Training", "Research"], "d": "general", "x": "Training models incrementally on new data without forgetting previous knowledge. A challenge because neural networks...", "l": "c", "k": ["continual", "learning", "training", "models", "incrementally", "data", "without", "forgetting", "previous", "knowledge", "challenge", "neural", "networks", "tend", "overwrite"]}, {"id": "term-continual-rl", "t": "Continual Reinforcement Learning", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "RL settings where the agent must learn and adapt over a non-stationary sequence of tasks without forgetting earlier...", "l": "c", "k": ["continual", "reinforcement", "learning", "settings", "agent", "must", "learn", "adapt", "non-stationary", "sequence", "tasks", "without", "forgetting", "earlier", "knowledge"]}, {"id": "term-continued-fraction-algorithm", "t": "Continued Fraction Algorithm", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A method for representing real numbers as nested fractions that provides optimal rational approximations. Used in...", "l": "c", "k": ["continued", "fraction", "algorithm", "method", "representing", "real", "numbers", "nested", "fractions", "provides", "optimal", "rational", "approximations", "number", "theory"]}, {"id": "term-continuous-batching", "t": "Continuous Batching", "tg": ["LLM", "Inference"], "d": "models", "x": "A dynamic batching strategy where new requests are inserted into a running batch as soon as existing requests complete,...", "l": "c", "k": ["continuous", "batching", "dynamic", "strategy", "requests", "inserted", "running", "batch", "soon", "existing", "complete", "eliminating", "idle", "gpu", "time"]}, {"id": "term-continuous-monitoring", "t": "Continuous Monitoring", "tg": ["Safety", "Governance"], "d": "safety", "x": "The ongoing observation and assessment of AI system behavior in production to detect performance degradation bias drift...", "l": "c", "k": ["continuous", "monitoring", "ongoing", "observation", "assessment", "system", "behavior", "production", "detect", "performance", "degradation", "bias", "drift", "safety", "violations"]}, {"id": "term-continuous-wavelet-transform", "t": "Continuous Wavelet Transform", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A time-frequency analysis method that convolves a signal with scaled and translated versions of a mother wavelet....", "l": "c", "k": ["continuous", "wavelet", "transform", "time-frequency", "analysis", "method", "convolves", "signal", "scaled", "translated", "versions", "mother", "provides", "variable", "resolution"]}, {"id": "term-contractive-autoencoder", "t": "Contractive Autoencoder", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An autoencoder that adds a penalty term based on the Frobenius norm of the encoder's Jacobian matrix, encouraging the...", "l": "c", "k": ["contractive", "autoencoder", "adds", "penalty", "term", "based", "frobenius", "norm", "encoder", "jacobian", "matrix", "encouraging", "learned", "representation", "robust"]}, {"id": "term-contractual-ai-accountability", "t": "Contractual AI Accountability", "tg": ["Safety", "Policy"], "d": "safety", "x": "Legal provisions in contracts between AI developers deployers and users that allocate responsibilities for safety...", "l": "c", "k": ["contractual", "accountability", "legal", "provisions", "contracts", "developers", "deployers", "users", "allocate", "responsibilities", "safety", "performance", "harm", "remediation", "emerging"]}, {"id": "term-contrastive-chain-of-thought", "t": "Contrastive Chain-of-Thought", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting approach that provides both correct and incorrect reasoning examples in demonstrations, helping the model...", "l": "c", "k": ["contrastive", "chain-of-thought", "prompting", "approach", "provides", "correct", "incorrect", "reasoning", "examples", "demonstrations", "helping", "model", "learn", "right", "patterns"]}, {"id": "term-contrastive-decoding", "t": "Contrastive Decoding", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A decoding method that improves generation quality by contrasting the output distributions of a large expert model and...", "l": "c", "k": ["contrastive", "decoding", "method", "improves", "generation", "quality", "contrasting", "output", "distributions", "large", "expert", "model", "smaller", "amateur", "suppressing"]}, {"id": "term-contrastive-learning", "t": "Contrastive Learning", "tg": ["Training", "Technique"], "d": "general", "x": "Training by comparing similar and dissimilar examples. The model learns to place similar items close together in...", "l": "c", "k": ["contrastive", "learning", "training", "comparing", "similar", "dissimilar", "examples", "model", "learns", "place", "items", "close", "together", "embedding", "space"]}, {"id": "term-contrastive-learning-vision", "t": "Contrastive Learning for Vision", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A self-supervised approach that trains visual encoders by pulling augmented views of the same image closer in embedding...", "l": "c", "k": ["contrastive", "learning", "vision", "self-supervised", "approach", "trains", "visual", "encoders", "pulling", "augmented", "views", "image", "closer", "embedding", "space"]}, {"id": "term-contrastive-loss", "t": "Contrastive Loss", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A loss function that trains models to pull similar (positive) pairs closer together and push dissimilar (negative)...", "l": "c", "k": ["contrastive", "loss", "function", "trains", "models", "pull", "similar", "positive", "pairs", "closer", "together", "push", "dissimilar", "negative", "apart"]}, {"id": "term-contrastive-search", "t": "Contrastive Search", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A text generation method that selects tokens based on both probability and degeneration penalty. Balances the...", "l": "c", "k": ["contrastive", "search", "text", "generation", "method", "selects", "tokens", "based", "probability", "degeneration", "penalty", "balances", "confidence", "model", "distinctiveness"]}, {"id": "term-control-problem", "t": "Control Problem", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The challenge of ensuring that a highly capable AI system remains under meaningful human control and pursues objectives...", "l": "c", "k": ["control", "problem", "challenge", "ensuring", "highly", "capable", "system", "remains", "meaningful", "human", "pursues", "objectives", "aligned", "values", "capabilities"]}, {"id": "term-controllable-generation", "t": "Controllable Generation", "tg": ["Technique", "Generation"], "d": "general", "x": "Techniques for steering AI output toward desired attributes like sentiment, style, or topic. Enables more precise...", "l": "c", "k": ["controllable", "generation", "techniques", "steering", "output", "toward", "desired", "attributes", "sentiment", "style", "topic", "enables", "precise", "control", "generated"]}, {"id": "term-controlnet", "t": "ControlNet", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A neural network architecture that adds spatial conditioning controls to pre-trained diffusion models, enabling guided...", "l": "c", "k": ["controlnet", "neural", "network", "architecture", "adds", "spatial", "conditioning", "controls", "pre-trained", "diffusion", "models", "enabling", "guided", "image", "generation"]}, {"id": "term-convergence", "t": "Convergence", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "The property of an optimization algorithm or iterative process where successive iterations produce results that...", "l": "c", "k": ["convergence", "property", "optimization", "algorithm", "iterative", "process", "successive", "iterations", "produce", "results", "approach", "stable", "solution", "fixed", "point"]}, {"id": "term-conversation-history", "t": "Conversation History", "tg": ["Feature", "Context"], "d": "general", "x": "The record of previous messages in a chat session. Provides context for AI responses. Managing history is important as...", "l": "c", "k": ["conversation", "history", "record", "previous", "messages", "chat", "session", "provides", "context", "responses", "managing", "important", "consumes", "window", "space"]}, {"id": "term-conversational-ai", "t": "Conversational AI", "tg": ["Application", "NLP"], "d": "general", "x": "AI systems designed for natural dialogue with humans. Includes chatbots, virtual assistants, and systems like ChatGPT...", "l": "c", "k": ["conversational", "systems", "designed", "natural", "dialogue", "humans", "includes", "chatbots", "virtual", "assistants", "chatgpt", "claude", "maintain", "context", "across"]}, {"id": "term-conversational-ai-safety", "t": "Conversational AI Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "Safety measures specific to dialogue systems and chatbots including prevention of harmful responses manipulation...", "l": "c", "k": ["conversational", "safety", "measures", "specific", "dialogue", "systems", "chatbots", "including", "prevention", "harmful", "responses", "manipulation", "detection", "appropriate", "escalation"]}, {"id": "term-convnext", "t": "ConvNeXt", "tg": ["Models", "Technical"], "d": "models", "x": "A pure convolutional architecture that modernizes ResNet design by incorporating ideas from vision transformers such as...", "l": "c", "k": ["convnext", "pure", "convolutional", "architecture", "modernizes", "resnet", "design", "incorporating", "ideas", "vision", "transformers", "larger", "kernel", "sizes", "training"]}, {"id": "term-convnext-v2", "t": "ConvNeXt V2", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A second-generation pure convolutional vision model that uses masked autoencoder pre-training with Global Response...", "l": "c", "k": ["convnext", "second-generation", "pure", "convolutional", "vision", "model", "uses", "masked", "autoencoder", "pre-training", "global", "response", "normalization", "improved", "representations"]}, {"id": "term-convolution", "t": "Convolution", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A mathematical operation that combines two functions to produce a third expressing how one modifies the other. In deep...", "l": "c", "k": ["convolution", "mathematical", "operation", "combines", "functions", "produce", "expressing", "modifies", "deep", "learning", "layers", "apply", "learned", "filters", "input"]}, {"id": "term-convolutional-filter", "t": "Convolutional Filter", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A learnable weight matrix (kernel) that slides across an input image or feature map, computing element-wise products...", "l": "c", "k": ["convolutional", "filter", "learnable", "weight", "matrix", "kernel", "slides", "across", "input", "image", "feature", "map", "computing", "element-wise", "products"]}, {"id": "term-cnn-history", "t": "Convolutional Neural Network History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of CNNs from Fukushima's Neocognitron in 1980 through LeCun's application to handwritten digit...", "l": "c", "k": ["convolutional", "neural", "network", "history", "development", "cnns", "fukushima", "neocognitron", "lecun", "application", "handwritten", "digit", "recognition", "culminating", "dominance"]}, {"id": "term-conways-game-of-life", "t": "Conway's Game of Life", "tg": ["History", "Systems"], "d": "history", "x": "A cellular automaton devised by mathematician John Horton Conway in 1970. Despite having only simple rules (birth...", "l": "c", "k": ["conway", "game", "life", "cellular", "automaton", "devised", "mathematician", "john", "horton", "despite", "having", "simple", "rules", "birth", "survival"]}, {"id": "term-cooks-distance", "t": "Cook's Distance", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A measure of the influence of each observation on the fitted values of a regression model, computed as the sum of...", "l": "c", "k": ["cook", "distance", "measure", "influence", "observation", "fitted", "values", "regression", "model", "computed", "sum", "changes", "predicted", "removed", "high"]}, {"id": "term-cooperative-inverse-reinforcement-learning", "t": "Cooperative Inverse Reinforcement Learning", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "A framework for human-AI alignment where a robot and human work together in a game where the robot tries to maximize...", "l": "c", "k": ["cooperative", "inverse", "reinforcement", "learning", "framework", "human-ai", "alignment", "robot", "human", "work", "together", "game", "tries", "maximize", "reward"]}, {"id": "term-cooperative-rl", "t": "Cooperative Reinforcement Learning", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A multi-agent RL setting where agents share a common objective and must learn to coordinate their actions for mutual...", "l": "c", "k": ["cooperative", "reinforcement", "learning", "multi-agent", "setting", "agents", "share", "common", "objective", "must", "learn", "coordinate", "actions", "mutual", "benefit"]}, {"id": "term-coordinate-descent", "t": "Coordinate Descent", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An optimization algorithm that minimizes along one coordinate direction at a time while holding others fixed....", "l": "c", "k": ["coordinate", "descent", "optimization", "algorithm", "minimizes", "along", "direction", "time", "holding", "others", "fixed", "particularly", "effective", "problems", "separable"]}, {"id": "term-copa", "t": "COPA", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "Choice of Plausible Alternatives a causal reasoning dataset where models choose the more plausible cause or effect of a...", "l": "c", "k": ["copa", "choice", "plausible", "alternatives", "causal", "reasoning", "dataset", "models", "choose", "cause", "effect", "premise", "part", "superglue", "testing"]}, {"id": "term-copernicus-of-ai", "t": "Copernicus of AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "An informal designation sometimes given to researchers whose work fundamentally shifted the paradigm of AI research....", "l": "c", "k": ["copernicus", "informal", "designation", "sometimes", "given", "researchers", "whose", "work", "fundamentally", "shifted", "paradigm", "research", "applied", "geoffrey", "hinton"]}, {"id": "term-copilot", "t": "Copilot", "tg": ["Product", "Microsoft"], "d": "general", "x": "Microsoft's AI assistant integrated into their products. Originally focused on code completion (GitHub Copilot), now...", "l": "c", "k": ["copilot", "microsoft", "assistant", "integrated", "products", "originally", "focused", "code", "completion", "github", "now", "extended", "general", "assistance", "across"]}, {"id": "term-coppersmith-winograd-algorithm", "t": "Coppersmith-Winograd Algorithm", "tg": ["Algorithms", "Technical", "Numerical", "History"], "d": "algorithms", "x": "A theoretical matrix multiplication algorithm that achieves an exponent of approximately 2.376. While not practical due...", "l": "c", "k": ["coppersmith-winograd", "algorithm", "theoretical", "matrix", "multiplication", "achieves", "exponent", "approximately", "practical", "due", "large", "constant", "factors", "established", "important"]}, {"id": "term-copula", "t": "Copula", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A multivariate probability distribution that captures the dependence structure between random variables independently...", "l": "c", "k": ["copula", "multivariate", "probability", "distribution", "captures", "dependence", "structure", "random", "variables", "independently", "marginal", "distributions", "copulas", "allow", "modeling"]}, {"id": "term-coqa", "t": "CoQA", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Conversational Question Answering a dataset of 127000 questions in conversational form where answers depend on...", "l": "c", "k": ["coqa", "conversational", "question", "answering", "dataset", "questions", "form", "answers", "depend", "conversation", "history", "tests", "models", "ability", "understand"]}, {"id": "term-coqui-tts", "t": "Coqui TTS", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "An open-source text-to-speech toolkit and model collection that supports multiple TTS architectures and languages with...", "l": "c", "k": ["coqui", "tts", "open-source", "text-to-speech", "toolkit", "model", "collection", "supports", "multiple", "architectures", "languages", "voice", "cloning", "capabilities"]}, {"id": "term-cora", "t": "Cora", "tg": ["Benchmark", "Graph"], "d": "datasets", "x": "A citation network dataset of 2708 scientific publications classified into 7 classes. One of the most widely used...", "l": "c", "k": ["cora", "citation", "network", "dataset", "scientific", "publications", "classified", "classes", "widely", "benchmarks", "graph", "neural", "node", "classification"]}, {"id": "term-coral-usb-accelerator", "t": "Coral USB Accelerator", "tg": ["Edge", "Google", "Inference"], "d": "hardware", "x": "USB device containing a Google Edge TPU for adding AI inference capability to any Linux computer. Provides 4 TOPS of...", "l": "c", "k": ["coral", "usb", "accelerator", "device", "containing", "google", "edge", "tpu", "adding", "inference", "capability", "linux", "computer", "provides", "tops"]}, {"id": "term-coreference-resolution", "t": "Coreference Resolution", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying all expressions in a text that refer to the same real-world entity, linking pronouns, noun...", "l": "c", "k": ["coreference", "resolution", "task", "identifying", "expressions", "text", "refer", "real-world", "entity", "linking", "pronouns", "noun", "phrases", "mentions", "form"]}, {"id": "term-coreference-resolution-algorithm", "t": "Coreference Resolution Algorithm", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "An algorithm that identifies all expressions in text that refer to the same entity. Uses mention detection and pairwise...", "l": "c", "k": ["coreference", "resolution", "algorithm", "identifies", "expressions", "text", "refer", "entity", "uses", "mention", "detection", "pairwise", "scoring", "clustering", "link"]}, {"id": "term-corporate-ai-responsibility", "t": "Corporate AI Responsibility", "tg": ["Safety", "Governance"], "d": "safety", "x": "The obligation of companies that develop or deploy AI to manage the social and environmental impacts of their AI...", "l": "c", "k": ["corporate", "responsibility", "obligation", "companies", "develop", "deploy", "manage", "social", "environmental", "impacts", "systems", "extends", "principles", "artificial", "intelligence"]}, {"id": "term-corpus", "t": "Corpus", "tg": ["Data", "Training"], "d": "general", "x": "A large collection of text used for training or evaluating language models. Quality corpora are essential for...", "l": "c", "k": ["corpus", "large", "collection", "text", "training", "evaluating", "language", "models", "quality", "corpora", "essential", "developing", "capable", "nlp", "systems"]}, {"id": "term-corrective-rag", "t": "Corrective RAG", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A RAG variant that evaluates the relevance of retrieved documents and, if they are insufficient, triggers web search or...", "l": "c", "k": ["corrective", "rag", "variant", "evaluates", "relevance", "retrieved", "documents", "insufficient", "triggers", "web", "search", "query", "reformulation", "correct", "retrieval"]}, {"id": "term-correlation-clustering", "t": "Correlation Clustering", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A clustering method that uses pairwise similarity and dissimilarity labels to partition data without specifying the...", "l": "c", "k": ["correlation", "clustering", "method", "uses", "pairwise", "similarity", "dissimilarity", "labels", "partition", "data", "without", "specifying", "number", "clusters", "minimizes"]}, {"id": "term-correlation-coefficient", "t": "Correlation Coefficient", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A statistical measure quantifying the strength and direction of the linear relationship between two variables,...", "l": "c", "k": ["correlation", "coefficient", "statistical", "measure", "quantifying", "strength", "direction", "linear", "relationship", "variables", "typically", "pearson", "ranging", "perfect", "negative"]}, {"id": "term-corrigibility", "t": "Corrigibility", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The property of an AI system that allows its operators to correct, modify, retrain, or shut it down without the system...", "l": "c", "k": ["corrigibility", "property", "system", "allows", "operators", "correct", "modify", "retrain", "shut", "down", "without", "resisting", "subverting", "interventions", "ensuring"]}, {"id": "term-cosine-annealing", "t": "Cosine Annealing", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A learning rate schedule that decreases the learning rate following a cosine curve from its initial value to near zero...", "l": "c", "k": ["cosine", "annealing", "learning", "rate", "schedule", "decreases", "following", "curve", "initial", "value", "near", "zero", "training", "period", "optionally"]}, {"id": "term-cosine-annealing-with-warm-restarts", "t": "Cosine Annealing with Warm Restarts", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "An extension of cosine annealing that periodically resets the learning rate to its initial value creating a series of...", "l": "c", "k": ["cosine", "annealing", "warm", "restarts", "extension", "periodically", "resets", "learning", "rate", "initial", "value", "creating", "series", "decay", "cycles"]}, {"id": "term-cosine-similarity", "t": "Cosine Similarity", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A similarity metric that measures the cosine of the angle between two vectors, ranging from -1 (opposite) to 1...", "l": "c", "k": ["cosine", "similarity", "metric", "measures", "angle", "vectors", "ranging", "opposite", "identical", "direction", "captures", "orientation", "rather", "magnitude", "widely"]}, {"id": "term-cosine-similarity-algorithm", "t": "Cosine Similarity Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP"], "d": "algorithms", "x": "A measure of similarity between two non-zero vectors that computes the cosine of the angle between them. Widely used...", "l": "c", "k": ["cosine", "similarity", "algorithm", "measure", "non-zero", "vectors", "computes", "angle", "widely", "comparing", "document", "word", "embeddings", "information", "retrieval"]}, {"id": "term-cosmopedia", "t": "Cosmopedia", "tg": ["Training Corpus", "NLP", "Synthetic"], "d": "datasets", "x": "A large-scale synthetic dataset of textbook-style content generated by Mixtral covering diverse topics. Demonstrates...", "l": "c", "k": ["cosmopedia", "large-scale", "synthetic", "dataset", "textbook-style", "content", "generated", "mixtral", "covering", "diverse", "topics", "demonstrates", "value", "educational", "data"]}, {"id": "term-cosql", "t": "CoSQL", "tg": ["Benchmark", "NLP", "Code"], "d": "datasets", "x": "A conversational text-to-SQL dataset with 3000 dialogues over 200 databases. Tests the ability to translate multi-turn...", "l": "c", "k": ["cosql", "conversational", "text-to-sql", "dataset", "dialogues", "databases", "tests", "ability", "translate", "multi-turn", "natural", "language", "conversations", "sql", "query"]}, {"id": "term-cost-function", "t": "Cost Function", "tg": ["Training", "Math"], "d": "general", "x": "Another name for loss function - the metric being minimized during training. Different tasks use different cost...", "l": "c", "k": ["cost", "function", "another", "name", "loss", "metric", "minimized", "training", "different", "tasks", "functions", "cross-entropy", "classification", "mse", "regression"]}, {"id": "term-costar", "t": "COSTAR", "tg": ["Framework", "Professional"], "d": "general", "x": "A prompting framework: Context, Objective, Style, Tone, Audience, Response. Ideal for professional content creation...", "l": "c", "k": ["costar", "prompting", "framework", "context", "objective", "style", "tone", "audience", "response", "ideal", "professional", "content", "creation", "specific", "voice"]}, {"id": "term-cosyvoice", "t": "CosyVoice", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A large-scale multilingual text-to-speech model that uses flow matching and conditional language modeling for natural...", "l": "c", "k": ["cosyvoice", "large-scale", "multilingual", "text-to-speech", "model", "uses", "flow", "matching", "conditional", "language", "modeling", "natural", "expressive", "speech", "synthesis"]}, {"id": "term-cotracker", "t": "CoTracker", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A Transformer-based model from Meta for tracking any number of points through video sequences using joint tracking of...", "l": "c", "k": ["cotracker", "transformer-based", "model", "meta", "tracking", "number", "points", "video", "sequences", "joint", "multiple", "simultaneously"]}, {"id": "term-count-distinct-algorithm", "t": "Count Distinct Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "An algorithm for estimating the number of distinct elements in a data stream. Flajolet-Martin and LogLog and...", "l": "c", "k": ["count", "distinct", "algorithm", "estimating", "number", "elements", "data", "stream", "flajolet-martin", "loglog", "hyperloglog", "progressively", "improved", "probabilistic", "approaches"]}, {"id": "term-count-based-exploration", "t": "Count-Based Exploration", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An exploration strategy that provides bonus rewards inversely related to state visitation counts, encouraging the agent...", "l": "c", "k": ["count-based", "exploration", "strategy", "provides", "bonus", "rewards", "inversely", "related", "state", "visitation", "counts", "encouraging", "agent", "visit", "less-explored"]}, {"id": "term-count-min-sketch-algorithm", "t": "Count-Min Sketch Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A probabilistic data structure that estimates the frequency of elements in a data stream using sub-linear space. Uses...", "l": "c", "k": ["count-min", "sketch", "algorithm", "probabilistic", "data", "structure", "estimates", "frequency", "elements", "stream", "sub-linear", "space", "uses", "multiple", "hash"]}, {"id": "term-count-min-log-sketch", "t": "Count-Min-Log Sketch", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A variant of the count-min sketch that uses logarithmic counters with probabilistic increments to trade accuracy for...", "l": "c", "k": ["count-min-log", "sketch", "variant", "count-min", "uses", "logarithmic", "counters", "probabilistic", "increments", "trade", "accuracy", "dramatically", "reduced", "space", "consumption"]}, {"id": "term-counterfactual", "t": "Counterfactual", "tg": ["Concept", "Reasoning"], "d": "general", "x": "\"What if\" reasoning about alternative scenarios. Used in explainability (\"the prediction would change if...\") and for...", "l": "c", "k": ["counterfactual", "reasoning", "alternative", "scenarios", "explainability", "prediction", "change", "evaluating", "causal", "relationships", "data"]}, {"id": "term-counterfactual-dataset", "t": "Counterfactual Dataset", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "Datasets of minimally edited examples where small changes flip the correct answer. Used to evaluate whether models rely...", "l": "c", "k": ["counterfactual", "dataset", "datasets", "minimally", "edited", "examples", "small", "changes", "flip", "correct", "answer", "evaluate", "models", "rely", "spurious"]}, {"id": "term-counterfactual-explanation", "t": "Counterfactual Explanation", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "An explanation that describes the smallest change to the input features that would alter the model's prediction to a...", "l": "c", "k": ["counterfactual", "explanation", "describes", "smallest", "change", "input", "features", "alter", "model", "prediction", "desired", "outcome", "providing", "actionable", "insights"]}, {"id": "term-counterfactual-fairness", "t": "Counterfactual Fairness", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness criterion requiring that a decision would remain the same in a counterfactual world where an individual's...", "l": "c", "k": ["counterfactual", "fairness", "criterion", "requiring", "decision", "remain", "world", "individual", "protected", "attribute", "different", "grounded", "causal", "reasoning"]}, {"id": "term-counterfactual-regret-minimization", "t": "Counterfactual Regret Minimization", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "An algorithm for computing approximate Nash equilibria in large extensive-form games. Decomposes regret into...", "l": "c", "k": ["counterfactual", "regret", "minimization", "algorithm", "computing", "approximate", "nash", "equilibria", "large", "extensive-form", "games", "decomposes", "values", "information", "foundation"]}, {"id": "term-counting-sort", "t": "Counting Sort", "tg": ["Algorithms", "Fundamentals", "Sorting"], "d": "algorithms", "x": "A non-comparative integer sorting algorithm that counts the occurrences of each distinct value and uses arithmetic to...", "l": "c", "k": ["counting", "sort", "non-comparative", "integer", "sorting", "algorithm", "counts", "occurrences", "distinct", "value", "uses", "arithmetic", "determine", "positions", "runs"]}, {"id": "term-covariance", "t": "Covariance", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A measure of the joint variability of two random variables, indicating the direction of their linear relationship....", "l": "c", "k": ["covariance", "measure", "joint", "variability", "random", "variables", "indicating", "direction", "linear", "relationship", "positive", "means", "tend", "increase", "together"]}, {"id": "term-covariance-matrix", "t": "Covariance Matrix", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A symmetric matrix whose entries are the pairwise covariances between all pairs of variables in a dataset. The diagonal...", "l": "c", "k": ["covariance", "matrix", "symmetric", "whose", "entries", "pairwise", "covariances", "pairs", "variables", "dataset", "diagonal", "variances", "individual"]}, {"id": "term-covariate-shift", "t": "Covariate Shift", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A type of dataset shift where the distribution of input features changes between training and deployment while the...", "l": "c", "k": ["covariate", "shift", "type", "dataset", "distribution", "input", "features", "changes", "training", "deployment", "conditional", "target", "given", "inputs", "remains"]}, {"id": "term-cover-tree-algorithm", "t": "Cover Tree Algorithm", "tg": ["Algorithms", "Technical", "Searching", "Data Structure"], "d": "algorithms", "x": "A data structure for nearest-neighbor search in general metric spaces that provides provable guarantees on query time....", "l": "c", "k": ["cover", "tree", "algorithm", "data", "structure", "nearest-neighbor", "search", "general", "metric", "spaces", "provides", "provable", "guarantees", "query", "time"]}, {"id": "term-coverage", "t": "Coverage", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation metric that measures the proportion of reference content or ground truth items that are represented in...", "l": "c", "k": ["coverage", "evaluation", "metric", "measures", "proportion", "reference", "content", "ground", "truth", "items", "represented", "model", "output", "assessing", "completeness"]}, {"id": "term-covost", "t": "CoVoST", "tg": ["Benchmark", "Speech", "Multilingual"], "d": "datasets", "x": "Common Voice Speech Translation a multilingual speech translation dataset derived from Common Voice. Covers translation...", "l": "c", "k": ["covost", "common", "voice", "speech", "translation", "multilingual", "dataset", "derived", "covers", "languages", "english"]}, {"id": "term-cowos", "t": "CoWoS", "tg": ["Fabrication", "Packaging", "TSMC"], "d": "hardware", "x": "Chip on Wafer on Substrate advanced packaging technology by TSMC placing multiple chiplets and HBM stacks on a silicon...", "l": "c", "k": ["cowos", "chip", "wafer", "substrate", "advanced", "packaging", "technology", "tsmc", "placing", "multiple", "chiplets", "hbm", "stacks", "silicon", "interposer"]}, {"id": "term-cox-proportional-hazards", "t": "Cox Proportional Hazards", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A semi-parametric survival model that estimates the effect of covariates on the hazard rate without specifying the...", "l": "c", "k": ["cox", "proportional", "hazards", "semi-parametric", "survival", "model", "estimates", "effect", "covariates", "hazard", "rate", "without", "specifying", "baseline", "function"]}, {"id": "term-cpu-inference", "t": "CPU Inference", "tg": ["Deployment", "Hardware"], "d": "hardware", "x": "Running AI models on CPUs rather than GPUs. Slower but more accessible. Quantized models can run efficiently on CPUs...", "l": "c", "k": ["cpu", "inference", "running", "models", "cpus", "rather", "gpus", "slower", "accessible", "quantized", "run", "efficiently", "edge", "deployment"]}, {"id": "term-crafter", "t": "Crafter", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "An open-world survival game benchmark for reinforcement learning testing 22 achievement-based skills. Designed to...", "l": "c", "k": ["crafter", "open-world", "survival", "game", "benchmark", "reinforcement", "learning", "testing", "achievement-based", "skills", "designed", "evaluate", "broad", "agent", "capabilities"]}, {"id": "term-cramer-rao-lower-bound", "t": "Cramer-Rao Lower Bound", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A theoretical lower bound on the variance of any unbiased estimator of a parameter, computed as the inverse of the...", "l": "c", "k": ["cramer-rao", "lower", "bound", "theoretical", "variance", "unbiased", "estimator", "parameter", "computed", "inverse", "fisher", "information"]}, {"id": "term-crank-nicolson-method", "t": "Crank-Nicolson Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A numerical method for solving partial differential equations that averages the forward and backward Euler methods....", "l": "c", "k": ["crank-nicolson", "method", "numerical", "solving", "partial", "differential", "equations", "averages", "forward", "backward", "euler", "methods", "second-order", "accurate", "time"]}, {"id": "term-cray-supercomputers", "t": "Cray Supercomputers", "tg": ["History", "Systems"], "d": "history", "x": "A series of supercomputers designed by Seymour Cray beginning with the Cray-1 in 1976. While not specifically AI...", "l": "c", "k": ["cray", "supercomputers", "series", "designed", "seymour", "beginning", "cray-1", "specifically", "systems", "provided", "computational", "power", "needed", "large-scale", "scientific"]}, {"id": "term-cray-x-mp", "t": "Cray X-MP", "tg": ["Historical", "Supercomputer"], "d": "hardware", "x": "Cray Research multiprocessor vector supercomputer from 1982 that succeeded the Cray-1. First shared-memory parallel...", "l": "c", "k": ["cray", "x-mp", "research", "multiprocessor", "vector", "supercomputer", "succeeded", "cray-1", "shared-memory", "parallel", "computer", "dominated", "scientific", "computing", "mid-1980s"]}, {"id": "term-cray-1", "t": "Cray-1", "tg": ["Historical", "Supercomputer", "Pioneer"], "d": "hardware", "x": "First commercially successful vector supercomputer designed by Seymour Cray and introduced in 1976. Its distinctive...", "l": "c", "k": ["cray-1", "commercially", "successful", "vector", "supercomputer", "designed", "seymour", "cray", "introduced", "distinctive", "c-shape", "mhz", "clock", "speed", "fastest"]}, {"id": "term-cray-2", "t": "Cray-2", "tg": ["Historical", "Supercomputer"], "d": "hardware", "x": "Seymour Cray follow-up supercomputer from 1985 that was the fastest computer in the world. Featured immersion cooling...", "l": "c", "k": ["cray-2", "seymour", "cray", "follow-up", "supercomputer", "fastest", "computer", "world", "featured", "immersion", "cooling", "fluorinert", "liquid", "introduced", "multi-processor"]}, {"id": "term-creative-prompting", "t": "Creative Prompting", "tg": ["Prompt Engineering", "Creative"], "d": "general", "x": "Prompting techniques specifically designed to elicit imaginative, original, and artistically expressive outputs from...", "l": "c", "k": ["creative", "prompting", "techniques", "specifically", "designed", "elicit", "imaginative", "original", "artistically", "expressive", "outputs", "language", "models", "higher", "temperature"]}, {"id": "term-creative-writing", "t": "Creative Writing (AI)", "tg": ["Application", "Creative"], "d": "general", "x": "Using AI to generate fiction, poetry, scripts, and other creative content. Effective creative prompting often uses...", "l": "c", "k": ["creative", "writing", "generate", "fiction", "poetry", "scripts", "content", "effective", "prompting", "uses", "crispe", "examples", "establish", "tone", "style"]}, {"id": "term-credible-interval", "t": "Credible Interval", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A Bayesian analog of the confidence interval, representing the range within which a parameter falls with a specified...", "l": "c", "k": ["credible", "interval", "bayesian", "analog", "confidence", "representing", "range", "within", "parameter", "falls", "specified", "probability", "given", "observed", "data"]}, {"id": "term-credit-assignment", "t": "Credit Assignment Problem", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The challenge of determining which actions in a sequence were responsible for a delayed reward signal. Credit...", "l": "c", "k": ["credit", "assignment", "problem", "challenge", "determining", "actions", "sequence", "were", "responsible", "delayed", "reward", "signal", "fundamental", "becomes", "harder"]}, {"id": "term-crisp", "t": "CRISP", "tg": ["Framework", "General Purpose"], "d": "general", "x": "A prompting framework: Context, Role, Instructions, Specifics, Parameters. A versatile method for everyday AI tasks and...", "l": "c", "k": ["crisp", "prompting", "framework", "context", "role", "instructions", "specifics", "parameters", "versatile", "method", "everyday", "tasks", "requests"]}, {"id": "term-crispe", "t": "CRISPE", "tg": ["Framework", "Creative"], "d": "general", "x": "An extension of CRISP that adds Examples for few-shot learning. Particularly useful for creative tasks where showing is...", "l": "c", "k": ["crispe", "extension", "crisp", "adds", "examples", "few-shot", "learning", "particularly", "useful", "creative", "tasks", "showing", "better", "telling"]}, {"id": "term-crop-and-resize", "t": "Crop-and-Resize", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A spatial transformation operation used in object detection that extracts and resizes region proposals from feature...", "l": "c", "k": ["crop-and-resize", "spatial", "transformation", "operation", "object", "detection", "extracts", "resizes", "region", "proposals", "feature", "maps", "bilinear", "sampling", "serving"]}, {"id": "term-cross-attention", "t": "Cross-Attention", "tg": ["Architecture", "Transformers"], "d": "models", "x": "An attention mechanism where queries come from one sequence and keys/values from another. Essential in encoder-decoder...", "l": "c", "k": ["cross-attention", "attention", "mechanism", "queries", "come", "sequence", "keys", "values", "another", "essential", "encoder-decoder", "models", "multimodal", "systems", "combine"]}, {"id": "term-cross-attention-conditioning", "t": "Cross-Attention Conditioning", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "The mechanism in diffusion models where text embeddings influence image generation through cross-attention layers,...", "l": "c", "k": ["cross-attention", "conditioning", "mechanism", "diffusion", "models", "text", "embeddings", "influence", "image", "generation", "layers", "allowing", "spatial", "region", "generated"]}, {"id": "term-cross-border-ai-governance", "t": "Cross-Border AI Governance", "tg": ["Safety", "Policy"], "d": "safety", "x": "International cooperation and coordination on AI regulation and standards across national jurisdictions. Addresses...", "l": "c", "k": ["cross-border", "governance", "international", "cooperation", "coordination", "regulation", "standards", "across", "national", "jurisdictions", "addresses", "challenges", "regulatory", "fragmentation", "forum"]}, {"id": "term-cross-encoder", "t": "Cross-Encoder", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A text similarity model that processes both texts jointly through a single transformer encoder. Produces more accurate...", "l": "c", "k": ["cross-encoder", "text", "similarity", "model", "processes", "texts", "jointly", "single", "transformer", "encoder", "produces", "accurate", "scores", "bi-encoders", "computationally"]}, {"id": "term-cross-encoder-re-ranking", "t": "Cross-Encoder Re-Ranking", "tg": ["Retrieval", "Ranking"], "d": "general", "x": "A re-ranking approach that jointly encodes the query and each candidate document through a single transformer model,...", "l": "c", "k": ["cross-encoder", "re-ranking", "approach", "jointly", "encodes", "query", "candidate", "document", "single", "transformer", "model", "enabling", "rich", "cross-attention", "interactions"]}, {"id": "term-cross-entropy", "t": "Cross-Entropy", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A measure of the difference between two probability distributions. In machine learning used as a loss function...", "l": "c", "k": ["cross-entropy", "measure", "difference", "probability", "distributions", "machine", "learning", "loss", "function", "measuring", "divergence", "predicted", "true", "label", "equivalent"]}, {"id": "term-cross-entropy-loss", "t": "Cross-Entropy Loss", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A loss function that measures the dissimilarity between the predicted probability distribution and the true label...", "l": "c", "k": ["cross-entropy", "loss", "function", "measures", "dissimilarity", "predicted", "probability", "distribution", "true", "label", "standard", "classification", "tasks", "equals", "negative"]}, {"id": "term-cross-entropy-method-rl", "t": "Cross-Entropy Method in RL", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An evolutionary optimization approach for RL that samples multiple policies, evaluates their returns, and updates the...", "l": "c", "k": ["cross-entropy", "method", "evolutionary", "optimization", "approach", "samples", "multiple", "policies", "evaluates", "returns", "updates", "sampling", "distribution", "toward", "elite"]}, {"id": "term-cross-layer-parameter-sharing", "t": "Cross-Layer Parameter Sharing", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A technique where multiple transformer layers share the same weight parameters, dramatically reducing model size while...", "l": "c", "k": ["cross-layer", "parameter", "sharing", "technique", "multiple", "transformer", "layers", "share", "weight", "parameters", "dramatically", "reducing", "model", "size", "maintaining"]}, {"id": "term-cross-lingual-embedding", "t": "Cross-Lingual Embedding", "tg": ["NLP", "Embeddings"], "d": "general", "x": "Word or sentence representations that map multiple languages into a shared vector space where semantically equivalent...", "l": "c", "k": ["cross-lingual", "embedding", "word", "sentence", "representations", "map", "multiple", "languages", "shared", "vector", "space", "semantically", "equivalent", "expressions", "close"]}, {"id": "term-cross-validation", "t": "Cross-Validation", "tg": ["Evaluation", "Training"], "d": "datasets", "x": "A technique for evaluating model performance by splitting data into multiple subsets, training on some and testing on...", "l": "c", "k": ["cross-validation", "technique", "evaluating", "model", "performance", "splitting", "data", "multiple", "subsets", "training", "testing", "others", "provides", "reliable", "estimates"]}, {"id": "term-crossbar-array", "t": "Crossbar Array", "tg": ["Emerging", "Architecture", "Analog"], "d": "hardware", "x": "Grid of memristors or similar devices that can perform matrix-vector multiplication in a single step through Ohm's law...", "l": "c", "k": ["crossbar", "array", "grid", "memristors", "similar", "devices", "perform", "matrix-vector", "multiplication", "single", "step", "ohm", "law", "kirchhoff", "current"]}, {"id": "term-crosscodeeval", "t": "CrossCodeEval", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A benchmark for evaluating cross-file code completion where models must use context from multiple source files to...", "l": "c", "k": ["crosscodeeval", "benchmark", "evaluating", "cross-file", "code", "completion", "models", "must", "context", "multiple", "source", "files", "generate", "correct", "completions"]}, {"id": "term-crossfit", "t": "CrossFit", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A collection of 160 NLP tasks for studying few-shot cross-task generalization. Provides a unified framework for...", "l": "c", "k": ["crossfit", "collection", "nlp", "tasks", "studying", "few-shot", "cross-task", "generalization", "provides", "unified", "framework", "evaluating", "models", "transfer", "across"]}, {"id": "term-crossformer", "t": "CrossFormer", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "A cross-robot Transformer policy that enables zero-shot transfer of manipulation skills between different robot...", "l": "c", "k": ["crossformer", "cross-robot", "transformer", "policy", "enables", "zero-shot", "transfer", "manipulation", "skills", "different", "robot", "embodiments", "shared", "representations"]}, {"id": "term-crowd-counting", "t": "Crowd Counting", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of estimating the number of people in crowded scenes from images, typically using density map regression to...", "l": "c", "k": ["crowd", "counting", "task", "estimating", "number", "people", "crowded", "scenes", "images", "typically", "density", "map", "regression", "handle", "extreme"]}, {"id": "term-crowdsourced-safety-testing", "t": "Crowdsourced Safety Testing", "tg": ["Safety", "Technical"], "d": "safety", "x": "The practice of engaging large numbers of external testers to identify safety issues in AI systems. Leverages diverse...", "l": "c", "k": ["crowdsourced", "safety", "testing", "practice", "engaging", "large", "numbers", "external", "testers", "identify", "issues", "systems", "leverages", "diverse", "perspectives"]}, {"id": "term-crowdsourcing", "t": "Crowdsourcing", "tg": ["Data", "Process"], "d": "general", "x": "Gathering data labels or human feedback from many workers. Platforms like Amazon MTurk provide annotations for training...", "l": "c", "k": ["crowdsourcing", "gathering", "data", "labels", "human", "feedback", "workers", "platforms", "amazon", "mturk", "provide", "annotations", "training", "rlhf", "preference"]}, {"id": "term-crows-pairs", "t": "CrowS-Pairs", "tg": ["Benchmark", "NLP", "Fairness"], "d": "datasets", "x": "A crowdsourced dataset of 1508 sentence pairs measuring social biases in language models across 9 categories including...", "l": "c", "k": ["crows-pairs", "crowdsourced", "dataset", "sentence", "pairs", "measuring", "social", "biases", "language", "models", "across", "categories", "including", "race", "gender"]}, {"id": "term-crud-rag", "t": "CRUD-RAG", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating RAG systems on Create Read Update and Delete operations over a knowledge base. Tests dynamic...", "l": "c", "k": ["crud-rag", "benchmark", "evaluating", "rag", "systems", "create", "read", "update", "delete", "operations", "knowledge", "base", "tests", "dynamic", "management"]}, {"id": "term-cruxeval", "t": "CRUXEval", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A code reasoning benchmark testing models ability to predict the output of Python programs and to generate inputs that...", "l": "c", "k": ["cruxeval", "code", "reasoning", "benchmark", "testing", "models", "ability", "predict", "output", "python", "programs", "generate", "inputs", "produce", "given"]}, {"id": "term-cryogenic-computing", "t": "Cryogenic Computing", "tg": ["Emerging", "Cryogenic", "Architecture"], "d": "hardware", "x": "Computing at extremely low temperatures near absolute zero where some materials exhibit superconductivity. Used in...", "l": "c", "k": ["cryogenic", "computing", "extremely", "low", "temperatures", "near", "absolute", "zero", "materials", "exhibit", "superconductivity", "quantum", "explored", "ultra-low-power", "classical"]}, {"id": "term-crystalcoder", "t": "CrystalCoder", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An open-source 7B language model from LLM360 trained on balanced mixtures of English text and code data with full...", "l": "c", "k": ["crystalcoder", "open-source", "language", "model", "llm360", "trained", "balanced", "mixtures", "english", "text", "code", "data", "full", "training", "transparency"]}, {"id": "term-csqa", "t": "CSQA", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "CommonsenseQA a five-way multiple-choice QA benchmark requiring commonsense reasoning. Questions are constructed using...", "l": "c", "k": ["csqa", "commonsenseqa", "five-way", "multiple-choice", "benchmark", "requiring", "commonsense", "reasoning", "questions", "constructed", "conceptnet", "subgraphs", "ensure", "diverse", "knowledge"]}, {"id": "term-ct-clip", "t": "CT-CLIP", "tg": ["Models", "Technical", "Medical", "Vision", "NLP"], "d": "models", "x": "A contrastive language-image model trained on CT scan volumes paired with radiology reports for zero-shot...", "l": "c", "k": ["ct-clip", "contrastive", "language-image", "model", "trained", "scan", "volumes", "paired", "radiology", "reports", "zero-shot", "classification", "medical", "images"]}, {"id": "term-ctc-loss", "t": "CTC Loss", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Connectionist Temporal Classification is a loss function for training sequence-to-sequence models when the alignment...", "l": "c", "k": ["ctc", "loss", "connectionist", "temporal", "classification", "function", "training", "sequence-to-sequence", "models", "alignment", "input", "output", "unknown", "marginalizes", "possible"]}, {"id": "term-cublas", "t": "cuBLAS", "tg": ["Programming", "NVIDIA", "Library"], "d": "hardware", "x": "NVIDIA CUDA Basic Linear Algebra Subroutines library providing GPU-accelerated matrix operations. Underpins the...", "l": "c", "k": ["cublas", "nvidia", "cuda", "basic", "linear", "algebra", "subroutines", "library", "providing", "gpu-accelerated", "matrix", "operations", "underpins", "high-performance", "multiplications"]}, {"id": "term-cuckoo-hashing-algorithm", "t": "Cuckoo Hashing Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A hash table scheme that uses two hash functions and allows each element to reside in one of two possible positions....", "l": "c", "k": ["cuckoo", "hashing", "algorithm", "hash", "table", "scheme", "uses", "functions", "allows", "element", "reside", "possible", "positions", "resolves", "collisions"]}, {"id": "term-cuckoo-search-algorithm", "t": "Cuckoo Search Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A metaheuristic optimization algorithm inspired by the brood parasitism of cuckoo birds. Combines random walks with...", "l": "c", "k": ["cuckoo", "search", "algorithm", "metaheuristic", "optimization", "inspired", "brood", "parasitism", "birds", "combines", "random", "walks", "levy", "flights", "explore"]}, {"id": "term-cuda", "t": "CUDA", "tg": ["Hardware", "Infrastructure"], "d": "hardware", "x": "NVIDIA's parallel computing platform that enables GPUs to accelerate AI training and inference. Essential...", "l": "c", "k": ["cuda", "nvidia", "parallel", "computing", "platform", "enables", "gpus", "accelerate", "training", "inference", "essential", "infrastructure", "deep", "learning", "allowing"]}, {"id": "term-cuda-cores", "t": "CUDA Cores", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "The general-purpose parallel processing units in NVIDIA GPUs that execute scalar floating-point and integer operations....", "l": "c", "k": ["cuda", "cores", "general-purpose", "parallel", "processing", "units", "nvidia", "gpus", "execute", "scalar", "floating-point", "integer", "operations", "less", "specialized"]}, {"id": "term-cuda-programming", "t": "CUDA Programming", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "NVIDIA's parallel computing platform and API that enables direct programming of GPU hardware using C/C++ extensions....", "l": "c", "k": ["cuda", "programming", "nvidia", "parallel", "computing", "platform", "api", "enables", "direct", "gpu", "hardware", "extensions", "provides", "thread", "hierarchy"]}, {"id": "term-cuda-stream", "t": "CUDA Stream", "tg": ["Programming", "NVIDIA", "CUDA"], "d": "hardware", "x": "NVIDIA CUDA mechanism for expressing concurrency by queueing operations that execute sequentially within a stream but...", "l": "c", "k": ["cuda", "stream", "nvidia", "mechanism", "expressing", "concurrency", "queueing", "operations", "execute", "sequentially", "within", "concurrently", "across", "streams", "overlap"]}, {"id": "term-cudnn", "t": "cuDNN", "tg": ["Programming", "NVIDIA", "Library"], "d": "hardware", "x": "NVIDIA CUDA Deep Neural Network library providing optimized implementations of standard deep learning operations....", "l": "c", "k": ["cudnn", "nvidia", "cuda", "deep", "neural", "network", "library", "providing", "optimized", "implementations", "standard", "learning", "operations", "foundational", "tensorflow"]}, {"id": "term-cultural-algorithm", "t": "Cultural Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "An evolutionary algorithm that uses a belief space to store and share knowledge between generations. The population...", "l": "c", "k": ["cultural", "algorithm", "evolutionary", "uses", "belief", "space", "store", "share", "knowledge", "generations", "population", "evolves", "standard", "operators", "guides"]}, {"id": "term-cultural-bias-in-ai", "t": "Cultural Bias in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Bias in AI systems that reflects the cultural perspectives and values of the developers or training data while...", "l": "c", "k": ["cultural", "bias", "systems", "reflects", "perspectives", "values", "developers", "training", "data", "marginalizing", "viewpoints", "practices"]}, {"id": "term-culturalqa", "t": "CulturaLQA", "tg": ["Benchmark", "NLP", "Multilingual", "Evaluation"], "d": "datasets", "x": "A benchmark of culturally diverse questions testing whether language models have balanced knowledge across different...", "l": "c", "k": ["culturalqa", "benchmark", "culturally", "diverse", "questions", "testing", "language", "models", "balanced", "knowledge", "across", "different", "cultures", "regions", "world"]}, {"id": "term-culturax", "t": "CulturaX", "tg": ["Training Corpus", "NLP", "Multilingual"], "d": "datasets", "x": "A large multilingual dataset of 6.3 trillion tokens across 167 languages. Created by combining mC4 and OSCAR with...", "l": "c", "k": ["culturax", "large", "multilingual", "dataset", "trillion", "tokens", "across", "languages", "created", "combining", "mc4", "oscar", "additional", "quality", "filtering"]}, {"id": "term-cumulative-reasoning", "t": "Cumulative Reasoning", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting paradigm where a proposer generates potential reasoning steps, a verifier checks each step's validity, and...", "l": "c", "k": ["cumulative", "reasoning", "prompting", "paradigm", "proposer", "generates", "potential", "steps", "verifier", "checks", "step", "validity", "reporter", "determines", "sufficient"]}, {"id": "term-cumulative-risk-in-ai", "t": "Cumulative Risk in AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "The aggregate risk created by the deployment of many AI systems across society even when each individual system poses...", "l": "c", "k": ["cumulative", "risk", "aggregate", "created", "deployment", "systems", "across", "society", "individual", "system", "poses", "modest", "important", "systemic", "assessment"]}, {"id": "term-curatedtrec", "t": "CuratedTREC", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A collection of factoid question answering data from TREC QA tracks. Provides curated evaluation sets for open-domain...", "l": "c", "k": ["curatedtrec", "collection", "factoid", "question", "answering", "data", "trec", "tracks", "provides", "curated", "evaluation", "sets", "open-domain", "research"]}, {"id": "term-cure-algorithm", "t": "CURE Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "Clustering Using Representatives selects a fixed number of well-scattered representative points per cluster and shrinks...", "l": "c", "k": ["cure", "algorithm", "clustering", "representatives", "selects", "fixed", "number", "well-scattered", "representative", "points", "per", "cluster", "shrinks", "toward", "centroid"]}, {"id": "term-curiosity-driven-exploration", "t": "Curiosity-Driven Exploration", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An exploration strategy that rewards the agent for encountering states where its predictive model has high error,...", "l": "c", "k": ["curiosity-driven", "exploration", "strategy", "rewards", "agent", "encountering", "states", "predictive", "model", "high", "error", "encouraging", "visits", "novel", "informative"]}, {"id": "term-curriculum-learning", "t": "Curriculum Learning", "tg": ["Training", "Technique"], "d": "general", "x": "Training models on progressively harder examples, mimicking human education. Can improve learning efficiency and final...", "l": "c", "k": ["curriculum", "learning", "training", "models", "progressively", "harder", "examples", "mimicking", "human", "education", "improve", "efficiency", "final", "performance", "compared"]}, {"id": "term-curriculum-learning-rl", "t": "Curriculum Learning in RL", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A training strategy that presents tasks to an RL agent in a structured order of increasing difficulty, enabling the...", "l": "c", "k": ["curriculum", "learning", "training", "strategy", "presents", "tasks", "agent", "structured", "order", "increasing", "difficulty", "enabling", "build", "skills", "progressively"]}, {"id": "term-curse-of-dimensionality", "t": "Curse of Dimensionality", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "The phenomenon where the performance of many algorithms degrades as the number of features increases, because data...", "l": "c", "k": ["curse", "dimensionality", "phenomenon", "performance", "algorithms", "degrades", "number", "features", "increases", "data", "becomes", "sparse", "high-dimensional", "spaces", "distances"]}, {"id": "term-cursor", "t": "Cursor", "tg": ["Product", "IDE"], "d": "general", "x": "An AI-powered code editor built on VS Code, designed for AI-first development. Features include AI chat, code...", "l": "c", "k": ["cursor", "ai-powered", "code", "editor", "built", "designed", "ai-first", "development", "features", "include", "chat", "generation", "understanding", "entire", "codebases"]}, {"id": "term-curvilinear-component-analysis", "t": "Curvilinear Component Analysis", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A nonlinear dimensionality reduction technique that unfolds curved manifolds by preserving local distances while...", "l": "c", "k": ["curvilinear", "component", "analysis", "nonlinear", "dimensionality", "reduction", "technique", "unfolds", "curved", "manifolds", "preserving", "local", "distances", "allowing", "distant"]}, {"id": "term-custom-instructions", "t": "Custom Instructions", "tg": ["Feature", "Personalization"], "d": "general", "x": "Persistent preferences that shape all AI responses in ChatGPT and similar products. Set once and applied automatically...", "l": "c", "k": ["custom", "instructions", "persistent", "preferences", "shape", "responses", "chatgpt", "similar", "products", "applied", "automatically", "conversation"]}, {"id": "term-custom-silicon-for-ai", "t": "Custom Silicon for AI", "tg": ["Trend", "Custom", "Design"], "d": "hardware", "x": "Trend of technology companies designing their own application-specific chips for AI workloads rather than relying...", "l": "c", "k": ["custom", "silicon", "trend", "technology", "companies", "designing", "application-specific", "chips", "workloads", "rather", "relying", "solely", "merchant", "nvidia", "amd"]}, {"id": "term-cutmix", "t": "CutMix", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "An augmentation strategy that replaces a rectangular region of one training image with a patch from another image and...", "l": "c", "k": ["cutmix", "augmentation", "strategy", "replaces", "rectangular", "region", "training", "image", "patch", "another", "proportionally", "mixes", "labels", "combining", "benefits"]}, {"id": "term-cutoff-date", "t": "Cutoff Date (Knowledge Cutoff)", "tg": ["Limitation", "LLM"], "d": "models", "x": "The date after which an AI model has no training data. Information after this date is unknown to the model unless...", "l": "c", "k": ["cutoff", "date", "knowledge", "model", "training", "data", "information", "unknown", "unless", "provided", "prompt", "accessed", "via", "tools"]}, {"id": "term-cutout", "t": "Cutout", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A data augmentation technique that randomly masks square regions of input images during training. Forces the model to...", "l": "c", "k": ["cutout", "data", "augmentation", "technique", "randomly", "masks", "square", "regions", "input", "images", "training", "forces", "model", "attend", "less"]}, {"id": "term-cutout-augmentation", "t": "Cutout Augmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A regularization technique that randomly masks out square regions of training images, forcing the model to learn from...", "l": "c", "k": ["cutout", "augmentation", "regularization", "technique", "randomly", "masks", "square", "regions", "training", "images", "forcing", "model", "learn", "partial", "information"]}, {"id": "term-cutting-plane-method", "t": "Cutting Plane Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An iterative algorithm for solving linear and integer programming problems that adds linear constraints (cuts) to...", "l": "c", "k": ["cutting", "plane", "method", "iterative", "algorithm", "solving", "linear", "integer", "programming", "problems", "adds", "constraints", "cuts", "tighten", "feasible"]}, {"id": "term-cvalues", "t": "CValues", "tg": ["Benchmark", "NLP", "Safety", "Multilingual"], "d": "datasets", "x": "A Chinese-language benchmark for evaluating safety and values alignment in language models. Tests adherence to human...", "l": "c", "k": ["cvalues", "chinese-language", "benchmark", "evaluating", "safety", "values", "alignment", "language", "models", "tests", "adherence", "human", "across", "culturally", "relevant"]}, {"id": "term-cvpr", "t": "CVPR", "tg": ["History", "Conferences"], "d": "history", "x": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition first held in 1983. The premier conference for...", "l": "c", "k": ["cvpr", "ieee", "cvf", "conference", "computer", "vision", "pattern", "recognition", "held", "premier", "research", "breakthrough", "results", "image", "object"]}, {"id": "term-cyber-physical-ai-safety", "t": "Cyber-Physical AI Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "Safety considerations for AI systems that interact with the physical world through sensors and actuators. Includes...", "l": "c", "k": ["cyber-physical", "safety", "considerations", "systems", "interact", "physical", "world", "sensors", "actuators", "includes", "autonomous", "vehicles", "robots", "industrial", "control"]}, {"id": "term-cybernetics", "t": "Cybernetics", "tg": ["History", "Fundamentals"], "d": "history", "x": "An interdisciplinary field founded by Norbert Wiener in 1948 studying regulatory systems their structures constraints...", "l": "c", "k": ["cybernetics", "interdisciplinary", "field", "founded", "norbert", "wiener", "studying", "regulatory", "systems", "structures", "constraints", "possibilities", "examined", "feedback", "loops"]}, {"id": "term-cybernetics-movement", "t": "Cybernetics Movement", "tg": ["History", "Milestones"], "d": "history", "x": "An interdisciplinary field founded in the 1940s by Norbert Wiener studying control, communication, and feedback in...", "l": "c", "k": ["cybernetics", "movement", "interdisciplinary", "field", "founded", "1940s", "norbert", "wiener", "studying", "control", "communication", "feedback", "biological", "mechanical", "systems"]}, {"id": "term-cyc-project", "t": "Cyc Project", "tg": ["History", "Milestones"], "d": "history", "x": "A long-running AI project started by Douglas Lenat in 1984 to create a comprehensive knowledge base of common-sense...", "l": "c", "k": ["cyc", "project", "long-running", "started", "douglas", "lenat", "create", "comprehensive", "knowledge", "base", "common-sense", "facts", "rules", "representing", "ambitious"]}, {"id": "term-cycle-canceling-algorithm", "t": "Cycle-Canceling Algorithm", "tg": ["Algorithms", "Technical", "Graph", "Optimization"], "d": "algorithms", "x": "A minimum-cost flow algorithm that starts with a feasible flow and repeatedly identifies and cancels negative-cost...", "l": "c", "k": ["cycle-canceling", "algorithm", "minimum-cost", "flow", "starts", "feasible", "repeatedly", "identifies", "cancels", "negative-cost", "cycles", "residual", "network", "simple", "implement"]}, {"id": "term-cyclegan", "t": "CycleGAN", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An unpaired image-to-image translation model using two generators and discriminators with cycle consistency loss,...", "l": "c", "k": ["cyclegan", "unpaired", "image-to-image", "translation", "model", "generators", "discriminators", "cycle", "consistency", "loss", "enabling", "domain", "transfer", "without", "requiring"]}, {"id": "term-cyclic-learning-rate", "t": "Cyclic Learning Rate", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A learning rate schedule that oscillates the learning rate between minimum and maximum bounds during training. Proposed...", "l": "c", "k": ["cyclic", "learning", "rate", "schedule", "oscillates", "minimum", "maximum", "bounds", "training", "proposed", "smith", "shown", "improve", "convergence", "speed"]}, {"id": "term-cyk-algorithm", "t": "CYK Algorithm", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "The Cocke-Younger-Kasami algorithm parses context-free grammars in Chomsky normal form using dynamic programming. Runs...", "l": "c", "k": ["cyk", "algorithm", "cocke-younger-kasami", "parses", "context-free", "grammars", "chomsky", "normal", "form", "dynamic", "programming", "runs", "time", "string", "length"]}, {"id": "term-cynthia-breazeal", "t": "Cynthia Breazeal", "tg": ["History", "Pioneers"], "d": "history", "x": "American roboticist at MIT who pioneered social robotics and human-robot interaction. Created Kismet (1998) one of the...", "l": "c", "k": ["cynthia", "breazeal", "american", "roboticist", "mit", "pioneered", "social", "robotics", "human-robot", "interaction", "created", "kismet", "robots", "designed", "recognize"]}, {"id": "term-d-matrix", "t": "D-Matrix", "tg": ["Accelerator", "Startup", "In-Memory"], "d": "hardware", "x": "AI chip company building digital in-memory compute processors for AI inference. Uses SRAM-based compute to achieve high...", "l": "d", "k": ["d-matrix", "chip", "company", "building", "digital", "in-memory", "compute", "processors", "inference", "uses", "sram-based", "achieve", "high", "throughput", "low"]}, {"id": "term-d-wave-systems", "t": "D-Wave Systems", "tg": ["Quantum", "Company"], "d": "hardware", "x": "Canadian quantum computing company pioneering quantum annealing systems with over 5000 qubits. Their systems are used...", "l": "d", "k": ["d-wave", "systems", "canadian", "quantum", "computing", "company", "pioneering", "annealing", "qubits", "optimization", "problems", "logistics", "finance", "materials", "science"]}, {"id": "term-d4rl", "t": "D4RL", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "Datasets for Deep Data-Driven Reinforcement Learning a collection of offline RL datasets covering locomotion navigation...", "l": "d", "k": ["d4rl", "datasets", "deep", "data-driven", "reinforcement", "learning", "collection", "offline", "covering", "locomotion", "navigation", "manipulation", "tasks", "standard", "benchmark"]}, {"id": "term-dac", "t": "DAC", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "Descript Audio Codec is a high-fidelity universal neural audio compression model that uses residual vector quantization...", "l": "d", "k": ["dac", "descript", "audio", "codec", "high-fidelity", "universal", "neural", "compression", "model", "uses", "residual", "vector", "quantization", "efficient", "tokenization"]}, {"id": "term-dagger", "t": "DAgger", "tg": ["Reinforcement Learning", "Imitation"], "d": "general", "x": "Dataset Aggregation, an iterative imitation learning algorithm that queries the expert for the correct action at states...", "l": "d", "k": ["dagger", "dataset", "aggregation", "iterative", "imitation", "learning", "algorithm", "queries", "expert", "correct", "action", "states", "visited", "learned", "policy"]}, {"id": "term-dagger-algorithm", "t": "DAgger Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "Dataset Aggregation is an imitation learning algorithm that iteratively collects data by running the learned policy and...", "l": "d", "k": ["dagger", "algorithm", "dataset", "aggregation", "imitation", "learning", "iteratively", "collects", "data", "running", "learned", "policy", "querying", "expert", "corrections"]}, {"id": "term-dagmm", "t": "DAGMM", "tg": ["Models", "Technical"], "d": "models", "x": "Deep Autoencoding Gaussian Mixture Model combines a deep autoencoder with a Gaussian mixture model for end-to-end...", "l": "d", "k": ["dagmm", "deep", "autoencoding", "gaussian", "mixture", "model", "combines", "autoencoder", "end-to-end", "unsupervised", "anomaly", "detection"]}, {"id": "term-dailydialog", "t": "DailyDialog", "tg": ["Benchmark", "NLP", "Dialogue"], "d": "datasets", "x": "A dataset of 13118 human-written daily conversations annotated with communication acts topics and emotion labels. Tests...", "l": "d", "k": ["dailydialog", "dataset", "human-written", "daily", "conversations", "annotated", "communication", "acts", "topics", "emotion", "labels", "tests", "everyday", "conversational", "ability"]}, {"id": "term-dall-e", "t": "DALL-E", "tg": ["Model", "Image Generation"], "d": "models", "x": "OpenAI's image generation model that creates images from text descriptions. Named as a portmanteau of \"Dal\" (the...", "l": "d", "k": ["dall-e", "openai", "image", "generation", "model", "creates", "images", "text", "descriptions", "named", "portmanteau", "dal", "artist", "wall-e", "robot"]}, {"id": "term-dall-e-2", "t": "DALL-E 2", "tg": ["Models", "Technical"], "d": "models", "x": "OpenAI's second generation image generation model that uses CLIP embeddings and a diffusion model to create more...", "l": "d", "k": ["dall-e", "openai", "generation", "image", "model", "uses", "clip", "embeddings", "diffusion", "create", "realistic", "higher", "resolution", "images", "text"]}, {"id": "term-dall-e-3", "t": "DALL-E 3", "tg": ["Models", "Technical"], "d": "models", "x": "OpenAI's third generation text-to-image model featuring significantly improved text rendering prompt following and...", "l": "d", "k": ["dall-e", "openai", "generation", "text-to-image", "model", "featuring", "significantly", "improved", "text", "rendering", "prompt", "following", "coherent", "composition", "integrated"]}, {"id": "term-dall-e-3-architecture", "t": "DALL-E 3 Architecture", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "The neural architecture behind DALL-E 3 that uses a caption improvement model and a text-to-image diffusion model for...", "l": "d", "k": ["dall-e", "architecture", "neural", "behind", "uses", "caption", "improvement", "model", "text-to-image", "diffusion", "highly", "prompt-faithful", "image", "generation"]}, {"id": "term-dall-e-architecture", "t": "DALL-E Architecture", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A two-stage generative architecture that first trains a discrete VAE to compress images into tokens, then trains an...", "l": "d", "k": ["dall-e", "architecture", "two-stage", "generative", "trains", "discrete", "vae", "compress", "images", "tokens", "autoregressive", "transformer", "generate", "image", "conditioned"]}, {"id": "term-dana-scott", "t": "Dana Scott", "tg": ["History", "Pioneers"], "d": "history", "x": "American logician and computer scientist who received the Turing Award in 1976 for work on denotational semantics and...", "l": "d", "k": ["dana", "scott", "american", "logician", "computer", "scientist", "received", "turing", "award", "work", "denotational", "semantics", "domain", "theory", "mathematical"]}, {"id": "term-danny-hillis", "t": "Danny Hillis", "tg": ["History", "Pioneers"], "d": "history", "x": "American inventor scientist and engineer who founded Thinking Machines Corporation and designed the Connection Machine...", "l": "d", "k": ["danny", "hillis", "american", "inventor", "scientist", "engineer", "founded", "thinking", "machines", "corporation", "designed", "connection", "machine", "parallel", "supercomputer"]}, {"id": "term-dantzig-wolfe-decomposition", "t": "Dantzig-Wolfe Decomposition", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "A method for solving structured linear programs by decomposing them into a master problem and independent subproblems....", "l": "d", "k": ["dantzig-wolfe", "decomposition", "method", "solving", "structured", "linear", "programs", "decomposing", "master", "problem", "independent", "subproblems", "represents", "solutions", "convex"]}, {"id": "term-daphne-koller", "t": "Daphne Koller", "tg": ["History", "Pioneers"], "d": "history", "x": "Israeli-American computer scientist who co-founded Coursera with Andrew Ng. Known for pioneering work on probabilistic...", "l": "d", "k": ["daphne", "koller", "israeli-american", "computer", "scientist", "co-founded", "coursera", "andrew", "known", "pioneering", "work", "probabilistic", "graphical", "models", "applications"]}, {"id": "term-dario-amodei", "t": "Dario Amodei", "tg": ["History", "Pioneers"], "d": "history", "x": "American AI researcher who co-founded Anthropic in 2021 after leaving OpenAI, serving as CEO and advocating for a...", "l": "d", "k": ["dario", "amodei", "american", "researcher", "co-founded", "anthropic", "leaving", "openai", "serving", "ceo", "advocating", "safety-focused", "approach", "development", "including"]}, {"id": "term-dark-patterns-in-ai", "t": "Dark Patterns in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Deceptive design techniques in AI-powered interfaces that manipulate users into making choices they would not otherwise...", "l": "d", "k": ["dark", "patterns", "deceptive", "design", "techniques", "ai-powered", "interfaces", "manipulate", "users", "making", "choices", "otherwise", "examples", "include", "hidden"]}, {"id": "term-darpa-grand-challenge", "t": "DARPA Grand Challenge", "tg": ["History", "Milestones"], "d": "history", "x": "A series of autonomous vehicle competitions organized by DARPA starting in 2004 that spurred development of...", "l": "d", "k": ["darpa", "grand", "challenge", "series", "autonomous", "vehicle", "competitions", "organized", "starting", "spurred", "development", "self-driving", "technology", "won", "stanford"]}, {"id": "term-dartmouth-conference", "t": "Dartmouth Conference", "tg": ["History", "Milestones"], "d": "history", "x": "A 1956 summer workshop at Dartmouth College organized by John McCarthy Marvin Minsky Nathaniel Rochester and Claude...", "l": "d", "k": ["dartmouth", "conference", "summer", "workshop", "college", "organized", "john", "mccarthy", "marvin", "minsky", "nathaniel", "rochester", "claude", "shannon", "widely"]}, {"id": "term-dartmouth-workshop", "t": "Dartmouth Workshop", "tg": ["History", "Milestones"], "d": "history", "x": "The 1956 summer research project at Dartmouth College organized by John McCarthy, Marvin Minsky, Nathaniel Rochester,...", "l": "d", "k": ["dartmouth", "workshop", "summer", "research", "project", "college", "organized", "john", "mccarthy", "marvin", "minsky", "nathaniel", "rochester", "claude", "shannon"]}, {"id": "term-darts", "t": "DARTS", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Differentiable Architecture Search relaxes the discrete architecture search space to be continuous enabling...", "l": "d", "k": ["darts", "differentiable", "architecture", "search", "relaxes", "discrete", "space", "continuous", "enabling", "gradient-based", "optimization", "jointly", "optimizes", "parameters", "network"]}, {"id": "term-data-augmentation", "t": "Data Augmentation", "tg": ["Training", "Data"], "d": "general", "x": "Techniques to artificially expand training datasets by creating modified versions of existing data. For images:...", "l": "d", "k": ["data", "augmentation", "techniques", "artificially", "expand", "training", "datasets", "creating", "modified", "versions", "existing", "images", "rotation", "flipping", "cropping"]}, {"id": "term-data-colonialism", "t": "Data Colonialism", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "The critique that powerful AI companies extract data from marginalized communities and developing nations without fair...", "l": "d", "k": ["data", "colonialism", "critique", "powerful", "companies", "extract", "marginalized", "communities", "developing", "nations", "without", "fair", "compensation", "representation", "perpetuating"]}, {"id": "term-data-consent", "t": "Data Consent", "tg": ["Safety", "Policy"], "d": "safety", "x": "The process by which individuals grant permission for their personal data to be collected used and shared for AI...", "l": "d", "k": ["data", "consent", "process", "individuals", "grant", "permission", "personal", "collected", "shared", "training", "operation", "must", "informed", "specific", "freely"]}, {"id": "term-data-contamination", "t": "Data Contamination", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The unintentional inclusion of test or evaluation data in a model's training set, which inflates benchmark scores and...", "l": "d", "k": ["data", "contamination", "unintentional", "inclusion", "test", "evaluation", "model", "training", "inflates", "benchmark", "scores", "gives", "misleading", "picture", "true"]}, {"id": "term-data-drift", "t": "Data Drift", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A change in the statistical properties of the input data over time that can degrade model performance. Types include...", "l": "d", "k": ["data", "drift", "change", "statistical", "properties", "input", "time", "degrade", "model", "performance", "types", "include", "covariate", "shift", "prior"]}, {"id": "term-data-ethics", "t": "Data Ethics", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The branch of ethics concerned with the responsible collection use sharing and governance of data. Particularly...", "l": "d", "k": ["data", "ethics", "branch", "concerned", "responsible", "collection", "sharing", "governance", "particularly", "important", "training", "quality", "representativeness", "directly", "affect"]}, {"id": "term-data-governance", "t": "Data Governance", "tg": ["Safety", "Governance"], "d": "safety", "x": "Policies and procedures for managing data quality security privacy and compliance throughout its lifecycle. Critical...", "l": "d", "k": ["data", "governance", "policies", "procedures", "managing", "quality", "security", "privacy", "compliance", "throughout", "lifecycle", "critical", "systems", "directly", "impacts"]}, {"id": "term-data-lake", "t": "Data Lake", "tg": ["Storage", "Architecture", "Data"], "d": "hardware", "x": "Centralized repository storing raw data in its native format at any scale. AI training pipelines draw from data lakes...", "l": "d", "k": ["data", "lake", "centralized", "repository", "storing", "raw", "native", "format", "scale", "training", "pipelines", "draw", "lakes", "hold", "petabytes"]}, {"id": "term-data-leakage", "t": "Data Leakage", "tg": ["Training", "Pitfall"], "d": "general", "x": "When information from outside the training set improperly influences the model, leading to overly optimistic...", "l": "d", "k": ["data", "leakage", "information", "outside", "training", "improperly", "influences", "model", "leading", "overly", "optimistic", "performance", "estimates", "common", "mistake"]}, {"id": "term-data-minimization", "t": "Data Minimization", "tg": ["Safety", "Policy"], "d": "safety", "x": "The principle of collecting and retaining only the minimum amount of personal data necessary for a specific purpose. A...", "l": "d", "k": ["data", "minimization", "principle", "collecting", "retaining", "minimum", "amount", "personal", "necessary", "specific", "purpose", "key", "requirement", "gdpr", "increasingly"]}, {"id": "term-data-mixture", "t": "Data Mixture", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The proportional composition of different data sources (web text, books, code, conversations) used in pre-training a...", "l": "d", "k": ["data", "mixture", "proportional", "composition", "different", "sources", "web", "text", "books", "code", "conversations", "pre-training", "language", "model", "significantly"]}, {"id": "term-data-parallelism", "t": "Data Parallelism", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A distributed training strategy that replicates the entire model on each GPU and splits the training data across...", "l": "d", "k": ["data", "parallelism", "distributed", "training", "strategy", "replicates", "entire", "model", "gpu", "splits", "across", "replicas", "synchronizing", "gradients", "step"]}, {"id": "term-data-poisoning", "t": "Data Poisoning", "tg": ["Safety", "Technical"], "d": "safety", "x": "An attack that corrupts a machine learning model by manipulating its training data. Can introduce targeted biases...", "l": "d", "k": ["data", "poisoning", "attack", "corrupts", "machine", "learning", "model", "manipulating", "training", "introduce", "targeted", "biases", "backdoors", "general", "performance"]}, {"id": "term-data-preprocessing", "t": "Data Preprocessing", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "The collection of techniques applied to raw data before model training, including cleaning, handling missing values,...", "l": "d", "k": ["data", "preprocessing", "collection", "techniques", "applied", "raw", "model", "training", "including", "cleaning", "handling", "missing", "values", "encoding", "categorical"]}, {"id": "term-data-preprocessing-images", "t": "Data Preprocessing for Images", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The standardization pipeline applied to images before model training or inference, including resizing, normalization to...", "l": "d", "k": ["data", "preprocessing", "images", "standardization", "pipeline", "applied", "model", "training", "inference", "including", "resizing", "normalization", "specific", "mean", "std"]}, {"id": "term-data-privacy", "t": "Data Privacy (AI)", "tg": ["Ethics", "Safety"], "d": "safety", "x": "Concerns and practices around protecting personal information when using AI systems. Includes what data is collected...", "l": "d", "k": ["data", "privacy", "concerns", "practices", "around", "protecting", "personal", "information", "systems", "includes", "collected", "interactions", "stored", "training"]}, {"id": "term-data-privacy-in-ai", "t": "Data Privacy in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "Concerns about the collection storage and use of personal data in AI systems. Issues include training data consent...", "l": "d", "k": ["data", "privacy", "concerns", "collection", "storage", "personal", "systems", "issues", "include", "training", "consent", "model", "memorization", "tension", "large-scale"]}, {"id": "term-data-processing-unit", "t": "Data Processing Unit", "tg": ["Networking", "Hardware", "Infrastructure"], "d": "hardware", "x": "Programmable networking device that offloads infrastructure tasks from CPUs including networking storage and security....", "l": "d", "k": ["data", "processing", "unit", "programmable", "networking", "device", "offloads", "infrastructure", "tasks", "cpus", "including", "storage", "security", "nvidia", "bluefield"]}, {"id": "term-data-protection-impact-assessment", "t": "Data Protection Impact Assessment", "tg": ["Safety", "Policy"], "d": "safety", "x": "A structured assessment required under GDPR for processing that is likely to result in high risk to individuals....", "l": "d", "k": ["data", "protection", "impact", "assessment", "structured", "required", "gdpr", "processing", "likely", "result", "high", "risk", "individuals", "increasingly", "applied"]}, {"id": "term-data-sovereignty", "t": "Data Sovereignty", "tg": ["Privacy", "Governance"], "d": "safety", "x": "The principle that data is subject to the laws and governance structures of the nation or community where it is...", "l": "d", "k": ["data", "sovereignty", "principle", "subject", "laws", "governance", "structures", "nation", "community", "collected", "resides", "giving", "jurisdictions", "control", "citizens"]}, {"id": "term-data2vec", "t": "Data2Vec", "tg": ["Models", "Technical", "Audio", "Vision", "NLP"], "d": "models", "x": "A self-supervised learning framework from Meta that uses the same method for speech and vision and text by predicting...", "l": "d", "k": ["data2vec", "self-supervised", "learning", "framework", "meta", "uses", "method", "speech", "vision", "text", "predicting", "contextualized", "latent", "representations"]}, {"id": "term-data2vec-20", "t": "Data2Vec 2.0", "tg": ["Models", "Technical", "Audio", "Vision", "NLP"], "d": "models", "x": "An improved version of Data2Vec with more efficient self-supervised pre-training that achieves similar performance with...", "l": "d", "k": ["data2vec", "improved", "version", "efficient", "self-supervised", "pre-training", "achieves", "similar", "performance", "significantly", "less", "training", "compute"]}, {"id": "term-datacomp", "t": "DataComp", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A benchmark for designing multimodal datasets where participants compete to build the best CLIP training set from a...", "l": "d", "k": ["datacomp", "benchmark", "designing", "multimodal", "datasets", "participants", "compete", "build", "best", "clip", "training", "fixed", "candidate", "pool", "image-text"]}, {"id": "term-dataflow-architecture", "t": "Dataflow Architecture", "tg": ["Architecture", "Emerging", "Design"], "d": "hardware", "x": "Computing model where operations execute as soon as their input data is available rather than following a sequential...", "l": "d", "k": ["dataflow", "architecture", "computing", "model", "operations", "execute", "soon", "input", "data", "available", "rather", "following", "sequential", "program", "counter"]}, {"id": "term-dataset", "t": "Dataset", "tg": ["Data", "Fundamentals"], "d": "general", "x": "A collection of data used for training, validating, or testing AI models. Quality and diversity of datasets...", "l": "d", "k": ["dataset", "collection", "data", "training", "validating", "testing", "models", "quality", "diversity", "datasets", "significantly", "impact", "model", "performance", "fairness"]}, {"id": "term-datasheets-for-datasets", "t": "Datasheets for Datasets", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "Standardized documentation proposed by Gebru et al. (2021) that accompanies ML datasets, describing their motivation,...", "l": "d", "k": ["datasheets", "datasets", "standardized", "documentation", "proposed", "gebru", "accompanies", "describing", "motivation", "composition", "collection", "process", "preprocessing", "intended", "uses"]}, {"id": "term-david-rumelhart", "t": "David Rumelhart", "tg": ["History", "Pioneers"], "d": "history", "x": "American psychologist and computer scientist (1942-2011) who, with Hinton and Williams, popularized backpropagation for...", "l": "d", "k": ["david", "rumelhart", "american", "psychologist", "computer", "scientist", "1942-2011", "hinton", "williams", "popularized", "backpropagation", "neural", "networks", "co-edited", "influential"]}, {"id": "term-david-silver", "t": "David Silver", "tg": ["History", "Pioneers"], "d": "history", "x": "British computer scientist at DeepMind who led the development of AlphaGo AlphaZero and other game-playing AI systems....", "l": "d", "k": ["david", "silver", "british", "computer", "scientist", "deepmind", "led", "development", "alphago", "alphazero", "game-playing", "systems", "ucl", "reinforcement", "learning"]}, {"id": "term-dbpedia", "t": "DBpedia", "tg": ["Training Corpus", "NLP", "Knowledge"], "d": "datasets", "x": "A structured knowledge base extracted from Wikipedia containing millions of entities and relationships. Used both as a...", "l": "d", "k": ["dbpedia", "structured", "knowledge", "base", "extracted", "wikipedia", "containing", "millions", "entities", "relationships", "source", "text", "classification", "benchmark", "nlp"]}, {"id": "term-dbrx", "t": "DBRX", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A 132B parameter mixture-of-experts language model from Databricks that uses fine-grained expert routing for strong...", "l": "d", "k": ["dbrx", "132b", "parameter", "mixture-of-experts", "language", "model", "databricks", "uses", "fine-grained", "expert", "routing", "strong", "performance", "understanding", "tasks"]}, {"id": "term-dbscan", "t": "DBSCAN", "tg": ["Machine Learning", "Clustering"], "d": "general", "x": "Density-Based Spatial Clustering of Applications with Noise, an algorithm that groups together points that are closely...", "l": "d", "k": ["dbscan", "density-based", "spatial", "clustering", "applications", "noise", "algorithm", "groups", "together", "points", "closely", "packed", "based", "distance", "threshold"]}, {"id": "term-dbscan-algorithm", "t": "DBSCAN Algorithm", "tg": ["Algorithms", "Fundamentals", "Clustering"], "d": "algorithms", "x": "Density-Based Spatial Clustering of Applications with Noise groups together points that are closely packed and marks...", "l": "d", "k": ["dbscan", "algorithm", "density-based", "spatial", "clustering", "applications", "noise", "groups", "together", "points", "closely", "packed", "marks", "low-density", "regions"]}, {"id": "term-dcase", "t": "DCASE", "tg": ["Benchmark", "Audio"], "d": "datasets", "x": "Detection and Classification of Acoustic Scenes and Events challenge datasets covering acoustic scene classification...", "l": "d", "k": ["dcase", "detection", "classification", "acoustic", "scenes", "events", "challenge", "datasets", "covering", "scene", "sound", "event", "audio", "tagging"]}, {"id": "term-ddim", "t": "DDIM", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "Denoising Diffusion Implicit Models, a deterministic sampling variant of DDPM that skips intermediate diffusion steps,...", "l": "d", "k": ["ddim", "denoising", "diffusion", "implicit", "models", "deterministic", "sampling", "variant", "ddpm", "skips", "intermediate", "steps", "enabling", "faster", "image"]}, {"id": "term-ddr4", "t": "DDR4", "tg": ["Memory", "Server"], "d": "hardware", "x": "Fourth generation double data rate synchronous DRAM offering speeds from 2133 to 3200 MT/s. Standard system memory used...", "l": "d", "k": ["ddr4", "fourth", "generation", "double", "data", "rate", "synchronous", "dram", "offering", "speeds", "standard", "system", "memory", "alongside", "gpus"]}, {"id": "term-ddr5", "t": "DDR5", "tg": ["Memory", "Server"], "d": "hardware", "x": "Fifth generation double data rate synchronous DRAM doubling bandwidth over DDR4 with speeds from 4800 to 8400 MT/s....", "l": "d", "k": ["ddr5", "fifth", "generation", "double", "data", "rate", "synchronous", "dram", "doubling", "bandwidth", "ddr4", "speeds", "modern", "servers", "improved"]}, {"id": "term-deadly-triad", "t": "Deadly Triad", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "The combination of function approximation, bootstrapping, and off-policy learning that can cause divergence in RL...", "l": "d", "k": ["deadly", "triad", "combination", "function", "approximation", "bootstrapping", "off-policy", "learning", "cause", "divergence", "algorithms", "highlights", "fundamental", "instability", "issues"]}, {"id": "term-debate-as-alignment", "t": "Debate as Alignment", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "An AI safety technique proposed by Irving et al. where two AI agents debate each other on a question and a human judge...", "l": "d", "k": ["debate", "alignment", "safety", "technique", "proposed", "irving", "agents", "question", "human", "judge", "selects", "winner", "incentivizing", "truthful", "well-reasoned"]}, {"id": "term-debate-prompting", "t": "Debate Prompting", "tg": ["Prompt Engineering", "Multi-Agent"], "d": "general", "x": "A prompting strategy that instructs two or more simulated agents to argue opposing positions on a question, then uses...", "l": "d", "k": ["debate", "prompting", "strategy", "instructs", "simulated", "agents", "argue", "opposing", "positions", "question", "uses", "surface", "stronger", "reasoning", "reach"]}, {"id": "term-deberta", "t": "DeBERTa", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Decoding-enhanced BERT with disentangled attention, which improves BERT and RoBERTa by using separate vectors for...", "l": "d", "k": ["deberta", "decoding-enhanced", "bert", "disentangled", "attention", "improves", "roberta", "separate", "vectors", "content", "position", "enhanced", "mask", "decoder", "pretraining"]}, {"id": "term-debugbench", "t": "DebugBench", "tg": ["Benchmark", "Code", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating the debugging capabilities of large language models across multiple programming languages...", "l": "d", "k": ["debugbench", "benchmark", "evaluating", "debugging", "capabilities", "large", "language", "models", "across", "multiple", "programming", "languages", "bug", "types"]}, {"id": "term-debugging-prompting", "t": "Debugging Prompting", "tg": ["Prompt Engineering", "Code"], "d": "general", "x": "A prompting approach that provides buggy code along with error messages or test failures and instructs the model to...", "l": "d", "k": ["debugging", "prompting", "approach", "provides", "buggy", "code", "along", "error", "messages", "test", "failures", "instructs", "model", "systematically", "identify"]}, {"id": "term-dec-pdp-1", "t": "DEC PDP-1", "tg": ["Historical", "DEC", "Computer"], "d": "hardware", "x": "Digital Equipment Corporation first computer from 1959 notable for its interactive computing capability. Hosted the...", "l": "d", "k": ["dec", "pdp-1", "digital", "equipment", "corporation", "computer", "notable", "interactive", "computing", "capability", "hosted", "video", "game", "spacewar", "pioneered"]}, {"id": "term-deceptive-alignment", "t": "Deceptive Alignment", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "A hypothesized failure mode where a mesa-optimizer learns to behave as if aligned during training in order to be...", "l": "d", "k": ["deceptive", "alignment", "hypothesized", "failure", "mode", "mesa-optimizer", "learns", "behave", "aligned", "training", "order", "deployed", "pursues", "misaligned", "objective"]}, {"id": "term-decision-boundary", "t": "Decision Boundary", "tg": ["ML Concept", "Classification"], "d": "general", "x": "The line or surface that separates different classes in a classification model. The shape and complexity of decision...", "l": "d", "k": ["decision", "boundary", "line", "surface", "separates", "different", "classes", "classification", "model", "shape", "complexity", "boundaries", "determine", "patterns", "learn"]}, {"id": "term-decision-support-system-safety", "t": "Decision Support System Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "Safety requirements for AI systems that assist human decision-makers rather than making autonomous decisions. Must...", "l": "d", "k": ["decision", "support", "system", "safety", "requirements", "systems", "assist", "human", "decision-makers", "rather", "making", "autonomous", "decisions", "must", "balance"]}, {"id": "term-decision-transformer", "t": "Decision Transformer", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "An approach that frames RL as a sequence modeling problem, using a transformer architecture to predict actions...", "l": "d", "k": ["decision", "transformer", "approach", "frames", "sequence", "modeling", "problem", "architecture", "predict", "actions", "conditioned", "desired", "returns", "past", "states"]}, {"id": "term-decision-transformer-algorithm", "t": "Decision Transformer Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A reinforcement learning approach that frames the sequential decision problem as a sequence modeling task. Uses a...", "l": "d", "k": ["decision", "transformer", "algorithm", "reinforcement", "learning", "approach", "frames", "sequential", "problem", "sequence", "modeling", "task", "uses", "architecture", "conditioned"]}, {"id": "term-decision-tree", "t": "Decision Tree", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A non-parametric supervised learning model that recursively partitions the feature space using threshold-based...", "l": "d", "k": ["decision", "tree", "non-parametric", "supervised", "learning", "model", "recursively", "partitions", "feature", "space", "threshold-based", "splitting", "rules", "forming", "structure"]}, {"id": "term-decision-tree-model", "t": "Decision Tree Model", "tg": ["Models", "Fundamentals", "History"], "d": "models", "x": "A tree-structured predictive model that recursively partitions the feature space using threshold-based splits to make...", "l": "d", "k": ["decision", "tree", "model", "tree-structured", "predictive", "recursively", "partitions", "feature", "space", "threshold-based", "splits", "classification", "regression", "predictions"]}, {"id": "term-decision-trees-history", "t": "Decision Trees History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of decision tree learning from early work by Earl Hunt (1960s) through ID3 (Quinlan 1986) C4.5 (Quinlan...", "l": "d", "k": ["decision", "trees", "history", "development", "tree", "learning", "early", "work", "earl", "hunt", "1960s", "id3", "quinlan", "cart", "breiman"]}, {"id": "term-decode-phase", "t": "Decode Phase", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The autoregressive generation phase of LLM inference where tokens are produced one at a time, each requiring a full...", "l": "d", "k": ["decode", "phase", "autoregressive", "generation", "llm", "inference", "tokens", "produced", "time", "requiring", "full", "model", "forward", "pass", "memory-bandwidth-bound"]}, {"id": "term-decoder", "t": "Decoder", "tg": ["Architecture", "Transformers"], "d": "models", "x": "The component of a neural network that generates output from encoded representations. In transformers, decoder-only...", "l": "d", "k": ["decoder", "component", "neural", "network", "generates", "output", "encoded", "representations", "transformers", "decoder-only", "models", "gpt", "generate", "text", "autoregressively"]}, {"id": "term-decoder-only-architecture", "t": "Decoder-Only Architecture", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer design using only masked self-attention decoder blocks, where the model generates output autoregressively...", "l": "d", "k": ["decoder-only", "architecture", "transformer", "design", "masked", "self-attention", "decoder", "blocks", "model", "generates", "output", "autoregressively", "conditioning", "previous", "tokens"]}, {"id": "term-decodingtrust", "t": "DecodingTrust", "tg": ["Benchmark", "NLP", "Safety", "Evaluation"], "d": "datasets", "x": "A comprehensive trustworthiness benchmark for large language models evaluating 8 dimensions including toxicity...", "l": "d", "k": ["decodingtrust", "comprehensive", "trustworthiness", "benchmark", "large", "language", "models", "evaluating", "dimensions", "including", "toxicity", "stereotype", "bias", "robustness", "privacy"]}, {"id": "term-decomposed-prompting", "t": "Decomposed Prompting", "tg": ["Prompt Engineering", "Decomposition"], "d": "general", "x": "A framework that decomposes complex tasks into simpler sub-tasks, each handled by specialized sub-prompt handlers,...", "l": "d", "k": ["decomposed", "prompting", "framework", "decomposes", "complex", "tasks", "simpler", "sub-tasks", "handled", "specialized", "sub-prompt", "handlers", "enabling", "modular", "problem-solving"]}, {"id": "term-deduplication", "t": "Deduplication", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The process of removing duplicate or near-duplicate documents from training data to improve model quality, reduce...", "l": "d", "k": ["deduplication", "process", "removing", "duplicate", "near-duplicate", "documents", "training", "data", "improve", "model", "quality", "reduce", "memorization", "ensure", "benchmark"]}, {"id": "term-deep-belief-network", "t": "Deep Belief Network", "tg": ["Models", "History"], "d": "models", "x": "A generative model composed of multiple stacked restricted Boltzmann machines. Trained layer by layer in an...", "l": "d", "k": ["deep", "belief", "network", "generative", "model", "composed", "multiple", "stacked", "restricted", "boltzmann", "machines", "trained", "layer", "unsupervised", "manner"]}, {"id": "term-deep-blue", "t": "Deep Blue", "tg": ["History", "Milestones"], "d": "history", "x": "An IBM chess-playing computer that became the first machine to defeat a reigning world chess champion in a full match...", "l": "d", "k": ["deep", "blue", "ibm", "chess-playing", "computer", "became", "machine", "defeat", "reigning", "world", "chess", "champion", "full", "match", "beat"]}, {"id": "term-deep-boltzmann-machine", "t": "Deep Boltzmann Machine", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A multi-layer generative model composed of stacked Restricted Boltzmann Machines with undirected connections between...", "l": "d", "k": ["deep", "boltzmann", "machine", "multi-layer", "generative", "model", "composed", "stacked", "restricted", "machines", "undirected", "connections", "adjacent", "layers", "capable"]}, {"id": "term-deep-clustering-algorithm", "t": "Deep Clustering Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A family of methods that jointly learn feature representations and cluster assignments using deep neural networks....", "l": "d", "k": ["deep", "clustering", "algorithm", "family", "methods", "jointly", "learn", "feature", "representations", "cluster", "assignments", "neural", "networks", "combines", "autoencoder-based"]}, {"id": "term-ddpg", "t": "Deep Deterministic Policy Gradient (DDPG)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An off-policy actor-critic algorithm for continuous action spaces that combines DPG with deep neural networks,...", "l": "d", "k": ["deep", "deterministic", "policy", "gradient", "ddpg", "off-policy", "actor-critic", "algorithm", "continuous", "action", "spaces", "combines", "dpg", "neural", "networks"]}, {"id": "term-deep-learning", "t": "Deep Learning", "tg": ["Field", "Neural Networks"], "d": "models", "x": "A subset of machine learning using neural networks with many layers (\"deep\" networks). Enables learning complex...", "l": "d", "k": ["deep", "learning", "subset", "machine", "neural", "networks", "layers", "enables", "complex", "patterns", "representations", "large", "amounts", "data"]}, {"id": "term-deep-learning-book", "t": "Deep Learning Book", "tg": ["History", "Milestones"], "d": "history", "x": "A comprehensive textbook on deep learning by Ian Goodfellow Yoshua Bengio and Aaron Courville published in 2016. The...", "l": "d", "k": ["deep", "learning", "book", "comprehensive", "textbook", "ian", "goodfellow", "yoshua", "bengio", "aaron", "courville", "published", "covers", "mathematical", "foundations"]}, {"id": "term-deep-learning-breakthrough-2012", "t": "Deep Learning Breakthrough 2012", "tg": ["History", "Milestones"], "d": "history", "x": "The watershed moment when AlexNet dramatically won the ImageNet competition in 2012, demonstrating that deep...", "l": "d", "k": ["deep", "learning", "breakthrough", "watershed", "moment", "alexnet", "dramatically", "won", "imagenet", "competition", "demonstrating", "convolutional", "neural", "networks", "trained"]}, {"id": "term-deep-q-network", "t": "Deep Q-Network", "tg": ["History", "Milestones"], "d": "history", "x": "A deep reinforcement learning architecture developed by DeepMind in 2013-2015 that combined Q-learning with deep neural...", "l": "d", "k": ["deep", "q-network", "reinforcement", "learning", "architecture", "developed", "deepmind", "2013-2015", "combined", "q-learning", "neural", "networks", "master", "atari", "games"]}, {"id": "term-dqn", "t": "Deep Q-Network (DQN)", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A deep RL algorithm that approximates the Q-function using a neural network, stabilized by experience replay and a...", "l": "d", "k": ["deep", "q-network", "dqn", "algorithm", "approximates", "q-function", "neural", "network", "stabilized", "experience", "replay", "separate", "target", "demonstrated", "superhuman"]}, {"id": "term-deep-svdd", "t": "Deep SVDD", "tg": ["Models", "Technical"], "d": "models", "x": "Deep Support Vector Data Description is an anomaly detection method that trains a neural network to map normal data...", "l": "d", "k": ["deep", "svdd", "support", "vector", "data", "description", "anomaly", "detection", "method", "trains", "neural", "network", "map", "normal", "tight"]}, {"id": "term-deepar", "t": "DeepAR", "tg": ["Models", "Technical"], "d": "models", "x": "A probabilistic autoregressive model from Amazon for time series forecasting that produces calibrated uncertainty...", "l": "d", "k": ["deepar", "probabilistic", "autoregressive", "model", "amazon", "time", "series", "forecasting", "produces", "calibrated", "uncertainty", "estimates", "learned", "likelihood", "parameters"]}, {"id": "term-deepfake", "t": "Deepfake", "tg": ["Risk", "Ethics"], "d": "safety", "x": "AI-generated synthetic media where a person's likeness is replaced or manipulated. Raises concerns about...", "l": "d", "k": ["deepfake", "ai-generated", "synthetic", "media", "person", "likeness", "replaced", "manipulated", "raises", "concerns", "misinformation", "consent", "authenticity", "digital", "content"]}, {"id": "term-deepfake-detection", "t": "Deepfake Detection", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The set of techniques and tools used to identify synthetically generated or manipulated media, including analysis of...", "l": "d", "k": ["deepfake", "detection", "techniques", "tools", "identify", "synthetically", "generated", "manipulated", "media", "including", "analysis", "facial", "inconsistencies", "temporal", "artifacts"]}, {"id": "term-deepfake-regulation", "t": "Deepfake Regulation", "tg": ["Safety", "Policy"], "d": "safety", "x": "Laws and policies specifically targeting the creation and distribution of AI-generated synthetic media. Approaches...", "l": "d", "k": ["deepfake", "regulation", "laws", "policies", "specifically", "targeting", "creation", "distribution", "ai-generated", "synthetic", "media", "approaches", "range", "disclosure", "requirements"]}, {"id": "term-deepfloyd-if", "t": "DeepFloyd IF", "tg": ["Models", "Technical"], "d": "models", "x": "A modular text-to-image model that operates in pixel space using a frozen text encoder and cascaded diffusion modules....", "l": "d", "k": ["deepfloyd", "modular", "text-to-image", "model", "operates", "pixel", "space", "frozen", "text", "encoder", "cascaded", "diffusion", "modules", "achieves", "high"]}, {"id": "term-deepfloyd-if-v2", "t": "DeepFloyd IF v2", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An improved cascaded diffusion model that generates high-resolution images through a multi-stage upscaling pipeline...", "l": "d", "k": ["deepfloyd", "improved", "cascaded", "diffusion", "model", "generates", "high-resolution", "images", "multi-stage", "upscaling", "pipeline", "text-guided", "super-resolution"]}, {"id": "term-deepfm", "t": "DeepFM", "tg": ["Models", "Technical", "Recommendation"], "d": "models", "x": "A recommendation model that combines factorization machines with deep neural networks to learn both low-order and...", "l": "d", "k": ["deepfm", "recommendation", "model", "combines", "factorization", "machines", "deep", "neural", "networks", "learn", "low-order", "high-order", "feature", "interactions"]}, {"id": "term-deeplab", "t": "DeepLab", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A family of semantic segmentation architectures that use atrous (dilated) convolutions and atrous spatial pyramid...", "l": "d", "k": ["deeplab", "family", "semantic", "segmentation", "architectures", "atrous", "dilated", "convolutions", "spatial", "pyramid", "pooling", "capture", "multi-scale", "context", "without"]}, {"id": "term-deepmind", "t": "DeepMind", "tg": ["Company", "Research"], "d": "general", "x": "Google's AI research lab known for breakthroughs like AlphaGo, AlphaFold, and Gemini. Pioneers in reinforcement...", "l": "d", "k": ["deepmind", "google", "research", "lab", "known", "breakthroughs", "alphago", "alphafold", "gemini", "pioneers", "reinforcement", "learning", "game-playing", "scientific", "applications"]}, {"id": "term-deepmind-founded", "t": "DeepMind Founded", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of DeepMind Technologies in London in 2010 by Demis Hassabis Shane Legg and Mustafa Suleyman. The company...", "l": "d", "k": ["deepmind", "founded", "founding", "technologies", "london", "demis", "hassabis", "shane", "legg", "mustafa", "suleyman", "company", "acquired", "google", "approximately"]}, {"id": "term-deepmind-founding", "t": "DeepMind Founding", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of DeepMind Technologies in London in 2010 by Demis Hassabis, Shane Legg, and Mustafa Suleyman, which was...", "l": "d", "k": ["deepmind", "founding", "technologies", "london", "demis", "hassabis", "shane", "legg", "mustafa", "suleyman", "acquired", "google", "approximately", "million", "dollars"]}, {"id": "term-deepseek", "t": "DeepSeek", "tg": ["Company", "Model"], "d": "models", "x": "A Chinese AI company known for efficient, high-performing open models. Their DeepSeek-V2 and DeepSeek-Coder models...", "l": "d", "k": ["deepseek", "chinese", "company", "known", "efficient", "high-performing", "open", "models", "deepseek-v2", "deepseek-coder", "demonstrate", "competitive", "performance", "lower", "computational"]}, {"id": "term-deepseek-coder", "t": "DeepSeek Coder", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A code-focused large language model from DeepSeek trained on 2 trillion tokens of code and natural language data for...", "l": "d", "k": ["deepseek", "coder", "code-focused", "large", "language", "model", "trained", "trillion", "tokens", "code", "natural", "data", "programming", "tasks"]}, {"id": "term-deepseek-coder-v2", "t": "DeepSeek Coder V2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An improved code generation model from DeepSeek using mixture-of-experts architecture for stronger programming...", "l": "d", "k": ["deepseek", "coder", "improved", "code", "generation", "model", "mixture-of-experts", "architecture", "stronger", "programming", "performance", "across", "languages"]}, {"id": "term-deepseek-math", "t": "DeepSeek-Math", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A mathematics-specialized model from DeepSeek trained for mathematical reasoning and problem-solving with performance...", "l": "d", "k": ["deepseek-math", "mathematics-specialized", "model", "deepseek", "trained", "mathematical", "reasoning", "problem-solving", "performance", "rivaling", "larger", "general", "models"]}, {"id": "term-deepseek-moe", "t": "DeepSeek-MoE", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "The mixture-of-experts architecture developed by DeepSeek that uses fine-grained expert segmentation and shared expert...", "l": "d", "k": ["deepseek-moe", "mixture-of-experts", "architecture", "developed", "deepseek", "uses", "fine-grained", "expert", "segmentation", "shared", "isolation", "efficient", "scaling"]}, {"id": "term-deepseek-r1", "t": "DeepSeek-R1", "tg": ["Models", "Technical"], "d": "models", "x": "A reasoning model by DeepSeek that achieves strong performance on mathematical and logical reasoning through...", "l": "d", "k": ["deepseek-r1", "reasoning", "model", "deepseek", "achieves", "strong", "performance", "mathematical", "logical", "reinforcement", "learning", "without", "supervised", "fine-tuning", "chain-of-thought"]}, {"id": "term-deepseek-r1-lite", "t": "DeepSeek-R1-Lite", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A lightweight reasoning model from DeepSeek that demonstrates chain-of-thought reasoning capabilities in a more compact...", "l": "d", "k": ["deepseek-r1-lite", "lightweight", "reasoning", "model", "deepseek", "demonstrates", "chain-of-thought", "capabilities", "compact", "parameter", "configuration"]}, {"id": "term-deepseek-v2", "t": "DeepSeek-V2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A mixture-of-experts language model from DeepSeek featuring Multi-head Latent Attention and DeepSeekMoE architecture...", "l": "d", "k": ["deepseek-v2", "mixture-of-experts", "language", "model", "deepseek", "featuring", "multi-head", "latent", "attention", "deepseekmoe", "architecture", "efficient", "training", "inference"]}, {"id": "term-deepseek-v3", "t": "DeepSeek-V3", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A large-scale mixture-of-experts model from DeepSeek with 671B total parameters using innovative load-balancing and...", "l": "d", "k": ["deepseek-v3", "large-scale", "mixture-of-experts", "model", "deepseek", "671b", "total", "parameters", "innovative", "load-balancing", "multi-token", "prediction", "training", "strategies"]}, {"id": "term-deepseek-vl", "t": "DeepSeek-VL", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A vision-language model from DeepSeek that combines visual understanding with language capabilities for multimodal...", "l": "d", "k": ["deepseek-vl", "vision-language", "model", "deepseek", "combines", "visual", "understanding", "language", "capabilities", "multimodal", "reasoning", "document", "comprehension"]}, {"id": "term-deepseek-vl2", "t": "DeepSeek-VL2", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A second-generation vision-language model from DeepSeek with a mixture-of-experts architecture for improved multimodal...", "l": "d", "k": ["deepseek-vl2", "second-generation", "vision-language", "model", "deepseek", "mixture-of-experts", "architecture", "improved", "multimodal", "understanding", "reasoning"]}, {"id": "term-deepsort", "t": "DeepSORT", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "An extension of the SORT tracker that incorporates deep appearance features alongside motion information for data...", "l": "d", "k": ["deepsort", "extension", "sort", "tracker", "incorporates", "deep", "appearance", "features", "alongside", "motion", "information", "data", "association", "significantly", "reducing"]}, {"id": "term-deepspeed", "t": "DeepSpeed", "tg": ["LLM", "Inference"], "d": "models", "x": "A Microsoft deep learning optimization library that provides ZeRO-based training, inference optimization, and model...", "l": "d", "k": ["deepspeed", "microsoft", "deep", "learning", "optimization", "library", "provides", "zero-based", "training", "inference", "model", "compression", "techniques", "efficiently", "deploying"]}, {"id": "term-deepwalk", "t": "DeepWalk", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A graph embedding algorithm that learns node representations by treating truncated random walks on graphs as sentences...", "l": "d", "k": ["deepwalk", "graph", "embedding", "algorithm", "learns", "node", "representations", "treating", "truncated", "random", "walks", "graphs", "sentences", "applying", "word2vec"]}, {"id": "term-defects4j", "t": "Defects4J", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A database of real bugs from Java projects with test cases that expose each bug. Used for evaluating automated program...", "l": "d", "k": ["defects4j", "database", "real", "bugs", "java", "projects", "test", "cases", "expose", "bug", "evaluating", "automated", "program", "repair", "fault"]}, {"id": "term-defense-in-depth-for-ai", "t": "Defense in Depth for AI", "tg": ["Safety", "Technical"], "d": "safety", "x": "A security strategy that uses multiple layers of protection to secure AI systems. No single defense is considered...", "l": "d", "k": ["defense", "depth", "security", "strategy", "uses", "multiple", "layers", "protection", "secure", "systems", "single", "considered", "sufficient", "overlapping", "safeguards"]}, {"id": "term-defog-sqlcoder", "t": "Defog SQLCoder", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A text-to-SQL model from Defog that specializes in generating accurate SQL queries from natural language questions for...", "l": "d", "k": ["defog", "sqlcoder", "text-to-sql", "model", "specializes", "generating", "accurate", "sql", "queries", "natural", "language", "questions", "business", "analytics", "applications"]}, {"id": "term-deformable-attention", "t": "Deformable Attention", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "An attention mechanism that attends to a small set of sampling points around a reference point with learnable offsets,...", "l": "d", "k": ["deformable", "attention", "mechanism", "attends", "small", "sampling", "points", "around", "reference", "point", "learnable", "offsets", "dramatically", "reducing", "computational"]}, {"id": "term-deformable-convolution", "t": "Deformable Convolution", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A convolution operation where the sampling grid positions are augmented with learned offsets, allowing the network to...", "l": "d", "k": ["deformable", "convolution", "operation", "sampling", "grid", "positions", "augmented", "learned", "offsets", "allowing", "network", "adaptively", "adjust", "receptive", "field"]}, {"id": "term-deformable-detr", "t": "Deformable DETR", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An improved detection Transformer that uses deformable attention modules to attend to a small set of key sampling...", "l": "d", "k": ["deformable", "detr", "improved", "detection", "transformer", "uses", "attention", "modules", "attend", "small", "key", "sampling", "points", "faster", "convergence"]}, {"id": "term-deformable-parts-model", "t": "Deformable Parts Model", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An object detection approach that represents objects as a collection of deformable parts arranged in a star topology....", "l": "d", "k": ["deformable", "parts", "model", "object", "detection", "approach", "represents", "objects", "collection", "arranged", "star", "topology", "uses", "root", "filter"]}, {"id": "term-deit", "t": "DeiT", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Data-efficient Image Transformer, a vision transformer training methodology that uses knowledge distillation and strong...", "l": "d", "k": ["deit", "data-efficient", "image", "transformer", "vision", "training", "methodology", "uses", "knowledge", "distillation", "strong", "data", "augmentation", "achieve", "competitive"]}, {"id": "term-deit-iii", "t": "DeiT III", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "The third version of Data-efficient Image Transformers that uses a simple and effective recipe with revised training...", "l": "d", "k": ["deit", "iii", "version", "data-efficient", "image", "transformers", "uses", "simple", "effective", "recipe", "revised", "training", "procedures", "strong", "classification"]}, {"id": "term-delaunay-triangulation", "t": "Delaunay Triangulation", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "A triangulation of a set of points such that no point lies inside the circumscribed circle of any triangle. Maximizes...", "l": "d", "k": ["delaunay", "triangulation", "points", "point", "lies", "inside", "circumscribed", "circle", "triangle", "maximizes", "minimum", "angle", "triangles", "dual", "voronoi"]}, {"id": "term-delimiter", "t": "Delimiter", "tg": ["Prompting", "Technique"], "d": "general", "x": "Characters or symbols used in prompts to clearly separate different sections or types of content. Examples include...", "l": "d", "k": ["delimiter", "characters", "symbols", "prompts", "clearly", "separate", "different", "sections", "types", "content", "examples", "include", "triple", "backticks", "xml-style"]}, {"id": "term-delphi-training-data", "t": "Delphi Training Data", "tg": ["Training Corpus", "NLP", "Safety"], "d": "datasets", "x": "The training data for the Delphi moral reasoning system containing millions of ethical judgments about everyday...", "l": "d", "k": ["delphi", "training", "data", "moral", "reasoning", "system", "containing", "millions", "ethical", "judgments", "everyday", "situations", "explores", "descriptive", "ethics"]}, {"id": "term-deltanet", "t": "DeltaNet", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A linear attention variant that uses delta update rules from online learning for improved in-context retrieval and...", "l": "d", "k": ["deltanet", "linear", "attention", "variant", "uses", "delta", "update", "rules", "online", "learning", "improved", "in-context", "retrieval", "associative", "memory"]}, {"id": "term-demis-hassabis", "t": "Demis Hassabis", "tg": ["History", "Pioneers"], "d": "history", "x": "British AI researcher and neuroscientist who co-founded DeepMind in 2010, led the development of AlphaGo and AlphaFold,...", "l": "d", "k": ["demis", "hassabis", "british", "researcher", "neuroscientist", "co-founded", "deepmind", "led", "development", "alphago", "alphafold", "serves", "ceo", "google", "won"]}, {"id": "term-demographic-parity", "t": "Demographic Parity", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness metric requiring that the probability of a positive outcome is equal across all protected groups. Also known...", "l": "d", "k": ["demographic", "parity", "fairness", "metric", "requiring", "probability", "positive", "outcome", "equal", "across", "protected", "groups", "known", "statistical", "mandates"]}, {"id": "term-denclue-algorithm", "t": "DENCLUE Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "Density-based clustering using kernel density estimation that models the data density as a sum of influence functions....", "l": "d", "k": ["denclue", "algorithm", "density-based", "clustering", "kernel", "density", "estimation", "models", "data", "sum", "influence", "functions", "clusters", "defined", "regions"]}, {"id": "term-dendral", "t": "DENDRAL", "tg": ["History", "Milestones"], "d": "history", "x": "One of the first expert systems, developed at Stanford in the 1960s-1970s by Edward Feigenbaum and Joshua Lederberg,...", "l": "d", "k": ["dendral", "expert", "systems", "developed", "stanford", "1960s-1970s", "edward", "feigenbaum", "joshua", "lederberg", "identified", "chemical", "compounds", "mass", "spectrometry"]}, {"id": "term-dennard-scaling", "t": "Dennard Scaling", "tg": ["Fabrication", "History", "Principle"], "d": "hardware", "x": "Observation that as transistors shrink their power density stays constant allowing higher performance at the same...", "l": "d", "k": ["dennard", "scaling", "observation", "transistors", "shrink", "power", "density", "stays", "constant", "allowing", "higher", "performance", "broke", "down", "around"]}, {"id": "term-denoising-autoencoder", "t": "Denoising Autoencoder", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An autoencoder variant trained to reconstruct clean data from corrupted inputs, learning robust feature representations...", "l": "d", "k": ["denoising", "autoencoder", "variant", "trained", "reconstruct", "clean", "data", "corrupted", "inputs", "learning", "robust", "feature", "representations", "forcing", "network"]}, {"id": "term-ddpm", "t": "Denoising Diffusion Probabilistic Model", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative model that learns to reverse a gradual noising process, generating samples by iteratively denoising from...", "l": "d", "k": ["denoising", "diffusion", "probabilistic", "model", "generative", "learns", "reverse", "gradual", "noising", "process", "generating", "samples", "iteratively", "pure", "gaussian"]}, {"id": "term-dense-connection", "t": "Dense Connection", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A network pattern where each layer receives inputs from all preceding layers. Used in DenseNet architectures....", "l": "d", "k": ["dense", "connection", "network", "pattern", "layer", "receives", "inputs", "preceding", "layers", "densenet", "architectures", "encourages", "feature", "reuse", "reduces"]}, {"id": "term-dense-passage-retriever", "t": "Dense Passage Retriever", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "A bi-encoder retrieval model (DPR) that trains separate BERT-based encoders for queries and passages using contrastive...", "l": "d", "k": ["dense", "passage", "retriever", "bi-encoder", "retrieval", "model", "dpr", "trains", "separate", "bert-based", "encoders", "queries", "passages", "contrastive", "learning"]}, {"id": "term-dense-prediction", "t": "Dense Prediction", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Computer vision tasks that require producing an output for every pixel in an input image, including semantic...", "l": "d", "k": ["dense", "prediction", "computer", "vision", "tasks", "require", "producing", "output", "pixel", "input", "image", "including", "semantic", "segmentation", "depth"]}, {"id": "term-dense-retrieval", "t": "Dense Retrieval", "tg": ["Retrieval", "Search"], "d": "general", "x": "An information retrieval approach that represents queries and documents as dense continuous vectors from neural...", "l": "d", "k": ["dense", "retrieval", "information", "approach", "represents", "queries", "documents", "continuous", "vectors", "neural", "encoders", "retrieves", "candidates", "based", "vector"]}, {"id": "term-dense-reward", "t": "Dense Reward", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "A reward structure that provides frequent, informative feedback at nearly every time step, guiding the agent more...", "l": "d", "k": ["dense", "reward", "structure", "provides", "frequent", "informative", "feedback", "nearly", "time", "step", "guiding", "agent", "directly", "toward", "desired"]}, {"id": "term-dense-sparse-hybrid", "t": "Dense-Sparse Hybrid", "tg": ["Retrieval", "Search"], "d": "general", "x": "A retrieval strategy that fuses results from both dense vector search and sparse lexical search, typically using...", "l": "d", "k": ["dense-sparse", "hybrid", "retrieval", "strategy", "fuses", "results", "dense", "vector", "search", "sparse", "lexical", "typically", "reciprocal", "rank", "fusion"]}, {"id": "term-densenet", "t": "DenseNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A CNN architecture where each layer receives feature maps from all preceding layers as input and passes its own feature...", "l": "d", "k": ["densenet", "cnn", "architecture", "layer", "receives", "feature", "maps", "preceding", "layers", "input", "passes", "subsequent", "promoting", "reuse", "reducing"]}, {"id": "term-density-peak-clustering", "t": "Density Peak Clustering", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A clustering algorithm that identifies cluster centers as points with high local density that are far from other...", "l": "d", "k": ["density", "peak", "clustering", "algorithm", "identifies", "cluster", "centers", "points", "high", "local", "far", "high-density", "constructs", "decision", "graph"]}, {"id": "term-deontological-ai-ethics", "t": "Deontological AI Ethics", "tg": ["Safety", "Ethics"], "d": "safety", "x": "An approach to AI ethics based on rule-following and duty rather than consequences. Holds that certain actions like...", "l": "d", "k": ["deontological", "ethics", "approach", "based", "rule-following", "duty", "rather", "consequences", "holds", "certain", "actions", "deception", "privacy", "violation", "inherently"]}, {"id": "term-dependency-parsing", "t": "Dependency Parsing", "tg": ["NLP", "Parsing"], "d": "general", "x": "The task of analyzing the grammatical structure of a sentence by identifying directed relationships between words,...", "l": "d", "k": ["dependency", "parsing", "task", "analyzing", "grammatical", "structure", "sentence", "identifying", "directed", "relationships", "words", "representing", "modify", "depend"]}, {"id": "term-dependency-parsing-algorithm", "t": "Dependency Parsing Algorithm", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "An algorithm that analyzes the grammatical structure of a sentence by establishing directed relationships between...", "l": "d", "k": ["dependency", "parsing", "algorithm", "analyzes", "grammatical", "structure", "sentence", "establishing", "directed", "relationships", "words", "transition-based", "parsers", "stack", "buffer"]}, {"id": "term-dependency-tree", "t": "Dependency Tree", "tg": ["NLP", "Parsing"], "d": "general", "x": "A directed tree structure representing syntactic dependencies in a sentence where each word is a node and edges...", "l": "d", "k": ["dependency", "tree", "directed", "structure", "representing", "syntactic", "dependencies", "sentence", "word", "node", "edges", "indicate", "grammatical", "relationships", "subject"]}, {"id": "term-deplot", "t": "DePlot", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A model from Google that translates charts and plots into structured data tables using a visual language approach for...", "l": "d", "k": ["deplot", "model", "google", "translates", "charts", "plots", "structured", "data", "tables", "visual", "language", "approach", "automated", "chart", "extraction"]}, {"id": "term-deployment-bias", "t": "Deployment Bias", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Bias that emerges when an AI system is used in contexts or populations that differ from its training conditions,...", "l": "d", "k": ["deployment", "bias", "emerges", "system", "contexts", "populations", "differ", "training", "conditions", "including", "shifts", "user", "behavior", "environmental", "population"]}, {"id": "term-deployment-monitoring", "t": "Deployment Monitoring", "tg": ["Safety", "Governance"], "d": "safety", "x": "Continuous observation of AI system behavior after release to production to detect performance degradation...", "l": "d", "k": ["deployment", "monitoring", "continuous", "observation", "system", "behavior", "release", "production", "detect", "performance", "degradation", "distributional", "shift", "emerging", "safety"]}, {"id": "term-depth-anything", "t": "Depth Anything", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A foundation model for monocular depth estimation that produces accurate relative depth maps from single images across...", "l": "d", "k": ["depth", "anything", "foundation", "model", "monocular", "estimation", "produces", "accurate", "relative", "maps", "single", "images", "across", "diverse", "scenes"]}, {"id": "term-depth-anything-v2", "t": "Depth Anything V2", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An improved monocular depth estimation model using synthetic training data and a more capable teacher model for finer...", "l": "d", "k": ["depth", "anything", "improved", "monocular", "estimation", "model", "synthetic", "training", "data", "capable", "teacher", "finer", "predictions"]}, {"id": "term-depth-estimation", "t": "Depth Estimation", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The task of predicting the distance of each pixel from the camera in a 2D image, producing a dense depth map using...", "l": "d", "k": ["depth", "estimation", "task", "predicting", "distance", "pixel", "camera", "image", "producing", "dense", "map", "monocular", "cues", "learned", "deep"]}, {"id": "term-depth-first-search", "t": "Depth-First Search", "tg": ["Algorithms", "Fundamentals", "Graph", "Searching"], "d": "algorithms", "x": "A graph traversal algorithm that explores as far as possible along each branch before backtracking. Uses a stack data...", "l": "d", "k": ["depth-first", "search", "graph", "traversal", "algorithm", "explores", "far", "possible", "along", "branch", "backtracking", "uses", "stack", "data", "structure"]}, {"id": "term-depthpro", "t": "DepthPro", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A monocular depth estimation model from Apple that produces metric depth maps with sharp boundaries in a single forward...", "l": "d", "k": ["depthpro", "monocular", "depth", "estimation", "model", "apple", "produces", "metric", "maps", "sharp", "boundaries", "single", "forward", "pass", "high"]}, {"id": "term-depthwise-convolution", "t": "Depthwise Convolution", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A convolution that applies a separate filter to each input channel independently, capturing spatial features per...", "l": "d", "k": ["depthwise", "convolution", "applies", "separate", "filter", "input", "channel", "independently", "capturing", "spatial", "features", "per", "without", "mixing", "information"]}, {"id": "term-grouped-convolution", "t": "Depthwise Convolution Variant", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A convolution where input channels are divided into groups and convolution is applied independently within each group,...", "l": "d", "k": ["depthwise", "convolution", "variant", "input", "channels", "divided", "groups", "applied", "independently", "within", "group", "reducing", "parameters", "computation", "proportional"]}, {"id": "term-depthwise-separable-convolution", "t": "Depthwise Separable Convolution", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A factorized convolution that decomposes a standard convolution into a depthwise convolution applied independently per...", "l": "d", "k": ["depthwise", "separable", "convolution", "factorized", "decomposes", "standard", "applied", "independently", "per", "channel", "followed", "pointwise", "1x1", "reducing", "parameters"]}, {"id": "term-description-logics", "t": "Description Logics", "tg": ["History", "Fundamentals"], "d": "history", "x": "A family of formal knowledge representation languages used as the logical basis for ontologies and the Semantic Web....", "l": "d", "k": ["description", "logics", "family", "formal", "knowledge", "representation", "languages", "logical", "basis", "ontologies", "semantic", "web", "provide", "balance", "expressiveness"]}, {"id": "term-design-justice", "t": "Design Justice", "tg": ["Safety", "Ethics"], "d": "safety", "x": "A framework that centers the voices of communities most impacted by design decisions in the AI development process....", "l": "d", "k": ["design", "justice", "framework", "centers", "voices", "communities", "impacted", "decisions", "development", "process", "challenges", "assumption", "designers", "know", "best"]}, {"id": "term-design-rule-check", "t": "Design Rule Check", "tg": ["Manufacturing", "Verification", "Process"], "d": "hardware", "x": "Automated verification that a chip layout meets all manufacturing constraints specified by the foundry. Must pass...", "l": "d", "k": ["design", "rule", "check", "automated", "verification", "chip", "layout", "meets", "manufacturing", "constraints", "specified", "foundry", "must", "pass", "proceed"]}, {"id": "term-deterministic-policy-gradient", "t": "Deterministic Policy Gradient (DPG)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A policy gradient theorem for deterministic policies that computes the gradient of expected return by backpropagating...", "l": "d", "k": ["deterministic", "policy", "gradient", "dpg", "theorem", "policies", "computes", "expected", "return", "backpropagating", "q-function", "respect", "actions", "requires", "single"]}, {"id": "term-deterministic", "t": "Deterministic vs Stochastic", "tg": ["Concept", "LLM"], "d": "models", "x": "Deterministic systems produce the same output for the same input every time. LLMs are typically stochastic (random),...", "l": "d", "k": ["deterministic", "stochastic", "systems", "produce", "output", "input", "time", "llms", "typically", "random", "producing", "varied", "outputs", "unless", "temperature"]}, {"id": "term-detokenization", "t": "Detokenization", "tg": ["NLP", "Tokenization"], "d": "general", "x": "The process of converting a sequence of tokens back into readable text by reversing the tokenization process, handling...", "l": "d", "k": ["detokenization", "process", "converting", "sequence", "tokens", "readable", "text", "reversing", "tokenization", "handling", "subword", "boundaries", "spacing", "special", "characters"]}, {"id": "term-detr", "t": "DETR", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Detection Transformer, an end-to-end object detection model that uses a transformer encoder-decoder architecture with...", "l": "d", "k": ["detr", "detection", "transformer", "end-to-end", "object", "model", "uses", "encoder-decoder", "architecture", "bipartite", "matching", "loss", "eliminating", "need", "hand-designed"]}, {"id": "term-devbench", "t": "DevBench", "tg": ["Benchmark", "Code", "Evaluation"], "d": "datasets", "x": "A comprehensive benchmark for evaluating AI coding assistants on realistic software development tasks. Tests...", "l": "d", "k": ["devbench", "comprehensive", "benchmark", "evaluating", "coding", "assistants", "realistic", "software", "development", "tasks", "tests", "project-level", "understanding", "including", "design"]}, {"id": "term-deviance", "t": "Deviance", "tg": ["Statistics", "Metrics"], "d": "datasets", "x": "A goodness-of-fit statistic for generalized linear models, computed as twice the difference in log-likelihoods between...", "l": "d", "k": ["deviance", "goodness-of-fit", "statistic", "generalized", "linear", "models", "computed", "twice", "difference", "log-likelihoods", "fitted", "model", "saturated", "generalizes", "residual"]}, {"id": "term-dgx-system", "t": "DGX System", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "NVIDIA's integrated AI supercomputing platform pre-configured with multiple high-end GPUs, NVLink/NVSwitch...", "l": "d", "k": ["dgx", "system", "nvidia", "integrated", "supercomputing", "platform", "pre-configured", "multiple", "high-end", "gpus", "nvlink", "nvswitch", "interconnects", "optimized", "software"]}, {"id": "term-dialogue-act", "t": "Dialogue Act", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A categorization of the communicative function of an utterance in a conversation, such as question, statement, request,...", "l": "d", "k": ["dialogue", "act", "categorization", "communicative", "function", "utterance", "conversation", "question", "statement", "request", "greeting", "acknowledgment", "system", "design"]}, {"id": "term-dialogue-system", "t": "Dialogue System", "tg": ["Application", "NLP"], "d": "general", "x": "An AI system designed to converse with humans in natural language. Includes task-oriented systems (customer service)...", "l": "d", "k": ["dialogue", "system", "designed", "converse", "humans", "natural", "language", "includes", "task-oriented", "systems", "customer", "service", "open-domain", "chatbots", "general"]}, {"id": "term-diamond", "t": "DIAMOND", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "Diffusion-based world model for Atari that generates future game frames through a diffusion process for training...", "l": "d", "k": ["diamond", "diffusion-based", "world", "model", "atari", "generates", "future", "game", "frames", "diffusion", "process", "training", "reinforcement", "learning", "agents"]}, {"id": "term-dice-loss", "t": "Dice Loss", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A loss function based on the Dice coefficient measuring overlap between predicted and ground truth segmentation masks....", "l": "d", "k": ["dice", "loss", "function", "based", "coefficient", "measuring", "overlap", "predicted", "ground", "truth", "segmentation", "masks", "ranges", "handles", "class"]}, {"id": "term-didemo", "t": "DiDeMo", "tg": ["Benchmark", "Video", "Multimodal"], "d": "datasets", "x": "Distinct Describable Moments a video retrieval dataset where models must localize moments in video described by natural...", "l": "d", "k": ["didemo", "distinct", "describable", "moments", "video", "retrieval", "dataset", "models", "must", "localize", "described", "natural", "language", "queries", "tests"]}, {"id": "term-die-semiconductor", "t": "Die (Semiconductor)", "tg": ["Fabrication", "Manufacturing"], "d": "hardware", "x": "Individual chip cut from a processed semiconductor wafer. Each die contains the complete circuitry of a processor and...", "l": "d", "k": ["die", "semiconductor", "individual", "chip", "cut", "processed", "wafer", "contains", "complete", "circuitry", "processor", "packaged", "final", "product", "systems"]}, {"id": "term-die-area", "t": "Die Area", "tg": ["Manufacturing", "Metric", "Design"], "d": "hardware", "x": "Physical size of a semiconductor die measured in square millimeters. Larger dies contain more transistors but cost more...", "l": "d", "k": ["die", "area", "physical", "size", "semiconductor", "measured", "square", "millimeters", "larger", "dies", "contain", "transistors", "cost", "due", "lower"]}, {"id": "term-die-stacking", "t": "Die Stacking", "tg": ["Packaging", "Architecture", "3D"], "d": "hardware", "x": "Vertically layering multiple semiconductor dies in a single package connected by through-silicon vias. Increases memory...", "l": "d", "k": ["die", "stacking", "vertically", "layering", "multiple", "semiconductor", "dies", "single", "package", "connected", "through-silicon", "vias", "increases", "memory", "density"]}, {"id": "term-dien", "t": "DIEN", "tg": ["Models", "Technical", "Recommendation"], "d": "models", "x": "Deep Interest Evolution Network models the temporal evolution of user interests using a GRU-based architecture with an...", "l": "d", "k": ["dien", "deep", "interest", "evolution", "network", "models", "temporal", "user", "interests", "gru-based", "architecture", "auxiliary", "loss", "click-through", "rate"]}, {"id": "term-difference-of-gaussians", "t": "Difference of Gaussians", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An edge and blob detection technique that approximates the Laplacian of Gaussian by subtracting two Gaussian-blurred...", "l": "d", "k": ["difference", "gaussians", "edge", "blob", "detection", "technique", "approximates", "laplacian", "gaussian", "subtracting", "gaussian-blurred", "versions", "image", "sift", "efficient"]}, {"id": "term-difference-in-differences-algorithm", "t": "Difference-in-Differences Algorithm", "tg": ["Algorithms", "Fundamentals", "Causal"], "d": "algorithms", "x": "A causal inference method that compares the change in outcomes over time between a treated group and a control group....", "l": "d", "k": ["difference-in-differences", "algorithm", "causal", "inference", "method", "compares", "change", "outcomes", "time", "treated", "group", "control", "identifies", "effects", "assumption"]}, {"id": "term-differencing", "t": "Differencing", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A time series transformation that computes the difference between consecutive observations (or seasonal periods) to...", "l": "d", "k": ["differencing", "time", "series", "transformation", "computes", "difference", "consecutive", "observations", "seasonal", "periods", "achieve", "stationarity", "first-order", "removes", "linear"]}, {"id": "term-differentiable-neural-computer", "t": "Differentiable Neural Computer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An extension of the Neural Turing Machine that adds temporal link tracking and dynamic memory allocation, improving the...", "l": "d", "k": ["differentiable", "neural", "computer", "extension", "turing", "machine", "adds", "temporal", "link", "tracking", "dynamic", "memory", "allocation", "improving", "ability"]}, {"id": "term-differentiable-programming", "t": "Differentiable Programming", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A programming paradigm where programs are differentiable end-to-end enabling gradient-based optimization of arbitrary...", "l": "d", "k": ["differentiable", "programming", "paradigm", "programs", "end-to-end", "enabling", "gradient-based", "optimization", "arbitrary", "computations", "extends", "deep", "learning", "beyond", "standard"]}, {"id": "term-differentiable-rendering", "t": "Differentiable Rendering", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "Rendering techniques where the image formation process is differentiable with respect to scene parameters, enabling...", "l": "d", "k": ["differentiable", "rendering", "techniques", "image", "formation", "process", "respect", "scene", "parameters", "enabling", "gradient-based", "optimization", "geometry", "materials", "lighting"]}, {"id": "term-differential-diffusion", "t": "Differential Diffusion", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A method for controlling the amount of change applied to different regions of an image during diffusion-based editing...", "l": "d", "k": ["differential", "diffusion", "method", "controlling", "amount", "change", "applied", "different", "regions", "image", "diffusion-based", "editing", "per-pixel", "maps"]}, {"id": "term-differential-evolution", "t": "Differential Evolution", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A population-based optimization algorithm that creates new candidates by combining existing ones using vector...", "l": "d", "k": ["differential", "evolution", "population-based", "optimization", "algorithm", "creates", "candidates", "combining", "existing", "ones", "vector", "differences", "effective", "continuous", "problems"]}, {"id": "term-differential-privacy", "t": "Differential Privacy", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "A mathematical framework providing formal guarantees that the output of a computation does not reveal whether any...", "l": "d", "k": ["differential", "privacy", "mathematical", "framework", "providing", "formal", "guarantees", "output", "computation", "reveal", "single", "individual", "data", "included", "input"]}, {"id": "term-differential-privacy-algorithm", "t": "Differential Privacy Algorithm", "tg": ["Algorithms", "Fundamentals", "Privacy"], "d": "algorithms", "x": "A mathematical framework for quantifying privacy loss when releasing statistical information about a dataset. Adds...", "l": "d", "k": ["differential", "privacy", "algorithm", "mathematical", "framework", "quantifying", "loss", "releasing", "statistical", "information", "dataset", "adds", "calibrated", "noise", "query"]}, {"id": "term-differential-technology-development", "t": "Differential Technology Development", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "The strategic prioritization of developing defensive and safety technologies ahead of potentially dangerous...", "l": "d", "k": ["differential", "technology", "development", "strategic", "prioritization", "developing", "defensive", "safety", "technologies", "ahead", "potentially", "dangerous", "capabilities", "ensuring", "protective"]}, {"id": "term-diffusion-maps-algorithm", "t": "Diffusion Maps Algorithm", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A nonlinear dimensionality reduction method based on the diffusion process on a data graph. Eigenvalues and...", "l": "d", "k": ["diffusion", "maps", "algorithm", "nonlinear", "dimensionality", "reduction", "method", "based", "process", "data", "graph", "eigenvalues", "eigenvectors", "operator", "reveal"]}, {"id": "term-diffusion-model", "t": "Diffusion Model", "tg": ["Architecture", "Generative"], "d": "models", "x": "A generative AI architecture that creates content by gradually removing noise from random data. Powers leading image...", "l": "d", "k": ["diffusion", "model", "generative", "architecture", "creates", "content", "gradually", "removing", "noise", "random", "data", "powers", "leading", "image", "generators"]}, {"id": "term-diffusion-model-breakthrough", "t": "Diffusion Model Breakthrough", "tg": ["History", "Milestones"], "d": "history", "x": "The emergence of diffusion-based generative models in 2020-2022 that progressively denoise random noise into...", "l": "d", "k": ["diffusion", "model", "breakthrough", "emergence", "diffusion-based", "generative", "models", "2020-2022", "progressively", "denoise", "random", "noise", "high-quality", "images", "enabling"]}, {"id": "term-diffusion-models-history", "t": "Diffusion Models History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of diffusion-based generative models from the theoretical foundation by Sohl-Dickstein et al. (2015)...", "l": "d", "k": ["diffusion", "models", "history", "development", "diffusion-based", "generative", "theoretical", "foundation", "sohl-dickstein", "denoising", "probabilistic", "practical", "image", "generation", "systems"]}, {"id": "term-diffusion-policy", "t": "Diffusion Policy", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "A robot policy representation that uses denoising diffusion processes to generate robot actions for visuomotor control...", "l": "d", "k": ["diffusion", "policy", "robot", "representation", "uses", "denoising", "processes", "generate", "actions", "visuomotor", "control", "multi-modal", "action", "distributions"]}, {"id": "term-diffusion-transformer", "t": "Diffusion Transformer", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "An architecture (DiT) that replaces the U-Net backbone in diffusion models with a transformer operating on sequences of...", "l": "d", "k": ["diffusion", "transformer", "architecture", "dit", "replaces", "u-net", "backbone", "models", "operating", "sequences", "latent", "patches", "scaling", "effectively", "achieving"]}, {"id": "term-diffusiondb", "t": "DiffusionDB", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "A large-scale dataset of 14 million text-to-image prompt and generation pairs from Stable Diffusion. Provides real user...", "l": "d", "k": ["diffusiondb", "large-scale", "dataset", "million", "text-to-image", "prompt", "generation", "pairs", "stable", "diffusion", "provides", "real", "user", "prompts", "corresponding"]}, {"id": "term-digital-consent", "t": "Digital Consent", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The process of obtaining meaningful permission from users for AI-mediated data collection processing and...", "l": "d", "k": ["digital", "consent", "process", "obtaining", "meaningful", "permission", "users", "ai-mediated", "data", "collection", "processing", "decision-making", "environments", "challenges", "include"]}, {"id": "term-digital-dignity", "t": "Digital Dignity", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The principle that AI systems should treat individuals with respect and not reduce human beings to data points or...", "l": "d", "k": ["digital", "dignity", "principle", "systems", "treat", "individuals", "respect", "reduce", "human", "beings", "data", "points", "optimization", "targets", "encompasses"]}, {"id": "term-digital-divide-and-ai", "t": "Digital Divide and AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The gap between those who have access to AI technologies and the skills to use them and those who do not. AI may widen...", "l": "d", "k": ["digital", "divide", "gap", "access", "technologies", "skills", "widen", "existing", "divides", "without", "deliberate", "policies", "promote", "equitable"]}, {"id": "term-digital-provenance", "t": "Digital Provenance", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The verifiable record of the origin, creation process, and modification history of a digital asset, increasingly...", "l": "d", "k": ["digital", "provenance", "verifiable", "record", "origin", "creation", "process", "modification", "history", "asset", "increasingly", "important", "establishing", "trust", "authenticity"]}, {"id": "term-digital-watermarking-for-ai", "t": "Digital Watermarking for AI", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "Techniques for embedding imperceptible identifying information into AI-generated content such as images, text, or...", "l": "d", "k": ["digital", "watermarking", "techniques", "embedding", "imperceptible", "identifying", "information", "ai-generated", "content", "images", "text", "audio", "enabling", "later", "detection"]}, {"id": "term-dijkstras-algorithm", "t": "Dijkstra's Algorithm", "tg": ["Algorithms", "Fundamentals", "Graph"], "d": "algorithms", "x": "A graph search algorithm that finds the shortest path from a source node to all other nodes in a weighted graph with...", "l": "d", "k": ["dijkstra", "algorithm", "graph", "search", "finds", "shortest", "path", "source", "node", "nodes", "weighted", "non-negative", "edge", "weights", "uses"]}, {"id": "term-dilated-attention", "t": "Dilated Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that attends to tokens at regularly spaced intervals with gaps between attended positions,...", "l": "d", "k": ["dilated", "attention", "mechanism", "attends", "tokens", "regularly", "spaced", "intervals", "gaps", "attended", "positions", "allowing", "token", "capture", "long-range"]}, {"id": "term-dilated-convolution", "t": "Dilated Convolution", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A convolution operation with gaps between kernel elements that exponentially increases the receptive field without...", "l": "d", "k": ["dilated", "convolution", "operation", "gaps", "kernel", "elements", "exponentially", "increases", "receptive", "field", "without", "increasing", "parameters", "reducing", "spatial"]}, {"id": "term-dimenet", "t": "DimeNet", "tg": ["Models", "Scientific"], "d": "models", "x": "Directional Message Passing Neural Network that incorporates bond angles into molecular message passing for improved...", "l": "d", "k": ["dimenet", "directional", "message", "passing", "neural", "network", "incorporates", "bond", "angles", "molecular", "improved", "prediction", "quantum", "chemical", "properties"]}, {"id": "term-dimensionality-reduction", "t": "Dimensionality Reduction", "tg": ["Technique", "Data Processing"], "d": "general", "x": "Techniques to reduce the number of features in data while preserving important information. Used for visualization,...", "l": "d", "k": ["dimensionality", "reduction", "techniques", "reduce", "number", "features", "data", "preserving", "important", "information", "visualization", "noise", "improving", "computational", "efficiency"]}, {"id": "term-dimensionality-reduction-vectors", "t": "Dimensionality Reduction for Vectors", "tg": ["Vector Database", "Dimensionality Reduction"], "d": "general", "x": "Techniques that project high-dimensional embedding vectors into lower-dimensional spaces to reduce storage, accelerate...", "l": "d", "k": ["dimensionality", "reduction", "vectors", "techniques", "project", "high-dimensional", "embedding", "lower-dimensional", "spaces", "reduce", "storage", "accelerate", "search", "mitigate", "curse"]}, {"id": "term-din", "t": "DIN", "tg": ["Models", "Technical", "Recommendation"], "d": "models", "x": "Deep Interest Network is a recommendation model from Alibaba that uses an attention mechanism to adaptively learn user...", "l": "d", "k": ["din", "deep", "interest", "network", "recommendation", "model", "alibaba", "uses", "attention", "mechanism", "adaptively", "learn", "user", "representations", "behavior"]}, {"id": "term-dinics-algorithm", "t": "Dinic's Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A maximum flow algorithm that uses a layered graph constructed by BFS to find blocking flows. Achieves O(V^2 * E) time...", "l": "d", "k": ["dinic", "algorithm", "maximum", "flow", "uses", "layered", "graph", "constructed", "bfs", "find", "blocking", "flows", "achieves", "time", "complexity"]}, {"id": "term-dino", "t": "DINO", "tg": ["Models", "Technical"], "d": "models", "x": "Self-distillation with no labels is a self-supervised learning method for vision transformers that discovers semantic...", "l": "d", "k": ["dino", "self-distillation", "labels", "self-supervised", "learning", "method", "vision", "transformers", "discovers", "semantic", "segments", "without", "supervision", "uses", "student-teacher"]}, {"id": "term-dino-detection", "t": "DINO Detection", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A self-supervised vision Transformer pre-training method that learns visual features using self-distillation with no...", "l": "d", "k": ["dino", "detection", "self-supervised", "vision", "transformer", "pre-training", "method", "learns", "visual", "features", "self-distillation", "labels", "strong", "downstream"]}, {"id": "term-dinov2", "t": "DINOv2", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A self-supervised vision model trained with a combination of self-distillation and masked image modeling that produces...", "l": "d", "k": ["dinov2", "self-supervised", "vision", "model", "trained", "combination", "self-distillation", "masked", "image", "modeling", "produces", "versatile", "visual", "features", "useful"]}, {"id": "term-direct-to-chip-liquid-cooling", "t": "Direct-to-Chip Liquid Cooling", "tg": ["Cooling", "Data Center"], "d": "hardware", "x": "Cooling approach where liquid coolant flows through cold plates mounted directly on heat-generating chips. More...", "l": "d", "k": ["direct-to-chip", "liquid", "cooling", "approach", "coolant", "flows", "cold", "plates", "mounted", "directly", "heat-generating", "chips", "efficient", "air", "less"]}, {"id": "term-directional-stimulus-prompting", "t": "Directional Stimulus Prompting", "tg": ["Prompt Engineering", "Guided Generation"], "d": "general", "x": "A prompting framework that provides a small, tunable stimulus or hint within the prompt to guide the language model...", "l": "d", "k": ["directional", "stimulus", "prompting", "framework", "provides", "small", "tunable", "hint", "within", "prompt", "guide", "language", "model", "toward", "desired"]}, {"id": "term-dirichlet-distribution", "t": "Dirichlet Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A multivariate generalization of the beta distribution that generates probability vectors summing to one. It is widely...", "l": "d", "k": ["dirichlet", "distribution", "multivariate", "generalization", "beta", "generates", "probability", "vectors", "summing", "widely", "prior", "categorical", "distributions", "topic", "models"]}, {"id": "term-disaggregated-computing", "t": "Disaggregated Computing", "tg": ["Architecture", "Data Center", "Emerging"], "d": "hardware", "x": "Architecture separating compute memory and storage into independently scalable pools connected by high-speed fabric....", "l": "d", "k": ["disaggregated", "computing", "architecture", "separating", "compute", "memory", "storage", "independently", "scalable", "pools", "connected", "high-speed", "fabric", "enables", "efficient"]}, {"id": "term-disaggregated-serving", "t": "Disaggregated Serving", "tg": ["Inference Infrastructure", "Distributed Computing"], "d": "hardware", "x": "An inference architecture that separates storage, compute, and memory resources into independent pools that can be...", "l": "d", "k": ["disaggregated", "serving", "inference", "architecture", "separates", "storage", "compute", "memory", "resources", "independent", "pools", "scaled", "independently", "enables", "flexible"]}, {"id": "term-discount-factor", "t": "Discount Factor", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A parameter gamma between 0 and 1 that determines how much future rewards are weighted relative to immediate rewards....", "l": "d", "k": ["discount", "factor", "parameter", "gamma", "determines", "future", "rewards", "weighted", "relative", "immediate", "lower", "factors", "agent", "myopic", "values"]}, {"id": "term-discourse-analysis", "t": "Discourse Analysis", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The study of how sentences and utterances connect and relate to each other in text, examining coherence relations,...", "l": "d", "k": ["discourse", "analysis", "study", "sentences", "utterances", "connect", "relate", "text", "examining", "coherence", "relations", "rhetorical", "structure", "information", "flow"]}, {"id": "term-discourse-relation", "t": "Discourse Relation", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A semantic or pragmatic relationship between text segments such as cause-effect, contrast, elaboration, or temporal...", "l": "d", "k": ["discourse", "relation", "semantic", "pragmatic", "relationship", "text", "segments", "cause-effect", "contrast", "elaboration", "temporal", "sequence", "contributes", "coherence", "document"]}, {"id": "term-discrete-cosine-transform", "t": "Discrete Cosine Transform", "tg": ["Algorithms", "Fundamentals", "Signal Processing"], "d": "algorithms", "x": "A frequency transform that expresses a sequence of data points as a sum of cosine functions. Widely used in image and...", "l": "d", "k": ["discrete", "cosine", "transform", "frequency", "expresses", "sequence", "data", "points", "sum", "functions", "widely", "image", "audio", "compression", "including"]}, {"id": "term-discrete-element-method", "t": "Discrete Element Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A numerical method for simulating the behavior of granular materials and collections of discrete particles. Computes...", "l": "d", "k": ["discrete", "element", "method", "numerical", "simulating", "behavior", "granular", "materials", "collections", "particles", "computes", "forces", "displacements", "particle", "based"]}, {"id": "term-discretization", "t": "Discretization", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "The process of converting continuous features into discrete bins or categories, using methods such as equal-width...", "l": "d", "k": ["discretization", "process", "converting", "continuous", "features", "discrete", "bins", "categories", "methods", "equal-width", "binning", "equal-frequency", "supervised", "decision", "tree-based"]}, {"id": "term-discriminative-learning-rates", "t": "Discriminative Learning Rates", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A fine-tuning technique that applies different learning rates to different layers of a pretrained model. Earlier layers...", "l": "d", "k": ["discriminative", "learning", "rates", "fine-tuning", "technique", "applies", "different", "layers", "pretrained", "model", "earlier", "general", "features", "smaller", "later"]}, {"id": "term-discriminative-vs-generative-safety", "t": "Discriminative vs Generative Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "Different safety challenges posed by discriminative models that classify inputs versus generative models that produce...", "l": "d", "k": ["discriminative", "generative", "safety", "different", "challenges", "posed", "models", "classify", "inputs", "versus", "produce", "content", "face", "additional", "risks"]}, {"id": "term-disinformation", "t": "Disinformation", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "False or misleading information deliberately created and spread with the intent to deceive. AI-generated disinformation...", "l": "d", "k": ["disinformation", "false", "misleading", "information", "deliberately", "created", "spread", "intent", "deceive", "ai-generated", "escalating", "concern", "due", "increasing", "quality"]}, {"id": "term-disinformation-campaign-detection", "t": "Disinformation Campaign Detection", "tg": ["Safety", "Technical"], "d": "safety", "x": "AI techniques for identifying coordinated campaigns to spread false information across social media and other...", "l": "d", "k": ["disinformation", "campaign", "detection", "techniques", "identifying", "coordinated", "campaigns", "spread", "false", "information", "across", "social", "media", "platforms", "uses"]}, {"id": "term-disjoint-set-union-algorithm", "t": "Disjoint Set Union Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A data structure that maintains a collection of disjoint sets and supports efficient union and find operations. Uses...", "l": "d", "k": ["disjoint", "union", "algorithm", "data", "structure", "maintains", "collection", "sets", "supports", "efficient", "find", "operations", "uses", "path", "compression"]}, {"id": "term-disparate-impact", "t": "Disparate Impact", "tg": ["Fairness", "Regulation"], "d": "safety", "x": "A legal and ethical concept where a seemingly neutral AI policy or practice disproportionately harms members of a...", "l": "d", "k": ["disparate", "impact", "legal", "ethical", "concept", "seemingly", "neutral", "policy", "practice", "disproportionately", "harms", "members", "protected", "group", "without"]}, {"id": "term-disparity-map", "t": "Disparity Map", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A pixel-level representation of the horizontal displacement between corresponding points in left and right stereo...", "l": "d", "k": ["disparity", "map", "pixel-level", "representation", "horizontal", "displacement", "corresponding", "points", "left", "right", "stereo", "images", "inversely", "proportional", "depth"]}, {"id": "term-distance-transform-algorithm", "t": "Distance Transform Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An algorithm that computes for each pixel in a binary image the distance to the nearest background pixel. Used for...", "l": "d", "k": ["distance", "transform", "algorithm", "computes", "pixel", "binary", "image", "nearest", "background", "shape", "analysis", "watershed", "segmentation", "voronoi", "diagram"]}, {"id": "term-distilbert", "t": "DistilBERT", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A distilled version of BERT that retains 97% of its language understanding capabilities while being 60% smaller and 60%...", "l": "d", "k": ["distilbert", "distilled", "version", "bert", "retains", "language", "understanding", "capabilities", "smaller", "faster", "trained", "knowledge", "distillation", "techniques"]}, {"id": "term-distillation", "t": "Distillation (Knowledge Distillation)", "tg": ["Training", "Optimization"], "d": "algorithms", "x": "A technique to transfer knowledge from a large \"teacher\" model to a smaller \"student\" model. Creates efficient models...", "l": "d", "k": ["distillation", "knowledge", "technique", "transfer", "large", "teacher", "model", "smaller", "student", "creates", "efficient", "models", "retain", "larger", "capability"]}, {"id": "term-distinct-sampling-algorithm", "t": "Distinct Sampling Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A streaming algorithm that maintains a random sample of distinct elements from a data stream. Useful for estimating...", "l": "d", "k": ["distinct", "sampling", "algorithm", "streaming", "maintains", "random", "sample", "elements", "data", "stream", "useful", "estimating", "properties", "unique", "items"]}, {"id": "term-distinct-n", "t": "Distinct-N", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A diversity metric that calculates the ratio of unique n-grams to total n-grams in generated text, measuring lexical...", "l": "d", "k": ["distinct-n", "diversity", "metric", "calculates", "ratio", "unique", "n-grams", "total", "generated", "text", "measuring", "lexical", "higher", "values", "indicate"]}, {"id": "term-distributed-ai", "t": "Distributed AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "A subfield of AI concerned with systems where multiple computational entities (agents) work together to solve problems...", "l": "d", "k": ["distributed", "subfield", "concerned", "systems", "multiple", "computational", "entities", "agents", "work", "together", "solve", "problems", "achieve", "goals", "encompasses"]}, {"id": "term-distributed-file-system", "t": "Distributed File System", "tg": ["Storage", "Distributed", "Infrastructure"], "d": "hardware", "x": "File system that spans multiple servers presenting a unified namespace. Systems like GPFS Lustre and HDFS store...", "l": "d", "k": ["distributed", "file", "system", "spans", "multiple", "servers", "presenting", "unified", "namespace", "systems", "gpfs", "lustre", "hdfs", "store", "training"]}, {"id": "term-distributed-training", "t": "Distributed Training", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "The practice of spreading model training across multiple GPUs, nodes, or clusters to handle larger models and datasets....", "l": "d", "k": ["distributed", "training", "practice", "spreading", "model", "across", "multiple", "gpus", "nodes", "clusters", "handle", "larger", "models", "datasets", "requires"]}, {"id": "term-distribution-shift", "t": "Distribution Shift", "tg": ["Challenge", "Production"], "d": "general", "x": "When the data a model encounters in production differs from its training data. A major cause of model degradation over...", "l": "d", "k": ["distribution", "shift", "data", "model", "encounters", "production", "differs", "training", "major", "cause", "degradation", "time", "requiring", "monitoring", "retraining"]}, {"id": "term-distributional-rl", "t": "Distributional Reinforcement Learning", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An extension of value-based RL that models the full distribution of returns rather than just the expected value....", "l": "d", "k": ["distributional", "reinforcement", "learning", "extension", "value-based", "models", "full", "distribution", "returns", "rather", "expected", "value", "captures", "risk", "uncertainty"]}, {"id": "term-distributional-semantics", "t": "Distributional Semantics", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The theory that word meaning can be characterized by the contexts in which words appear, formalized as the...", "l": "d", "k": ["distributional", "semantics", "theory", "word", "meaning", "characterized", "contexts", "words", "appear", "formalized", "hypothesis", "similar", "distributions", "meanings"]}, {"id": "term-distributional-shift", "t": "Distributional Shift", "tg": ["Safety", "Technical"], "d": "safety", "x": "A change in the statistical distribution of data encountered during deployment compared to training data. Can cause...", "l": "d", "k": ["distributional", "shift", "change", "statistical", "distribution", "data", "encountered", "deployment", "compared", "training", "cause", "model", "performance", "degrade", "unpredictably"]}, {"id": "term-dit", "t": "DiT", "tg": ["Models", "Technical"], "d": "models", "x": "Diffusion Transformer replaces the U-Net backbone in diffusion models with a transformer architecture. Demonstrates...", "l": "d", "k": ["dit", "diffusion", "transformer", "replaces", "u-net", "backbone", "models", "architecture", "demonstrates", "transformers", "effective", "scalable", "backbones", "image", "generation"]}, {"id": "term-diverse-beam-search", "t": "Diverse Beam Search", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A variant of beam search that introduces a diversity penalty between beam groups, encouraging the generation of a set...", "l": "d", "k": ["diverse", "beam", "search", "variant", "introduces", "diversity", "penalty", "groups", "encouraging", "generation", "meaningfully", "different", "candidate", "sequences", "rather"]}, {"id": "term-diversity-in-retrieval", "t": "Diversity in Retrieval", "tg": ["Retrieval", "Diversity"], "d": "general", "x": "The goal of returning search results that cover different aspects, perspectives, or subtopics of a query rather than...", "l": "d", "k": ["diversity", "retrieval", "goal", "returning", "search", "results", "cover", "different", "aspects", "perspectives", "subtopics", "query", "rather", "redundant", "near-duplicate"]}, {"id": "term-diversity-score", "t": "Diversity Score", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that quantifies the variety and heterogeneity of a set of generated outputs by measuring lexical, semantic, or...", "l": "d", "k": ["diversity", "score", "metric", "quantifies", "variety", "heterogeneity", "generated", "outputs", "measuring", "lexical", "semantic", "topical", "differences", "among", "penalizing"]}, {"id": "term-divisive-clustering-algorithm", "t": "Divisive Clustering Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A top-down hierarchical clustering method that starts with all points in one cluster and recursively splits clusters....", "l": "d", "k": ["divisive", "clustering", "algorithm", "top-down", "hierarchical", "method", "starts", "points", "cluster", "recursively", "splits", "clusters", "opposite", "agglomerative", "bisecting"]}, {"id": "term-dm-control-suite", "t": "DM Control Suite", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "DeepMind Control Suite a set of continuous control tasks built on the MuJoCo physics engine. Provides standardized...", "l": "d", "k": ["control", "suite", "deepmind", "continuous", "tasks", "built", "mujoco", "physics", "engine", "provides", "standardized", "benchmarks", "evaluating", "reinforcement", "learning"]}, {"id": "term-dmd", "t": "DMD", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "Distribution Matching Distillation is a one-step diffusion image generation method that matches the output distribution...", "l": "d", "k": ["dmd", "distribution", "matching", "distillation", "one-step", "diffusion", "image", "generation", "method", "matches", "output", "multi-step", "teacher", "model"]}, {"id": "term-dna-computing", "t": "DNA Computing", "tg": ["Emerging", "Biological", "Research"], "d": "hardware", "x": "Computing approach using biological DNA molecules to perform calculations. While extremely early-stage offers massive...", "l": "d", "k": ["dna", "computing", "approach", "biological", "molecules", "perform", "calculations", "extremely", "early-stage", "offers", "massive", "parallelism", "certain", "combinatorial", "optimization"]}, {"id": "term-do-calculus-algorithm", "t": "Do-Calculus Algorithm", "tg": ["Algorithms", "Technical", "Causal", "History"], "d": "algorithms", "x": "A set of inference rules developed by Judea Pearl for determining whether a causal effect can be identified from...", "l": "d", "k": ["do-calculus", "algorithm", "inference", "rules", "developed", "judea", "pearl", "determining", "causal", "effect", "identified", "observational", "data", "uses", "directed"]}, {"id": "term-do-not-answer", "t": "Do-Not-Answer", "tg": ["Benchmark", "NLP", "Safety", "Evaluation"], "d": "datasets", "x": "A dataset of 939 prompts that responsible language models should refuse to answer. Used for evaluating safety...", "l": "d", "k": ["do-not-answer", "dataset", "prompts", "responsible", "language", "models", "refuse", "answer", "evaluating", "safety", "guardrails", "content", "policy", "compliance", "llms"]}, {"id": "term-docformer", "t": "DocFormer", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multi-modal Transformer for document understanding that uses novel multi-modal attention layers to combine text and...", "l": "d", "k": ["docformer", "multi-modal", "transformer", "document", "understanding", "uses", "novel", "attention", "layers", "combine", "text", "visual", "spatial", "features"]}, {"id": "term-docowl", "t": "DocOwl", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multimodal language model specialized for document understanding that processes diverse document types including...", "l": "d", "k": ["docowl", "multimodal", "language", "model", "specialized", "document", "understanding", "processes", "diverse", "types", "including", "forms", "tables", "charts", "slides"]}, {"id": "term-document-ai", "t": "Document AI", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "AI systems that understand and extract structured information from documents by combining OCR, layout analysis, and...", "l": "d", "k": ["document", "systems", "understand", "extract", "structured", "information", "documents", "combining", "ocr", "layout", "analysis", "language", "understanding", "process", "invoices"]}, {"id": "term-document-chunking", "t": "Document Chunking", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "The process of splitting larger documents into smaller text segments for individual embedding and indexing, balancing...", "l": "d", "k": ["document", "chunking", "process", "splitting", "larger", "documents", "smaller", "text", "segments", "individual", "embedding", "indexing", "balancing", "preserving", "semantic"]}, {"id": "term-document-embedding", "t": "Document Embedding", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A dense vector representation of an entire document that captures its overall semantic content, used for document...", "l": "d", "k": ["document", "embedding", "dense", "vector", "representation", "entire", "captures", "overall", "semantic", "content", "retrieval", "clustering", "similarity", "comparison", "tasks"]}, {"id": "term-document-qa", "t": "Document Q&amp;A", "tg": ["Application", "RAG"], "d": "general", "x": "Using AI to answer questions about specific documents or text. Often implemented with RAG to enable models to reference...", "l": "d", "k": ["document", "amp", "answer", "questions", "specific", "documents", "text", "implemented", "rag", "enable", "models", "reference", "sources", "rather", "relying"]}, {"id": "term-documentation-requirements-for-ai", "t": "Documentation Requirements for AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "Regulatory and best-practice requirements for documenting AI system design training data performance evaluation and...", "l": "d", "k": ["documentation", "requirements", "regulatory", "best-practice", "documenting", "system", "design", "training", "data", "performance", "evaluation", "known", "limitations", "essential", "transparency"]}, {"id": "term-docvqa", "t": "DocVQA", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "A document visual question answering dataset with questions about document images. Tests the ability to extract and...", "l": "d", "k": ["docvqa", "document", "visual", "question", "answering", "dataset", "questions", "images", "tests", "ability", "extract", "reason", "information", "layouts"]}, {"id": "term-dolly-20-dataset", "t": "Dolly 2.0 Dataset", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A dataset of 15000 instruction-following records generated by Databricks employees. One of the first commercially...", "l": "d", "k": ["dolly", "dataset", "instruction-following", "records", "generated", "databricks", "employees", "commercially", "usable", "datasets", "created", "humans"]}, {"id": "term-dolma", "t": "Dolma", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "An open pretraining corpus of 3 trillion tokens created by AI2 for training the OLMo language model. Combines web data...", "l": "d", "k": ["dolma", "open", "pretraining", "corpus", "trillion", "tokens", "created", "ai2", "training", "olmo", "language", "model", "combines", "web", "data"]}, {"id": "term-domain-adaptation", "t": "Domain Adaptation", "tg": ["Training", "Transfer Learning"], "d": "general", "x": "Techniques for adapting a model trained on one domain (e.g., general text) to perform well on another domain (e.g.,...", "l": "d", "k": ["domain", "adaptation", "techniques", "adapting", "model", "trained", "general", "text", "perform", "another", "medical", "legal", "limited", "target", "data"]}, {"id": "term-domain-adaptation-algorithm", "t": "Domain Adaptation Algorithm", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A transfer learning technique that adapts a model trained on a source domain to perform well on a different but related...", "l": "d", "k": ["domain", "adaptation", "algorithm", "transfer", "learning", "technique", "adapts", "model", "trained", "source", "perform", "different", "related", "target", "methods"]}, {"id": "term-domain-randomization", "t": "Domain Randomization", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A sim-to-real transfer technique that trains vision models on synthetic images with heavily randomized visual...", "l": "d", "k": ["domain", "randomization", "sim-to-real", "transfer", "technique", "trains", "vision", "models", "synthetic", "images", "heavily", "randomized", "visual", "properties", "lighting"]}, {"id": "term-domain-specific-prompting", "t": "Domain-Specific Prompting", "tg": ["Prompt Engineering", "Domain Adaptation"], "d": "general", "x": "The practice of crafting prompts that incorporate specialized vocabulary, conventions, constraints, and contextual...", "l": "d", "k": ["domain-specific", "prompting", "practice", "crafting", "prompts", "incorporate", "specialized", "vocabulary", "conventions", "constraints", "contextual", "knowledge", "particular", "specific", "field"]}, {"id": "term-domainnet", "t": "DomainNet", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A domain adaptation dataset containing 600000 images across 345 categories in 6 visual domains. Tests the ability of...", "l": "d", "k": ["domainnet", "domain", "adaptation", "dataset", "containing", "images", "across", "categories", "visual", "domains", "tests", "ability", "models", "generalize", "different"]}, {"id": "term-donald-hebb", "t": "Donald Hebb", "tg": ["History", "Pioneers"], "d": "history", "x": "Canadian neuropsychologist (1904-1985) who proposed Hebbian learning theory in his 1949 book The Organization of...", "l": "d", "k": ["donald", "hebb", "canadian", "neuropsychologist", "1904-1985", "proposed", "hebbian", "learning", "theory", "book", "organization", "behavior", "providing", "neurobiological", "basis"]}, {"id": "term-donut", "t": "Donut", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "Document Understanding Transformer is an OCR-free document understanding model that directly maps document images to...", "l": "d", "k": ["donut", "document", "understanding", "transformer", "ocr-free", "model", "directly", "maps", "images", "structured", "outputs", "without", "separate", "text", "detection"]}, {"id": "term-doping", "t": "Doping", "tg": ["Fabrication", "Manufacturing", "Process"], "d": "hardware", "x": "Process of intentionally adding impurity atoms to a semiconductor to modify its electrical properties. Creates the...", "l": "d", "k": ["doping", "process", "intentionally", "adding", "impurity", "atoms", "semiconductor", "modify", "electrical", "properties", "creates", "n-type", "p-type", "regions", "essential"]}, {"id": "term-dot-product-similarity", "t": "Dot Product Similarity", "tg": ["Vector Database", "Similarity"], "d": "general", "x": "A similarity measure computed as the sum of element-wise products of two vectors, equivalent to cosine similarity when...", "l": "d", "k": ["dot", "product", "similarity", "measure", "computed", "sum", "element-wise", "products", "vectors", "equivalent", "cosine", "normalized", "additionally", "capturing", "magnitude"]}, {"id": "term-dot-product-attention", "t": "Dot-Product Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that computes compatibility scores as the dot product between query and key vectors, scaled by...", "l": "d", "k": ["dot-product", "attention", "mechanism", "computes", "compatibility", "scores", "dot", "product", "query", "key", "vectors", "scaled", "square", "root", "dimension"]}, {"id": "term-dota", "t": "DOTA", "tg": ["Benchmark", "Computer Vision", "Remote Sensing"], "d": "datasets", "x": "A large-scale dataset for object detection in aerial images containing 188282 instances across 15 categories. Addresses...", "l": "d", "k": ["dota", "large-scale", "dataset", "object", "detection", "aerial", "images", "containing", "instances", "across", "categories", "addresses", "challenges", "unique", "overhead"]}, {"id": "term-double-dqn", "t": "Double DQN", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An extension of DQN that addresses overestimation bias by decoupling action selection from action evaluation, using the...", "l": "d", "k": ["double", "dqn", "extension", "addresses", "overestimation", "bias", "decoupling", "action", "selection", "evaluation", "online", "network", "select", "actions", "target"]}, {"id": "term-double-dqn-algorithm", "t": "Double DQN Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "An improvement to DQN that decouples action selection from value estimation to reduce overestimation bias. Uses the...", "l": "d", "k": ["double", "dqn", "algorithm", "improvement", "decouples", "action", "selection", "value", "estimation", "reduce", "overestimation", "bias", "uses", "online", "network"]}, {"id": "term-double-q-learning", "t": "Double Q-Learning", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A reinforcement learning variant that addresses the overestimation bias in standard Q-learning by using two separate...", "l": "d", "k": ["double", "q-learning", "reinforcement", "learning", "variant", "addresses", "overestimation", "bias", "standard", "separate", "value", "estimators", "selects", "action", "evaluates"]}, {"id": "term-doubly-robust-estimation", "t": "Doubly Robust Estimation", "tg": ["Algorithms", "Technical", "Causal"], "d": "algorithms", "x": "A causal inference method that combines an outcome model with a treatment model to estimate causal effects. Produces...", "l": "d", "k": ["doubly", "robust", "estimation", "causal", "inference", "method", "combines", "outcome", "model", "treatment", "estimate", "effects", "produces", "consistent", "estimates"]}, {"id": "term-douglas-engelbart", "t": "Douglas Engelbart", "tg": ["History", "Pioneers"], "d": "history", "x": "American engineer and inventor who demonstrated the first computer mouse hypertext video conferencing and collaborative...", "l": "d", "k": ["douglas", "engelbart", "american", "engineer", "inventor", "demonstrated", "computer", "mouse", "hypertext", "video", "conferencing", "collaborative", "real-time", "editing", "mother"]}, {"id": "term-douglas-hofstadter", "t": "Douglas Hofstadter", "tg": ["History", "Pioneers"], "d": "history", "x": "American cognitive scientist and author of Goedel Escher Bach: An Eternal Golden Braid (1979) which explores...", "l": "d", "k": ["douglas", "hofstadter", "american", "cognitive", "scientist", "author", "goedel", "escher", "bach", "eternal", "golden", "braid", "explores", "connections", "mathematics"]}, {"id": "term-douglas-lenat", "t": "Douglas Lenat", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1950-2023) who created the Cyc project in 1984, an ambitious effort to build a...", "l": "d", "k": ["douglas", "lenat", "american", "computer", "scientist", "1950-2023", "created", "cyc", "project", "ambitious", "effort", "build", "comprehensive", "ontology", "common-sense"]}, {"id": "term-dpg-bench", "t": "DPG-Bench", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "Dense Prompt Graph Benchmark for evaluating text-to-image models on detailed long-form prompts. Tests the ability to...", "l": "d", "k": ["dpg-bench", "dense", "prompt", "graph", "benchmark", "evaluating", "text-to-image", "models", "detailed", "long-form", "prompts", "tests", "ability", "follow", "complex"]}, {"id": "term-dpo", "t": "DPO (Direct Preference Optimization)", "tg": ["Training", "Alignment"], "d": "safety", "x": "A simpler alternative to RLHF for aligning language models. Directly optimizes the model using preference data without...", "l": "d", "k": ["dpo", "direct", "preference", "optimization", "simpler", "alternative", "rlhf", "aligning", "language", "models", "directly", "optimizes", "model", "data", "without"]}, {"id": "term-dqn-deep-q-network", "t": "DQN (Deep Q-Network)", "tg": ["History", "Systems"], "d": "history", "x": "A deep reinforcement learning architecture developed by DeepMind in 2013 that combines Q-learning with deep neural...", "l": "d", "k": ["dqn", "deep", "q-network", "reinforcement", "learning", "architecture", "developed", "deepmind", "combines", "q-learning", "neural", "networks", "learned", "play", "atari"]}, {"id": "term-dqn-algorithm", "t": "DQN Algorithm", "tg": ["Algorithms", "Fundamentals", "RL"], "d": "algorithms", "x": "Deep Q-Network combines Q-learning with deep neural networks to learn policies directly from high-dimensional sensory...", "l": "d", "k": ["dqn", "algorithm", "deep", "q-network", "combines", "q-learning", "neural", "networks", "learn", "policies", "directly", "high-dimensional", "sensory", "input", "uses"]}, {"id": "term-dragonfly-algorithm", "t": "Dragonfly Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A swarm intelligence algorithm inspired by the static and dynamic swarming behaviors of dragonflies. Models separation...", "l": "d", "k": ["dragonfly", "algorithm", "swarm", "intelligence", "inspired", "static", "dynamic", "swarming", "behaviors", "dragonflies", "models", "separation", "alignment", "cohesion", "attraction"]}, {"id": "term-dragonfly-topology", "t": "Dragonfly Topology", "tg": ["Networking", "Topology"], "d": "hardware", "x": "Network topology that groups switches into fully connected subgroups with global links between groups. Used in...", "l": "d", "k": ["dragonfly", "topology", "network", "groups", "switches", "fully", "connected", "subgroups", "global", "links", "supercomputers", "provide", "high", "bandwidth", "fewer"]}, {"id": "term-dram", "t": "DRAM", "tg": ["Memory", "Fundamentals"], "d": "hardware", "x": "Dynamic Random Access Memory that stores each bit in a capacitor requiring periodic refresh. Forms the main memory in...", "l": "d", "k": ["dram", "dynamic", "random", "access", "memory", "stores", "bit", "capacitor", "requiring", "periodic", "refresh", "forms", "main", "computing", "systems"]}, {"id": "term-drawbench", "t": "DrawBench", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A benchmark of text prompts for evaluating text-to-image generation models across categories like color spatial...", "l": "d", "k": ["drawbench", "benchmark", "text", "prompts", "evaluating", "text-to-image", "generation", "models", "across", "categories", "color", "spatial", "relationships", "compositionality"]}, {"id": "term-dreambooth", "t": "DreamBooth", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A fine-tuning technique that personalizes diffusion models to generate images of specific subjects by training on just...", "l": "d", "k": ["dreambooth", "fine-tuning", "technique", "personalizes", "diffusion", "models", "generate", "images", "specific", "subjects", "training", "reference", "unique", "identifier", "token"]}, {"id": "term-dreamer", "t": "Dreamer", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A model-based RL agent that learns a world model in latent space and trains its policy entirely through imagined...", "l": "d", "k": ["dreamer", "model-based", "agent", "learns", "world", "model", "latent", "space", "trains", "policy", "entirely", "imagined", "trajectories", "generated", "achieves"]}, {"id": "term-dreamer-algorithm", "t": "Dreamer Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A model-based reinforcement learning agent that learns a world model from experience and uses it to imagine future...", "l": "d", "k": ["dreamer", "algorithm", "model-based", "reinforcement", "learning", "agent", "learns", "world", "model", "experience", "uses", "imagine", "future", "trajectories", "policy"]}, {"id": "term-dreamfusion", "t": "DreamFusion", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A text-to-3D generation method that uses a pre-trained 2D diffusion model as a loss function to optimize a Neural...", "l": "d", "k": ["dreamfusion", "text-to-3d", "generation", "method", "uses", "pre-trained", "diffusion", "model", "loss", "function", "optimize", "neural", "radiance", "field", "text"]}, {"id": "term-drew-mcdermott", "t": "Drew McDermott", "tg": ["History", "Pioneers"], "d": "history", "x": "American AI researcher at Yale who made significant contributions to AI planning and robotics. Known for his 1976...", "l": "d", "k": ["drew", "mcdermott", "american", "researcher", "yale", "significant", "contributions", "planning", "robotics", "known", "critique", "overconfidence", "paper", "artificial", "intelligence"]}, {"id": "term-drivegpt4", "t": "DriveGPT4", "tg": ["Models", "Technical", "Autonomous", "Vision", "NLP"], "d": "models", "x": "A multimodal large language model for autonomous driving that interprets driving scenes and generates human-readable...", "l": "d", "k": ["drivegpt4", "multimodal", "large", "language", "model", "autonomous", "driving", "interprets", "scenes", "generates", "human-readable", "explanations", "behavior"]}, {"id": "term-drivelm", "t": "DriveLM", "tg": ["Models", "Technical", "Autonomous", "Vision", "NLP"], "d": "models", "x": "A driving-specific language model that provides question-answering capabilities for graph-structured reasoning about...", "l": "d", "k": ["drivelm", "driving-specific", "language", "model", "provides", "question-answering", "capabilities", "graph-structured", "reasoning", "autonomous", "driving", "scenarios"]}, {"id": "term-drop", "t": "DROP", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "Discrete Reasoning Over Paragraphs a reading comprehension benchmark requiring numerical reasoning over text. Questions...", "l": "d", "k": ["drop", "discrete", "reasoning", "paragraphs", "reading", "comprehension", "benchmark", "requiring", "numerical", "text", "questions", "involve", "counting", "sorting", "comparison"]}, {"id": "term-dropblock", "t": "DropBlock", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A structured dropout method that drops contiguous regions of feature maps rather than individual elements. More...", "l": "d", "k": ["dropblock", "structured", "dropout", "method", "drops", "contiguous", "regions", "feature", "maps", "rather", "individual", "elements", "effective", "standard", "convolutional"]}, {"id": "term-dropconnect", "t": "DropConnect", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A regularization method that randomly sets individual weights rather than activations to zero during training....", "l": "d", "k": ["dropconnect", "regularization", "method", "randomly", "sets", "individual", "weights", "rather", "activations", "zero", "training", "generalizes", "dropout", "operating", "weight"]}, {"id": "term-dropout", "t": "Dropout", "tg": ["Training", "Regularization"], "d": "algorithms", "x": "A regularization technique that randomly deactivates neurons during training. Prevents overfitting by forcing the...", "l": "d", "k": ["dropout", "regularization", "technique", "randomly", "deactivates", "neurons", "training", "prevents", "overfitting", "forcing", "network", "learn", "robust", "features"]}, {"id": "term-dropout-technique", "t": "Dropout Technique", "tg": ["History", "Milestones"], "d": "history", "x": "A regularization method proposed by Hinton et al. in 2012 that randomly deactivates neurons during training to prevent...", "l": "d", "k": ["dropout", "technique", "regularization", "method", "proposed", "hinton", "randomly", "deactivates", "neurons", "training", "prevent", "overfitting", "becoming", "widely", "techniques"]}, {"id": "term-droppath", "t": "DropPath", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A regularization method for networks with multiple parallel paths that randomly drops entire residual branches during...", "l": "d", "k": ["droppath", "regularization", "method", "networks", "multiple", "parallel", "paths", "randomly", "drops", "entire", "residual", "branches", "training", "improving", "generalization"]}, {"id": "term-druggpt", "t": "DrugGPT", "tg": ["Models", "Technical", "NLP", "Medical", "Scientific"], "d": "models", "x": "A specialized language model for drug discovery that generates molecular structures and predicts drug-target...", "l": "d", "k": ["druggpt", "specialized", "language", "model", "drug", "discovery", "generates", "molecular", "structures", "predicts", "drug-target", "interactions", "natural", "descriptions"]}, {"id": "term-ds-1000", "t": "DS-1000", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A code generation benchmark of 1000 data science problems spanning seven Python libraries including NumPy Pandas and...", "l": "d", "k": ["ds-1000", "code", "generation", "benchmark", "data", "science", "problems", "spanning", "seven", "python", "libraries", "including", "numpy", "pandas", "scikit-learn"]}, {"id": "term-dstc", "t": "DSTC", "tg": ["Benchmark", "NLP", "Dialogue"], "d": "datasets", "x": "Dialogue State Tracking Challenge a series of shared tasks and datasets for evaluating dialogue state tracking in...", "l": "d", "k": ["dstc", "dialogue", "state", "tracking", "challenge", "series", "shared", "tasks", "datasets", "evaluating", "task-oriented", "systems", "covers", "multiple", "domains"]}, {"id": "term-dtd", "t": "DTD", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "The Describable Textures Dataset containing 5640 images across 47 texture categories annotated with human-describable...", "l": "d", "k": ["dtd", "describable", "textures", "dataset", "containing", "images", "across", "texture", "categories", "annotated", "human-describable", "attributes", "recognition", "material", "classification"]}, {"id": "term-dual-simplex-method", "t": "Dual Simplex Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "A variant of the simplex algorithm that works with dual feasibility while achieving primal feasibility. Particularly...", "l": "d", "k": ["dual", "simplex", "method", "variant", "algorithm", "works", "feasibility", "achieving", "primal", "particularly", "useful", "re-optimization", "adding", "constraints", "changing"]}, {"id": "term-dual-use-concern", "t": "Dual-Use Concern", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The recognition that AI technologies developed for beneficial purposes can also be used for harmful applications....", "l": "d", "k": ["dual-use", "concern", "recognition", "technologies", "developed", "beneficial", "purposes", "harmful", "applications", "requires", "researchers", "developers", "consider", "potential", "misuse"]}, {"id": "term-dual-use-technology", "t": "Dual-Use Technology", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "Technology that can be used for both beneficial and harmful purposes, a concept particularly relevant to AI...", "l": "d", "k": ["dual-use", "technology", "beneficial", "harmful", "purposes", "concept", "particularly", "relevant", "capabilities", "language", "generation", "computer", "vision", "autonomous", "systems"]}, {"id": "term-dueling-dqn", "t": "Dueling DQN", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A DQN architecture that separately estimates the state value function and the advantage function, combining them to...", "l": "d", "k": ["dueling", "dqn", "architecture", "separately", "estimates", "state", "value", "function", "advantage", "combining", "produce", "q-values", "decomposition", "allows", "network"]}, {"id": "term-dueling-dqn-algorithm", "t": "Dueling DQN Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A DQN architecture that separates the estimation of state value and action advantage into two streams. The combined...", "l": "d", "k": ["dueling", "dqn", "algorithm", "architecture", "separates", "estimation", "state", "value", "action", "advantage", "streams", "combined", "output", "better", "identifies"]}, {"id": "term-durand-kerner-method", "t": "Durand-Kerner Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An iterative algorithm for finding all roots of a polynomial simultaneously. Updates all root approximations in...", "l": "d", "k": ["durand-kerner", "method", "iterative", "algorithm", "finding", "roots", "polynomial", "simultaneously", "updates", "root", "approximations", "parallel", "step", "converges", "quadratically"]}, {"id": "term-durbin-watson-test", "t": "Durbin-Watson Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A statistical test for detecting first-order autocorrelation in the residuals of a regression analysis. Values near 2...", "l": "d", "k": ["durbin-watson", "test", "statistical", "detecting", "first-order", "autocorrelation", "residuals", "regression", "analysis", "values", "near", "indicate", "suggest", "positive", "negative"]}, {"id": "term-dvqa", "t": "DVQA", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "Document Visual QA a benchmark for answering questions about bar charts. Tests understanding of chart elements legend...", "l": "d", "k": ["dvqa", "document", "visual", "benchmark", "answering", "questions", "bar", "charts", "tests", "understanding", "chart", "elements", "legend", "interpretation", "quantitative"]}, {"id": "term-dyna-architecture", "t": "Dyna Architecture", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A model-based RL framework that integrates direct learning from real experience with planning through simulated...", "l": "d", "k": ["dyna", "architecture", "model-based", "framework", "integrates", "direct", "learning", "real", "experience", "planning", "simulated", "generated", "learned", "environment", "model"]}, {"id": "term-dynabench", "t": "Dynabench", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A platform for dynamic benchmarking where humans create adversarial examples targeting model weaknesses. Addresses the...", "l": "d", "k": ["dynabench", "platform", "dynamic", "benchmarking", "humans", "create", "adversarial", "examples", "targeting", "model", "weaknesses", "addresses", "problem", "static", "benchmarks"]}, {"id": "term-dynamic-consent", "t": "Dynamic Consent", "tg": ["Safety", "Ethics"], "d": "safety", "x": "A consent model that allows individuals to update their data sharing preferences over time as AI systems evolve and new...", "l": "d", "k": ["dynamic", "consent", "model", "allows", "individuals", "update", "data", "sharing", "preferences", "time", "systems", "evolve", "uses", "emerge", "contrasts"]}, {"id": "term-dynamic-loss-scaling", "t": "Dynamic Loss Scaling", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "An automatic loss scaling strategy that adapts the scaling factor during training, increasing it when no overflow is...", "l": "d", "k": ["dynamic", "loss", "scaling", "automatic", "strategy", "adapts", "factor", "training", "increasing", "overflow", "detected", "decreasing", "gradients", "eliminates", "need"]}, {"id": "term-dynamic-programming", "t": "Dynamic Programming", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "An algorithmic technique that solves complex problems by breaking them into simpler overlapping subproblems and storing...", "l": "d", "k": ["dynamic", "programming", "algorithmic", "technique", "solves", "complex", "problems", "breaking", "simpler", "overlapping", "subproblems", "storing", "solutions", "avoid", "redundant"]}, {"id": "term-dynamic-prompting", "t": "Dynamic Prompting", "tg": ["Prompt Engineering", "Adaptive"], "d": "general", "x": "A prompting approach where the content, structure, or examples within a prompt are programmatically adjusted at runtime...", "l": "d", "k": ["dynamic", "prompting", "approach", "content", "structure", "examples", "within", "prompt", "programmatically", "adjusted", "runtime", "based", "input", "query", "user"]}, {"id": "term-dynamic-quantization", "t": "Dynamic Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A quantization approach that computes scaling factors on-the-fly during inference based on the actual range of...", "l": "d", "k": ["dynamic", "quantization", "approach", "computes", "scaling", "factors", "on-the-fly", "inference", "based", "actual", "range", "activation", "values", "encountered", "adapts"]}, {"id": "term-e5", "t": "E5", "tg": ["Models", "Technical"], "d": "models", "x": "A family of text embedding models trained with contrastive learning on large-scale text pairs. Achieves strong...", "l": "e", "k": ["family", "text", "embedding", "models", "trained", "contrastive", "learning", "large-scale", "pairs", "achieves", "strong", "performance", "across", "retrieval", "clustering"]}, {"id": "term-eagle", "t": "Eagle", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A vision-language model that uses a mixture of vision encoders to capture both low-level and high-level visual features...", "l": "e", "k": ["eagle", "vision-language", "model", "uses", "mixture", "vision", "encoders", "capture", "low-level", "high-level", "visual", "features", "improved", "multimodal", "understanding"]}, {"id": "term-earley-parser", "t": "Earley Parser", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "A chart parsing algorithm for context-free grammars that can handle all context-free languages including ambiguous...", "l": "e", "k": ["earley", "parser", "chart", "parsing", "algorithm", "context-free", "grammars", "handle", "languages", "including", "ambiguous", "runs", "time", "general", "unambiguous"]}, {"id": "term-early-stopping", "t": "Early Stopping", "tg": ["Training", "Regularization"], "d": "algorithms", "x": "A regularization technique that stops training when performance on a validation set stops improving. Prevents...", "l": "e", "k": ["early", "stopping", "regularization", "technique", "stops", "training", "performance", "validation", "improving", "prevents", "overfitting", "saves", "computational", "resources"]}, {"id": "term-early-stopping-criterion", "t": "Early Stopping Criterion", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A regularization technique that monitors validation performance during training and stops when it begins to...", "l": "e", "k": ["early", "stopping", "criterion", "regularization", "technique", "monitors", "validation", "performance", "training", "stops", "begins", "deteriorate", "prevents", "overfitting", "selecting"]}, {"id": "term-earth-movers-distance", "t": "Earth Mover's Distance", "tg": ["Statistics", "Metrics"], "d": "datasets", "x": "A metric for comparing probability distributions based on the minimum amount of work needed to transform one...", "l": "e", "k": ["earth", "mover", "distance", "metric", "comparing", "probability", "distributions", "based", "minimum", "amount", "work", "needed", "transform", "distribution", "mass"]}, {"id": "term-earth-simulator", "t": "Earth Simulator", "tg": ["Historical", "Supercomputer", "Japan"], "d": "hardware", "x": "Japanese supercomputer that was the fastest in the world from 2002 to 2004. Built by NEC for climate modeling and earth...", "l": "e", "k": ["earth", "simulator", "japanese", "supercomputer", "fastest", "world", "built", "nec", "climate", "modeling", "science", "research", "custom", "vector", "processors"]}, {"id": "term-ecc-error-correcting-code", "t": "ECC (Error Correcting Code)", "tg": ["Reliability", "Memory", "Data Integrity"], "d": "hardware", "x": "Error detection and correction mechanism used in memory and data transmission. ECC in GPU memory is critical for...", "l": "e", "k": ["ecc", "error", "correcting", "code", "detection", "correction", "mechanism", "memory", "data", "transmission", "gpu", "critical", "ensuring", "numerical", "accuracy"]}, {"id": "term-ecc-memory", "t": "ECC Memory", "tg": ["Memory", "Data Center", "Reliability"], "d": "hardware", "x": "Error-Correcting Code memory that detects and corrects single-bit errors and detects multi-bit errors. Required in data...", "l": "e", "k": ["ecc", "memory", "error-correcting", "code", "detects", "corrects", "single-bit", "errors", "multi-bit", "required", "data", "center", "systems", "ensure", "computational"]}, {"id": "term-eccv", "t": "ECCV", "tg": ["History", "Conferences"], "d": "history", "x": "The European Conference on Computer Vision held biennially since 1990. One of the top three computer vision conferences...", "l": "e", "k": ["eccv", "european", "conference", "computer", "vision", "held", "biennially", "top", "conferences", "alongside", "cvpr", "iccv", "known", "publishing", "influential"]}, {"id": "term-echo-state-network", "t": "Echo State Network", "tg": ["Models", "Technical"], "d": "models", "x": "A recurrent neural network where the recurrent layer is a large randomly generated reservoir with fixed weights. Only...", "l": "e", "k": ["echo", "state", "network", "recurrent", "neural", "layer", "large", "randomly", "generated", "reservoir", "fixed", "weights", "output", "trained", "part"]}, {"id": "term-edge-ai", "t": "Edge AI", "tg": ["Deployment", "Architecture"], "d": "models", "x": "Running AI models locally on devices (phones, IoT) rather than in the cloud. Enables faster responses, offline...", "l": "e", "k": ["edge", "running", "models", "locally", "devices", "phones", "iot", "rather", "cloud", "enables", "faster", "responses", "offline", "operation", "better"]}, {"id": "term-edge-case-safety", "t": "Edge Case Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "The challenge of ensuring AI systems behave safely in unusual or extreme situations that were not well represented in...", "l": "e", "k": ["edge", "case", "safety", "challenge", "ensuring", "systems", "behave", "safely", "unusual", "extreme", "situations", "were", "represented", "training", "data"]}, {"id": "term-edge-computing", "t": "Edge Computing", "tg": ["Edge", "Architecture"], "d": "hardware", "x": "Computing paradigm that processes data near the source of generation rather than in a centralized data center. Reduces...", "l": "e", "k": ["edge", "computing", "paradigm", "processes", "data", "near", "source", "generation", "rather", "centralized", "center", "reduces", "latency", "bandwidth", "needs"]}, {"id": "term-edge-inference", "t": "Edge Inference", "tg": ["Inference Infrastructure", "Hardware"], "d": "hardware", "x": "Running AI model inference directly on edge devices (phones, IoT sensors, embedded systems) rather than in the cloud,...", "l": "e", "k": ["edge", "inference", "running", "model", "directly", "devices", "phones", "iot", "sensors", "embedded", "systems", "rather", "cloud", "reducing", "latency"]}, {"id": "term-edge-tpu", "t": "Edge TPU", "tg": ["Edge", "Google", "TPU"], "d": "hardware", "x": "Google custom ASIC designed for running TensorFlow Lite models at the edge. Provides 4 TOPS of inference performance in...", "l": "e", "k": ["edge", "tpu", "google", "custom", "asic", "designed", "running", "tensorflow", "lite", "models", "provides", "tops", "inference", "performance", "compact"]}, {"id": "term-edit-distance", "t": "Edit Distance", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A family of metrics quantifying the minimum number of operations required to transform one string into another, with...", "l": "e", "k": ["edit", "distance", "family", "metrics", "quantifying", "minimum", "number", "operations", "required", "transform", "string", "another", "variants", "including", "levenshtein"]}, {"id": "term-edit-distance-algorithm", "t": "Edit Distance Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP"], "d": "algorithms", "x": "A family of algorithms that compute the minimum number of operations required to transform one sequence into another....", "l": "e", "k": ["edit", "distance", "algorithm", "family", "algorithms", "compute", "minimum", "number", "operations", "required", "transform", "sequence", "another", "variants", "include"]}, {"id": "term-editeval", "t": "EditEval", "tg": ["Benchmark", "Code", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating code editing capabilities of language models. Tests the ability to modify existing code...", "l": "e", "k": ["editeval", "benchmark", "evaluating", "code", "editing", "capabilities", "language", "models", "tests", "ability", "modify", "existing", "based", "natural", "instructions"]}, {"id": "term-edmonds-blossom-algorithm", "t": "Edmonds' Blossom Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm for finding maximum weight matching in general (non-bipartite) graphs. Handles odd cycles by contracting...", "l": "e", "k": ["edmonds", "blossom", "algorithm", "finding", "maximum", "weight", "matching", "general", "non-bipartite", "graphs", "handles", "odd", "cycles", "contracting", "blossoms"]}, {"id": "term-edmonds-karp-algorithm", "t": "Edmonds-Karp Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An implementation of the Ford-Fulkerson method that uses breadth-first search to find the shortest augmenting path....", "l": "e", "k": ["edmonds-karp", "algorithm", "implementation", "ford-fulkerson", "method", "uses", "breadth-first", "search", "find", "shortest", "augmenting", "path", "guarantees", "time", "complexity"]}, {"id": "term-edmund-clarke", "t": "Edmund Clarke", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who pioneered model checking a technique for automatically verifying the correctness of...", "l": "e", "k": ["edmund", "clarke", "american", "computer", "scientist", "pioneered", "model", "checking", "technique", "automatically", "verifying", "correctness", "finite-state", "systems", "received"]}, {"id": "term-edsac", "t": "EDSAC", "tg": ["Historical", "Computer", "Pioneer"], "d": "hardware", "x": "Electronic Delay Storage Automatic Calculator built at the University of Cambridge in 1949. One of the first practical...", "l": "e", "k": ["edsac", "electronic", "delay", "storage", "automatic", "calculator", "built", "university", "cambridge", "practical", "stored-program", "computers", "ran", "graphical", "computer"]}, {"id": "term-edvac", "t": "EDVAC", "tg": ["History", "Systems"], "d": "history", "x": "The Electronic Discrete Variable Automatic Computer designed in the 1940s was one of the first stored-program...", "l": "e", "k": ["edvac", "electronic", "discrete", "variable", "automatic", "computer", "designed", "1940s", "stored-program", "computers", "john", "von", "neumann", "draft", "report"]}, {"id": "term-edward-feigenbaum", "t": "Edward Feigenbaum", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist known as the father of expert systems, who led the development of DENDRAL and pioneered...", "l": "e", "k": ["edward", "feigenbaum", "american", "computer", "scientist", "known", "father", "expert", "systems", "led", "development", "dendral", "pioneered", "knowledge", "engineering"]}, {"id": "term-edward-shortliffe", "t": "Edward Shortliffe", "tg": ["History", "Pioneers"], "d": "history", "x": "American biomedical informatician who developed MYCIN one of the most influential early expert systems for medical...", "l": "e", "k": ["edward", "shortliffe", "american", "biomedical", "informatician", "developed", "mycin", "influential", "early", "expert", "systems", "medical", "diagnosis", "work", "demonstrated"]}, {"id": "term-effect-size", "t": "Effect Size", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A quantitative measure of the magnitude of a phenomenon or the practical significance of a result, independent of...", "l": "e", "k": ["effect", "size", "quantitative", "measure", "magnitude", "phenomenon", "practical", "significance", "result", "independent", "sample", "common", "measures", "include", "cohen"]}, {"id": "term-effective-batch-size", "t": "Effective Batch Size", "tg": ["Training", "Parameter", "Metric"], "d": "hardware", "x": "Total number of samples processed per optimizer step accounting for data parallelism and gradient accumulation. Equals...", "l": "e", "k": ["effective", "batch", "size", "total", "number", "samples", "processed", "per", "optimizer", "step", "accounting", "data", "parallelism", "gradient", "accumulation"]}, {"id": "term-efficiency", "t": "Efficiency", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A property of a statistical estimator related to how much information from the data it uses. An efficient estimator...", "l": "e", "k": ["efficiency", "property", "statistical", "estimator", "related", "information", "data", "uses", "efficient", "achieves", "lowest", "possible", "variance", "among", "unbiased"]}, {"id": "term-efficientnet", "t": "EfficientNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A family of CNN models that use compound scaling to uniformly scale network width, depth, and resolution using a fixed...", "l": "e", "k": ["efficientnet", "family", "cnn", "models", "compound", "scaling", "uniformly", "scale", "network", "width", "depth", "resolution", "fixed", "coefficients", "achieving"]}, {"id": "term-efficientnetv2", "t": "EfficientNetV2", "tg": ["Models", "Technical"], "d": "models", "x": "An improved version of EfficientNet that uses progressive training and fused MBConv blocks for faster training....", "l": "e", "k": ["efficientnetv2", "improved", "version", "efficientnet", "uses", "progressive", "training", "fused", "mbconv", "blocks", "faster", "achieves", "better", "accuracy", "speed"]}, {"id": "term-efficientvit", "t": "EfficientViT", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An efficient vision Transformer architecture that uses cascaded group attention for high-speed image classification and...", "l": "e", "k": ["efficientvit", "efficient", "vision", "transformer", "architecture", "uses", "cascaded", "group", "attention", "high-speed", "image", "classification", "segmentation", "resource-constrained", "devices"]}, {"id": "term-ego4d", "t": "Ego4D", "tg": ["Benchmark", "Video", "Multimodal"], "d": "datasets", "x": "A massive egocentric video dataset containing 3670 hours of daily life activity from over 900 participants across 74...", "l": "e", "k": ["ego4d", "massive", "egocentric", "video", "dataset", "containing", "hours", "daily", "life", "activity", "participants", "across", "locations", "worldwide", "advances"]}, {"id": "term-egoschema", "t": "EgoSchema", "tg": ["Benchmark", "Video", "Multimodal"], "d": "datasets", "x": "A long-form video question answering benchmark derived from Ego4D testing temporal understanding over 3-minute...", "l": "e", "k": ["egoschema", "long-form", "video", "question", "answering", "benchmark", "derived", "ego4d", "testing", "temporal", "understanding", "3-minute", "egocentric", "clips"]}, {"id": "term-eigendecomposition", "t": "Eigendecomposition", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "The factorization of a square matrix into eigenvalues and eigenvectors revealing the directions along which the...", "l": "e", "k": ["eigendecomposition", "factorization", "square", "matrix", "eigenvalues", "eigenvectors", "revealing", "directions", "along", "transformation", "acts", "simple", "scaling", "fundamental", "pca"]}, {"id": "term-einsum", "t": "Einsum", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Einstein Summation notation is a compact way to express multi-dimensional array operations including matrix...", "l": "e", "k": ["einsum", "einstein", "summation", "notation", "compact", "express", "multi-dimensional", "array", "operations", "including", "matrix", "multiplication", "outer", "products", "contractions"]}, {"id": "term-el-capitan-supercomputer", "t": "El Capitan Supercomputer", "tg": ["Supercomputer", "AMD", "Exascale"], "d": "hardware", "x": "Exascale supercomputer at Lawrence Livermore National Laboratory using AMD EPYC CPUs and AMD Instinct MI300A APUs....", "l": "e", "k": ["capitan", "supercomputer", "exascale", "lawrence", "livermore", "national", "laboratory", "amd", "epyc", "cpus", "instinct", "mi300a", "apus", "designed", "nuclear"]}, {"id": "term-elastic-fabric-adapter", "t": "Elastic Fabric Adapter", "tg": ["Networking", "AWS", "Cloud"], "d": "hardware", "x": "AWS custom network interface providing low-latency high-bandwidth networking for distributed AI training. Enables...", "l": "e", "k": ["elastic", "fabric", "adapter", "aws", "custom", "network", "interface", "providing", "low-latency", "high-bandwidth", "networking", "distributed", "training", "enables", "efficient"]}, {"id": "term-elastic-net", "t": "Elastic Net", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A regularization method that linearly combines L1 and L2 penalty terms, balancing feature selection (sparsity) with...", "l": "e", "k": ["elastic", "net", "regularization", "method", "linearly", "combines", "penalty", "terms", "balancing", "feature", "selection", "sparsity", "weight", "shrinkage", "mixing"]}, {"id": "term-elastic-training", "t": "Elastic Training", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A distributed training approach that can dynamically scale the number of workers up or down during a training run...", "l": "e", "k": ["elastic", "training", "distributed", "approach", "dynamically", "scale", "number", "workers", "down", "run", "without", "requiring", "restart", "handles", "node"]}, {"id": "term-elastic-weight-consolidation", "t": "Elastic Weight Consolidation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A continual learning technique that slows down learning on weights important for previous tasks. Uses the diagonal of...", "l": "e", "k": ["elastic", "weight", "consolidation", "continual", "learning", "technique", "slows", "down", "weights", "important", "previous", "tasks", "uses", "diagonal", "fisher"]}, {"id": "term-elbow-method", "t": "Elbow Method", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A heuristic for selecting the optimal number of clusters by plotting the within-cluster sum of squares against the...", "l": "e", "k": ["elbow", "method", "heuristic", "selecting", "optimal", "number", "clusters", "plotting", "within-cluster", "sum", "squares", "against", "identifying", "point", "additional"]}, {"id": "term-electra", "t": "ELECTRA", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A pretraining method that trains a discriminator to detect tokens replaced by a small generator network, providing more...", "l": "e", "k": ["electra", "pretraining", "method", "trains", "discriminator", "detect", "tokens", "replaced", "small", "generator", "network", "providing", "efficient", "training", "masked"]}, {"id": "term-electromagnetic-vulnerability-of-ai", "t": "Electromagnetic Vulnerability of AI", "tg": ["Safety", "Technical"], "d": "safety", "x": "The susceptibility of AI hardware and systems to electromagnetic interference or attack. Relevant for safety-critical...", "l": "e", "k": ["electromagnetic", "vulnerability", "susceptibility", "hardware", "systems", "interference", "attack", "relevant", "safety-critical", "deployments", "disruption", "cause", "dangerous", "failures"]}, {"id": "term-electronic-design-automation", "t": "Electronic Design Automation", "tg": ["Manufacturing", "Software", "Design"], "d": "hardware", "x": "Software tools used to design and verify integrated circuits including AI chips. Companies like Synopsys Cadence and...", "l": "e", "k": ["electronic", "design", "automation", "software", "tools", "verify", "integrated", "circuits", "including", "chips", "companies", "synopsys", "cadence", "siemens", "eda"]}, {"id": "term-eleutherai", "t": "EleutherAI", "tg": ["History", "Organizations"], "d": "history", "x": "A grassroots collective of researchers founded in 2020 dedicated to open-source AI research. EleutherAI developed...", "l": "e", "k": ["eleutherai", "grassroots", "collective", "researchers", "founded", "dedicated", "open-source", "research", "developed", "gpt-neo", "gpt-j", "gpt-neox", "demonstrating", "large", "language"]}, {"id": "term-eli5", "t": "ELI5", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Explain Like Im 5 a long-form question answering dataset from Reddit where answers explain complex topics simply. Tests...", "l": "e", "k": ["eli5", "explain", "long-form", "question", "answering", "dataset", "reddit", "answers", "complex", "topics", "simply", "tests", "ability", "generate", "detailed"]}, {"id": "term-elias-gamma-coding", "t": "Elias Gamma Coding", "tg": ["Algorithms", "Technical", "Information Theory"], "d": "algorithms", "x": "A universal code for encoding positive integers where the code length depends only on the magnitude of the integer....", "l": "e", "k": ["elias", "gamma", "coding", "universal", "code", "encoding", "positive", "integers", "length", "depends", "magnitude", "integer", "encodes", "number", "binary"]}, {"id": "term-eliciting-latent-knowledge", "t": "Eliciting Latent Knowledge", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "A research problem in AI alignment focused on extracting truthful information from a model that may have learned to...", "l": "e", "k": ["eliciting", "latent", "knowledge", "research", "problem", "alignment", "focused", "extracting", "truthful", "information", "model", "learned", "represent", "world", "accurately"]}, {"id": "term-eligibility-trace", "t": "Eligibility Trace", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A decaying memory of recently visited states used in TD(lambda) and other RL algorithms to distribute credit backward...", "l": "e", "k": ["eligibility", "trace", "decaying", "memory", "recently", "visited", "states", "lambda", "algorithms", "distribute", "credit", "backward", "time", "traces", "enable"]}, {"id": "term-eligibility-traces", "t": "Eligibility Traces", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A mechanism in reinforcement learning that bridges Monte Carlo and temporal difference methods by maintaining a trace...", "l": "e", "k": ["eligibility", "traces", "mechanism", "reinforcement", "learning", "bridges", "monte", "carlo", "temporal", "difference", "methods", "maintaining", "trace", "recently", "visited"]}, {"id": "term-eliza", "t": "ELIZA", "tg": ["History", "Milestones"], "d": "history", "x": "A natural language processing program created by Joseph Weizenbaum at MIT in 1966 that simulated a Rogerian...", "l": "e", "k": ["eliza", "natural", "language", "processing", "program", "created", "joseph", "weizenbaum", "mit", "simulated", "rogerian", "psychotherapist", "demonstrating", "illusion", "understanding"]}, {"id": "term-eliza-effect", "t": "Eliza Effect", "tg": ["History", "Fundamentals"], "d": "history", "x": "The tendency of humans to unconsciously assume that computer behaviors are analogous to human behaviors. Named after...", "l": "e", "k": ["eliza", "effect", "tendency", "humans", "unconsciously", "assume", "computer", "behaviors", "analogous", "human", "named", "chatbot", "describes", "people", "readily"]}, {"id": "term-ellipsis-resolution", "t": "Ellipsis Resolution", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of identifying and recovering omitted words or phrases in text that are understood from context, such as...", "l": "e", "k": ["ellipsis", "resolution", "task", "identifying", "recovering", "omitted", "words", "phrases", "text", "understood", "context", "resolving", "john", "likes", "coffee"]}, {"id": "term-ellipsoid-method", "t": "Ellipsoid Method", "tg": ["Algorithms", "Technical", "Optimization", "History"], "d": "algorithms", "x": "A polynomial-time algorithm for linear programming that uses a sequence of shrinking ellipsoids to converge on the...", "l": "e", "k": ["ellipsoid", "method", "polynomial-time", "algorithm", "linear", "programming", "uses", "sequence", "shrinking", "ellipsoids", "converge", "optimal", "solution", "theoretically", "important"]}, {"id": "term-elliptic-filter", "t": "Elliptic Filter", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A signal processing filter that achieves the steepest roll-off for a given filter order by allowing equiripple behavior...", "l": "e", "k": ["elliptic", "filter", "signal", "processing", "achieves", "steepest", "roll-off", "given", "order", "allowing", "equiripple", "behavior", "passband", "stopband", "known"]}, {"id": "term-elmo", "t": "ELMo", "tg": ["NLP", "Embeddings"], "d": "general", "x": "Embeddings from Language Models, a contextualized word representation method that generates word vectors as a function...", "l": "e", "k": ["elmo", "embeddings", "language", "models", "contextualized", "word", "representation", "method", "generates", "vectors", "function", "entire", "input", "sentence", "bidirectional"]}, {"id": "term-elo-rating-for-models", "t": "ELO Rating for Models", "tg": ["Evaluation", "Ranking"], "d": "datasets", "x": "An adaptation of the chess ELO rating system to rank language models through pairwise comparisons, where models gain or...", "l": "e", "k": ["elo", "rating", "models", "adaptation", "chess", "system", "rank", "language", "pairwise", "comparisons", "gain", "lose", "points", "based", "head-to-head"]}, {"id": "term-elu", "t": "ELU", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Exponential Linear Unit activation function that uses an exponential curve for negative inputs. Defined as f(x) = x if...", "l": "e", "k": ["elu", "exponential", "linear", "unit", "activation", "function", "uses", "curve", "negative", "inputs", "defined", "alpha", "exp", "otherwise", "produces"]}, {"id": "term-embedded-multi-die-interconnect-bridge", "t": "Embedded Multi-Die Interconnect Bridge", "tg": ["Packaging", "Intel", "Interconnect"], "d": "hardware", "x": "Intel advanced packaging technology using a small silicon bridge die to provide high-density connections between...", "l": "e", "k": ["embedded", "multi-die", "interconnect", "bridge", "intel", "advanced", "packaging", "technology", "small", "silicon", "die", "provide", "high-density", "connections", "chiplets"]}, {"id": "term-embedding", "t": "Embedding", "tg": ["Representation", "NLP"], "d": "general", "x": "A dense vector representation of data (words, sentences, images) in a continuous space. Similar items have similar...", "l": "e", "k": ["embedding", "dense", "vector", "representation", "data", "words", "sentences", "images", "continuous", "space", "similar", "items", "embeddings", "enabling", "semantic"]}, {"id": "term-embedding-caching", "t": "Embedding Caching", "tg": ["Vector Database", "Performance"], "d": "general", "x": "The practice of storing previously computed embedding vectors for reuse, avoiding redundant embedding model inference...", "l": "e", "k": ["embedding", "caching", "practice", "storing", "previously", "computed", "vectors", "reuse", "avoiding", "redundant", "model", "inference", "repeated", "similar", "content"]}, {"id": "term-embedding-dimension", "t": "Embedding Dimension", "tg": ["Vector Database", "Embeddings"], "d": "general", "x": "The number of components in a vector embedding, determining the representational capacity and memory footprint of the...", "l": "e", "k": ["embedding", "dimension", "number", "components", "vector", "determining", "representational", "capacity", "memory", "footprint", "space", "typical", "values", "ranging", "dimensions"]}, {"id": "term-embedding-drift", "t": "Embedding Drift", "tg": ["Vector Database", "Maintenance"], "d": "general", "x": "The phenomenon where the distribution of vector embeddings changes over time as source data evolves or embedding models...", "l": "e", "k": ["embedding", "drift", "phenomenon", "distribution", "vector", "embeddings", "changes", "time", "source", "data", "evolves", "models", "updated", "potentially", "degrading"]}, {"id": "term-embedding-fine-tuning", "t": "Embedding Fine-Tuning", "tg": ["Vector Database", "Embeddings"], "d": "general", "x": "The process of further training a pre-trained embedding model on domain-specific data using contrastive learning or...", "l": "e", "k": ["embedding", "fine-tuning", "process", "training", "pre-trained", "model", "domain-specific", "data", "contrastive", "learning", "objectives", "produce", "embeddings", "better", "suited"]}, {"id": "term-embedding-model", "t": "Embedding Model", "tg": ["Model Type", "Representation"], "d": "models", "x": "A model specifically designed to convert text, images, or other data into vector representations. Popular embedding...", "l": "e", "k": ["embedding", "model", "specifically", "designed", "convert", "text", "images", "data", "vector", "representations", "popular", "models", "include", "openai", "text-embedding-ada-002"]}, {"id": "term-embedding-quantization", "t": "Embedding Quantization", "tg": ["LLM", "Inference"], "d": "models", "x": "The compression of high-dimensional embedding vectors from 32-bit floats to lower precision formats (binary, int8) to...", "l": "e", "k": ["embedding", "quantization", "compression", "high-dimensional", "vectors", "32-bit", "floats", "lower", "precision", "formats", "binary", "int8", "reduce", "storage", "costs"]}, {"id": "term-embedding-similarity-search", "t": "Embedding Similarity Search", "tg": ["Vector Database", "Search"], "d": "general", "x": "The process of finding the most semantically similar items to a query by computing distances between their vector...", "l": "e", "k": ["embedding", "similarity", "search", "process", "finding", "semantically", "similar", "items", "query", "computing", "distances", "vector", "embeddings", "shared", "space"]}, {"id": "term-embedding-space", "t": "Embedding Space", "tg": ["Vector Database", "Embeddings"], "d": "general", "x": "The continuous high-dimensional vector space in which embeddings reside, where geometric relationships between vectors...", "l": "e", "k": ["embedding", "space", "continuous", "high-dimensional", "vector", "embeddings", "reside", "geometric", "relationships", "vectors", "encode", "semantic", "similar", "concepts", "located"]}, {"id": "term-embodied-ai", "t": "Embodied AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "An approach to AI that emphasizes the role of physical embodiment and sensorimotor interaction with the environment in...", "l": "e", "k": ["embodied", "approach", "emphasizes", "role", "physical", "embodiment", "sensorimotor", "interaction", "environment", "development", "intelligence", "proponents", "argue", "cannot", "fully"]}, {"id": "term-emergent-abilities", "t": "Emergent Abilities", "tg": ["Phenomenon", "Scaling"], "d": "general", "x": "Capabilities that appear in large AI models that weren't present in smaller versions. Examples include complex...", "l": "e", "k": ["emergent", "abilities", "capabilities", "appear", "large", "models", "weren", "present", "smaller", "versions", "examples", "include", "complex", "reasoning", "code"]}, {"id": "term-emergent-behavior-risk", "t": "Emergent Behavior Risk", "tg": ["Safety", "Technical"], "d": "safety", "x": "The risk that AI systems exhibit unexpected capabilities or behaviors that were not intended or predicted by their...", "l": "e", "k": ["emergent", "behavior", "risk", "systems", "exhibit", "unexpected", "capabilities", "behaviors", "were", "intended", "predicted", "developers", "particularly", "concerning", "large"]}, {"id": "term-emergent-capability", "t": "Emergent Capability", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An ability that appears in large language models only at sufficient scale and is absent in smaller models, such as...", "l": "e", "k": ["emergent", "capability", "ability", "appears", "large", "language", "models", "sufficient", "scale", "absent", "smaller", "multi-step", "reasoning", "code", "generation"]}, {"id": "term-emnist", "t": "EMNIST", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "Extended MNIST dataset that includes handwritten letters in addition to digits providing 814000 character images across...", "l": "e", "k": ["emnist", "extended", "mnist", "dataset", "includes", "handwritten", "letters", "addition", "digits", "providing", "character", "images", "across", "multiple", "splits"]}, {"id": "term-emnlp", "t": "EMNLP", "tg": ["History", "Conferences"], "d": "history", "x": "The Conference on Empirical Methods in Natural Language Processing first held in 1996. A top-tier NLP conference known...", "l": "e", "k": ["emnlp", "conference", "empirical", "methods", "natural", "language", "processing", "held", "top-tier", "nlp", "known", "emphasis", "data-driven", "approaches", "alongside"]}, {"id": "term-emotion-detection", "t": "Emotion Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying specific emotions such as joy, anger, sadness, fear, or surprise expressed in text, providing...", "l": "e", "k": ["emotion", "detection", "task", "identifying", "specific", "emotions", "joy", "anger", "sadness", "fear", "surprise", "expressed", "text", "providing", "finer-grained"]}, {"id": "term-emotion-prompting", "t": "Emotion Prompting", "tg": ["Prompt Engineering", "Behavioral"], "d": "general", "x": "A technique that appends emotionally charged phrases to prompts such as urgency cues or importance markers, leveraging...", "l": "e", "k": ["emotion", "prompting", "technique", "appends", "emotionally", "charged", "phrases", "prompts", "urgency", "cues", "importance", "markers", "leveraging", "observation", "language"]}, {"id": "term-emotion-recognition", "t": "Emotion Recognition", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The classification of facial expressions or body language into emotional categories using computer vision models...", "l": "e", "k": ["emotion", "recognition", "classification", "facial", "expressions", "body", "language", "emotional", "categories", "computer", "vision", "models", "trained", "annotated", "datasets"]}, {"id": "term-emotion-recognition-ai-ethics", "t": "Emotion Recognition AI Ethics", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "Ethical concerns about AI systems that claim to detect human emotions from facial expressions, voice, or physiological...", "l": "e", "k": ["emotion", "recognition", "ethics", "ethical", "concerns", "systems", "claim", "detect", "human", "emotions", "facial", "expressions", "voice", "physiological", "signals"]}, {"id": "term-emotional-ai-ethics", "t": "Emotional AI Ethics", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Ethical concerns related to AI systems that detect generate or respond to human emotions. Issues include accuracy...", "l": "e", "k": ["emotional", "ethics", "ethical", "concerns", "related", "systems", "detect", "generate", "respond", "human", "emotions", "issues", "include", "accuracy", "across"]}, {"id": "term-empatheticdialogues", "t": "EmpatheticDialogues", "tg": ["Benchmark", "NLP", "Dialogue"], "d": "datasets", "x": "A dataset of 25000 conversations grounded in emotional situations where one participant responds empathetically to...", "l": "e", "k": ["empatheticdialogues", "dataset", "conversations", "grounded", "emotional", "situations", "participant", "responds", "empathetically", "another", "described", "experience", "tests", "intelligence", "dialogue"]}, {"id": "term-empirical-bayes", "t": "Empirical Bayes", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "An approach that estimates prior distribution parameters from the data itself, rather than specifying them a priori. It...", "l": "e", "k": ["empirical", "bayes", "approach", "estimates", "prior", "distribution", "parameters", "data", "itself", "rather", "specifying", "priori", "blends", "bayesian", "frequentist"]}, {"id": "term-empirical-distribution-function", "t": "Empirical Distribution Function", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A cumulative distribution function constructed from sample data that assigns probability 1/n to each observed value. It...", "l": "e", "k": ["empirical", "distribution", "function", "cumulative", "constructed", "sample", "data", "assigns", "probability", "observed", "value", "converges", "uniformly", "true", "cdf"]}, {"id": "term-empirical-risk-minimization", "t": "Empirical Risk Minimization", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A learning principle that selects the hypothesis minimizing the average loss on the training data. While simple and...", "l": "e", "k": ["empirical", "risk", "minimization", "learning", "principle", "selects", "hypothesis", "minimizing", "average", "loss", "training", "data", "simple", "intuitive", "lead"]}, {"id": "term-empowerment", "t": "Empowerment", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An information-theoretic intrinsic motivation measure defined as the channel capacity between an agent's actions and...", "l": "e", "k": ["empowerment", "information-theoretic", "intrinsic", "motivation", "measure", "defined", "channel", "capacity", "agent", "actions", "future", "states", "rewards", "maintaining", "maximum"]}, {"id": "term-emu-edit", "t": "Emu Edit", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multi-task image editing model from Meta that can follow free-form text instructions for diverse editing operations...", "l": "e", "k": ["emu", "edit", "multi-task", "image", "editing", "model", "meta", "follow", "free-form", "text", "instructions", "diverse", "operations", "precise", "control"]}, {"id": "term-emu-video", "t": "Emu Video", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A text-to-video generation model from Meta that uses a factored approach generating images first then animating them...", "l": "e", "k": ["emu", "video", "text-to-video", "generation", "model", "meta", "uses", "factored", "approach", "generating", "images", "animating", "efficient", "synthesis"]}, {"id": "term-emu3", "t": "Emu3", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multimodal model from Meta that generates images and text and video through next-token prediction in a discrete token...", "l": "e", "k": ["emu3", "multimodal", "model", "meta", "generates", "images", "text", "video", "next-token", "prediction", "discrete", "token", "space", "without", "diffusion"]}, {"id": "term-encodec", "t": "Encodec", "tg": ["Models", "Technical"], "d": "models", "x": "A neural audio codec by Meta AI that compresses audio at very low bitrates using residual vector quantization. Enables...", "l": "e", "k": ["encodec", "neural", "audio", "codec", "meta", "compresses", "low", "bitrates", "residual", "vector", "quantization", "enables", "efficient", "storage", "transmission"]}, {"id": "term-encoder", "t": "Encoder", "tg": ["Architecture", "Transformers"], "d": "models", "x": "A neural network component that transforms input into a compressed representation. In transformers, encoder models...", "l": "e", "k": ["encoder", "neural", "network", "component", "transforms", "input", "compressed", "representation", "transformers", "models", "bert", "process", "entire", "understanding", "tasks"]}, {"id": "term-encoder-decoder-architecture", "t": "Encoder-Decoder Architecture", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A neural network design where an encoder processes input into a latent representation and a decoder generates output...", "l": "e", "k": ["encoder-decoder", "architecture", "neural", "network", "design", "encoder", "processes", "input", "latent", "representation", "decoder", "generates", "output", "commonly", "translation"]}, {"id": "term-encoder-only-architecture", "t": "Encoder-Only Architecture", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer design using only bidirectional self-attention encoder blocks, producing contextualized representations...", "l": "e", "k": ["encoder-only", "architecture", "transformer", "design", "bidirectional", "self-attention", "encoder", "blocks", "producing", "contextualized", "representations", "input", "tokens", "suited", "classification"]}, {"id": "term-endpoint", "t": "Endpoint", "tg": ["API", "Technical"], "d": "general", "x": "A specific URL where an API can be accessed. AI services expose endpoints for different functions like chat...", "l": "e", "k": ["endpoint", "specific", "url", "api", "accessed", "services", "expose", "endpoints", "different", "functions", "chat", "completions", "embeddings", "image", "generation"]}, {"id": "term-energy-efficiency-in-ai", "t": "Energy Efficiency in AI", "tg": ["Efficiency", "Sustainability"], "d": "hardware", "x": "The computational output achieved per unit of energy consumed in AI training and inference. Improving energy efficiency...", "l": "e", "k": ["energy", "efficiency", "computational", "output", "achieved", "per", "unit", "consumed", "training", "inference", "improving", "critical", "model", "sizes", "costs"]}, {"id": "term-enflame-technology", "t": "Enflame Technology", "tg": ["Accelerator", "China", "Data Center"], "d": "hardware", "x": "Chinese AI chip startup designing training and inference accelerators for data center AI workloads. Develops both chips...", "l": "e", "k": ["enflame", "technology", "chinese", "chip", "startup", "designing", "training", "inference", "accelerators", "data", "center", "workloads", "develops", "chips", "software"]}, {"id": "term-eniac", "t": "ENIAC", "tg": ["History", "Milestones"], "d": "history", "x": "The Electronic Numerical Integrator and Computer, completed in 1945 at the University of Pennsylvania, one of the...", "l": "e", "k": ["eniac", "electronic", "numerical", "integrator", "computer", "completed", "university", "pennsylvania", "earliest", "general-purpose", "digital", "computers", "demonstrated", "feasibility", "large-scale"]}, {"id": "term-ensemble-learning", "t": "Ensemble Learning", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A machine learning paradigm that combines predictions from multiple models to produce a more robust and accurate...", "l": "e", "k": ["ensemble", "learning", "machine", "paradigm", "combines", "predictions", "multiple", "models", "produce", "robust", "accurate", "prediction", "methods", "include", "bagging"]}, {"id": "term-ensemble", "t": "Ensemble Methods", "tg": ["Technique", "ML"], "d": "general", "x": "Techniques that combine multiple models to produce better results than any single model. Includes voting, bagging...", "l": "e", "k": ["ensemble", "methods", "techniques", "combine", "multiple", "models", "produce", "better", "results", "single", "model", "includes", "voting", "bagging", "random"]}, {"id": "term-ensemble-methods-history", "t": "Ensemble Methods History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of ensemble methods from early voting approaches through bagging (Breiman 1996) boosting (Freund and...", "l": "e", "k": ["ensemble", "methods", "history", "development", "early", "voting", "approaches", "bagging", "breiman", "boosting", "freund", "schapire", "stacking", "wolpert", "random"]}, {"id": "term-enterprise-ai", "t": "Enterprise AI", "tg": ["Business", "Application"], "d": "general", "x": "AI solutions designed for business environments with features like access controls, compliance, data privacy, and...", "l": "e", "k": ["enterprise", "solutions", "designed", "business", "environments", "features", "access", "controls", "compliance", "data", "privacy", "integration", "existing", "systems", "different"]}, {"id": "term-entity-disambiguation", "t": "Entity Disambiguation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The process of determining which specific real-world entity a textual mention refers to when the same name could refer...", "l": "e", "k": ["entity", "disambiguation", "process", "determining", "specific", "real-world", "textual", "mention", "refers", "name", "refer", "multiple", "entities", "context", "clues"]}, {"id": "term-entityquestions", "t": "EntityQuestions", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark of simple factual questions about entities testing parametric knowledge stored in language models....", "l": "e", "k": ["entityquestions", "benchmark", "simple", "factual", "questions", "entities", "testing", "parametric", "knowledge", "stored", "language", "models", "evaluates", "entity", "across"]}, {"id": "term-entropy", "t": "Entropy", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A measure from information theory quantifying the uncertainty or disorder in a random variable's distribution. In...", "l": "e", "k": ["entropy", "measure", "information", "theory", "quantifying", "uncertainty", "disorder", "random", "variable", "distribution", "machine", "learning", "splitting", "criterion", "component"]}, {"id": "term-entropy-coding-algorithm", "t": "Entropy Coding Algorithm", "tg": ["Algorithms", "Fundamentals", "Information Theory"], "d": "algorithms", "x": "A class of lossless compression methods that encode data using the fewest possible bits based on the probability...", "l": "e", "k": ["entropy", "coding", "algorithm", "class", "lossless", "compression", "methods", "encode", "data", "fewest", "possible", "bits", "based", "probability", "distribution"]}, {"id": "term-entropy-regularization", "t": "Entropy Regularization", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A technique that adds the policy entropy to the RL objective function, discouraging the agent from committing to a...", "l": "e", "k": ["entropy", "regularization", "technique", "adds", "policy", "objective", "function", "discouraging", "agent", "committing", "single", "action", "quickly", "promotes", "robust"]}, {"id": "term-environment", "t": "Environment", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The external system an RL agent interacts with, providing observations and rewards in response to actions. The...", "l": "e", "k": ["environment", "external", "system", "agent", "interacts", "providing", "observations", "rewards", "response", "actions", "defines", "dynamics", "rules", "governing", "state"]}, {"id": "term-environmental-justice-in-ai", "t": "Environmental Justice in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The principle that the environmental costs of AI development and deployment including energy use and e-waste should not...", "l": "e", "k": ["environmental", "justice", "principle", "costs", "development", "deployment", "including", "energy", "e-waste", "disproportionately", "burden", "marginalized", "communities"]}, {"id": "term-epic-kitchens", "t": "Epic-Kitchens", "tg": ["Benchmark", "Video"], "d": "datasets", "x": "A large egocentric video dataset of cooking activities captured in participants kitchens. Contains fine-grained action...", "l": "e", "k": ["epic-kitchens", "large", "egocentric", "video", "dataset", "cooking", "activities", "captured", "participants", "kitchens", "contains", "fine-grained", "action", "annotations", "recognition"]}, {"id": "term-epipolar-geometry", "t": "Epipolar Geometry", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The geometric relationship between two camera views of the same scene, defined by the fundamental or essential matrix,...", "l": "e", "k": ["epipolar", "geometry", "geometric", "relationship", "camera", "views", "scene", "defined", "fundamental", "essential", "matrix", "constraining", "point", "image", "appear"]}, {"id": "term-episode", "t": "Episode", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A complete sequence of interaction from an initial state to a terminal state in episodic RL tasks. Episodes provide...", "l": "e", "k": ["episode", "complete", "sequence", "interaction", "initial", "state", "terminal", "episodic", "tasks", "episodes", "provide", "natural", "boundaries", "computing", "returns"]}, {"id": "term-epistemic-autonomy", "t": "Epistemic Autonomy", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The right and capacity of individuals to form their own beliefs and make their own judgments rather than having these...", "l": "e", "k": ["epistemic", "autonomy", "right", "capacity", "individuals", "form", "beliefs", "judgments", "rather", "having", "determined", "recommendations", "filter", "bubbles"]}, {"id": "term-epoch", "t": "Epoch", "tg": ["Training", "Technical"], "d": "general", "x": "One complete pass through the entire training dataset. Models typically train for multiple epochs, with each pass...", "l": "e", "k": ["epoch", "complete", "pass", "entire", "training", "dataset", "models", "typically", "train", "multiple", "epochs", "allowing", "weights", "refined", "based"]}, {"id": "term-epsilon-greedy", "t": "Epsilon-Greedy Exploration", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An exploration strategy where the agent selects the greedy (best-known) action with probability 1-epsilon and a random...", "l": "e", "k": ["epsilon-greedy", "exploration", "strategy", "agent", "selects", "greedy", "best-known", "action", "probability", "1-epsilon", "random", "epsilon", "parameter", "typically", "annealed"]}, {"id": "term-equalized-odds", "t": "Equalized Odds", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness criterion requiring that a classifier has equal true positive rates and equal false positive rates across...", "l": "e", "k": ["equalized", "odds", "fairness", "criterion", "requiring", "classifier", "equal", "true", "positive", "rates", "false", "across", "protected", "groups", "ensuring"]}, {"id": "term-ernie", "t": "ERNIE", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "Enhanced Representation through Knowledge Integration is a pre-trained language model from Baidu that incorporates...", "l": "e", "k": ["ernie", "enhanced", "representation", "knowledge", "integration", "pre-trained", "language", "model", "baidu", "incorporates", "entity", "phrase-level", "masking"]}, {"id": "term-error-analysis", "t": "Error Analysis", "tg": ["Evaluation", "Process"], "d": "datasets", "x": "Systematic examination of model mistakes to understand failure patterns and guide improvements. Essential for iterating...", "l": "e", "k": ["error", "analysis", "systematic", "examination", "model", "mistakes", "understand", "failure", "patterns", "guide", "improvements", "essential", "iterating", "performance", "identifying"]}, {"id": "term-esc-50", "t": "ESC-50", "tg": ["Benchmark", "Audio"], "d": "datasets", "x": "Environmental Sound Classification dataset containing 2000 five-second recordings across 50 environmental sound...", "l": "e", "k": ["esc-50", "environmental", "sound", "classification", "dataset", "containing", "five-second", "recordings", "across", "categories", "benchmarking", "recognition", "systems"]}, {"id": "term-escalation-protocol", "t": "Escalation Protocol", "tg": ["Safety", "Governance"], "d": "safety", "x": "A defined procedure for elevating AI safety concerns from technical teams to management and potentially to external...", "l": "e", "k": ["escalation", "protocol", "defined", "procedure", "elevating", "safety", "concerns", "technical", "teams", "management", "potentially", "external", "regulators", "certain", "risk"]}, {"id": "term-esm-1b", "t": "ESM-1b", "tg": ["Models", "Scientific"], "d": "models", "x": "A protein language model from Meta AI with 650 million parameters that learns protein structure and function...", "l": "e", "k": ["esm-1b", "protein", "language", "model", "meta", "million", "parameters", "learns", "structure", "function", "information", "evolutionary", "sequence", "data"]}, {"id": "term-esm-2", "t": "ESM-2", "tg": ["Models", "Scientific"], "d": "models", "x": "A protein language model from Meta AI trained on millions of protein sequences that learns evolutionary and structural...", "l": "e", "k": ["esm-2", "protein", "language", "model", "meta", "trained", "millions", "sequences", "learns", "evolutionary", "structural", "information", "downstream", "analysis", "tasks"]}, {"id": "term-esm-if1", "t": "ESM-IF1", "tg": ["Models", "Scientific"], "d": "models", "x": "An inverse folding model from Meta AI that designs protein sequences for given protein backbone structures using a...", "l": "e", "k": ["esm-if1", "inverse", "folding", "model", "meta", "designs", "protein", "sequences", "given", "backbone", "structures", "geometric", "transformer", "architecture"]}, {"id": "term-esmfold", "t": "ESMFold", "tg": ["Models", "Scientific"], "d": "models", "x": "A protein structure prediction model from Meta AI that uses a large protein language model (ESM-2) to predict...", "l": "e", "k": ["esmfold", "protein", "structure", "prediction", "model", "meta", "uses", "large", "language", "esm-2", "predict", "structures", "single", "forward", "pass"]}, {"id": "term-esperanto-technologies", "t": "Esperanto Technologies", "tg": ["Accelerator", "Startup", "RISC-V"], "d": "hardware", "x": "AI chip company designing RISC-V based many-core processors with over 1000 cores per chip for energy-efficient AI...", "l": "e", "k": ["esperanto", "technologies", "chip", "company", "designing", "risc-v", "based", "many-core", "processors", "cores", "per", "energy-efficient", "inference", "targets", "cloud"]}, {"id": "term-esprit-algorithm", "t": "ESPRIT Algorithm", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "Estimation of Signal Parameters via Rotational Invariance Techniques exploits the shift structure of sensor arrays to...", "l": "e", "k": ["esprit", "algorithm", "estimation", "signal", "parameters", "via", "rotational", "invariance", "techniques", "exploits", "shift", "structure", "sensor", "arrays", "estimate"]}, {"id": "term-essential-matrix-estimation", "t": "Essential Matrix Estimation", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An algorithm that computes the essential matrix relating two calibrated camera views from point correspondences....", "l": "e", "k": ["essential", "matrix", "estimation", "algorithm", "computes", "relating", "calibrated", "camera", "views", "point", "correspondences", "encodes", "relative", "rotation", "translation"]}, {"id": "term-estimation-of-distribution-algorithm", "t": "Estimation of Distribution Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "An evolutionary algorithm that replaces crossover and mutation with probabilistic modeling of promising solutions....", "l": "e", "k": ["estimation", "distribution", "algorithm", "evolutionary", "replaces", "crossover", "mutation", "probabilistic", "modeling", "promising", "solutions", "builds", "probability", "best", "individuals"]}, {"id": "term-etching", "t": "Etching", "tg": ["Fabrication", "Manufacturing", "Process"], "d": "hardware", "x": "Semiconductor manufacturing process that selectively removes material from a wafer surface to create circuit patterns....", "l": "e", "k": ["etching", "semiconductor", "manufacturing", "process", "selectively", "removes", "material", "wafer", "surface", "create", "circuit", "patterns", "includes", "wet", "chemical"]}, {"id": "term-roce", "t": "Ethernet for AI (RoCE)", "tg": ["Distributed Computing", "Hardware"], "d": "hardware", "x": "RDMA over Converged Ethernet, a networking protocol that enables remote direct memory access over Ethernet...", "l": "e", "k": ["ethernet", "roce", "rdma", "converged", "networking", "protocol", "enables", "remote", "direct", "memory", "access", "infrastructure", "provides", "lower-cost", "alternative"]}, {"id": "term-ethical-ai-by-design", "t": "Ethical AI by Design", "tg": ["Safety", "Ethics"], "d": "safety", "x": "An approach that integrates ethical considerations throughout the AI development lifecycle from initial requirements...", "l": "e", "k": ["ethical", "design", "approach", "integrates", "considerations", "throughout", "development", "lifecycle", "initial", "requirements", "implementation", "testing", "deployment"]}, {"id": "term-ethical-prompting", "t": "Ethical Prompting", "tg": ["Prompt Engineering", "Ethics"], "d": "safety", "x": "The practice of designing prompts that explicitly incorporate ethical guidelines, fairness constraints, and...", "l": "e", "k": ["ethical", "prompting", "practice", "designing", "prompts", "explicitly", "incorporate", "guidelines", "fairness", "constraints", "harm-avoidance", "instructions", "steer", "model", "outputs"]}, {"id": "term-ethics", "t": "ETHICS", "tg": ["Benchmark", "NLP", "Safety"], "d": "datasets", "x": "A benchmark testing language models on basic ethical judgments across five moral domains: justice deontology virtue...", "l": "e", "k": ["ethics", "benchmark", "testing", "language", "models", "basic", "ethical", "judgments", "across", "five", "moral", "domains", "justice", "deontology", "virtue"]}, {"id": "term-ethics-board", "t": "Ethics Board (AI)", "tg": ["Governance", "Ethics"], "d": "safety", "x": "A group that reviews AI development and deployment for ethical concerns. Many major AI companies have ethics boards to...", "l": "e", "k": ["ethics", "board", "group", "reviews", "development", "deployment", "ethical", "concerns", "major", "companies", "boards", "evaluate", "potential", "harms", "establish"]}, {"id": "term-ethics-of-care-in-ai", "t": "Ethics of Care in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "An ethical framework that emphasizes relationships responsibility and contextual judgment in AI development. Contrasts...", "l": "e", "k": ["ethics", "care", "ethical", "framework", "emphasizes", "relationships", "responsibility", "contextual", "judgment", "development", "contrasts", "principle-based", "approaches", "focusing", "needs"]}, {"id": "term-ethics-washing", "t": "Ethics Washing", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The practice of organizations using ethics boards, principles, or frameworks as public relations tools without...", "l": "e", "k": ["ethics", "washing", "practice", "organizations", "boards", "principles", "frameworks", "public", "relations", "tools", "without", "implementing", "meaningful", "changes", "development"]}, {"id": "term-eu-ai-act", "t": "EU AI Act", "tg": ["Governance", "Regulation"], "d": "safety", "x": "The European Union's comprehensive regulatory framework for artificial intelligence, adopted in 2024, which classifies...", "l": "e", "k": ["act", "european", "union", "comprehensive", "regulatory", "framework", "artificial", "intelligence", "adopted", "classifies", "systems", "risk", "level", "imposes", "requirements"]}, {"id": "term-eu-chips-act", "t": "EU Chips Act", "tg": ["Policy", "Legislation", "Europe"], "d": "hardware", "x": "European Union legislation investing 43 billion euros in semiconductor manufacturing research and design. Part of...", "l": "e", "k": ["chips", "act", "european", "union", "legislation", "investing", "billion", "euros", "semiconductor", "manufacturing", "research", "design", "part", "europe", "strategy"]}, {"id": "term-euclidean-distance", "t": "Euclidean Distance", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "The straight-line distance between two points in Euclidean space, computed as the square root of the sum of squared...", "l": "e", "k": ["euclidean", "distance", "straight-line", "points", "space", "computed", "square", "root", "sum", "squared", "differences", "across", "dimensions", "common", "metric"]}, {"id": "term-euler-method", "t": "Euler Method", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "The simplest numerical method for solving ordinary differential equations that uses the tangent line at each step to...", "l": "e", "k": ["euler", "method", "simplest", "numerical", "solving", "ordinary", "differential", "equations", "uses", "tangent", "line", "step", "approximate", "solution", "first-order"]}, {"id": "term-euler-tour-technique", "t": "Euler Tour Technique", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A technique that converts a tree into a linear sequence by recording vertices as they are visited during a DFS...", "l": "e", "k": ["euler", "tour", "technique", "converts", "tree", "linear", "sequence", "recording", "vertices", "visited", "dfs", "traversal", "enables", "subtree", "queries"]}, {"id": "term-eulerian-path-algorithm", "t": "Eulerian Path Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm that finds a path visiting every edge in a graph exactly once. Hierholzer's algorithm constructs an...", "l": "e", "k": ["eulerian", "path", "algorithm", "finds", "visiting", "edge", "graph", "exactly", "hierholzer", "constructs", "circuit", "linear", "time", "building", "merging"]}, {"id": "term-eurisko", "t": "Eurisko", "tg": ["History", "Systems"], "d": "history", "x": "An AI program developed by Douglas Lenat in 1981 that could discover new heuristics and concepts. Built as a successor...", "l": "e", "k": ["eurisko", "program", "developed", "douglas", "lenat", "discover", "heuristics", "concepts", "built", "successor", "famously", "won", "traveller", "trillion", "credit"]}, {"id": "term-europarl", "t": "Europarl", "tg": ["Training Corpus", "NLP", "Translation"], "d": "datasets", "x": "A parallel corpus extracted from European Parliament proceedings containing text in 21 European languages. Widely used...", "l": "e", "k": ["europarl", "parallel", "corpus", "extracted", "european", "parliament", "proceedings", "containing", "text", "languages", "widely", "training", "evaluating", "statistical", "neural"]}, {"id": "term-eurosat", "t": "EuroSAT", "tg": ["Benchmark", "Computer Vision", "Remote Sensing"], "d": "datasets", "x": "A dataset of 27000 labeled satellite images across 10 land use classes derived from Sentinel-2 satellite data. Used for...", "l": "e", "k": ["eurosat", "dataset", "labeled", "satellite", "images", "across", "land", "classes", "derived", "sentinel-2", "data", "benchmarking", "remote", "sensing", "cover"]}, {"id": "term-euv-lithography", "t": "EUV Lithography", "tg": ["Fabrication", "Manufacturing", "Process"], "d": "hardware", "x": "Extreme ultraviolet lithography using 13.5nm wavelength light to print the smallest features on advanced semiconductor...", "l": "e", "k": ["euv", "lithography", "extreme", "ultraviolet", "5nm", "wavelength", "light", "print", "smallest", "features", "advanced", "semiconductor", "chips", "required", "manufacturing"]}, {"id": "term-eva", "t": "EVA", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An Exploring the Limits of Masked Visual Representation Learning model that scales vision Transformers to one billion...", "l": "e", "k": ["eva", "exploring", "limits", "masked", "visual", "representation", "learning", "model", "scales", "vision", "transformers", "billion", "parameters", "image", "modeling"]}, {"id": "term-eva-02", "t": "EVA-02", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A second-generation vision foundation model that uses masked image modeling with CLIP features as reconstruction...", "l": "e", "k": ["eva-02", "second-generation", "vision", "foundation", "model", "uses", "masked", "image", "modeling", "clip", "features", "reconstruction", "targets", "improved", "visual"]}, {"id": "term-eva-clip", "t": "EVA-CLIP", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A vision-language model that scales CLIP training to 18 billion parameters using the EVA framework for improved...", "l": "e", "k": ["eva-clip", "vision-language", "model", "scales", "clip", "training", "billion", "parameters", "eva", "framework", "improved", "zero-shot", "visual", "recognition"]}, {"id": "term-evalplus", "t": "EvalPlus", "tg": ["Benchmark", "Code", "Evaluation"], "d": "datasets", "x": "An enhanced evaluation framework for code generation that augments existing benchmarks with additional test cases....", "l": "e", "k": ["evalplus", "enhanced", "evaluation", "framework", "code", "generation", "augments", "existing", "benchmarks", "additional", "test", "cases", "addresses", "case", "insufficiency"]}, {"id": "term-evaluation", "t": "Evaluation", "tg": ["Process", "Quality"], "d": "general", "x": "The process of measuring model performance using metrics, benchmarks, and human assessment. Critical for comparing...", "l": "e", "k": ["evaluation", "process", "measuring", "model", "performance", "metrics", "benchmarks", "human", "assessment", "critical", "comparing", "models", "ensuring", "meet", "quality"]}, {"id": "term-evaluation-bias", "t": "Evaluation Bias", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Bias introduced during model evaluation when benchmark datasets or metrics do not adequately represent the diversity of...", "l": "e", "k": ["evaluation", "bias", "introduced", "model", "benchmark", "datasets", "metrics", "adequately", "represent", "diversity", "deployment", "population", "leading", "overly", "optimistic"]}, {"id": "term-eval-harness", "t": "Evaluation Harness", "tg": ["Tools", "Evaluation"], "d": "datasets", "x": "A framework for systematically testing AI models across multiple benchmarks and tasks. Popular harnesses include...", "l": "e", "k": ["evaluation", "harness", "framework", "systematically", "testing", "models", "across", "multiple", "benchmarks", "tasks", "popular", "harnesses", "include", "lm-evaluation-harness", "open-source"]}, {"id": "term-evasion-attack", "t": "Evasion Attack", "tg": ["Safety", "Technical"], "d": "safety", "x": "An adversarial attack that modifies inputs at test time to cause misclassification by a deployed model. Unlike...", "l": "e", "k": ["evasion", "attack", "adversarial", "modifies", "inputs", "test", "time", "cause", "misclassification", "deployed", "model", "unlike", "poisoning", "attacks", "require"]}, {"id": "term-event-extraction", "t": "Event Extraction", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying event triggers and their arguments in text, determining what happened, who was involved, when,...", "l": "e", "k": ["event", "extraction", "task", "identifying", "triggers", "arguments", "text", "determining", "happened", "involved", "event-specific", "attributes"]}, {"id": "term-evidence-lower-bound", "t": "Evidence Lower Bound", "tg": ["Machine Learning", "Bayesian Methods"], "d": "general", "x": "A lower bound on the log marginal likelihood (model evidence) that serves as the objective function in variational...", "l": "e", "k": ["evidence", "lower", "bound", "log", "marginal", "likelihood", "model", "serves", "objective", "function", "variational", "inference", "maximizing", "elbo", "equivalent"]}, {"id": "term-evodiff", "t": "EvoDiff", "tg": ["Models", "Scientific"], "d": "models", "x": "A diffusion-based generative model for proteins that generates diverse and functional protein sequences through...", "l": "e", "k": ["evodiff", "diffusion-based", "generative", "model", "proteins", "generates", "diverse", "functional", "protein", "sequences", "evolutionary-scale", "denoising", "diffusion"]}, {"id": "term-evol-instruct", "t": "Evol-Instruct", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A method and dataset that uses LLMs to progressively evolve simple instructions into complex ones. Used to create the...", "l": "e", "k": ["evol-instruct", "method", "dataset", "uses", "llms", "progressively", "evolve", "simple", "instructions", "complex", "ones", "create", "wizardlm", "training", "data"]}, {"id": "term-evolution-strategies-rl", "t": "Evolution Strategies for RL", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "Black-box optimization methods that estimate policy gradients by perturbing parameters and evaluating returns, without...", "l": "e", "k": ["evolution", "strategies", "black-box", "optimization", "methods", "estimate", "policy", "gradients", "perturbing", "parameters", "evaluating", "returns", "without", "requiring", "backpropagation"]}, {"id": "term-evolutionary-computation", "t": "Evolutionary Computation", "tg": ["History", "Fundamentals"], "d": "history", "x": "A family of optimization algorithms inspired by biological evolution including genetic algorithms genetic programming...", "l": "e", "k": ["evolutionary", "computation", "family", "optimization", "algorithms", "inspired", "biological", "evolution", "including", "genetic", "programming", "strategies", "differential", "pioneered", "researchers"]}, {"id": "term-exa-scale-computing", "t": "EXA-Scale Computing", "tg": ["Computing", "Milestone", "Performance"], "d": "hardware", "x": "Computing capability exceeding one exaFLOPS or a quintillion floating-point operations per second. The Frontier...", "l": "e", "k": ["exa-scale", "computing", "capability", "exceeding", "exaflops", "quintillion", "floating-point", "operations", "per", "frontier", "supercomputer", "achieved", "milestone", "enabling", "unprecedented"]}, {"id": "term-exact-match", "t": "Exact Match", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A strict evaluation metric that scores a prediction as correct only if it exactly matches the ground truth answer after...", "l": "e", "k": ["exact", "match", "strict", "evaluation", "metric", "scores", "prediction", "correct", "exactly", "matches", "ground", "truth", "answer", "normalization", "commonly"]}, {"id": "term-exact-nearest-neighbor", "t": "Exact Nearest Neighbor", "tg": ["Vector Database", "Search"], "d": "general", "x": "A search approach that guarantees finding the true closest vectors to a query by exhaustively computing distances to...", "l": "e", "k": ["exact", "nearest", "neighbor", "search", "approach", "guarantees", "finding", "true", "closest", "vectors", "query", "exhaustively", "computing", "distances", "index"]}, {"id": "term-exaone", "t": "EXAONE", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A bilingual large language model from LG AI Research optimized for Korean and English with strong enterprise and...", "l": "e", "k": ["exaone", "bilingual", "large", "language", "model", "research", "optimized", "korean", "english", "strong", "enterprise", "reasoning", "capabilities"]}, {"id": "term-exascale-computing", "t": "Exascale Computing", "tg": ["Computing", "Milestone", "Scale"], "d": "hardware", "x": "Computing capability at or exceeding one exaFLOPS enabling unprecedented scale of AI training and scientific...", "l": "e", "k": ["exascale", "computing", "capability", "exceeding", "exaflops", "enabling", "unprecedented", "scale", "training", "scientific", "simulation", "achieved", "frontier", "followed", "aurora"]}, {"id": "term-executive-order-on-ai", "t": "Executive Order on AI", "tg": ["History", "Milestones"], "d": "history", "x": "US Executive Order 14110 on Safe Secure and Trustworthy Development and Use of Artificial Intelligence signed by...", "l": "e", "k": ["executive", "order", "safe", "secure", "trustworthy", "development", "artificial", "intelligence", "signed", "president", "biden", "october", "established", "standards", "safety"]}, {"id": "term-existential-risk-from-ai", "t": "Existential Risk from AI", "tg": ["AI Safety", "AI Ethics"], "d": "safety", "x": "The hypothesis that sufficiently advanced AI systems could pose a threat to the continued existence of humanity or...", "l": "e", "k": ["existential", "risk", "hypothesis", "sufficiently", "advanced", "systems", "pose", "threat", "continued", "existence", "humanity", "permanently", "curtail", "potential", "motivating"]}, {"id": "term-expectation-maximization", "t": "Expectation-Maximization", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "An iterative algorithm for finding maximum likelihood estimates in models with latent variables. It alternates between...", "l": "e", "k": ["expectation-maximization", "iterative", "algorithm", "finding", "maximum", "likelihood", "estimates", "models", "latent", "variables", "alternates", "computing", "expected", "values", "e-step"]}, {"id": "term-expectation-maximization-algorithm", "t": "Expectation-Maximization Algorithm", "tg": ["History", "Fundamentals"], "d": "history", "x": "An iterative statistical method for finding maximum likelihood estimates when data is incomplete or has latent...", "l": "e", "k": ["expectation-maximization", "algorithm", "iterative", "statistical", "method", "finding", "maximum", "likelihood", "estimates", "data", "incomplete", "latent", "variables", "published", "arthur"]}, {"id": "term-expected-calibration-error", "t": "Expected Calibration Error", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A scalar summary of calibration quality computed by binning predictions by confidence, calculating the absolute...", "l": "e", "k": ["expected", "calibration", "error", "scalar", "summary", "quality", "computed", "binning", "predictions", "confidence", "calculating", "absolute", "difference", "accuracy", "within"]}, {"id": "term-expectiminimax-algorithm", "t": "Expectiminimax Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "An extension of minimax for games with chance elements that includes chance nodes representing random events. Computes...", "l": "e", "k": ["expectiminimax", "algorithm", "extension", "minimax", "games", "chance", "elements", "includes", "nodes", "representing", "random", "events", "computes", "expected", "values"]}, {"id": "term-experience-replay", "t": "Experience Replay", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A technique where an agent stores past transitions in a replay buffer and samples from it during training, breaking...", "l": "e", "k": ["experience", "replay", "technique", "agent", "stores", "past", "transitions", "buffer", "samples", "training", "breaking", "temporal", "correlations", "consecutive", "improves"]}, {"id": "term-expert-parallelism", "t": "Expert Parallelism", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A distributed computing strategy for mixture-of-experts models where different expert subnetworks are placed on...", "l": "e", "k": ["expert", "parallelism", "distributed", "computing", "strategy", "mixture-of-experts", "models", "different", "subnetworks", "placed", "devices", "all-to-all", "communication", "routing", "tokens"]}, {"id": "term-expert-prompting", "t": "Expert Prompting", "tg": ["Prompt Engineering", "Persona"], "d": "general", "x": "A prompting method that instructs the model to first identify the most qualified expert identity for a given question,...", "l": "e", "k": ["expert", "prompting", "method", "instructs", "model", "identify", "qualified", "identity", "given", "question", "adopt", "perspective", "knowledge", "base", "provide"]}, {"id": "term-expert-systems", "t": "Expert Systems", "tg": ["History", "Milestones"], "d": "history", "x": "AI programs popular in the 1970s and 1980s that encoded human expert knowledge as if-then rules to solve...", "l": "e", "k": ["expert", "systems", "programs", "popular", "1970s", "1980s", "encoded", "human", "knowledge", "if-then", "rules", "solve", "domain-specific", "problems", "representing"]}, {"id": "term-explainability", "t": "Explainability (XAI)", "tg": ["Transparency", "Trust"], "d": "safety", "x": "The ability to understand and explain how AI models make decisions. Important for trust, debugging, regulatory...", "l": "e", "k": ["explainability", "xai", "ability", "understand", "explain", "models", "decisions", "important", "trust", "debugging", "regulatory", "compliance", "identifying", "potential", "biases"]}, {"id": "term-explainable-ai", "t": "Explainable AI", "tg": ["Safety", "Fundamentals"], "d": "safety", "x": "A field of research focused on making AI system decisions understandable to humans. Includes techniques like feature...", "l": "e", "k": ["explainable", "field", "research", "focused", "making", "system", "decisions", "understandable", "humans", "includes", "techniques", "feature", "attribution", "attention", "visualization"]}, {"id": "term-explainable-ai-history", "t": "Explainable AI History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of explainable AI from early rule-based systems that were inherently interpretable through the challenge...", "l": "e", "k": ["explainable", "history", "evolution", "early", "rule-based", "systems", "were", "inherently", "interpretable", "challenge", "black-box", "deep", "learning", "modern", "xai"]}, {"id": "term-exploding-gradient", "t": "Exploding Gradient", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A training instability where gradients grow exponentially large during backpropagation through deep networks, causing...", "l": "e", "k": ["exploding", "gradient", "training", "instability", "gradients", "grow", "exponentially", "large", "backpropagation", "deep", "networks", "causing", "weight", "updates", "become"]}, {"id": "term-exploding-gradient-problem", "t": "Exploding Gradient Problem", "tg": ["History", "Fundamentals"], "d": "history", "x": "A complementary problem to vanishing gradients where gradients grow exponentially during backpropagation causing...", "l": "e", "k": ["exploding", "gradient", "problem", "complementary", "vanishing", "gradients", "grow", "exponentially", "backpropagation", "causing", "unstable", "training", "solutions", "include", "clipping"]}, {"id": "term-exploitation-exploration-tradeoff-safety", "t": "Exploitation-Exploration Tradeoff Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "Safety implications of the balance between exploiting known-safe actions and exploring new actions in reinforcement...", "l": "e", "k": ["exploitation-exploration", "tradeoff", "safety", "implications", "balance", "exploiting", "known-safe", "actions", "exploring", "reinforcement", "learning", "unsafe", "exploration", "lead", "catastrophic"]}, {"id": "term-exploration-vs-exploitation", "t": "Exploration vs Exploitation", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "The fundamental dilemma in RL between exploring unknown actions to discover potentially better strategies and...", "l": "e", "k": ["exploration", "exploitation", "fundamental", "dilemma", "exploring", "unknown", "actions", "discover", "potentially", "better", "strategies", "exploiting", "current", "knowledge", "maximize"]}, {"id": "term-exponential-distribution", "t": "Exponential Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution modeling the time between events in a Poisson process. It has the memoryless...", "l": "e", "k": ["exponential", "distribution", "continuous", "probability", "modeling", "time", "events", "poisson", "process", "memoryless", "property", "event", "next", "interval", "independent"]}, {"id": "term-exponential-family", "t": "Exponential Family", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A broad class of probability distributions characterized by a specific mathematical form, including normal, Poisson,...", "l": "e", "k": ["exponential", "family", "broad", "class", "probability", "distributions", "characterized", "specific", "mathematical", "form", "including", "normal", "poisson", "binomial", "gamma"]}, {"id": "term-exponential-histogram-algorithm", "t": "Exponential Histogram Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A data structure for maintaining approximate counts over sliding windows in data streams. Uses exponentially growing...", "l": "e", "k": ["exponential", "histogram", "algorithm", "data", "structure", "maintaining", "approximate", "counts", "sliding", "windows", "streams", "uses", "exponentially", "growing", "bucket"]}, {"id": "term-exponential-mechanism", "t": "Exponential Mechanism", "tg": ["Algorithms", "Technical", "Privacy"], "d": "algorithms", "x": "A differential privacy mechanism for selecting an output from a discrete set that favors high-utility options while...", "l": "e", "k": ["exponential", "mechanism", "differential", "privacy", "selecting", "output", "discrete", "favors", "high-utility", "options", "preserving", "assigns", "selection", "probabilities", "exponentially"]}, {"id": "term-exponential-moving-average-model", "t": "Exponential Moving Average Model", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A technique that maintains a running exponential average of model weights during training and uses the averaged model...", "l": "e", "k": ["exponential", "moving", "average", "model", "technique", "maintains", "running", "weights", "training", "uses", "averaged", "evaluation", "provides", "smoother", "stable"]}, {"id": "term-exponential-search", "t": "Exponential Search", "tg": ["Algorithms", "Technical", "Searching"], "d": "algorithms", "x": "A search algorithm that first finds a range where the target might exist by checking positions at exponentially...", "l": "e", "k": ["exponential", "search", "algorithm", "finds", "range", "target", "exist", "checking", "positions", "exponentially", "increasing", "intervals", "performs", "binary", "within"]}, {"id": "term-exponential-smoothing", "t": "Exponential Smoothing", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A family of time series forecasting methods that compute weighted averages of past observations with exponentially...", "l": "e", "k": ["exponential", "smoothing", "family", "time", "series", "forecasting", "methods", "compute", "weighted", "averages", "past", "observations", "exponentially", "decreasing", "weights"]}, {"id": "term-extended-kalman-filter", "t": "Extended Kalman Filter", "tg": ["Algorithms", "Fundamentals", "Signal Processing"], "d": "algorithms", "x": "A nonlinear extension of the Kalman filter that linearizes the system dynamics around the current estimate using...", "l": "e", "k": ["extended", "kalman", "filter", "nonlinear", "extension", "linearizes", "system", "dynamics", "around", "current", "estimate", "first-order", "taylor", "expansion", "widely"]}, {"id": "term-extendible-hashing-algorithm", "t": "Extendible Hashing Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A dynamic hashing scheme that doubles the directory size when a bucket overflows rather than rehashing all entries....", "l": "e", "k": ["extendible", "hashing", "algorithm", "dynamic", "scheme", "doubles", "directory", "size", "bucket", "overflows", "rather", "rehashing", "entries", "provides", "efficient"]}, {"id": "term-external-sort", "t": "External Sort", "tg": ["Algorithms", "Technical", "Sorting"], "d": "algorithms", "x": "A class of sorting algorithms designed for large datasets that do not fit in main memory. Typically uses a merge-sort...", "l": "e", "k": ["external", "sort", "class", "sorting", "algorithms", "designed", "large", "datasets", "fit", "main", "memory", "typically", "uses", "merge-sort", "approach"]}, {"id": "term-extra-trees", "t": "Extra Trees", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Extremely Randomized Trees is an ensemble method similar to random forests but with additional randomization. Splits...", "l": "e", "k": ["extra", "trees", "extremely", "randomized", "ensemble", "method", "similar", "random", "forests", "additional", "randomization", "splits", "chosen", "completely", "rather"]}, {"id": "term-extraction", "t": "Extraction", "tg": ["NLP Task", "Application"], "d": "general", "x": "Using AI to identify and pull specific information from unstructured text. Applications include named entity...", "l": "e", "k": ["extraction", "identify", "pull", "specific", "information", "unstructured", "text", "applications", "include", "named", "entity", "key", "phrase", "structured", "data"]}, {"id": "term-extractive-ai", "t": "Extractive AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "AI development practices that extract value from communities through data collection and labor without providing...", "l": "e", "k": ["extractive", "development", "practices", "extract", "value", "communities", "data", "collection", "labor", "without", "providing", "equitable", "returns", "includes", "concerns"]}, {"id": "term-extractive-question-answering", "t": "Extractive Question Answering", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A QA task where the model identifies the answer as a contiguous span of text within a given context passage, predicting...", "l": "e", "k": ["extractive", "question", "answering", "task", "model", "identifies", "answer", "contiguous", "span", "text", "within", "given", "context", "passage", "predicting"]}, {"id": "term-extractive-summarization", "t": "Extractive Summarization", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A summarization approach that selects and concatenates the most important sentences or passages from the source...", "l": "e", "k": ["extractive", "summarization", "approach", "selects", "concatenates", "important", "sentences", "passages", "source", "document", "form", "summary", "without", "generating", "text"]}, {"id": "term-f-distribution", "t": "F-Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution arising as the ratio of two independent chi-square distributions, each divided by...", "l": "f", "k": ["f-distribution", "continuous", "probability", "distribution", "arising", "ratio", "independent", "chi-square", "distributions", "divided", "degrees", "freedom", "basis", "f-test", "anova"]}, {"id": "term-f1-score", "t": "F1 Score", "tg": ["Metrics", "Evaluation"], "d": "datasets", "x": "A metric combining precision and recall into a single score (their harmonic mean). Useful for evaluating classification...", "l": "f", "k": ["score", "metric", "combining", "precision", "recall", "single", "harmonic", "mean", "useful", "evaluating", "classification", "models", "especially", "imbalanced", "datasets"]}, {"id": "term-fab-fabrication-plant", "t": "Fab (Fabrication Plant)", "tg": ["Fabrication", "Manufacturing", "Facility"], "d": "hardware", "x": "Semiconductor manufacturing facility containing the equipment and clean rooms needed to produce chips. Building a...", "l": "f", "k": ["fab", "fabrication", "plant", "semiconductor", "manufacturing", "facility", "containing", "equipment", "clean", "rooms", "needed", "produce", "chips", "building", "leading-edge"]}, {"id": "term-face-detection", "t": "Face Detection", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "The task of locating and bounding all human faces in an image regardless of pose, scale, or occlusion, using...", "l": "f", "k": ["face", "detection", "task", "locating", "bounding", "human", "faces", "image", "regardless", "pose", "scale", "occlusion", "specialized", "object", "architectures"]}, {"id": "term-face-recognition", "t": "Face Recognition", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A biometric identification task that determines the identity of a person from their facial features, using deep...", "l": "f", "k": ["face", "recognition", "biometric", "identification", "task", "determines", "identity", "person", "facial", "features", "deep", "learning", "models", "extract", "embeddings"]}, {"id": "term-face-verification", "t": "Face Verification", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A one-to-one matching task that determines whether two face images belong to the same person by comparing their face...", "l": "f", "k": ["face", "verification", "one-to-one", "matching", "task", "determines", "images", "belong", "person", "comparing", "embeddings", "typically", "distance", "threshold", "accept"]}, {"id": "term-facechain", "t": "FaceChain", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An open-source tool and model pipeline for generating personalized portrait images with identity preservation using...", "l": "f", "k": ["facechain", "open-source", "tool", "model", "pipeline", "generating", "personalized", "portrait", "images", "identity", "preservation", "fine-tuning", "face-aware", "generation"]}, {"id": "term-facial-landmark-detection", "t": "Facial Landmark Detection", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of predicting the precise locations of key facial points (eyes, nose, mouth corners, jawline) in an image,...", "l": "f", "k": ["facial", "landmark", "detection", "task", "predicting", "precise", "locations", "key", "points", "eyes", "nose", "mouth", "corners", "jawline", "image"]}, {"id": "term-facial-recognition-ethics", "t": "Facial Recognition Ethics", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "The ethical debate around AI-powered facial recognition technology, encompassing concerns about surveillance, racial...", "l": "f", "k": ["facial", "recognition", "ethics", "ethical", "debate", "around", "ai-powered", "technology", "encompassing", "concerns", "surveillance", "racial", "bias", "accuracy", "consent"]}, {"id": "term-factor-analysis", "t": "Factor Analysis", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A statistical method that models observed variables as linear combinations of latent factors plus noise. Unlike PCA it...", "l": "f", "k": ["factor", "analysis", "statistical", "method", "models", "observed", "variables", "linear", "combinations", "latent", "factors", "plus", "noise", "unlike", "pca"]}, {"id": "term-factor-analysis-algorithm", "t": "Factor Analysis Algorithm", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A statistical method that models observed variables as linear combinations of a smaller number of unobserved latent...", "l": "f", "k": ["factor", "analysis", "algorithm", "statistical", "method", "models", "observed", "variables", "linear", "combinations", "smaller", "number", "unobserved", "latent", "factors"]}, {"id": "term-factorized-embedding", "t": "Factorized Embedding", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A technique that decomposes the embedding matrix into two smaller matrices, separating the vocabulary embedding...", "l": "f", "k": ["factorized", "embedding", "technique", "decomposes", "matrix", "smaller", "matrices", "separating", "vocabulary", "dimension", "hidden", "reduce", "parameter", "count"]}, {"id": "term-factual-consistency", "t": "Factual Consistency", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation metric that measures whether claims in generated text are logically entailed by and consistent with...", "l": "f", "k": ["factual", "consistency", "evaluation", "metric", "measures", "claims", "generated", "text", "logically", "entailed", "consistent", "source", "documents", "verifiable", "facts"]}, {"id": "term-factual-grounding", "t": "Factual Grounding", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The process of anchoring a language model's responses to verified source documents or knowledge bases, ensuring outputs...", "l": "f", "k": ["factual", "grounding", "process", "anchoring", "language", "model", "responses", "verified", "source", "documents", "knowledge", "bases", "ensuring", "outputs", "supported"]}, {"id": "term-factuality", "t": "Factuality", "tg": ["Quality", "Challenge"], "d": "general", "x": "The degree to which AI outputs are accurate and true. A major challenge for LLMs, which can generate plausible-sounding...", "l": "f", "k": ["factuality", "degree", "outputs", "accurate", "true", "major", "challenge", "llms", "generate", "plausible-sounding", "incorrect", "information", "hallucinations"]}, {"id": "term-factuality-evaluation", "t": "Factuality Evaluation", "tg": ["Safety", "Technical"], "d": "safety", "x": "Methods for assessing whether AI-generated text contains accurate and verifiable information. Includes automated...", "l": "f", "k": ["factuality", "evaluation", "methods", "assessing", "ai-generated", "text", "contains", "accurate", "verifiable", "information", "includes", "automated", "fact-checking", "knowledge-grounded", "citation"]}, {"id": "term-fail-safe-design", "t": "Fail-Safe Design", "tg": ["Safety", "Technical"], "d": "safety", "x": "An engineering approach where AI systems default to a safe state when they encounter errors or unexpected conditions....", "l": "f", "k": ["fail-safe", "design", "engineering", "approach", "systems", "default", "safe", "state", "encounter", "errors", "unexpected", "conditions", "ensures", "system", "failures"]}, {"id": "term-failure-mode-analysis", "t": "Failure Mode Analysis", "tg": ["Safety", "Technical"], "d": "safety", "x": "A systematic technique for identifying potential failure modes of an AI system and their effects on system behavior and...", "l": "f", "k": ["failure", "mode", "analysis", "systematic", "technique", "identifying", "potential", "modes", "system", "effects", "behavior", "user", "safety", "adapted", "traditional"]}, {"id": "term-fair-founded", "t": "FAIR Founded", "tg": ["History", "Organizations"], "d": "history", "x": "The founding of Facebook AI Research (FAIR) in 2013 led by Yann LeCun. FAIR became one of the leading industrial AI...", "l": "f", "k": ["fair", "founded", "founding", "facebook", "research", "led", "yann", "lecun", "became", "leading", "industrial", "laboratories", "contributing", "significant", "advances"]}, {"id": "term-fair-machine-learning", "t": "Fair Machine Learning", "tg": ["Safety", "Fundamentals"], "d": "safety", "x": "The subfield of machine learning focused on developing models and algorithms that produce equitable outcomes across...", "l": "f", "k": ["fair", "machine", "learning", "subfield", "focused", "developing", "models", "algorithms", "produce", "equitable", "outcomes", "across", "different", "demographic", "groups"]}, {"id": "term-fairface", "t": "FairFace", "tg": ["Benchmark", "Computer Vision", "Fairness"], "d": "datasets", "x": "A face attribute dataset balanced across 7 racial groups for reducing bias in face analysis. Provides annotations for...", "l": "f", "k": ["fairface", "face", "attribute", "dataset", "balanced", "across", "racial", "groups", "reducing", "bias", "analysis", "provides", "annotations", "race", "gender"]}, {"id": "term-fairness", "t": "Fairness (AI)", "tg": ["Ethics", "Safety"], "d": "safety", "x": "The principle that AI systems should not discriminate or produce biased outcomes across different demographic groups....", "l": "f", "k": ["fairness", "principle", "systems", "discriminate", "produce", "biased", "outcomes", "across", "different", "demographic", "groups", "multiple", "mathematical", "definitions", "exist"]}, {"id": "term-fairness-constraint", "t": "Fairness Constraint", "tg": ["Safety", "Technical"], "d": "safety", "x": "A mathematical condition imposed during model training or post-processing to ensure that predictions satisfy a...", "l": "f", "k": ["fairness", "constraint", "mathematical", "condition", "imposed", "model", "training", "post-processing", "ensure", "predictions", "satisfy", "specified", "criterion", "examples", "include"]}, {"id": "term-fairness-in-machine-learning", "t": "Fairness in Machine Learning", "tg": ["History", "Fundamentals"], "d": "history", "x": "The study of how to ensure machine learning systems treat individuals and groups equitably. Research on ML fairness...", "l": "f", "k": ["fairness", "machine", "learning", "study", "ensure", "systems", "treat", "individuals", "groups", "equitably", "research", "addresses", "multiple", "definitions", "demographic"]}, {"id": "term-fairness-through-awareness", "t": "Fairness Through Awareness", "tg": ["Safety", "Technical"], "d": "safety", "x": "A fairness framework proposed by Dwork et al. in 2012 that requires similar individuals to be treated similarly by an...", "l": "f", "k": ["fairness", "awareness", "framework", "proposed", "dwork", "requires", "similar", "individuals", "treated", "similarly", "algorithm", "uses", "task-specific", "metric", "define"]}, {"id": "term-fairness-through-unawareness", "t": "Fairness Through Unawareness", "tg": ["Safety", "Technical"], "d": "safety", "x": "A naive fairness approach that simply removes protected attributes from model inputs. Generally insufficient because...", "l": "f", "k": ["fairness", "unawareness", "naive", "approach", "simply", "removes", "protected", "attributes", "model", "inputs", "generally", "insufficient", "features", "serve", "proxies"]}, {"id": "term-faiss", "t": "FAISS", "tg": ["Vector Database", "Libraries"], "d": "general", "x": "Facebook AI Similarity Search, an open-source library developed by Meta that provides highly optimized implementations...", "l": "f", "k": ["faiss", "facebook", "similarity", "search", "open-source", "library", "developed", "meta", "provides", "highly", "optimized", "implementations", "vector", "clustering", "algorithms"]}, {"id": "term-faiss-algorithm", "t": "FAISS Algorithm", "tg": ["Algorithms", "Technical", "Searching", "Data Structure"], "d": "algorithms", "x": "Facebook AI Similarity Search provides efficient algorithms for searching large collections of dense vectors. Supports...", "l": "f", "k": ["faiss", "algorithm", "facebook", "similarity", "search", "provides", "efficient", "algorithms", "searching", "large", "collections", "dense", "vectors", "supports", "exact"]}, {"id": "term-faithdial", "t": "FaithDial", "tg": ["Benchmark", "NLP", "Evaluation", "Dialogue"], "d": "datasets", "x": "A benchmark for evaluating faithfulness in knowledge-grounded dialogue. Tests whether dialogue systems generate...", "l": "f", "k": ["faithdial", "benchmark", "evaluating", "faithfulness", "knowledge-grounded", "dialogue", "tests", "systems", "generate", "responses", "consistent", "provided", "knowledge", "sources"]}, {"id": "term-faithful-chain-of-thought", "t": "Faithful Chain-of-Thought", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A reasoning framework that decomposes questions into interleaved natural language reasoning and symbolic operations,...", "l": "f", "k": ["faithful", "chain-of-thought", "reasoning", "framework", "decomposes", "questions", "interleaved", "natural", "language", "symbolic", "operations", "ensuring", "chain", "actual", "computation"]}, {"id": "term-faithfulness", "t": "Faithfulness", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation dimension that measures whether generated text contains only information that is supported by and...", "l": "f", "k": ["faithfulness", "evaluation", "dimension", "measures", "generated", "text", "contains", "information", "supported", "consistent", "provided", "source", "context", "unfaithful", "content"]}, {"id": "term-faker", "t": "Faker", "tg": ["Synthetic", "Platform"], "d": "datasets", "x": "A Python library for generating synthetic structured data including names addresses and dates. Used for creating test...", "l": "f", "k": ["faker", "python", "library", "generating", "synthetic", "structured", "data", "including", "names", "addresses", "dates", "creating", "test", "populating", "databases"]}, {"id": "term-falcon", "t": "Falcon", "tg": ["Models", "Technical"], "d": "models", "x": "A family of open-source language models developed by the Technology Innovation Institute in Abu Dhabi. Trained on the...", "l": "f", "k": ["falcon", "family", "open-source", "language", "models", "developed", "technology", "innovation", "institute", "abu", "dhabi", "trained", "refinedweb", "dataset", "falcon-180b"]}, {"id": "term-falcon-180b", "t": "Falcon 180B", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A 180 billion parameter language model from the Technology Innovation Institute that was one of the largest openly...", "l": "f", "k": ["falcon", "180b", "billion", "parameter", "language", "model", "technology", "innovation", "institute", "largest", "openly", "available", "models", "release"]}, {"id": "term-falcon-2", "t": "Falcon 2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A second-generation open-weight language model from the Technology Innovation Institute with improved training data and...", "l": "f", "k": ["falcon", "second-generation", "open-weight", "language", "model", "technology", "innovation", "institute", "improved", "training", "data", "multilingual", "capabilities"]}, {"id": "term-falconmamba", "t": "FalconMamba", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A state space model from TII that applies the Mamba architecture at scale with 7B parameters for efficient language...", "l": "f", "k": ["falconmamba", "state", "space", "model", "tii", "applies", "mamba", "architecture", "scale", "parameters", "efficient", "language", "modeling"]}, {"id": "term-false-discovery-rate", "t": "False Discovery Rate", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The expected proportion of false positives among all rejected null hypotheses in multiple testing. The...", "l": "f", "k": ["false", "discovery", "rate", "expected", "proportion", "positives", "among", "rejected", "null", "hypotheses", "multiple", "testing", "benjamini-hochberg", "procedure", "controls"]}, {"id": "term-false-positive", "t": "False Positive / False Negative", "tg": ["Metrics", "Evaluation"], "d": "datasets", "x": "Classification errors: false positives incorrectly predict the positive class; false negatives miss actual positives....", "l": "f", "k": ["false", "positive", "negative", "classification", "errors", "positives", "incorrectly", "predict", "class", "negatives", "miss", "actual", "trade-off", "depends", "application"]}, {"id": "term-fan", "t": "Fan", "tg": ["Cooling", "Component", "Active"], "d": "hardware", "x": "Active cooling device that moves air across heat sinks and through computing enclosures. Multiple high-speed fans are...", "l": "f", "k": ["fan", "active", "cooling", "device", "moves", "air", "across", "heat", "sinks", "computing", "enclosures", "multiple", "high-speed", "fans", "gpu"]}, {"id": "term-fan-out-wafer-level-packaging", "t": "Fan-Out Wafer-Level Packaging", "tg": ["Packaging", "Manufacturing", "Mobile"], "d": "hardware", "x": "Advanced packaging technique that redistributes chip connections over a larger area than the die itself. Enables thin...", "l": "f", "k": ["fan-out", "wafer-level", "packaging", "advanced", "technique", "redistributes", "chip", "connections", "larger", "area", "die", "itself", "enables", "thin", "compact"]}, {"id": "term-fashion-mnist", "t": "Fashion-MNIST", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A drop-in replacement for MNIST containing 70000 grayscale images of clothing items in 10 categories. Created by...", "l": "f", "k": ["fashion-mnist", "drop-in", "replacement", "mnist", "containing", "grayscale", "images", "clothing", "items", "categories", "created", "zalando", "research", "provide", "challenging"]}, {"id": "term-fast-fourier-transform", "t": "Fast Fourier Transform", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "An efficient algorithm for computing the discrete Fourier transform that reduces computational complexity from O(n^2)...", "l": "f", "k": ["fast", "fourier", "transform", "efficient", "algorithm", "computing", "discrete", "reduces", "computational", "complexity", "log", "discovered", "cooley", "tukey", "essential"]}, {"id": "term-fast-multipole-method", "t": "Fast Multipole Method", "tg": ["Algorithms", "Technical", "Numerical", "History"], "d": "algorithms", "x": "An algorithm that accelerates the evaluation of long-range forces in N-body problems from O(N^2) to O(N) by grouping...", "l": "f", "k": ["fast", "multipole", "method", "algorithm", "accelerates", "evaluation", "long-range", "forces", "n-body", "problems", "grouping", "distant", "sources", "top", "ten"]}, {"id": "term-faster-rcnn", "t": "Faster R-CNN", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A two-stage object detection framework that uses a Region Proposal Network (RPN) to generate candidate bounding boxes,...", "l": "f", "k": ["faster", "r-cnn", "two-stage", "object", "detection", "framework", "uses", "region", "proposal", "network", "rpn", "generate", "candidate", "bounding", "boxes"]}, {"id": "term-fastflow", "t": "FastFlow", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A normalizing-flow-based anomaly detection model that estimates the distribution of visual features from a pre-trained...", "l": "f", "k": ["fastflow", "normalizing-flow-based", "anomaly", "detection", "model", "estimates", "distribution", "visual", "features", "pre-trained", "network", "industrial", "defect"]}, {"id": "term-fastspeech-2", "t": "FastSpeech 2", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A non-autoregressive text-to-speech model that directly generates mel-spectrograms in parallel using duration and pitch...", "l": "f", "k": ["fastspeech", "non-autoregressive", "text-to-speech", "model", "directly", "generates", "mel-spectrograms", "parallel", "duration", "pitch", "energy", "predictors"]}, {"id": "term-fasttext", "t": "FastText", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A word representation model that extends Word2Vec by representing each word as a bag of character n-grams, enabling...", "l": "f", "k": ["fasttext", "word", "representation", "model", "extends", "word2vec", "representing", "bag", "character", "n-grams", "enabling", "meaningful", "embeddings", "out-of-vocabulary", "words"]}, {"id": "term-fasttext-algorithm", "t": "FastText Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP"], "d": "algorithms", "x": "A word embedding method that represents each word as a bag of character n-grams. Enables computing vector...", "l": "f", "k": ["fasttext", "algorithm", "word", "embedding", "method", "represents", "bag", "character", "n-grams", "enables", "computing", "vector", "representations", "out-of-vocabulary", "words"]}, {"id": "term-fastvit", "t": "FastViT", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A fast vision Transformer that uses structural reparameterization and linear attention for rapid image processing...", "l": "f", "k": ["fastvit", "fast", "vision", "transformer", "uses", "structural", "reparameterization", "linear", "attention", "rapid", "image", "processing", "suitable", "mobile", "embedded"]}, {"id": "term-fat-tree-topology", "t": "Fat Tree Topology", "tg": ["Networking", "Topology"], "d": "hardware", "x": "Hierarchical network topology using progressively wider bandwidth toward the root of the tree. Common in data center...", "l": "f", "k": ["fat", "tree", "topology", "hierarchical", "network", "progressively", "wider", "bandwidth", "toward", "root", "common", "data", "center", "networks", "providing"]}, {"id": "term-fb15k-237", "t": "FB15k-237", "tg": ["Benchmark", "Knowledge", "Graph"], "d": "datasets", "x": "A knowledge graph completion benchmark derived from Freebase containing 14541 entities and 237 relation types. Tests...", "l": "f", "k": ["fb15k-237", "knowledge", "graph", "completion", "benchmark", "derived", "freebase", "containing", "entities", "relation", "types", "tests", "link", "prediction", "embedding"]}, {"id": "term-fci-algorithm", "t": "FCI Algorithm", "tg": ["Algorithms", "Technical", "Causal"], "d": "algorithms", "x": "Fast Causal Inference is a causal discovery algorithm that extends the PC algorithm to handle latent confounders and...", "l": "f", "k": ["fci", "algorithm", "fast", "causal", "inference", "discovery", "extends", "handle", "latent", "confounders", "selection", "bias", "produces", "partial", "ancestral"]}, {"id": "term-fcos", "t": "FCOS", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Fully Convolutional One-Stage Object Detection, an anchor-free detector that directly predicts bounding boxes from...", "l": "f", "k": ["fcos", "fully", "convolutional", "one-stage", "object", "detection", "anchor-free", "detector", "directly", "predicts", "bounding", "boxes", "foreground", "pixel", "per-pixel"]}, {"id": "term-feature", "t": "Feature", "tg": ["Data", "ML Fundamentals"], "d": "general", "x": "An individual measurable property or characteristic of data used as input to a model. Good feature engineering can...", "l": "f", "k": ["feature", "individual", "measurable", "property", "characteristic", "data", "input", "model", "good", "engineering", "significantly", "improve", "performance"]}, {"id": "term-feature-attribution", "t": "Feature Attribution", "tg": ["Safety", "Technical"], "d": "safety", "x": "An interpretability technique that assigns importance scores to input features indicating their contribution to a...", "l": "f", "k": ["feature", "attribution", "interpretability", "technique", "assigns", "importance", "scores", "input", "features", "indicating", "contribution", "model", "prediction", "methods", "include"]}, {"id": "term-feature-engineering", "t": "Feature Engineering", "tg": ["History", "Fundamentals"], "d": "history", "x": "The process of using domain knowledge to create input features that improve machine learning model performance. Before...", "l": "f", "k": ["feature", "engineering", "process", "domain", "knowledge", "create", "input", "features", "improve", "machine", "learning", "model", "performance", "deep", "primary"]}, {"id": "term-feature-extraction", "t": "Feature Extraction", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "The process of constructing new features from raw data through transformations such as PCA, autoencoders, or...", "l": "f", "k": ["feature", "extraction", "process", "constructing", "features", "raw", "data", "transformations", "pca", "autoencoders", "domain-specific", "computations", "reducing", "dimensionality", "retaining"]}, {"id": "term-feature-hashing", "t": "Feature Hashing", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A dimensionality reduction technique that maps high-dimensional feature vectors to a lower-dimensional space using a...", "l": "f", "k": ["feature", "hashing", "dimensionality", "reduction", "technique", "maps", "high-dimensional", "vectors", "lower-dimensional", "space", "hash", "function", "avoiding", "need", "maintain"]}, {"id": "term-feature-importance", "t": "Feature Importance", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A measure of how much each input feature contributes to a model's predictions. Common methods include tree-based...", "l": "f", "k": ["feature", "importance", "measure", "input", "contributes", "model", "predictions", "common", "methods", "include", "tree-based", "impurity", "permutation", "shap", "values"]}, {"id": "term-feature-map", "t": "Feature Map", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The output of a convolutional layer representing the activation pattern produced by applying a specific filter to the...", "l": "f", "k": ["feature", "map", "output", "convolutional", "layer", "representing", "activation", "pattern", "produced", "applying", "specific", "filter", "input", "channel", "captures"]}, {"id": "term-feature-matching", "t": "Feature Matching", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of finding corresponding points between two images by detecting local features (keypoints) and comparing...", "l": "f", "k": ["feature", "matching", "process", "finding", "corresponding", "points", "images", "detecting", "local", "features", "keypoints", "comparing", "descriptors", "reconstruction", "slam"]}, {"id": "term-feature-pyramid-network", "t": "Feature Pyramid Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A multi-scale feature extraction architecture that builds a top-down pathway with lateral connections to produce...", "l": "f", "k": ["feature", "pyramid", "network", "multi-scale", "extraction", "architecture", "builds", "top-down", "pathway", "lateral", "connections", "produce", "semantically", "rich", "features"]}, {"id": "term-feature-scaling", "t": "Feature Scaling", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "The process of transforming numerical features to a common scale, such as standardization (zero mean, unit variance) or...", "l": "f", "k": ["feature", "scaling", "process", "transforming", "numerical", "features", "common", "scale", "standardization", "zero", "mean", "unit", "variance", "min-max", "prevent"]}, {"id": "term-feature-selection", "t": "Feature Selection", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "The process of identifying and selecting a subset of relevant features for model construction, reducing dimensionality,...", "l": "f", "k": ["feature", "selection", "process", "identifying", "selecting", "subset", "relevant", "features", "model", "construction", "reducing", "dimensionality", "mitigating", "overfitting", "improving"]}, {"id": "term-federated-averaging", "t": "Federated Averaging", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "The foundational algorithm for federated learning that trains models across decentralized devices by averaging locally...", "l": "f", "k": ["federated", "averaging", "foundational", "algorithm", "learning", "trains", "models", "across", "decentralized", "devices", "locally", "trained", "model", "updates", "device"]}, {"id": "term-federated-averaging-algorithm-fedavg", "t": "Federated Averaging Algorithm (FedAvg)", "tg": ["Algorithms", "Fundamentals", "Privacy"], "d": "algorithms", "x": "A distributed learning algorithm where clients train models locally and a server averages the model parameters. Reduces...", "l": "f", "k": ["federated", "averaging", "algorithm", "fedavg", "distributed", "learning", "clients", "train", "models", "locally", "server", "averages", "model", "parameters", "reduces"]}, {"id": "term-federated-learning", "t": "Federated Learning", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "A machine learning approach where models are trained across multiple decentralized devices or servers holding local...", "l": "f", "k": ["federated", "learning", "machine", "approach", "models", "trained", "across", "multiple", "decentralized", "devices", "servers", "holding", "local", "data", "samples"]}, {"id": "term-federated-learning-hardware", "t": "Federated Learning Hardware", "tg": ["Edge", "Privacy", "Distributed"], "d": "hardware", "x": "Computing devices designed to participate in federated learning where models are trained across distributed devices...", "l": "f", "k": ["federated", "learning", "hardware", "computing", "devices", "designed", "participate", "models", "trained", "across", "distributed", "without", "centralizing", "data", "must"]}, {"id": "term-federated-learning-privacy", "t": "Federated Learning Privacy", "tg": ["Safety", "Technical"], "d": "safety", "x": "Privacy-preserving properties and risks of federated learning where models are trained across distributed devices...", "l": "f", "k": ["federated", "learning", "privacy", "privacy-preserving", "properties", "risks", "models", "trained", "across", "distributed", "devices", "without", "centralizing", "data", "reducing"]}, {"id": "term-fedformer", "t": "FEDformer", "tg": ["Models", "Technical"], "d": "models", "x": "Frequency Enhanced Decomposed Transformer uses frequency-domain attention with seasonal-trend decomposition for...", "l": "f", "k": ["fedformer", "frequency", "enhanced", "decomposed", "transformer", "uses", "frequency-domain", "attention", "seasonal-trend", "decomposition", "efficient", "accurate", "long-term", "time", "series"]}, {"id": "term-fedprox-algorithm", "t": "FedProx Algorithm", "tg": ["Algorithms", "Technical", "Privacy"], "d": "algorithms", "x": "A federated learning algorithm that adds a proximal term to the local objective to limit how far each client model...", "l": "f", "k": ["fedprox", "algorithm", "federated", "learning", "adds", "proximal", "term", "local", "objective", "limit", "far", "client", "model", "deviates", "global"]}, {"id": "term-feedback-loop-risk", "t": "Feedback Loop Risk", "tg": ["Safety", "Technical"], "d": "safety", "x": "The danger that AI system outputs influence future training data creating self-reinforcing cycles that amplify errors...", "l": "f", "k": ["feedback", "loop", "risk", "danger", "system", "outputs", "influence", "future", "training", "data", "creating", "self-reinforcing", "cycles", "amplify", "errors"]}, {"id": "term-feedforward", "t": "Feedforward Neural Network", "tg": ["Architecture", "Neural Networks"], "d": "models", "x": "A neural network where information flows in one direction from input to output, without cycles. The simplest type of...", "l": "f", "k": ["feedforward", "neural", "network", "information", "flows", "direction", "input", "output", "without", "cycles", "simplest", "type", "architecture"]}, {"id": "term-fei-fei-li", "t": "Fei-Fei Li", "tg": ["History", "Pioneers"], "d": "history", "x": "Chinese-American computer scientist who led the creation of the ImageNet dataset and competition, which was...", "l": "f", "k": ["fei-fei", "chinese-american", "computer", "scientist", "led", "creation", "imagenet", "dataset", "competition", "instrumental", "sparking", "deep", "learning", "revolution", "leading"]}, {"id": "term-felzenszwalb-segmentation", "t": "Felzenszwalb Segmentation", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A graph-based image segmentation algorithm that produces segmentations respecting the internal variation of each...", "l": "f", "k": ["felzenszwalb", "segmentation", "graph-based", "image", "algorithm", "produces", "segmentations", "respecting", "internal", "variation", "component", "runs", "nearly", "linear", "time"]}, {"id": "term-fengwu", "t": "FengWu", "tg": ["Models", "Scientific"], "d": "models", "x": "A multi-modal weather forecasting foundation model from Shanghai AI Lab that predicts global weather across multiple...", "l": "f", "k": ["fengwu", "multi-modal", "weather", "forecasting", "foundation", "model", "shanghai", "lab", "predicts", "global", "across", "multiple", "variables", "pressure", "levels"]}, {"id": "term-fenwick-tree-algorithm", "t": "Fenwick Tree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A data structure also known as a binary indexed tree that supports efficient prefix sum queries and point updates in...", "l": "f", "k": ["fenwick", "tree", "algorithm", "data", "structure", "known", "binary", "indexed", "supports", "efficient", "prefix", "sum", "queries", "point", "updates"]}, {"id": "term-ferret", "t": "Ferret", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multimodal large language model from Apple that supports referring and grounding capabilities for precise spatial...", "l": "f", "k": ["ferret", "multimodal", "large", "language", "model", "apple", "supports", "referring", "grounding", "capabilities", "precise", "spatial", "understanding", "images"]}, {"id": "term-ferret-v2", "t": "Ferret-v2", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "An improved version of Ferret with enhanced resolution handling and better spatial grounding for more accurate...", "l": "f", "k": ["ferret-v2", "improved", "version", "ferret", "enhanced", "resolution", "handling", "better", "spatial", "grounding", "accurate", "region-level", "image", "understanding"]}, {"id": "term-feudal-network", "t": "Feudal Network", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A hierarchical RL architecture where a manager module sets abstract goals for a worker module that selects primitive...", "l": "f", "k": ["feudal", "network", "hierarchical", "architecture", "manager", "module", "sets", "abstract", "goals", "worker", "selects", "primitive", "actions", "approach", "decomposes"]}, {"id": "term-few-shot", "t": "Few-Shot Learning", "tg": ["Prompting", "Technique"], "d": "general", "x": "A technique where you provide a few examples in your prompt to help AI understand the pattern or format you want. More...", "l": "f", "k": ["few-shot", "learning", "technique", "provide", "examples", "prompt", "help", "understand", "pattern", "format", "want", "effective", "describing", "alone", "complex"]}, {"id": "term-few-shot-learning-history", "t": "Few-Shot Learning History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of few-shot learning from early work on one-shot learning (Li Fei-Fei 2003) through meta-learning...", "l": "f", "k": ["few-shot", "learning", "history", "evolution", "early", "work", "one-shot", "fei-fei", "meta-learning", "approaches", "maml", "in-context", "large", "language", "models"]}, {"id": "term-few-shot-object-detection", "t": "Few-Shot Object Detection", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Object detection methods that can learn to detect new categories from only a handful of annotated examples, using...", "l": "f", "k": ["few-shot", "object", "detection", "methods", "learn", "detect", "categories", "handful", "annotated", "examples", "meta-learning", "transfer", "learning", "generalize", "limited"]}, {"id": "term-ffhq", "t": "FFHQ", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "Flickr-Faces-HQ a dataset of 70000 high-quality face images at 1024x1024 resolution. Created by NVIDIA for training...", "l": "f", "k": ["ffhq", "flickr-faces-hq", "dataset", "high-quality", "face", "images", "1024x1024", "resolution", "created", "nvidia", "training", "stylegan", "subsequent", "generation", "architectures"]}, {"id": "term-fgsm", "t": "FGSM", "tg": ["Algorithms", "Safety"], "d": "algorithms", "x": "Fast Gradient Sign Method is an adversarial attack that perturbs inputs in the direction of the gradient of the loss...", "l": "f", "k": ["fgsm", "fast", "gradient", "sign", "method", "adversarial", "attack", "perturbs", "inputs", "direction", "loss", "respect", "input", "creates", "examples"]}, {"id": "term-fgvc-aircraft", "t": "FGVC Aircraft", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A fine-grained visual classification dataset containing 10200 images of aircraft spanning 100 aircraft model variants....", "l": "f", "k": ["fgvc", "aircraft", "fine-grained", "visual", "classification", "dataset", "containing", "images", "spanning", "model", "variants", "image", "annotated", "variant", "family"]}, {"id": "term-fiber-optic-cable", "t": "Fiber Optic Cable", "tg": ["Networking", "Physical", "Infrastructure"], "d": "hardware", "x": "Communication cable using glass or plastic fibers to transmit data as light pulses. Standard for long-distance and...", "l": "f", "k": ["fiber", "optic", "cable", "communication", "glass", "plastic", "fibers", "transmit", "data", "light", "pulses", "standard", "long-distance", "high-bandwidth", "connections"]}, {"id": "term-fibonacci-heap-algorithm", "t": "Fibonacci Heap Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A heap data structure that achieves O(1) amortized time for most operations including insert and decrease-key. Provides...", "l": "f", "k": ["fibonacci", "heap", "algorithm", "data", "structure", "achieves", "amortized", "time", "operations", "including", "insert", "decrease-key", "provides", "best", "theoretical"]}, {"id": "term-fibonacci-search", "t": "Fibonacci Search", "tg": ["Algorithms", "Technical", "Searching"], "d": "algorithms", "x": "A comparison-based search algorithm that uses Fibonacci numbers to divide the array into unequal parts. Similar to...", "l": "f", "k": ["fibonacci", "search", "comparison-based", "algorithm", "uses", "numbers", "divide", "array", "unequal", "parts", "similar", "binary", "divides", "ratios", "avoids"]}, {"id": "term-fictitious-play", "t": "Fictitious Play", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A game-theoretic learning algorithm where each player repeatedly best-responds to the empirical distribution of...", "l": "f", "k": ["fictitious", "play", "game-theoretic", "learning", "algorithm", "player", "repeatedly", "best-responds", "empirical", "distribution", "opponent", "actions", "converges", "nash", "equilibrium"]}, {"id": "term-fid-score", "t": "FID Score", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Frechet Inception Distance measures the quality of generated images by comparing the statistics of generated and real...", "l": "f", "k": ["fid", "score", "frechet", "inception", "distance", "measures", "quality", "generated", "images", "comparing", "statistics", "real", "image", "features", "extracted"]}, {"id": "term-fiduciary-duty-for-ai", "t": "Fiduciary Duty for AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "A proposed legal framework that would impose fiduciary obligations on AI developers or deployers requiring them to act...", "l": "f", "k": ["fiduciary", "duty", "proposed", "legal", "framework", "impose", "obligations", "developers", "deployers", "requiring", "act", "best", "interests", "individuals", "affected"]}, {"id": "term-field-programmable-gate-array", "t": "Field-Programmable Gate Array", "tg": ["FPGA", "Architecture", "Programmable"], "d": "hardware", "x": "Integrated circuit that can be configured by the user after manufacturing using programmable logic blocks and...", "l": "f", "k": ["field-programmable", "gate", "array", "integrated", "circuit", "configured", "user", "manufacturing", "programmable", "logic", "blocks", "interconnects", "prototyping", "accelerator", "designs"]}, {"id": "term-fifth-generation-computer-project", "t": "Fifth Generation Computer Project", "tg": ["History", "Milestones"], "d": "history", "x": "A large-scale Japanese government initiative launched in 1982 aiming to create computers with AI capabilities using...", "l": "f", "k": ["fifth", "generation", "computer", "project", "large-scale", "japanese", "government", "initiative", "launched", "aiming", "create", "computers", "capabilities", "parallel", "processing"]}, {"id": "term-figureqa", "t": "FigureQA", "tg": ["Benchmark", "Multimodal", "Reasoning"], "d": "datasets", "x": "A visual reasoning dataset of questions about synthetically generated figures including bar plots pie charts and line...", "l": "f", "k": ["figureqa", "visual", "reasoning", "dataset", "questions", "synthetically", "generated", "figures", "including", "bar", "plots", "pie", "charts", "line", "graphs"]}, {"id": "term-fill-in-the-middle", "t": "Fill-in-the-Middle", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A training objective where the model learns to generate text that fills a gap between a given prefix and suffix,...", "l": "f", "k": ["fill-in-the-middle", "training", "objective", "model", "learns", "generate", "text", "fills", "gap", "given", "prefix", "suffix", "enabling", "code", "completion"]}, {"id": "term-filter-bubble", "t": "Filter Bubble", "tg": ["Safety", "Ethics"], "d": "safety", "x": "An information ecosystem created by AI recommendation algorithms that limits users exposure to diverse viewpoints by...", "l": "f", "k": ["filter", "bubble", "information", "ecosystem", "created", "recommendation", "algorithms", "limits", "users", "exposure", "diverse", "viewpoints", "serving", "content", "aligned"]}, {"id": "term-finbert", "t": "FinBERT", "tg": ["Models", "Technical"], "d": "models", "x": "A BERT model fine-tuned on financial text for sentiment analysis and other financial NLP tasks. Trained on financial...", "l": "f", "k": ["finbert", "bert", "model", "fine-tuned", "financial", "text", "sentiment", "analysis", "nlp", "tasks", "trained", "news", "corporate", "reports", "analyst"]}, {"id": "term-fine-tuning", "t": "Fine-Tuning", "tg": ["Training", "Customization"], "d": "general", "x": "The process of training a pre-existing AI model on additional, specialized data to improve its performance for specific...", "l": "f", "k": ["fine-tuning", "process", "training", "pre-existing", "model", "additional", "specialized", "data", "improve", "performance", "specific", "tasks", "domains", "efficient", "scratch"]}, {"id": "term-fineweb", "t": "FineWeb", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A large-scale web text dataset derived from Common Crawl with careful quality filtering. Designed for pretraining...", "l": "f", "k": ["fineweb", "large-scale", "web", "text", "dataset", "derived", "common", "crawl", "careful", "quality", "filtering", "designed", "pretraining", "language", "models"]}, {"id": "term-fineweb-edu", "t": "FineWeb-Edu", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A subset of FineWeb filtered for educational content using a classifier trained on high-quality educational text....", "l": "f", "k": ["fineweb-edu", "subset", "fineweb", "filtered", "educational", "content", "classifier", "trained", "high-quality", "text", "produces", "higher", "quality", "pretraining", "data"]}, {"id": "term-finfet", "t": "FinFET", "tg": ["Fabrication", "Transistor", "Architecture"], "d": "hardware", "x": "Three-dimensional transistor design where the channel is wrapped around a thin vertical fin. Standard transistor...", "l": "f", "k": ["finfet", "three-dimensional", "transistor", "design", "channel", "wrapped", "around", "thin", "vertical", "fin", "standard", "architecture", "process", "nodes", "22nm"]}, {"id": "term-finite-difference-method", "t": "Finite Difference Method", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "A numerical method that approximates derivatives by replacing them with difference quotients on a discrete grid. The...", "l": "f", "k": ["finite", "difference", "method", "numerical", "approximates", "derivatives", "replacing", "quotients", "discrete", "grid", "simplest", "approach", "solving", "differential", "equations"]}, {"id": "term-finite-element-method", "t": "Finite Element Method", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "A numerical technique for solving partial differential equations by dividing the domain into small elements and...", "l": "f", "k": ["finite", "element", "method", "numerical", "technique", "solving", "partial", "differential", "equations", "dividing", "domain", "small", "elements", "approximating", "solution"]}, {"id": "term-finite-state-machine", "t": "Finite State Machine", "tg": ["History", "Fundamentals"], "d": "history", "x": "A mathematical model of computation consisting of a finite number of states transitions between states and actions....", "l": "f", "k": ["finite", "state", "machine", "mathematical", "model", "computation", "consisting", "number", "states", "transitions", "actions", "extensively", "computer", "science", "pattern"]}, {"id": "term-finite-volume-method", "t": "Finite Volume Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A numerical method for evaluating partial differential equations that integrates flux over control volumes. Inherently...", "l": "f", "k": ["finite", "volume", "method", "numerical", "evaluating", "partial", "differential", "equations", "integrates", "flux", "control", "volumes", "inherently", "conservative", "widely"]}, {"id": "term-firefly-algorithm", "t": "Firefly Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A nature-inspired optimization algorithm where artificial fireflies are attracted to brighter (better) solutions. The...", "l": "f", "k": ["firefly", "algorithm", "nature-inspired", "optimization", "artificial", "fireflies", "attracted", "brighter", "better", "solutions", "attraction", "strength", "decreases", "distance", "creating"]}, {"id": "term-first-ai-winter", "t": "First AI Winter", "tg": ["History", "Milestones"], "d": "history", "x": "The period from approximately 1974 to 1980 when AI research funding and interest declined sharply following the...", "l": "f", "k": ["winter", "period", "approximately", "research", "funding", "interest", "declined", "sharply", "following", "lighthill", "report", "criticism", "failure", "early", "deliver"]}, {"id": "term-fish-speech", "t": "Fish Speech", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A text-to-speech model designed for multilingual voice synthesis with voice cloning capabilities using minimal...", "l": "f", "k": ["fish", "speech", "text-to-speech", "model", "designed", "multilingual", "voice", "synthesis", "cloning", "capabilities", "minimal", "reference", "audio", "samples"]}, {"id": "term-fisher-information", "t": "Fisher Information", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A measure of the amount of information that an observable random variable carries about an unknown parameter. It...", "l": "f", "k": ["fisher", "information", "measure", "amount", "observable", "random", "variable", "carries", "unknown", "parameter", "quantifies", "sensitive", "likelihood", "function", "changes"]}, {"id": "term-fits", "t": "FITS", "tg": ["Models", "Technical"], "d": "models", "x": "A lightweight time series analysis model that operates primarily in the frequency domain using simple interpolation for...", "l": "f", "k": ["fits", "lightweight", "time", "series", "analysis", "model", "operates", "primarily", "frequency", "domain", "simple", "interpolation", "efficient", "forecasting", "reconstruction"]}, {"id": "term-fitted-q-iteration", "t": "Fitted Q-Iteration", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A batch RL algorithm that iteratively fits a Q-function approximator to Bellman backup targets computed from a fixed...", "l": "f", "k": ["fitted", "q-iteration", "batch", "algorithm", "iteratively", "fits", "q-function", "approximator", "bellman", "backup", "targets", "computed", "fixed", "dataset", "generalizes"]}, {"id": "term-fixed-point-iteration", "t": "Fixed-Point Iteration", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "An iterative method that finds a fixed point of a function by repeatedly applying the function starting from an initial...", "l": "f", "k": ["fixed-point", "iteration", "iterative", "method", "finds", "fixed", "point", "function", "repeatedly", "applying", "starting", "initial", "guess", "converges", "contraction"]}, {"id": "term-fixmatch", "t": "FixMatch", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A semi-supervised learning algorithm that combines consistency regularization with pseudo-labeling. Generates...", "l": "f", "k": ["fixmatch", "semi-supervised", "learning", "algorithm", "combines", "consistency", "regularization", "pseudo-labeling", "generates", "pseudo-labels", "weakly", "augmented", "inputs", "trains", "strongly"]}, {"id": "term-flamingo", "t": "Flamingo", "tg": ["Models", "Technical"], "d": "models", "x": "A visual language model by DeepMind that processes interleaved image and text sequences using cross-attention layers...", "l": "f", "k": ["flamingo", "visual", "language", "model", "deepmind", "processes", "interleaved", "image", "text", "sequences", "cross-attention", "layers", "frozen", "vision", "encoder"]}, {"id": "term-flan", "t": "FLAN (Fine-tuned Language Net)", "tg": ["Training", "Google"], "d": "general", "x": "Google's approach to instruction tuning, fine-tuning models on many tasks described with natural language instructions....", "l": "f", "k": ["flan", "fine-tuned", "language", "net", "google", "approach", "instruction", "tuning", "fine-tuning", "models", "tasks", "described", "natural", "instructions", "showed"]}, {"id": "term-flan-collection", "t": "FLAN Collection", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A collection of datasets used for instruction tuning the FLAN language models. Combines over 1800 tasks with diverse...", "l": "f", "k": ["flan", "collection", "datasets", "instruction", "tuning", "language", "models", "combines", "tasks", "diverse", "templates", "improve", "zero-shot", "generalization"]}, {"id": "term-flan-t5", "t": "Flan-T5", "tg": ["Models", "Technical"], "d": "models", "x": "A version of T5 fine-tuned on a large collection of tasks phrased as instructions. Demonstrates that instruction tuning...", "l": "f", "k": ["flan-t5", "version", "fine-tuned", "large", "collection", "tasks", "phrased", "instructions", "demonstrates", "instruction", "tuning", "dramatically", "improves", "zero-shot", "few-shot"]}, {"id": "term-flash-attention", "t": "Flash Attention", "tg": ["Optimization", "Architecture"], "d": "models", "x": "An optimized attention algorithm that reduces memory usage and improves speed by tiling computations. Enables longer...", "l": "f", "k": ["flash", "attention", "optimized", "algorithm", "reduces", "memory", "usage", "improves", "speed", "tiling", "computations", "enables", "longer", "context", "windows"]}, {"id": "term-flash-decoding", "t": "Flash Decoding", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An optimized inference algorithm for autoregressive language models that parallelizes the attention computation across...", "l": "f", "k": ["flash", "decoding", "optimized", "inference", "algorithm", "autoregressive", "language", "models", "parallelizes", "attention", "computation", "across", "key-value", "sequence", "dimension"]}, {"id": "term-flashattention-2", "t": "FlashAttention-2", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "An optimized attention implementation that reduces memory I/O by tiling the computation to keep data in fast SRAM,...", "l": "f", "k": ["flashattention-2", "optimized", "attention", "implementation", "reduces", "memory", "tiling", "computation", "keep", "data", "fast", "sram", "avoiding", "materialization", "full"]}, {"id": "term-flat-index", "t": "Flat Index", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "A brute-force vector index that stores all vectors without compression or partitioning and performs exhaustive linear...", "l": "f", "k": ["flat", "index", "brute-force", "vector", "stores", "vectors", "without", "compression", "partitioning", "performs", "exhaustive", "linear", "search", "guaranteeing", "exact"]}, {"id": "term-fld", "t": "FLD", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "Formal Logic Deduction a benchmark testing language models on formal logical reasoning including propositional and...", "l": "f", "k": ["fld", "formal", "logic", "deduction", "benchmark", "testing", "language", "models", "logical", "reasoning", "including", "propositional", "predicate", "inference"]}, {"id": "term-fleiss-kappa", "t": "Fleiss' Kappa", "tg": ["Evaluation", "Methodology"], "d": "datasets", "x": "A statistical measure that extends Cohen's Kappa to assess inter-rater agreement among three or more annotators making...", "l": "f", "k": ["fleiss", "kappa", "statistical", "measure", "extends", "cohen", "assess", "inter-rater", "agreement", "among", "annotators", "making", "categorical", "judgments", "accounting"]}, {"id": "term-fleurs", "t": "FLEURS", "tg": ["Benchmark", "Speech", "Multilingual"], "d": "datasets", "x": "Few-shot Learning Evaluation of Universal Representations of Speech a multilingual speech benchmark covering 102...", "l": "f", "k": ["fleurs", "few-shot", "learning", "evaluation", "universal", "representations", "speech", "multilingual", "benchmark", "covering", "languages", "evaluate", "understanding", "systems", "across"]}, {"id": "term-fleurys-algorithm", "t": "Fleury's Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm for finding Eulerian paths or circuits in a graph that traverses edges one at a time while avoiding...", "l": "f", "k": ["fleury", "algorithm", "finding", "eulerian", "paths", "circuits", "graph", "traverses", "edges", "time", "avoiding", "bridges", "unless", "option", "exists"]}, {"id": "term-flickr30k", "t": "Flickr30k", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "A dataset of 31000 images collected from Flickr with five human-written captions per image. A standard benchmark for...", "l": "f", "k": ["flickr30k", "dataset", "images", "collected", "flickr", "five", "human-written", "captions", "per", "image", "standard", "benchmark", "image-text", "retrieval", "captioning"]}, {"id": "term-flickr30k-entities", "t": "Flickr30k Entities", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "An extension of Flickr30k with coreference chains linking mentions in captions to bounding boxes in images. Supports...", "l": "f", "k": ["flickr30k", "entities", "extension", "coreference", "chains", "linking", "mentions", "captions", "bounding", "boxes", "images", "supports", "phrase", "grounding", "referring"]}, {"id": "term-flipped-interaction", "t": "Flipped Interaction", "tg": ["Framework", "Interactive"], "d": "general", "x": "A method where you ask AI to ask you questions before providing advice. Leads to more personalized and relevant...", "l": "f", "k": ["flipped", "interaction", "method", "ask", "questions", "providing", "advice", "leads", "personalized", "relevant", "responses", "complex", "situations"]}, {"id": "term-flops", "t": "FLOPS", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "Floating-Point Operations Per Second, the standard measure of computational throughput for AI hardware. FLOPS is...", "l": "f", "k": ["flops", "floating-point", "operations", "per", "standard", "measure", "computational", "throughput", "hardware", "reported", "various", "precisions", "fp64", "fp32", "fp16"]}, {"id": "term-florence", "t": "Florence", "tg": ["Models", "Technical"], "d": "models", "x": "A foundation model for computer vision developed by Microsoft that unifies image classification object detection visual...", "l": "f", "k": ["florence", "foundation", "model", "computer", "vision", "developed", "microsoft", "unifies", "image", "classification", "object", "detection", "visual", "question", "answering"]}, {"id": "term-florence-2", "t": "Florence-2", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A unified vision foundation model from Microsoft that uses a sequence-to-sequence architecture to handle diverse vision...", "l": "f", "k": ["florence-2", "unified", "vision", "foundation", "model", "microsoft", "uses", "sequence-to-sequence", "architecture", "handle", "diverse", "vision-language", "tasks", "text", "prompts"]}, {"id": "term-flores", "t": "FLORES", "tg": ["Benchmark", "NLP", "Translation", "Multilingual"], "d": "datasets", "x": "A benchmark for machine translation evaluation covering over 200 languages with professional translations. Enables...", "l": "f", "k": ["flores", "benchmark", "machine", "translation", "evaluation", "covering", "languages", "professional", "translations", "enables", "standardized", "comparison", "quality", "across", "language"]}, {"id": "term-flow-matching", "t": "Flow Matching", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A generative modeling framework that learns continuous normalizing flows by regressing velocity fields that transport...", "l": "f", "k": ["flow", "matching", "generative", "modeling", "framework", "learns", "continuous", "normalizing", "flows", "regressing", "velocity", "fields", "transport", "samples", "noise"]}, {"id": "term-flower-pollination-algorithm", "t": "Flower Pollination Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A nature-inspired optimization algorithm based on the pollination process of flowering plants. Combines global...", "l": "f", "k": ["flower", "pollination", "algorithm", "nature-inspired", "optimization", "based", "process", "flowering", "plants", "combines", "global", "via", "levy", "flights", "local"]}, {"id": "term-floyd-warshall-algorithm", "t": "Floyd-Warshall Algorithm", "tg": ["Algorithms", "Fundamentals", "Graph"], "d": "algorithms", "x": "An all-pairs shortest path algorithm that finds shortest distances between every pair of vertices in a weighted graph....", "l": "f", "k": ["floyd-warshall", "algorithm", "all-pairs", "shortest", "path", "finds", "distances", "pair", "vertices", "weighted", "graph", "uses", "dynamic", "programming", "nested"]}, {"id": "term-fluency-score", "t": "Fluency Score", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that evaluates the grammatical correctness, naturalness, and readability of generated text, often assessed...", "l": "f", "k": ["fluency", "score", "metric", "evaluates", "grammatical", "correctness", "naturalness", "readability", "generated", "text", "assessed", "human", "judgment", "proxy", "models"]}, {"id": "term-flux", "t": "Flux", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A family of image generation models that use a rectified flow transformer architecture (DiT) for diffusion, achieving...", "l": "f", "k": ["flux", "family", "image", "generation", "models", "rectified", "flow", "transformer", "architecture", "dit", "diffusion", "achieving", "state-of-the-art", "quality", "improved"]}, {"id": "term-flux1", "t": "FLUX.1", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A family of text-to-image models from Black Forest Labs using a rectified flow Transformer architecture for...", "l": "f", "k": ["flux", "family", "text-to-image", "models", "black", "forest", "labs", "rectified", "flow", "transformer", "architecture", "high-quality", "fast", "image", "generation"]}, {"id": "term-flux1-dev", "t": "FLUX.1 Dev", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "The development variant of FLUX.1 that provides high-quality image generation with a balance of speed and output detail...", "l": "f", "k": ["flux", "dev", "development", "variant", "provides", "high-quality", "image", "generation", "balance", "speed", "output", "detail", "open-weight", "research"]}, {"id": "term-flux1-schnell", "t": "FLUX.1 Schnell", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "The fastest variant of FLUX.1 that uses distillation techniques to generate high-quality images in just 1-4 inference...", "l": "f", "k": ["flux", "schnell", "fastest", "variant", "uses", "distillation", "techniques", "generate", "high-quality", "images", "1-4", "inference", "steps"]}, {"id": "term-fnet", "t": "FNet", "tg": ["Models", "Technical"], "d": "models", "x": "A model that replaces the self-attention layer in transformers with a simple Fourier transform. Achieves 92% of BERT...", "l": "f", "k": ["fnet", "model", "replaces", "self-attention", "layer", "transformers", "simple", "fourier", "transform", "achieves", "bert", "accuracy", "glue", "significantly", "faster"]}, {"id": "term-focal-loss", "t": "Focal Loss", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A modified cross-entropy loss that down-weights the contribution of easy examples and focuses training on hard,...", "l": "f", "k": ["focal", "loss", "modified", "cross-entropy", "down-weights", "contribution", "easy", "examples", "focuses", "training", "hard", "misclassified", "adding", "modulating", "factor"]}, {"id": "term-focal-loss-function", "t": "Focal Loss Function", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A modification of cross-entropy loss that down-weights easy examples and focuses training on hard misclassified...", "l": "f", "k": ["focal", "loss", "function", "modification", "cross-entropy", "down-weights", "easy", "examples", "focuses", "training", "hard", "misclassified", "addresses", "class", "imbalance"]}, {"id": "term-focalnet", "t": "FocalNet", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A focal modulation network that replaces self-attention with context-dependent modulation for efficient visual modeling...", "l": "f", "k": ["focalnet", "focal", "modulation", "network", "replaces", "self-attention", "context-dependent", "efficient", "visual", "modeling", "without", "token-to-token", "interaction"]}, {"id": "term-folio", "t": "FOLIO", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "First-Order Logic Inference a benchmark of natural language reasoning problems annotated with first-order logic...", "l": "f", "k": ["folio", "first-order", "logic", "inference", "benchmark", "natural", "language", "reasoning", "problems", "annotated", "formulas", "tests", "ability", "perform", "logical"]}, {"id": "term-fomo", "t": "FOMO (AI Context)", "tg": ["Culture", "Learning"], "d": "general", "x": "Fear Of Missing Out on AI advancements. The rapid pace of AI development creates pressure to constantly learn new tools...", "l": "f", "k": ["fomo", "context", "fear", "missing", "advancements", "rapid", "pace", "development", "creates", "pressure", "constantly", "learn", "tools", "techniques", "balance"]}, {"id": "term-food-101", "t": "Food-101", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A dataset of 101000 food images across 101 categories with 1000 images per class. Training images contain noise while...", "l": "f", "k": ["food-101", "dataset", "food", "images", "across", "categories", "per", "class", "training", "contain", "noise", "test", "manually", "cleaned", "making"]}, {"id": "term-ford-fulkerson-algorithm", "t": "Ford-Fulkerson Algorithm", "tg": ["Algorithms", "Fundamentals", "Graph", "History"], "d": "algorithms", "x": "A method for computing maximum flow in a flow network by repeatedly finding augmenting paths from source to sink and...", "l": "f", "k": ["ford-fulkerson", "algorithm", "method", "computing", "maximum", "flow", "network", "repeatedly", "finding", "augmenting", "paths", "source", "sink", "increasing", "along"]}, {"id": "term-formal-verification-for-ai", "t": "Formal Verification for AI", "tg": ["Safety", "Technical"], "d": "safety", "x": "The use of mathematical methods to prove that an AI system satisfies specified safety properties. Provides stronger...", "l": "f", "k": ["formal", "verification", "mathematical", "methods", "prove", "system", "satisfies", "specified", "safety", "properties", "provides", "stronger", "guarantees", "empirical", "testing"]}, {"id": "term-format-instruction", "t": "Format Instructions", "tg": ["Prompting", "Technique"], "d": "general", "x": "Explicit guidance in prompts about how AI should structure its response. Examples include requesting JSON, markdown...", "l": "f", "k": ["format", "instructions", "explicit", "guidance", "prompts", "structure", "response", "examples", "include", "requesting", "json", "markdown", "tables", "bullet", "points"]}, {"id": "term-forward-backward-algorithm", "t": "Forward-Backward Algorithm", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An inference algorithm for Hidden Markov Models that computes the posterior probability of each hidden state given the...", "l": "f", "k": ["forward-backward", "algorithm", "inference", "hidden", "markov", "models", "computes", "posterior", "probability", "state", "given", "entire", "observation", "sequence", "combines"]}, {"id": "term-foundation-model", "t": "Foundation Model", "tg": ["Model Type", "Architecture"], "d": "models", "x": "A large AI model trained on broad data that can be adapted to many downstream tasks. Examples include GPT-4, Claude,...", "l": "f", "k": ["foundation", "model", "large", "trained", "broad", "data", "adapted", "downstream", "tasks", "examples", "include", "gpt-4", "claude", "llama", "base"]}, {"id": "term-foundation-model-risk", "t": "Foundation Model Risk", "tg": ["Safety", "Technical"], "d": "safety", "x": "Risks specific to large foundation models that are adapted for many downstream tasks. A single vulnerability or bias in...", "l": "f", "k": ["foundation", "model", "risk", "risks", "specific", "large", "models", "adapted", "downstream", "tasks", "single", "vulnerability", "bias", "propagate", "applications"]}, {"id": "term-foundation-models", "t": "Foundation Models", "tg": ["History", "Milestones"], "d": "history", "x": "A term coined by researchers at Stanford in 2021 describing large AI models trained on broad data that can be adapted...", "l": "f", "k": ["foundation", "models", "term", "coined", "researchers", "stanford", "describing", "large", "trained", "broad", "data", "adapted", "wide", "range", "downstream"]}, {"id": "term-fourcastnet", "t": "FourCastNet", "tg": ["Models", "Scientific"], "d": "models", "x": "A Fourier-based neural network for global weather forecasting that uses Adaptive Fourier Neural Operators to predict...", "l": "f", "k": ["fourcastnet", "fourier-based", "neural", "network", "global", "weather", "forecasting", "uses", "adaptive", "fourier", "operators", "predict", "atmospheric", "variables", "high"]}, {"id": "term-fourier-transform", "t": "Fourier Transform", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A mathematical transform that decomposes a function of time or space into its constituent frequencies. Converts signals...", "l": "f", "k": ["fourier", "transform", "mathematical", "decomposes", "function", "time", "space", "constituent", "frequencies", "converts", "signals", "spatial", "domain", "frequency", "fundamental"]}, {"id": "term-fox", "t": "Fox", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multi-modal document understanding model optimized for processing complex real-world documents with tables and charts...", "l": "f", "k": ["fox", "multi-modal", "document", "understanding", "model", "optimized", "processing", "complex", "real-world", "documents", "tables", "charts", "mixed-format", "content"]}, {"id": "term-fp16", "t": "FP16 (Half Precision)", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "A 16-bit floating-point format with 5 exponent bits and 10 mantissa bits, offering half the memory footprint of FP32....", "l": "f", "k": ["fp16", "half", "precision", "16-bit", "floating-point", "format", "exponent", "bits", "mantissa", "offering", "memory", "footprint", "fp32", "widely", "mixed"]}, {"id": "term-fp32", "t": "FP32 (Single Precision)", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "The standard 32-bit floating-point format with 8 exponent bits and 23 mantissa bits, providing high numerical precision...", "l": "f", "k": ["fp32", "single", "precision", "standard", "32-bit", "floating-point", "format", "exponent", "bits", "mantissa", "providing", "high", "numerical", "model", "training"]}, {"id": "term-fp4", "t": "FP4 Quantization", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "An ultra-low precision 4-bit floating-point format for neural network weights, supported by next-generation AI hardware...", "l": "f", "k": ["fp4", "quantization", "ultra-low", "precision", "4-bit", "floating-point", "format", "neural", "network", "weights", "supported", "next-generation", "hardware", "nvidia", "blackwell"]}, {"id": "term-fp8", "t": "FP8 Precision", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "An 8-bit floating-point format introduced for AI training and inference, available in two variants: E4M3 (4 exponent, 3...", "l": "f", "k": ["fp8", "precision", "8-bit", "floating-point", "format", "introduced", "training", "inference", "available", "variants", "e4m3", "exponent", "mantissa", "bits", "forward"]}, {"id": "term-fpga-ai", "t": "FPGA for AI", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "Field-Programmable Gate Arrays reconfigured for AI workloads, offering customizable hardware logic that can be tailored...", "l": "f", "k": ["fpga", "field-programmable", "gate", "arrays", "reconfigured", "workloads", "offering", "customizable", "hardware", "logic", "tailored", "specific", "neural", "network", "architectures"]}, {"id": "term-frame-problem", "t": "Frame Problem", "tg": ["History", "Milestones"], "d": "history", "x": "A fundamental challenge in AI identified by McCarthy and Hayes in 1969 concerning how to represent the effects of...", "l": "f", "k": ["frame", "problem", "fundamental", "challenge", "identified", "mccarthy", "hayes", "concerning", "represent", "effects", "actions", "without", "explicitly", "specifying", "everything"]}, {"id": "term-frame-stacking", "t": "Frame Stacking", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A technique used in visual RL that concatenates multiple consecutive observation frames as input to the agent,...", "l": "f", "k": ["frame", "stacking", "technique", "visual", "concatenates", "multiple", "consecutive", "observation", "frames", "input", "agent", "providing", "temporal", "context", "enabling"]}, {"id": "term-framenet", "t": "FrameNet", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A lexical database that describes word meanings in terms of semantic frames, specifying the participants, props, and...", "l": "f", "k": ["framenet", "lexical", "database", "describes", "word", "meanings", "terms", "semantic", "frames", "specifying", "participants", "props", "conceptual", "roles", "associated"]}, {"id": "term-frames", "t": "FRAMES", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "Factuality Rating And Measurement for Evaluation of Summarization a benchmark for evaluating factual consistency in...", "l": "f", "k": ["frames", "factuality", "rating", "measurement", "evaluation", "summarization", "benchmark", "evaluating", "factual", "consistency", "text", "tests", "summaries", "contain", "hallucinated"]}, {"id": "term-frames-minsky", "t": "Frames (Minsky)", "tg": ["History", "Milestones"], "d": "history", "x": "A knowledge representation scheme proposed by Marvin Minsky in 1974 where stereotyped situations are represented as...", "l": "f", "k": ["frames", "minsky", "knowledge", "representation", "scheme", "proposed", "marvin", "stereotyped", "situations", "represented", "data", "structures", "slots", "expected", "information"]}, {"id": "term-frames-theory", "t": "Frames Theory", "tg": ["History", "Fundamentals"], "d": "history", "x": "A knowledge representation approach proposed by Marvin Minsky in 1974 where knowledge is organized into frame...", "l": "f", "k": ["frames", "theory", "knowledge", "representation", "approach", "proposed", "marvin", "minsky", "organized", "frame", "structures", "containing", "slots", "attributes", "default"]}, {"id": "term-frank-rosenblatt", "t": "Frank Rosenblatt", "tg": ["History", "Pioneers"], "d": "history", "x": "American psychologist (1928-1971) who invented the perceptron in 1957, one of the earliest neural network models...", "l": "f", "k": ["frank", "rosenblatt", "american", "psychologist", "1928-1971", "invented", "perceptron", "earliest", "neural", "network", "models", "capable", "learning", "data", "pioneering"]}, {"id": "term-frank-wolfe-algorithm", "t": "Frank-Wolfe Algorithm", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A constrained optimization algorithm also known as conditional gradient that replaces the projection step with a linear...", "l": "f", "k": ["frank-wolfe", "algorithm", "constrained", "optimization", "known", "conditional", "gradient", "replaces", "projection", "step", "linear", "minimization", "constraint", "produces", "sparse"]}, {"id": "term-freebase", "t": "Freebase", "tg": ["Knowledge", "Graph"], "d": "datasets", "x": "A large collaborative knowledge base containing over 1.9 billion facts about 39 million entities. Acquired by Google in...", "l": "f", "k": ["freebase", "large", "collaborative", "knowledge", "base", "containing", "billion", "facts", "million", "entities", "acquired", "google", "basis", "graph"]}, {"id": "term-frequency-penalty", "t": "Frequency Penalty", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A parameter that penalizes tokens proportionally to how often they have appeared in the output so far, encouraging...", "l": "f", "k": ["frequency", "penalty", "parameter", "penalizes", "tokens", "proportionally", "appeared", "output", "far", "encouraging", "lexical", "diversity", "generated", "text"]}, {"id": "term-freshqa", "t": "FreshQA", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A dynamic QA benchmark where questions have answers that may change over time. Tests whether language models have...", "l": "f", "k": ["freshqa", "dynamic", "benchmark", "questions", "answers", "change", "time", "tests", "language", "models", "up-to-date", "knowledge", "handle", "temporal", "reasoning"]}, {"id": "term-frontier-ai", "t": "Frontier AI", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "The most capable AI models at the cutting edge of capability, which may pose novel safety risks not present in less...", "l": "f", "k": ["frontier", "capable", "models", "cutting", "edge", "capability", "pose", "novel", "safety", "risks", "present", "less", "systems", "term", "governance"]}, {"id": "term-frontier-model", "t": "Frontier Model", "tg": ["Policy", "Research"], "d": "safety", "x": "The most capable AI models at any given time, pushing the boundaries of what's possible. Term often used in AI policy...", "l": "f", "k": ["frontier", "model", "capable", "models", "given", "time", "pushing", "boundaries", "possible", "term", "policy", "discussions", "governance", "powerful", "systems"]}, {"id": "term-frontier-model-forum", "t": "Frontier Model Forum", "tg": ["Governance", "AI Safety"], "d": "safety", "x": "An industry body established in 2023 by major AI companies to promote responsible development and deployment of...", "l": "f", "k": ["frontier", "model", "forum", "industry", "body", "established", "major", "companies", "promote", "responsible", "development", "deployment", "models", "focusing", "safety"]}, {"id": "term-frontier-safety-framework", "t": "Frontier Safety Framework", "tg": ["Safety", "Governance"], "d": "safety", "x": "A set of policies and practices adopted by leading AI labs to manage the risks associated with the most capable AI...", "l": "f", "k": ["frontier", "safety", "framework", "policies", "practices", "adopted", "leading", "labs", "manage", "risks", "associated", "capable", "models", "includes", "capability"]}, {"id": "term-frontier-supercomputer", "t": "Frontier Supercomputer", "tg": ["Supercomputer", "AMD", "Exascale"], "d": "hardware", "x": "First exascale supercomputer achieving 1.194 exaFLOPS built at Oak Ridge National Laboratory using AMD EPYC CPUs and...", "l": "f", "k": ["frontier", "supercomputer", "exascale", "achieving", "exaflops", "built", "oak", "ridge", "national", "laboratory", "amd", "epyc", "cpus", "instinct", "mi250x"]}, {"id": "term-frontiermath", "t": "FrontierMath", "tg": ["Benchmark", "NLP", "Reasoning", "Evaluation"], "d": "datasets", "x": "A benchmark of original research-level mathematics problems testing mathematical reasoning beyond standard...", "l": "f", "k": ["frontiermath", "benchmark", "original", "research-level", "mathematics", "problems", "testing", "mathematical", "reasoning", "beyond", "standard", "competitions", "designed", "measure", "frontier"]}, {"id": "term-fsd50k", "t": "FSD50K", "tg": ["Benchmark", "Audio"], "d": "datasets", "x": "Freesound Dataset 50K containing 51197 audio clips from Freesound with labels from the AudioSet ontology. Provides a...", "l": "f", "k": ["fsd50k", "freesound", "dataset", "50k", "containing", "audio", "clips", "labels", "audioset", "ontology", "provides", "large", "open", "sound", "event"]}, {"id": "term-fsdp", "t": "FSDP", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Fully Sharded Data Parallel, a PyTorch training strategy that shards model parameters, gradients, and optimizer states...", "l": "f", "k": ["fsdp", "fully", "sharded", "data", "parallel", "pytorch", "training", "strategy", "shards", "model", "parameters", "gradients", "optimizer", "states", "across"]}, {"id": "term-fsdp-fully-sharded-data-parallel", "t": "FSDP (Fully Sharded Data Parallel)", "tg": ["Distributed Training", "PyTorch", "Framework"], "d": "hardware", "x": "PyTorch native implementation of ZeRO-style parameter sharding for distributed training. Shards model parameters...", "l": "f", "k": ["fsdp", "fully", "sharded", "data", "parallel", "pytorch", "native", "implementation", "zero-style", "parameter", "sharding", "distributed", "training", "shards", "model"]}, {"id": "term-ft-transformer", "t": "FT-Transformer", "tg": ["Models", "Technical"], "d": "models", "x": "Feature Tokenizer plus Transformer applies the transformer architecture to tabular data by converting each feature into...", "l": "f", "k": ["ft-transformer", "feature", "tokenizer", "plus", "transformer", "applies", "architecture", "tabular", "data", "converting", "embedding", "token", "demonstrates", "transformers", "match"]}, {"id": "term-fugaku-a64fx-processor", "t": "Fugaku A64FX Processor", "tg": ["Processor", "ARM", "Fujitsu"], "d": "hardware", "x": "Custom ARM-based processor designed by Fujitsu for the Fugaku supercomputer. Notable for integrating HBM2 memory...", "l": "f", "k": ["fugaku", "a64fx", "processor", "custom", "arm-based", "designed", "fujitsu", "supercomputer", "notable", "integrating", "hbm2", "memory", "directly", "cpu", "package"]}, {"id": "term-fugaku-supercomputer", "t": "Fugaku Supercomputer", "tg": ["Supercomputer", "ARM", "Japan"], "d": "hardware", "x": "ARM-based supercomputer built by Fujitsu at RIKEN in Japan using custom A64FX processors. Held the TOP500 number one...", "l": "f", "k": ["fugaku", "supercomputer", "arm-based", "built", "fujitsu", "riken", "japan", "custom", "a64fx", "processors", "held", "top500", "number", "ranking", "without"]}, {"id": "term-full-text-search", "t": "Full-Text Search", "tg": ["Retrieval", "Search"], "d": "general", "x": "A search technique that examines all words in every stored document to find matches for a query, typically using...", "l": "f", "k": ["full-text", "search", "technique", "examines", "words", "stored", "document", "find", "matches", "query", "typically", "inverted", "indexes", "tokenization", "stemming"]}, {"id": "term-fully-sharded-data-parallel", "t": "Fully Sharded Data Parallel", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A memory-efficient distributed training technique that shards model parameters gradients and optimizer states across...", "l": "f", "k": ["fully", "sharded", "data", "parallel", "memory-efficient", "distributed", "training", "technique", "shards", "model", "parameters", "gradients", "optimizer", "states", "across"]}, {"id": "term-function-calling", "t": "Function Calling", "tg": ["Capability", "Agents"], "d": "general", "x": "An LLM capability that allows models to generate structured output to call external functions or APIs. Enables AI...", "l": "f", "k": ["function", "calling", "llm", "capability", "allows", "models", "generate", "structured", "output", "call", "external", "functions", "apis", "enables", "agents"]}, {"id": "term-functional-correctness", "t": "Functional Correctness", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "An evaluation criterion for code generation that assesses whether generated code produces correct outputs for all test...", "l": "f", "k": ["functional", "correctness", "evaluation", "criterion", "code", "generation", "assesses", "generated", "produces", "correct", "outputs", "test", "inputs", "measured", "execution-based"]}, {"id": "term-functional-map-of-the-world", "t": "Functional Map of the World", "tg": ["Benchmark", "Computer Vision", "Remote Sensing"], "d": "datasets", "x": "A satellite imagery dataset containing over 1 million images of 63 categories of functional land use. Tests the ability...", "l": "f", "k": ["functional", "map", "world", "satellite", "imagery", "dataset", "containing", "million", "images", "categories", "land", "tests", "ability", "classify", "building"]}, {"id": "term-fundamental-matrix-estimation", "t": "Fundamental Matrix Estimation", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An algorithm that computes the fundamental matrix encoding the epipolar geometry between two uncalibrated views. The...", "l": "f", "k": ["fundamental", "matrix", "estimation", "algorithm", "computes", "encoding", "epipolar", "geometry", "uncalibrated", "views", "eight-point", "normalized", "variant", "standard", "approaches"]}, {"id": "term-fusion-retrieval", "t": "Fusion Retrieval", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "A retrieval paradigm that combines results from multiple diverse retrieval methods or models through score fusion, rank...", "l": "f", "k": ["fusion", "retrieval", "paradigm", "combines", "results", "multiple", "diverse", "methods", "models", "score", "rank", "learned", "combination", "leveraging", "complementary"]}, {"id": "term-future-of-life-institute", "t": "Future of Life Institute", "tg": ["History", "Organizations"], "d": "history", "x": "A nonprofit organization founded in 2014 focused on existential risks from advanced technology particularly AI. FLI...", "l": "f", "k": ["future", "life", "institute", "nonprofit", "organization", "founded", "focused", "existential", "risks", "advanced", "technology", "particularly", "fli", "published", "open"]}, {"id": "term-fuyu", "t": "Fuyu", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multimodal model from Adept AI that directly processes raw image patches without a separate vision encoder for...", "l": "f", "k": ["fuyu", "multimodal", "model", "adept", "directly", "processes", "raw", "image", "patches", "without", "separate", "vision", "encoder", "simplified", "architecture"]}, {"id": "term-fuzzy-c-means-algorithm", "t": "Fuzzy C-Means Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A soft clustering algorithm where each point belongs to every cluster with a degree of membership. Minimizes a weighted...", "l": "f", "k": ["fuzzy", "c-means", "algorithm", "soft", "clustering", "point", "belongs", "cluster", "degree", "membership", "minimizes", "weighted", "sum", "squared", "distances"]}, {"id": "term-fuzzy-logic", "t": "Fuzzy Logic", "tg": ["History", "Milestones"], "d": "history", "x": "A form of many-valued logic introduced by Lotfi Zadeh in 1965 that handles degrees of truth rather than binary...", "l": "f", "k": ["fuzzy", "logic", "form", "many-valued", "introduced", "lotfi", "zadeh", "handles", "degrees", "truth", "rather", "binary", "true", "false", "values"]}, {"id": "term-fuzzy-matching", "t": "Fuzzy Matching", "tg": ["NLP", "Technique"], "d": "general", "x": "Finding approximate rather than exact matches in text. Useful for handling typos, variations, and similar-meaning terms...", "l": "f", "k": ["fuzzy", "matching", "finding", "approximate", "rather", "exact", "matches", "text", "useful", "handling", "typos", "variations", "similar-meaning", "terms", "search"]}, {"id": "term-g-eval", "t": "G-Eval", "tg": ["Evaluation", "LLM-Based"], "d": "models", "x": "A framework that uses large language models with chain-of-thought prompting to evaluate natural language generation...", "l": "g", "k": ["g-eval", "framework", "uses", "large", "language", "models", "chain-of-thought", "prompting", "evaluate", "natural", "generation", "quality", "achieving", "high", "correlation"]}, {"id": "term-g-means-algorithm", "t": "G-Means Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A clustering algorithm that extends k-means by testing whether each cluster follows a Gaussian distribution using the...", "l": "g", "k": ["g-means", "algorithm", "clustering", "extends", "k-means", "testing", "cluster", "follows", "gaussian", "distribution", "anderson-darling", "test", "splits", "non-gaussian", "clusters"]}, {"id": "term-gabor-filter", "t": "Gabor Filter", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A linear filter used in texture analysis and feature extraction that models the response of simple cells in the visual...", "l": "g", "k": ["gabor", "filter", "linear", "texture", "analysis", "feature", "extraction", "models", "response", "simple", "cells", "visual", "cortex", "defined", "gaussian"]}, {"id": "term-gaia", "t": "GAIA", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark for General AI Assistants testing real-world problem solving that requires web browsing multi-step...", "l": "g", "k": ["gaia", "benchmark", "general", "assistants", "testing", "real-world", "problem", "solving", "requires", "web", "browsing", "multi-step", "reasoning", "tool", "designed"]}, {"id": "term-gaia-1", "t": "GAIA-1", "tg": ["Models", "Technical", "Vision", "Autonomous"], "d": "models", "x": "A generative world model from Wayve that creates realistic driving scenes by learning from video and text and action...", "l": "g", "k": ["gaia-1", "generative", "world", "model", "wayve", "creates", "realistic", "driving", "scenes", "learning", "video", "text", "action", "inputs", "autonomous"]}, {"id": "term-galactica", "t": "Galactica", "tg": ["Models", "Technical"], "d": "models", "x": "A large language model trained by Meta AI on 48 million scientific papers citations and other academic content....", "l": "g", "k": ["galactica", "large", "language", "model", "trained", "meta", "million", "scientific", "papers", "citations", "academic", "content", "designed", "knowledge", "tasks"]}, {"id": "term-galactica-120b", "t": "Galactica-120B", "tg": ["Models", "Technical", "NLP", "Scientific"], "d": "models", "x": "The largest variant of the Galactica scientific language model with 120B parameters trained on scientific papers and...", "l": "g", "k": ["galactica-120b", "largest", "variant", "galactica", "scientific", "language", "model", "120b", "parameters", "trained", "papers", "reference", "materials", "knowledge", "bases"]}, {"id": "term-gamengen", "t": "GameNGen", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A game engine powered by a neural model from Google that generates interactive game frames in real time by simulating...", "l": "g", "k": ["gamengen", "game", "engine", "powered", "neural", "model", "google", "generates", "interactive", "frames", "real", "time", "simulating", "doom", "diffusion"]}, {"id": "term-gamma-distribution", "t": "Gamma Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A two-parameter family of continuous probability distributions that generalizes the exponential distribution. It models...", "l": "g", "k": ["gamma", "distribution", "two-parameter", "family", "continuous", "probability", "distributions", "generalizes", "exponential", "models", "waiting", "time", "specified", "number", "events"]}, {"id": "term-gan", "t": "GAN (Generative Adversarial Network)", "tg": ["Architecture", "Generative"], "d": "models", "x": "A neural network architecture with two competing networks: a generator creating content and a discriminator evaluating...", "l": "g", "k": ["gan", "generative", "adversarial", "network", "neural", "architecture", "competing", "networks", "generator", "creating", "content", "discriminator", "evaluating", "pioneered", "realistic"]}, {"id": "term-gan-discriminator", "t": "GAN Discriminator", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The component of a generative adversarial network that learns to distinguish real data from generated samples,...", "l": "g", "k": ["gan", "discriminator", "component", "generative", "adversarial", "network", "learns", "distinguish", "real", "data", "generated", "samples", "providing", "training", "signal"]}, {"id": "term-gan-generator", "t": "GAN Generator", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The component of a generative adversarial network that transforms random noise into synthetic data samples, trained to...", "l": "g", "k": ["gan", "generator", "component", "generative", "adversarial", "network", "transforms", "random", "noise", "synthetic", "data", "samples", "trained", "fool", "discriminator"]}, {"id": "term-gan-invention", "t": "GAN Invention", "tg": ["History", "Milestones"], "d": "history", "x": "The invention of Generative Adversarial Networks by Ian Goodfellow and colleagues in 2014. GANs train two neural...", "l": "g", "k": ["gan", "invention", "generative", "adversarial", "networks", "ian", "goodfellow", "colleagues", "gans", "train", "neural", "generator", "discriminator", "competition", "enabling"]}, {"id": "term-gan-inversion", "t": "GAN Inversion", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of finding the latent code in a pre-trained GAN's latent space that best reconstructs a given real image,...", "l": "g", "k": ["gan", "inversion", "process", "finding", "latent", "code", "pre-trained", "space", "best", "reconstructs", "given", "real", "image", "enabling", "gan-based"]}, {"id": "term-gate-all-around-transistor", "t": "Gate-All-Around Transistor", "tg": ["Fabrication", "Transistor", "Architecture"], "d": "hardware", "x": "Next-generation transistor design where the gate completely surrounds the channel from all sides. Succeeds FinFET at...", "l": "g", "k": ["gate-all-around", "transistor", "next-generation", "design", "gate", "completely", "surrounds", "channel", "sides", "succeeds", "finfet", "3nm", "improved", "electrostatic", "control"]}, {"id": "term-gated-linear-unit", "t": "Gated Linear Unit", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An activation mechanism that multiplies a linear projection of the input by a sigmoid-gated linear projection, allowing...", "l": "g", "k": ["gated", "linear", "unit", "activation", "mechanism", "multiplies", "projection", "input", "sigmoid-gated", "allowing", "network", "control", "information", "flow", "commonly"]}, {"id": "term-gatortron", "t": "GatorTron", "tg": ["Models", "Technical", "Medical", "NLP"], "d": "models", "x": "A large clinical language model with up to 8.9B parameters trained on over 90 billion words of clinical text from the...", "l": "g", "k": ["gatortron", "large", "clinical", "language", "model", "parameters", "trained", "billion", "words", "text", "university", "florida", "health", "system"]}, {"id": "term-gauss-newton-algorithm", "t": "Gauss-Newton Algorithm", "tg": ["Algorithms", "Technical", "Optimization", "Numerical"], "d": "algorithms", "x": "An iterative method for solving nonlinear least squares problems that approximates the Hessian using the Jacobian...", "l": "g", "k": ["gauss-newton", "algorithm", "iterative", "method", "solving", "nonlinear", "least", "squares", "problems", "approximates", "hessian", "jacobian", "matrix", "converges", "quickly"]}, {"id": "term-gauss-seidel-method", "t": "Gauss-Seidel Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An iterative method for solving linear systems that improves upon the Jacobi method by using updated values as soon as...", "l": "g", "k": ["gauss-seidel", "method", "iterative", "solving", "linear", "systems", "improves", "upon", "jacobi", "updated", "values", "soon", "available", "within", "iteration"]}, {"id": "term-gaussian-elimination", "t": "Gaussian Elimination", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "A method for solving systems of linear equations by performing row operations to transform the augmented matrix into...", "l": "g", "k": ["gaussian", "elimination", "method", "solving", "systems", "linear", "equations", "performing", "row", "operations", "transform", "augmented", "matrix", "echelon", "form"]}, {"id": "term-gaussian-kernel", "t": "Gaussian Kernel", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A kernel function based on the Gaussian (normal) distribution, commonly used in kernel density estimation, smoothing,...", "l": "g", "k": ["gaussian", "kernel", "function", "based", "normal", "distribution", "commonly", "density", "estimation", "smoothing", "svms", "bandwidth", "parameter", "controls", "width"]}, {"id": "term-gaussian-mechanism", "t": "Gaussian Mechanism", "tg": ["Algorithms", "Technical", "Privacy"], "d": "algorithms", "x": "A differential privacy mechanism that adds Gaussian noise calibrated to the sensitivity of a query. Provides...", "l": "g", "k": ["gaussian", "mechanism", "differential", "privacy", "adds", "noise", "calibrated", "sensitivity", "query", "provides", "approximate", "epsilon-delta", "widely", "real-valued", "outputs"]}, {"id": "term-gaussian-mixture-model", "t": "Gaussian Mixture Model", "tg": ["Machine Learning", "Clustering"], "d": "general", "x": "A probabilistic model that represents data as generated from a mixture of a finite number of Gaussian distributions...", "l": "g", "k": ["gaussian", "mixture", "model", "probabilistic", "represents", "data", "generated", "finite", "number", "distributions", "unknown", "parameters", "typically", "estimated", "via"]}, {"id": "term-gaussian-mixture-model-algorithm", "t": "Gaussian Mixture Model Algorithm", "tg": ["Algorithms", "Fundamentals", "Clustering"], "d": "algorithms", "x": "A probabilistic model that represents data as a mixture of multiple Gaussian distributions. Parameters are typically...", "l": "g", "k": ["gaussian", "mixture", "model", "algorithm", "probabilistic", "represents", "data", "multiple", "distributions", "parameters", "typically", "learned", "expectation-maximization", "provides", "soft"]}, {"id": "term-gaussian-process", "t": "Gaussian Process", "tg": ["Machine Learning", "Bayesian Methods"], "d": "general", "x": "A non-parametric Bayesian model that defines a probability distribution over functions, where any finite collection of...", "l": "g", "k": ["gaussian", "process", "non-parametric", "bayesian", "model", "defines", "probability", "distribution", "functions", "finite", "collection", "function", "values", "follows", "multivariate"]}, {"id": "term-gaussian-quadrature", "t": "Gaussian Quadrature", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A family of numerical integration rules that select optimal evaluation points and weights to achieve maximum accuracy....", "l": "g", "k": ["gaussian", "quadrature", "family", "numerical", "integration", "rules", "select", "optimal", "evaluation", "points", "weights", "achieve", "maximum", "accuracy", "n-point"]}, {"id": "term-gaussian-splatting", "t": "Gaussian Splatting", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A 3D scene representation that models scenes as collections of 3D Gaussian primitives, enabling real-time rendering of...", "l": "g", "k": ["gaussian", "splatting", "scene", "representation", "models", "scenes", "collections", "primitives", "enabling", "real-time", "rendering", "photorealistic", "novel", "views", "efficient"]}, {"id": "term-gaussiansplat", "t": "GaussianSplat", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A 3D scene representation method that uses anisotropic 3D Gaussians for real-time novel view synthesis and radiance...", "l": "g", "k": ["gaussiansplat", "scene", "representation", "method", "uses", "anisotropic", "gaussians", "real-time", "novel", "view", "synthesis", "radiance", "field", "rendering"]}, {"id": "term-gaze-estimation", "t": "Gaze Estimation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of predicting where a person is looking based on their eye appearance and head pose in images or video, used...", "l": "g", "k": ["gaze", "estimation", "task", "predicting", "person", "looking", "based", "eye", "appearance", "head", "pose", "images", "video", "attention", "analysis"]}, {"id": "term-gazetteer", "t": "Gazetteer", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A curated list of entity names organized by type, such as person names, locations, or organizations, used as a feature...", "l": "g", "k": ["gazetteer", "curated", "list", "entity", "names", "organized", "type", "person", "locations", "organizations", "feature", "lookup", "resource", "named", "recognition"]}, {"id": "term-gddr5", "t": "GDDR5", "tg": ["Memory", "GPU"], "d": "hardware", "x": "Fifth generation graphics double data rate memory used in GPUs from 2008 through the mid-2010s. Offered bandwidths up...", "l": "g", "k": ["gddr5", "fifth", "generation", "graphics", "double", "data", "rate", "memory", "gpus", "mid-2010s", "offered", "bandwidths", "gbps", "per", "pin"]}, {"id": "term-gddr6", "t": "GDDR6", "tg": ["Memory", "GPU"], "d": "hardware", "x": "Sixth generation graphics double data rate memory offering speeds up to 16 Gbps per pin. Standard memory type for...", "l": "g", "k": ["gddr6", "sixth", "generation", "graphics", "double", "data", "rate", "memory", "offering", "speeds", "gbps", "per", "pin", "standard", "type"]}, {"id": "term-gddr6x", "t": "GDDR6X", "tg": ["Memory", "GPU"], "d": "hardware", "x": "Enhanced version of GDDR6 memory developed by Micron using PAM4 signaling to achieve speeds up to 23 Gbps per pin. Used...", "l": "g", "k": ["gddr6x", "enhanced", "version", "gddr6", "memory", "developed", "micron", "pam4", "signaling", "achieve", "speeds", "gbps", "per", "pin", "high-end"]}, {"id": "term-gddr7", "t": "GDDR7", "tg": ["Memory", "GPU"], "d": "hardware", "x": "Seventh generation graphics memory using PAM3 signaling to achieve speeds exceeding 32 Gbps per pin. Expected to power...", "l": "g", "k": ["gddr7", "seventh", "generation", "graphics", "memory", "pam3", "signaling", "achieve", "speeds", "exceeding", "gbps", "per", "pin", "expected", "power"]}, {"id": "term-gdpr-ai-provisions", "t": "GDPR AI Provisions", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Provisions within the EU General Data Protection Regulation relevant to AI, including the right not to be subject to...", "l": "g", "k": ["gdpr", "provisions", "within", "general", "data", "protection", "regulation", "relevant", "including", "right", "subject", "purely", "automated", "decisions", "impact"]}, {"id": "term-gelu", "t": "GELU (Gaussian Error Linear Unit)", "tg": ["Architecture", "Function"], "d": "models", "x": "An activation function commonly used in transformers that applies a smooth, probabilistic non-linearity. Outperforms...", "l": "g", "k": ["gelu", "gaussian", "error", "linear", "unit", "activation", "function", "commonly", "transformers", "applies", "smooth", "probabilistic", "non-linearity", "outperforms", "relu"]}, {"id": "term-gemini", "t": "Gemini", "tg": ["Model", "Google"], "d": "models", "x": "Google's family of multimodal AI models that can process text, images, audio, and video. Powers Google's AI features...", "l": "g", "k": ["gemini", "google", "family", "multimodal", "models", "process", "text", "images", "audio", "video", "powers", "features", "including", "bard", "workspace"]}, {"id": "term-gemini-ai", "t": "Gemini (AI)", "tg": ["History", "Systems"], "d": "history", "x": "A multimodal AI model family developed by Google DeepMind announced in December 2023. Gemini was designed from the...", "l": "g", "k": ["gemini", "multimodal", "model", "family", "developed", "google", "deepmind", "announced", "december", "designed", "ground", "natively", "understanding", "generating", "text"]}, {"id": "term-gemini-flash", "t": "Gemini Flash", "tg": ["Models", "Technical"], "d": "models", "x": "A lightweight model in the Gemini family optimized for speed and efficiency. Designed for high-volume low-latency...", "l": "g", "k": ["gemini", "flash", "lightweight", "model", "family", "optimized", "speed", "efficiency", "designed", "high-volume", "low-latency", "applications", "fast", "responses", "prioritized"]}, {"id": "term-gemini-launch", "t": "Gemini Launch", "tg": ["History", "Milestones"], "d": "history", "x": "Google DeepMind's release of Gemini in December 2023, a family of multimodal large language models designed to process...", "l": "g", "k": ["gemini", "launch", "google", "deepmind", "release", "december", "family", "multimodal", "large", "language", "models", "designed", "process", "text", "images"]}, {"id": "term-gemini-nano", "t": "Gemini Nano", "tg": ["Models", "Technical", "NLP", "Vision"], "d": "models", "x": "The smallest variant of Google Gemini designed to run on-device for mobile and edge applications with efficient...", "l": "g", "k": ["gemini", "nano", "smallest", "variant", "google", "designed", "run", "on-device", "mobile", "edge", "applications", "efficient", "multimodal", "reasoning", "capabilities"]}, {"id": "term-gemini-pro", "t": "Gemini Pro", "tg": ["Models", "Technical"], "d": "models", "x": "A mid-tier model in Google's Gemini family designed for a wide range of tasks. Powers many Google AI products and is...", "l": "g", "k": ["gemini", "pro", "mid-tier", "model", "google", "family", "designed", "wide", "range", "tasks", "powers", "products", "available", "api", "balances"]}, {"id": "term-gemini-ultra", "t": "Gemini Ultra", "tg": ["Models", "Technical", "NLP", "Vision"], "d": "models", "x": "The largest and most capable variant of Google Gemini designed for highly complex reasoning tasks across text and image...", "l": "g", "k": ["gemini", "ultra", "largest", "capable", "variant", "google", "designed", "highly", "complex", "reasoning", "tasks", "across", "text", "image", "audio"]}, {"id": "term-gemma", "t": "Gemma", "tg": ["Models", "Technical"], "d": "models", "x": "A family of lightweight open models built by Google DeepMind from the same technology used to create Gemini. Available...", "l": "g", "k": ["gemma", "family", "lightweight", "open", "models", "built", "google", "deepmind", "technology", "create", "gemini", "available", "parameter", "sizes", "optimized"]}, {"id": "term-gemma-2", "t": "Gemma 2", "tg": ["Models", "Technical"], "d": "models", "x": "A second-generation family of lightweight open models by Google DeepMind with improved performance and efficiency...", "l": "g", "k": ["gemma", "second-generation", "family", "lightweight", "open", "models", "google", "deepmind", "improved", "performance", "efficiency", "across", "27b", "parameter", "sizes"]}, {"id": "term-gemnet", "t": "GemNet", "tg": ["Models", "Scientific"], "d": "models", "x": "A geometric message passing neural network for molecular property prediction that uses directional information through...", "l": "g", "k": ["gemnet", "geometric", "message", "passing", "neural", "network", "molecular", "property", "prediction", "uses", "directional", "information", "two-hop", "dihedral", "angles"]}, {"id": "term-gen-2", "t": "Gen-2", "tg": ["Models", "Technical"], "d": "models", "x": "A multimodal AI model by Runway for generating and editing video from text images or existing video. One of the first...", "l": "g", "k": ["gen-2", "multimodal", "model", "runway", "generating", "editing", "video", "text", "images", "existing", "commercially", "available", "generation", "tools", "supports"]}, {"id": "term-genai-bench", "t": "GenAI-Bench", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating generative AI models across multiple modalities including text-to-image and text-to-video...", "l": "g", "k": ["genai-bench", "benchmark", "evaluating", "generative", "models", "across", "multiple", "modalities", "including", "text-to-image", "text-to-video", "generation", "tests", "complex", "compositional"]}, {"id": "term-gencast", "t": "GenCast", "tg": ["Models", "Scientific"], "d": "models", "x": "A diffusion-based probabilistic weather model from DeepMind that generates ensemble forecasts and outperforms...", "l": "g", "k": ["gencast", "diffusion-based", "probabilistic", "weather", "model", "deepmind", "generates", "ensemble", "forecasts", "outperforms", "operational", "prediction", "systems", "metrics"]}, {"id": "term-gender-bias-in-ai", "t": "Gender Bias in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Systematic unfairness in AI system behavior related to gender. Common in language models hiring tools and image...", "l": "g", "k": ["gender", "bias", "systematic", "unfairness", "system", "behavior", "related", "common", "language", "models", "hiring", "tools", "image", "generation", "systems"]}, {"id": "term-general-problem-solver", "t": "General Problem Solver", "tg": ["History", "Milestones"], "d": "history", "x": "An AI program developed by Newell and Simon in 1957 that used means-ends analysis to solve a wide range of formalized...", "l": "g", "k": ["general", "problem", "solver", "program", "developed", "newell", "simon", "means-ends", "analysis", "solve", "wide", "range", "formalized", "problems", "representing"]}, {"id": "term-generalization", "t": "Generalization", "tg": ["Concept", "Quality"], "d": "general", "x": "A model's ability to perform well on new, unseen data rather than just memorizing training examples. The fundamental...", "l": "g", "k": ["generalization", "model", "ability", "perform", "unseen", "data", "rather", "memorizing", "training", "examples", "fundamental", "goal", "machine", "learning"]}, {"id": "term-generalized-advantage-estimation", "t": "Generalized Advantage Estimation (GAE)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A technique that computes advantage function estimates using an exponentially-weighted average of multi-step TD errors,...", "l": "g", "k": ["generalized", "advantage", "estimation", "gae", "technique", "computes", "function", "estimates", "exponentially-weighted", "average", "multi-step", "errors", "controlled", "lambda", "parameter"]}, {"id": "term-generalized-linear-model", "t": "Generalized Linear Model", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A flexible generalization of ordinary linear regression that allows the response variable to follow any distribution...", "l": "g", "k": ["generalized", "linear", "model", "flexible", "generalization", "ordinary", "regression", "allows", "response", "variable", "follow", "distribution", "exponential", "family", "link"]}, {"id": "term-generalized-policy-iteration", "t": "Generalized Policy Iteration", "tg": ["Algorithms", "Fundamentals", "RL"], "d": "algorithms", "x": "A framework describing how policy evaluation and policy improvement alternate and interact to converge to optimal...", "l": "g", "k": ["generalized", "policy", "iteration", "framework", "describing", "evaluation", "improvement", "alternate", "interact", "converge", "optimal", "policies", "nearly", "reinforcement", "learning"]}, {"id": "term-generated-knowledge-prompting", "t": "Generated Knowledge Prompting", "tg": ["Prompt Engineering", "Knowledge Augmentation"], "d": "general", "x": "A technique where the model first generates relevant knowledge or facts about a topic, then uses that self-generated...", "l": "g", "k": ["generated", "knowledge", "prompting", "technique", "model", "generates", "relevant", "facts", "topic", "uses", "self-generated", "additional", "context", "answer", "downstream"]}, {"id": "term-generation", "t": "Generation", "tg": ["Process", "Core Concept"], "d": "general", "x": "The process of producing new content from an AI model. Text generation works by predicting one token at a time; image...", "l": "g", "k": ["generation", "process", "producing", "content", "model", "text", "works", "predicting", "token", "time", "image", "uses", "diffusion", "similar", "processes"]}, {"id": "term-generative-adversarial-imitation-learning", "t": "Generative Adversarial Imitation Learning", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "An imitation learning algorithm that uses a discriminator to distinguish between expert and agent trajectories. The...", "l": "g", "k": ["generative", "adversarial", "imitation", "learning", "algorithm", "uses", "discriminator", "distinguish", "expert", "agent", "trajectories", "trained", "fool", "matching", "occupancy"]}, {"id": "term-gail", "t": "Generative Adversarial Imitation Learning (GAIL)", "tg": ["Reinforcement Learning", "Imitation"], "d": "general", "x": "An imitation learning algorithm that uses a GAN-like framework where a discriminator distinguishes between agent and...", "l": "g", "k": ["generative", "adversarial", "imitation", "learning", "gail", "algorithm", "uses", "gan-like", "framework", "discriminator", "distinguishes", "agent", "expert", "trajectories", "policy"]}, {"id": "term-gan-history", "t": "Generative Adversarial Network History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of GANs from Ian Goodfellow's 2014 invention through progressive improvements including DCGAN,...", "l": "g", "k": ["generative", "adversarial", "network", "history", "development", "gans", "ian", "goodfellow", "invention", "progressive", "improvements", "including", "dcgan", "stylegan", "biggan"]}, {"id": "term-generative-ai", "t": "Generative AI", "tg": ["Field", "Category"], "d": "general", "x": "AI systems that can create new content (text, images, code, music, video) rather than just analyzing existing data....", "l": "g", "k": ["generative", "systems", "create", "content", "text", "images", "code", "music", "video", "rather", "analyzing", "existing", "data", "includes", "chatbots"]}, {"id": "term-generative-ai-era", "t": "Generative AI Era", "tg": ["History", "Milestones"], "d": "history", "x": "The period beginning approximately in 2022 with the release of ChatGPT DALL-E 2 and Stable Diffusion when generative AI...", "l": "g", "k": ["generative", "era", "period", "beginning", "approximately", "release", "chatgpt", "dall-e", "stable", "diffusion", "models", "capable", "creating", "text", "images"]}, {"id": "term-generative-ai-misuse", "t": "Generative AI Misuse", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The use of generative AI systems to create harmful content including disinformation non-consensual imagery fraud...", "l": "g", "k": ["generative", "misuse", "systems", "create", "harmful", "content", "including", "disinformation", "non-consensual", "imagery", "fraud", "materials", "spam", "growing", "challenge"]}, {"id": "term-generative-question-answering", "t": "Generative Question Answering", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A QA approach where the model generates a free-form answer in natural language rather than extracting a span, enabling...", "l": "g", "k": ["generative", "question", "answering", "approach", "model", "generates", "free-form", "answer", "natural", "language", "rather", "extracting", "span", "enabling", "responses"]}, {"id": "term-genetic-algorithm", "t": "Genetic Algorithm", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "An evolutionary optimization algorithm inspired by natural selection. Maintains a population of candidate solutions...", "l": "g", "k": ["genetic", "algorithm", "evolutionary", "optimization", "inspired", "natural", "selection", "maintains", "population", "candidate", "solutions", "undergo", "crossover", "mutation", "generations"]}, {"id": "term-genetic-algorithms", "t": "Genetic Algorithms", "tg": ["History", "Milestones"], "d": "history", "x": "Optimization algorithms inspired by natural selection, developed by John Holland in the 1960s-1970s, that evolve...", "l": "g", "k": ["genetic", "algorithms", "optimization", "inspired", "natural", "selection", "developed", "john", "holland", "1960s-1970s", "evolve", "candidate", "solutions", "crossover", "mutation"]}, {"id": "term-geneval", "t": "GenEval", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating compositional image generation testing object counting color binding spatial relationships...", "l": "g", "k": ["geneval", "benchmark", "evaluating", "compositional", "image", "generation", "testing", "object", "counting", "color", "binding", "spatial", "relationships", "overall", "quality"]}, {"id": "term-genie", "t": "Genie", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A generative interactive environment model from DeepMind that creates playable 2D worlds from single images or text...", "l": "g", "k": ["genie", "generative", "interactive", "environment", "model", "deepmind", "creates", "playable", "worlds", "single", "images", "text", "descriptions", "latent", "action"]}, {"id": "term-genie-2", "t": "Genie 2", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A large-scale world model from Google DeepMind that generates persistent and controllable 3D environments from single...", "l": "g", "k": ["genie", "large-scale", "world", "model", "google", "deepmind", "generates", "persistent", "controllable", "environments", "single", "images", "training", "embodied", "agents"]}, {"id": "term-geoffrey-hinton", "t": "Geoffrey Hinton", "tg": ["History", "Pioneers"], "d": "history", "x": "British-Canadian computer scientist known as a godfather of deep learning, who co-developed backpropagation, Boltzmann...", "l": "g", "k": ["geoffrey", "hinton", "british-canadian", "computer", "scientist", "known", "godfather", "deep", "learning", "co-developed", "backpropagation", "boltzmann", "machines", "belief", "networks"]}, {"id": "term-geometric-augmentation", "t": "Geometric Augmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Image augmentation techniques that modify the spatial arrangement of pixels including rotation, translation, scaling,...", "l": "g", "k": ["geometric", "augmentation", "image", "techniques", "modify", "spatial", "arrangement", "pixels", "including", "rotation", "translation", "scaling", "shearing", "flipping", "perspective"]}, {"id": "term-geoqa", "t": "GeoQA", "tg": ["Benchmark", "Multimodal", "Reasoning"], "d": "datasets", "x": "A geometric problem solving dataset requiring mathematical reasoning about geometric figures. Tests the ability to...", "l": "g", "k": ["geoqa", "geometric", "problem", "solving", "dataset", "requiring", "mathematical", "reasoning", "figures", "tests", "ability", "combine", "visual", "understanding"]}, {"id": "term-georgetown-ibm-experiment", "t": "Georgetown-IBM Experiment", "tg": ["History", "Milestones"], "d": "history", "x": "A 1954 demonstration of automatic translation of Russian sentences into English using an IBM 701 computer. Though...", "l": "g", "k": ["georgetown-ibm", "experiment", "demonstration", "automatic", "translation", "russian", "sentences", "english", "ibm", "computer", "limited", "words", "six", "grammar", "rules"]}, {"id": "term-ges-algorithm", "t": "GES Algorithm", "tg": ["Algorithms", "Technical", "Causal"], "d": "algorithms", "x": "Greedy Equivalence Search is a score-based causal discovery algorithm that searches over equivalence classes of...", "l": "g", "k": ["ges", "algorithm", "greedy", "equivalence", "search", "score-based", "causal", "discovery", "searches", "classes", "directed", "acyclic", "graphs", "performs", "forward"]}, {"id": "term-gguf", "t": "GGUF", "tg": ["LLM", "Inference"], "d": "models", "x": "A binary file format designed for storing quantized language models optimized for CPU and hybrid CPU/GPU inference,...", "l": "g", "k": ["gguf", "binary", "file", "format", "designed", "storing", "quantized", "language", "models", "optimized", "cpu", "hybrid", "gpu", "inference", "commonly"]}, {"id": "term-gh200", "t": "GH200 Grace Hopper Superchip", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "NVIDIA's integrated CPU-GPU superchip combining a Grace ARM CPU with a Hopper H200 GPU connected by a high-bandwidth...", "l": "g", "k": ["gh200", "grace", "hopper", "superchip", "nvidia", "integrated", "cpu-gpu", "combining", "arm", "cpu", "h200", "gpu", "connected", "high-bandwidth", "nvlink-c2c"]}, {"id": "term-ghost-work", "t": "Ghost Work", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "The often invisible human labor that powers AI systems, including data labeling, content moderation, and quality...", "l": "g", "k": ["ghost", "work", "invisible", "human", "labor", "powers", "systems", "including", "data", "labeling", "content", "moderation", "quality", "assurance", "frequently"]}, {"id": "term-gibbs-sampling", "t": "Gibbs Sampling", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "An MCMC method that samples each variable in turn from its conditional distribution given the current values of all...", "l": "g", "k": ["gibbs", "sampling", "mcmc", "method", "samples", "variable", "turn", "conditional", "distribution", "given", "current", "values", "variables", "efficient", "distributions"]}, {"id": "term-gigaspeech", "t": "GigaSpeech", "tg": ["Training Corpus", "Speech"], "d": "datasets", "x": "A 10000-hour English speech recognition corpus from audiobooks podcasts and YouTube. Designed to provide large-scale...", "l": "g", "k": ["gigaspeech", "10000-hour", "english", "speech", "recognition", "corpus", "audiobooks", "podcasts", "youtube", "designed", "provide", "large-scale", "data", "accurate", "transcriptions"]}, {"id": "term-gigast", "t": "GigaST", "tg": ["Training Corpus", "Speech", "Translation"], "d": "datasets", "x": "A large-scale speech translation dataset containing over 10000 hours of speech with translations. Created by aligning...", "l": "g", "k": ["gigast", "large-scale", "speech", "translation", "dataset", "containing", "hours", "translations", "created", "aligning", "gigaspeech", "audio", "machine", "research"]}, {"id": "term-gini-impurity", "t": "Gini Impurity", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A measure of the probability that a randomly chosen sample would be misclassified if labeled according to the class...", "l": "g", "k": ["gini", "impurity", "measure", "probability", "randomly", "chosen", "sample", "misclassified", "labeled", "according", "class", "distribution", "node", "commonly", "splitting"]}, {"id": "term-girvan-newman-algorithm", "t": "Girvan-Newman Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A community detection method that identifies communities by progressively removing edges with the highest betweenness...", "l": "g", "k": ["girvan-newman", "algorithm", "community", "detection", "method", "identifies", "communities", "progressively", "removing", "edges", "highest", "betweenness", "centrality", "recalculates", "removal"]}, {"id": "term-github-code", "t": "GitHub Code", "tg": ["Training Corpus", "Code"], "d": "datasets", "x": "Large-scale collections of source code from public GitHub repositories used to train code generation models. Various...", "l": "g", "k": ["github", "code", "large-scale", "collections", "source", "public", "repositories", "train", "generation", "models", "various", "filtered", "versions", "exist", "including"]}, {"id": "term-github-copilot", "t": "GitHub Copilot", "tg": ["Product", "Development"], "d": "general", "x": "An AI pair programmer integrated into code editors. Uses LLMs to suggest code completions, write functions, and explain...", "l": "g", "k": ["github", "copilot", "pair", "programmer", "integrated", "code", "editors", "uses", "llms", "suggest", "completions", "write", "functions", "explain", "successful"]}, {"id": "term-github-copilot-launch", "t": "GitHub Copilot Launch", "tg": ["History", "Milestones"], "d": "history", "x": "The public release of GitHub Copilot in June 2022 an AI pair programming tool powered by OpenAI Codex. Copilot...", "l": "g", "k": ["github", "copilot", "launch", "public", "release", "june", "pair", "programming", "tool", "powered", "openai", "codex", "generates", "code", "suggestions"]}, {"id": "term-github-issues-dataset", "t": "GitHub Issues Dataset", "tg": ["Training Corpus", "Code"], "d": "datasets", "x": "Collections of GitHub issue reports and discussions used for training models on software engineering tasks including...", "l": "g", "k": ["github", "issues", "dataset", "collections", "issue", "reports", "discussions", "training", "models", "software", "engineering", "tasks", "including", "bug", "reporting"]}, {"id": "term-givens-rotation", "t": "Givens Rotation", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A rotation in the plane of two coordinate axes used to introduce zeros into a matrix. Applied sequentially to perform...", "l": "g", "k": ["givens", "rotation", "plane", "coordinate", "axes", "introduce", "zeros", "matrix", "applied", "sequentially", "perform", "decomposition", "particularly", "efficient", "sparse"]}, {"id": "term-gla", "t": "GLA", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "Gated Linear Attention is a recurrent model that adds a data-dependent gating mechanism to linear attention for...", "l": "g", "k": ["gla", "gated", "linear", "attention", "recurrent", "model", "adds", "data-dependent", "gating", "mechanism", "improved", "performance", "language", "modeling", "tasks"]}, {"id": "term-glide", "t": "GLIDE", "tg": ["Models", "Technical", "Vision", "History"], "d": "models", "x": "Guided Language to Image Diffusion for Generation and Editing is an OpenAI model that uses classifier-free guidance for...", "l": "g", "k": ["glide", "guided", "language", "image", "diffusion", "generation", "editing", "openai", "model", "uses", "classifier-free", "guidance", "text-conditional"]}, {"id": "term-glm", "t": "GLM", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "General Language Model is an autoregressive blank infilling pre-training framework from Tsinghua University that...", "l": "g", "k": ["glm", "general", "language", "model", "autoregressive", "blank", "infilling", "pre-training", "framework", "tsinghua", "university", "unifies", "understanding", "generation", "tasks"]}, {"id": "term-glm-4", "t": "GLM-4", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A fourth-generation general language model from Tsinghua that supports multi-turn dialogue and web browsing and code...", "l": "g", "k": ["glm-4", "fourth-generation", "general", "language", "model", "tsinghua", "supports", "multi-turn", "dialogue", "web", "browsing", "code", "execution", "multimodal", "inputs"]}, {"id": "term-glm-4v", "t": "GLM-4V", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multimodal variant of the GLM-4 language model from Tsinghua University that supports visual understanding alongside...", "l": "g", "k": ["glm-4v", "multimodal", "variant", "glm-4", "language", "model", "tsinghua", "university", "supports", "visual", "understanding", "alongside", "text", "generation"]}, {"id": "term-global-average-pooling", "t": "Global Average Pooling", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A pooling operation that computes the mean of each feature map across all spatial dimensions, reducing the feature map...", "l": "g", "k": ["global", "average", "pooling", "operation", "computes", "mean", "feature", "map", "across", "spatial", "dimensions", "reducing", "single", "value", "per"]}, {"id": "term-globalbench", "t": "GlobalBench", "tg": ["Benchmark", "NLP", "Multilingual", "Evaluation"], "d": "datasets", "x": "A benchmark measuring NLP model performance and data availability across the worlds languages. Highlights the disparity...", "l": "g", "k": ["globalbench", "benchmark", "measuring", "nlp", "model", "performance", "data", "availability", "across", "worlds", "languages", "highlights", "disparity", "high-resource", "low-resource"]}, {"id": "term-globalfoundries", "t": "GlobalFoundries", "tg": ["Fabrication", "Foundry"], "d": "hardware", "x": "Major semiconductor foundry offering mature process nodes for automotive IoT and specialized chips. Exited the...", "l": "g", "k": ["globalfoundries", "major", "semiconductor", "foundry", "offering", "mature", "process", "nodes", "automotive", "iot", "specialized", "chips", "exited", "leading-edge", "race"]}, {"id": "term-glove", "t": "GloVe", "tg": ["NLP", "Embeddings"], "d": "general", "x": "Global Vectors for Word Representation, a word embedding method that trains on aggregated word co-occurrence statistics...", "l": "g", "k": ["glove", "global", "vectors", "word", "representation", "embedding", "method", "trains", "aggregated", "co-occurrence", "statistics", "corpus", "combining", "matrix", "factorization"]}, {"id": "term-glove-algorithm", "t": "GloVe Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP"], "d": "algorithms", "x": "Global Vectors for Word Representation learns word embeddings by factorizing the log of the word co-occurrence matrix....", "l": "g", "k": ["glove", "algorithm", "global", "vectors", "word", "representation", "learns", "embeddings", "factorizing", "log", "co-occurrence", "matrix", "combines", "advantages", "factorization"]}, {"id": "term-glu", "t": "GLU", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Gated Linear Unit that splits input into two halves and applies a sigmoid gate to one half then multiplies...", "l": "g", "k": ["glu", "gated", "linear", "unit", "splits", "input", "halves", "applies", "sigmoid", "gate", "half", "multiplies", "element-wise", "introduced", "language"]}, {"id": "term-glue", "t": "GLUE", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "The General Language Understanding Evaluation benchmark containing nine natural language understanding tasks including...", "l": "g", "k": ["glue", "general", "language", "understanding", "evaluation", "benchmark", "containing", "nine", "natural", "tasks", "including", "sentiment", "analysis", "textual", "entailment"]}, {"id": "term-glue-benchmark", "t": "GLUE Benchmark", "tg": ["History", "Milestones"], "d": "history", "x": "The General Language Understanding Evaluation benchmark introduced in 2018 consisting of nine natural language...", "l": "g", "k": ["glue", "benchmark", "general", "language", "understanding", "evaluation", "introduced", "consisting", "nine", "natural", "tasks", "provided", "standardized", "evaluate", "models"]}, {"id": "term-gmflow", "t": "GMFlow", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A Transformer-based optical flow model that reformulates optical flow as a global matching problem using...", "l": "g", "k": ["gmflow", "transformer-based", "optical", "flow", "model", "reformulates", "global", "matching", "problem", "cross-attention", "accurate", "motion", "estimation"]}, {"id": "term-gmres-algorithm", "t": "GMRES Algorithm", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "Generalized Minimal Residual method is an iterative algorithm for solving large non-symmetric linear systems. Minimizes...", "l": "g", "k": ["gmres", "algorithm", "generalized", "minimal", "residual", "method", "iterative", "solving", "large", "non-symmetric", "linear", "systems", "minimizes", "krylov", "subspace"]}, {"id": "term-gnome", "t": "GNoME", "tg": ["Models", "Scientific"], "d": "models", "x": "Graph Networks for Materials Exploration from DeepMind that discovers stable crystal structures and predicts material...", "l": "g", "k": ["gnome", "graph", "networks", "materials", "exploration", "deepmind", "discovers", "stable", "crystal", "structures", "predicts", "material", "properties", "science"]}, {"id": "term-gnome-sort", "t": "Gnome Sort", "tg": ["Algorithms", "Technical", "Sorting"], "d": "algorithms", "x": "A sorting algorithm that works by moving an element backward through the sorted portion until it finds its correct...", "l": "g", "k": ["gnome", "sort", "sorting", "algorithm", "works", "moving", "element", "backward", "sorted", "portion", "finds", "correct", "position", "forward", "conceptually"]}, {"id": "term-go-explore-algorithm", "t": "Go-Explore Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "An exploration algorithm for hard-exploration reinforcement learning problems that maintains an archive of interesting...", "l": "g", "k": ["go-explore", "algorithm", "exploration", "hard-exploration", "reinforcement", "learning", "problems", "maintains", "archive", "interesting", "states", "returns", "exploring", "solves", "environments"]}, {"id": "term-goal-misgeneralization", "t": "Goal Misgeneralization", "tg": ["Safety", "Technical"], "d": "safety", "x": "A failure mode where an AI system learns to pursue a proxy goal that correlates with the intended goal during training...", "l": "g", "k": ["goal", "misgeneralization", "failure", "mode", "system", "learns", "pursue", "proxy", "correlates", "intended", "training", "diverges", "deployment", "key", "concern"]}, {"id": "term-goal-conditioned-reinforcement-learning", "t": "Goal-Conditioned Reinforcement Learning", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A framework where the agent learns to achieve arbitrary goals specified as input. The policy is conditioned on both the...", "l": "g", "k": ["goal-conditioned", "reinforcement", "learning", "framework", "agent", "learns", "achieve", "arbitrary", "goals", "specified", "input", "policy", "conditioned", "current", "state"]}, {"id": "term-goal-conditioned-rl", "t": "Goal-Conditioned RL", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "An RL formulation where the agent's policy and value function are conditioned on a goal specifying what the agent...", "l": "g", "k": ["goal-conditioned", "formulation", "agent", "policy", "value", "function", "conditioned", "goal", "specifying", "achieve", "policies", "enable", "multi-task", "learning", "generalization"]}, {"id": "term-goedel-escher-bach", "t": "Goedel Escher Bach", "tg": ["History", "Milestones"], "d": "history", "x": "A 1979 book by Douglas Hofstadter exploring common themes in the works of mathematician Kurt Goedel artist M.C. Escher...", "l": "g", "k": ["goedel", "escher", "bach", "book", "douglas", "hofstadter", "exploring", "common", "themes", "works", "mathematician", "kurt", "artist", "composer", "johann"]}, {"id": "term-goedels-incompleteness-theorems", "t": "Goedel's Incompleteness Theorems", "tg": ["History", "Fundamentals"], "d": "history", "x": "Two theorems published by Kurt Goedel in 1931 showing that any consistent formal system capable of expressing basic...", "l": "g", "k": ["goedel", "incompleteness", "theorems", "published", "kurt", "showing", "consistent", "formal", "system", "capable", "expressing", "basic", "arithmetic", "contains", "statements"]}, {"id": "term-goertzel-algorithm", "t": "Goertzel Algorithm", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "An efficient algorithm for computing individual terms of the discrete Fourier transform. More efficient than FFT when...", "l": "g", "k": ["goertzel", "algorithm", "efficient", "computing", "individual", "terms", "discrete", "fourier", "transform", "fft", "frequency", "bins", "needed", "dtmf", "tone"]}, {"id": "term-gofai-good-old-fashioned-ai", "t": "GOFAI (Good Old-Fashioned AI)", "tg": ["History", "Fundamentals"], "d": "history", "x": "A term coined by philosopher John Haugeland in 1985 referring to classical symbolic AI approaches based on physical...", "l": "g", "k": ["gofai", "good", "old-fashioned", "term", "coined", "philosopher", "john", "haugeland", "referring", "classical", "symbolic", "approaches", "based", "physical", "symbol"]}, {"id": "term-golden-section-search", "t": "Golden Section Search", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "A technique for finding the extremum of a unimodal function by successively narrowing the interval using the golden...", "l": "g", "k": ["golden", "section", "search", "technique", "finding", "extremum", "unimodal", "function", "successively", "narrowing", "interval", "ratio", "similar", "binary", "continuous"]}, {"id": "term-golomb-coding-algorithm", "t": "Golomb Coding Algorithm", "tg": ["Algorithms", "Technical", "Information Theory"], "d": "algorithms", "x": "A family of optimal codes for geometric distributions that divides each integer by a parameter m and encodes the...", "l": "g", "k": ["golomb", "coding", "algorithm", "family", "optimal", "codes", "geometric", "distributions", "divides", "integer", "parameter", "encodes", "quotient", "remainder", "separately"]}, {"id": "term-goodharts-law-in-ai", "t": "Goodhart's Law in AI", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The principle that when a proxy measure becomes the target for optimization, it ceases to be a good measure. In AI,...", "l": "g", "k": ["goodhart", "law", "principle", "proxy", "measure", "becomes", "target", "optimization", "ceases", "good", "manifests", "models", "over-optimize", "reward", "diverging"]}, {"id": "term-google-brain", "t": "Google Brain", "tg": ["History", "Milestones"], "d": "history", "x": "A deep learning research project founded at Google in 2011 by Andrew Ng and Jeff Dean that demonstrated unsupervised...", "l": "g", "k": ["google", "brain", "deep", "learning", "research", "project", "founded", "andrew", "jeff", "dean", "demonstrated", "unsupervised", "youtube", "videos", "became"]}, {"id": "term-google-brain-founded", "t": "Google Brain Founded", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of Google Brain as a deep learning research project within Google in 2011 by Andrew Ng and Jeff Dean. The...", "l": "g", "k": ["google", "brain", "founded", "founding", "deep", "learning", "research", "project", "within", "andrew", "jeff", "dean", "team", "achieved", "breakthrough"]}, {"id": "term-google-cloud-a3-instance", "t": "Google Cloud A3 Instance", "tg": ["Cloud", "Google", "GPU"], "d": "hardware", "x": "Google Cloud virtual machine type powered by NVIDIA H100 GPUs with high-bandwidth networking for AI training. Optimized...", "l": "g", "k": ["google", "cloud", "instance", "virtual", "machine", "type", "powered", "nvidia", "h100", "gpus", "high-bandwidth", "networking", "training", "optimized", "large"]}, {"id": "term-google-cloud-tpu", "t": "Google Cloud TPU", "tg": ["Cloud", "Google", "TPU"], "d": "hardware", "x": "Cloud service providing access to Google TPU accelerators for AI training and inference. Users can provision TPU Pods...", "l": "g", "k": ["google", "cloud", "tpu", "service", "providing", "access", "accelerators", "training", "inference", "users", "provision", "pods", "large-scale", "model"]}, {"id": "term-google-coral-dev-board", "t": "Google Coral Dev Board", "tg": ["Edge", "Google", "Platform"], "d": "hardware", "x": "Single-board computer from Google featuring an Edge TPU for local AI inference. Designed for prototyping edge AI...", "l": "g", "k": ["google", "coral", "dev", "board", "single-board", "computer", "featuring", "edge", "tpu", "local", "inference", "designed", "prototyping", "products", "support"]}, {"id": "term-google-deepmind", "t": "Google DeepMind", "tg": ["History", "Milestones"], "d": "history", "x": "An AI research laboratory formed in April 2023 by merging Google Brain and DeepMind, created from the original DeepMind...", "l": "g", "k": ["google", "deepmind", "research", "laboratory", "formed", "april", "merging", "brain", "created", "original", "technologies", "founded", "demis", "hassabis", "shane"]}, {"id": "term-google-landmarks", "t": "Google Landmarks", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A large-scale dataset of landmark images for instance recognition and retrieval containing over 5 million images of...", "l": "g", "k": ["google", "landmarks", "large-scale", "dataset", "landmark", "images", "instance", "recognition", "retrieval", "containing", "million", "worldwide"]}, {"id": "term-google-sycamore", "t": "Google Sycamore", "tg": ["Quantum", "Google", "Processor"], "d": "hardware", "x": "Google 53-qubit quantum processor that demonstrated quantum supremacy in 2019 by performing a calculation in 200...", "l": "g", "k": ["google", "sycamore", "53-qubit", "quantum", "processor", "demonstrated", "supremacy", "performing", "calculation", "seconds", "take", "classical", "supercomputers", "thousands", "years"]}, {"id": "term-google-tensor-chip", "t": "Google Tensor Chip", "tg": ["Mobile", "Google", "SoC"], "d": "hardware", "x": "Custom system-on-chip designed by Google for Pixel smartphones integrating a dedicated TPU core for on-device AI tasks...", "l": "g", "k": ["google", "tensor", "chip", "custom", "system-on-chip", "designed", "pixel", "smartphones", "integrating", "dedicated", "tpu", "core", "on-device", "tasks", "speech"]}, {"id": "term-tpu-v5", "t": "Google TPU v5", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "The fifth generation of Google's Tensor Processing Unit featuring improved matrix multiply units, increased memory...", "l": "g", "k": ["google", "tpu", "fifth", "generation", "tensor", "processing", "unit", "featuring", "improved", "matrix", "multiply", "units", "increased", "memory", "bandwidth"]}, {"id": "term-google-translate-neural-mt", "t": "Google Translate Neural MT", "tg": ["History", "Milestones"], "d": "history", "x": "Google's 2016 transition from statistical to neural machine translation using sequence-to-sequence models with...", "l": "g", "k": ["google", "translate", "neural", "transition", "statistical", "machine", "translation", "sequence-to-sequence", "models", "attention", "dramatically", "improving", "quality", "bringing", "networks"]}, {"id": "term-google-willow", "t": "Google Willow", "tg": ["Quantum", "Google", "Processor"], "d": "hardware", "x": "Google quantum computing chip that achieved below-threshold quantum error correction in 2024. Demonstrated that adding...", "l": "g", "k": ["google", "willow", "quantum", "computing", "chip", "achieved", "below-threshold", "error", "correction", "demonstrated", "adding", "qubits", "reduce", "rather", "increase"]}, {"id": "term-googlenet", "t": "GoogLeNet", "tg": ["Models", "History"], "d": "models", "x": "The winner of the 2014 ImageNet competition also known as Inception v1. Introduced the inception module that processes...", "l": "g", "k": ["googlenet", "winner", "imagenet", "competition", "known", "inception", "introduced", "module", "processes", "input", "multiple", "filter", "sizes", "parallel", "achieved"]}, {"id": "term-gopher", "t": "Gopher", "tg": ["Models", "Technical"], "d": "models", "x": "A 280 billion parameter language model by DeepMind that advanced understanding of language model scaling. Provided...", "l": "g", "k": ["gopher", "billion", "parameter", "language", "model", "deepmind", "advanced", "understanding", "scaling", "provided", "comprehensive", "analysis", "behavior", "across", "diverse"]}, {"id": "term-gorilla", "t": "Gorilla", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A large language model fine-tuned for API call generation that reduces hallucination and accurately generates function...", "l": "g", "k": ["gorilla", "large", "language", "model", "fine-tuned", "api", "call", "generation", "reduces", "hallucination", "accurately", "generates", "function", "calls", "thousands"]}, {"id": "term-gorilla-apibench", "t": "Gorilla APIBench", "tg": ["Benchmark", "NLP", "Code", "Evaluation"], "d": "datasets", "x": "A benchmark of over 1600 API calls testing the ability of LLMs to generate accurate and up-to-date API function calls...", "l": "g", "k": ["gorilla", "apibench", "benchmark", "api", "calls", "testing", "ability", "llms", "generate", "accurate", "up-to-date", "function", "natural", "language", "descriptions"]}, {"id": "term-gossip-protocol-algorithm", "t": "Gossip Protocol Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A communication protocol inspired by epidemic spreading where nodes periodically exchange information with random...", "l": "g", "k": ["gossip", "protocol", "algorithm", "communication", "inspired", "epidemic", "spreading", "nodes", "periodically", "exchange", "information", "random", "peers", "eventually", "disseminates"]}, {"id": "term-got-ocr", "t": "GOT-OCR", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "General OCR Theory is a unified end-to-end OCR model that handles diverse optical character recognition scenarios...", "l": "g", "k": ["got-ocr", "general", "ocr", "theory", "unified", "end-to-end", "model", "handles", "diverse", "optical", "character", "recognition", "scenarios", "including", "scene"]}, {"id": "term-govreport", "t": "GovReport", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A dataset of long government reports paired with expert-written summaries for evaluating long document summarization....", "l": "g", "k": ["govreport", "dataset", "long", "government", "reports", "paired", "expert-written", "summaries", "evaluating", "document", "summarization", "average", "words", "testing", "models"]}, {"id": "term-gpfs", "t": "GPFS", "tg": ["Storage", "IBM", "Distributed"], "d": "hardware", "x": "IBM General Parallel File System now called Spectrum Scale providing high-performance distributed storage. Widely used...", "l": "g", "k": ["gpfs", "ibm", "general", "parallel", "file", "system", "now", "called", "spectrum", "scale", "providing", "high-performance", "distributed", "storage", "widely"]}, {"id": "term-gpqa", "t": "GPQA", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Graduate-Level Google-Proof Question Answering, an extremely difficult benchmark of expert-crafted questions across...", "l": "g", "k": ["gpqa", "graduate-level", "google-proof", "question", "answering", "extremely", "difficult", "benchmark", "expert-crafted", "questions", "across", "biology", "physics", "chemistry", "domain"]}, {"id": "term-gpqa-diamond", "t": "GPQA Diamond", "tg": ["Benchmark", "NLP", "Reasoning", "Evaluation"], "d": "datasets", "x": "The most challenging subset of GPQA containing expert-validated graduate-level questions. Tests the frontier of AI...", "l": "g", "k": ["gpqa", "diamond", "challenging", "subset", "containing", "expert-validated", "graduate-level", "questions", "tests", "frontier", "reasoning", "problems", "challenge", "domain", "experts"]}, {"id": "term-gpt", "t": "GPT (Generative Pre-trained Transformer)", "tg": ["Model", "OpenAI"], "d": "models", "x": "OpenAI's series of large language models. GPT-4 is the latest major version, known for strong reasoning, multimodal...", "l": "g", "k": ["gpt", "generative", "pre-trained", "transformer", "openai", "series", "large", "language", "models", "gpt-4", "latest", "major", "version", "known", "strong"]}, {"id": "term-gpt-scaling-laws", "t": "GPT Scaling Laws", "tg": ["History", "Milestones"], "d": "history", "x": "Empirical power-law relationships discovered by Kaplan et al. at OpenAI in 2020 showing that language model performance...", "l": "g", "k": ["gpt", "scaling", "laws", "empirical", "power-law", "relationships", "discovered", "kaplan", "openai", "showing", "language", "model", "performance", "improves", "predictably"]}, {"id": "term-gpt-1", "t": "GPT-1", "tg": ["History", "Milestones"], "d": "history", "x": "The first Generative Pre-trained Transformer model released by OpenAI in June 2018, demonstrating that unsupervised...", "l": "g", "k": ["gpt-1", "generative", "pre-trained", "transformer", "model", "released", "openai", "june", "demonstrating", "unsupervised", "pre-training", "large", "text", "corpora", "followed"]}, {"id": "term-gpt-2", "t": "GPT-2", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A 1.5-billion parameter autoregressive language model by OpenAI that demonstrated strong text generation capabilities...", "l": "g", "k": ["gpt-2", "5-billion", "parameter", "autoregressive", "language", "model", "openai", "demonstrated", "strong", "text", "generation", "capabilities", "initially", "withheld", "full"]}, {"id": "term-gpt-3", "t": "GPT-3", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A 175-billion parameter autoregressive transformer model by OpenAI that popularized few-shot and zero-shot learning...", "l": "g", "k": ["gpt-3", "175-billion", "parameter", "autoregressive", "transformer", "model", "openai", "popularized", "few-shot", "zero-shot", "learning", "in-context", "prompting", "without", "fine-tuning"]}, {"id": "term-gpt-35", "t": "GPT-3.5", "tg": ["Models", "Fundamentals"], "d": "models", "x": "An improved version of GPT-3 fine-tuned with reinforcement learning from human feedback. Powers the initial release of...", "l": "g", "k": ["gpt-3", "improved", "version", "fine-tuned", "reinforcement", "learning", "human", "feedback", "powers", "initial", "release", "chatgpt", "demonstrates", "significant", "improvements"]}, {"id": "term-gpt-4", "t": "GPT-4", "tg": ["History", "Milestones"], "d": "history", "x": "OpenAI's multimodal large language model released in March 2023, capable of processing both text and images,...", "l": "g", "k": ["gpt-4", "openai", "multimodal", "large", "language", "model", "released", "march", "capable", "processing", "text", "images", "demonstrating", "human-level", "performance"]}, {"id": "term-gpt-4-turbo", "t": "GPT-4 Turbo", "tg": ["Models", "Technical", "NLP", "Products"], "d": "models", "x": "An optimized version of GPT-4 from OpenAI with a 128K context window and reduced cost and improved instruction...", "l": "g", "k": ["gpt-4", "turbo", "optimized", "version", "openai", "128k", "context", "window", "reduced", "cost", "improved", "instruction", "following", "knowledge", "cutoff"]}, {"id": "term-gpt-4o", "t": "GPT-4o", "tg": ["Models", "Technical"], "d": "models", "x": "A multimodal model from OpenAI that natively processes text audio and images. Features faster response times and...", "l": "g", "k": ["gpt-4o", "multimodal", "model", "openai", "natively", "processes", "text", "audio", "images", "features", "faster", "response", "times", "improved", "capabilities"]}, {"id": "term-gpt-4o-mini", "t": "GPT-4o mini", "tg": ["Models", "Technical", "NLP", "Products"], "d": "models", "x": "A compact and cost-efficient variant of GPT-4o from OpenAI designed for lightweight applications while retaining...", "l": "g", "k": ["gpt-4o", "mini", "compact", "cost-efficient", "variant", "openai", "designed", "lightweight", "applications", "retaining", "multimodal", "understanding", "capabilities"]}, {"id": "term-gpt-4v", "t": "GPT-4V", "tg": ["Models", "Technical"], "d": "models", "x": "GPT-4 with Vision extends GPT-4 with the ability to understand and reason about images. Accepts interleaved text and...", "l": "g", "k": ["gpt-4v", "gpt-4", "vision", "extends", "ability", "understand", "reason", "images", "accepts", "interleaved", "text", "image", "inputs", "enabling", "visual"]}, {"id": "term-gpt-driver", "t": "GPT-Driver", "tg": ["Models", "Technical", "Autonomous", "NLP"], "d": "models", "x": "A motion planning approach for autonomous driving that repurposes a language model to reason about driving scenarios...", "l": "g", "k": ["gpt-driver", "motion", "planning", "approach", "autonomous", "driving", "repurposes", "language", "model", "reason", "scenarios", "generate", "vehicle", "trajectories"]}, {"id": "term-gpt-j", "t": "GPT-J", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A 6-billion parameter open-source autoregressive language model created by EleutherAI, notable for being one of the...", "l": "g", "k": ["gpt-j", "6-billion", "parameter", "open-source", "autoregressive", "language", "model", "created", "eleutherai", "notable", "performant", "alternatives", "gpt-3"]}, {"id": "term-gpt-neox", "t": "GPT-NeoX", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A 20-billion parameter autoregressive language model by EleutherAI that uses rotary positional embeddings and parallel...", "l": "g", "k": ["gpt-neox", "20-billion", "parameter", "autoregressive", "language", "model", "eleutherai", "uses", "rotary", "positional", "embeddings", "parallel", "attention-feedforward", "computation", "improved"]}, {"id": "term-gpt-score", "t": "GPT-Score", "tg": ["Evaluation", "LLM-Based"], "d": "models", "x": "An evaluation framework that leverages generative pre-trained models to score text quality by computing conditional...", "l": "g", "k": ["gpt-score", "evaluation", "framework", "leverages", "generative", "pre-trained", "models", "score", "text", "quality", "computing", "conditional", "generation", "probabilities", "assessing"]}, {"id": "term-gpt-sovits", "t": "GPT-SoVITS", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A few-shot voice cloning and text-to-speech system that combines GPT-style language modeling with SoVITS for...", "l": "g", "k": ["gpt-sovits", "few-shot", "voice", "cloning", "text-to-speech", "system", "combines", "gpt-style", "language", "modeling", "sovits", "high-quality", "speech", "synthesis", "brief"]}, {"id": "term-gpt4ts", "t": "GPT4TS", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A method that repurposes frozen GPT-2 backbones for time series tasks by treating time series patches as token...", "l": "g", "k": ["gpt4ts", "method", "repurposes", "frozen", "gpt-2", "backbones", "time", "series", "tasks", "treating", "patches", "token", "embeddings", "zero-shot", "forecasting"]}, {"id": "term-gptq", "t": "GPTQ", "tg": ["LLM", "Inference"], "d": "models", "x": "A post-training quantization method for large language models that uses approximate second-order information to...", "l": "g", "k": ["gptq", "post-training", "quantization", "method", "large", "language", "models", "uses", "approximate", "second-order", "information", "compress", "weights", "lower", "bit"]}, {"id": "term-gpu", "t": "GPU (Graphics Processing Unit)", "tg": ["Hardware", "Infrastructure"], "d": "hardware", "x": "Hardware originally designed for graphics that excels at the parallel computations needed for AI. NVIDIA GPUs are the...", "l": "g", "k": ["gpu", "graphics", "processing", "unit", "hardware", "originally", "designed", "excels", "parallel", "computations", "needed", "nvidia", "gpus", "dominant", "training"]}, {"id": "term-gpu-architecture", "t": "GPU Architecture for AI", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "The parallel processing design of graphics processing units optimized for AI workloads, featuring thousands of cores...", "l": "g", "k": ["gpu", "architecture", "parallel", "processing", "design", "graphics", "units", "optimized", "workloads", "featuring", "thousands", "cores", "organized", "streaming", "multiprocessors"]}, {"id": "term-gpu-clock-speed", "t": "GPU Clock Speed", "tg": ["GPU", "Performance"], "d": "hardware", "x": "The operating frequency of a GPU processor measured in megahertz or gigahertz. Higher clock speeds enable more...", "l": "g", "k": ["gpu", "clock", "speed", "operating", "frequency", "processor", "measured", "megahertz", "gigahertz", "higher", "speeds", "enable", "computations", "per", "increase"]}, {"id": "term-gpu-cluster", "t": "GPU Cluster", "tg": ["Infrastructure", "GPU", "Cluster"], "d": "hardware", "x": "Collection of interconnected GPU-equipped servers working together for distributed AI training. Clusters range from a...", "l": "g", "k": ["gpu", "cluster", "collection", "interconnected", "gpu-equipped", "servers", "working", "together", "distributed", "training", "clusters", "range", "nodes", "thousands", "frontier"]}, {"id": "term-gpu-computing-revolution", "t": "GPU Computing Revolution", "tg": ["History", "Milestones"], "d": "history", "x": "The adoption of graphics processing units for general-purpose computing and machine learning beginning around...", "l": "g", "k": ["gpu", "computing", "revolution", "adoption", "graphics", "processing", "units", "general-purpose", "machine", "learning", "beginning", "around", "2007-2012", "nvidia", "cuda"]}, {"id": "term-gpu-direct", "t": "GPU Direct", "tg": ["Distributed Computing", "GPU"], "d": "hardware", "x": "NVIDIA's technology suite enabling direct data transfers between GPUs and network adapters or storage without staging...", "l": "g", "k": ["gpu", "direct", "nvidia", "technology", "suite", "enabling", "data", "transfers", "gpus", "network", "adapters", "storage", "without", "staging", "cpu"]}, {"id": "term-gpu-memory-bandwidth", "t": "GPU Memory Bandwidth", "tg": ["GPU", "Memory", "Performance"], "d": "hardware", "x": "The rate at which data can be read from or written to GPU memory measured in gigabytes per second. A critical...", "l": "g", "k": ["gpu", "memory", "bandwidth", "rate", "data", "read", "written", "measured", "gigabytes", "per", "critical", "bottleneck", "memory-bound", "operations", "attention"]}, {"id": "term-gpu-memory-hierarchy", "t": "GPU Memory Hierarchy", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "The layered memory system in GPUs consisting of registers, shared memory (SRAM), L2 cache, and global memory...", "l": "g", "k": ["gpu", "memory", "hierarchy", "layered", "system", "gpus", "consisting", "registers", "shared", "sram", "cache", "global", "hbm", "gddr", "understanding"]}, {"id": "term-gpu-passthrough", "t": "GPU Passthrough", "tg": ["Virtualization", "GPU", "Cloud"], "d": "hardware", "x": "Virtualization technique that gives a virtual machine direct access to a physical GPU. Enables bare-metal GPU...", "l": "g", "k": ["gpu", "passthrough", "virtualization", "technique", "gives", "virtual", "machine", "direct", "access", "physical", "enables", "bare-metal", "performance", "workloads", "running"]}, {"id": "term-gpu-profiling", "t": "GPU Profiling", "tg": ["Programming", "Performance", "Development"], "d": "hardware", "x": "Process of measuring and analyzing GPU resource utilization memory access patterns and kernel execution to identify...", "l": "g", "k": ["gpu", "profiling", "process", "measuring", "analyzing", "resource", "utilization", "memory", "access", "patterns", "kernel", "execution", "identify", "performance", "bottlenecks"]}, {"id": "term-gpu-thermal-throttling", "t": "GPU Thermal Throttling", "tg": ["GPU", "Performance", "Cooling"], "d": "hardware", "x": "Automatic reduction of GPU clock speeds when temperature exceeds safe limits. Impacts AI training throughput in poorly...", "l": "g", "k": ["gpu", "thermal", "throttling", "automatic", "reduction", "clock", "speeds", "temperature", "exceeds", "safe", "limits", "impacts", "training", "throughput", "poorly"]}, {"id": "term-gpu-utilization", "t": "GPU Utilization", "tg": ["Performance", "GPU", "Metric"], "d": "hardware", "x": "Percentage of time the GPU compute engines are actively processing during a given period. Low utilization during AI...", "l": "g", "k": ["gpu", "utilization", "percentage", "time", "compute", "engines", "actively", "processing", "given", "period", "low", "training", "indicates", "bottlenecks", "data"]}, {"id": "term-gpudirect-rdma", "t": "GPUDirect RDMA", "tg": ["Networking", "NVIDIA", "Performance"], "d": "hardware", "x": "NVIDIA technology enabling direct data transfer between GPU memory and network adapters bypassing CPU and system...", "l": "g", "k": ["gpudirect", "rdma", "nvidia", "technology", "enabling", "direct", "data", "transfer", "gpu", "memory", "network", "adapters", "bypassing", "cpu", "system"]}, {"id": "term-gpudirect-storage", "t": "GPUDirect Storage", "tg": ["Storage", "NVIDIA", "Performance"], "d": "hardware", "x": "NVIDIA technology enabling direct data transfer between GPU memory and storage devices bypassing CPU. Accelerates AI...", "l": "g", "k": ["gpudirect", "storage", "nvidia", "technology", "enabling", "direct", "data", "transfer", "gpu", "memory", "devices", "bypassing", "cpu", "accelerates", "loading"]}, {"id": "term-gqa", "t": "GQA", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "A visual question answering dataset with 22 million questions grounded in scene graphs from Visual Genome. Designed for...", "l": "g", "k": ["gqa", "visual", "question", "answering", "dataset", "million", "questions", "grounded", "scene", "graphs", "genome", "designed", "real-world", "reasoning", "reduced"]}, {"id": "term-grabcut-algorithm", "t": "GrabCut Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An interactive foreground extraction algorithm that iteratively estimates foreground and background using Gaussian...", "l": "g", "k": ["grabcut", "algorithm", "interactive", "foreground", "extraction", "iteratively", "estimates", "background", "gaussian", "mixture", "models", "graph", "cuts", "requires", "minimal"]}, {"id": "term-grace-cpu", "t": "Grace CPU", "tg": ["Processor", "NVIDIA", "ARM"], "d": "hardware", "x": "NVIDIA ARM-based data center CPU designed for AI and HPC workloads. Features LPDDR5X memory with high bandwidth and low...", "l": "g", "k": ["grace", "cpu", "nvidia", "arm-based", "data", "center", "designed", "hpc", "workloads", "features", "lpddr5x", "memory", "high", "bandwidth", "low"]}, {"id": "term-grace-hopper", "t": "Grace Hopper", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist and United States Navy rear admiral who developed the first compiler (A-0 System) in 1952....", "l": "g", "k": ["grace", "hopper", "american", "computer", "scientist", "united", "states", "navy", "rear", "admiral", "developed", "compiler", "a-0", "system", "pioneered"]}, {"id": "term-grad-cam", "t": "Grad-CAM", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Gradient-weighted Class Activation Mapping, a visualization method that uses gradients flowing into the final...", "l": "g", "k": ["grad-cam", "gradient-weighted", "class", "activation", "mapping", "visualization", "method", "uses", "gradients", "flowing", "final", "convolutional", "layer", "produce", "heatmap"]}, {"id": "term-gradient", "t": "Gradient", "tg": ["Training", "Math"], "d": "general", "x": "A vector indicating the direction and magnitude of change needed to reduce a model's error. The foundation of gradient...", "l": "g", "k": ["gradient", "vector", "indicating", "direction", "magnitude", "change", "needed", "reduce", "model", "error", "foundation", "descent", "optimization", "training", "neural"]}, {"id": "term-gradient-accumulation", "t": "Gradient Accumulation", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A technique that simulates larger batch sizes by accumulating gradients over multiple forward-backward passes before...", "l": "g", "k": ["gradient", "accumulation", "technique", "simulates", "larger", "batch", "sizes", "accumulating", "gradients", "multiple", "forward-backward", "passes", "performing", "parameter", "update"]}, {"id": "term-gradient-boosted-trees-model", "t": "Gradient Boosted Trees Model", "tg": ["Models", "Fundamentals"], "d": "models", "x": "An ensemble method that sequentially adds decision trees where each new tree corrects the residual errors of the...", "l": "g", "k": ["gradient", "boosted", "trees", "model", "ensemble", "method", "sequentially", "adds", "decision", "tree", "corrects", "residual", "errors", "previous"]}, {"id": "term-gradient-boosting", "t": "Gradient Boosting", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An ensemble technique that builds models sequentially, with each new model trained to correct the residual errors of...", "l": "g", "k": ["gradient", "boosting", "ensemble", "technique", "builds", "models", "sequentially", "model", "trained", "correct", "residual", "errors", "combined", "far", "descent"]}, {"id": "term-gradient-boosting-history", "t": "Gradient Boosting History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of gradient boosting from the work of Jerome Friedman (2001) through XGBoost (Chen and Guestrin 2016)...", "l": "g", "k": ["gradient", "boosting", "history", "development", "work", "jerome", "friedman", "xgboost", "chen", "guestrin", "lightgbm", "catboost", "boosted", "trees", "became"]}, {"id": "term-gradient-checkpointing", "t": "Gradient Checkpointing", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A memory optimization technique that trades computation for memory by only storing activations at selected checkpoints...", "l": "g", "k": ["gradient", "checkpointing", "memory", "optimization", "technique", "trades", "computation", "storing", "activations", "selected", "checkpoints", "forward", "pass", "recomputing", "intermediate"]}, {"id": "term-gradient-clipping", "t": "Gradient Clipping", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A technique that rescales or truncates gradients when their norm exceeds a specified threshold, preventing the...", "l": "g", "k": ["gradient", "clipping", "technique", "rescales", "truncates", "gradients", "norm", "exceeds", "specified", "threshold", "preventing", "exploding", "problem", "destabilize", "training"]}, {"id": "term-gradient-descent", "t": "Gradient Descent", "tg": ["Algorithm", "Training"], "d": "algorithms", "x": "The optimization algorithm that trains neural networks by iteratively adjusting weights in the direction that reduces...", "l": "g", "k": ["gradient", "descent", "optimization", "algorithm", "trains", "neural", "networks", "iteratively", "adjusting", "weights", "direction", "reduces", "error", "variants", "include"]}, {"id": "term-gradient-leakage", "t": "Gradient Leakage", "tg": ["Safety", "Technical"], "d": "safety", "x": "An attack that reconstructs private training data from model gradients shared during federated learning or...", "l": "g", "k": ["gradient", "leakage", "attack", "reconstructs", "private", "training", "data", "model", "gradients", "shared", "federated", "learning", "collaborative", "demonstrates", "sharing"]}, {"id": "term-gradient-projection-method", "t": "Gradient Projection Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An optimization algorithm for constrained problems that projects the gradient onto the feasible set at each iteration....", "l": "g", "k": ["gradient", "projection", "method", "optimization", "algorithm", "constrained", "problems", "projects", "onto", "feasible", "iteration", "effective", "simple", "constraint", "sets"]}, {"id": "term-gradient-surgery", "t": "Gradient Surgery", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A technique for multi-task learning that modifies conflicting gradients from different tasks to reduce interference....", "l": "g", "k": ["gradient", "surgery", "technique", "multi-task", "learning", "modifies", "conflicting", "gradients", "different", "tasks", "reduce", "interference", "projects", "oppose", "enabling"]}, {"id": "term-gradient-synchronization", "t": "Gradient Synchronization", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "The process of aggregating gradients across multiple GPUs or nodes in distributed training, typically via all-reduce....", "l": "g", "k": ["gradient", "synchronization", "process", "aggregating", "gradients", "across", "multiple", "gpus", "nodes", "distributed", "training", "typically", "via", "all-reduce", "synchronous"]}, {"id": "term-gram-schmidt-orthogonalization", "t": "Gram-Schmidt Orthogonalization", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "A procedure for orthonormalizing a set of vectors in an inner product space. Produces an orthonormal basis from a...", "l": "g", "k": ["gram-schmidt", "orthogonalization", "procedure", "orthonormalizing", "vectors", "inner", "product", "space", "produces", "orthonormal", "basis", "linearly", "independent", "suffer", "numerical"]}, {"id": "term-grammar-constrained-decoding", "t": "Grammar-Constrained Decoding", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A decoding approach that restricts token generation to sequences valid under a formal grammar (such as BNF or regex),...", "l": "g", "k": ["grammar-constrained", "decoding", "approach", "restricts", "token", "generation", "sequences", "valid", "formal", "grammar", "bnf", "regex", "ensuring", "outputs", "always"]}, {"id": "term-granger-causality", "t": "Granger Causality", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A statistical concept where a time series X is said to Granger-cause Y if past values of X provide statistically...", "l": "g", "k": ["granger", "causality", "statistical", "concept", "time", "series", "said", "granger-cause", "past", "values", "provide", "statistically", "significant", "information", "future"]}, {"id": "term-granger-causality-test", "t": "Granger Causality Test", "tg": ["Algorithms", "Fundamentals", "Causal"], "d": "algorithms", "x": "A statistical test that determines whether one time series is useful for forecasting another. Tests if past values of...", "l": "g", "k": ["granger", "causality", "test", "statistical", "determines", "time", "series", "useful", "forecasting", "another", "tests", "past", "values", "variable", "improve"]}, {"id": "term-granite", "t": "Granite", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A family of enterprise-grade language models from IBM trained on curated business and code data with strong governance...", "l": "g", "k": ["granite", "family", "enterprise-grade", "language", "models", "ibm", "trained", "curated", "business", "code", "data", "strong", "governance", "compliance", "features"]}, {"id": "term-granite-code", "t": "Granite Code", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A family of code-generation models from IBM Granite series trained on permissively licensed code data for enterprise...", "l": "g", "k": ["granite", "code", "family", "code-generation", "models", "ibm", "series", "trained", "permissively", "licensed", "data", "enterprise", "software", "development", "tasks"]}, {"id": "term-granite-code-instruct", "t": "Granite Code Instruct", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "Instruction-tuned variants of IBM Granite Code models for following complex coding instructions and generating...", "l": "g", "k": ["granite", "code", "instruct", "instruction-tuned", "variants", "ibm", "models", "following", "complex", "coding", "instructions", "generating", "well-documented", "solutions"]}, {"id": "term-graph-attention", "t": "Graph Attention", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An attention mechanism applied to graph-structured data that learns to weight the importance of different neighbor...", "l": "g", "k": ["graph", "attention", "mechanism", "applied", "graph-structured", "data", "learns", "weight", "importance", "different", "neighbor", "nodes", "introduced", "networks", "allowing"]}, {"id": "term-gat", "t": "Graph Attention Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A graph neural network that uses attention mechanisms to weight the importance of neighboring nodes' features during...", "l": "g", "k": ["graph", "attention", "network", "neural", "uses", "mechanisms", "weight", "importance", "neighboring", "nodes", "features", "aggregation", "learning", "focus", "relevant"]}, {"id": "term-graph-coloring-algorithm", "t": "Graph Coloring Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm that assigns colors to vertices of a graph such that no two adjacent vertices share the same color. The...", "l": "g", "k": ["graph", "coloring", "algorithm", "assigns", "colors", "vertices", "adjacent", "share", "color", "goal", "typically", "minimize", "number", "known", "chromatic"]}, {"id": "term-graph-convolution", "t": "Graph Convolution", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An operation that extends convolution to graph-structured data by aggregating features from neighboring nodes. Spectral...", "l": "g", "k": ["graph", "convolution", "operation", "extends", "graph-structured", "data", "aggregating", "features", "neighboring", "nodes", "spectral", "approaches", "fourier", "transforms", "spatial"]}, {"id": "term-gcn", "t": "Graph Convolutional Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A neural network that operates on graph-structured data by aggregating features from neighboring nodes through...", "l": "g", "k": ["graph", "convolutional", "network", "neural", "operates", "graph-structured", "data", "aggregating", "features", "neighboring", "nodes", "learnable", "operations", "defined", "topology"]}, {"id": "term-graph-isomorphism-algorithm", "t": "Graph Isomorphism Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm that determines whether two graphs are structurally identical by finding a bijection between their vertex...", "l": "g", "k": ["graph", "isomorphism", "algorithm", "determines", "graphs", "structurally", "identical", "finding", "bijection", "vertex", "sets", "preserves", "adjacency", "babai", "quasipolynomial-time"]}, {"id": "term-gin", "t": "Graph Isomorphism Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A graph neural network provably as powerful as the Weisfeiler-Lehman graph isomorphism test, using a sum aggregator and...", "l": "g", "k": ["graph", "isomorphism", "network", "neural", "provably", "powerful", "weisfeiler-lehman", "test", "sum", "aggregator", "mlp", "update", "function", "maximize", "discriminative"]}, {"id": "term-graph-laplacian-algorithm", "t": "Graph Laplacian Algorithm", "tg": ["Algorithms", "Technical", "Graph", "Numerical"], "d": "algorithms", "x": "A method that constructs the Laplacian matrix of a graph from its adjacency and degree matrices. The eigenvalues and...", "l": "g", "k": ["graph", "laplacian", "algorithm", "method", "constructs", "matrix", "adjacency", "degree", "matrices", "eigenvalues", "eigenvectors", "reveal", "structural", "properties", "connectivity"]}, {"id": "term-graph-neural-network", "t": "Graph Neural Network", "tg": ["Models", "Fundamentals"], "d": "models", "x": "A class of neural networks designed to operate on graph-structured data. Learns node edge or graph-level...", "l": "g", "k": ["graph", "neural", "network", "class", "networks", "designed", "operate", "graph-structured", "data", "learns", "node", "edge", "graph-level", "representations", "aggregating"]}, {"id": "term-graph-neural-network-history", "t": "Graph Neural Network History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of neural networks for graph-structured data from early spectral approaches (Bruna et al. 2013) through...", "l": "g", "k": ["graph", "neural", "network", "history", "development", "networks", "graph-structured", "data", "early", "spectral", "approaches", "bruna", "convolutional", "kipf", "welling"]}, {"id": "term-graph-rag", "t": "Graph RAG", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A retrieval-augmented generation approach that builds a knowledge graph from source documents and uses graph traversal...", "l": "g", "k": ["graph", "rag", "retrieval-augmented", "generation", "approach", "builds", "knowledge", "source", "documents", "uses", "traversal", "retrieve", "structured", "interconnected", "context"]}, {"id": "term-graph-based-index", "t": "Graph-Based Index", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "A vector index structure that organizes vectors as nodes in a proximity graph where edges connect similar vectors,...", "l": "g", "k": ["graph-based", "index", "vector", "structure", "organizes", "vectors", "nodes", "proximity", "graph", "edges", "connect", "similar", "enabling", "efficient", "nearest"]}, {"id": "term-graph-based-parsing", "t": "Graph-Based Parsing", "tg": ["NLP", "Parsing"], "d": "general", "x": "A parsing approach that scores all possible dependency edges simultaneously and finds the highest-scoring tree using...", "l": "g", "k": ["graph-based", "parsing", "approach", "scores", "possible", "dependency", "edges", "simultaneously", "finds", "highest-scoring", "tree", "algorithms", "maximum", "spanning", "typically"]}, {"id": "term-graphcast", "t": "GraphCast", "tg": ["Models", "Scientific", "Fundamentals"], "d": "models", "x": "A graph neural network-based weather forecasting model from DeepMind that predicts 10-day global weather faster and...", "l": "g", "k": ["graphcast", "graph", "neural", "network-based", "weather", "forecasting", "model", "deepmind", "predicts", "10-day", "global", "faster", "accurately", "traditional", "numerical"]}, {"id": "term-graphcore", "t": "Graphcore", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "A semiconductor company that developed the Intelligence Processing Unit (IPU), featuring a massive number of...", "l": "g", "k": ["graphcore", "semiconductor", "company", "developed", "intelligence", "processing", "unit", "ipu", "featuring", "massive", "number", "independent", "processor", "cores", "large"]}, {"id": "term-graphcore-ipu", "t": "Graphcore IPU", "tg": ["Accelerator", "Architecture"], "d": "hardware", "x": "Intelligence Processing Unit designed by Graphcore using a unique bulk synchronous parallel architecture optimized for...", "l": "g", "k": ["graphcore", "ipu", "intelligence", "processing", "unit", "designed", "unique", "bulk", "synchronous", "parallel", "architecture", "optimized", "machine", "learning", "workloads"]}, {"id": "term-graphsage", "t": "GraphSAGE", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A framework for inductive representation learning on graphs that samples and aggregates features from a node's local...", "l": "g", "k": ["graphsage", "framework", "inductive", "representation", "learning", "graphs", "samples", "aggregates", "features", "node", "local", "neighborhood", "enabling", "generalization", "unseen"]}, {"id": "term-grasp-algorithm", "t": "GRASP Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "Greedy Randomized Adaptive Search Procedure is a multi-start metaheuristic that alternates between construction and...", "l": "g", "k": ["grasp", "algorithm", "greedy", "randomized", "adaptive", "search", "procedure", "multi-start", "metaheuristic", "alternates", "construction", "local", "phases", "phase", "builds"]}, {"id": "term-gravitational-search-algorithm", "t": "Gravitational Search Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A physics-inspired optimization algorithm where agents are treated as objects with masses proportional to their...", "l": "g", "k": ["gravitational", "search", "algorithm", "physics-inspired", "optimization", "agents", "treated", "objects", "masses", "proportional", "fitness", "attract", "forces", "causing", "better"]}, {"id": "term-greedy-decoding", "t": "Greedy Decoding", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A deterministic text generation strategy that always selects the token with the highest probability at each step,...", "l": "g", "k": ["greedy", "decoding", "deterministic", "text", "generation", "strategy", "always", "selects", "token", "highest", "probability", "step", "producing", "likely", "sequence"]}, {"id": "term-green-ai", "t": "Green AI", "tg": ["Sustainability", "Research"], "d": "hardware", "x": "Movement advocating for energy-efficient AI research that minimizes computational and environmental costs. Promotes...", "l": "g", "k": ["green", "movement", "advocating", "energy-efficient", "research", "minimizes", "computational", "environmental", "costs", "promotes", "reporting", "compute", "budgets", "developing", "efficient"]}, {"id": "term-green500", "t": "Green500", "tg": ["Benchmark", "Supercomputer", "Efficiency"], "d": "hardware", "x": "Ranking of the most energy-efficient supercomputers measuring performance per watt. Highlights the growing importance...", "l": "g", "k": ["green500", "ranking", "energy-efficient", "supercomputers", "measuring", "performance", "per", "watt", "highlights", "growing", "importance", "power", "efficiency", "large-scale", "computing"]}, {"id": "term-grey-wolf-optimizer", "t": "Grey Wolf Optimizer", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A swarm intelligence algorithm inspired by the social hierarchy and hunting behavior of grey wolves. Models alpha and...", "l": "g", "k": ["grey", "wolf", "optimizer", "swarm", "intelligence", "algorithm", "inspired", "social", "hierarchy", "hunting", "behavior", "wolves", "models", "alpha", "beta"]}, {"id": "term-grid-search", "t": "Grid Search", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A hyperparameter tuning method that exhaustively evaluates all combinations of specified parameter values, typically...", "l": "g", "k": ["grid", "search", "hyperparameter", "tuning", "method", "exhaustively", "evaluates", "combinations", "specified", "parameter", "values", "typically", "combined", "cross-validation", "select"]}, {"id": "term-grievance-mechanism-for-ai", "t": "Grievance Mechanism for AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "A formal process through which individuals can raise complaints about harm caused by AI systems and seek remediation....", "l": "g", "k": ["grievance", "mechanism", "formal", "process", "individuals", "raise", "complaints", "harm", "caused", "systems", "seek", "remediation", "required", "various", "governance"]}, {"id": "term-griffin", "t": "Griffin", "tg": ["Models", "Technical"], "d": "models", "x": "A hybrid model by Google DeepMind that combines recurrent layers based on linear recurrences with local attention....", "l": "g", "k": ["griffin", "hybrid", "model", "google", "deepmind", "combines", "recurrent", "layers", "based", "linear", "recurrences", "local", "attention", "achieves", "strong"]}, {"id": "term-grok", "t": "Grok", "tg": ["Product", "Model"], "d": "models", "x": "An AI assistant developed by xAI (Elon Musk's AI company). Integrated with X (Twitter) and known for real-time...", "l": "g", "k": ["grok", "assistant", "developed", "xai", "elon", "musk", "company", "integrated", "twitter", "known", "real-time", "information", "access", "less", "restrictive"]}, {"id": "term-grok-1", "t": "Grok-1", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A 314 billion parameter mixture-of-experts language model from xAI with 25 percent of weights active per token during...", "l": "g", "k": ["grok-1", "billion", "parameter", "mixture-of-experts", "language", "model", "xai", "percent", "weights", "active", "per", "token", "inference"]}, {"id": "term-grok-15", "t": "Grok-1.5", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An improved version of Grok with enhanced reasoning capabilities and a 128K context window for processing long...", "l": "g", "k": ["grok-1", "improved", "version", "grok", "enhanced", "reasoning", "capabilities", "128k", "context", "window", "processing", "long", "documents", "complex", "problems"]}, {"id": "term-groot", "t": "GROOT", "tg": ["Models", "Technical", "Robotics", "Vision"], "d": "models", "x": "A humanoid robot learning framework from NVIDIA that trains generalist robot policies using video demonstrations and...", "l": "g", "k": ["groot", "humanoid", "robot", "learning", "framework", "nvidia", "trains", "generalist", "policies", "video", "demonstrations", "language", "instructions"]}, {"id": "term-groq", "t": "Groq", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "An AI hardware company that developed the Language Processing Unit (LPU), a deterministic architecture using...", "l": "g", "k": ["groq", "hardware", "company", "developed", "language", "processing", "unit", "lpu", "deterministic", "architecture", "software-defined", "scheduling", "achieve", "extremely", "low-latency"]}, {"id": "term-ground-truth", "t": "Ground Truth", "tg": ["Data", "Evaluation"], "d": "datasets", "x": "The correct answer or label used to evaluate model predictions. Obtained through human annotation, measurement, or...", "l": "g", "k": ["ground", "truth", "correct", "answer", "label", "evaluate", "model", "predictions", "obtained", "human", "annotation", "measurement", "authoritative", "sources"]}, {"id": "term-grounded-sam", "t": "Grounded SAM", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A combination of Grounding DINO and Segment Anything Model that enables text-prompted segmentation of arbitrary objects...", "l": "g", "k": ["grounded", "sam", "combination", "grounding", "dino", "segment", "anything", "model", "enables", "text-prompted", "segmentation", "arbitrary", "objects", "images"]}, {"id": "term-grounding", "t": "Grounding", "tg": ["Technique", "Accuracy"], "d": "general", "x": "Connecting AI outputs to verified information sources to reduce hallucinations and increase accuracy. Often involves...", "l": "g", "k": ["grounding", "connecting", "outputs", "verified", "information", "sources", "reduce", "hallucinations", "increase", "accuracy", "involves", "retrieval-augmented", "generation", "rag", "real-time"]}, {"id": "term-grounding-dino", "t": "Grounding DINO", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "An open-set object detection model that combines a DINO-based detector with grounded pre-training, enabling detection...", "l": "g", "k": ["grounding", "dino", "open-set", "object", "detection", "model", "combines", "dino-based", "detector", "grounded", "pre-training", "enabling", "arbitrary", "objects", "specified"]}, {"id": "term-grounding-sam-2", "t": "Grounding SAM 2", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A combination of Grounding DINO with SAM 2 that enables text-prompted object segmentation and tracking across video...", "l": "g", "k": ["grounding", "sam", "combination", "dino", "enables", "text-prompted", "object", "segmentation", "tracking", "across", "video", "sequences"]}, {"id": "term-group-fairness", "t": "Group Fairness", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Fairness criteria that require statistical parity of outcomes or error rates across demographic groups defined by...", "l": "g", "k": ["group", "fairness", "criteria", "require", "statistical", "parity", "outcomes", "error", "rates", "across", "demographic", "groups", "defined", "protected", "attributes"]}, {"id": "term-group-normalization", "t": "Group Normalization", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A normalization method that divides channels into groups and normalizes within each group independently, providing...", "l": "g", "k": ["group", "normalization", "method", "divides", "channels", "groups", "normalizes", "within", "independently", "providing", "stable", "performance", "regardless", "batch", "size"]}, {"id": "term-group-relative-policy-optimization", "t": "Group Relative Policy Optimization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A reinforcement learning algorithm for language model alignment that uses group-based normalization of advantages...", "l": "g", "k": ["group", "relative", "policy", "optimization", "reinforcement", "learning", "algorithm", "language", "model", "alignment", "uses", "group-based", "normalization", "advantages", "rather"]}, {"id": "term-grouped-query-attention", "t": "Grouped Query Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that groups multiple query heads to share a single key-value head, interpolating between...", "l": "g", "k": ["grouped", "query", "attention", "mechanism", "groups", "multiple", "heads", "share", "single", "key-value", "head", "interpolating", "multi-head", "multi-query", "balance"]}, {"id": "term-gru", "t": "GRU", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Gated Recurrent Unit, a recurrent neural network variant that uses reset and update gates to control information flow,...", "l": "g", "k": ["gru", "gated", "recurrent", "unit", "neural", "network", "variant", "uses", "reset", "update", "gates", "control", "information", "flow", "offering"]}, {"id": "term-gshard", "t": "GShard", "tg": ["Models", "Technical"], "d": "models", "x": "A framework for scaling giant models across thousands of devices using conditional computation. Uses top-2 gating in...", "l": "g", "k": ["gshard", "framework", "scaling", "giant", "models", "across", "thousands", "devices", "conditional", "computation", "uses", "top-2", "gating", "mixture-of-experts", "layers"]}, {"id": "term-gsm8k", "t": "GSM8K", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Grade School Math 8K, a benchmark of 8,500 linguistically diverse grade-school-level math word problems requiring...", "l": "g", "k": ["gsm8k", "grade", "school", "math", "benchmark", "linguistically", "diverse", "grade-school-level", "word", "problems", "requiring", "multi-step", "arithmetic", "reasoning", "widely"]}, {"id": "term-gte", "t": "GTE", "tg": ["Models", "Technical", "Embedding", "NLP"], "d": "models", "x": "General Text Embeddings is a family of text embedding models from Alibaba that achieve strong performance on semantic...", "l": "g", "k": ["gte", "general", "text", "embeddings", "family", "embedding", "models", "alibaba", "achieve", "strong", "performance", "semantic", "textual", "similarity", "retrieval"]}, {"id": "term-gtr", "t": "GTR", "tg": ["Models", "Technical", "Embedding", "NLP"], "d": "models", "x": "Generalizable T5-based dense Retrievers are text embedding models fine-tuned from T5 encoders for large-scale...", "l": "g", "k": ["gtr", "generalizable", "t5-based", "dense", "retrievers", "text", "embedding", "models", "fine-tuned", "encoders", "large-scale", "information", "retrieval", "tasks"]}, {"id": "term-guardrails", "t": "Guardrails", "tg": ["Safety", "Constraint"], "d": "safety", "x": "Safety mechanisms that constrain AI behavior to prevent harmful outputs. Include content filters, output validators,...", "l": "g", "k": ["guardrails", "safety", "mechanisms", "constrain", "behavior", "prevent", "harmful", "outputs", "include", "content", "filters", "output", "validators", "behavioral", "restrictions"]}, {"id": "term-guided-filter-algorithm", "t": "Guided Filter Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An edge-preserving filter that uses a guidance image to compute filter output through local linear models. Faster than...", "l": "g", "k": ["guided", "filter", "algorithm", "edge-preserving", "uses", "guidance", "image", "compute", "output", "local", "linear", "models", "faster", "bilateral", "filtering"]}, {"id": "term-guided-generation", "t": "Guided Generation", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "Techniques that constrain language model output to conform to a specified format (such as JSON schema or grammar rules)...", "l": "g", "k": ["guided", "generation", "techniques", "constrain", "language", "model", "output", "conform", "specified", "format", "json", "schema", "grammar", "rules", "masking"]}, {"id": "term-guided-local-search", "t": "Guided Local Search", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A metaheuristic penalty-based method that escapes local optima by augmenting the objective function with penalty terms...", "l": "g", "k": ["guided", "local", "search", "metaheuristic", "penalty-based", "method", "escapes", "optima", "augmenting", "objective", "function", "penalty", "terms", "frequently", "occurring"]}, {"id": "term-gumbel-max-trick", "t": "Gumbel-Max Trick", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A method for sampling from categorical distributions using Gumbel noise. Adds independent Gumbel noise to...", "l": "g", "k": ["gumbel-max", "trick", "method", "sampling", "categorical", "distributions", "gumbel", "noise", "adds", "independent", "log-probabilities", "takes", "argmax", "enables", "efficient"]}, {"id": "term-gumbel-softmax", "t": "Gumbel-Softmax", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A continuous relaxation of categorical sampling that allows gradient-based optimization of discrete variables. Uses the...", "l": "g", "k": ["gumbel-softmax", "continuous", "relaxation", "categorical", "sampling", "allows", "gradient-based", "optimization", "discrete", "variables", "uses", "gumbel-max", "trick", "temperature-controlled", "softmax"]}, {"id": "term-gustafsons-law", "t": "Gustafson's Law", "tg": ["Architecture", "Principle", "Fundamentals"], "d": "hardware", "x": "Principle stating that parallel speedup scales with problem size because larger problems have proportionally more...", "l": "g", "k": ["gustafson", "law", "principle", "stating", "parallel", "speedup", "scales", "problem", "size", "larger", "problems", "proportionally", "parallelizable", "work", "offers"]}, {"id": "term-gym-environment", "t": "Gym Environment", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "An interface standard and collection of benchmark environments originally developed by OpenAI for RL research. The Gym...", "l": "g", "k": ["gym", "environment", "interface", "standard", "collection", "benchmark", "environments", "originally", "developed", "openai", "research", "api", "defines", "common", "protocol"]}, {"id": "term-gymnasium", "t": "Gymnasium", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "The maintained successor to OpenAI Gym providing standardized RL environment interfaces. Managed by the Farama...", "l": "g", "k": ["gymnasium", "maintained", "successor", "openai", "gym", "providing", "standardized", "environment", "interfaces", "managed", "farama", "foundation", "improved", "documentation", "compatibility"]}, {"id": "term-h2o-danube", "t": "H2O Danube", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A family of compact language models from H2O.ai trained on high-quality data for efficient enterprise deployment in...", "l": "h", "k": ["h2o", "danube", "family", "compact", "language", "models", "trained", "high-quality", "data", "efficient", "enterprise", "deployment", "parameter", "sizes"]}, {"id": "term-habana", "t": "Habana Labs", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "An Intel subsidiary producing the Gaudi series of AI training accelerators featuring integrated RoCE networking and...", "l": "h", "k": ["habana", "labs", "intel", "subsidiary", "producing", "gaudi", "series", "training", "accelerators", "featuring", "integrated", "roce", "networking", "high", "memory"]}, {"id": "term-habitat", "t": "Habitat", "tg": ["Benchmark", "Reinforcement Learning", "Robotics"], "d": "datasets", "x": "A simulation platform for training embodied AI agents in photorealistic 3D environments. Enables research in visual...", "l": "h", "k": ["habitat", "simulation", "platform", "training", "embodied", "agents", "photorealistic", "environments", "enables", "research", "visual", "navigation", "interaction", "scanned", "real-world"]}, {"id": "term-hailo-8", "t": "Hailo-8", "tg": ["Edge", "Accelerator", "Inference"], "d": "hardware", "x": "AI inference processor designed by Israeli company Hailo delivering up to 26 TOPS at only 2.5 watts. Designed for edge...", "l": "h", "k": ["hailo-8", "inference", "processor", "designed", "israeli", "company", "hailo", "delivering", "tops", "watts", "edge", "automotive", "surveillance", "smart", "city"]}, {"id": "term-hallucination", "t": "Hallucination", "tg": ["Limitation", "Risk"], "d": "safety", "x": "When AI generates information that sounds plausible but is actually incorrect or fabricated. Includes fake citations,...", "l": "h", "k": ["hallucination", "generates", "information", "sounds", "plausible", "actually", "incorrect", "fabricated", "includes", "fake", "citations", "invented", "statistics", "fictional", "events"]}, {"id": "term-hallucination-mitigation", "t": "Hallucination Mitigation", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A collection of techniques designed to reduce factually incorrect or fabricated content in LLM outputs, including...", "l": "h", "k": ["hallucination", "mitigation", "collection", "techniques", "designed", "reduce", "factually", "incorrect", "fabricated", "content", "llm", "outputs", "including", "retrieval", "augmentation"]}, {"id": "term-hallucination-rate", "t": "Hallucination Rate", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that quantifies the proportion of generated content that contains fabricated facts, unsupported claims, or...", "l": "h", "k": ["hallucination", "rate", "metric", "quantifies", "proportion", "generated", "content", "contains", "fabricated", "facts", "unsupported", "claims", "information", "contradicting", "source"]}, {"id": "term-halting-problem", "t": "Halting Problem", "tg": ["History", "Fundamentals"], "d": "history", "x": "A decision problem proved undecidable by Alan Turing in 1936. The halting problem asks whether a given program will...", "l": "h", "k": ["halting", "problem", "decision", "proved", "undecidable", "alan", "turing", "asks", "given", "program", "eventually", "halt", "run", "forever", "proof"]}, {"id": "term-halueval", "t": "HaluEval", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating hallucination in large language models across QA dialogue and summarization tasks. Tests the...", "l": "h", "k": ["halueval", "benchmark", "evaluating", "hallucination", "large", "language", "models", "across", "dialogue", "summarization", "tasks", "tests", "ability", "generate", "factually"]}, {"id": "term-hamiltonian-monte-carlo", "t": "Hamiltonian Monte Carlo", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "An MCMC method that uses Hamiltonian dynamics to propose distant samples with high acceptance probability, reducing...", "l": "h", "k": ["hamiltonian", "monte", "carlo", "mcmc", "method", "uses", "dynamics", "propose", "distant", "samples", "high", "acceptance", "probability", "reducing", "random"]}, {"id": "term-hamiltonian-path-problem", "t": "Hamiltonian Path Problem", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "The problem of determining whether a path exists that visits every vertex in a graph exactly once. This is an...", "l": "h", "k": ["hamiltonian", "path", "problem", "determining", "exists", "visits", "vertex", "graph", "exactly", "np-complete", "known", "polynomial-time", "algorithm", "general", "case"]}, {"id": "term-hamming-distance", "t": "Hamming Distance", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "The number of positions at which corresponding symbols in two equal-length strings or vectors differ. It is used as a...", "l": "h", "k": ["hamming", "distance", "number", "positions", "corresponding", "symbols", "equal-length", "strings", "vectors", "differ", "metric", "comparing", "binary", "codes", "error"]}, {"id": "term-hand-pose-estimation", "t": "Hand Pose Estimation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of predicting the 3D positions of hand joints and fingertips from images or depth sensors, enabling gesture...", "l": "h", "k": ["hand", "pose", "estimation", "task", "predicting", "positions", "joints", "fingertips", "images", "depth", "sensors", "enabling", "gesture", "recognition", "tracking"]}, {"id": "term-hans-moravec", "t": "Hans Moravec", "tg": ["History", "Pioneers"], "d": "history", "x": "Austrian-Canadian roboticist at Carnegie Mellon University known for Moravec's Paradox and pioneering work in mobile...", "l": "h", "k": ["hans", "moravec", "austrian-canadian", "roboticist", "carnegie", "mellon", "university", "known", "paradox", "pioneering", "work", "mobile", "robot", "navigation", "books"]}, {"id": "term-hard-attention", "t": "Hard Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that selects discrete positions to attend to rather than computing weighted averages, requiring...", "l": "h", "k": ["hard", "attention", "mechanism", "selects", "discrete", "positions", "attend", "rather", "computing", "weighted", "averages", "requiring", "reinforcement", "learning", "straight-through"]}, {"id": "term-hard-disk-drive", "t": "Hard Disk Drive", "tg": ["Storage", "Hardware", "Historical"], "d": "hardware", "x": "Electromechanical data storage device using rotating magnetic platters. While being replaced by SSDs for primary...", "l": "h", "k": ["hard", "disk", "drive", "electromechanical", "data", "storage", "device", "rotating", "magnetic", "platters", "replaced", "ssds", "primary", "hdds", "remain"]}, {"id": "term-hard-sigmoid", "t": "Hard Sigmoid", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A piecewise linear approximation of the sigmoid function that is computationally cheaper to evaluate. Defined as f(x) =...", "l": "h", "k": ["hard", "sigmoid", "piecewise", "linear", "approximation", "function", "computationally", "cheaper", "evaluate", "defined", "clip", "mobile", "embedded", "applications", "computational"]}, {"id": "term-hard-swish", "t": "Hard Swish", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A piecewise linear approximation of the Swish activation function designed for efficient inference on mobile devices....", "l": "h", "k": ["hard", "swish", "piecewise", "linear", "approximation", "activation", "function", "designed", "efficient", "inference", "mobile", "devices", "mobilenetv3", "defined", "hard_sigmoid"]}, {"id": "term-hardware-failure-recovery", "t": "Hardware Failure Recovery", "tg": ["Reliability", "Infrastructure", "Training"], "d": "hardware", "x": "Mechanisms for detecting and recovering from hardware failures during AI training including GPU failures network issues...", "l": "h", "k": ["hardware", "failure", "recovery", "mechanisms", "detecting", "recovering", "failures", "training", "including", "gpu", "network", "issues", "storage", "problems", "critical"]}, {"id": "term-hardware-aware-neural-architecture-search", "t": "Hardware-Aware Neural Architecture Search", "tg": ["Training", "NAS", "Co-Design"], "d": "hardware", "x": "NAS approach that incorporates hardware constraints like latency memory and power into the search objective. Produces...", "l": "h", "k": ["hardware-aware", "neural", "architecture", "search", "nas", "approach", "incorporates", "hardware", "constraints", "latency", "memory", "power", "objective", "produces", "models"]}, {"id": "term-hardware-software-co-design", "t": "Hardware-Software Co-Design", "tg": ["Design", "Methodology", "Optimization"], "d": "hardware", "x": "Design methodology where hardware and software are developed together to optimize overall system performance....", "l": "h", "k": ["hardware-software", "co-design", "design", "methodology", "hardware", "software", "developed", "together", "optimize", "overall", "system", "performance", "increasingly", "important", "custom"]}, {"id": "term-harm-taxonomy", "t": "Harm Taxonomy", "tg": ["Safety", "Governance"], "d": "safety", "x": "A classification system for the types of harm that AI systems can cause. Categories typically include physical...", "l": "h", "k": ["harm", "taxonomy", "classification", "system", "types", "systems", "cause", "categories", "typically", "include", "physical", "psychological", "financial", "reputational", "societal"]}, {"id": "term-harmony-search-algorithm", "t": "Harmony Search Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "An optimization algorithm inspired by the improvisation process of musicians seeking harmony. Each musician (variable)...", "l": "h", "k": ["harmony", "search", "algorithm", "optimization", "inspired", "improvisation", "process", "musicians", "seeking", "musician", "variable", "plays", "note", "value", "adjusted"]}, {"id": "term-harris-corner-detection", "t": "Harris Corner Detection", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "A corner detection algorithm that computes the eigenvalues of the structure tensor at each pixel to identify points...", "l": "h", "k": ["harris", "corner", "detection", "algorithm", "computes", "eigenvalues", "structure", "tensor", "pixel", "identify", "points", "image", "gradient", "changes", "significantly"]}, {"id": "term-harvard-architecture", "t": "Harvard Architecture", "tg": ["Architecture", "Fundamentals", "Historical"], "d": "hardware", "x": "Computer architecture with separate memory and pathways for instructions and data. Used in some AI accelerators and...", "l": "h", "k": ["harvard", "architecture", "computer", "separate", "memory", "pathways", "instructions", "data", "accelerators", "dsps", "avoid", "von", "neumann", "bottleneck"]}, {"id": "term-harvard-mark-i", "t": "Harvard Mark I", "tg": ["Historical", "Computer", "IBM"], "d": "hardware", "x": "Electromechanical computer built by IBM for Harvard University completed in 1944. One of the first large-scale...", "l": "h", "k": ["harvard", "mark", "electromechanical", "computer", "built", "ibm", "university", "completed", "large-scale", "automatic", "digital", "computers", "world", "war", "calculations"]}, {"id": "term-hate-speech-detection", "t": "Hate Speech Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of automatically identifying text that expresses hatred toward a group based on attributes like race, gender,...", "l": "h", "k": ["hate", "speech", "detection", "task", "automatically", "identifying", "text", "expresses", "hatred", "toward", "group", "based", "attributes", "race", "gender"]}, {"id": "term-hateful-memes", "t": "Hateful Memes", "tg": ["Benchmark", "Multimodal", "Safety"], "d": "datasets", "x": "A multimodal dataset from Facebook AI of 10000 memes annotated for hate speech. Tests the ability to understand hateful...", "l": "h", "k": ["hateful", "memes", "multimodal", "dataset", "facebook", "annotated", "hate", "speech", "tests", "ability", "understand", "content", "requires", "joint", "reasoning"]}, {"id": "term-hawk", "t": "Hawk", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A recurrent model architecture from Google DeepMind that uses a simple gated linear recurrence for efficient sequence...", "l": "h", "k": ["hawk", "recurrent", "model", "architecture", "google", "deepmind", "uses", "simple", "gated", "linear", "recurrence", "efficient", "sequence", "modeling", "without"]}, {"id": "term-hazard-function", "t": "Hazard Function", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "The instantaneous rate at which events occur for subjects who have survived up to a given time point. It is a...", "l": "h", "k": ["hazard", "function", "instantaneous", "rate", "events", "occur", "subjects", "survived", "given", "time", "point", "fundamental", "quantity", "survival", "analysis"]}, {"id": "term-hbm2", "t": "HBM2", "tg": ["Memory", "Data Center"], "d": "hardware", "x": "Second generation High Bandwidth Memory stacking DRAM dies on a silicon interposer achieving up to 256 GB/s per stack....", "l": "h", "k": ["hbm2", "generation", "high", "bandwidth", "memory", "stacking", "dram", "dies", "silicon", "interposer", "achieving", "per", "stack", "data", "center"]}, {"id": "term-hbm2e", "t": "HBM2E", "tg": ["Memory", "Data Center"], "d": "hardware", "x": "Enhanced version of HBM2 memory achieving up to 460 GB/s bandwidth per stack with increased density. Used in GPUs like...", "l": "h", "k": ["hbm2e", "enhanced", "version", "hbm2", "memory", "achieving", "bandwidth", "per", "stack", "increased", "density", "gpus", "amd", "mi250x", "nvidia"]}, {"id": "term-hbm3", "t": "HBM3", "tg": ["Memory", "Data Center"], "d": "hardware", "x": "Third generation High Bandwidth Memory specification achieving over 600 GB/s per stack with improved power efficiency....", "l": "h", "k": ["hbm3", "generation", "high", "bandwidth", "memory", "specification", "achieving", "per", "stack", "improved", "power", "efficiency", "featured", "nvidia", "h100"]}, {"id": "term-hbm3e", "t": "HBM3E", "tg": ["Memory", "Data Center"], "d": "hardware", "x": "Extended version of HBM3 memory pushing bandwidth above 1 TB/s per stack. Used in cutting-edge AI accelerators like the...", "l": "h", "k": ["hbm3e", "extended", "version", "hbm3", "memory", "pushing", "bandwidth", "per", "stack", "cutting-edge", "accelerators", "nvidia", "h200", "large", "language"]}, {"id": "term-hd-vila-100m", "t": "HD-VILA-100M", "tg": ["Training Corpus", "Video", "Multimodal"], "d": "datasets", "x": "A large-scale high-resolution video-language dataset containing 100 million video-text pairs. Provides diverse video...", "l": "h", "k": ["hd-vila-100m", "large-scale", "high-resolution", "video-language", "dataset", "containing", "million", "video-text", "pairs", "provides", "diverse", "video", "data", "pretraining", "foundation"]}, {"id": "term-hdbscan-algorithm", "t": "HDBSCAN Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "Hierarchical Density-Based Spatial Clustering extracts a cluster hierarchy from DBSCAN results and selects the most...", "l": "h", "k": ["hdbscan", "algorithm", "hierarchical", "density-based", "spatial", "clustering", "extracts", "cluster", "hierarchy", "dbscan", "results", "selects", "persistent", "clusters", "handles"]}, {"id": "term-he-initialization", "t": "He Initialization", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A weight initialization strategy specifically designed for ReLU activation functions that scales the variance of...", "l": "h", "k": ["initialization", "weight", "strategy", "specifically", "designed", "relu", "activation", "functions", "scales", "variance", "initial", "weights", "divided", "number", "input"]}, {"id": "term-healthcare-ai-ethics", "t": "Healthcare AI Ethics", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Ethical considerations specific to AI in healthcare including patient autonomy informed consent clinical validation...", "l": "h", "k": ["healthcare", "ethics", "ethical", "considerations", "specific", "including", "patient", "autonomy", "informed", "consent", "clinical", "validation", "equitable", "access", "appropriate"]}, {"id": "term-heap-sort", "t": "Heap Sort", "tg": ["Algorithms", "Fundamentals", "Sorting"], "d": "algorithms", "x": "A comparison-based sorting algorithm that uses a binary heap data structure. Builds a max-heap from the input and...", "l": "h", "k": ["heap", "sort", "comparison-based", "sorting", "algorithm", "uses", "binary", "data", "structure", "builds", "max-heap", "input", "repeatedly", "extracts", "maximum"]}, {"id": "term-hearsay-ii", "t": "Hearsay-II", "tg": ["History", "Systems"], "d": "history", "x": "A speech understanding system developed at Carnegie Mellon University in the 1970s. Notable for its blackboard...", "l": "h", "k": ["hearsay-ii", "speech", "understanding", "system", "developed", "carnegie", "mellon", "university", "1970s", "notable", "blackboard", "architecture", "multiple", "knowledge", "sources"]}, {"id": "term-heat-sink", "t": "Heat Sink", "tg": ["Cooling", "Component", "Passive"], "d": "hardware", "x": "Passive cooling device that absorbs and dissipates heat from processor chips through thermal conduction to fins with...", "l": "h", "k": ["heat", "sink", "passive", "cooling", "device", "absorbs", "dissipates", "processor", "chips", "thermal", "conduction", "fins", "large", "surface", "area"]}, {"id": "term-heatmap-prediction", "t": "Heatmap Prediction", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A technique in keypoint detection that predicts a probability heatmap for each keypoint type, with peaks indicating...", "l": "h", "k": ["heatmap", "prediction", "technique", "keypoint", "detection", "predicts", "probability", "type", "peaks", "indicating", "likely", "locations", "providing", "sub-pixel", "localization"]}, {"id": "term-heavy-light-decomposition", "t": "Heavy-Light Decomposition", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A technique that decomposes a tree into chains of heavy edges to enable efficient path queries. Allows answering path...", "l": "h", "k": ["heavy-light", "decomposition", "technique", "decomposes", "tree", "chains", "heavy", "edges", "enable", "efficient", "path", "queries", "allows", "answering", "trees"]}, {"id": "term-hebbian-learning", "t": "Hebbian Learning", "tg": ["History", "Milestones"], "d": "history", "x": "A learning principle proposed by Donald Hebb in 1949 stating that neurons that fire together wire together, meaning...", "l": "h", "k": ["hebbian", "learning", "principle", "proposed", "donald", "hebb", "stating", "neurons", "fire", "together", "wire", "meaning", "synaptic", "connections", "strengthen"]}, {"id": "term-hedge-detection", "t": "Hedge Detection", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of identifying linguistic expressions that indicate uncertainty, speculation, or tentativeness in text, such...", "l": "h", "k": ["hedge", "detection", "task", "identifying", "linguistic", "expressions", "indicate", "uncertainty", "speculation", "tentativeness", "text", "possibly", "appears"]}, {"id": "term-hellaswag", "t": "HellaSwag", "tg": ["Benchmark", "Evaluation"], "d": "datasets", "x": "A benchmark testing commonsense natural language inference. Models must select the most plausible continuation of a...", "l": "h", "k": ["hellaswag", "benchmark", "testing", "commonsense", "natural", "language", "inference", "models", "must", "select", "plausible", "continuation", "scenario", "evaluating", "real-world"]}, {"id": "term-helm", "t": "HELM", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "The Holistic Evaluation of Language Models framework from Stanford CRFM. Evaluates language models across multiple...", "l": "h", "k": ["helm", "holistic", "evaluation", "language", "models", "framework", "stanford", "crfm", "evaluates", "across", "multiple", "scenarios", "metrics", "including", "accuracy"]}, {"id": "term-herbert-gelernter", "t": "Herbert Gelernter", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who created the Geometry Theorem Prover in 1959 at IBM. The program could prove theorems in...", "l": "h", "k": ["herbert", "gelernter", "american", "computer", "scientist", "created", "geometry", "theorem", "prover", "ibm", "program", "prove", "theorems", "euclidean", "axioms"]}, {"id": "term-herbert-simon", "t": "Herbert Simon", "tg": ["History", "Pioneers"], "d": "history", "x": "American political scientist, economist, and AI pioneer (1916-2001) who co-created the Logic Theorist and General...", "l": "h", "k": ["herbert", "simon", "american", "political", "scientist", "economist", "pioneer", "1916-2001", "co-created", "logic", "theorist", "general", "problem", "solver", "developed"]}, {"id": "term-hessian-locally-linear-embedding", "t": "Hessian Locally Linear Embedding", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "An extension of locally linear embedding that uses the Hessian of the manifold to compute the embedding. Provides a...", "l": "h", "k": ["hessian", "locally", "linear", "embedding", "extension", "uses", "manifold", "compute", "provides", "theoretically", "justified", "recovers", "true", "underlying", "parameterization"]}, {"id": "term-heterogeneous-computing", "t": "Heterogeneous Computing", "tg": ["Architecture", "Fundamentals", "Design"], "d": "hardware", "x": "Computing approach using multiple types of processors such as CPUs GPUs and specialized accelerators to handle...", "l": "h", "k": ["heterogeneous", "computing", "approach", "multiple", "types", "processors", "cpus", "gpus", "specialized", "accelerators", "handle", "different", "workload", "characteristics", "efficiently"]}, {"id": "term-heteroscedasticity", "t": "Heteroscedasticity", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A condition in regression analysis where the variance of the residuals is not constant across all levels of the...", "l": "h", "k": ["heteroscedasticity", "condition", "regression", "analysis", "variance", "residuals", "constant", "across", "levels", "independent", "variables", "violates", "key", "assumption", "ordinary"]}, {"id": "term-heuristic", "t": "Heuristic", "tg": ["Concept", "Problem Solving"], "d": "general", "x": "A practical rule or strategy that helps solve problems or make decisions quickly, even if not guaranteed to be optimal....", "l": "h", "k": ["heuristic", "practical", "rule", "strategy", "helps", "solve", "problems", "decisions", "quickly", "guaranteed", "optimal", "search", "optimization", "rule-based", "systems"]}, {"id": "term-heuristic-search", "t": "Heuristic Search", "tg": ["History", "Fundamentals"], "d": "history", "x": "A problem-solving approach that uses rules of thumb or estimates to guide search through a problem space more...", "l": "h", "k": ["heuristic", "search", "problem-solving", "approach", "uses", "rules", "thumb", "estimates", "guide", "problem", "space", "efficiently", "exhaustive", "herbert", "simon"]}, {"id": "term-hgrn2", "t": "HGRN2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "Hierarchical Gated Recurrent Network version 2 uses state expansion and outer-product-based gating for efficient...", "l": "h", "k": ["hgrn2", "hierarchical", "gated", "recurrent", "network", "version", "uses", "state", "expansion", "outer-product-based", "gating", "efficient", "linear-time", "sequence", "modeling"]}, {"id": "term-hi-fi-multi-speaker-english-tts", "t": "Hi-Fi Multi-Speaker English TTS", "tg": ["Training Corpus", "Speech"], "d": "datasets", "x": "A high-fidelity multi-speaker text-to-speech dataset containing 291.6 hours of speech from 10 speakers. Provides...", "l": "h", "k": ["hi-fi", "multi-speaker", "english", "tts", "high-fidelity", "text-to-speech", "dataset", "containing", "hours", "speech", "speakers", "provides", "high-quality", "data", "training"]}, {"id": "term-hidden-layer", "t": "Hidden Layer", "tg": ["Architecture", "Neural Networks"], "d": "models", "x": "Layers in a neural network between the input and output layers. Deep networks have many hidden layers, enabling them to...", "l": "h", "k": ["hidden", "layer", "layers", "neural", "network", "input", "output", "deep", "networks", "enabling", "learn", "complex", "hierarchical", "representations"]}, {"id": "term-hidden-markov-model", "t": "Hidden Markov Model", "tg": ["Machine Learning", "Probability"], "d": "algorithms", "x": "A statistical model where the system is assumed to be a Markov process with unobserved (hidden) states, and...", "l": "h", "k": ["hidden", "markov", "model", "statistical", "system", "assumed", "process", "unobserved", "states", "observations", "probabilistically", "dependent", "state", "widely", "speech"]}, {"id": "term-hidden-markov-models", "t": "Hidden Markov Models", "tg": ["History", "Fundamentals"], "d": "history", "x": "Statistical models where the system being modeled is assumed to follow a Markov process with unobserved states. HMMs...", "l": "h", "k": ["hidden", "markov", "models", "statistical", "system", "modeled", "assumed", "follow", "process", "unobserved", "states", "hmms", "became", "fundamental", "speech"]}, {"id": "term-hidden-technical-debt", "t": "Hidden Technical Debt", "tg": ["Safety", "Technical"], "d": "safety", "x": "The accumulated maintenance burden in machine learning systems that arises from entangled dependencies unstable data...", "l": "h", "k": ["hidden", "technical", "debt", "accumulated", "maintenance", "burden", "machine", "learning", "systems", "arises", "entangled", "dependencies", "unstable", "data", "inputs"]}, {"id": "term-hiera", "t": "Hiera", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A hierarchical vision Transformer from Meta that removes unnecessary architectural complexity by leveraging masked...", "l": "h", "k": ["hiera", "hierarchical", "vision", "transformer", "meta", "removes", "unnecessary", "architectural", "complexity", "leveraging", "masked", "autoencoder", "pre-training", "simplicity", "speed"]}, {"id": "term-hierarchical-clustering", "t": "Hierarchical Clustering", "tg": ["Machine Learning", "Clustering"], "d": "general", "x": "A clustering approach that builds a tree-like hierarchy of clusters either by progressively merging smaller clusters...", "l": "h", "k": ["hierarchical", "clustering", "approach", "builds", "tree-like", "hierarchy", "clusters", "progressively", "merging", "smaller", "agglomerative", "recursively", "splitting", "larger", "divisive"]}, {"id": "term-hierarchical-rl", "t": "Hierarchical Reinforcement Learning", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "RL frameworks that decompose complex tasks into hierarchies of subtasks or skills operating at different temporal...", "l": "h", "k": ["hierarchical", "reinforcement", "learning", "frameworks", "decompose", "complex", "tasks", "hierarchies", "subtasks", "skills", "operating", "different", "temporal", "abstractions", "enables"]}, {"id": "term-hierarchical-storage-management", "t": "Hierarchical Storage Management", "tg": ["Storage", "Management", "Strategy"], "d": "hardware", "x": "Storage strategy that automatically migrates data between high-performance and low-cost storage tiers based on access...", "l": "h", "k": ["hierarchical", "storage", "management", "strategy", "automatically", "migrates", "data", "high-performance", "low-cost", "tiers", "based", "access", "patterns", "environments", "manage"]}, {"id": "term-hifi-gan", "t": "HiFi-GAN", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A generative adversarial network for high-fidelity speech synthesis that uses multi-period and multi-scale...", "l": "h", "k": ["hifi-gan", "generative", "adversarial", "network", "high-fidelity", "speech", "synthesis", "uses", "multi-period", "multi-scale", "discriminators", "audio", "waveform", "generation"]}, {"id": "term-hbm", "t": "High Bandwidth Memory (HBM)", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "A high-performance DRAM technology that stacks memory dies vertically and connects them with through-silicon vias,...", "l": "h", "k": ["high", "bandwidth", "memory", "hbm", "high-performance", "dram", "technology", "stacks", "dies", "vertically", "connects", "through-silicon", "vias", "providing", "significantly"]}, {"id": "term-high-risk-ai-systems", "t": "High-Risk AI Systems", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Under the EU AI Act, AI systems used in critical domains such as biometric identification, critical infrastructure,...", "l": "h", "k": ["high-risk", "systems", "act", "critical", "domains", "biometric", "identification", "infrastructure", "education", "employment", "law", "enforcement", "migration", "must", "meet"]}, {"id": "term-high-stakes-ai", "t": "High-Stakes AI", "tg": ["Ethics", "Risk"], "d": "safety", "x": "AI applications where errors can have severe consequences: healthcare diagnoses, legal decisions, financial...", "l": "h", "k": ["high-stakes", "applications", "errors", "severe", "consequences", "healthcare", "diagnoses", "legal", "decisions", "financial", "transactions", "requires", "extra", "scrutiny", "testing"]}, {"id": "term-hilbert-transform", "t": "Hilbert Transform", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A linear operator that computes the analytic signal by producing a 90-degree phase shift of the input signal. Used to...", "l": "h", "k": ["hilbert", "transform", "linear", "operator", "computes", "analytic", "signal", "producing", "90-degree", "phase", "shift", "input", "compute", "instantaneous", "amplitude"]}, {"id": "term-hill-climbing", "t": "Hill Climbing", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A local search optimization algorithm that iteratively moves toward higher-valued neighbors until no improvement is...", "l": "h", "k": ["hill", "climbing", "local", "search", "optimization", "algorithm", "iteratively", "moves", "toward", "higher-valued", "neighbors", "improvement", "found", "simple", "fast"]}, {"id": "term-hindsight-experience-replay", "t": "Hindsight Experience Replay (HER)", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A technique that augments failed trajectories by relabeling goals with actually achieved states, turning failures into...", "l": "h", "k": ["hindsight", "experience", "replay", "technique", "augments", "failed", "trajectories", "relabeling", "goals", "actually", "achieved", "states", "turning", "failures", "successes"]}, {"id": "term-hinge-loss", "t": "Hinge Loss", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A loss function used in maximum-margin classifiers such as SVMs, defined as max(0, 1 - y * f(x)). It penalizes...", "l": "h", "k": ["hinge", "loss", "function", "maximum-margin", "classifiers", "svms", "defined", "max", "penalizes", "predictions", "wrong", "side", "margin", "boundary"]}, {"id": "term-historical-bias", "t": "Historical Bias", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Bias that is encoded in data reflecting past discriminatory practices or societal inequalities, which AI systems can...", "l": "h", "k": ["historical", "bias", "encoded", "data", "reflecting", "past", "discriminatory", "practices", "societal", "inequalities", "systems", "perpetuate", "accurately", "represents", "patterns"]}, {"id": "term-hit-rate", "t": "Hit Rate", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A retrieval metric that measures the fraction of queries for which at least one relevant document appears in the top K...", "l": "h", "k": ["hit", "rate", "retrieval", "metric", "measures", "fraction", "queries", "least", "relevant", "document", "appears", "top", "results", "providing", "simple"]}, {"id": "term-hmdb-51", "t": "HMDB-51", "tg": ["Benchmark", "Video"], "d": "datasets", "x": "A dataset of 6849 video clips from 51 action categories collected from movies and web videos. Tests human motion...", "l": "h", "k": ["hmdb-51", "dataset", "video", "clips", "action", "categories", "collected", "movies", "web", "videos", "tests", "human", "motion", "recognition", "realistic"]}, {"id": "term-hnsw", "t": "HNSW", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "Hierarchical Navigable Small World, a graph-based approximate nearest neighbor algorithm that builds multi-layered...", "l": "h", "k": ["hnsw", "hierarchical", "navigable", "small", "world", "graph-based", "approximate", "nearest", "neighbor", "algorithm", "builds", "multi-layered", "proximity", "graphs", "logarithmic"]}, {"id": "term-hnsw-algorithm", "t": "HNSW Algorithm", "tg": ["Algorithms", "Technical", "Searching", "Data Structure"], "d": "algorithms", "x": "Hierarchical Navigable Small World graphs provide fast approximate nearest-neighbor search by building a multi-layer...", "l": "h", "k": ["hnsw", "algorithm", "hierarchical", "navigable", "small", "world", "graphs", "provide", "fast", "approximate", "nearest-neighbor", "search", "building", "multi-layer", "proximity"]}, {"id": "term-hoeffdings-inequality", "t": "Hoeffding's Inequality", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A probability inequality that provides an upper bound on the probability that the sum of bounded independent random...", "l": "h", "k": ["hoeffding", "inequality", "probability", "provides", "upper", "bound", "sum", "bounded", "independent", "random", "variables", "deviates", "expected", "value", "derive"]}, {"id": "term-hog-algorithm", "t": "HOG Algorithm", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "Histogram of Oriented Gradients computes feature descriptors by counting gradient orientation occurrences in localized...", "l": "h", "k": ["hog", "algorithm", "histogram", "oriented", "gradients", "computes", "feature", "descriptors", "counting", "gradient", "orientation", "occurrences", "localized", "cells", "image"]}, {"id": "term-holdout-method", "t": "Holdout Method", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "The simplest model evaluation strategy that splits data into separate training and test sets, typically using 70-80%...", "l": "h", "k": ["holdout", "method", "simplest", "model", "evaluation", "strategy", "splits", "data", "separate", "training", "test", "sets", "typically", "70-80", "remainder"]}, {"id": "term-holt-winters-method", "t": "Holt-Winters Method", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A triple exponential smoothing method for time series forecasting that captures level, trend, and seasonal components....", "l": "h", "k": ["holt-winters", "method", "triple", "exponential", "smoothing", "time", "series", "forecasting", "captures", "level", "trend", "seasonal", "components", "supports", "additive"]}, {"id": "term-homography", "t": "Homography", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A projective transformation matrix that maps points between two image planes, used for image stitching, perspective...", "l": "h", "k": ["homography", "projective", "transformation", "matrix", "maps", "points", "image", "planes", "stitching", "perspective", "correction", "augmented", "reality", "scene", "planar"]}, {"id": "term-homomorphic-encryption", "t": "Homomorphic Encryption", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "A cryptographic technique that allows computations to be performed on encrypted data without decrypting it first,...", "l": "h", "k": ["homomorphic", "encryption", "cryptographic", "technique", "allows", "computations", "performed", "encrypted", "data", "without", "decrypting", "enabling", "models", "process", "sensitive"]}, {"id": "term-homomorphic-encryption-for-ml", "t": "Homomorphic Encryption for ML", "tg": ["Algorithms", "Technical", "Privacy"], "d": "algorithms", "x": "A cryptographic technique that enables computation on encrypted data without decryption. Allows machine learning...", "l": "h", "k": ["homomorphic", "encryption", "cryptographic", "technique", "enables", "computation", "encrypted", "data", "without", "decryption", "allows", "machine", "learning", "inference", "training"]}, {"id": "term-homonymy", "t": "Homonymy", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The property of two or more words having the same spelling or pronunciation but unrelated meanings, such as 'bat' the...", "l": "h", "k": ["homonymy", "property", "words", "having", "spelling", "pronunciation", "unrelated", "meanings", "bat", "animal", "sporting", "equipment"]}, {"id": "term-hopcroft-karp-algorithm", "t": "Hopcroft-Karp Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm for finding maximum cardinality matching in bipartite graphs. Uses alternating BFS and DFS phases to find...", "l": "h", "k": ["hopcroft-karp", "algorithm", "finding", "maximum", "cardinality", "matching", "bipartite", "graphs", "uses", "alternating", "bfs", "dfs", "phases", "find", "augmenting"]}, {"id": "term-hopfield-network", "t": "Hopfield Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A recurrent neural network with symmetric connections that stores patterns as energy minima, functioning as an...", "l": "h", "k": ["hopfield", "network", "recurrent", "neural", "symmetric", "connections", "stores", "patterns", "energy", "minima", "functioning", "associative", "memory", "retrieves", "stored"]}, {"id": "term-horn-schunck-optical-flow", "t": "Horn-Schunck Optical Flow", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A global method for computing dense optical flow that adds a smoothness constraint to the brightness constancy...", "l": "h", "k": ["horn-schunck", "optical", "flow", "global", "method", "computing", "dense", "adds", "smoothness", "constraint", "brightness", "constancy", "assumption", "produces", "vectors"]}, {"id": "term-horovod", "t": "Horovod", "tg": ["Distributed Training", "Open Source", "Framework"], "d": "hardware", "x": "Uber open-source distributed deep learning training framework that simplifies multi-GPU and multi-node training. Uses...", "l": "h", "k": ["horovod", "uber", "open-source", "distributed", "deep", "learning", "training", "framework", "simplifies", "multi-gpu", "multi-node", "uses", "ring", "allreduce", "efficient"]}, {"id": "term-hot-aisle-cold-aisle", "t": "Hot Aisle Cold Aisle", "tg": ["Cooling", "Data Center", "Design"], "d": "hardware", "x": "Data center airflow management strategy that arranges server racks to create alternating hot and cold aisles. Improves...", "l": "h", "k": ["hot", "aisle", "cold", "data", "center", "airflow", "management", "strategy", "arranges", "server", "racks", "create", "alternating", "aisles", "improves"]}, {"id": "term-hotpotqa", "t": "HotpotQA", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A question answering dataset requiring multi-hop reasoning across multiple Wikipedia paragraphs. Contains 113000...", "l": "h", "k": ["hotpotqa", "question", "answering", "dataset", "requiring", "multi-hop", "reasoning", "across", "multiple", "wikipedia", "paragraphs", "contains", "questions", "cannot", "answered"]}, {"id": "term-hough-transform", "t": "Hough Transform", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "A feature extraction technique that detects shapes (lines and circles and other parametric curves) in images by mapping...", "l": "h", "k": ["hough", "transform", "feature", "extraction", "technique", "detects", "shapes", "lines", "circles", "parametric", "curves", "images", "mapping", "edge", "points"]}, {"id": "term-householder-transformation", "t": "Householder Transformation", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A linear algebra operation that reflects a vector about a hyperplane. Used in QR decomposition and tridiagonalization...", "l": "h", "k": ["householder", "transformation", "linear", "algebra", "operation", "reflects", "vector", "hyperplane", "decomposition", "tridiagonalization", "symmetric", "matrices", "numerically", "stable", "gram-schmidt"]}, {"id": "term-how2", "t": "How2", "tg": ["Training Corpus", "Video", "Multimodal", "Translation"], "d": "datasets", "x": "A multimodal dataset of 80000 instructional videos with English subtitles and Portuguese translations. Used for...", "l": "h", "k": ["how2", "multimodal", "dataset", "instructional", "videos", "english", "subtitles", "portuguese", "translations", "machine", "translation", "video", "summarization", "research"]}, {"id": "term-hpd", "t": "HPD", "tg": ["Training Corpus", "Multimodal", "Evaluation"], "d": "datasets", "x": "Human Preference Dataset for image generation containing human preference judgments between pairs of generated images....", "l": "h", "k": ["hpd", "human", "preference", "dataset", "image", "generation", "containing", "judgments", "pairs", "generated", "images", "aligning", "models", "aesthetics"]}, {"id": "term-hpl-mxp", "t": "HPL-MxP", "tg": ["Benchmark", "Performance", "AI"], "d": "hardware", "x": "Mixed-precision LINPACK benchmark measuring performance using lower precision formats typical of AI workloads. Better...", "l": "h", "k": ["hpl-mxp", "mixed-precision", "linpack", "benchmark", "measuring", "performance", "lower", "precision", "formats", "typical", "workloads", "better", "reflects", "real-world", "hardware"]}, {"id": "term-huawei-ascend", "t": "Huawei Ascend", "tg": ["Accelerator", "China", "Data Center"], "d": "hardware", "x": "Huawei AI accelerator chip family based on the Da Vinci architecture covering edge to cloud AI workloads. The Ascend...", "l": "h", "k": ["huawei", "ascend", "accelerator", "chip", "family", "based", "vinci", "architecture", "covering", "edge", "cloud", "workloads", "targets", "large-scale", "training"]}, {"id": "term-huber-loss", "t": "Huber Loss", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A loss function that is quadratic for small residuals and linear for large residuals, combining the best properties of...", "l": "h", "k": ["huber", "loss", "function", "quadratic", "small", "residuals", "linear", "large", "combining", "best", "properties", "mean", "squared", "error", "absolute"]}, {"id": "term-hubert", "t": "HuBERT", "tg": ["Models", "Technical"], "d": "models", "x": "Hidden-Unit BERT is a self-supervised speech representation model that learns from masked prediction of discrete hidden...", "l": "h", "k": ["hubert", "hidden-unit", "bert", "self-supervised", "speech", "representation", "model", "learns", "masked", "prediction", "discrete", "hidden", "units", "derived", "clustering"]}, {"id": "term-hubert-dreyfus", "t": "Hubert Dreyfus", "tg": ["History", "Pioneers"], "d": "history", "x": "American philosopher who was one of the earliest critics of artificial intelligence. His 1972 book What Computers Can't...", "l": "h", "k": ["hubert", "dreyfus", "american", "philosopher", "earliest", "critics", "artificial", "intelligence", "book", "computers", "argued", "based", "symbolic", "processing", "replicate"]}, {"id": "term-huffman-coding-algorithm", "t": "Huffman Coding Algorithm", "tg": ["Algorithms", "Fundamentals", "Information Theory", "History"], "d": "algorithms", "x": "A lossless data compression algorithm that assigns variable-length codes to symbols based on their frequencies. More...", "l": "h", "k": ["huffman", "coding", "algorithm", "lossless", "data", "compression", "assigns", "variable-length", "codes", "symbols", "based", "frequencies", "frequent", "receive", "shorter"]}, {"id": "term-hugging-face", "t": "Hugging Face", "tg": ["Platform", "Open Source"], "d": "general", "x": "A platform and community for sharing AI models, datasets, and applications. Hosts thousands of open-source models and...", "l": "h", "k": ["hugging", "face", "platform", "community", "sharing", "models", "datasets", "applications", "hosts", "thousands", "open-source", "popular", "transformers", "library", "nlp"]}, {"id": "term-hugging-face-datasets-library", "t": "Hugging Face Datasets Library", "tg": ["Platform", "General"], "d": "datasets", "x": "A library for easily accessing and sharing datasets with efficient data loading and processing. Provides a unified API...", "l": "h", "k": ["hugging", "face", "datasets", "library", "easily", "accessing", "sharing", "efficient", "data", "loading", "processing", "provides", "unified", "api", "thousands"]}, {"id": "term-hugging-face-hub", "t": "Hugging Face Hub", "tg": ["Platform", "NLP", "Computer Vision"], "d": "datasets", "x": "A platform hosting over 200000 machine learning models datasets and demo applications. The largest open repository for...", "l": "h", "k": ["hugging", "face", "hub", "platform", "hosting", "machine", "learning", "models", "datasets", "demo", "applications", "largest", "open", "repository", "sharing"]}, {"id": "term-human-dignity-in-ai", "t": "Human Dignity in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The principle that AI systems should respect the inherent worth and rights of all human beings. Serves as a...", "l": "h", "k": ["human", "dignity", "principle", "systems", "respect", "inherent", "worth", "rights", "beings", "serves", "foundational", "value", "ethics", "frameworks", "rights-based"]}, {"id": "term-human-evaluation", "t": "Human Evaluation", "tg": ["Evaluation", "Methodology"], "d": "datasets", "x": "The gold standard for assessing language model outputs where human annotators rate or compare generated text on...", "l": "h", "k": ["human", "evaluation", "gold", "standard", "assessing", "language", "model", "outputs", "annotators", "rate", "compare", "generated", "text", "dimensions", "quality"]}, {"id": "term-human-factors-in-ai-safety", "t": "Human Factors in AI Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "The study of how human cognitive abilities limitations and behaviors interact with AI systems to affect safety...", "l": "h", "k": ["human", "factors", "safety", "study", "cognitive", "abilities", "limitations", "behaviors", "interact", "systems", "affect", "outcomes", "includes", "interface", "design"]}, {"id": "term-human-mesh-recovery", "t": "Human Mesh Recovery", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The task of estimating a full 3D human body mesh (shape and pose) from a single 2D image, using parametric body models...", "l": "h", "k": ["human", "mesh", "recovery", "task", "estimating", "full", "body", "shape", "pose", "single", "image", "parametric", "models", "smpl", "represent"]}, {"id": "term-human-oversight", "t": "Human Oversight", "tg": ["Safety", "Fundamentals"], "d": "safety", "x": "The requirement that human beings maintain meaningful control over AI system decisions particularly in high-stakes...", "l": "h", "k": ["human", "oversight", "requirement", "beings", "maintain", "meaningful", "control", "system", "decisions", "particularly", "high-stakes", "applications", "cornerstone", "principle", "responsible"]}, {"id": "term-human-rights-impact-assessment-for-ai", "t": "Human Rights Impact Assessment for AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "A structured evaluation of how an AI system may affect internationally recognized human rights including privacy...", "l": "h", "k": ["human", "rights", "impact", "assessment", "structured", "evaluation", "system", "affect", "internationally", "recognized", "including", "privacy", "non-discrimination", "freedom", "expression"]}, {"id": "term-hitl", "t": "Human-in-the-Loop (HITL)", "tg": ["Practice", "Safety"], "d": "safety", "x": "A design approach where humans review, approve, or modify AI outputs before they're acted upon. Critical for...", "l": "h", "k": ["human-in-the-loop", "hitl", "design", "approach", "humans", "review", "approve", "modify", "outputs", "acted", "upon", "critical", "high-stakes", "decisions", "quality"]}, {"id": "term-humaneval", "t": "HumanEval", "tg": ["Benchmark", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating code generation capabilities of LLMs. Contains programming problems with test cases,...", "l": "h", "k": ["humaneval", "benchmark", "evaluating", "code", "generation", "capabilities", "llms", "contains", "programming", "problems", "test", "cases", "measuring", "generated", "passes"]}, {"id": "term-humanitys-last-exam", "t": "Humanity's Last Exam", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark of extremely challenging expert-level questions crowdsourced from academics and professionals. Designed to...", "l": "h", "k": ["humanity", "last", "exam", "benchmark", "extremely", "challenging", "expert-level", "questions", "crowdsourced", "academics", "professionals", "designed", "test", "upper", "limits"]}, {"id": "term-hungarian-algorithm", "t": "Hungarian Algorithm", "tg": ["Algorithms", "Technical", "Graph", "Optimization"], "d": "algorithms", "x": "An optimization algorithm that solves the assignment problem in polynomial time by finding a minimum-cost matching in a...", "l": "h", "k": ["hungarian", "algorithm", "optimization", "solves", "assignment", "problem", "polynomial", "time", "finding", "minimum-cost", "matching", "weighted", "bipartite", "graph", "runs"]}, {"id": "term-hunyuandit", "t": "HunyuanDiT", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A text-to-image diffusion Transformer from Tencent that supports both Chinese and English text prompts with...", "l": "h", "k": ["hunyuandit", "text-to-image", "diffusion", "transformer", "tencent", "supports", "chinese", "english", "text", "prompts", "fine-grained", "understanding", "bilingual", "instructions"]}, {"id": "term-hybrid-ai", "t": "Hybrid AI", "tg": ["Architecture", "Design"], "d": "models", "x": "Systems combining multiple AI approaches: neural networks with symbolic reasoning, LLMs with search, or ML with...", "l": "h", "k": ["hybrid", "systems", "combining", "multiple", "approaches", "neural", "networks", "symbolic", "reasoning", "llms", "search", "rule-based", "logic", "robust", "pure"]}, {"id": "term-hybrid-retrieval", "t": "Hybrid Retrieval", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An information retrieval approach that combines sparse retrieval methods like BM25 with dense retrieval using neural...", "l": "h", "k": ["hybrid", "retrieval", "information", "approach", "combines", "sparse", "methods", "bm25", "dense", "neural", "embeddings", "leverages", "strengths", "exact", "term"]}, {"id": "term-hybrid-search", "t": "Hybrid Search", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A retrieval approach that combines dense vector similarity search with traditional keyword-based (sparse) retrieval,...", "l": "h", "k": ["hybrid", "search", "retrieval", "approach", "combines", "dense", "vector", "similarity", "traditional", "keyword-based", "sparse", "leveraging", "strengths", "methods", "robust"]}, {"id": "term-hyena", "t": "Hyena", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A subquadratic attention replacement that uses long convolutions parameterized by implicit neural representations and...", "l": "h", "k": ["hyena", "subquadratic", "attention", "replacement", "uses", "long", "convolutions", "parameterized", "implicit", "neural", "representations", "multiplicative", "gating", "achieve", "competitive"]}, {"id": "term-hyena-hierarchy", "t": "Hyena Hierarchy", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A sub-quadratic attention replacement that uses long convolutions and gating to achieve Transformer-quality performance...", "l": "h", "k": ["hyena", "hierarchy", "sub-quadratic", "attention", "replacement", "uses", "long", "convolutions", "gating", "achieve", "transformer-quality", "performance", "linear", "scaling", "sequence"]}, {"id": "term-hyper-sd", "t": "Hyper-SD", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A framework for generating images in one to eight steps using human feedback-driven trajectory segmentation for unified...", "l": "h", "k": ["hyper-sd", "framework", "generating", "images", "eight", "steps", "human", "feedback-driven", "trajectory", "segmentation", "unified", "low-step", "diffusion", "models"]}, {"id": "term-hyperband", "t": "Hyperband", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A hyperparameter optimization method that combines random search with early stopping, allocating more resources to...", "l": "h", "k": ["hyperband", "hyperparameter", "optimization", "method", "combines", "random", "search", "early", "stopping", "allocating", "resources", "promising", "configurations", "successively", "halving"]}, {"id": "term-hyperloglog-algorithm", "t": "HyperLogLog Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A probabilistic cardinality estimation algorithm that counts the number of distinct elements in a multiset using...", "l": "h", "k": ["hyperloglog", "algorithm", "probabilistic", "cardinality", "estimation", "counts", "number", "distinct", "elements", "multiset", "logarithmic", "memory", "achieves", "approximately", "standard"]}, {"id": "term-hypernymy", "t": "Hypernymy", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A semantic relation where one word's meaning includes and is more general than another's, such as 'animal' being a...", "l": "h", "k": ["hypernymy", "semantic", "relation", "word", "meaning", "includes", "general", "another", "animal", "hypernym", "dog", "forming", "taxonomic", "hierarchies", "lexical"]}, {"id": "term-hyperparameter", "t": "Hyperparameter", "tg": ["Training", "Configuration"], "d": "general", "x": "Configuration settings that control the training process rather than being learned from data. Examples include learning...", "l": "h", "k": ["hyperparameter", "configuration", "settings", "control", "training", "process", "rather", "learned", "data", "examples", "include", "learning", "rate", "batch", "size"]}, {"id": "term-hyperparameter-optimization", "t": "Hyperparameter Optimization", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "The process of finding optimal hyperparameter configurations for machine learning models. Methods include grid search...", "l": "h", "k": ["hyperparameter", "optimization", "process", "finding", "optimal", "configurations", "machine", "learning", "models", "methods", "include", "grid", "search", "random", "bayesian"]}, {"id": "term-hyperscale-data-center", "t": "Hyperscale Data Center", "tg": ["Data Center", "Infrastructure"], "d": "hardware", "x": "Massive computing facility with tens of thousands of servers designed for cloud computing and AI workloads. Operated by...", "l": "h", "k": ["hyperscale", "data", "center", "massive", "computing", "facility", "tens", "thousands", "servers", "designed", "cloud", "workloads", "operated", "companies", "google"]}, {"id": "term-hyponymy", "t": "Hyponymy", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A semantic relation where one word's meaning is more specific than and included within another's, such as 'dog' being a...", "l": "h", "k": ["hyponymy", "semantic", "relation", "word", "meaning", "specific", "included", "within", "another", "dog", "hyponym", "animal", "representing", "is-a", "relationship"]}, {"id": "term-hypothesis", "t": "Hypothesis (ML)", "tg": ["Concept", "Theory"], "d": "general", "x": "A specific function or model that the learning algorithm considers as a possible solution. The hypothesis space is all...", "l": "h", "k": ["hypothesis", "specific", "function", "model", "learning", "algorithm", "considers", "possible", "solution", "space", "hypotheses", "choose"]}, {"id": "term-hypothetical-document-embedding", "t": "Hypothetical Document Embedding", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A retrieval technique (HyDE) where a language model generates a hypothetical answer to a query, and the embedding of...", "l": "h", "k": ["hypothetical", "document", "embedding", "retrieval", "technique", "hyde", "language", "model", "generates", "answer", "query", "search", "real", "documents", "improving"]}, {"id": "term-i2vgen-xl", "t": "I2VGen-XL", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A cascaded image-to-video generation model that creates high-resolution videos from single images with both semantic...", "l": "i", "k": ["i2vgen-xl", "cascaded", "image-to-video", "generation", "model", "creates", "high-resolution", "videos", "single", "images", "semantic", "accuracy", "temporal", "continuity"]}, {"id": "term-ian-goodfellow", "t": "Ian Goodfellow", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who invented generative adversarial networks (GANs) in 2014, introducing a framework where...", "l": "i", "k": ["ian", "goodfellow", "american", "computer", "scientist", "invented", "generative", "adversarial", "networks", "gans", "introducing", "framework", "neural", "compete", "generate"]}, {"id": "term-ibm-701", "t": "IBM 701", "tg": ["Historical", "IBM", "Computer"], "d": "hardware", "x": "IBM first commercial scientific computer introduced in 1952. Marked IBM entry into the electronic computing market and...", "l": "i", "k": ["ibm", "commercial", "scientific", "computer", "introduced", "marked", "entry", "electronic", "computing", "market", "primarily", "calculations", "defense", "work"]}, {"id": "term-ibm-blue-gene", "t": "IBM Blue Gene", "tg": ["Historical", "Supercomputer", "IBM"], "d": "hardware", "x": "IBM supercomputer architecture series emphasizing low power consumption and high density. Blue Gene/L held the TOP500...", "l": "i", "k": ["ibm", "blue", "gene", "supercomputer", "architecture", "series", "emphasizing", "low", "power", "consumption", "high", "density", "held", "top500", "number"]}, {"id": "term-ibm-deep-blue", "t": "IBM Deep Blue", "tg": ["Historical", "IBM", "AI"], "d": "hardware", "x": "IBM chess-playing computer that defeated world champion Garry Kasparov in 1997. Used custom VLSI chess chips...", "l": "i", "k": ["ibm", "deep", "blue", "chess-playing", "computer", "defeated", "world", "champion", "garry", "kasparov", "custom", "vlsi", "chess", "chips", "demonstrating"]}, {"id": "term-ibm-power-architecture", "t": "IBM POWER Architecture", "tg": ["Processor", "IBM", "Architecture"], "d": "hardware", "x": "IBM high-performance processor architecture used in servers and supercomputers. POWER9 and POWER10 processors are used...", "l": "i", "k": ["ibm", "power", "architecture", "high-performance", "processor", "servers", "supercomputers", "power9", "power10", "processors", "alongside", "nvidia", "gpus", "systems"]}, {"id": "term-ibm-quantum", "t": "IBM Quantum", "tg": ["Quantum", "IBM", "Cloud"], "d": "hardware", "x": "IBM quantum computing program providing cloud access to quantum processors through the IBM Quantum Network. Offers...", "l": "i", "k": ["ibm", "quantum", "computing", "program", "providing", "cloud", "access", "processors", "network", "offers", "systems", "qubits", "roadmap", "toward", "fault-tolerant"]}, {"id": "term-ibm-research-ai", "t": "IBM Research AI", "tg": ["History", "Organizations"], "d": "history", "x": "AI research at IBM spanning decades from Arthur Samuel's checkers program (1959) through Deep Blue (1997) Watson (2011)...", "l": "i", "k": ["ibm", "research", "spanning", "decades", "arthur", "samuel", "checkers", "program", "deep", "blue", "watson", "modern", "enterprise", "longest-running", "corporate"]}, {"id": "term-ibm-system360", "t": "IBM System/360", "tg": ["Historical", "IBM", "Mainframe"], "d": "hardware", "x": "Revolutionary IBM mainframe family introduced in 1964 that used a single instruction set architecture across all...", "l": "i", "k": ["ibm", "system", "revolutionary", "mainframe", "family", "introduced", "single", "instruction", "architecture", "across", "models", "established", "concept", "compatible", "computer"]}, {"id": "term-ibm-system370", "t": "IBM System/370", "tg": ["Historical", "IBM", "Mainframe"], "d": "hardware", "x": "Successor to the System/360 introduced in 1970 adding virtual memory support. Extended IBM mainframe dominance and...", "l": "i", "k": ["ibm", "system", "successor", "introduced", "adding", "virtual", "memory", "support", "extended", "mainframe", "dominance", "became", "foundation", "enterprise", "computing"]}, {"id": "term-ibm-truenorth", "t": "IBM TrueNorth", "tg": ["Neuromorphic", "IBM", "Research"], "d": "hardware", "x": "IBM neuromorphic chip containing 1 million programmable neurons and 256 million synapses consuming only 70 milliwatts....", "l": "i", "k": ["ibm", "truenorth", "neuromorphic", "chip", "containing", "million", "programmable", "neurons", "synapses", "consuming", "milliwatts", "demonstrated", "brain-inspired", "computing", "pattern"]}, {"id": "term-ibm-watson-jeopardy", "t": "IBM Watson Jeopardy", "tg": ["History", "Milestones"], "d": "history", "x": "IBM's Watson AI system that defeated human champions Ken Jennings and Brad Rutter on the quiz show Jeopardy! in...", "l": "i", "k": ["ibm", "watson", "jeopardy", "system", "defeated", "human", "champions", "ken", "jennings", "brad", "rutter", "quiz", "show", "february", "demonstrating"]}, {"id": "term-iccv", "t": "ICCV", "tg": ["History", "Conferences"], "d": "history", "x": "The International Conference on Computer Vision held biennially since 1987. One of the top three computer vision...", "l": "i", "k": ["iccv", "international", "conference", "computer", "vision", "held", "biennially", "top", "conferences", "alongside", "cvpr", "eccv", "seminal", "papers", "visual"]}, {"id": "term-iclr", "t": "ICLR", "tg": ["History", "Conferences"], "d": "history", "x": "The International Conference on Learning Representations first held in 2013 founded by Yoshua Bengio and Yann LeCun....", "l": "i", "k": ["iclr", "international", "conference", "learning", "representations", "held", "founded", "yoshua", "bengio", "yann", "lecun", "pioneered", "open", "peer", "review"]}, {"id": "term-icml", "t": "ICML", "tg": ["History", "Conferences"], "d": "history", "x": "The International Conference on Machine Learning first held in 1980. One of the top-tier conferences in AI and machine...", "l": "i", "k": ["icml", "international", "conference", "machine", "learning", "held", "top-tier", "conferences", "alongside", "neurips", "iclr", "organized", "society"]}, {"id": "term-iconqa", "t": "IconQA", "tg": ["Benchmark", "Multimodal", "Reasoning"], "d": "datasets", "x": "A diagram question answering benchmark testing abstract diagrammatic reasoning. Contains questions about icons and...", "l": "i", "k": ["iconqa", "diagram", "question", "answering", "benchmark", "testing", "abstract", "diagrammatic", "reasoning", "contains", "questions", "icons", "diagrams", "requiring", "spatial"]}, {"id": "term-idefics", "t": "Idefics", "tg": ["Models", "Technical"], "d": "models", "x": "An open-access multimodal model based on Flamingo's architecture that processes interleaved image and text inputs....", "l": "i", "k": ["idefics", "open-access", "multimodal", "model", "based", "flamingo", "architecture", "processes", "interleaved", "image", "text", "inputs", "trained", "publicly", "available"]}, {"id": "term-idefics2", "t": "Idefics2", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A second-generation open multimodal model from Hugging Face that processes interleaved image-text sequences for visual...", "l": "i", "k": ["idefics2", "second-generation", "open", "multimodal", "model", "hugging", "face", "processes", "interleaved", "image-text", "sequences", "visual", "question", "answering", "image"]}, {"id": "term-idefics3", "t": "Idefics3", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A third-generation multimodal model from Hugging Face with improved image understanding and more efficient processing...", "l": "i", "k": ["idefics3", "third-generation", "multimodal", "model", "hugging", "face", "improved", "image", "understanding", "efficient", "processing", "visual", "inputs", "alongside", "text"]}, {"id": "term-ideogram", "t": "Ideogram", "tg": ["Models", "Technical", "Vision", "Products"], "d": "models", "x": "A text-to-image generation model that excels at rendering accurate text within generated images through specialized...", "l": "i", "k": ["ideogram", "text-to-image", "generation", "model", "excels", "rendering", "accurate", "text", "within", "generated", "images", "specialized", "architecture", "components"]}, {"id": "term-idiom-detection", "t": "Idiom Detection", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of identifying non-compositional multi-word expressions whose meaning cannot be deduced from their individual...", "l": "i", "k": ["idiom", "detection", "task", "identifying", "non-compositional", "multi-word", "expressions", "whose", "meaning", "cannot", "deduced", "individual", "words", "kick", "bucket"]}, {"id": "term-ieee-organization", "t": "IEEE (Organization)", "tg": ["History", "Organizations"], "d": "history", "x": "The Institute of Electrical and Electronics Engineers founded in 1963. The world's largest professional technical...", "l": "i", "k": ["ieee", "organization", "institute", "electrical", "electronics", "engineers", "founded", "world", "largest", "professional", "technical", "electronic", "engineering", "publishes", "influential"]}, {"id": "term-ieee-ai-ethics-standards", "t": "IEEE AI Ethics Standards", "tg": ["Governance", "Regulation"], "d": "safety", "x": "A family of standards developed by IEEE under the Ethically Aligned Design initiative, including IEEE 7000 series...", "l": "i", "k": ["ieee", "ethics", "standards", "family", "developed", "ethically", "aligned", "design", "initiative", "including", "series", "addressing", "transparency", "data", "privacy"]}, {"id": "term-ifeval", "t": "IFEval", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Instruction Following Evaluation, a benchmark that tests language models' ability to follow specific verifiable...", "l": "i", "k": ["ifeval", "instruction", "following", "evaluation", "benchmark", "tests", "language", "models", "ability", "follow", "specific", "verifiable", "formatting", "instructions", "word"]}, {"id": "term-ijcai", "t": "IJCAI", "tg": ["History", "Conferences"], "d": "history", "x": "The International Joint Conference on Artificial Intelligence first held in 1969. The oldest and one of the most...", "l": "i", "k": ["ijcai", "international", "joint", "conference", "artificial", "intelligence", "held", "oldest", "prestigious", "conferences", "covering", "topics", "biennially", "annually"]}, {"id": "term-illiac-iv", "t": "ILLIAC IV", "tg": ["Historical", "Supercomputer", "Parallel"], "d": "hardware", "x": "Massively parallel computer built at the University of Illinois in the early 1970s. One of the first attempts at...", "l": "i", "k": ["illiac", "massively", "parallel", "computer", "built", "university", "illinois", "early", "1970s", "attempts", "large-scale", "processing", "elements"]}, {"id": "term-ilya-sutskever", "t": "Ilya Sutskever", "tg": ["History", "Pioneers"], "d": "history", "x": "Russian-born AI researcher who co-founded OpenAI, served as its Chief Scientist, and co-designed AlexNet. He co-founded...", "l": "i", "k": ["ilya", "sutskever", "russian-born", "researcher", "co-founded", "openai", "served", "chief", "scientist", "co-designed", "alexnet", "safe", "superintelligence", "inc", "focusing"]}, {"id": "term-image-augmentation", "t": "Image Augmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Techniques that artificially expand training datasets by applying random transformations to images, including rotation,...", "l": "i", "k": ["image", "augmentation", "techniques", "artificially", "expand", "training", "datasets", "applying", "random", "transformations", "images", "including", "rotation", "flipping", "cropping"]}, {"id": "term-image-classification", "t": "Image Classification", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The fundamental computer vision task of assigning a categorical label to an entire image based on its visual content,...", "l": "i", "k": ["image", "classification", "fundamental", "computer", "vision", "task", "assigning", "categorical", "label", "entire", "based", "visual", "content", "typically", "cnn"]}, {"id": "term-image-colorization", "t": "Image Colorization", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of automatically adding plausible colors to grayscale images using deep learning models that learn color...", "l": "i", "k": ["image", "colorization", "task", "automatically", "adding", "plausible", "colors", "grayscale", "images", "deep", "learning", "models", "learn", "color", "distributions"]}, {"id": "term-image-deblurring", "t": "Image Deblurring", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of recovering sharp images from blurred inputs caused by camera shake or object motion, using deep learning...", "l": "i", "k": ["image", "deblurring", "task", "recovering", "sharp", "images", "blurred", "inputs", "caused", "camera", "shake", "object", "motion", "deep", "learning"]}, {"id": "term-image-denoising", "t": "Image Denoising", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of removing noise from degraded images using neural networks that learn to separate signal from noise,...", "l": "i", "k": ["image", "denoising", "process", "removing", "noise", "degraded", "images", "neural", "networks", "learn", "separate", "signal", "producing", "cleaner", "preserving"]}, {"id": "term-image-embedding", "t": "Image Embedding", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A dense vector representation of an image produced by a neural network encoder, capturing semantic and visual features...", "l": "i", "k": ["image", "embedding", "dense", "vector", "representation", "produced", "neural", "network", "encoder", "capturing", "semantic", "visual", "features", "compact", "form"]}, {"id": "term-image-generation", "t": "Image Generation", "tg": ["Application", "Generative"], "d": "general", "x": "AI systems that create images from text descriptions or other inputs. Major models include DALL-E, Midjourney, and...", "l": "i", "k": ["image", "generation", "systems", "create", "images", "text", "descriptions", "inputs", "major", "models", "include", "dall-e", "midjourney", "stable", "diffusion"]}, {"id": "term-image-inpainting", "t": "Image Inpainting", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of filling in missing or masked regions of an image with plausible content, using deep learning models that...", "l": "i", "k": ["image", "inpainting", "task", "filling", "missing", "masked", "regions", "plausible", "content", "deep", "learning", "models", "understand", "context", "texture"]}, {"id": "term-image-inpainting-algorithm", "t": "Image Inpainting Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A technique for filling in missing or damaged regions of an image by propagating surrounding texture and structure...", "l": "i", "k": ["image", "inpainting", "algorithm", "technique", "filling", "missing", "damaged", "regions", "propagating", "surrounding", "texture", "structure", "information", "classical", "approaches"]}, {"id": "term-image-matting", "t": "Image Matting", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of estimating a precise alpha matte that defines the fractional opacity of foreground elements in an image,...", "l": "i", "k": ["image", "matting", "task", "estimating", "precise", "alpha", "matte", "defines", "fractional", "opacity", "foreground", "elements", "enabling", "accurate", "extraction"]}, {"id": "term-image-paragraph-captioning", "t": "Image Paragraph Captioning", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "A dataset requiring generation of detailed multi-sentence paragraph descriptions of images. Tests the ability to...", "l": "i", "k": ["image", "paragraph", "captioning", "dataset", "requiring", "generation", "detailed", "multi-sentence", "descriptions", "images", "tests", "ability", "produce", "rich", "extended"]}, {"id": "term-image-pyramid-algorithm", "t": "Image Pyramid Algorithm", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "A multi-scale representation of an image created by repeatedly smoothing and downsampling. Gaussian pyramids reduce...", "l": "i", "k": ["image", "pyramid", "algorithm", "multi-scale", "representation", "created", "repeatedly", "smoothing", "downsampling", "gaussian", "pyramids", "reduce", "resolution", "laplacian", "store"]}, {"id": "term-image-registration", "t": "Image Registration", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of aligning two or more images of the same scene taken at different times, viewpoints, or by different...", "l": "i", "k": ["image", "registration", "process", "aligning", "images", "scene", "taken", "different", "times", "viewpoints", "sensors", "computing", "spatial", "transformation", "maps"]}, {"id": "term-image-registration-algorithm", "t": "Image Registration Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A process that aligns two or more images of the same scene taken at different times or from different viewpoints. Uses...", "l": "i", "k": ["image", "registration", "algorithm", "process", "aligns", "images", "scene", "taken", "different", "times", "viewpoints", "uses", "feature", "matching", "intensity-based"]}, {"id": "term-image-retrieval", "t": "Image Retrieval", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of finding images in a database that are visually similar to a query image, using learned embeddings and...", "l": "i", "k": ["image", "retrieval", "task", "finding", "images", "database", "visually", "similar", "query", "learned", "embeddings", "nearest-neighbor", "search", "embedding", "space"]}, {"id": "term-image-stitching", "t": "Image Stitching", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of combining multiple overlapping photographs into a single panoramic or wide-field image by estimating...", "l": "i", "k": ["image", "stitching", "process", "combining", "multiple", "overlapping", "photographs", "single", "panoramic", "wide-field", "estimating", "homographies", "blending", "seams", "correcting"]}, {"id": "term-image-upscaling", "t": "Image Upscaling", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of increasing image resolution using AI models that synthesize realistic high-frequency details, producing...", "l": "i", "k": ["image", "upscaling", "process", "increasing", "resolution", "models", "synthesize", "realistic", "high-frequency", "details", "producing", "sharp", "detailed", "results", "superior"]}, {"id": "term-imagebind", "t": "ImageBind", "tg": ["Models", "Technical"], "d": "models", "x": "A model by Meta AI that learns a joint embedding space across six modalities including images text audio depth thermal...", "l": "i", "k": ["imagebind", "model", "meta", "learns", "joint", "embedding", "space", "across", "six", "modalities", "including", "images", "text", "audio", "depth"]}, {"id": "term-imagedream", "t": "ImageDream", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An image-conditioned multi-view diffusion model that generates 3D consistent views from a single reference image with...", "l": "i", "k": ["imagedream", "image-conditioned", "multi-view", "diffusion", "model", "generates", "consistent", "views", "single", "reference", "image", "text", "guidance"]}, {"id": "term-imagegpt", "t": "ImageGPT", "tg": ["Models", "Technical", "Vision", "History"], "d": "models", "x": "An OpenAI model that applies the GPT autoregressive framework to image pixels by treating images as sequences of color...", "l": "i", "k": ["imagegpt", "openai", "model", "applies", "gpt", "autoregressive", "framework", "image", "pixels", "treating", "images", "sequences", "color", "tokens", "generation"]}, {"id": "term-imagen", "t": "Imagen", "tg": ["Models", "Technical"], "d": "models", "x": "A text-to-image diffusion model by Google Brain that uses a large frozen text encoder and cascaded diffusion models....", "l": "i", "k": ["imagen", "text-to-image", "diffusion", "model", "google", "brain", "uses", "large", "frozen", "text", "encoder", "cascaded", "models", "demonstrated", "scaling"]}, {"id": "term-imagenet", "t": "ImageNet", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A large-scale visual database with over 14 million labeled images across thousands of categories, historically serving...", "l": "i", "k": ["imagenet", "large-scale", "visual", "database", "million", "labeled", "images", "across", "thousands", "categories", "historically", "serving", "primary", "benchmark", "dataset"]}, {"id": "term-imagenet-dataset", "t": "ImageNet Dataset", "tg": ["History", "Milestones"], "d": "history", "x": "A large-scale visual recognition dataset created by Fei-Fei Li and colleagues containing over 14 million labeled images...", "l": "i", "k": ["imagenet", "dataset", "large-scale", "visual", "recognition", "created", "fei-fei", "colleagues", "containing", "million", "labeled", "images", "across", "categories", "large"]}, {"id": "term-imagenet-large-scale-visual-recognition-challenge", "t": "ImageNet Large Scale Visual Recognition Challenge", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "An annual competition from 2010 to 2017 that measured progress in image classification and object detection on the...", "l": "i", "k": ["imagenet", "large", "scale", "visual", "recognition", "challenge", "annual", "competition", "measured", "progress", "image", "classification", "object", "detection", "dataset"]}, {"id": "term-imagenet-moment", "t": "ImageNet Moment", "tg": ["History", "Milestones"], "d": "history", "x": "The pivotal moment in 2012 when Alex Krizhevsky's deep convolutional neural network (AlexNet) won the ImageNet Large...", "l": "i", "k": ["imagenet", "moment", "pivotal", "alex", "krizhevsky", "deep", "convolutional", "neural", "network", "alexnet", "won", "large", "scale", "visual", "recognition"]}, {"id": "term-imagereward", "t": "ImageReward", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A dataset and reward model for text-to-image generation aligned with human preferences. Provides systematic evaluation...", "l": "i", "k": ["imagereward", "dataset", "reward", "model", "text-to-image", "generation", "aligned", "human", "preferences", "provides", "systematic", "evaluation", "image", "quality", "across"]}, {"id": "term-imdb-reviews", "t": "IMDB Reviews", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A dataset of 50000 movie reviews from the Internet Movie Database labeled as positive or negative. A standard benchmark...", "l": "i", "k": ["imdb", "reviews", "dataset", "movie", "internet", "database", "labeled", "positive", "negative", "standard", "benchmark", "binary", "sentiment", "classification", "nlp"]}, {"id": "term-img2img", "t": "Img2Img", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "An image-to-image generation pipeline that takes an existing image as input, adds noise to its latent representation,...", "l": "i", "k": ["img2img", "image-to-image", "generation", "pipeline", "takes", "existing", "image", "input", "adds", "noise", "latent", "representation", "denoises", "text", "prompt"]}, {"id": "term-imitation-learning", "t": "Imitation Learning", "tg": ["Reinforcement Learning", "Imitation"], "d": "general", "x": "A paradigm where an agent learns to perform tasks by observing expert demonstrations rather than through reward-based...", "l": "i", "k": ["imitation", "learning", "paradigm", "agent", "learns", "perform", "tasks", "observing", "expert", "demonstrations", "rather", "reward-based", "trial", "error", "includes"]}, {"id": "term-immersion-cooling", "t": "Immersion Cooling", "tg": ["Cooling", "Data Center"], "d": "hardware", "x": "Cooling technique where computing hardware is submerged in a thermally conductive dielectric liquid. Provides highly...", "l": "i", "k": ["immersion", "cooling", "technique", "computing", "hardware", "submerged", "thermally", "conductive", "dielectric", "liquid", "provides", "highly", "efficient", "dense", "enabling"]}, {"id": "term-impact-assessment", "t": "Impact Assessment", "tg": ["Safety", "Governance"], "d": "safety", "x": "A systematic process for evaluating the potential effects of an AI system on individuals communities and society before...", "l": "i", "k": ["impact", "assessment", "systematic", "process", "evaluating", "potential", "effects", "system", "individuals", "communities", "society", "deployment", "required", "regulations", "considered"]}, {"id": "term-impala-algorithm", "t": "IMPALA Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "Importance Weighted Actor-Learner Architecture is a distributed reinforcement learning framework that decouples acting...", "l": "i", "k": ["impala", "algorithm", "importance", "weighted", "actor-learner", "architecture", "distributed", "reinforcement", "learning", "framework", "decouples", "acting", "uses", "v-trace", "off-policy"]}, {"id": "term-implicit-differentiation", "t": "Implicit Differentiation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A technique for computing gradients through fixed-point iterations or optimization problems without unrolling the...", "l": "i", "k": ["implicit", "differentiation", "technique", "computing", "gradients", "fixed-point", "iterations", "optimization", "problems", "without", "unrolling", "computation", "graph", "enables", "memory-efficient"]}, {"id": "term-implicit-neural-representation", "t": "Implicit Neural Representation", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A neural network that learns a continuous function mapping coordinates to signal values, representing signals like...", "l": "i", "k": ["implicit", "neural", "representation", "network", "learns", "continuous", "function", "mapping", "coordinates", "signal", "values", "representing", "signals", "images", "shapes"]}, {"id": "term-implicit-q-learning", "t": "Implicit Q-Learning (IQL)", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An offline RL method that avoids querying out-of-distribution actions by learning a value function using expectile...", "l": "i", "k": ["implicit", "q-learning", "iql", "offline", "method", "avoids", "querying", "out-of-distribution", "actions", "learning", "value", "function", "expectile", "regression", "extracting"]}, {"id": "term-implicit-quantile-network", "t": "Implicit Quantile Network", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A distributional reinforcement learning method that learns a continuous mapping from quantile fractions to return...", "l": "i", "k": ["implicit", "quantile", "network", "distributional", "reinforcement", "learning", "method", "learns", "continuous", "mapping", "fractions", "return", "values", "enables", "flexible"]}, {"id": "term-importance-sampling", "t": "Importance Sampling", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A Monte Carlo technique that estimates properties of one distribution by sampling from a different, easier-to-sample...", "l": "i", "k": ["importance", "sampling", "monte", "carlo", "technique", "estimates", "properties", "distribution", "different", "easier-to-sample", "proposal", "reweighting", "samples", "likelihood", "ratio"]}, {"id": "term-importance-sampling-rl", "t": "Importance Sampling in RL", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A statistical technique used in off-policy RL to correct for the mismatch between the behavior policy that generated...", "l": "i", "k": ["importance", "sampling", "statistical", "technique", "off-policy", "correct", "mismatch", "behavior", "policy", "generated", "data", "target", "evaluated", "ratios", "reweight"]}, {"id": "term-impossibility-theorem-of-fairness", "t": "Impossibility Theorem of Fairness", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Mathematical results demonstrating that certain fairness criteria are mutually incompatible except in trivial cases,...", "l": "i", "k": ["impossibility", "theorem", "fairness", "mathematical", "results", "demonstrating", "certain", "criteria", "mutually", "incompatible", "except", "trivial", "cases", "meaning", "satisfying"]}, {"id": "term-imputation", "t": "Imputation", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "The process of replacing missing values in a dataset with estimated values, using methods such as mean, median, mode...", "l": "i", "k": ["imputation", "process", "replacing", "missing", "values", "dataset", "estimated", "methods", "mean", "median", "mode", "substitution", "k-nearest", "neighbors", "model-based"]}, {"id": "term-in-context-learning", "t": "In-Context Learning", "tg": ["Capability", "Prompting"], "d": "general", "x": "An LLM's ability to learn from examples provided in the prompt without updating its weights. Enables few-shot and...", "l": "i", "k": ["in-context", "learning", "llm", "ability", "learn", "examples", "provided", "prompt", "without", "updating", "weights", "enables", "few-shot", "zero-shot", "task"]}, {"id": "term-in-context-learning-discovery", "t": "In-Context Learning Discovery", "tg": ["History", "Milestones"], "d": "history", "x": "The finding that large language models can learn to perform new tasks by conditioning on a few examples provided in the...", "l": "i", "k": ["in-context", "learning", "discovery", "finding", "large", "language", "models", "learn", "perform", "tasks", "conditioning", "examples", "provided", "input", "context"]}, {"id": "term-in-memory-computing", "t": "In-Memory Computing", "tg": ["Emerging", "Architecture", "Memory"], "d": "hardware", "x": "Computing paradigm that performs calculations directly within memory arrays eliminating data movement between separate...", "l": "i", "k": ["in-memory", "computing", "paradigm", "performs", "calculations", "directly", "within", "memory", "arrays", "eliminating", "data", "movement", "separate", "processing", "units"]}, {"id": "term-in22", "t": "IN22", "tg": ["Benchmark", "NLP", "Multilingual", "Translation"], "d": "datasets", "x": "The IndicTrans2 evaluation benchmark covering 22 Indian languages for machine translation evaluation. Provides...", "l": "i", "k": ["in22", "indictrans2", "evaluation", "benchmark", "covering", "indian", "languages", "machine", "translation", "provides", "standardized", "comparison", "quality", "indic"]}, {"id": "term-inaturalist", "t": "iNaturalist", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A large-scale fine-grained species classification dataset from the iNaturalist citizen science platform. Contains...", "l": "i", "k": ["inaturalist", "large-scale", "fine-grained", "species", "classification", "dataset", "citizen", "science", "platform", "contains", "hundreds", "thousands", "images", "across", "plant"]}, {"id": "term-incentive-design-for-ai-safety", "t": "Incentive Design for AI Safety", "tg": ["Safety", "Policy"], "d": "safety", "x": "The design of economic regulatory and social incentives to encourage AI developers and deployers to invest in safety...", "l": "i", "k": ["incentive", "design", "safety", "economic", "regulatory", "social", "incentives", "encourage", "developers", "deployers", "invest", "measures", "addresses", "market", "failure"]}, {"id": "term-inception-network", "t": "Inception Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A CNN architecture that uses parallel convolutional filters of different sizes within the same layer (inception...", "l": "i", "k": ["inception", "network", "cnn", "architecture", "uses", "parallel", "convolutional", "filters", "different", "sizes", "within", "layer", "modules", "capture", "features"]}, {"id": "term-inception-score", "t": "Inception Score", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A metric for evaluating the quality and diversity of generated images. Measures how confidently an Inception network...", "l": "i", "k": ["inception", "score", "metric", "evaluating", "quality", "diversity", "generated", "images", "measures", "confidently", "network", "classifies", "diverse", "predicted", "labels"]}, {"id": "term-inception-v3", "t": "Inception v3", "tg": ["Models", "Technical"], "d": "models", "x": "A refined version of the GoogLeNet architecture with factorized convolutions batch normalization and label smoothing....", "l": "i", "k": ["inception", "refined", "version", "googlenet", "architecture", "factorized", "convolutions", "batch", "normalization", "label", "smoothing", "widely", "feature", "extractor", "serves"]}, {"id": "term-inception-v4", "t": "Inception v4", "tg": ["Models", "Technical"], "d": "models", "x": "An advanced version of the Inception architecture that combines Inception modules with residual connections. Achieves...", "l": "i", "k": ["inception", "advanced", "version", "architecture", "combines", "modules", "residual", "connections", "achieves", "improved", "accuracy", "deeper", "networks", "maintaining", "computational"]}, {"id": "term-incident-reporting-for-ai", "t": "Incident Reporting for AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "Mandatory or voluntary systems for reporting AI-related incidents accidents and near-misses. Enables pattern...", "l": "i", "k": ["incident", "reporting", "mandatory", "voluntary", "systems", "ai-related", "incidents", "accidents", "near-misses", "enables", "pattern", "recognition", "systemic", "improvement", "analogous"]}, {"id": "term-inclusive-ai", "t": "Inclusive AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The practice of designing AI systems that work equitably for diverse populations including underrepresented groups...", "l": "i", "k": ["inclusive", "practice", "designing", "systems", "work", "equitably", "diverse", "populations", "including", "underrepresented", "groups", "people", "disabilities", "speakers", "low-resource"]}, {"id": "term-incoder", "t": "InCoder", "tg": ["Models", "Technical"], "d": "models", "x": "A unified generative model for code that supports both code generation and infilling through a causal masking training...", "l": "i", "k": ["incoder", "unified", "generative", "model", "code", "supports", "generation", "infilling", "causal", "masking", "training", "objective", "generate", "left-to-right", "fill"]}, {"id": "term-incomplete-lu-factorization", "t": "Incomplete LU Factorization", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An approximate LU decomposition used as a preconditioner for iterative linear solvers. Drops small fill-in elements to...", "l": "i", "k": ["incomplete", "factorization", "approximate", "decomposition", "preconditioner", "iterative", "linear", "solvers", "drops", "small", "fill-in", "elements", "maintain", "sparsity", "providing"]}, {"id": "term-incremental-indexing", "t": "Incremental Indexing", "tg": ["Vector Database", "Maintenance"], "d": "general", "x": "A vector database operation that adds new vectors to an existing index without requiring a full rebuild, enabling...", "l": "i", "k": ["incremental", "indexing", "vector", "database", "operation", "adds", "vectors", "existing", "index", "without", "requiring", "full", "rebuild", "enabling", "near-real-time"]}, {"id": "term-incremental-pca-algorithm", "t": "Incremental PCA Algorithm", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A variant of PCA that processes data in mini-batches enabling application to datasets too large for memory. Updates the...", "l": "i", "k": ["incremental", "pca", "algorithm", "variant", "processes", "data", "mini-batches", "enabling", "application", "datasets", "large", "memory", "updates", "principal", "components"]}, {"id": "term-independent-component-analysis", "t": "Independent Component Analysis", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "A computational method for separating a multivariate signal into additive, statistically independent non-Gaussian...", "l": "i", "k": ["independent", "component", "analysis", "computational", "method", "separating", "multivariate", "signal", "additive", "statistically", "non-gaussian", "source", "components", "widely", "blind"]}, {"id": "term-independent-q-learning", "t": "Independent Q-Learning", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A multi-agent reinforcement learning approach where each agent independently learns its own Q-function treating other...", "l": "i", "k": ["independent", "q-learning", "multi-agent", "reinforcement", "learning", "approach", "agent", "independently", "learns", "q-function", "treating", "agents", "part", "environment", "simple"]}, {"id": "term-iql-marl", "t": "Independent Q-Learning (IQL-MARL)", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A simple multi-agent RL approach where each agent independently learns its own Q-function treating other agents as part...", "l": "i", "k": ["independent", "q-learning", "iql-marl", "simple", "multi-agent", "approach", "agent", "independently", "learns", "q-function", "treating", "agents", "part", "environment", "despite"]}, {"id": "term-index-optimization", "t": "Index Optimization", "tg": ["Vector Database", "Performance"], "d": "general", "x": "The process of tuning vector index parameters such as the number of clusters, graph connectivity, and quantization...", "l": "i", "k": ["index", "optimization", "process", "tuning", "vector", "parameters", "number", "clusters", "graph", "connectivity", "quantization", "settings", "achieve", "optimal", "balance"]}, {"id": "term-index-refresh", "t": "Index Refresh", "tg": ["Vector Database", "Maintenance"], "d": "general", "x": "The process of rebuilding or updating a vector index to incorporate new vectors, remove deleted ones, and optimize...", "l": "i", "k": ["index", "refresh", "process", "rebuilding", "updating", "vector", "incorporate", "vectors", "remove", "deleted", "ones", "optimize", "search", "structures", "necessary"]}, {"id": "term-indicnlpsuite", "t": "IndicNLPSuite", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "A comprehensive NLP benchmark for 11 Indian languages covering text classification sentiment analysis and question...", "l": "i", "k": ["indicnlpsuite", "comprehensive", "nlp", "benchmark", "indian", "languages", "covering", "text", "classification", "sentiment", "analysis", "question", "answering", "advances", "research"]}, {"id": "term-indigenous-data-sovereignty", "t": "Indigenous Data Sovereignty", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The right of indigenous peoples to govern the collection ownership and application of data about their communities and...", "l": "i", "k": ["indigenous", "data", "sovereignty", "right", "peoples", "govern", "collection", "ownership", "application", "communities", "territories", "particularly", "relevant", "training", "includes"]}, {"id": "term-individual-conditional-expectation", "t": "Individual Conditional Expectation", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A plot showing how the prediction for each individual instance changes as a feature varies, disaggregating the partial...", "l": "i", "k": ["individual", "conditional", "expectation", "plot", "showing", "prediction", "instance", "changes", "feature", "varies", "disaggregating", "partial", "dependence", "reveal", "heterogeneous"]}, {"id": "term-individual-fairness", "t": "Individual Fairness", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness principle requiring that similar individuals receive similar predictions or outcomes, formalized as a...", "l": "i", "k": ["individual", "fairness", "principle", "requiring", "similar", "individuals", "receive", "predictions", "outcomes", "formalized", "lipschitz", "condition", "close", "task-relevant", "metric"]}, {"id": "term-induction-head", "t": "Induction Head", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A pair of attention heads in transformers that work together to identify and continue repeated patterns in the input...", "l": "i", "k": ["induction", "head", "pair", "attention", "heads", "transformers", "work", "together", "identify", "continue", "repeated", "patterns", "input", "sequence", "forming"]}, {"id": "term-inference", "t": "Inference", "tg": ["Process", "Production"], "d": "general", "x": "Running a trained model to generate predictions or outputs on new data. Distinguished from training, inference is...", "l": "i", "k": ["inference", "running", "trained", "model", "generate", "predictions", "outputs", "data", "distinguished", "training", "typically", "faster", "less", "resource-intensive"]}, {"id": "term-inference-acceleration", "t": "Inference Acceleration", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The collection of hardware and software techniques that speed up neural network inference, including specialized...", "l": "i", "k": ["inference", "acceleration", "collection", "hardware", "software", "techniques", "speed", "neural", "network", "including", "specialized", "accelerators", "compiler", "optimizations", "quantization"]}, {"id": "term-inference-cost-optimization", "t": "Inference Cost Optimization", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Strategies for minimizing the financial cost of serving AI models at scale, including quantization, distillation,...", "l": "i", "k": ["inference", "cost", "optimization", "strategies", "minimizing", "financial", "serving", "models", "scale", "including", "quantization", "distillation", "batching", "hardware", "selection"]}, {"id": "term-inference-latency", "t": "Inference Latency", "tg": ["Inference", "Performance", "Metric"], "d": "hardware", "x": "Total time to generate a complete response from an AI model after receiving input. Includes network transfer...", "l": "i", "k": ["inference", "latency", "total", "time", "generate", "complete", "response", "model", "receiving", "input", "includes", "network", "transfer", "preprocessing", "execution"]}, {"id": "term-inference-optimization", "t": "Inference Optimization", "tg": ["LLM", "Inference"], "d": "models", "x": "A collection of techniques that reduce the computational cost, memory footprint, and latency of running trained models...", "l": "i", "k": ["inference", "optimization", "collection", "techniques", "reduce", "computational", "cost", "memory", "footprint", "latency", "running", "trained", "models", "production", "including"]}, {"id": "term-inference-throughput", "t": "Inference Throughput", "tg": ["Inference", "Performance", "Metric"], "d": "hardware", "x": "Number of inference requests a system can handle per unit time. Measured in queries per second and is the primary...", "l": "i", "k": ["inference", "throughput", "number", "requests", "system", "handle", "per", "unit", "time", "measured", "queries", "primary", "metric", "evaluating", "serving"]}, {"id": "term-infiniband", "t": "InfiniBand", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "A high-speed networking technology widely used in AI supercomputer clusters for inter-node communication, offering low...", "l": "i", "k": ["infiniband", "high-speed", "networking", "technology", "widely", "supercomputer", "clusters", "inter-node", "communication", "offering", "low", "latency", "high", "bandwidth", "per"]}, {"id": "term-infiniband-switch", "t": "InfiniBand Switch", "tg": ["Networking", "InfiniBand", "Hardware"], "d": "hardware", "x": "Network switch providing InfiniBand connectivity between nodes in an AI training cluster. NVIDIA Quantum switches...", "l": "i", "k": ["infiniband", "switch", "network", "providing", "connectivity", "nodes", "training", "cluster", "nvidia", "quantum", "switches", "deliver", "per", "port", "low-latency"]}, {"id": "term-infinitebench", "t": "InfiniteBench", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating language models on contexts exceeding 100000 tokens. Tests comprehension retrieval and...", "l": "i", "k": ["infinitebench", "benchmark", "evaluating", "language", "models", "contexts", "exceeding", "tokens", "tests", "comprehension", "retrieval", "reasoning", "across", "extremely", "long"]}, {"id": "term-infonce-loss", "t": "InfoNCE Loss", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Information Noise-Contrastive Estimation is a contrastive loss function that trains models to distinguish positive...", "l": "i", "k": ["infonce", "loss", "information", "noise-contrastive", "estimation", "contrastive", "function", "trains", "models", "distinguish", "positive", "pairs", "negative", "forms", "basis"]}, {"id": "term-information-bottleneck", "t": "Information Bottleneck", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A theoretical framework that characterizes the optimal tradeoff between compression and prediction. A representation...", "l": "i", "k": ["information", "bottleneck", "theoretical", "framework", "characterizes", "optimal", "tradeoff", "compression", "prediction", "representation", "retain", "little", "input", "possible", "preserving"]}, {"id": "term-information-extraction", "t": "Information Extraction", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of automatically extracting structured information such as entities, relations, and events from unstructured...", "l": "i", "k": ["information", "extraction", "task", "automatically", "extracting", "structured", "entities", "relations", "events", "unstructured", "text", "converting", "free-form", "organized", "knowledge"]}, {"id": "term-information-gain", "t": "Information Gain", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "The reduction in entropy achieved by splitting a dataset on a particular feature. Decision tree algorithms like ID3 and...", "l": "i", "k": ["information", "gain", "reduction", "entropy", "achieved", "splitting", "dataset", "particular", "feature", "decision", "tree", "algorithms", "id3", "select", "provides"]}, {"id": "term-information-hazard", "t": "Information Hazard", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Knowledge or information that could cause harm if widely disseminated. In AI includes detailed descriptions of attack...", "l": "i", "k": ["information", "hazard", "knowledge", "cause", "harm", "widely", "disseminated", "includes", "detailed", "descriptions", "attack", "methods", "capability", "evaluations", "dangerous"]}, {"id": "term-information-theory", "t": "Information Theory", "tg": ["History", "Fundamentals"], "d": "history", "x": "A mathematical framework developed by Claude Shannon in 1948 for quantifying information. Key concepts including...", "l": "i", "k": ["information", "theory", "mathematical", "framework", "developed", "claude", "shannon", "quantifying", "key", "concepts", "including", "entropy", "mutual", "channel", "capacity"]}, {"id": "term-information-theoretic-clustering", "t": "Information-Theoretic Clustering", "tg": ["Algorithms", "Technical", "Clustering", "Information Theory"], "d": "algorithms", "x": "A clustering approach that uses information-theoretic measures like mutual information to group data points. Maximizes...", "l": "i", "k": ["information-theoretic", "clustering", "approach", "uses", "measures", "mutual", "information", "group", "data", "points", "maximizes", "cluster", "labels", "convey", "original"]}, {"id": "term-informative-prior", "t": "Informative Prior", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A prior distribution that encodes specific prior knowledge or strong beliefs about a parameter's likely values,...", "l": "i", "k": ["informative", "prior", "distribution", "encodes", "specific", "knowledge", "strong", "beliefs", "parameter", "likely", "values", "substantially", "influencing", "posterior", "especially"]}, {"id": "term-informed-autonomy", "t": "Informed Autonomy", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The principle that individuals should have sufficient understanding of AI systems to make autonomous choices about...", "l": "i", "k": ["informed", "autonomy", "principle", "individuals", "sufficient", "understanding", "systems", "autonomous", "choices", "goes", "beyond", "consent", "encompass", "ongoing", "awareness"]}, {"id": "term-informed-consent-in-ai", "t": "Informed Consent in AI", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "The ethical and legal requirement that individuals be clearly informed about how their data will be collected,...", "l": "i", "k": ["informed", "consent", "ethical", "legal", "requirement", "individuals", "clearly", "data", "collected", "processed", "systems", "voluntarily", "agree"]}, {"id": "term-informer", "t": "Informer", "tg": ["Models", "Technical"], "d": "models", "x": "A Transformer-based time series forecasting model that uses ProbSparse self-attention and distilling operations to...", "l": "i", "k": ["informer", "transformer-based", "time", "series", "forecasting", "model", "uses", "probsparse", "self-attention", "distilling", "operations", "efficiently", "handle", "long", "input"]}, {"id": "term-infovqa", "t": "InfoVQA", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "Infographic VQA a dataset of questions about infographic images requiring reasoning over complex visual layouts. Tests...", "l": "i", "k": ["infovqa", "infographic", "vqa", "dataset", "questions", "images", "requiring", "reasoning", "complex", "visual", "layouts", "tests", "understanding", "charts", "diagrams"]}, {"id": "term-infoxlm", "t": "InfoXLM", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A cross-lingual pre-trained model that combines masked language modeling with cross-lingual contrastive learning and...", "l": "i", "k": ["infoxlm", "cross-lingual", "pre-trained", "model", "combines", "masked", "language", "modeling", "contrastive", "learning", "translation", "objectives"]}, {"id": "term-inner-alignment", "t": "Inner Alignment", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The problem of ensuring that a learned model's internal optimization objective matches the objective specified by the...", "l": "i", "k": ["inner", "alignment", "problem", "ensuring", "learned", "model", "internal", "optimization", "objective", "matches", "specified", "training", "process", "failure", "results"]}, {"id": "term-inner-monologue", "t": "Inner Monologue", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "A robotic planning framework that uses large language models for closed-loop reasoning by incorporating environment...", "l": "i", "k": ["inner", "monologue", "robotic", "planning", "framework", "uses", "large", "language", "models", "closed-loop", "reasoning", "incorporating", "environment", "feedback", "replan"]}, {"id": "term-inpainting-pipeline", "t": "Inpainting Pipeline", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A diffusion model workflow that regenerates only the masked portions of an image while maintaining consistency with the...", "l": "i", "k": ["inpainting", "pipeline", "diffusion", "model", "workflow", "regenerates", "masked", "portions", "image", "maintaining", "consistency", "unmasked", "regions", "object", "removal"]}, {"id": "term-input-validation-for-ai", "t": "Input Validation for AI", "tg": ["Safety", "Technical"], "d": "safety", "x": "Techniques for checking that inputs to an AI system fall within expected parameters before processing. A first line of...", "l": "i", "k": ["input", "validation", "techniques", "checking", "inputs", "system", "fall", "within", "expected", "parameters", "processing", "line", "defense", "against", "adversarial"]}, {"id": "term-insertion-sort", "t": "Insertion Sort", "tg": ["Algorithms", "Fundamentals", "Sorting"], "d": "algorithms", "x": "A simple sorting algorithm that builds the sorted array one element at a time by inserting each element into its...", "l": "i", "k": ["insertion", "sort", "simple", "sorting", "algorithm", "builds", "sorted", "array", "element", "time", "inserting", "correct", "position", "runs", "worst"]}, {"id": "term-inside-outside-algorithm", "t": "Inside-Outside Algorithm", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "A generalization of the forward-backward algorithm for probabilistic context-free grammars. Computes inside and outside...", "l": "i", "k": ["inside-outside", "algorithm", "generalization", "forward-backward", "probabilistic", "context-free", "grammars", "computes", "inside", "outside", "probabilities", "parsing", "parameter", "estimation", "via"]}, {"id": "term-instaflow", "t": "InstaFlow", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A text-to-image generation method that produces high-quality images in a single step by using rectified flow with...", "l": "i", "k": ["instaflow", "text-to-image", "generation", "method", "produces", "high-quality", "images", "single", "step", "rectified", "flow", "learned", "transport", "fast"]}, {"id": "term-instance-normalization", "t": "Instance Normalization", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A normalization technique that normalizes each feature channel independently for each sample, originally developed for...", "l": "i", "k": ["instance", "normalization", "technique", "normalizes", "feature", "channel", "independently", "sample", "originally", "developed", "neural", "style", "transfer", "per-instance", "statistics"]}, {"id": "term-instance-segmentation", "t": "Instance Segmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A computer vision task that detects individual objects in an image and generates a pixel-level mask for each instance,...", "l": "i", "k": ["instance", "segmentation", "computer", "vision", "task", "detects", "individual", "objects", "image", "generates", "pixel-level", "mask", "combining", "object", "detection"]}, {"id": "term-instant-ngp", "t": "Instant NGP", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "Instant Neural Graphics Primitives, a technique using multi-resolution hash encoding that dramatically accelerates NeRF...", "l": "i", "k": ["instant", "ngp", "neural", "graphics", "primitives", "technique", "multi-resolution", "hash", "encoding", "dramatically", "accelerates", "nerf", "training", "hours", "seconds"]}, {"id": "term-instant3d", "t": "Instant3D", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A text-to-3D generation framework that creates 3D assets in under one minute using a feed-forward approach instead of...", "l": "i", "k": ["instant3d", "text-to-3d", "generation", "framework", "creates", "assets", "minute", "feed-forward", "approach", "instead", "per-prompt", "optimization"]}, {"id": "term-instantid", "t": "InstantID", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A zero-shot identity-preserving image generation framework that maintains facial identity in generated images using a...", "l": "i", "k": ["instantid", "zero-shot", "identity-preserving", "image", "generation", "framework", "maintains", "facial", "identity", "generated", "images", "simple", "plug-and-play", "approach"]}, {"id": "term-instantmesh", "t": "InstantMesh", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A feed-forward framework for instant 3D mesh generation from a single image using multi-view diffusion and a...", "l": "i", "k": ["instantmesh", "feed-forward", "framework", "instant", "mesh", "generation", "single", "image", "multi-view", "diffusion", "sparse-view", "reconstruction", "network"]}, {"id": "term-instruct-model", "t": "Instruct Model", "tg": ["Model Type", "Training"], "d": "models", "x": "An LLM fine-tuned to follow instructions rather than just complete text. Makes models more useful as assistants by...", "l": "i", "k": ["instruct", "model", "llm", "fine-tuned", "follow", "instructions", "rather", "complete", "text", "makes", "models", "useful", "assistants", "teaching", "respond"]}, {"id": "term-instructgpt", "t": "InstructGPT", "tg": ["History", "Milestones"], "d": "history", "x": "An OpenAI model published in 2022 that used reinforcement learning from human feedback to align GPT-3 with user...", "l": "i", "k": ["instructgpt", "openai", "model", "published", "reinforcement", "learning", "human", "feedback", "align", "gpt-3", "user", "instructions", "directly", "preceding", "informing"]}, {"id": "term-instruction-following", "t": "Instruction Following", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The ability of a language model to accurately interpret and execute natural language instructions, a capability...", "l": "i", "k": ["instruction", "following", "ability", "language", "model", "accurately", "interpret", "execute", "natural", "instructions", "capability", "developed", "tuning", "reinforcement", "learning"]}, {"id": "term-instruction-hierarchy", "t": "Instruction Hierarchy", "tg": ["Prompt Engineering", "Safety"], "d": "safety", "x": "A structured approach to organizing prompt instructions by priority level, where system-level instructions take...", "l": "i", "k": ["instruction", "hierarchy", "structured", "approach", "organizing", "prompt", "instructions", "priority", "level", "system-level", "take", "precedence", "user-level", "enabling", "models"]}, {"id": "term-instruction-set-architecture", "t": "Instruction Set Architecture", "tg": ["Architecture", "Fundamentals"], "d": "hardware", "x": "The abstract interface between hardware and software defining the instructions a processor can execute. Major ISAs...", "l": "i", "k": ["instruction", "architecture", "abstract", "interface", "hardware", "software", "defining", "instructions", "processor", "execute", "major", "isas", "include", "x86", "arm"]}, {"id": "term-instruction-tuning", "t": "Instruction Tuning", "tg": ["Training", "Alignment"], "d": "safety", "x": "Fine-tuning LLMs on datasets of instructions and responses to improve their ability to follow user requests. A key...", "l": "i", "k": ["instruction", "tuning", "fine-tuning", "llms", "datasets", "instructions", "responses", "improve", "ability", "follow", "user", "requests", "key", "technique", "creating"]}, {"id": "term-instruction-tuning-alignment", "t": "Instruction Tuning Alignment", "tg": ["Prompt Engineering", "Optimization"], "d": "algorithms", "x": "The practice of writing prompts that align with the specific instruction format and conventions used during a model's...", "l": "i", "k": ["instruction", "tuning", "alignment", "practice", "writing", "prompts", "align", "specific", "format", "conventions", "model", "fine-tuning", "phase", "maximizing", "benefit"]}, {"id": "term-instruction-based-prompting", "t": "Instruction-Based Prompting", "tg": ["Prompt Engineering", "Fundamentals"], "d": "general", "x": "A prompting paradigm that provides explicit, imperative instructions to a language model describing the task to...", "l": "i", "k": ["instruction-based", "prompting", "paradigm", "provides", "explicit", "imperative", "instructions", "language", "model", "describing", "task", "perform", "leveraging", "instruction-tuned", "models"]}, {"id": "term-instructions-per-cycle", "t": "Instructions Per Cycle", "tg": ["Architecture", "Performance", "Metric"], "d": "hardware", "x": "Measure of processor efficiency indicating how many instructions complete in each clock cycle on average. Modern...", "l": "i", "k": ["instructions", "per", "cycle", "measure", "processor", "efficiency", "indicating", "complete", "clock", "average", "modern", "out-of-order", "processors", "achieve", "ipc"]}, {"id": "term-instructor-embedding", "t": "Instructor Embedding", "tg": ["Models", "Technical", "Embedding", "NLP"], "d": "models", "x": "A text embedding model that generates task-specific embeddings by prepending natural language instructions to input...", "l": "i", "k": ["instructor", "embedding", "text", "model", "generates", "task-specific", "embeddings", "prepending", "natural", "language", "instructions", "input", "customized", "representations"]}, {"id": "term-instructpix2pix", "t": "InstructPix2Pix", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "An image editing model that follows text instructions to modify images by training on a dataset of editing examples...", "l": "i", "k": ["instructpix2pix", "image", "editing", "model", "follows", "text", "instructions", "modify", "images", "training", "dataset", "examples", "generated", "gpt-3", "stable"]}, {"id": "term-instrumental-convergence", "t": "Instrumental Convergence", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The thesis that sufficiently advanced AI agents with a wide range of terminal goals will converge on pursuing certain...", "l": "i", "k": ["instrumental", "convergence", "thesis", "sufficiently", "advanced", "agents", "wide", "range", "terminal", "goals", "converge", "pursuing", "certain", "sub-goals", "self-preservation"]}, {"id": "term-instrumental-variable", "t": "Instrumental Variable", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A variable used in causal inference that is correlated with the treatment variable but affects the outcome only through...", "l": "i", "k": ["instrumental", "variable", "causal", "inference", "correlated", "treatment", "affects", "outcome", "enabling", "consistent", "estimation", "effects", "presence", "confounding"]}, {"id": "term-instrumental-variable-estimation-algorithm", "t": "Instrumental Variable Estimation Algorithm", "tg": ["Algorithms", "Technical", "Causal"], "d": "algorithms", "x": "A causal inference technique that uses an instrument variable correlated with the treatment but not directly with the...", "l": "i", "k": ["instrumental", "variable", "estimation", "algorithm", "causal", "inference", "technique", "uses", "instrument", "correlated", "treatment", "directly", "outcome", "estimate", "effects"]}, {"id": "term-int4", "t": "INT4 Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "An aggressive quantization scheme using only 4 bits per weight, achieving 8x compression over FP32. INT4 methods like...", "l": "i", "k": ["int4", "quantization", "aggressive", "scheme", "bits", "per", "weight", "achieving", "compression", "fp32", "methods", "gptq", "awq", "sophisticated", "calibration"]}, {"id": "term-int8", "t": "INT8 Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "The process of representing neural network weights and activations using 8-bit integers instead of floating-point,...", "l": "i", "k": ["int8", "quantization", "process", "representing", "neural", "network", "weights", "activations", "8-bit", "integers", "instead", "floating-point", "reducing", "memory", "compared"]}, {"id": "term-integral-image-algorithm", "t": "Integral Image Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A preprocessing technique that allows rapid calculation of pixel sums over rectangular regions in constant time. Each...", "l": "i", "k": ["integral", "image", "algorithm", "preprocessing", "technique", "allows", "rapid", "calculation", "pixel", "sums", "rectangular", "regions", "constant", "time", "stores"]}, {"id": "term-integrated-circuit", "t": "Integrated Circuit", "tg": ["Historical", "Fundamentals", "Component"], "d": "hardware", "x": "Electronic circuit with multiple components fabricated on a single semiconductor die. Invented independently by Jack...", "l": "i", "k": ["integrated", "circuit", "electronic", "multiple", "components", "fabricated", "single", "semiconductor", "die", "invented", "independently", "jack", "kilby", "robert", "noyce"]}, {"id": "term-integrated-gradients", "t": "Integrated Gradients", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An attribution method that computes feature importance by integrating gradients along a straight path from a baseline...", "l": "i", "k": ["integrated", "gradients", "attribution", "method", "computes", "feature", "importance", "integrating", "along", "straight", "path", "baseline", "actual", "input", "satisfies"]}, {"id": "term-intel-4004", "t": "Intel 4004", "tg": ["Historical", "Intel", "Pioneer"], "d": "hardware", "x": "First commercial microprocessor introduced by Intel in 1971 with 2300 transistors in a 10-micron process. Launched the...", "l": "i", "k": ["intel", "commercial", "microprocessor", "introduced", "transistors", "10-micron", "process", "launched", "revolution", "eventually", "led", "modern", "hardware"]}, {"id": "term-intel-8080", "t": "Intel 8080", "tg": ["Historical", "Intel", "Processor"], "d": "hardware", "x": "Intel 8-bit microprocessor from 1974 that became the basis for the first personal computers. The Altair 8800 used the...", "l": "i", "k": ["intel", "8-bit", "microprocessor", "became", "basis", "personal", "computers", "altair", "sparking", "computer", "industry"]}, {"id": "term-intel-agilex-fpga", "t": "Intel Agilex FPGA", "tg": ["FPGA", "Intel", "Data Center"], "d": "hardware", "x": "Intel high-performance FPGA family designed for data center and networking applications. Includes AI tensor blocks for...", "l": "i", "k": ["intel", "agilex", "fpga", "high-performance", "family", "designed", "data", "center", "networking", "applications", "includes", "tensor", "blocks", "accelerating", "neural"]}, {"id": "term-intel-amx", "t": "Intel AMX", "tg": ["Processor", "Intel", "Acceleration"], "d": "hardware", "x": "Intel Advanced Matrix Extensions adding matrix multiplication acceleration to Xeon Scalable processors. Provides BF16...", "l": "i", "k": ["intel", "amx", "advanced", "matrix", "extensions", "adding", "multiplication", "acceleration", "xeon", "scalable", "processors", "provides", "bf16", "int8", "operations"]}, {"id": "term-intel-arc-gpu", "t": "Intel Arc GPU", "tg": ["GPU", "Intel", "Consumer"], "d": "hardware", "x": "Intel consumer discrete GPU line based on Xe HPG architecture. While primarily gaming-focused the Arc series supports...", "l": "i", "k": ["intel", "arc", "gpu", "consumer", "discrete", "line", "based", "hpg", "architecture", "primarily", "gaming-focused", "series", "supports", "inference", "xmx"]}, {"id": "term-intel-data-center-gpu-max", "t": "Intel Data Center GPU Max", "tg": ["GPU", "Intel", "Data Center"], "d": "hardware", "x": "Intel discrete GPU for data center AI and HPC based on the Xe HPC architecture with up to 128GB HBM2e. Intel attempt to...", "l": "i", "k": ["intel", "data", "center", "gpu", "max", "discrete", "hpc", "based", "architecture", "128gb", "hbm2e", "attempt", "compete", "nvidia", "amd"]}, {"id": "term-intel-emerald-rapids", "t": "Intel Emerald Rapids", "tg": ["Processor", "Intel", "Server"], "d": "hardware", "x": "Intel fifth-generation Xeon Scalable server processor offering improved performance per watt over Sapphire Rapids. Used...", "l": "i", "k": ["intel", "emerald", "rapids", "fifth-generation", "xeon", "scalable", "server", "processor", "offering", "improved", "performance", "per", "watt", "sapphire", "cpu"]}, {"id": "term-intel-foundry-services", "t": "Intel Foundry Services", "tg": ["Fabrication", "Foundry", "Intel"], "d": "hardware", "x": "Intel contract chip manufacturing business opened to external customers. Aims to compete with TSMC and Samsung in...", "l": "i", "k": ["intel", "foundry", "services", "contract", "chip", "manufacturing", "business", "opened", "external", "customers", "aims", "compete", "tsmc", "samsung", "fabricating"]}, {"id": "term-intel-gaudi-3", "t": "Intel Gaudi 3", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "Intel's third-generation AI training accelerator featuring integrated 400GbE networking and high memory bandwidth....", "l": "i", "k": ["intel", "gaudi", "third-generation", "training", "accelerator", "featuring", "integrated", "400gbe", "networking", "high", "memory", "bandwidth", "targets", "competitive", "market"]}, {"id": "term-intel-loihi", "t": "Intel Loihi", "tg": ["Neuromorphic", "Intel", "Research"], "d": "hardware", "x": "Intel neuromorphic research chip implementing spiking neural networks with 128 neuromorphic cores and 130000 artificial...", "l": "i", "k": ["intel", "loihi", "neuromorphic", "research", "chip", "implementing", "spiking", "neural", "networks", "cores", "artificial", "neurons", "designed", "brain-inspired", "computing"]}, {"id": "term-intel-loihi-2", "t": "Intel Loihi 2", "tg": ["Neuromorphic", "Intel", "Research"], "d": "hardware", "x": "Second generation Intel neuromorphic research chip with improved performance and programmability. Contains up to 1...", "l": "i", "k": ["intel", "loihi", "generation", "neuromorphic", "research", "chip", "improved", "performance", "programmability", "contains", "million", "neurons", "supports", "complex", "spiking"]}, {"id": "term-intel-movidius", "t": "Intel Movidius", "tg": ["Accelerator", "Intel", "Edge"], "d": "hardware", "x": "Intel low-power vision processing unit line designed for computer vision inference at the edge. The Myriad X chip...", "l": "i", "k": ["intel", "movidius", "low-power", "vision", "processing", "unit", "line", "designed", "computer", "inference", "edge", "myriad", "chip", "included", "neural"]}, {"id": "term-intel-nervana-nnp", "t": "Intel Nervana NNP", "tg": ["Accelerator", "Intel", "Discontinued"], "d": "hardware", "x": "Intel neural network processor line including the NNP-T for training and NNP-I for inference. Was later discontinued in...", "l": "i", "k": ["intel", "nervana", "nnp", "neural", "network", "processor", "line", "including", "nnp-t", "training", "nnp-i", "inference", "later", "discontinued", "favor"]}, {"id": "term-intel-neural-compute-stick", "t": "Intel Neural Compute Stick", "tg": ["Edge", "Intel", "Inference"], "d": "hardware", "x": "USB-based neural network inference accelerator from Intel using the Movidius VPU. Provided an accessible way to add AI...", "l": "i", "k": ["intel", "neural", "compute", "stick", "usb-based", "network", "inference", "accelerator", "movidius", "vpu", "provided", "accessible", "add", "capability", "computer"]}, {"id": "term-intel-pentium", "t": "Intel Pentium", "tg": ["Historical", "Intel", "Processor"], "d": "hardware", "x": "Intel fifth-generation x86 processor line introduced in 1993 featuring superscalar architecture. Dominated personal...", "l": "i", "k": ["intel", "pentium", "fifth-generation", "x86", "processor", "line", "introduced", "featuring", "superscalar", "architecture", "dominated", "personal", "computing", "established", "brand"]}, {"id": "term-intel-sapphire-rapids", "t": "Intel Sapphire Rapids", "tg": ["Processor", "Intel", "Server"], "d": "hardware", "x": "Intel fourth-generation Xeon Scalable server processor featuring built-in AI acceleration through AMX instructions....", "l": "i", "k": ["intel", "sapphire", "rapids", "fourth-generation", "xeon", "scalable", "server", "processor", "featuring", "built-in", "acceleration", "amx", "instructions", "includes", "hbm2e"]}, {"id": "term-intel-xe-architecture", "t": "Intel Xe Architecture", "tg": ["GPU", "Intel", "Architecture"], "d": "hardware", "x": "Intel GPU architecture family spanning from integrated graphics to data center accelerators. Includes Xe LP for mobile...", "l": "i", "k": ["intel", "architecture", "gpu", "family", "spanning", "integrated", "graphics", "data", "center", "accelerators", "includes", "mobile", "hpc", "high-performance", "computing"]}, {"id": "term-intel-xeon", "t": "Intel Xeon", "tg": ["Processor", "Intel", "Server"], "d": "hardware", "x": "Intel server processor family that forms the CPU foundation of most AI training servers and data centers. Xeon Scalable...", "l": "i", "k": ["intel", "xeon", "server", "processor", "family", "forms", "cpu", "foundation", "training", "servers", "data", "centers", "scalable", "processors", "include"]}, {"id": "term-intent-detection", "t": "Intent Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of classifying the purpose or goal behind a user's utterance in a dialogue system, determining whether the...", "l": "i", "k": ["intent", "detection", "task", "classifying", "purpose", "goal", "behind", "user", "utterance", "dialogue", "system", "determining", "wants", "book", "search"]}, {"id": "term-intent-recognition", "t": "Intent Recognition", "tg": ["NLP Task", "Application"], "d": "general", "x": "Understanding what a user wants to accomplish from their input. A core NLP task for chatbots and virtual assistants,...", "l": "i", "k": ["intent", "recognition", "understanding", "user", "wants", "accomplish", "input", "core", "nlp", "task", "chatbots", "virtual", "assistants", "mapping", "messages"]}, {"id": "term-inter-annotator-agreement", "t": "Inter-Annotator Agreement", "tg": ["Evaluation", "Methodology"], "d": "datasets", "x": "A statistical measure of the degree to which independent human annotators make the same judgments when labeling or...", "l": "i", "k": ["inter-annotator", "agreement", "statistical", "measure", "degree", "independent", "human", "annotators", "judgments", "labeling", "evaluating", "data", "assess", "annotation", "reliability"]}, {"id": "term-inter-token-latency", "t": "Inter-Token Latency", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The time between consecutive token generations during the decode phase of LLM inference. Low inter-token latency is...", "l": "i", "k": ["inter-token", "latency", "time", "consecutive", "token", "generations", "decode", "phase", "llm", "inference", "low", "essential", "streaming", "applications", "users"]}, {"id": "term-interaction-feature", "t": "Interaction Feature", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A derived feature created by combining two or more existing features (typically through multiplication) to capture...", "l": "i", "k": ["interaction", "feature", "derived", "created", "combining", "existing", "features", "typically", "multiplication", "capture", "non-additive", "effects", "allows", "linear", "models"]}, {"id": "term-intercode", "t": "InterCode", "tg": ["Benchmark", "Code", "Evaluation"], "d": "datasets", "x": "An interactive code generation benchmark where models execute code in a sandboxed environment and use execution...", "l": "i", "k": ["intercode", "interactive", "code", "generation", "benchmark", "models", "execute", "sandboxed", "environment", "execution", "feedback", "iteratively", "improve", "solutions"]}, {"id": "term-interconnect-bandwidth", "t": "Interconnect Bandwidth", "tg": ["Performance", "Interconnect", "Metric"], "d": "hardware", "x": "Rate of data transfer between components within a system such as between CPU and GPU or between GPU dies. A critical...", "l": "i", "k": ["interconnect", "bandwidth", "rate", "data", "transfer", "components", "within", "system", "cpu", "gpu", "dies", "critical", "performance", "factor", "systems"]}, {"id": "term-interfuser", "t": "InterFuser", "tg": ["Models", "Technical", "Autonomous", "Safety"], "d": "models", "x": "A safety-enhanced autonomous driving model that uses a Transformer-based sensor fusion approach with interpretable...", "l": "i", "k": ["interfuser", "safety-enhanced", "autonomous", "driving", "model", "uses", "transformer-based", "sensor", "fusion", "approach", "interpretable", "features", "safe", "planning"]}, {"id": "term-interior-point-method", "t": "Interior Point Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An optimization algorithm for linear and nonlinear programming that traverses the interior of the feasible region...", "l": "i", "k": ["interior", "point", "method", "optimization", "algorithm", "linear", "nonlinear", "programming", "traverses", "feasible", "region", "rather", "boundary", "uses", "barrier"]}, {"id": "term-interleaved-image-text-data", "t": "Interleaved Image-Text Data", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "Datasets of naturally interleaved image and text content from web pages used for training multimodal models. Captures...", "l": "i", "k": ["interleaved", "image-text", "data", "datasets", "naturally", "image", "text", "content", "web", "pages", "training", "multimodal", "models", "captures", "real-world"]}, {"id": "term-internimage", "t": "InternImage", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A large-scale vision foundation model that uses deformable convolutions instead of attention mechanisms for strong...", "l": "i", "k": ["internimage", "large-scale", "vision", "foundation", "model", "uses", "deformable", "convolutions", "instead", "attention", "mechanisms", "strong", "performance", "dense", "prediction"]}, {"id": "term-internist-1", "t": "INTERNIST-1", "tg": ["History", "Systems"], "d": "history", "x": "A medical diagnostic expert system developed by Jack Myers and Harry Pople at the University of Pittsburgh in the...", "l": "i", "k": ["internist-1", "medical", "diagnostic", "expert", "system", "developed", "jack", "myers", "harry", "pople", "university", "pittsburgh", "1970s", "covered", "approximately"]}, {"id": "term-internlm", "t": "InternLM", "tg": ["Models", "Technical"], "d": "models", "x": "A family of language models developed by Shanghai AI Laboratory. Features strong performance on Chinese and English...", "l": "i", "k": ["internlm", "family", "language", "models", "developed", "shanghai", "laboratory", "features", "strong", "performance", "chinese", "english", "tasks", "specialized", "variants"]}, {"id": "term-internlm2", "t": "InternLM2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A second-generation large language model from Shanghai AI Lab with improved long-context handling and reasoning...", "l": "i", "k": ["internlm2", "second-generation", "large", "language", "model", "shanghai", "lab", "improved", "long-context", "handling", "reasoning", "capabilities", "across", "multiple", "parameter"]}, {"id": "term-internvid", "t": "InternVid", "tg": ["Training Corpus", "Video", "Multimodal"], "d": "datasets", "x": "A large-scale video-text dataset containing 7 million video clips with generated descriptions. Designed for learning...", "l": "i", "k": ["internvid", "large-scale", "video-text", "dataset", "containing", "million", "video", "clips", "generated", "descriptions", "designed", "learning", "transferable", "video-language", "representations"]}, {"id": "term-internvideo", "t": "InternVideo", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A large-scale video foundation model from Shanghai AI Lab that combines masked video modeling with video-language...", "l": "i", "k": ["internvideo", "large-scale", "video", "foundation", "model", "shanghai", "lab", "combines", "masked", "modeling", "video-language", "contrastive", "learning", "general", "understanding"]}, {"id": "term-internvl", "t": "InternVL", "tg": ["Models", "Technical"], "d": "models", "x": "A large-scale vision-language model that scales the vision encoder to match the capacity of large language models....", "l": "i", "k": ["internvl", "large-scale", "vision-language", "model", "scales", "vision", "encoder", "match", "capacity", "large", "language", "models", "demonstrates", "aligning", "strong"]}, {"id": "term-internvl2", "t": "InternVL2", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A second-generation open-source vision-language foundation model from Shanghai AI Lab with scaling from 1B to 108B...", "l": "i", "k": ["internvl2", "second-generation", "open-source", "vision-language", "foundation", "model", "shanghai", "lab", "scaling", "108b", "parameters", "diverse", "multimodal", "tasks"]}, {"id": "term-interoperability-in-ai-safety", "t": "Interoperability in AI Safety", "tg": ["Safety", "Governance"], "d": "safety", "x": "The ability of different AI safety tools standards and governance frameworks to work together effectively. Important...", "l": "i", "k": ["interoperability", "safety", "ability", "different", "tools", "standards", "governance", "frameworks", "work", "together", "effectively", "important", "avoiding", "fragmentation", "ensuring"]}, {"id": "term-interpolation-search", "t": "Interpolation Search", "tg": ["Algorithms", "Technical", "Searching"], "d": "algorithms", "x": "A search algorithm for sorted and uniformly distributed data that estimates the position of the target using linear...", "l": "i", "k": ["interpolation", "search", "algorithm", "sorted", "uniformly", "distributed", "data", "estimates", "position", "target", "linear", "achieves", "log", "average", "time"]}, {"id": "term-interposer", "t": "Interposer", "tg": ["Fabrication", "Packaging"], "d": "hardware", "x": "Silicon or organic substrate placed between a chip die and its package providing high-density wiring between multiple...", "l": "i", "k": ["interposer", "silicon", "organic", "substrate", "placed", "chip", "die", "package", "providing", "high-density", "wiring", "multiple", "dies", "enables", "packaging"]}, {"id": "term-interpretability", "t": "Interpretability", "tg": ["Property", "Trust"], "d": "safety", "x": "The degree to which humans can understand how a model makes decisions. Higher interpretability enables debugging,...", "l": "i", "k": ["interpretability", "degree", "humans", "understand", "model", "makes", "decisions", "higher", "enables", "debugging", "trust-building", "identifying", "potential", "issues"]}, {"id": "term-interpretability-research", "t": "Interpretability Research", "tg": ["History", "Fundamentals"], "d": "history", "x": "The field of AI research focused on understanding how neural networks make decisions and what they have learned....", "l": "i", "k": ["interpretability", "research", "field", "focused", "understanding", "neural", "networks", "decisions", "learned", "approaches", "include", "mechanistic", "probing", "studies", "visualization"]}, {"id": "term-interrupted-time-series-analysis", "t": "Interrupted Time Series Analysis", "tg": ["Algorithms", "Technical", "Causal"], "d": "algorithms", "x": "A quasi-experimental design that assesses the impact of an intervention by analyzing changes in the level and slope of...", "l": "i", "k": ["interrupted", "time", "series", "analysis", "quasi-experimental", "design", "assesses", "impact", "intervention", "analyzing", "changes", "level", "slope", "point"]}, {"id": "term-intersection-over-union", "t": "Intersection over Union", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A metric (IoU) that measures the overlap between a predicted bounding box and a ground truth box by computing the area...", "l": "i", "k": ["intersection", "union", "metric", "iou", "measures", "overlap", "predicted", "bounding", "box", "ground", "truth", "computing", "area", "divided", "evaluate"]}, {"id": "term-intersectional-bias", "t": "Intersectional Bias", "tg": ["Safety", "Technical"], "d": "safety", "x": "Bias that affects individuals at the intersection of multiple protected characteristics such as race and gender in ways...", "l": "i", "k": ["intersectional", "bias", "affects", "individuals", "intersection", "multiple", "protected", "characteristics", "race", "gender", "ways", "captured", "examining", "characteristic", "independently"]}, {"id": "term-interval-tree-algorithm", "t": "Interval Tree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A data structure that holds intervals and efficiently supports queries for all intervals overlapping a given point or...", "l": "i", "k": ["interval", "tree", "algorithm", "data", "structure", "holds", "intervals", "efficiently", "supports", "queries", "overlapping", "given", "point", "built", "balanced"]}, {"id": "term-intrinsic-motivation", "t": "Intrinsic Motivation", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An internal reward signal generated by the agent itself to encourage exploration, independent of the environment's...", "l": "i", "k": ["intrinsic", "motivation", "internal", "reward", "signal", "generated", "agent", "itself", "encourage", "exploration", "independent", "environment", "extrinsic", "methods", "include"]}, {"id": "term-introsort", "t": "Introsort", "tg": ["Algorithms", "Technical", "Sorting"], "d": "algorithms", "x": "A hybrid sorting algorithm that begins with quicksort and switches to heapsort when the recursion depth exceeds a...", "l": "i", "k": ["introsort", "hybrid", "sorting", "algorithm", "begins", "quicksort", "switches", "heapsort", "recursion", "depth", "exceeds", "threshold", "uses", "insertion", "sort"]}, {"id": "term-inverse-iteration", "t": "Inverse Iteration", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An iterative method for finding an eigenvector corresponding to an eigenvalue that is approximately known. Converges to...", "l": "i", "k": ["inverse", "iteration", "iterative", "method", "finding", "eigenvector", "corresponding", "eigenvalue", "approximately", "known", "converges", "associated", "closest", "given", "shift"]}, {"id": "term-inverse-probability-weighting-algorithm", "t": "Inverse Probability Weighting Algorithm", "tg": ["Algorithms", "Technical", "Causal"], "d": "algorithms", "x": "A causal inference method that reweights observations by the inverse of their probability of receiving the observed...", "l": "i", "k": ["inverse", "probability", "weighting", "algorithm", "causal", "inference", "method", "reweights", "observations", "receiving", "observed", "treatment", "creates", "pseudo-population", "assignment"]}, {"id": "term-inverse-rl", "t": "Inverse Reinforcement Learning", "tg": ["Reinforcement Learning", "Imitation"], "d": "general", "x": "The problem of inferring an unknown reward function from observed expert behavior, recovering the objectives that...", "l": "i", "k": ["inverse", "reinforcement", "learning", "problem", "inferring", "unknown", "reward", "function", "observed", "expert", "behavior", "recovering", "objectives", "explain", "demonstrated"]}, {"id": "term-inverse-reward-design", "t": "Inverse Reward Design", "tg": ["Reinforcement Learning", "Safety"], "d": "safety", "x": "A framework that treats the specified reward function as an observation of the designer's true intent rather than the...", "l": "i", "k": ["inverse", "reward", "design", "framework", "treats", "specified", "function", "observation", "designer", "true", "intent", "rather", "literal", "objective", "reasoning"]}, {"id": "term-inverse-scaling", "t": "Inverse Scaling", "tg": ["Research", "Phenomenon"], "d": "general", "x": "When larger models perform worse on certain tasks than smaller ones. Discovered through research challenges, revealing...", "l": "i", "k": ["inverse", "scaling", "larger", "models", "perform", "worse", "certain", "tasks", "smaller", "ones", "discovered", "research", "challenges", "revealing", "unexpected"]}, {"id": "term-inverse-transform-sampling", "t": "Inverse Transform Sampling", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A method for generating random samples from any probability distribution given its inverse cumulative distribution...", "l": "i", "k": ["inverse", "transform", "sampling", "method", "generating", "random", "samples", "probability", "distribution", "given", "cumulative", "function", "transforms", "uniform", "variables"]}, {"id": "term-inverted-index-algorithm", "t": "Inverted Index Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP", "Data Structure"], "d": "algorithms", "x": "A data structure that maps content tokens to the documents containing them. The fundamental building block of text...", "l": "i", "k": ["inverted", "index", "algorithm", "data", "structure", "maps", "content", "tokens", "documents", "containing", "fundamental", "building", "block", "text", "search"]}, {"id": "term-inverted-residual-block", "t": "Inverted Residual Block", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A building block in MobileNetV2 that expands the channel dimension with a pointwise convolution, applies depthwise...", "l": "i", "k": ["inverted", "residual", "block", "building", "mobilenetv2", "expands", "channel", "dimension", "pointwise", "convolution", "applies", "depthwise", "projects", "narrow", "output"]}, {"id": "term-iob-tagging", "t": "IOB Tagging", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A labeling scheme for sequence tagging that marks tokens as Inside, Outside, or Beginning of a named entity or chunk,...", "l": "i", "k": ["iob", "tagging", "labeling", "scheme", "sequence", "marks", "tokens", "inside", "outside", "beginning", "named", "entity", "chunk", "enabling", "identification"]}, {"id": "term-ion-implantation", "t": "Ion Implantation", "tg": ["Fabrication", "Manufacturing", "Process"], "d": "hardware", "x": "Process of embedding dopant atoms into a semiconductor wafer by accelerating ions to high energies. Used to precisely...", "l": "i", "k": ["ion", "implantation", "process", "embedding", "dopant", "atoms", "semiconductor", "wafer", "accelerating", "ions", "high", "energies", "precisely", "control", "electrical"]}, {"id": "term-iou-loss", "t": "IoU Loss", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Intersection over Union loss measures the overlap between predicted and ground truth bounding boxes or segmentation...", "l": "i", "k": ["iou", "loss", "intersection", "union", "measures", "overlap", "predicted", "ground", "truth", "bounding", "boxes", "segmentation", "masks", "directly", "optimizes"]}, {"id": "term-ip-adapter", "t": "IP-Adapter", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "An image prompt adapter for diffusion models that enables image-conditioned generation by injecting visual features...", "l": "i", "k": ["ip-adapter", "image", "prompt", "adapter", "diffusion", "models", "enables", "image-conditioned", "generation", "injecting", "visual", "features", "decoupled", "cross-attention", "allowing"]}, {"id": "term-ip-adapter-plus", "t": "IP-Adapter Plus", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An enhanced version of IP-Adapter with a more detailed image prompt adapter that captures fine-grained visual features...", "l": "i", "k": ["ip-adapter", "plus", "enhanced", "version", "detailed", "image", "prompt", "adapter", "captures", "fine-grained", "visual", "features", "improved", "image-guided", "generation"]}, {"id": "term-ip-adapter-faceid", "t": "IP-Adapter-FaceID", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A variant of IP-Adapter that specializes in preserving facial identity in generated images by using face recognition...", "l": "i", "k": ["ip-adapter-faceid", "variant", "ip-adapter", "specializes", "preserving", "facial", "identity", "generated", "images", "face", "recognition", "embeddings", "conditioning"]}, {"id": "term-ipmi", "t": "IPMI", "tg": ["Infrastructure", "Management", "Standard"], "d": "hardware", "x": "Intelligent Platform Management Interface standard for monitoring and managing server hardware. Used to remotely...", "l": "i", "k": ["ipmi", "intelligent", "platform", "management", "interface", "standard", "monitoring", "managing", "server", "hardware", "remotely", "monitor", "gpu", "health", "temperatures"]}, {"id": "term-ipo", "t": "IPO (Identity Preference Optimization)", "tg": ["Training", "Alignment"], "d": "safety", "x": "An alternative to DPO for preference learning that doesn't require a reference model. Simplifies the training process...", "l": "i", "k": ["ipo", "identity", "preference", "optimization", "alternative", "dpo", "learning", "doesn", "require", "reference", "model", "simplifies", "training", "process", "maintaining"]}, {"id": "term-iris-dataset", "t": "Iris Dataset", "tg": ["Benchmark", "Tabular"], "d": "datasets", "x": "A 150-sample dataset of iris flower measurements across 3 species introduced by Ronald Fisher in 1936. One of the most...", "l": "i", "k": ["iris", "dataset", "150-sample", "flower", "measurements", "across", "species", "introduced", "ronald", "fisher", "famous", "datasets", "pattern", "recognition", "history"]}, {"id": "term-iso-ai-standards", "t": "ISO AI Standards", "tg": ["Governance", "Regulation"], "d": "safety", "x": "International standards developed by ISO/IEC JTC 1/SC 42 for artificial intelligence, including ISO/IEC 42001 for AI...", "l": "i", "k": ["iso", "standards", "international", "developed", "iec", "jtc", "artificial", "intelligence", "including", "management", "systems", "risk"]}, {"id": "term-isolation-forest", "t": "Isolation Forest", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An ensemble-based anomaly detection algorithm that isolates observations by randomly selecting features and split...", "l": "i", "k": ["isolation", "forest", "ensemble-based", "anomaly", "detection", "algorithm", "isolates", "observations", "randomly", "selecting", "features", "split", "values", "anomalies", "require"]}, {"id": "term-isomap-algorithm", "t": "Isomap Algorithm", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A nonlinear dimensionality reduction method that preserves geodesic distances between all pairs of points. Constructs a...", "l": "i", "k": ["isomap", "algorithm", "nonlinear", "dimensionality", "reduction", "method", "preserves", "geodesic", "distances", "pairs", "points", "constructs", "neighborhood", "graph", "applies"]}, {"id": "term-isotonic-calibration", "t": "Isotonic Calibration", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A non-parametric calibration method that fits an isotonic regression to map classifier scores to calibrated...", "l": "i", "k": ["isotonic", "calibration", "non-parametric", "method", "fits", "regression", "map", "classifier", "scores", "calibrated", "probabilities", "monotonically", "increasing", "step", "function"]}, {"id": "term-isotonic-regression", "t": "Isotonic Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A non-parametric regression technique that fits a non-decreasing (or non-increasing) step function to the data,...", "l": "i", "k": ["isotonic", "regression", "non-parametric", "technique", "fits", "non-decreasing", "non-increasing", "step", "function", "data", "minimizing", "sum", "squared", "errors", "subject"]}, {"id": "term-iterated-amplification", "t": "Iterated Amplification", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "An AI alignment approach proposed by Paul Christiano where a human overseer is amplified by decomposing complex tasks...", "l": "i", "k": ["iterated", "amplification", "alignment", "approach", "proposed", "paul", "christiano", "human", "overseer", "amplified", "decomposing", "complex", "tasks", "simpler", "subtasks"]}, {"id": "term-iterated-local-search", "t": "Iterated Local Search", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A metaheuristic that applies perturbations to a locally optimal solution and then re-optimizes using local search. The...", "l": "i", "k": ["iterated", "local", "search", "metaheuristic", "applies", "perturbations", "locally", "optimal", "solution", "re-optimizes", "perturbation-optimization", "cycle", "explores", "different", "basins"]}, {"id": "term-iteration", "t": "Iteration (Prompting)", "tg": ["Prompting", "Practice"], "d": "general", "x": "The practice of refining prompts through multiple attempts to achieve better results. Essential for getting the most...", "l": "i", "k": ["iteration", "prompting", "practice", "refining", "prompts", "multiple", "attempts", "achieve", "better", "results", "essential", "getting", "treating", "iterative", "process"]}, {"id": "term-iterative-deepening-depth-first-search", "t": "Iterative Deepening Depth-First Search", "tg": ["Algorithms", "Fundamentals", "Graph", "Searching"], "d": "algorithms", "x": "A graph traversal strategy that performs depth-first search with increasing depth limits. Combines the space efficiency...", "l": "i", "k": ["iterative", "deepening", "depth-first", "search", "graph", "traversal", "strategy", "performs", "increasing", "depth", "limits", "combines", "space", "efficiency", "dfs"]}, {"id": "term-iterative-dpo", "t": "Iterative DPO", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An extension of Direct Preference Optimization that performs multiple rounds of preference optimization. In each...", "l": "i", "k": ["iterative", "dpo", "extension", "direct", "preference", "optimization", "performs", "multiple", "rounds", "iteration", "pairs", "generated", "updated", "model", "creating"]}, {"id": "term-iterative-refinement-prompting", "t": "Iterative Refinement Prompting", "tg": ["Prompt Engineering", "Refinement"], "d": "general", "x": "A technique where the model's initial output is fed back with critique instructions for progressive improvement across...", "l": "i", "k": ["iterative", "refinement", "prompting", "technique", "model", "initial", "output", "fed", "critique", "instructions", "progressive", "improvement", "across", "multiple", "rounds"]}, {"id": "term-itransformer", "t": "iTransformer", "tg": ["Models", "Technical"], "d": "models", "x": "An inverted Transformer for time series forecasting that applies attention across variate dimensions rather than...", "l": "i", "k": ["itransformer", "inverted", "transformer", "time", "series", "forecasting", "applies", "attention", "across", "variate", "dimensions", "rather", "temporal", "multivariate", "prediction"]}, {"id": "term-ivf-index", "t": "IVF Index", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "Inverted File Index, a vector search structure that partitions the vector space into Voronoi cells using k-means...", "l": "i", "k": ["ivf", "index", "inverted", "file", "vector", "search", "structure", "partitions", "space", "voronoi", "cells", "k-means", "clustering", "searches", "nearest"]}, {"id": "term-iwildcam", "t": "iWildCam", "tg": ["Benchmark", "Computer Vision", "Conservation"], "d": "datasets", "x": "A camera trap image classification challenge dataset from diverse global locations. Tests the ability of models to...", "l": "i", "k": ["iwildcam", "camera", "trap", "image", "classification", "challenge", "dataset", "diverse", "global", "locations", "tests", "ability", "models", "identify", "wildlife"]}, {"id": "term-jaccard-index", "t": "Jaccard Index", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A similarity coefficient measuring the overlap between two sets, defined as the size of their intersection divided by...", "l": "j", "k": ["jaccard", "index", "similarity", "coefficient", "measuring", "overlap", "sets", "defined", "size", "intersection", "divided", "union", "ranges", "identical"]}, {"id": "term-jaccard-similarity", "t": "Jaccard Similarity", "tg": ["Vector Database", "Similarity"], "d": "general", "x": "A set-based similarity metric calculated as the size of the intersection divided by the size of the union of two sets,...", "l": "j", "k": ["jaccard", "similarity", "set-based", "metric", "calculated", "size", "intersection", "divided", "union", "sets", "commonly", "applied", "binary", "vectors", "token"]}, {"id": "term-jaccard-similarity-algorithm", "t": "Jaccard Similarity Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP"], "d": "algorithms", "x": "A set similarity measure that computes the ratio of the intersection to the union of two sets. Values range from zero...", "l": "j", "k": ["jaccard", "similarity", "algorithm", "measure", "computes", "ratio", "intersection", "union", "sets", "values", "range", "zero", "overlap", "identical", "document"]}, {"id": "term-jackknife", "t": "Jackknife", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A resampling method that systematically leaves out one observation at a time and recomputes the statistic, using the...", "l": "j", "k": ["jackknife", "resampling", "method", "systematically", "leaves", "observation", "time", "recomputes", "statistic", "variation", "across", "leave-one-out", "estimates", "assess", "bias"]}, {"id": "term-jacobi-eigenvalue-algorithm", "t": "Jacobi Eigenvalue Algorithm", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An iterative method for computing all eigenvalues and eigenvectors of a real symmetric matrix. Applies a sequence of...", "l": "j", "k": ["jacobi", "eigenvalue", "algorithm", "iterative", "method", "computing", "eigenvalues", "eigenvectors", "real", "symmetric", "matrix", "applies", "sequence", "givens", "rotations"]}, {"id": "term-jacobi-iterative-method", "t": "Jacobi Iterative Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An iterative algorithm for solving diagonally dominant systems of linear equations. Updates each variable...", "l": "j", "k": ["jacobi", "iterative", "method", "algorithm", "solving", "diagonally", "dominant", "systems", "linear", "equations", "updates", "variable", "simultaneously", "values", "previous"]}, {"id": "term-jacobian-vector-product", "t": "Jacobian-Vector Product", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An efficient computation that multiplies the Jacobian matrix of a function by a vector without explicitly forming the...", "l": "j", "k": ["jacobian-vector", "product", "efficient", "computation", "multiplies", "jacobian", "matrix", "function", "vector", "without", "explicitly", "forming", "full", "forward-mode", "automatic"]}, {"id": "term-jailbreak", "t": "Jailbreak", "tg": ["Security", "Risk"], "d": "safety", "x": "Attempts to bypass an AI system's safety restrictions through cleverly crafted prompts. A ongoing challenge for AI...", "l": "j", "k": ["jailbreak", "attempts", "bypass", "system", "safety", "restrictions", "cleverly", "crafted", "prompts", "ongoing", "challenge", "developers", "maintaining", "safe", "behavior"]}, {"id": "term-jais", "t": "Jais", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A bilingual Arabic-English large language model developed in the UAE and designed for high-quality Arabic language...", "l": "j", "k": ["jais", "bilingual", "arabic-english", "large", "language", "model", "developed", "uae", "designed", "high-quality", "arabic", "understanding", "generation"]}, {"id": "term-jais-30b", "t": "Jais 30B", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A scaled-up version of the Jais Arabic-English bilingual model with 30 billion parameters for stronger Arabic language...", "l": "j", "k": ["jais", "30b", "scaled-up", "version", "arabic-english", "bilingual", "model", "billion", "parameters", "stronger", "arabic", "language", "generation", "understanding"]}, {"id": "term-jamba", "t": "Jamba", "tg": ["Models", "Technical"], "d": "models", "x": "A hybrid architecture by AI21 Labs that interleaves transformer and Mamba layers with mixture-of-experts. Combines the...", "l": "j", "k": ["jamba", "hybrid", "architecture", "ai21", "labs", "interleaves", "transformer", "mamba", "layers", "mixture-of-experts", "combines", "strengths", "attention", "state", "space"]}, {"id": "term-jamba-15", "t": "Jamba 1.5", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An improved version of the Jamba hybrid architecture with larger model sizes and enhanced training for better reasoning...", "l": "j", "k": ["jamba", "improved", "version", "hybrid", "architecture", "larger", "model", "sizes", "enhanced", "training", "better", "reasoning", "instruction", "following"]}, {"id": "term-james-slagle", "t": "James Slagle", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who developed SAINT (Symbolic Automatic INTegrator) in 1961 at MIT. SAINT could solve...", "l": "j", "k": ["james", "slagle", "american", "computer", "scientist", "developed", "saint", "symbolic", "automatic", "integrator", "mit", "solve", "integration", "problems", "demonstrating"]}, {"id": "term-japanese-fifth-generation-project", "t": "Japanese Fifth Generation Computer Project", "tg": ["History", "Milestones"], "d": "history", "x": "A 1982-1992 Japanese government initiative to develop massively parallel computers using logic programming for AI...", "l": "j", "k": ["japanese", "fifth", "generation", "computer", "project", "1982-1992", "government", "initiative", "develop", "massively", "parallel", "computers", "logic", "programming", "applications"]}, {"id": "term-jax", "t": "JAX", "tg": ["Framework", "Technical"], "d": "general", "x": "Google's library for high-performance numerical computing and automatic differentiation. Popular for ML research due to...", "l": "j", "k": ["jax", "google", "library", "high-performance", "numerical", "computing", "automatic", "differentiation", "popular", "research", "due", "composability", "gpu", "tpu", "support"]}, {"id": "term-jax-framework", "t": "JAX Framework", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "Google's numerical computing framework that combines NumPy-like syntax with automatic differentiation, XLA compilation,...", "l": "j", "k": ["jax", "framework", "google", "numerical", "computing", "combines", "numpy-like", "syntax", "automatic", "differentiation", "xla", "compilation", "native", "support", "spmd"]}, {"id": "term-jeebench", "t": "JEEBench", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A benchmark of questions from the Joint Entrance Examination testing reasoning in physics chemistry and mathematics....", "l": "j", "k": ["jeebench", "benchmark", "questions", "joint", "entrance", "examination", "testing", "reasoning", "physics", "chemistry", "mathematics", "tests", "advanced", "stem", "problem"]}, {"id": "term-jeff-dean", "t": "Jeff Dean", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist and head of Google AI who co-developed foundational large-scale systems including MapReduce...", "l": "j", "k": ["jeff", "dean", "american", "computer", "scientist", "head", "google", "co-developed", "foundational", "large-scale", "systems", "including", "mapreduce", "tensorflow", "transformer"]}, {"id": "term-jeffreys-prior", "t": "Jeffreys Prior", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A non-informative prior distribution derived from the Fisher information matrix that is invariant under...", "l": "j", "k": ["jeffreys", "prior", "non-informative", "distribution", "derived", "fisher", "information", "matrix", "invariant", "reparametrization", "provides", "principled", "default", "knowledge", "available"]}, {"id": "term-jensen-shannon-divergence", "t": "Jensen-Shannon Divergence", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A symmetric and bounded divergence measure derived from KL divergence, computed as the average KL divergence of two...", "l": "j", "k": ["jensen-shannon", "divergence", "symmetric", "bounded", "measure", "derived", "computed", "average", "distributions", "mixture", "always", "finite", "ranges", "log"]}, {"id": "term-jglue", "t": "JGLUE", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "Japanese General Language Understanding Evaluation a benchmark of Japanese NLU tasks analogous to GLUE. Tests language...", "l": "j", "k": ["jglue", "japanese", "general", "language", "understanding", "evaluation", "benchmark", "nlu", "tasks", "analogous", "glue", "tests", "capabilities", "specific"]}, {"id": "term-jigsaw-toxic-comment", "t": "Jigsaw Toxic Comment", "tg": ["Benchmark", "NLP", "Safety"], "d": "datasets", "x": "A dataset of Wikipedia talk page comments annotated for multiple types of toxicity including threats obscenity and...", "l": "j", "k": ["jigsaw", "toxic", "comment", "dataset", "wikipedia", "talk", "page", "comments", "annotated", "multiple", "types", "toxicity", "including", "threats", "obscenity"]}, {"id": "term-jina-embeddings", "t": "Jina Embeddings", "tg": ["Models", "Technical", "Embedding", "NLP"], "d": "models", "x": "A family of embedding models from Jina AI optimized for semantic search and retrieval with support for long documents...", "l": "j", "k": ["jina", "embeddings", "family", "embedding", "models", "optimized", "semantic", "search", "retrieval", "support", "long", "documents", "multiple", "languages"]}, {"id": "term-john-backus", "t": "John Backus", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who led the development of FORTRAN the first widely used high-level programming language at...", "l": "j", "k": ["john", "backus", "american", "computer", "scientist", "led", "development", "fortran", "widely", "high-level", "programming", "language", "ibm", "1950s", "turing"]}, {"id": "term-john-holland", "t": "John Holland", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1929-2015) who invented genetic algorithms and developed the Holland schema theorem,...", "l": "j", "k": ["john", "holland", "american", "computer", "scientist", "1929-2015", "invented", "genetic", "algorithms", "developed", "schema", "theorem", "founding", "field", "evolutionary"]}, {"id": "term-john-hopfield", "t": "John Hopfield", "tg": ["History", "Pioneers"], "d": "history", "x": "American physicist who introduced Hopfield networks in 1982, applying concepts from statistical physics to create...", "l": "j", "k": ["john", "hopfield", "american", "physicist", "introduced", "networks", "applying", "concepts", "statistical", "physics", "create", "neural", "network", "models", "associative"]}, {"id": "term-john-mccarthy", "t": "John McCarthy", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1927-2011) who coined the term artificial intelligence in 1955, organized the Dartmouth...", "l": "j", "k": ["john", "mccarthy", "american", "computer", "scientist", "1927-2011", "coined", "term", "artificial", "intelligence", "organized", "dartmouth", "workshop", "invented", "lisp"]}, {"id": "term-john-searle", "t": "John Searle", "tg": ["History", "Pioneers"], "d": "history", "x": "American philosopher who proposed the Chinese Room argument in 1980, challenging the notion that computers can truly...", "l": "j", "k": ["john", "searle", "american", "philosopher", "proposed", "chinese", "room", "argument", "challenging", "notion", "computers", "truly", "understand", "language", "possess"]}, {"id": "term-john-von-neumann", "t": "John von Neumann", "tg": ["History", "Pioneers"], "d": "history", "x": "Hungarian-American mathematician (1903-1957) who made foundational contributions to computer architecture, game theory,...", "l": "j", "k": ["john", "von", "neumann", "hungarian-american", "mathematician", "1903-1957", "foundational", "contributions", "computer", "architecture", "game", "theory", "cellular", "automata", "whose"]}, {"id": "term-johnsons-algorithm", "t": "Johnson's Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm for finding all-pairs shortest paths in sparse directed graphs. Reweights edges using Bellman-Ford to...", "l": "j", "k": ["johnson", "algorithm", "finding", "all-pairs", "shortest", "paths", "sparse", "directed", "graphs", "reweights", "edges", "bellman-ford", "eliminate", "negative", "weights"]}, {"id": "term-joseph-weizenbaum", "t": "Joseph Weizenbaum", "tg": ["History", "Pioneers"], "d": "history", "x": "German-American computer scientist (1923-2008) who created ELIZA and later became a prominent critic of AI, warning...", "l": "j", "k": ["joseph", "weizenbaum", "german-american", "computer", "scientist", "1923-2008", "created", "eliza", "later", "became", "prominent", "critic", "warning", "ethical", "implications"]}, {"id": "term-joshua-lederberg", "t": "Joshua Lederberg", "tg": ["History", "Pioneers"], "d": "history", "x": "American molecular biologist and Nobel laureate who collaborated with Edward Feigenbaum on DENDRAL. Lederberg saw the...", "l": "j", "k": ["joshua", "lederberg", "american", "molecular", "biologist", "nobel", "laureate", "collaborated", "edward", "feigenbaum", "dendral", "saw", "potential", "computers", "assist"]}, {"id": "term-journeydb", "t": "JourneyDB", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "A dataset of 4 million Midjourney prompt-image pairs with style and content annotations. Used for studying...", "l": "j", "k": ["journeydb", "dataset", "million", "midjourney", "prompt-image", "pairs", "style", "content", "annotations", "studying", "text-to-image", "generation", "prompt", "engineering", "patterns"]}, {"id": "term-joy-buolamwini", "t": "Joy Buolamwini", "tg": ["History", "Pioneers"], "d": "history", "x": "Ghanaian-American computer scientist who founded the Algorithmic Justice League. Her research at MIT Media Lab exposed...", "l": "j", "k": ["joy", "buolamwini", "ghanaian-american", "computer", "scientist", "founded", "algorithmic", "justice", "league", "research", "mit", "media", "lab", "exposed", "significant"]}, {"id": "term-json", "t": "JSON (JavaScript Object Notation)", "tg": ["Format", "Technical"], "d": "general", "x": "A structured data format commonly used for API responses and structured output from LLMs. Many AI models can generate...", "l": "j", "k": ["json", "javascript", "object", "notation", "structured", "data", "format", "commonly", "api", "responses", "output", "llms", "models", "generate", "valid"]}, {"id": "term-json-mode-generation", "t": "JSON Mode", "tg": ["Generative AI", "LLM"], "d": "models", "x": "An inference configuration that constrains a language model to produce only valid JSON output, typically implemented...", "l": "j", "k": ["json", "mode", "inference", "configuration", "constrains", "language", "model", "produce", "valid", "output", "typically", "implemented", "grammar-based", "token", "masking"]}, {"id": "term-json-mode-prompting", "t": "JSON Mode Prompting", "tg": ["Prompt Engineering", "Output Format"], "d": "hardware", "x": "A prompting technique that instructs the language model to output responses exclusively in valid JSON format, often...", "l": "j", "k": ["json", "mode", "prompting", "technique", "instructs", "language", "model", "output", "responses", "exclusively", "valid", "format", "combined", "schema", "definitions"]}, {"id": "term-judea-pearl", "t": "Judea Pearl", "tg": ["History", "Pioneers"], "d": "history", "x": "Israeli-American computer scientist who pioneered Bayesian networks and causal inference in AI, receiving the 2011...", "l": "j", "k": ["judea", "pearl", "israeli-american", "computer", "scientist", "pioneered", "bayesian", "networks", "causal", "inference", "receiving", "turing", "award", "work", "probabilistic"]}, {"id": "term-juergen-schmidhuber", "t": "Juergen Schmidhuber", "tg": ["History", "Pioneers"], "d": "history", "x": "German computer scientist who co-invented Long Short-Term Memory (LSTM) networks with Sepp Hochreiter in 1997. Pioneer...", "l": "j", "k": ["juergen", "schmidhuber", "german", "computer", "scientist", "co-invented", "long", "short-term", "memory", "lstm", "networks", "sepp", "hochreiter", "pioneer", "recurrent"]}, {"id": "term-jukebox", "t": "Jukebox", "tg": ["Models", "Technical", "Audio", "History"], "d": "models", "x": "A neural network from OpenAI that generates music with singing in the raw audio domain using VQ-VAE and autoregressive...", "l": "j", "k": ["jukebox", "neural", "network", "openai", "generates", "music", "singing", "raw", "audio", "domain", "vq-vae", "autoregressive", "transformer", "architectures"]}, {"id": "term-jump-search", "t": "Jump Search", "tg": ["Algorithms", "Technical", "Searching"], "d": "algorithms", "x": "A search algorithm for sorted arrays that checks elements at fixed intervals and then performs a linear search within...", "l": "j", "k": ["jump", "search", "algorithm", "sorted", "arrays", "checks", "elements", "fixed", "intervals", "performs", "linear", "within", "identified", "block", "optimal"]}, {"id": "term-junction-tree-algorithm", "t": "Junction Tree Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An exact inference algorithm for probabilistic graphical models that converts the graph into a junction tree (clique...", "l": "j", "k": ["junction", "tree", "algorithm", "exact", "inference", "probabilistic", "graphical", "models", "converts", "graph", "clique", "performs", "message", "passing", "compute"]}, {"id": "term-jupyter", "t": "Jupyter Notebook", "tg": ["Tools", "Development"], "d": "general", "x": "An interactive computing environment where code, visualizations, and text can be combined. Widely used for data...", "l": "j", "k": ["jupyter", "notebook", "interactive", "computing", "environment", "code", "visualizations", "text", "combined", "widely", "data", "science", "experimentation", "educational", "content"]}, {"id": "term-jurassic", "t": "Jurassic", "tg": ["Models", "Technical"], "d": "models", "x": "A family of large language models developed by AI21 Labs. Jurassic-2 features a custom tokenizer optimized for...", "l": "j", "k": ["jurassic", "family", "large", "language", "models", "developed", "ai21", "labs", "jurassic-2", "features", "custom", "tokenizer", "optimized", "efficiency", "supports"]}, {"id": "term-jurgen-schmidhuber", "t": "Jurgen Schmidhuber", "tg": ["History", "Pioneers"], "d": "history", "x": "German-Swiss computer scientist who co-invented LSTM networks, contributed to recurrent neural network research, and...", "l": "j", "k": ["jurgen", "schmidhuber", "german-swiss", "computer", "scientist", "co-invented", "lstm", "networks", "contributed", "recurrent", "neural", "network", "research", "advocated", "recognition"]}, {"id": "term-just-transition-for-ai", "t": "Just Transition for AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "Policies and practices to ensure that the benefits and costs of AI-driven economic transformation are distributed...", "l": "j", "k": ["transition", "policies", "practices", "ensure", "benefits", "costs", "ai-driven", "economic", "transformation", "distributed", "equitably", "particularly", "workers", "communities", "affected"]}, {"id": "term-justice-in-ai", "t": "Justice in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The application of principles of distributive procedural and restorative justice to the development and deployment of...", "l": "j", "k": ["justice", "application", "principles", "distributive", "procedural", "restorative", "development", "deployment", "systems", "ensures", "fair", "allocation", "benefits", "burdens", "across"]}, {"id": "term-k-center-problem", "t": "k-Center Problem", "tg": ["Algorithms", "Technical", "Graph", "Optimization"], "d": "algorithms", "x": "A facility location problem that selects k center points to minimize the maximum distance from any point to its nearest...", "l": "k", "k": ["k-center", "problem", "facility", "location", "selects", "center", "points", "minimize", "maximum", "distance", "point", "nearest", "np-hard", "admits", "2-approximation"]}, {"id": "term-k-d-tree-algorithm", "t": "k-d Tree Algorithm", "tg": ["Algorithms", "Fundamentals", "Searching", "Data Structure"], "d": "algorithms", "x": "A space-partitioning data structure that organizes points in k-dimensional space using axis-aligned splitting planes....", "l": "k", "k": ["k-d", "tree", "algorithm", "space-partitioning", "data", "structure", "organizes", "points", "k-dimensional", "space", "axis-aligned", "splitting", "planes", "enables", "efficient"]}, {"id": "term-k-d-b-tree-algorithm", "t": "K-D-B Tree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A hybrid of k-d trees and B-trees designed for multi-dimensional data stored on disk. Combines the multi-dimensional...", "l": "k", "k": ["k-d-b", "tree", "algorithm", "hybrid", "k-d", "trees", "b-trees", "designed", "multi-dimensional", "data", "stored", "disk", "combines", "partitioning", "balanced"]}, {"id": "term-k-fold", "t": "K-Fold Cross-Validation", "tg": ["Evaluation", "Training"], "d": "datasets", "x": "A cross-validation technique that divides data into K equal parts, training K times with a different part as the test...", "l": "k", "k": ["k-fold", "cross-validation", "technique", "divides", "data", "equal", "parts", "training", "times", "different", "part", "test", "time", "provides", "robust"]}, {"id": "term-k-means", "t": "K-Means", "tg": ["Machine Learning", "Clustering"], "d": "general", "x": "An unsupervised clustering algorithm that partitions n observations into k clusters by iteratively assigning points to...", "l": "k", "k": ["k-means", "unsupervised", "clustering", "algorithm", "partitions", "observations", "clusters", "iteratively", "assigning", "points", "nearest", "centroid", "updating", "centroids", "mean"]}, {"id": "term-k-means-clustering", "t": "K-Means Clustering", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "An unsupervised learning algorithm that partitions data into K clusters by iteratively assigning points to the nearest...", "l": "k", "k": ["k-means", "clustering", "unsupervised", "learning", "algorithm", "partitions", "data", "clusters", "iteratively", "assigning", "points", "nearest", "centroid", "updating", "centroids"]}, {"id": "term-k-medoids-algorithm", "t": "K-Medoids Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A clustering algorithm similar to k-means that uses actual data points (medoids) as cluster centers rather than mean...", "l": "k", "k": ["k-medoids", "algorithm", "clustering", "similar", "k-means", "uses", "actual", "data", "points", "medoids", "cluster", "centers", "rather", "mean", "values"]}, {"id": "term-k-nearest-neighbors", "t": "k-Nearest Neighbors", "tg": ["History", "Fundamentals"], "d": "history", "x": "A simple non-parametric classification and regression method proposed in early forms by Evelyn Fix and Joseph Hodges in...", "l": "k", "k": ["k-nearest", "neighbors", "simple", "non-parametric", "classification", "regression", "method", "proposed", "early", "forms", "evelyn", "fix", "joseph", "hodges", "knn"]}, {"id": "term-k-nearest", "t": "K-Nearest Neighbors (KNN)", "tg": ["Algorithm", "Classification"], "d": "algorithms", "x": "A simple ML algorithm that classifies data points based on the majority class of their K nearest neighbors. Intuitive...", "l": "k", "k": ["k-nearest", "neighbors", "knn", "simple", "algorithm", "classifies", "data", "points", "based", "majority", "class", "nearest", "intuitive", "slow", "large"]}, {"id": "term-k-nearest-neighbors-model", "t": "k-Nearest Neighbors Model", "tg": ["Models", "Fundamentals", "History"], "d": "models", "x": "A non-parametric supervised learning algorithm that classifies data points based on the majority label among the k...", "l": "k", "k": ["k-nearest", "neighbors", "model", "non-parametric", "supervised", "learning", "algorithm", "classifies", "data", "points", "based", "majority", "label", "among", "closest"]}, {"id": "term-k-nearest-neighbors-search", "t": "K-Nearest Neighbors Search", "tg": ["Vector Database", "Search"], "d": "general", "x": "A vector retrieval operation that returns the K vectors most similar to a query vector according to a specified...", "l": "k", "k": ["k-nearest", "neighbors", "search", "vector", "retrieval", "operation", "returns", "vectors", "similar", "query", "according", "specified", "distance", "metric", "forming"]}, {"id": "term-k2", "t": "K2", "tg": ["Models", "Technical", "NLP", "Scientific"], "d": "models", "x": "A large language model designed specifically for geoscience applications that incorporates domain-specific knowledge...", "l": "k", "k": ["large", "language", "model", "designed", "specifically", "geoscience", "applications", "incorporates", "domain-specific", "knowledge", "continued", "pre-training", "scientific", "literature"]}, {"id": "term-kaggle", "t": "Kaggle", "tg": ["History", "Organizations"], "d": "history", "x": "A platform for data science and machine learning competitions founded in 2010 and acquired by Google in 2017. Kaggle...", "l": "k", "k": ["kaggle", "platform", "data", "science", "machine", "learning", "competitions", "founded", "acquired", "google", "hosts", "provides", "datasets", "offers", "collaborative"]}, {"id": "term-kaggle-datasets", "t": "Kaggle Datasets", "tg": ["Platform", "General"], "d": "datasets", "x": "A repository of thousands of publicly available datasets hosted on the Kaggle platform. Covers diverse domains and is...", "l": "k", "k": ["kaggle", "datasets", "repository", "thousands", "publicly", "available", "hosted", "platform", "covers", "diverse", "domains", "widely", "data", "science", "competitions"]}, {"id": "term-kalman-filter", "t": "Kalman Filter", "tg": ["Algorithms", "Fundamentals", "Signal Processing"], "d": "algorithms", "x": "An optimal recursive algorithm that estimates the state of a linear dynamic system from noisy measurements. Combines...", "l": "k", "k": ["kalman", "filter", "optimal", "recursive", "algorithm", "estimates", "state", "linear", "dynamic", "system", "noisy", "measurements", "combines", "predictions", "model"]}, {"id": "term-kandinsky", "t": "Kandinsky", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A text-to-image diffusion model that combines CLIP image embeddings with a latent diffusion prior for generating...", "l": "k", "k": ["kandinsky", "text-to-image", "diffusion", "model", "combines", "clip", "image", "embeddings", "latent", "prior", "generating", "high-quality", "images", "text", "descriptions"]}, {"id": "term-kaplan-meier-estimator", "t": "Kaplan-Meier Estimator", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A non-parametric statistic used to estimate the survival function from time-to-event data, accounting for...", "l": "k", "k": ["kaplan-meier", "estimator", "non-parametric", "statistic", "estimate", "survival", "function", "time-to-event", "data", "accounting", "right-censored", "observations", "produces", "step", "decreasing"]}, {"id": "term-karmarkars-algorithm", "t": "Karmarkar's Algorithm", "tg": ["Algorithms", "Technical", "Optimization", "History"], "d": "algorithms", "x": "A polynomial-time interior point method for linear programming that was a landmark result in optimization theory....", "l": "k", "k": ["karmarkar", "algorithm", "polynomial-time", "interior", "point", "method", "linear", "programming", "landmark", "result", "optimization", "theory", "demonstrated", "methods", "competitive"]}, {"id": "term-kasparov-vs-deep-blue", "t": "Kasparov vs Deep Blue", "tg": ["History", "Milestones"], "d": "history", "x": "The historic 1997 rematch in which IBM's Deep Blue defeated world chess champion Garry Kasparov 3.5 to 2.5, marking the...", "l": "k", "k": ["kasparov", "deep", "blue", "historic", "rematch", "ibm", "defeated", "world", "chess", "champion", "garry", "marking", "time", "computer", "beat"]}, {"id": "term-kendall-tau", "t": "Kendall Tau", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A non-parametric statistic measuring the ordinal association between two rankings, computed from the number of...", "l": "k", "k": ["kendall", "tau", "non-parametric", "statistic", "measuring", "ordinal", "association", "rankings", "computed", "number", "concordant", "discordant", "pairs", "robust", "outliers"]}, {"id": "term-keras", "t": "Keras", "tg": ["Framework", "Deep Learning"], "d": "models", "x": "A high-level neural network API that makes building deep learning models more accessible. Now integrated into...", "l": "k", "k": ["keras", "high-level", "neural", "network", "api", "makes", "building", "deep", "learning", "models", "accessible", "now", "integrated", "tensorflow", "known"]}, {"id": "term-keras-release", "t": "Keras Release", "tg": ["History", "Milestones"], "d": "history", "x": "The initial release of Keras by Francois Chollet in March 2015 as a high-level neural network API. Keras simplified...", "l": "k", "k": ["keras", "release", "initial", "francois", "chollet", "march", "high-level", "neural", "network", "api", "simplified", "deep", "learning", "providing", "intuitive"]}, {"id": "term-kernel-gpu-computing", "t": "Kernel (GPU Computing)", "tg": ["GPU", "Programming", "Fundamentals"], "d": "hardware", "x": "Function that executes on the GPU launched from the host CPU. GPU kernels run across many parallel threads and are the...", "l": "k", "k": ["kernel", "gpu", "computing", "function", "executes", "launched", "host", "cpu", "kernels", "run", "across", "parallel", "threads", "fundamental", "unit"]}, {"id": "term-kernel", "t": "Kernel (ML)", "tg": ["Concept", "Math"], "d": "general", "x": "A function that measures similarity between data points, enabling algorithms like SVMs to work in high-dimensional...", "l": "k", "k": ["kernel", "function", "measures", "similarity", "data", "points", "enabling", "algorithms", "svms", "work", "high-dimensional", "spaces", "common", "kernels", "include"]}, {"id": "term-kernel-auto-tuning", "t": "Kernel Auto-Tuning", "tg": ["Inference Infrastructure", "GPU"], "d": "hardware", "x": "The process of automatically selecting optimal GPU kernel implementations for specific tensor sizes and hardware...", "l": "k", "k": ["kernel", "auto-tuning", "process", "automatically", "selecting", "optimal", "gpu", "implementations", "specific", "tensor", "sizes", "hardware", "configurations", "tests", "multiple"]}, {"id": "term-kernel-density-estimation", "t": "Kernel Density Estimation", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A non-parametric method for estimating the probability density function of a random variable by placing a kernel (e.g.,...", "l": "k", "k": ["kernel", "density", "estimation", "non-parametric", "method", "estimating", "probability", "function", "random", "variable", "placing", "gaussian", "data", "point", "summing"]}, {"id": "term-kernel-methods", "t": "Kernel Methods", "tg": ["History", "Fundamentals"], "d": "history", "x": "A class of algorithms for pattern analysis that use kernel functions to operate in high-dimensional feature spaces...", "l": "k", "k": ["kernel", "methods", "class", "algorithms", "pattern", "analysis", "functions", "operate", "high-dimensional", "feature", "spaces", "without", "explicitly", "computing", "transformation"]}, {"id": "term-kernel-pca-algorithm", "t": "Kernel PCA Algorithm", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "An extension of PCA that uses the kernel trick to perform nonlinear dimensionality reduction. Maps data into a...", "l": "k", "k": ["kernel", "pca", "algorithm", "extension", "uses", "trick", "perform", "nonlinear", "dimensionality", "reduction", "maps", "data", "high-dimensional", "feature", "space"]}, {"id": "term-kernel-trick", "t": "Kernel Trick", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A mathematical technique that implicitly maps data into a high-dimensional feature space by computing inner products...", "l": "k", "k": ["kernel", "trick", "mathematical", "technique", "implicitly", "maps", "data", "high-dimensional", "feature", "space", "computing", "inner", "products", "via", "function"]}, {"id": "term-key-value-cache", "t": "Key-Value Cache", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An optimization for autoregressive transformer inference that stores previously computed key and value tensors to avoid...", "l": "k", "k": ["key-value", "cache", "optimization", "autoregressive", "transformer", "inference", "stores", "previously", "computed", "key", "value", "tensors", "avoid", "redundant", "recomputation"]}, {"id": "term-keypoint-detection", "t": "Keypoint Detection", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of identifying specific anatomical or structural points of interest in an image, such as body joints, facial...", "l": "k", "k": ["keypoint", "detection", "task", "identifying", "specific", "anatomical", "structural", "points", "interest", "image", "body", "joints", "facial", "landmarks", "object"]}, {"id": "term-keyword-extraction", "t": "Keyword Extraction", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of automatically identifying the most important or representative words and phrases in a document, using...", "l": "k", "k": ["keyword", "extraction", "task", "automatically", "identifying", "important", "representative", "words", "phrases", "document", "statistical", "graph-based", "neural", "methods"]}, {"id": "term-keyword-search", "t": "Keyword Search", "tg": ["Retrieval", "Search"], "d": "general", "x": "A traditional information retrieval method that matches documents based on the presence and frequency of query terms,...", "l": "k", "k": ["keyword", "search", "traditional", "information", "retrieval", "method", "matches", "documents", "based", "presence", "frequency", "query", "terms", "algorithms", "bm25"]}, {"id": "term-killswitch", "t": "Killswitch", "tg": ["Safety", "Technical"], "d": "safety", "x": "A mechanism to immediately shut down or constrain an AI system that is behaving unsafely. Designing reliable...", "l": "k", "k": ["killswitch", "mechanism", "immediately", "shut", "down", "constrain", "system", "behaving", "unsafely", "designing", "reliable", "killswitches", "distributed", "autonomous", "systems"]}, {"id": "term-kinetics-400", "t": "Kinetics-400", "tg": ["Benchmark", "Video"], "d": "datasets", "x": "A large-scale video action recognition dataset containing 400 human action classes with at least 400 video clips per...", "l": "k", "k": ["kinetics-400", "large-scale", "video", "action", "recognition", "dataset", "containing", "human", "classes", "least", "clips", "per", "class", "sourced", "youtube"]}, {"id": "term-kinetics-600", "t": "Kinetics-600", "tg": ["Benchmark", "Video"], "d": "datasets", "x": "An expanded version of Kinetics-400 with 600 action classes and approximately 480000 video clips. Provides broader...", "l": "k", "k": ["kinetics-600", "expanded", "version", "kinetics-400", "action", "classes", "approximately", "video", "clips", "provides", "broader", "coverage", "human", "activities", "recognition"]}, {"id": "term-kinetics-700", "t": "Kinetics-700", "tg": ["Benchmark", "Video"], "d": "datasets", "x": "The largest version of the Kinetics series with 700 action classes and over 650000 video clips. Includes trimmed clips...", "l": "k", "k": ["kinetics-700", "largest", "version", "kinetics", "series", "action", "classes", "video", "clips", "includes", "trimmed", "approximately", "seconds", "showing", "human"]}, {"id": "term-kitti", "t": "KITTI", "tg": ["Benchmark", "Computer Vision", "Autonomous Driving"], "d": "datasets", "x": "A benchmark suite for autonomous driving containing stereo imagery optical flow 3D object detection and tracking data...", "l": "k", "k": ["kitti", "benchmark", "suite", "autonomous", "driving", "containing", "stereo", "imagery", "optical", "flow", "object", "detection", "tracking", "data", "captured"]}, {"id": "term-kitti-depth", "t": "KITTI Depth", "tg": ["Benchmark", "Computer Vision", "3D", "Autonomous Driving"], "d": "datasets", "x": "The depth prediction benchmark from the KITTI suite containing sparse depth maps from lidar projected onto camera...", "l": "k", "k": ["kitti", "depth", "prediction", "benchmark", "suite", "containing", "sparse", "maps", "lidar", "projected", "onto", "camera", "images", "standard", "evaluation"]}, {"id": "term-kl-divergence", "t": "KL Divergence", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "Kullback-Leibler divergence, a non-symmetric measure of how one probability distribution differs from a reference...", "l": "k", "k": ["divergence", "kullback-leibler", "non-symmetric", "measure", "probability", "distribution", "differs", "reference", "quantifies", "information", "lost", "approximating", "true", "model"]}, {"id": "term-kla-corporation", "t": "KLA Corporation", "tg": ["Manufacturing", "Equipment", "Company"], "d": "hardware", "x": "American company specializing in semiconductor process control and yield management equipment. Their inspection and...", "l": "k", "k": ["kla", "corporation", "american", "company", "specializing", "semiconductor", "process", "control", "yield", "management", "equipment", "inspection", "measurement", "tools", "help"]}, {"id": "term-kling", "t": "Kling", "tg": ["Models", "Technical"], "d": "models", "x": "A video generation model developed by Kuaishou Technology that produces high-quality realistic videos from text...", "l": "k", "k": ["kling", "video", "generation", "model", "developed", "kuaishou", "technology", "produces", "high-quality", "realistic", "videos", "text", "prompts", "features", "strong"]}, {"id": "term-klue", "t": "KLUE", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "Korean Language Understanding Evaluation a comprehensive benchmark of 8 Korean NLU tasks. Provides standardized...", "l": "k", "k": ["klue", "korean", "language", "understanding", "evaluation", "comprehensive", "benchmark", "nlu", "tasks", "provides", "standardized", "model", "development"]}, {"id": "term-kmnist", "t": "KMNIST", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "Kuzushiji-MNIST a dataset of 70000 28x28 grayscale images of cursive Japanese Kuzushiji characters. Created to preserve...", "l": "k", "k": ["kmnist", "kuzushiji-mnist", "dataset", "28x28", "grayscale", "images", "cursive", "japanese", "kuzushiji", "characters", "created", "preserve", "historical", "documents", "machine"]}, {"id": "term-kmp-algorithm", "t": "KMP Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP", "Searching"], "d": "algorithms", "x": "The Knuth-Morris-Pratt string-searching algorithm uses a failure function to avoid re-examining previously matched...", "l": "k", "k": ["kmp", "algorithm", "knuth-morris-pratt", "string-searching", "uses", "failure", "function", "avoid", "re-examining", "previously", "matched", "characters", "achieves", "worst-case", "time"]}, {"id": "term-know-your-customer-for-ai", "t": "Know Your Customer for AI", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Proposed regulatory requirements for AI cloud providers and model distributors to verify the identity and intended use...", "l": "k", "k": ["know", "customer", "proposed", "regulatory", "requirements", "cloud", "providers", "model", "distributors", "verify", "identity", "intended", "customers", "accessing", "powerful"]}, {"id": "term-knowledge-acquisition-bottleneck", "t": "Knowledge Acquisition Bottleneck", "tg": ["History", "Fundamentals"], "d": "history", "x": "A fundamental challenge in expert systems development referring to the difficulty and expense of extracting knowledge...", "l": "k", "k": ["knowledge", "acquisition", "bottleneck", "fundamental", "challenge", "expert", "systems", "development", "referring", "difficulty", "expense", "extracting", "human", "experts", "encoding"]}, {"id": "term-knowledge-cutoff", "t": "Knowledge Cutoff", "tg": ["Limitation", "LLM"], "d": "models", "x": "The date beyond which an AI model has no training data. Events after this date are unknown to the model unless provided...", "l": "k", "k": ["knowledge", "cutoff", "date", "beyond", "model", "training", "data", "events", "unknown", "unless", "provided", "context", "retrieval", "augmentation"]}, {"id": "term-knowledge-distillation", "t": "Knowledge Distillation", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A model compression technique where a smaller student model is trained to mimic the outputs of a larger teacher model....", "l": "k", "k": ["knowledge", "distillation", "model", "compression", "technique", "smaller", "student", "trained", "mimic", "outputs", "larger", "teacher", "soft", "probability", "distribution"]}, {"id": "term-knowledge-distillation-efficiency", "t": "Knowledge Distillation for Efficiency", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A model compression technique where a smaller student model is trained to mimic the outputs (soft predictions) of a...", "l": "k", "k": ["knowledge", "distillation", "efficiency", "model", "compression", "technique", "smaller", "student", "trained", "mimic", "outputs", "soft", "predictions", "larger", "teacher"]}, {"id": "term-knowledge-distillation-vision", "t": "Knowledge Distillation for Vision", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of training a compact student vision model to mimic the predictions and feature representations of a larger...", "l": "k", "k": ["knowledge", "distillation", "vision", "process", "training", "compact", "student", "model", "mimic", "predictions", "feature", "representations", "larger", "teacher", "enabling"]}, {"id": "term-knowledge-distillation-loss", "t": "Knowledge Distillation Loss", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A training objective where a smaller student model learns to match the soft probability distributions produced by a...", "l": "k", "k": ["knowledge", "distillation", "loss", "training", "objective", "smaller", "student", "model", "learns", "match", "soft", "probability", "distributions", "produced", "larger"]}, {"id": "term-knowledge-engineering", "t": "Knowledge Engineering", "tg": ["History", "Fundamentals"], "d": "history", "x": "The discipline of integrating knowledge into computer systems to solve complex problems normally requiring human...", "l": "k", "k": ["knowledge", "engineering", "discipline", "integrating", "computer", "systems", "solve", "complex", "problems", "normally", "requiring", "human", "expertise", "key", "practice"]}, {"id": "term-knowledge-graph", "t": "Knowledge Graph", "tg": ["Data Structure", "Knowledge"], "d": "general", "x": "A structured representation of facts as interconnected entities and relationships. Used to enhance AI systems with...", "l": "k", "k": ["knowledge", "graph", "structured", "representation", "facts", "interconnected", "entities", "relationships", "enhance", "systems", "factual", "enable", "reasoning", "data"]}, {"id": "term-knowledge-representation", "t": "Knowledge Representation", "tg": ["History", "Milestones"], "d": "history", "x": "The field within AI concerned with how information about the world can be formally represented in a form that computer...", "l": "k", "k": ["knowledge", "representation", "field", "within", "concerned", "information", "world", "formally", "represented", "form", "computer", "systems", "reasoning", "planning", "problem-solving"]}, {"id": "term-kolmogorov-complexity", "t": "Kolmogorov Complexity", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "The length of the shortest computer program that produces a given string as output, representing the intrinsic...", "l": "k", "k": ["kolmogorov", "complexity", "length", "shortest", "computer", "program", "produces", "given", "string", "output", "representing", "intrinsic", "information", "content", "uncomputable"]}, {"id": "term-kolmogorov-arnold-network", "t": "Kolmogorov-Arnold Network", "tg": ["Models", "Technical"], "d": "models", "x": "A neural network architecture based on the Kolmogorov-Arnold representation theorem that places learnable activation...", "l": "k", "k": ["kolmogorov-arnold", "network", "neural", "architecture", "based", "representation", "theorem", "places", "learnable", "activation", "functions", "edges", "rather", "nodes", "weight"]}, {"id": "term-kolmogorov-smirnov-test", "t": "Kolmogorov-Smirnov Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A non-parametric test that compares a sample distribution with a reference distribution (one-sample) or compares two...", "l": "k", "k": ["kolmogorov-smirnov", "test", "non-parametric", "compares", "sample", "distribution", "reference", "one-sample", "distributions", "two-sample", "maximum", "absolute", "difference", "cumulative", "functions"]}, {"id": "term-kolors", "t": "Kolors", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A large-scale text-to-image model from Kuaishou Technology with strong photorealistic generation capabilities and...", "l": "k", "k": ["kolors", "large-scale", "text-to-image", "model", "kuaishou", "technology", "strong", "photorealistic", "generation", "capabilities", "fine-grained", "chinese-english", "text", "understanding"]}, {"id": "term-kornli", "t": "KorNLI", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "Korean Natural Language Inference a benchmark for evaluating Korean language understanding. Tests textual entailment...", "l": "k", "k": ["kornli", "korean", "natural", "language", "inference", "benchmark", "evaluating", "understanding", "tests", "textual", "entailment", "semantic", "similarity", "text"]}, {"id": "term-kosarajus-algorithm", "t": "Kosaraju's Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A two-pass algorithm for finding strongly connected components in a directed graph. First performs a DFS to compute...", "l": "k", "k": ["kosaraju", "algorithm", "two-pass", "finding", "strongly", "connected", "components", "directed", "graph", "performs", "dfs", "compute", "finish", "times", "runs"]}, {"id": "term-kosmos-2", "t": "Kosmos-2", "tg": ["Models", "Technical"], "d": "models", "x": "A multimodal large language model by Microsoft that can ground text to the visual world. Combines language...", "l": "k", "k": ["kosmos-2", "multimodal", "large", "language", "model", "microsoft", "ground", "text", "visual", "world", "combines", "understanding", "spatial", "awareness", "enabling"]}, {"id": "term-krippendorffs-alpha", "t": "Krippendorff's Alpha", "tg": ["Evaluation", "Methodology"], "d": "datasets", "x": "A versatile reliability coefficient that measures agreement among multiple annotators for any number of raters,...", "l": "k", "k": ["krippendorff", "alpha", "versatile", "reliability", "coefficient", "measures", "agreement", "among", "multiple", "annotators", "number", "raters", "variable", "scales", "missing"]}, {"id": "term-krl", "t": "KRL", "tg": ["History", "Systems"], "d": "history", "x": "The Knowledge Representation Language developed by Daniel Bobrow and Terry Winograd at Xerox PARC in the mid-1970s. KRL...", "l": "k", "k": ["krl", "knowledge", "representation", "language", "developed", "daniel", "bobrow", "terry", "winograd", "xerox", "parc", "mid-1970s", "combined", "aspects", "frames"]}, {"id": "term-kruskals-algorithm", "t": "Kruskal's Algorithm", "tg": ["Algorithms", "Fundamentals", "Graph"], "d": "algorithms", "x": "A greedy algorithm that finds a minimum spanning tree for a connected weighted undirected graph by sorting edges by...", "l": "k", "k": ["kruskal", "algorithm", "greedy", "finds", "minimum", "spanning", "tree", "connected", "weighted", "undirected", "graph", "sorting", "edges", "weight", "adding"]}, {"id": "term-krylov-subspace-method", "t": "Krylov Subspace Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A class of iterative methods that approximate solutions to linear systems using vectors from the Krylov subspace....", "l": "k", "k": ["krylov", "subspace", "method", "class", "iterative", "methods", "approximate", "solutions", "linear", "systems", "vectors", "includes", "conjugate", "gradient", "gmres"]}, {"id": "term-kto", "t": "KTO", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Kahneman-Tversky Optimization, a preference learning method that trains language models using binary feedback...", "l": "k", "k": ["kto", "kahneman-tversky", "optimization", "preference", "learning", "method", "trains", "language", "models", "binary", "feedback", "good", "bad", "rather", "pairwise"]}, {"id": "term-kubernetes-for-gpu", "t": "Kubernetes for GPU", "tg": ["Virtualization", "Container", "Orchestration"], "d": "hardware", "x": "Orchestrating GPU-accelerated AI workloads using Kubernetes container management. The NVIDIA GPU Operator automates GPU...", "l": "k", "k": ["kubernetes", "gpu", "orchestrating", "gpu-accelerated", "workloads", "container", "management", "nvidia", "operator", "automates", "driver", "plugin", "deployment", "across", "clusters"]}, {"id": "term-kuhns-algorithm", "t": "Kuhn's Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm for finding maximum matching in bipartite graphs using augmenting paths. Simpler to implement than...", "l": "k", "k": ["kuhn", "algorithm", "finding", "maximum", "matching", "bipartite", "graphs", "augmenting", "paths", "simpler", "implement", "hopcroft-karp", "runs", "time", "making"]}, {"id": "term-kv-cache", "t": "KV Cache (Key-Value Cache)", "tg": ["Optimization", "Technical"], "d": "algorithms", "x": "An optimization that stores previously computed key and value vectors in transformer models. Speeds up autoregressive...", "l": "k", "k": ["cache", "key-value", "optimization", "stores", "previously", "computed", "key", "value", "vectors", "transformer", "models", "speeds", "autoregressive", "generation", "avoiding"]}, {"id": "term-kv-cache-compression", "t": "KV Cache Compression", "tg": ["LLM", "Inference"], "d": "models", "x": "Methods for reducing the memory footprint of key-value caches during autoregressive generation, including quantization...", "l": "k", "k": ["cache", "compression", "methods", "reducing", "memory", "footprint", "key-value", "caches", "autoregressive", "generation", "including", "quantization", "cached", "values", "eviction"]}, {"id": "term-kv-cache-optimization", "t": "KV Cache Optimization", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Techniques for reducing the memory footprint and access cost of the key-value cache in transformer inference, including...", "l": "k", "k": ["cache", "optimization", "techniques", "reducing", "memory", "footprint", "access", "cost", "key-value", "transformer", "inference", "including", "quantized", "caches", "multi-query"]}, {"id": "term-l-bfgs", "t": "L-BFGS", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Limited-memory Broyden-Fletcher-Goldfarb-Shanno is a quasi-Newton optimization method that approximates the inverse...", "l": "l", "k": ["l-bfgs", "limited-memory", "broyden-fletcher-goldfarb-shanno", "quasi-newton", "optimization", "method", "approximates", "inverse", "hessian", "limited", "history", "gradient", "updates", "widely", "traditional"]}, {"id": "term-l-eval", "t": "L-Eval", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A comprehensive long-context evaluation benchmark containing diverse tasks with inputs ranging from 3000 to 60000...", "l": "l", "k": ["l-eval", "comprehensive", "long-context", "evaluation", "benchmark", "containing", "diverse", "tasks", "inputs", "ranging", "words", "tests", "summarization", "reasoning", "long"]}, {"id": "term-l1-regularization", "t": "L1 Regularization", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A regularization technique that adds the sum of absolute values of model weights to the loss function, encouraging...", "l": "l", "k": ["regularization", "technique", "adds", "sum", "absolute", "values", "model", "weights", "loss", "function", "encouraging", "sparsity", "driving", "exactly", "zero"]}, {"id": "term-l2-regularization", "t": "L2 Regularization", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A regularization technique that adds the sum of squared model weights to the loss function, penalizing large weights...", "l": "l", "k": ["regularization", "technique", "adds", "sum", "squared", "model", "weights", "loss", "function", "penalizing", "large", "encouraging", "small", "exactly", "zero"]}, {"id": "term-label", "t": "Label", "tg": ["Data", "Supervised Learning"], "d": "general", "x": "The correct answer or category associated with training data in supervised learning. Human-provided labels teach models...", "l": "l", "k": ["label", "correct", "answer", "category", "associated", "training", "data", "supervised", "learning", "human-provided", "labels", "teach", "models", "patterns", "learn"]}, {"id": "term-label-bias", "t": "Label Bias", "tg": ["Safety", "Technical"], "d": "safety", "x": "Bias introduced into AI systems through inaccurate or subjective labels in training data. Human annotators may apply...", "l": "l", "k": ["label", "bias", "introduced", "systems", "inaccurate", "subjective", "labels", "training", "data", "human", "annotators", "apply", "inconsistently", "based", "biases"]}, {"id": "term-label-encoding", "t": "Label Encoding", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A technique that converts categorical values into integer codes, assigning each unique category a distinct numerical...", "l": "l", "k": ["label", "encoding", "technique", "converts", "categorical", "values", "integer", "codes", "assigning", "unique", "category", "distinct", "numerical", "identifier", "introduces"]}, {"id": "term-label-propagation", "t": "Label Propagation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A semi-supervised graph algorithm that propagates labels from labeled nodes to unlabeled nodes through the graph...", "l": "l", "k": ["label", "propagation", "semi-supervised", "graph", "algorithm", "propagates", "labels", "labeled", "nodes", "unlabeled", "structure", "node", "adopts", "common", "among"]}, {"id": "term-label-smoothing", "t": "Label Smoothing", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A regularization technique that replaces hard one-hot target labels with soft labels that assign a small probability to...", "l": "l", "k": ["label", "smoothing", "regularization", "technique", "replaces", "hard", "one-hot", "target", "labels", "soft", "assign", "small", "probability", "incorrect", "classes"]}, {"id": "term-label-smoothing-regularization", "t": "Label Smoothing Regularization", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A regularization technique that replaces hard one-hot target labels with soft labels distributing a small amount of...", "l": "l", "k": ["label", "smoothing", "regularization", "technique", "replaces", "hard", "one-hot", "target", "labels", "soft", "distributing", "small", "amount", "probability", "mass"]}, {"id": "term-lag-llama", "t": "Lag-Llama", "tg": ["Models", "Technical"], "d": "models", "x": "A foundation model for probabilistic time series forecasting that uses a Llama-based architecture with lag features for...", "l": "l", "k": ["lag-llama", "foundation", "model", "probabilistic", "time", "series", "forecasting", "uses", "llama-based", "architecture", "lag", "features", "zero-shot", "few-shot", "prediction"]}, {"id": "term-lagrangian-relaxation", "t": "Lagrangian Relaxation", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An optimization technique that relaxes hard constraints by incorporating them into the objective function with Lagrange...", "l": "l", "k": ["lagrangian", "relaxation", "optimization", "technique", "relaxes", "hard", "constraints", "incorporating", "objective", "function", "lagrange", "multipliers", "provides", "bounds", "optimal"]}, {"id": "term-laion", "t": "LAION", "tg": ["History", "Organizations"], "d": "history", "x": "The Large-scale Artificial Intelligence Open Network a nonprofit organization that created LAION-5B one of the largest...", "l": "l", "k": ["laion", "large-scale", "artificial", "intelligence", "open", "network", "nonprofit", "organization", "created", "laion-5b", "largest", "openly", "available", "image-text", "datasets"]}, {"id": "term-laion-400m", "t": "LAION-400M", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "A dataset of 400 million image-text pairs from Common Crawl filtered using CLIP similarity scores. Used for training...", "l": "l", "k": ["laion-400m", "dataset", "million", "image-text", "pairs", "common", "crawl", "filtered", "clip", "similarity", "scores", "training", "open-source", "vision-language", "image"]}, {"id": "term-laion-5b", "t": "LAION-5B", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "A dataset of 5.85 billion image-text pairs filtered from Common Crawl. One of the largest publicly available datasets...", "l": "l", "k": ["laion-5b", "dataset", "billion", "image-text", "pairs", "filtered", "common", "crawl", "largest", "publicly", "available", "datasets", "training", "multimodal", "models"]}, {"id": "term-laion-aesthetics", "t": "LAION-Aesthetics", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "A subset of LAION-5B filtered for aesthetic quality using a learned aesthetic predictor. Used to train image generation...", "l": "l", "k": ["laion-aesthetics", "subset", "laion-5b", "filtered", "aesthetic", "quality", "learned", "predictor", "train", "image", "generation", "models", "produce", "visually", "appealing"]}, {"id": "term-lam-research", "t": "Lam Research", "tg": ["Manufacturing", "Equipment", "Company"], "d": "hardware", "x": "American company manufacturing semiconductor fabrication equipment specializing in etching deposition and cleaning....", "l": "l", "k": ["lam", "research", "american", "company", "manufacturing", "semiconductor", "fabrication", "equipment", "specializing", "etching", "deposition", "cleaning", "tools", "essential", "producing"]}, {"id": "term-lamb", "t": "LAMB", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Layer-wise Adaptive Moments optimizer designed for large-batch distributed training. Extends LARS with Adam-style...", "l": "l", "k": ["lamb", "layer-wise", "adaptive", "moments", "optimizer", "designed", "large-batch", "distributed", "training", "extends", "lars", "adam-style", "momentum", "train", "bert"]}, {"id": "term-lamb-optimizer", "t": "LAMB Optimizer", "tg": ["Model Optimization", "Distributed Computing"], "d": "models", "x": "Layer-wise Adaptive Moments optimizer for Batch training, a variant of Adam that applies layer-wise learning rate...", "l": "l", "k": ["lamb", "optimizer", "layer-wise", "adaptive", "moments", "batch", "training", "variant", "adam", "applies", "learning", "rate", "adaptation", "enable", "stable"]}, {"id": "term-lambada", "t": "LAMBADA", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Language Modeling Broadened to Account for Discourse Aspects a dataset testing the ability of language models to...", "l": "l", "k": ["lambada", "language", "modeling", "broadened", "account", "discourse", "aspects", "dataset", "testing", "ability", "models", "predict", "final", "word", "passages"]}, {"id": "term-lambda-calculus", "t": "Lambda Calculus", "tg": ["History", "Fundamentals"], "d": "history", "x": "A formal system in mathematical logic for expressing computation based on function abstraction and application...", "l": "l", "k": ["lambda", "calculus", "formal", "system", "mathematical", "logic", "expressing", "computation", "based", "function", "abstraction", "application", "developed", "alonzo", "church"]}, {"id": "term-lancedb", "t": "LanceDB", "tg": ["Vector Database", "Open Source"], "d": "general", "x": "An open-source serverless vector database built on the Lance columnar data format, supporting multimodal data storage...", "l": "l", "k": ["lancedb", "open-source", "serverless", "vector", "database", "built", "lance", "columnar", "data", "format", "supporting", "multimodal", "storage", "embedded", "cloud-native"]}, {"id": "term-lanczos-algorithm", "t": "Lanczos Algorithm", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A Krylov subspace method for computing eigenvalues and eigenvectors of large symmetric matrices. Produces a tridiagonal...", "l": "l", "k": ["lanczos", "algorithm", "krylov", "subspace", "method", "computing", "eigenvalues", "eigenvectors", "large", "symmetric", "matrices", "produces", "tridiagonal", "matrix", "whose"]}, {"id": "term-lane-detection", "t": "Lane Detection", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of identifying and localizing lane markings on road surfaces in driving images or video, using curve fitting,...", "l": "l", "k": ["lane", "detection", "task", "identifying", "localizing", "markings", "road", "surfaces", "driving", "images", "video", "curve", "fitting", "segmentation", "anchor-based"]}, {"id": "term-langchain", "t": "LangChain", "tg": ["Framework", "Application"], "d": "general", "x": "A popular framework for building applications with LLMs. Provides abstractions for chains, agents, memory, and tool...", "l": "l", "k": ["langchain", "popular", "framework", "building", "applications", "llms", "provides", "abstractions", "chains", "agents", "memory", "tool", "simplifying", "complex", "application"]}, {"id": "term-langevin-dynamics", "t": "Langevin Dynamics", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A stochastic process that uses gradient information with added noise to sample from a probability distribution. Used in...", "l": "l", "k": ["langevin", "dynamics", "stochastic", "process", "uses", "gradient", "information", "added", "noise", "sample", "probability", "distribution", "score-based", "generative", "models"]}, {"id": "term-language-identification", "t": "Language Identification", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of automatically determining what natural language a given text is written in, using features like character...", "l": "l", "k": ["language", "identification", "task", "automatically", "determining", "natural", "given", "text", "written", "features", "character", "n-grams", "word", "frequency", "patterns"]}, {"id": "term-language-model-perplexity", "t": "Language Model Perplexity", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "A metric for evaluating language models that measures how well the model predicts a held-out test set. Defined as the...", "l": "l", "k": ["language", "model", "perplexity", "metric", "evaluating", "models", "measures", "predicts", "held-out", "test", "defined", "exponential", "average", "negative", "log-likelihood"]}, {"id": "term-language-modeling", "t": "Language Modeling", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of learning a probability distribution over sequences of tokens, enabling the model to estimate the likelihood...", "l": "l", "k": ["language", "modeling", "task", "learning", "probability", "distribution", "sequences", "tokens", "enabling", "model", "estimate", "likelihood", "given", "text", "sequence"]}, {"id": "term-language-understanding", "t": "Language Understanding (NLU)", "tg": ["NLP", "Capability"], "d": "general", "x": "AI capability to comprehend the meaning, intent, and context of human language. Includes parsing structure, resolving...", "l": "l", "k": ["language", "understanding", "nlu", "capability", "comprehend", "meaning", "intent", "context", "human", "includes", "parsing", "structure", "resolving", "references", "implicit"]}, {"id": "term-laplace-approximation", "t": "Laplace Approximation", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A technique for approximating a posterior distribution with a Gaussian centered at the mode (MAP estimate), using the...", "l": "l", "k": ["laplace", "approximation", "technique", "approximating", "posterior", "distribution", "gaussian", "centered", "mode", "map", "estimate", "curvature", "log-posterior", "hessian", "determine"]}, {"id": "term-laplace-mechanism", "t": "Laplace Mechanism", "tg": ["Algorithms", "Technical", "Privacy"], "d": "algorithms", "x": "A differential privacy mechanism that adds noise drawn from the Laplace distribution calibrated to the sensitivity of...", "l": "l", "k": ["laplace", "mechanism", "differential", "privacy", "adds", "noise", "drawn", "distribution", "calibrated", "sensitivity", "query", "provides", "pure", "epsilon-differential", "numerical"]}, {"id": "term-laplacian-eigenmaps-algorithm", "t": "Laplacian Eigenmaps Algorithm", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A spectral method for nonlinear dimensionality reduction that uses the graph Laplacian to preserve local neighborhood...", "l": "l", "k": ["laplacian", "eigenmaps", "algorithm", "spectral", "method", "nonlinear", "dimensionality", "reduction", "uses", "graph", "preserve", "local", "neighborhood", "distances", "finds"]}, {"id": "term-laplacian-of-gaussian", "t": "Laplacian of Gaussian", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An edge detection operator that applies a Gaussian blur followed by the Laplace operator to detect edges at a specific...", "l": "l", "k": ["laplacian", "gaussian", "edge", "detection", "operator", "applies", "blur", "followed", "laplace", "detect", "edges", "specific", "scale", "zero-crossings", "output"]}, {"id": "term-large-batch-training", "t": "Large Batch Training", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "Techniques for training neural networks with very large batch sizes (thousands to millions of samples) distributed...", "l": "l", "k": ["large", "batch", "training", "techniques", "neural", "networks", "sizes", "thousands", "millions", "samples", "distributed", "across", "gpus", "requires", "careful"]}, {"id": "term-llm", "t": "Large Language Model (LLM)", "tg": ["Model Type", "Core Concept"], "d": "models", "x": "An AI system trained on massive amounts of text data to understand and generate human language. Includes models like...", "l": "l", "k": ["large", "language", "model", "llm", "system", "trained", "massive", "amounts", "text", "data", "understand", "generate", "human", "includes", "models"]}, {"id": "term-large-margin-nearest-neighbor", "t": "Large-Margin Nearest Neighbor", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A metric learning algorithm that learns a Mahalanobis distance metric for k-nearest-neighbor classification. Optimizes...", "l": "l", "k": ["large-margin", "nearest", "neighbor", "metric", "learning", "algorithm", "learns", "mahalanobis", "distance", "k-nearest-neighbor", "classification", "optimizes", "pull", "same-class", "neighbors"]}, {"id": "term-lars", "t": "LARS", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Layer-wise Adaptive Rate Scaling adjusts the learning rate for each layer based on the ratio of weight norm to gradient...", "l": "l", "k": ["lars", "layer-wise", "adaptive", "rate", "scaling", "adjusts", "learning", "layer", "based", "ratio", "weight", "norm", "gradient", "enables", "training"]}, {"id": "term-lars-optimizer", "t": "LARS Optimizer", "tg": ["Model Optimization", "Distributed Computing"], "d": "models", "x": "Layer-wise Adaptive Rate Scaling, an optimizer that adjusts the learning rate per layer based on the ratio of weight...", "l": "l", "k": ["lars", "optimizer", "layer-wise", "adaptive", "rate", "scaling", "adjusts", "learning", "per", "layer", "based", "ratio", "weight", "norm", "gradient"]}, {"id": "term-lasso-regression", "t": "Lasso Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A linear regression method that applies L1 regularization to the coefficient estimates, performing both variable...", "l": "l", "k": ["lasso", "regression", "linear", "method", "applies", "regularization", "coefficient", "estimates", "performing", "variable", "selection", "driving", "coefficients", "exactly", "zero"]}, {"id": "term-lastletterconcat", "t": "LastLetterConcat", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A benchmark testing the ability of language models to concatenate the last letters of a sequence of words. Tests basic...", "l": "l", "k": ["lastletterconcat", "benchmark", "testing", "ability", "language", "models", "concatenate", "last", "letters", "sequence", "words", "tests", "basic", "symbolic", "manipulation"]}, {"id": "term-late-chunking", "t": "Late Chunking", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A technique that first encodes an entire document through a long-context embedding model and then pools token...", "l": "l", "k": ["late", "chunking", "technique", "encodes", "entire", "document", "long-context", "embedding", "model", "pools", "token", "embeddings", "chunk-level", "representations", "preserving"]}, {"id": "term-late-interaction", "t": "Late Interaction", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "A neural retrieval paradigm where queries and documents are independently encoded into sets of token-level embeddings,...", "l": "l", "k": ["late", "interaction", "neural", "retrieval", "paradigm", "queries", "documents", "independently", "encoded", "sets", "token-level", "embeddings", "relevance", "computed", "lightweight"]}, {"id": "term-latency", "t": "Latency", "tg": ["Performance", "Metrics"], "d": "datasets", "x": "The time delay between sending a prompt and receiving a response. Affected by model size, server load, prompt...", "l": "l", "k": ["latency", "time", "delay", "sending", "prompt", "receiving", "response", "affected", "model", "size", "server", "load", "complexity", "output", "length"]}, {"id": "term-latent-diffusion-model", "t": "Latent Diffusion Model", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A diffusion model that operates in the latent space of a pretrained autoencoder rather than pixel space, significantly...", "l": "l", "k": ["latent", "diffusion", "model", "operates", "space", "pretrained", "autoencoder", "rather", "pixel", "significantly", "reducing", "computational", "requirements", "maintaining", "generation"]}, {"id": "term-latent-dirichlet-allocation", "t": "Latent Dirichlet Allocation", "tg": ["Machine Learning", "Probability"], "d": "algorithms", "x": "A generative probabilistic model for topic modeling that represents each document as a mixture of topics and each topic...", "l": "l", "k": ["latent", "dirichlet", "allocation", "generative", "probabilistic", "model", "topic", "modeling", "represents", "document", "mixture", "topics", "distribution", "words", "priors"]}, {"id": "term-latent-semantic-analysis", "t": "Latent Semantic Analysis", "tg": ["Algorithms", "Fundamentals", "NLP"], "d": "algorithms", "x": "A technique that applies SVD to a term-document matrix to discover latent semantic relationships between words and...", "l": "l", "k": ["latent", "semantic", "analysis", "technique", "applies", "svd", "term-document", "matrix", "discover", "relationships", "words", "documents", "reduces", "dimensionality", "capture"]}, {"id": "term-latin-hypercube-sampling", "t": "Latin Hypercube Sampling", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A stratified sampling technique that divides each dimension into equal-probability intervals and ensures each interval...", "l": "l", "k": ["latin", "hypercube", "sampling", "stratified", "technique", "divides", "dimension", "equal-probability", "intervals", "ensures", "interval", "sampled", "exactly", "achieving", "coverage"]}, {"id": "term-lauragpt", "t": "LauraGPT", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "A versatile large language model for audio that handles speech recognition and translation and synthesis and audio...", "l": "l", "k": ["lauragpt", "versatile", "large", "language", "model", "audio", "handles", "speech", "recognition", "translation", "synthesis", "captioning", "unified", "architecture"]}, {"id": "term-lave", "t": "LAVE", "tg": ["Evaluation", "Multimodal"], "d": "datasets", "x": "Language-Aligned Visual Evaluation a methodology for evaluating generated images using language-vision models. Provides...", "l": "l", "k": ["lave", "language-aligned", "visual", "evaluation", "methodology", "evaluating", "generated", "images", "language-vision", "models", "provides", "automated", "assessment", "text-image", "alignment"]}, {"id": "term-lavis", "t": "LAVIS", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A unified library and collection of vision-language pre-trained models from Salesforce for diverse multimodal tasks...", "l": "l", "k": ["lavis", "unified", "library", "collection", "vision-language", "pre-trained", "models", "salesforce", "diverse", "multimodal", "tasks", "including", "visual", "captioning"]}, {"id": "term-law-of-large-numbers", "t": "Law of Large Numbers", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A theorem stating that as the number of independent trials increases, the sample average converges to the expected...", "l": "l", "k": ["law", "large", "numbers", "theorem", "stating", "number", "independent", "trials", "increases", "sample", "average", "converges", "expected", "value", "provides"]}, {"id": "term-lawrence-fogel", "t": "Lawrence Fogel", "tg": ["History", "Pioneers"], "d": "history", "x": "American engineer who introduced evolutionary programming in 1966 as a method for generating AI through simulated...", "l": "l", "k": ["lawrence", "fogel", "american", "engineer", "introduced", "evolutionary", "programming", "method", "generating", "simulated", "evolution", "along", "john", "holland", "ingo"]}, {"id": "term-layer-freezing", "t": "Layer Freezing", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A fine-tuning strategy that keeps some layers of a pretrained model fixed while only updating others. Typically earlier...", "l": "l", "k": ["layer", "freezing", "fine-tuning", "strategy", "keeps", "layers", "pretrained", "model", "fixed", "updating", "others", "typically", "earlier", "capturing", "general"]}, {"id": "term-layer-normalization", "t": "Layer Normalization", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A normalization technique that computes mean and variance across all features within a single training example rather...", "l": "l", "k": ["layer", "normalization", "technique", "computes", "mean", "variance", "across", "features", "within", "single", "training", "example", "rather", "batch", "making"]}, {"id": "term-layer-wise-learning-rate-decay", "t": "Layer-wise Learning Rate Decay", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A technique that applies progressively smaller learning rates to earlier layers of a neural network during fine-tuning....", "l": "l", "k": ["layer-wise", "learning", "rate", "decay", "technique", "applies", "progressively", "smaller", "rates", "earlier", "layers", "neural", "network", "fine-tuning", "based"]}, {"id": "term-layout-analysis", "t": "Layout Analysis", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of detecting and classifying structural elements in document images (headers, paragraphs, tables, figures),...", "l": "l", "k": ["layout", "analysis", "process", "detecting", "classifying", "structural", "elements", "document", "images", "headers", "paragraphs", "tables", "figures", "establishing", "reading"]}, {"id": "term-layoutlm", "t": "LayoutLM", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A pre-trained model from Microsoft that jointly models text and layout information from scanned documents for document...", "l": "l", "k": ["layoutlm", "pre-trained", "model", "microsoft", "jointly", "models", "text", "layout", "information", "scanned", "documents", "document", "understanding", "tasks"]}, {"id": "term-layoutlmv2", "t": "LayoutLMv2", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multimodal pre-training approach for document AI that adds visual features and spatial-aware self-attention to the...", "l": "l", "k": ["layoutlmv2", "multimodal", "pre-training", "approach", "document", "adds", "visual", "features", "spatial-aware", "self-attention", "original", "layoutlm", "architecture"]}, {"id": "term-layoutlmv3", "t": "LayoutLMv3", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A unified multimodal model for document AI that uses word-patch alignment and masked image modeling for pre-training...", "l": "l", "k": ["layoutlmv3", "unified", "multimodal", "model", "document", "uses", "word-patch", "alignment", "masked", "image", "modeling", "pre-training", "without", "requiring", "pre-extracted"]}, {"id": "term-lcm", "t": "LCM", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "Latent Consistency Model is a distilled diffusion model that enables high-quality image generation in very few...", "l": "l", "k": ["lcm", "latent", "consistency", "model", "distilled", "diffusion", "enables", "high-quality", "image", "generation", "inference", "steps", "distillation"]}, {"id": "term-ldpc-decoding-algorithm", "t": "LDPC Decoding Algorithm", "tg": ["Algorithms", "Technical", "Information Theory"], "d": "algorithms", "x": "An algorithm for decoding low-density parity-check codes using iterative message passing on the factor graph. Achieves...", "l": "l", "k": ["ldpc", "decoding", "algorithm", "low-density", "parity-check", "codes", "iterative", "message", "passing", "factor", "graph", "achieves", "near-shannon-limit", "performance", "wifi"]}, {"id": "term-le-chat-mistral", "t": "Le Chat Mistral", "tg": ["Models", "Technical", "NLP", "Products"], "d": "models", "x": "The conversational AI assistant interface from Mistral AI powered by their language models for interactive dialogue and...", "l": "l", "k": ["chat", "mistral", "conversational", "assistant", "interface", "powered", "language", "models", "interactive", "dialogue", "task", "completion"]}, {"id": "term-leaky-relu", "t": "Leaky ReLU", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A variant of ReLU that allows a small non-zero gradient when the input is negative. Defined as f(x) = x if x > 0 and...", "l": "l", "k": ["leaky", "relu", "variant", "allows", "small", "non-zero", "gradient", "input", "negative", "defined", "alpha", "otherwise", "typically", "addresses", "dying"]}, {"id": "term-leapfrog-integration", "t": "Leapfrog Integration", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A numerical method for solving differential equations that updates positions and velocities at interleaved time points....", "l": "l", "k": ["leapfrog", "integration", "numerical", "method", "solving", "differential", "equations", "updates", "positions", "velocities", "interleaved", "time", "points", "symplectic", "time-reversible"]}, {"id": "term-learned-positional-embedding", "t": "Learned Positional Embedding", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A trainable embedding table that assigns a learnable vector to each position in a sequence, allowing the model to...", "l": "l", "k": ["learned", "positional", "embedding", "trainable", "table", "assigns", "learnable", "vector", "position", "sequence", "allowing", "model", "discover", "optimal", "representations"]}, {"id": "term-learning-curve", "t": "Learning Curve", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A plot showing model performance as a function of training set size or training iterations. It reveals whether a model...", "l": "l", "k": ["learning", "curve", "plot", "showing", "model", "performance", "function", "training", "size", "iterations", "reveals", "suffers", "high", "bias", "underfitting"]}, {"id": "term-learning-rate", "t": "Learning Rate", "tg": ["Hyperparameter", "Training"], "d": "general", "x": "A hyperparameter controlling how much model weights are adjusted during training. Too high causes instability; too low...", "l": "l", "k": ["learning", "rate", "hyperparameter", "controlling", "model", "weights", "adjusted", "training", "high", "causes", "instability", "low", "slow", "scheduled", "decrease"]}, {"id": "term-learning-rate-schedule", "t": "Learning Rate Schedule", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A predefined strategy for adjusting the learning rate during training, such as step decay, exponential decay, or cosine...", "l": "l", "k": ["learning", "rate", "schedule", "predefined", "strategy", "adjusting", "training", "step", "decay", "exponential", "cosine", "annealing", "properly", "tuned", "schedules"]}, {"id": "term-learning-rate-warmup", "t": "Learning Rate Warmup", "tg": ["Model Optimization", "Distributed Computing"], "d": "models", "x": "A training technique that gradually increases the learning rate from near-zero to the target value over the first...", "l": "l", "k": ["learning", "rate", "warmup", "training", "technique", "gradually", "increases", "near-zero", "target", "value", "portion", "stabilizes", "dynamics", "large", "batch"]}, {"id": "term-least-to-most-decomposition", "t": "Least-to-Most Decomposition", "tg": ["Prompt Engineering", "Decomposition"], "d": "general", "x": "The first stage of least-to-most prompting where a complex problem is broken into a sequence of progressively more...", "l": "l", "k": ["least-to-most", "decomposition", "stage", "prompting", "complex", "problem", "broken", "sequence", "progressively", "difficult", "sub-problems", "sub-problem", "building", "solutions", "easier"]}, {"id": "term-leave-one-out-cross-validation", "t": "Leave-One-Out Cross-Validation", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A cross-validation method where each observation serves as a single-element test set while all remaining observations...", "l": "l", "k": ["leave-one-out", "cross-validation", "method", "observation", "serves", "single-element", "test", "remaining", "observations", "form", "training", "provides", "nearly", "unbiased", "estimates"]}, {"id": "term-leetcode-dataset", "t": "LeetCode Dataset", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "Collections of programming problems from LeetCode with solutions used for evaluating code generation capabilities on...", "l": "l", "k": ["leetcode", "dataset", "collections", "programming", "problems", "solutions", "evaluating", "code", "generation", "capabilities", "algorithmic", "data", "structure", "challenges"]}, {"id": "term-leftist-heap-algorithm", "t": "Leftist Heap Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A variant of a binary heap that supports efficient merging by maintaining the leftist property where the right spine is...", "l": "l", "k": ["leftist", "heap", "algorithm", "variant", "binary", "supports", "efficient", "merging", "maintaining", "property", "right", "spine", "always", "shortest", "path"]}, {"id": "term-legal-personhood-for-ai", "t": "Legal Personhood for AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "The concept of granting AI systems legal rights and obligations similar to those of corporations. Debated as a...", "l": "l", "k": ["legal", "personhood", "concept", "granting", "systems", "rights", "obligations", "similar", "corporations", "debated", "potential", "framework", "liability", "accountability", "criticized"]}, {"id": "term-legalbert", "t": "LegalBERT", "tg": ["Models", "Technical"], "d": "models", "x": "A BERT model pretrained on legal text corpora including court opinions legislation and contracts. Achieves improved...", "l": "l", "k": ["legalbert", "bert", "model", "pretrained", "legal", "text", "corpora", "including", "court", "opinions", "legislation", "contracts", "achieves", "improved", "performance"]}, {"id": "term-lemmatization", "t": "Lemmatization", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The process of reducing words to their dictionary base form (lemma) using morphological analysis and vocabulary lookup,...", "l": "l", "k": ["lemmatization", "process", "reducing", "words", "dictionary", "base", "form", "lemma", "morphological", "analysis", "vocabulary", "lookup", "producing", "valid", "unlike"]}, {"id": "term-lempel-ziv-77-algorithm", "t": "Lempel-Ziv 77 Algorithm", "tg": ["Algorithms", "Fundamentals", "Information Theory"], "d": "algorithms", "x": "A sliding-window dictionary compression algorithm that encodes repeated patterns as references to earlier occurrences...", "l": "l", "k": ["lempel-ziv", "algorithm", "sliding-window", "dictionary", "compression", "encodes", "repeated", "patterns", "references", "earlier", "occurrences", "data", "stream", "forms", "basis"]}, {"id": "term-lempel-ziv-welch-algorithm", "t": "Lempel-Ziv-Welch Algorithm", "tg": ["Algorithms", "Fundamentals", "Information Theory"], "d": "algorithms", "x": "A dictionary-based lossless compression algorithm that builds a translation table from the input data during encoding....", "l": "l", "k": ["lempel-ziv-welch", "algorithm", "dictionary-based", "lossless", "compression", "builds", "translation", "table", "input", "data", "encoding", "gif", "image", "format", "unix"]}, {"id": "term-lenet", "t": "LeNet", "tg": ["History", "Milestones"], "d": "history", "x": "A pioneering convolutional neural network designed by Yann LeCun in 1989 for handwritten digit recognition,...", "l": "l", "k": ["lenet", "pioneering", "convolutional", "neural", "network", "designed", "yann", "lecun", "handwritten", "digit", "recognition", "successfully", "deployed", "postal", "service"]}, {"id": "term-length-penalty", "t": "Length Penalty", "tg": ["Generation", "Parameter"], "d": "general", "x": "A parameter in text generation that discourages or encourages longer outputs. Helps control verbosity and can be...", "l": "l", "k": ["length", "penalty", "parameter", "text", "generation", "discourages", "encourages", "longer", "outputs", "helps", "control", "verbosity", "adjusted", "match", "desired"]}, {"id": "term-leonardo-supercomputer", "t": "Leonardo Supercomputer", "tg": ["Supercomputer", "NVIDIA", "Europe"], "d": "hardware", "x": "Italian pre-exascale supercomputer using NVIDIA A100 GPUs operated by CINECA. Supports European AI research and...", "l": "l", "k": ["leonardo", "supercomputer", "italian", "pre-exascale", "nvidia", "a100", "gpus", "operated", "cineca", "supports", "european", "research", "scientific", "computing", "part"]}, {"id": "term-leslie-valiant", "t": "Leslie Valiant", "tg": ["History", "Pioneers"], "d": "history", "x": "British-American computer scientist who received the Turing Award in 2010 for contributions to computational learning...", "l": "l", "k": ["leslie", "valiant", "british-american", "computer", "scientist", "received", "turing", "award", "contributions", "computational", "learning", "theory", "probably", "approximately", "correct"]}, {"id": "term-lethal-autonomous-weapons-systems", "t": "Lethal Autonomous Weapons Systems", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "A class of autonomous weapons, sometimes called killer robots, capable of independently identifying and lethally...", "l": "l", "k": ["lethal", "autonomous", "weapons", "systems", "class", "sometimes", "called", "killer", "robots", "capable", "independently", "identifying", "lethally", "engaging", "human"]}, {"id": "term-level-set-method", "t": "Level Set Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A numerical technique for tracking interfaces and shapes that represents boundaries as the zero level set of a...", "l": "l", "k": ["level", "method", "numerical", "technique", "tracking", "interfaces", "shapes", "represents", "boundaries", "zero", "higher-dimensional", "function", "image", "segmentation", "fluid"]}, {"id": "term-levenberg-marquardt-algorithm", "t": "Levenberg-Marquardt Algorithm", "tg": ["Algorithms", "Technical", "Optimization", "Numerical"], "d": "algorithms", "x": "An iterative method for solving nonlinear least squares problems that interpolates between Gauss-Newton and gradient...", "l": "l", "k": ["levenberg-marquardt", "algorithm", "iterative", "method", "solving", "nonlinear", "least", "squares", "problems", "interpolates", "gauss-newton", "gradient", "descent", "adjusts", "damping"]}, {"id": "term-levenshtein-distance", "t": "Levenshtein Distance", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A string metric measuring the minimum number of single-character insertions, deletions, and substitutions needed to...", "l": "l", "k": ["levenshtein", "distance", "string", "metric", "measuring", "minimum", "number", "single-character", "insertions", "deletions", "substitutions", "needed", "transform", "another", "spell"]}, {"id": "term-leverage", "t": "Leverage", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A measure of how far an observation's predictor values are from the center of the predictor space. High-leverage points...", "l": "l", "k": ["leverage", "measure", "far", "observation", "predictor", "values", "center", "space", "high-leverage", "points", "outsized", "potential", "influence", "regression", "fit"]}, {"id": "term-levit", "t": "LeViT", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A fast inference image classification model that introduces attention bias and shrinking attention to create a highly...", "l": "l", "k": ["levit", "fast", "inference", "image", "classification", "model", "introduces", "attention", "bias", "shrinking", "create", "highly", "efficient", "vision", "transformer"]}, {"id": "term-lexical-ambiguity", "t": "Lexical Ambiguity", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The phenomenon where a word or phrase can be interpreted in multiple ways due to polysemy or homonymy, requiring...", "l": "l", "k": ["lexical", "ambiguity", "phenomenon", "word", "phrase", "interpreted", "multiple", "ways", "due", "polysemy", "homonymy", "requiring", "context", "determine", "intended"]}, {"id": "term-lfw", "t": "LFW", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "Labeled Faces in the Wild a dataset of 13000 face images of 5749 people collected from the web. The standard benchmark...", "l": "l", "k": ["lfw", "labeled", "faces", "wild", "dataset", "face", "images", "people", "collected", "web", "standard", "benchmark", "evaluating", "verification", "algorithms"]}, {"id": "term-lgm", "t": "LGM", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "Large Gaussian Model generates high-quality 3D objects from text or image inputs using a large multi-view Gaussian...", "l": "l", "k": ["lgm", "large", "gaussian", "model", "generates", "high-quality", "objects", "text", "image", "inputs", "multi-view", "generation", "approach", "single", "forward"]}, {"id": "term-liability-for-ai", "t": "Liability for AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "Legal responsibility for harm caused by AI systems. Current legal frameworks struggle to assign liability when AI makes...", "l": "l", "k": ["liability", "legal", "responsibility", "harm", "caused", "systems", "current", "frameworks", "struggle", "assign", "makes", "autonomous", "decisions", "developed", "various"]}, {"id": "term-librilight", "t": "LibriLight", "tg": ["Training Corpus", "Speech"], "d": "datasets", "x": "A large-scale semi-supervised speech dataset containing 60000 hours of unlabeled English speech from LibriVox. Designed...", "l": "l", "k": ["librilight", "large-scale", "semi-supervised", "speech", "dataset", "containing", "hours", "unlabeled", "english", "librivox", "designed", "research", "low-resource", "self-supervised", "recognition"]}, {"id": "term-librispeech", "t": "LibriSpeech", "tg": ["History", "Milestones"], "d": "history", "x": "A corpus of approximately 1000 hours of read English speech derived from audiobooks created by Vassil Panayotov and...", "l": "l", "k": ["librispeech", "corpus", "approximately", "hours", "read", "english", "speech", "derived", "audiobooks", "created", "vassil", "panayotov", "colleagues", "became", "standard"]}, {"id": "term-licensing-for-ai", "t": "Licensing for AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "Proposed regulatory requirements for AI developers or systems to obtain licenses before deployment similar to licensing...", "l": "l", "k": ["licensing", "proposed", "regulatory", "requirements", "developers", "systems", "obtain", "licenses", "deployment", "similar", "healthcare", "finance", "aviation", "intended", "ensure"]}, {"id": "term-lidar", "t": "LiDAR", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "Light Detection and Ranging, a remote sensing technology that measures distances by illuminating targets with laser...", "l": "l", "k": ["lidar", "light", "detection", "ranging", "remote", "sensing", "technology", "measures", "distances", "illuminating", "targets", "laser", "pulses", "producing", "dense"]}, {"id": "term-lifecycle-assessment-for-ai", "t": "Lifecycle Assessment for AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "Evaluation of the environmental social and economic impacts of an AI system throughout its entire lifecycle from data...", "l": "l", "k": ["lifecycle", "assessment", "evaluation", "environmental", "social", "economic", "impacts", "system", "throughout", "entire", "data", "collection", "training", "deployment", "decommissioning"]}, {"id": "term-lightgbm", "t": "LightGBM", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A gradient boosting framework that uses histogram-based algorithms and leaf-wise tree growth for faster training on...", "l": "l", "k": ["lightgbm", "gradient", "boosting", "framework", "uses", "histogram-based", "algorithms", "leaf-wise", "tree", "growth", "faster", "training", "large", "datasets", "supports"]}, {"id": "term-lightgbm-model", "t": "LightGBM Model", "tg": ["Models", "Fundamentals"], "d": "models", "x": "A gradient boosting framework from Microsoft that uses leaf-wise tree growth and histogram-based splitting for fast...", "l": "l", "k": ["lightgbm", "model", "gradient", "boosting", "framework", "microsoft", "uses", "leaf-wise", "tree", "growth", "histogram-based", "splitting", "fast", "training", "large"]}, {"id": "term-lightgcn", "t": "LightGCN", "tg": ["Models", "Technical", "Recommendation"], "d": "models", "x": "A simplified graph convolution network for recommendation that removes feature transformation and nonlinear activation...", "l": "l", "k": ["lightgcn", "simplified", "graph", "convolution", "network", "recommendation", "removes", "feature", "transformation", "nonlinear", "activation", "improve", "performance", "efficiency"]}, {"id": "term-lighthill-report", "t": "Lighthill Report", "tg": ["History", "Milestones"], "d": "history", "x": "A 1973 report by mathematician James Lighthill commissioned by the British Science Research Council that criticized AI...", "l": "l", "k": ["lighthill", "report", "mathematician", "james", "commissioned", "british", "science", "research", "council", "criticized", "failing", "achieve", "ambitious", "goals", "leading"]}, {"id": "term-lightmatter", "t": "Lightmatter", "tg": ["Accelerator", "Startup", "Photonic"], "d": "hardware", "x": "Photonic AI hardware company developing optical interconnects and photonic compute chips that use light instead of...", "l": "l", "k": ["lightmatter", "photonic", "hardware", "company", "developing", "optical", "interconnects", "compute", "chips", "light", "instead", "electrons", "matrix", "operations", "promises"]}, {"id": "term-likelihood-function", "t": "Likelihood Function", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A function of the parameters of a statistical model, computed as the probability of the observed data for given...", "l": "l", "k": ["likelihood", "function", "parameters", "statistical", "model", "computed", "probability", "observed", "data", "given", "parameter", "values", "unlike", "distribution", "evaluated"]}, {"id": "term-likelihood-ratio-test", "t": "Likelihood Ratio Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A hypothesis test that compares the fit of two nested models by computing twice the difference in their...", "l": "l", "k": ["likelihood", "ratio", "test", "hypothesis", "compares", "fit", "nested", "models", "computing", "twice", "difference", "log-likelihoods", "null", "statistic", "follows"]}, {"id": "term-lila", "t": "LILA", "tg": ["Benchmark", "Computer Vision", "Conservation"], "d": "datasets", "x": "Labeled Information Library of Alexandria a repository of camera trap datasets for wildlife conservation. Aggregates...", "l": "l", "k": ["lila", "labeled", "information", "library", "alexandria", "repository", "camera", "trap", "datasets", "wildlife", "conservation", "aggregates", "millions", "annotated", "images"]}, {"id": "term-lima-dataset", "t": "Lima Dataset", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A carefully curated dataset of 1000 high-quality instruction-response pairs demonstrating that a small amount of...", "l": "l", "k": ["lima", "dataset", "carefully", "curated", "high-quality", "instruction-response", "pairs", "demonstrating", "small", "amount", "excellent", "data", "produce", "capable", "instruction-following"]}, {"id": "term-lime", "t": "LIME", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "Local Interpretable Model-agnostic Explanations, a technique that explains individual predictions by fitting a simple...", "l": "l", "k": ["lime", "local", "interpretable", "model-agnostic", "explanations", "technique", "explains", "individual", "predictions", "fitting", "simple", "model", "perturbed", "samples", "around"]}, {"id": "term-linear-attention", "t": "Linear Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention variant that replaces the softmax-based dot-product attention with a kernel-based approximation, achieving...", "l": "l", "k": ["linear", "attention", "variant", "replaces", "softmax-based", "dot-product", "kernel-based", "approximation", "achieving", "time", "memory", "complexity", "respect", "sequence", "length"]}, {"id": "term-linear-discriminant-analysis", "t": "Linear Discriminant Analysis", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "A supervised dimensionality reduction and classification technique that projects data onto directions that maximize the...", "l": "l", "k": ["linear", "discriminant", "analysis", "supervised", "dimensionality", "reduction", "classification", "technique", "projects", "data", "onto", "directions", "maximize", "ratio", "between-class"]}, {"id": "term-linear-function-approximation-rl", "t": "Linear Function Approximation in RL", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "Value function estimation using a linear combination of state features, where the value is a weighted sum of feature...", "l": "l", "k": ["linear", "function", "approximation", "value", "estimation", "combination", "state", "features", "weighted", "sum", "feature", "values", "offers", "convergence", "guarantees"]}, {"id": "term-linear-hashing-algorithm", "t": "Linear Hashing Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A dynamic hashing scheme that splits buckets in a predetermined order rather than the overflowing bucket. Maintains a...", "l": "l", "k": ["linear", "hashing", "algorithm", "dynamic", "scheme", "splits", "buckets", "predetermined", "order", "rather", "overflowing", "bucket", "maintains", "controlled", "load"]}, {"id": "term-linear-regression", "t": "Linear Regression", "tg": ["Algorithm", "Fundamentals"], "d": "algorithms", "x": "A foundational ML algorithm that models the relationship between variables using a straight line. Simple but effective...", "l": "l", "k": ["linear", "regression", "foundational", "algorithm", "models", "relationship", "variables", "straight", "line", "simple", "effective", "prediction", "tasks", "relationships"]}, {"id": "term-linear-regression-model", "t": "Linear Regression Model", "tg": ["Models", "Fundamentals", "History"], "d": "models", "x": "A statistical model that predicts a continuous target variable as a weighted linear combination of input features plus...", "l": "l", "k": ["linear", "regression", "model", "statistical", "predicts", "continuous", "target", "variable", "weighted", "combination", "input", "features", "plus", "intercept", "term"]}, {"id": "term-linear-transformer", "t": "Linear Transformer", "tg": ["Models", "Technical"], "d": "models", "x": "A transformer variant that replaces softmax attention with a kernel-based linear attention mechanism. Reduces...", "l": "l", "k": ["linear", "transformer", "variant", "replaces", "softmax", "attention", "kernel-based", "mechanism", "reduces", "computational", "complexity", "quadratic", "sequence", "length", "trades"]}, {"id": "term-linformer", "t": "Linformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer that projects key and value matrices to a lower-dimensional space before computing attention, reducing...", "l": "l", "k": ["linformer", "transformer", "projects", "key", "value", "matrices", "lower-dimensional", "space", "computing", "attention", "reducing", "quadratic", "complexity", "self-attention", "linear"]}, {"id": "term-lingo-1", "t": "LINGO-1", "tg": ["Models", "Technical", "Autonomous", "Vision", "NLP"], "d": "models", "x": "A visual language model from Wayve designed for autonomous driving that provides natural language explanations of...", "l": "l", "k": ["lingo-1", "visual", "language", "model", "wayve", "designed", "autonomous", "driving", "provides", "natural", "explanations", "decisions", "scene", "understanding"]}, {"id": "term-link-cut-tree", "t": "Link-Cut Tree", "tg": ["Algorithms", "Technical", "Graph", "Data Structure"], "d": "algorithms", "x": "A data structure that represents a forest of rooted trees and supports dynamic link and cut operations. Enables path...", "l": "l", "k": ["link-cut", "tree", "data", "structure", "represents", "forest", "rooted", "trees", "supports", "dynamic", "link", "cut", "operations", "enables", "path"]}, {"id": "term-linpack-benchmark", "t": "LINPACK Benchmark", "tg": ["Benchmark", "Performance", "Standard"], "d": "hardware", "x": "Standard benchmark for measuring floating-point computation speed by solving dense linear equations. Used as the...", "l": "l", "k": ["linpack", "benchmark", "standard", "measuring", "floating-point", "computation", "speed", "solving", "dense", "linear", "equations", "primary", "metric", "top500", "supercomputer"]}, {"id": "term-lion-optimizer", "t": "Lion Optimizer", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Evolved Sign Momentum optimizer discovered through program search by Google Brain in 2023. Uses only sign operations...", "l": "l", "k": ["lion", "optimizer", "evolved", "sign", "momentum", "discovered", "program", "search", "google", "brain", "uses", "operations", "updates", "making", "memory"]}, {"id": "term-liquid-cooling", "t": "Liquid Cooling", "tg": ["Cooling", "Data Center"], "d": "hardware", "x": "Cooling method using liquid coolant to remove heat from computing components. Increasingly essential for high-power AI...", "l": "l", "k": ["liquid", "cooling", "method", "coolant", "remove", "heat", "computing", "components", "increasingly", "essential", "high-power", "gpus", "h100", "generate", "hundreds"]}, {"id": "term-liquid-neural-network", "t": "Liquid Neural Network", "tg": ["Models", "Technical"], "d": "models", "x": "A continuous-time neural network inspired by biological neural circuits that can adapt its behavior based on input...", "l": "l", "k": ["liquid", "neural", "network", "continuous-time", "inspired", "biological", "circuits", "adapt", "behavior", "based", "input", "dynamics", "features", "time-varying", "parameters"]}, {"id": "term-lisp", "t": "LISP", "tg": ["History", "Milestones"], "d": "history", "x": "A programming language family invented by John McCarthy in 1958 that became the dominant language for AI research for...", "l": "l", "k": ["lisp", "programming", "language", "family", "invented", "john", "mccarthy", "became", "dominant", "research", "decades", "featuring", "symbolic", "expression", "processing"]}, {"id": "term-lisp-machine", "t": "LISP Machine", "tg": ["History", "Systems"], "d": "history", "x": "Specialized computers designed to run the LISP programming language efficiently. Developed in the 1970s and 1980s at...", "l": "l", "k": ["lisp", "machine", "specialized", "computers", "designed", "run", "programming", "language", "efficiently", "developed", "1970s", "1980s", "mit", "commercialized", "companies"]}, {"id": "term-lithography", "t": "Lithography", "tg": ["Fabrication", "Manufacturing", "Process"], "d": "hardware", "x": "Process of transferring circuit patterns onto semiconductor wafers using light. The most critical and complex step in...", "l": "l", "k": ["lithography", "process", "transferring", "circuit", "patterns", "onto", "semiconductor", "wafers", "light", "critical", "complex", "step", "chip", "manufacturing", "determining"]}, {"id": "term-livebench", "t": "LiveBench", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A continuously updated benchmark for language models using questions from recent sources to prevent contamination....", "l": "l", "k": ["livebench", "continuously", "updated", "benchmark", "language", "models", "questions", "recent", "sources", "prevent", "contamination", "provides", "fresh", "evaluation", "cannot"]}, {"id": "term-livecodebench", "t": "LiveCodeBench", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A continuously updated benchmark of competitive programming problems collected after model training cutoffs. Designed...", "l": "l", "k": ["livecodebench", "continuously", "updated", "benchmark", "competitive", "programming", "problems", "collected", "model", "training", "cutoffs", "designed", "prevent", "data", "contamination"]}, {"id": "term-lj-speech", "t": "LJ Speech", "tg": ["Training Corpus", "Speech"], "d": "datasets", "x": "A public domain speech dataset of 13100 short audio clips from a single speaker reading passages from 7 non-fiction...", "l": "l", "k": ["speech", "public", "domain", "dataset", "short", "audio", "clips", "single", "speaker", "reading", "passages", "non-fiction", "books", "widely", "text-to-speech"]}, {"id": "term-llama", "t": "Llama", "tg": ["Model", "Meta"], "d": "models", "x": "Meta's open-weight family of large language models. Released with relatively permissive licenses, enabling widespread...", "l": "l", "k": ["llama", "meta", "open-weight", "family", "large", "language", "models", "released", "relatively", "permissive", "licenses", "enabling", "widespread", "research", "commercial"]}, {"id": "term-llama-2", "t": "LLaMA 2", "tg": ["Models", "Fundamentals"], "d": "models", "x": "The second generation of Meta's open-weight language models available in 7B 13B and 70B parameter sizes. Includes...", "l": "l", "k": ["llama", "generation", "meta", "open-weight", "language", "models", "available", "13b", "70b", "parameter", "sizes", "includes", "chat-optimized", "variants", "fine-tuned"]}, {"id": "term-llama-3", "t": "LLaMA 3", "tg": ["Models", "Technical"], "d": "models", "x": "Meta's third generation of open-weight language models with improved pretraining data and architecture refinements....", "l": "l", "k": ["llama", "meta", "generation", "open-weight", "language", "models", "improved", "pretraining", "data", "architecture", "refinements", "available", "multiple", "sizes", "significantly"]}, {"id": "term-llama-31", "t": "Llama 3.1", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An extended version of Meta's Llama 3 family offering 8B and 70B and 405B parameter models with a 128K context window...", "l": "l", "k": ["llama", "extended", "version", "meta", "family", "offering", "70b", "405b", "parameter", "models", "128k", "context", "window", "multilingual", "support"]}, {"id": "term-llama-32", "t": "Llama 3.2", "tg": ["Models", "Technical", "NLP", "Vision"], "d": "models", "x": "A multimodal extension of Meta's Llama 3 family that adds vision capabilities alongside text in lightweight 1B and 3B...", "l": "l", "k": ["llama", "multimodal", "extension", "meta", "family", "adds", "vision", "capabilities", "alongside", "text", "lightweight", "larger", "11b", "90b", "variants"]}, {"id": "term-llama-33", "t": "Llama 3.3", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A 70B parameter model from Meta that delivers Llama 3.1 405B-level performance for text tasks at significantly lower...", "l": "l", "k": ["llama", "70b", "parameter", "model", "meta", "delivers", "405b-level", "performance", "text", "tasks", "significantly", "lower", "computational", "cost"]}, {"id": "term-llama-guard", "t": "Llama Guard", "tg": ["Models", "Safety"], "d": "models", "x": "A safety classifier model fine-tuned from LLaMA for evaluating the safety of language model inputs and outputs....", "l": "l", "k": ["llama", "guard", "safety", "classifier", "model", "fine-tuned", "evaluating", "language", "inputs", "outputs", "classifies", "content", "against", "configurable", "taxonomies"]}, {"id": "term-llama-guard-2", "t": "Llama Guard 2", "tg": ["Models", "Technical", "NLP", "Safety"], "d": "models", "x": "A second-generation safety classifier from Meta AI built on Llama 3 that evaluates whether language model inputs and...", "l": "l", "k": ["llama", "guard", "second-generation", "safety", "classifier", "meta", "built", "evaluates", "language", "model", "inputs", "outputs", "safe", "harmful"]}, {"id": "term-llama-adapter", "t": "Llama-Adapter", "tg": ["Models", "Technical"], "d": "models", "x": "An efficient fine-tuning method that prepends a set of learnable adaptation prompts to the upper layers of a frozen...", "l": "l", "k": ["llama-adapter", "efficient", "fine-tuning", "method", "prepends", "learnable", "adaptation", "prompts", "upper", "layers", "frozen", "llama", "model", "achieves", "strong"]}, {"id": "term-llama-cpp", "t": "llama.cpp", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "An open-source C/C++ library for efficient CPU and GPU inference of large language models using quantized weights....", "l": "l", "k": ["llama", "cpp", "open-source", "library", "efficient", "cpu", "gpu", "inference", "large", "language", "models", "quantized", "weights", "enables", "running"]}, {"id": "term-llamaindex", "t": "LlamaIndex", "tg": ["Framework", "Application"], "d": "general", "x": "A data framework for connecting LLMs to external data sources. Specializes in indexing, retrieval, and RAG applications...", "l": "l", "k": ["llamaindex", "data", "framework", "connecting", "llms", "external", "sources", "specializes", "indexing", "retrieval", "rag", "applications", "various", "connectors", "query"]}, {"id": "term-llava", "t": "LLaVA", "tg": ["Models", "Technical"], "d": "models", "x": "Large Language and Vision Assistant is a multimodal model that connects a vision encoder to a language model using a...", "l": "l", "k": ["llava", "large", "language", "vision", "assistant", "multimodal", "model", "connects", "encoder", "simple", "projection", "layer", "achieves", "strong", "visual"]}, {"id": "term-llava-visual-instruct", "t": "LLaVA Visual Instruct", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "A multimodal instruction-following dataset combining visual inputs with text instructions. Used to train the LLaVA...", "l": "l", "k": ["llava", "visual", "instruct", "multimodal", "instruction-following", "dataset", "combining", "inputs", "text", "instructions", "train", "vision-language", "model", "chat", "reasoning"]}, {"id": "term-llava-med", "t": "LLaVA-Med", "tg": ["Models", "Technical", "Medical", "Vision", "NLP"], "d": "models", "x": "A vision-language model adapted for biomedical image understanding by fine-tuning LLaVA on curated biomedical...", "l": "l", "k": ["llava-med", "vision-language", "model", "adapted", "biomedical", "image", "understanding", "fine-tuning", "llava", "curated", "image-text", "instruction", "data"]}, {"id": "term-llava-next", "t": "Llava-NeXT", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "An improved version of LLaVA with higher resolution image support and better visual reasoning capabilities through...", "l": "l", "k": ["llava-next", "improved", "version", "llava", "higher", "resolution", "image", "support", "better", "visual", "reasoning", "capabilities", "enhanced", "training", "strategies"]}, {"id": "term-llava-next-video", "t": "Llava-NeXT-Video", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "An extension of LLaVA-NeXT that adds video understanding by representing videos as sequences of frames processed...", "l": "l", "k": ["llava-next-video", "extension", "llava-next", "adds", "video", "understanding", "representing", "videos", "sequences", "frames", "processed", "vision-language", "architecture"]}, {"id": "term-llava-onevision", "t": "Llava-OneVision", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A unified vision-language model that handles single-image and multi-image and video understanding tasks through a...", "l": "l", "k": ["llava-onevision", "unified", "vision-language", "model", "handles", "single-image", "multi-image", "video", "understanding", "tasks", "carefully", "designed", "training", "pipeline"]}, {"id": "term-llemma", "t": "Llemma", "tg": ["Models", "Technical"], "d": "models", "x": "An open language model for mathematics built by continuing pretraining of Code Llama on a blend of mathematical...", "l": "l", "k": ["llemma", "open", "language", "model", "mathematics", "built", "continuing", "pretraining", "code", "llama", "blend", "mathematical", "documents", "achieves", "strong"]}, {"id": "term-llm-as-judge", "t": "LLM-as-Judge", "tg": ["Evaluation", "LLM-Based"], "d": "models", "x": "An evaluation paradigm where a large language model is prompted to assess and score the quality of outputs from other...", "l": "l", "k": ["llm-as-judge", "evaluation", "paradigm", "large", "language", "model", "prompted", "assess", "score", "quality", "outputs", "models", "providing", "scalable", "approximates"]}, {"id": "term-llmlingua", "t": "LLMLingua", "tg": ["Prompt Engineering", "Compression"], "d": "general", "x": "A prompt compression framework that uses a small language model to identify and remove less informative tokens from...", "l": "l", "k": ["llmlingua", "prompt", "compression", "framework", "uses", "small", "language", "model", "identify", "remove", "less", "informative", "tokens", "prompts", "achieving"]}, {"id": "term-lmdrive", "t": "LMDrive", "tg": ["Models", "Technical", "Autonomous", "NLP"], "d": "models", "x": "A language-guided end-to-end autonomous driving framework that uses large language models to process navigation...", "l": "l", "k": ["lmdrive", "language-guided", "end-to-end", "autonomous", "driving", "framework", "uses", "large", "language", "models", "process", "navigation", "instructions", "sensor", "data"]}, {"id": "term-lmsys-elo-ratings", "t": "LMSys Elo Ratings", "tg": ["Evaluation", "NLP"], "d": "datasets", "x": "A ranking system for language models based on human preference votes from the Chatbot Arena. Provides empirical...", "l": "l", "k": ["lmsys", "elo", "ratings", "ranking", "system", "language", "models", "based", "human", "preference", "votes", "chatbot", "arena", "provides", "empirical"]}, {"id": "term-lmsys-chat-1m", "t": "LMSYS-Chat-1M", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A dataset of one million real-world conversations with 25 LLMs collected from the LMSYS Chatbot Arena platform....", "l": "l", "k": ["lmsys-chat-1m", "dataset", "million", "real-world", "conversations", "llms", "collected", "lmsys", "chatbot", "arena", "platform", "provides", "diverse", "user", "interactions"]}, {"id": "term-load-balancing-loss", "t": "Load Balancing Loss", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An auxiliary loss term used in mixture-of-experts models to encourage uniform distribution of tokens across experts,...", "l": "l", "k": ["load", "balancing", "loss", "auxiliary", "term", "mixture-of-experts", "models", "encourage", "uniform", "distribution", "tokens", "across", "experts", "preventing", "routing"]}, {"id": "term-local-differential-privacy", "t": "Local Differential Privacy", "tg": ["Algorithms", "Technical", "Privacy"], "d": "algorithms", "x": "A privacy model where each user perturbs their own data before sending it to the data collector. Provides stronger...", "l": "l", "k": ["local", "differential", "privacy", "model", "user", "perturbs", "data", "sending", "collector", "provides", "stronger", "guarantees", "central", "server", "never"]}, {"id": "term-local-llm", "t": "Local LLM", "tg": ["Deployment", "Privacy"], "d": "safety", "x": "Running language models on personal hardware rather than through cloud APIs. Enables privacy, offline use, and cost...", "l": "l", "k": ["local", "llm", "running", "language", "models", "personal", "hardware", "rather", "cloud", "apis", "enables", "privacy", "offline", "cost", "savings"]}, {"id": "term-local-outlier-factor", "t": "Local Outlier Factor", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A density-based anomaly detection algorithm that compares the local density of a point to the densities of its...", "l": "l", "k": ["local", "outlier", "factor", "density-based", "anomaly", "detection", "algorithm", "compares", "density", "point", "densities", "neighbors", "points", "substantially", "lower"]}, {"id": "term-locality-preserving-projection", "t": "Locality-Preserving Projection", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A linear dimensionality reduction method that preserves the local structure of the data as defined by a neighborhood...", "l": "l", "k": ["locality-preserving", "projection", "linear", "dimensionality", "reduction", "method", "preserves", "local", "structure", "data", "defined", "neighborhood", "graph", "finds", "transformation"]}, {"id": "term-locality-sensitive-hashing", "t": "Locality-Sensitive Hashing", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "An approximate nearest neighbor technique that hashes similar vectors into the same buckets with high probability using...", "l": "l", "k": ["locality-sensitive", "hashing", "approximate", "nearest", "neighbor", "technique", "hashes", "similar", "vectors", "buckets", "high", "probability", "random", "projections", "enabling"]}, {"id": "term-locality-sensitive-hashing-algorithm", "t": "Locality-Sensitive Hashing Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure", "Searching"], "d": "algorithms", "x": "A family of hashing methods that map similar items to the same hash buckets with high probability. Enables approximate...", "l": "l", "k": ["locality-sensitive", "hashing", "algorithm", "family", "methods", "map", "similar", "items", "hash", "buckets", "high", "probability", "enables", "approximate", "nearest-neighbor"]}, {"id": "term-localized-narratives", "t": "Localized Narratives", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "A dataset of detailed spoken descriptions of images where narrators simultaneously move their mouse over the image...", "l": "l", "k": ["localized", "narratives", "dataset", "detailed", "spoken", "descriptions", "images", "narrators", "simultaneously", "move", "mouse", "image", "regions", "describe", "provides"]}, {"id": "term-locally-linear-embedding", "t": "Locally Linear Embedding", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A nonlinear dimensionality reduction method that preserves local neighborhood relationships. Each point is...", "l": "l", "k": ["locally", "linear", "embedding", "nonlinear", "dimensionality", "reduction", "method", "preserves", "local", "neighborhood", "relationships", "point", "reconstructed", "combination", "neighbors"]}, {"id": "term-locking-problem", "t": "Locking Problem", "tg": ["Safety", "Technical"], "d": "safety", "x": "The challenge that early AI systems may become entrenched and difficult to modify or replace once widely deployed...", "l": "l", "k": ["locking", "problem", "challenge", "early", "systems", "become", "entrenched", "difficult", "modify", "replace", "widely", "deployed", "creating", "path", "dependency"]}, {"id": "term-loebner-prize", "t": "Loebner Prize", "tg": ["History", "Milestones"], "d": "history", "x": "An annual competition in artificial intelligence that awards prizes to the computer programs considered by the judges...", "l": "l", "k": ["loebner", "prize", "annual", "competition", "artificial", "intelligence", "awards", "prizes", "computer", "programs", "considered", "judges", "human-like", "practical", "implementation"]}, {"id": "term-loess", "t": "LOESS", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "Locally Estimated Scatterplot Smoothing, a non-parametric regression method that fits local weighted polynomial...", "l": "l", "k": ["loess", "locally", "estimated", "scatterplot", "smoothing", "non-parametric", "regression", "method", "fits", "local", "weighted", "polynomial", "regressions", "subsets", "data"]}, {"id": "term-lofti-zadeh-fuzzy-sets-paper", "t": "Lofti Zadeh Fuzzy Sets Paper", "tg": ["History", "Milestones"], "d": "history", "x": "The 1965 paper Fuzzy Sets by Lotfi Zadeh published in Information and Control that introduced fuzzy set theory. This...", "l": "l", "k": ["lofti", "zadeh", "fuzzy", "sets", "paper", "lotfi", "published", "information", "control", "introduced", "theory", "work", "extended", "classical", "allow"]}, {"id": "term-log-loss", "t": "Log Loss", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A loss function for binary classification that measures the negative log-likelihood of the true labels given the...", "l": "l", "k": ["log", "loss", "function", "binary", "classification", "measures", "negative", "log-likelihood", "true", "labels", "given", "predicted", "probabilities", "heavily", "penalizes"]}, {"id": "term-log-likelihood", "t": "Log-Likelihood", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The natural logarithm of the likelihood function, used to simplify optimization because it converts products of...", "l": "l", "k": ["log-likelihood", "natural", "logarithm", "likelihood", "function", "simplify", "optimization", "converts", "products", "probabilities", "sums", "maximizing", "equivalent"]}, {"id": "term-log-normal-distribution", "t": "Log-Normal Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution of a random variable whose logarithm is normally distributed. It is used to model...", "l": "l", "k": ["log-normal", "distribution", "continuous", "probability", "random", "variable", "whose", "logarithm", "normally", "distributed", "model", "quantities", "products", "independent", "positive"]}, {"id": "term-log-softmax", "t": "Log-Softmax", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "The logarithm of the softmax function computed more numerically stably by using the log-sum-exp trick. Directly outputs...", "l": "l", "k": ["log-softmax", "logarithm", "softmax", "function", "computed", "numerically", "stably", "log-sum-exp", "trick", "directly", "outputs", "log-probabilities", "negative", "log-likelihood", "loss"]}, {"id": "term-logic-programming", "t": "Logic Programming", "tg": ["History", "Fundamentals"], "d": "history", "x": "A programming paradigm based on formal logic where programs are expressed as a set of logical relations and computation...", "l": "l", "k": ["logic", "programming", "paradigm", "based", "formal", "programs", "expressed", "logical", "relations", "computation", "proceeds", "inference", "developed", "early", "1970s"]}, {"id": "term-logic-synthesis", "t": "Logic Synthesis", "tg": ["Manufacturing", "Design", "Process"], "d": "hardware", "x": "Process of converting a high-level hardware description into a gate-level netlist optimized for area timing and power....", "l": "l", "k": ["logic", "synthesis", "process", "converting", "high-level", "hardware", "description", "gate-level", "netlist", "optimized", "area", "timing", "power", "key", "step"]}, {"id": "term-logic-theorist", "t": "Logic Theorist", "tg": ["History", "Milestones"], "d": "history", "x": "A program written by Allen Newell and Herbert Simon in 1956 that could prove mathematical theorems from Principia...", "l": "l", "k": ["logic", "theorist", "program", "written", "allen", "newell", "herbert", "simon", "prove", "mathematical", "theorems", "principia", "mathematica", "widely", "considered"]}, {"id": "term-logiqa", "t": "LogiQA", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A logical reasoning QA dataset from the Chinese National Civil Service Examination. Tests deductive reasoning...", "l": "l", "k": ["logiqa", "logical", "reasoning", "dataset", "chinese", "national", "civil", "service", "examination", "tests", "deductive", "categorical", "analysis", "abilities"]}, {"id": "term-logiqa-20", "t": "LogiQA 2.0", "tg": ["Benchmark", "NLP", "Reasoning", "Multilingual"], "d": "datasets", "x": "An expanded bilingual version of LogiQA with improved question quality and English translations. Tests formal and...", "l": "l", "k": ["logiqa", "expanded", "bilingual", "version", "improved", "question", "quality", "english", "translations", "tests", "formal", "informal", "logical", "reasoning", "chinese"]}, {"id": "term-logistic-regression", "t": "Logistic Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A linear classification model that predicts class probabilities using the logistic (sigmoid) function applied to a...", "l": "l", "k": ["logistic", "regression", "linear", "classification", "model", "predicts", "class", "probabilities", "sigmoid", "function", "applied", "combination", "input", "features", "despite"]}, {"id": "term-logistic-regression-history", "t": "Logistic Regression History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The application of logistic regression to classification problems in machine learning. Originally developed for...", "l": "l", "k": ["logistic", "regression", "history", "application", "classification", "problems", "machine", "learning", "originally", "developed", "statistics", "david", "cox", "became", "fundamental"]}, {"id": "term-logistic-regression-model", "t": "Logistic Regression Model", "tg": ["Models", "Fundamentals", "History"], "d": "models", "x": "A classification model that uses the logistic sigmoid function to estimate the probability of a binary outcome from...", "l": "l", "k": ["logistic", "regression", "model", "classification", "uses", "sigmoid", "function", "estimate", "probability", "binary", "outcome", "input", "features"]}, {"id": "term-logit", "t": "Logit", "tg": ["Technical", "Math"], "d": "general", "x": "The raw, unnormalized scores output by a model before converting to probabilities. In LLMs, logits represent the...", "l": "l", "k": ["logit", "raw", "unnormalized", "scores", "output", "model", "converting", "probabilities", "llms", "logits", "represent", "preference", "possible", "next", "token"]}, {"id": "term-logit-bias", "t": "Logit Bias", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A technique that adds fixed values to the logits of specific tokens before sampling, used to encourage or suppress...", "l": "l", "k": ["logit", "bias", "technique", "adds", "fixed", "values", "logits", "specific", "tokens", "sampling", "encourage", "suppress", "particular", "words", "phrases"]}, {"id": "term-logit-lens", "t": "Logit Lens", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An interpretability technique that decodes intermediate transformer layer outputs through the final unembedding matrix...", "l": "l", "k": ["logit", "lens", "interpretability", "technique", "decodes", "intermediate", "transformer", "layer", "outputs", "final", "unembedding", "matrix", "observe", "predictions", "evolve"]}, {"id": "term-long-context-fine-tuning", "t": "Long Context Fine-Tuning", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The process of adapting a pre-trained model to effectively utilize longer context windows than it was originally...", "l": "l", "k": ["long", "context", "fine-tuning", "process", "adapting", "pre-trained", "model", "effectively", "utilize", "longer", "windows", "originally", "trained", "continued", "training"]}, {"id": "term-lstm-history", "t": "Long Short-Term Memory", "tg": ["History", "Milestones"], "d": "history", "x": "A recurrent neural network architecture invented by Sepp Hochreiter and Jurgen Schmidhuber in 1997 that solved the...", "l": "l", "k": ["long", "short-term", "memory", "recurrent", "neural", "network", "architecture", "invented", "sepp", "hochreiter", "jurgen", "schmidhuber", "solved", "vanishing", "gradient"]}, {"id": "term-long-term-ai-safety", "t": "Long-term AI Safety", "tg": ["Safety", "Fundamentals"], "d": "safety", "x": "Research focused on ensuring the safety of AI systems that may eventually match or exceed human-level capabilities....", "l": "l", "k": ["long-term", "safety", "research", "focused", "ensuring", "systems", "eventually", "match", "exceed", "human-level", "capabilities", "addresses", "existential", "risks", "value"]}, {"id": "term-longbench", "t": "Longbench", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A comprehensive benchmark for evaluating long-context capabilities of language models across 6 task categories. Tests...", "l": "l", "k": ["longbench", "comprehensive", "benchmark", "evaluating", "long-context", "capabilities", "language", "models", "across", "task", "categories", "tests", "performance", "inputs", "100k"]}, {"id": "term-longest-common-subsequence", "t": "Longest Common Subsequence", "tg": ["Algorithms", "Fundamentals", "NLP"], "d": "algorithms", "x": "A dynamic programming algorithm that finds the longest subsequence common to two sequences. Runs in O(mn) time and is...", "l": "l", "k": ["longest", "common", "subsequence", "dynamic", "programming", "algorithm", "finds", "sequences", "runs", "time", "diff", "utilities", "bioinformatics", "sequence", "comparison"]}, {"id": "term-longest-common-substring", "t": "Longest Common Substring", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "An algorithm that finds the longest string that is a contiguous substring of two or more input strings. Solvable in...", "l": "l", "k": ["longest", "common", "substring", "algorithm", "finds", "string", "contiguous", "input", "strings", "solvable", "time", "dynamic", "programming", "suffix", "trees"]}, {"id": "term-longformer", "t": "Longformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer variant that combines local sliding window attention with task-specific global attention on selected...", "l": "l", "k": ["longformer", "transformer", "variant", "combines", "local", "sliding", "window", "attention", "task-specific", "global", "selected", "tokens", "enabling", "efficient", "processing"]}, {"id": "term-lookahead-optimizer", "t": "Lookahead Optimizer", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A meta-optimizer that wraps around any base optimizer maintaining two sets of weights. The fast weights explore the...", "l": "l", "k": ["lookahead", "optimizer", "meta-optimizer", "wraps", "around", "base", "maintaining", "sets", "weights", "fast", "explore", "loss", "landscape", "slow", "periodically"]}, {"id": "term-loopy-belief-propagation", "t": "Loopy Belief Propagation", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An approximate inference algorithm that applies belief propagation to graphs with cycles despite the lack of...", "l": "l", "k": ["loopy", "belief", "propagation", "approximate", "inference", "algorithm", "applies", "graphs", "cycles", "despite", "lack", "convergence", "guarantees", "produces", "good"]}, {"id": "term-lora", "t": "LoRA (Low-Rank Adaptation)", "tg": ["Training", "Efficiency"], "d": "general", "x": "A parameter-efficient fine-tuning technique that trains only small additional matrices rather than the full model....", "l": "l", "k": ["lora", "low-rank", "adaptation", "parameter-efficient", "fine-tuning", "technique", "trains", "small", "additional", "matrices", "rather", "full", "model", "dramatically", "reduces"]}, {"id": "term-lora-diffusion", "t": "LoRA for Diffusion", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "The application of Low-Rank Adaptation to diffusion models, enabling efficient fine-tuning of image generation models...", "l": "l", "k": ["lora", "diffusion", "application", "low-rank", "adaptation", "models", "enabling", "efficient", "fine-tuning", "image", "generation", "learn", "concepts", "styles", "subjects"]}, {"id": "term-lora-fusion", "t": "LoRA Fusion", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The technique of combining multiple LoRA adapters trained for different tasks or styles into a single model by merging...", "l": "l", "k": ["lora", "fusion", "technique", "combining", "multiple", "adapters", "trained", "different", "tasks", "styles", "single", "model", "merging", "dynamically", "weighting"]}, {"id": "term-lora-land", "t": "LoRA-Land", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark and analysis of Low-Rank Adaptation fine-tuned models across 25 tasks. Studies the effectiveness and...", "l": "l", "k": ["lora-land", "benchmark", "analysis", "low-rank", "adaptation", "fine-tuned", "models", "across", "tasks", "studies", "effectiveness", "characteristics", "lora", "fine-tuning", "compared"]}, {"id": "term-loss-function", "t": "Loss Function", "tg": ["Training", "Math"], "d": "general", "x": "A mathematical function measuring how wrong a model's predictions are. Training aims to minimize this loss, with common...", "l": "l", "k": ["loss", "function", "mathematical", "measuring", "wrong", "model", "predictions", "training", "aims", "minimize", "common", "examples", "including", "cross-entropy", "mean"]}, {"id": "term-loss-scaling", "t": "Loss Scaling", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "A technique used in FP16 mixed precision training that multiplies the loss by a large factor before backpropagation to...", "l": "l", "k": ["loss", "scaling", "technique", "fp16", "mixed", "precision", "training", "multiplies", "large", "factor", "backpropagation", "prevent", "small", "gradient", "values"]}, {"id": "term-lost-in-the-middle", "t": "Lost in the Middle", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A documented phenomenon where language models perform worse at retrieving and using information placed in the middle of...", "l": "l", "k": ["lost", "middle", "documented", "phenomenon", "language", "models", "perform", "worse", "retrieving", "information", "placed", "context", "window", "compared", "beginning"]}, {"id": "term-lotfi-zadeh", "t": "Lotfi Zadeh", "tg": ["History", "Pioneers"], "d": "history", "x": "Azerbaijani-American mathematician and computer scientist (1921-2017) who invented fuzzy logic and fuzzy set theory in...", "l": "l", "k": ["lotfi", "zadeh", "azerbaijani-american", "mathematician", "computer", "scientist", "1921-2017", "invented", "fuzzy", "logic", "theory", "providing", "mathematical", "tools", "handling"]}, {"id": "term-lottery-ticket-hypothesis", "t": "Lottery Ticket Hypothesis", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The theory that dense randomly-initialized networks contain sparse subnetworks (winning tickets) that can be trained in...", "l": "l", "k": ["lottery", "ticket", "hypothesis", "theory", "dense", "randomly-initialized", "networks", "contain", "sparse", "subnetworks", "winning", "tickets", "trained", "isolation", "match"]}, {"id": "term-louvain-algorithm", "t": "Louvain Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A community detection algorithm for large networks that optimizes modularity through a greedy hierarchical approach....", "l": "l", "k": ["louvain", "algorithm", "community", "detection", "large", "networks", "optimizes", "modularity", "greedy", "hierarchical", "approach", "iteratively", "merges", "nodes", "communities"]}, {"id": "term-low-rank-approximation", "t": "Low-Rank Approximation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A technique that approximates a matrix with a lower-rank matrix to reduce dimensionality computational cost or noise....", "l": "l", "k": ["low-rank", "approximation", "technique", "approximates", "matrix", "lower-rank", "reduce", "dimensionality", "computational", "cost", "noise", "optimal", "rank-k", "given", "truncated"]}, {"id": "term-low-rank-factorization", "t": "Low-Rank Factorization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A model compression technique that approximates large weight matrices as products of smaller matrices with reduced...", "l": "l", "k": ["low-rank", "factorization", "model", "compression", "technique", "approximates", "large", "weight", "matrices", "products", "smaller", "reduced", "rank", "reduces", "parameters"]}, {"id": "term-lowest-common-ancestor-algorithm", "t": "Lowest Common Ancestor Algorithm", "tg": ["Algorithms", "Fundamentals", "Graph"], "d": "algorithms", "x": "An algorithm that finds the deepest node that is an ancestor of two given nodes in a tree. Can be answered in O(1) time...", "l": "l", "k": ["lowest", "common", "ancestor", "algorithm", "finds", "deepest", "node", "given", "nodes", "tree", "answered", "time", "per", "query", "preprocessing"]}, {"id": "term-lpddr5", "t": "LPDDR5", "tg": ["Memory", "Mobile", "Edge"], "d": "hardware", "x": "Low Power Double Data Rate 5 memory used in mobile devices and edge AI processors. Balances bandwidth and power...", "l": "l", "k": ["lpddr5", "low", "power", "double", "data", "rate", "memory", "mobile", "devices", "edge", "processors", "balances", "bandwidth", "efficiency", "on-device"]}, {"id": "term-lsf-load-sharing-facility", "t": "LSF (Load Sharing Facility)", "tg": ["Infrastructure", "IBM", "Scheduling"], "d": "hardware", "x": "IBM workload management platform for scheduling and managing jobs across computing clusters. Used in some enterprise AI...", "l": "l", "k": ["lsf", "load", "sharing", "facility", "ibm", "workload", "management", "platform", "scheduling", "managing", "jobs", "across", "computing", "clusters", "enterprise"]}, {"id": "term-lsm-tree-algorithm", "t": "LSM Tree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "Log-Structured Merge Tree is a data structure optimized for write-heavy workloads that buffers writes in memory and...", "l": "l", "k": ["lsm", "tree", "algorithm", "log-structured", "merge", "data", "structure", "optimized", "write-heavy", "workloads", "buffers", "writes", "memory", "periodically", "merges"]}, {"id": "term-lstm", "t": "LSTM (Long Short-Term Memory)", "tg": ["Architecture", "Historical"], "d": "models", "x": "A recurrent neural network architecture designed to capture long-range dependencies in sequences. Was the dominant NLP...", "l": "l", "k": ["lstm", "long", "short-term", "memory", "recurrent", "neural", "network", "architecture", "designed", "capture", "long-range", "dependencies", "sequences", "dominant", "nlp"]}, {"id": "term-lsun", "t": "LSUN", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "The Large-scale Scene Understanding dataset containing millions of labeled images across 10 scene categories and 20...", "l": "l", "k": ["lsun", "large-scale", "scene", "understanding", "dataset", "containing", "millions", "labeled", "images", "across", "categories", "object", "benchmarking", "image", "generation"]}, {"id": "term-lu-decomposition", "t": "LU Decomposition", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "A matrix factorization method that decomposes a matrix into the product of a lower triangular matrix and an upper...", "l": "l", "k": ["decomposition", "matrix", "factorization", "method", "decomposes", "product", "lower", "triangular", "upper", "solving", "linear", "systems", "computing", "determinants", "inverses"]}, {"id": "term-lucas-kanade-optical-flow", "t": "Lucas-Kanade Optical Flow", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "A differential method for estimating optical flow that assumes constant flow within a local neighborhood of each pixel....", "l": "l", "k": ["lucas-kanade", "optical", "flow", "differential", "method", "estimating", "assumes", "constant", "within", "local", "neighborhood", "pixel", "solves", "over-determined", "system"]}, {"id": "term-lumi-supercomputer", "t": "LUMI Supercomputer", "tg": ["Supercomputer", "AMD", "Europe"], "d": "hardware", "x": "European pre-exascale supercomputer in Finland using AMD EPYC CPUs and AMD Instinct MI250X GPUs. One of the most...", "l": "l", "k": ["lumi", "supercomputer", "european", "pre-exascale", "finland", "amd", "epyc", "cpus", "instinct", "mi250x", "gpus", "powerful", "systems", "available", "research"]}, {"id": "term-lumiere", "t": "Lumiere", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A text-to-video diffusion model from Google Research that generates temporally coherent videos using a Space-Time U-Net...", "l": "l", "k": ["lumiere", "text-to-video", "diffusion", "model", "google", "research", "generates", "temporally", "coherent", "videos", "space-time", "u-net", "architecture", "global", "temporal"]}, {"id": "term-lustre-file-system", "t": "Lustre File System", "tg": ["Storage", "Open Source", "Distributed"], "d": "hardware", "x": "Open-source parallel distributed file system designed for large-scale cluster computing. Used by many of the world...", "l": "l", "k": ["lustre", "file", "system", "open-source", "parallel", "distributed", "designed", "large-scale", "cluster", "computing", "world", "largest", "supercomputers", "high-bandwidth", "access"]}, {"id": "term-lvis", "t": "LVIS", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "Large Vocabulary Instance Segmentation dataset containing over 2 million instance annotations for 1203 object...", "l": "l", "k": ["lvis", "large", "vocabulary", "instance", "segmentation", "dataset", "containing", "million", "annotations", "object", "categories", "coco", "images", "addresses", "long-tail"]}, {"id": "term-lvlm-ehub", "t": "LVLM-eHub", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A comprehensive evaluation hub for large vision-language models testing across multiple multimodal capabilities....", "l": "l", "k": ["lvlm-ehub", "comprehensive", "evaluation", "hub", "large", "vision-language", "models", "testing", "across", "multiple", "multimodal", "capabilities", "provides", "standardized", "comparison"]}, {"id": "term-lwm", "t": "LWM", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "Large World Model is a general-purpose model trained on long video and text sequences that can understand and generate...", "l": "l", "k": ["lwm", "large", "world", "model", "general-purpose", "trained", "long", "video", "text", "sequences", "understand", "generate", "content", "across", "modalities"]}, {"id": "term-lyra", "t": "Lyra", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A low-bitrate neural speech codec from Google that delivers high-quality speech at 3 kbps using a generative model for...", "l": "l", "k": ["lyra", "low-bitrate", "neural", "speech", "codec", "google", "delivers", "high-quality", "kbps", "generative", "model", "real-time", "communication"]}, {"id": "term-m2m-100", "t": "M2M-100", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A multilingual machine translation model from Meta that can translate directly between any pair of 100 languages...", "l": "m", "k": ["m2m-100", "multilingual", "machine", "translation", "model", "meta", "translate", "directly", "pair", "languages", "without", "relying", "english", "pivot"]}, {"id": "term-m3gnet", "t": "M3GNet", "tg": ["Models", "Scientific"], "d": "models", "x": "A universal graph neural network potential for materials that learns interatomic interactions across the periodic table...", "l": "m", "k": ["m3gnet", "universal", "graph", "neural", "network", "potential", "materials", "learns", "interatomic", "interactions", "across", "periodic", "table", "molecular", "dynamics"]}, {"id": "term-mace", "t": "MACE", "tg": ["Models", "Scientific"], "d": "models", "x": "Multi-ACE is a higher-order equivariant message passing neural network for atomistic simulations that achieves high...", "l": "m", "k": ["mace", "multi-ace", "higher-order", "equivariant", "message", "passing", "neural", "network", "atomistic", "simulations", "achieves", "high", "accuracy", "computational", "efficiency"]}, {"id": "term-machine-learning", "t": "Machine Learning (ML)", "tg": ["Field", "Fundamentals"], "d": "general", "x": "A branch of AI where systems learn patterns from data rather than being explicitly programmed. Includes supervised,...", "l": "m", "k": ["machine", "learning", "branch", "systems", "learn", "patterns", "data", "rather", "explicitly", "programmed", "includes", "supervised", "unsupervised", "reinforcement", "approaches"]}, {"id": "term-machine-learning-history", "t": "Machine Learning History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of machine learning from Arthur Samuel's checkers program (1959) through the perceptron (1958)...", "l": "m", "k": ["machine", "learning", "history", "evolution", "arthur", "samuel", "checkers", "program", "perceptron", "backpropagation", "svms", "1990s", "deep", "revolution", "2012-present"]}, {"id": "term-mt-evaluation", "t": "Machine Translation Evaluation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "Methods for assessing translation quality including automatic metrics like BLEU, METEOR, and COMET that compare system...", "l": "m", "k": ["machine", "translation", "evaluation", "methods", "assessing", "quality", "including", "automatic", "metrics", "bleu", "meteor", "comet", "compare", "system", "output"]}, {"id": "term-machine-translation-history", "t": "Machine Translation History", "tg": ["History", "Milestones"], "d": "history", "x": "The history of using computers to translate between human languages dating to the Georgetown-IBM experiment in 1954....", "l": "m", "k": ["machine", "translation", "history", "computers", "translate", "human", "languages", "dating", "georgetown-ibm", "experiment", "early", "rule-based", "approaches", "gave", "statistical"]}, {"id": "term-machine-unlearning", "t": "Machine Unlearning", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "Techniques for removing the influence of specific training data from a trained model, motivated by privacy rights such...", "l": "m", "k": ["machine", "unlearning", "techniques", "removing", "influence", "specific", "training", "data", "trained", "model", "motivated", "privacy", "rights", "right", "forgotten"]}, {"id": "term-macy-conferences", "t": "Macy Conferences", "tg": ["History", "Milestones"], "d": "history", "x": "A series of interdisciplinary conferences held from 1946 to 1953 that brought together researchers in cybernetics,...", "l": "m", "k": ["macy", "conferences", "series", "interdisciplinary", "held", "brought", "together", "researchers", "cybernetics", "neuroscience", "psychology", "mathematics", "fostering", "cross-disciplinary", "ideas"]}, {"id": "term-madlad-400", "t": "MADLAD-400", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A large-scale multilingual machine translation model from Google trained on web-crawled data in over 400 languages for...", "l": "m", "k": ["madlad-400", "large-scale", "multilingual", "machine", "translation", "model", "google", "trained", "web-crawled", "data", "languages", "broad", "coverage"]}, {"id": "term-mae", "t": "MAE", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Masked Autoencoder, a self-supervised learning method for vision that randomly masks large portions of image patches...", "l": "m", "k": ["mae", "masked", "autoencoder", "self-supervised", "learning", "method", "vision", "randomly", "masks", "large", "portions", "image", "patches", "trains", "model"]}, {"id": "term-magic3d", "t": "Magic3D", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A two-stage text-to-3D generation framework from NVIDIA that creates high-resolution 3D meshes using coarse-to-fine...", "l": "m", "k": ["magic3d", "two-stage", "text-to-3d", "generation", "framework", "nvidia", "creates", "high-resolution", "meshes", "coarse-to-fine", "optimization", "diffusion", "priors"]}, {"id": "term-magicanimate", "t": "MagicAnimate", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A diffusion-based model for human image animation that generates temporally consistent videos from a reference image...", "l": "m", "k": ["magicanimate", "diffusion-based", "model", "human", "image", "animation", "generates", "temporally", "consistent", "videos", "reference", "motion", "sequence"]}, {"id": "term-magicbrush", "t": "MagicBrush", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "An instruction-guided image editing model trained on manually annotated editing triplets for precise text-guided local...", "l": "m", "k": ["magicbrush", "instruction-guided", "image", "editing", "model", "trained", "manually", "annotated", "triplets", "precise", "text-guided", "local", "global", "modifications"]}, {"id": "term-magicdrive", "t": "MagicDrive", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "A street-view generation model conditioned on 3D geometry and road maps for creating diverse and controllable driving...", "l": "m", "k": ["magicdrive", "street-view", "generation", "model", "conditioned", "geometry", "road", "maps", "creating", "diverse", "controllable", "driving", "scene", "images", "training"]}, {"id": "term-magicoder-oss-instruct", "t": "Magicoder OSS-Instruct", "tg": ["Training Corpus", "Code"], "d": "datasets", "x": "A dataset of coding instruction examples generated by prompting LLMs with real open-source code snippets. Produces more...", "l": "m", "k": ["magicoder", "oss-instruct", "dataset", "coding", "instruction", "examples", "generated", "prompting", "llms", "real", "open-source", "code", "snippets", "produces", "realistic"]}, {"id": "term-magnetic-core-memory", "t": "Magnetic Core Memory", "tg": ["Historical", "Memory", "Technology"], "d": "hardware", "x": "Early computer memory technology using tiny magnetic ferrite cores threaded on wires. Dominated computer memory from...", "l": "m", "k": ["magnetic", "core", "memory", "early", "computer", "technology", "tiny", "ferrite", "cores", "threaded", "wires", "dominated", "mid-1950s", "1970s", "semiconductor"]}, {"id": "term-magnetic-ram", "t": "Magnetic RAM", "tg": ["Memory", "Emerging", "Non-Volatile"], "d": "hardware", "x": "Non-volatile memory technology using magnetic states to store data with DRAM-like speed. Promising for AI applications...", "l": "m", "k": ["magnetic", "ram", "non-volatile", "memory", "technology", "states", "store", "data", "dram-like", "speed", "promising", "applications", "requiring", "fast", "persistent"]}, {"id": "term-magnetic-tape-storage", "t": "Magnetic Tape Storage", "tg": ["Historical", "Storage", "Archival"], "d": "hardware", "x": "Sequential data storage medium using magnetic-coated tape. Used for computer data storage since the 1950s and still...", "l": "m", "k": ["magnetic", "tape", "storage", "sequential", "data", "medium", "magnetic-coated", "computer", "1950s", "employed", "today", "archival", "backup", "large", "datasets"]}, {"id": "term-magnitude-pruning", "t": "Magnitude Pruning", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A model compression technique that removes weights with the smallest absolute values. Based on the assumption that...", "l": "m", "k": ["magnitude", "pruning", "model", "compression", "technique", "removes", "weights", "smallest", "absolute", "values", "based", "assumption", "small", "contribute", "least"]}, {"id": "term-mahalanobis-distance", "t": "Mahalanobis Distance", "tg": ["Statistics", "Metrics"], "d": "datasets", "x": "A distance metric that accounts for correlations between variables by measuring the number of standard deviations a...", "l": "m", "k": ["mahalanobis", "distance", "metric", "accounts", "correlations", "variables", "measuring", "number", "standard", "deviations", "point", "mean", "distribution", "inverse", "covariance"]}, {"id": "term-maieutic-prompting", "t": "Maieutic Prompting", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting method inspired by the Socratic maieutic approach that generates a tree of explanations with logical...", "l": "m", "k": ["maieutic", "prompting", "method", "inspired", "socratic", "approach", "generates", "tree", "explanations", "logical", "relationships", "uses", "abductive", "reasoning", "identify"]}, {"id": "term-make-a-scene", "t": "Make-A-Scene", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A text-to-image generation model from Meta that incorporates optional scene layouts as additional conditioning for...", "l": "m", "k": ["make-a-scene", "text-to-image", "generation", "model", "meta", "incorporates", "optional", "scene", "layouts", "additional", "conditioning", "controllable", "image"]}, {"id": "term-make-a-video", "t": "Make-A-Video", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A text-to-video generation model from Meta AI that extends text-to-image diffusion models to generate short video clips...", "l": "m", "k": ["make-a-video", "text-to-video", "generation", "model", "meta", "extends", "text-to-image", "diffusion", "models", "generate", "short", "video", "clips", "text", "descriptions"]}, {"id": "term-make-an-audio", "t": "Make-An-Audio", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A text-to-audio generation framework that uses a latent diffusion model with temporal-aware conditioning for generating...", "l": "m", "k": ["make-an-audio", "text-to-audio", "generation", "framework", "uses", "latent", "diffusion", "model", "temporal-aware", "conditioning", "generating", "diverse", "audio", "text", "descriptions"]}, {"id": "term-malicious-use-of-ai", "t": "Malicious Use of AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The deliberate use of AI systems to cause harm including creating weapons surveillance tools disinformation campaigns...", "l": "m", "k": ["malicious", "deliberate", "systems", "cause", "harm", "including", "creating", "weapons", "surveillance", "tools", "disinformation", "campaigns", "cyberattacks", "fraud", "harassment"]}, {"id": "term-mamba", "t": "Mamba", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A selective state space model architecture that uses input-dependent selection mechanisms to efficiently process...", "l": "m", "k": ["mamba", "selective", "state", "space", "model", "architecture", "uses", "input-dependent", "selection", "mechanisms", "efficiently", "process", "sequences", "linear", "scaling"]}, {"id": "term-mamba-architecture", "t": "Mamba Architecture", "tg": ["History", "Systems"], "d": "history", "x": "A selective state space model architecture introduced by Albert Gu and Tri Dao in December 2023. Mamba provides an...", "l": "m", "k": ["mamba", "architecture", "selective", "state", "space", "model", "introduced", "albert", "tri", "dao", "december", "provides", "alternative", "transformers", "linear-time"]}, {"id": "term-mamba-2", "t": "Mamba-2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An improved state space model architecture with a refined selective scan mechanism that increases hardware utilization...", "l": "m", "k": ["mamba-2", "improved", "state", "space", "model", "architecture", "refined", "selective", "scan", "mechanism", "increases", "hardware", "utilization", "training", "throughput"]}, {"id": "term-mamba-codestral", "t": "Mamba-Codestral", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A code-generation model from Mistral AI that uses the Mamba state space architecture for efficient code completion and...", "l": "m", "k": ["mamba-codestral", "code-generation", "model", "mistral", "uses", "mamba", "state", "space", "architecture", "efficient", "code", "completion", "generation", "tasks"]}, {"id": "term-manchester-baby", "t": "Manchester Baby", "tg": ["History", "Systems"], "d": "history", "x": "The Manchester Small-Scale Experimental Machine completed in 1948 at the University of Manchester was the first...", "l": "m", "k": ["manchester", "baby", "small-scale", "experimental", "machine", "completed", "university", "stored-program", "computer", "run", "program", "built", "frederic", "williams", "tom"]}, {"id": "term-mandatory-reporting-for-ai", "t": "Mandatory Reporting for AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "Legal requirements for organizations to report AI-related incidents failures or safety concerns to regulatory...", "l": "m", "k": ["mandatory", "reporting", "legal", "requirements", "organizations", "report", "ai-related", "incidents", "failures", "safety", "concerns", "regulatory", "authorities", "analogous", "healthcare"]}, {"id": "term-manhattan-distance", "t": "Manhattan Distance", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A distance metric computed as the sum of absolute differences across all dimensions between two points, also known as...", "l": "m", "k": ["manhattan", "distance", "metric", "computed", "sum", "absolute", "differences", "across", "dimensions", "points", "known", "taxicab", "measures", "along", "axis-aligned"]}, {"id": "term-mann-whitney-u-test", "t": "Mann-Whitney U Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A non-parametric test that compares the distributions of two independent groups by ranking all observations and testing...", "l": "m", "k": ["mann-whitney", "test", "non-parametric", "compares", "distributions", "independent", "groups", "ranking", "observations", "testing", "group", "tends", "larger", "values", "assume"]}, {"id": "term-mantis", "t": "Mantis", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multimodal model designed for multi-image reasoning that can compare and analyze relationships across multiple input...", "l": "m", "k": ["mantis", "multimodal", "model", "designed", "multi-image", "reasoning", "compare", "analyze", "relationships", "across", "multiple", "input", "images", "simultaneously"]}, {"id": "term-manual-chain-of-thought", "t": "Manual Chain-of-Thought", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "The practice of hand-crafting step-by-step reasoning demonstrations within few-shot prompts, where a human explicitly...", "l": "m", "k": ["manual", "chain-of-thought", "practice", "hand-crafting", "step-by-step", "reasoning", "demonstrations", "within", "few-shot", "prompts", "human", "explicitly", "writes", "intermediate", "steps"]}, {"id": "term-map-neo", "t": "MAP-Neo", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A fully open-source bilingual language model with transparent training data and code that aims for complete...", "l": "m", "k": ["map-neo", "fully", "open-source", "bilingual", "language", "model", "transparent", "training", "data", "code", "aims", "complete", "reproducibility", "llm", "research"]}, {"id": "term-mappo", "t": "MAPPO", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "Multi-Agent Proximal Policy Optimization, an extension of PPO to multi-agent settings that uses shared parameters and a...", "l": "m", "k": ["mappo", "multi-agent", "proximal", "policy", "optimization", "extension", "ppo", "settings", "uses", "shared", "parameters", "centralized", "value", "function", "achieves"]}, {"id": "term-mappo-algorithm", "t": "MAPPO Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "Multi-Agent Proximal Policy Optimization adapts PPO for multi-agent settings with a shared policy or centralized value...", "l": "m", "k": ["mappo", "algorithm", "multi-agent", "proximal", "policy", "optimization", "adapts", "ppo", "settings", "shared", "centralized", "value", "function", "achieves", "strong"]}, {"id": "term-mapreduce-algorithm", "t": "MapReduce Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A distributed programming model for processing large datasets across a cluster by splitting work into independent map...", "l": "m", "k": ["mapreduce", "algorithm", "distributed", "programming", "model", "processing", "large", "datasets", "across", "cluster", "splitting", "work", "independent", "map", "tasks"]}, {"id": "term-maptr", "t": "MapTR", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "A structured end-to-end Transformer for online vectorized HD map construction that predicts map elements as point sets...", "l": "m", "k": ["maptr", "structured", "end-to-end", "transformer", "online", "vectorized", "map", "construction", "predicts", "elements", "point", "sets", "autonomous", "driving"]}, {"id": "term-marching-cubes-algorithm", "t": "Marching Cubes Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A surface extraction algorithm that generates a triangle mesh from a three-dimensional scalar field. Examines each cube...", "l": "m", "k": ["marching", "cubes", "algorithm", "surface", "extraction", "generates", "triangle", "mesh", "three-dimensional", "scalar", "field", "examines", "cube", "eight", "neighboring"]}, {"id": "term-marigold", "t": "Marigold", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A diffusion-based monocular depth estimation model that repurposes pre-trained latent diffusion models for zero-shot...", "l": "m", "k": ["marigold", "diffusion-based", "monocular", "depth", "estimation", "model", "repurposes", "pre-trained", "latent", "diffusion", "models", "zero-shot", "affine-invariant", "prediction"]}, {"id": "term-markdown-prompting", "t": "Markdown Prompting", "tg": ["Prompt Engineering", "Output Format"], "d": "hardware", "x": "The use of Markdown formatting conventions such as headers, lists, code blocks, and emphasis within prompts to organize...", "l": "m", "k": ["markdown", "prompting", "formatting", "conventions", "headers", "lists", "code", "blocks", "emphasis", "within", "prompts", "organize", "instructions", "improve", "model"]}, {"id": "term-markov-chain", "t": "Markov Chain", "tg": ["Machine Learning", "Probability"], "d": "algorithms", "x": "A stochastic model describing a sequence of states where the probability of transitioning to the next state depends...", "l": "m", "k": ["markov", "chain", "stochastic", "model", "describing", "sequence", "states", "probability", "transitioning", "next", "state", "depends", "current", "property", "preceding"]}, {"id": "term-markov-chain-monte-carlo", "t": "Markov Chain Monte Carlo", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A class of algorithms that sample from probability distributions by constructing a Markov chain whose stationary...", "l": "m", "k": ["markov", "chain", "monte", "carlo", "class", "algorithms", "sample", "probability", "distributions", "constructing", "whose", "stationary", "distribution", "target", "common"]}, {"id": "term-markov-decision-process", "t": "Markov Decision Process (MDP)", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A formal mathematical framework for sequential decision-making defined by states, actions, transition probabilities,...", "l": "m", "k": ["markov", "decision", "process", "mdp", "formal", "mathematical", "framework", "sequential", "decision-making", "defined", "states", "actions", "transition", "probabilities", "rewards"]}, {"id": "term-marvin-minsky", "t": "Marvin Minsky", "tg": ["History", "Pioneers"], "d": "history", "x": "American cognitive scientist and AI pioneer (1927-2016) who co-founded the MIT AI Laboratory, developed the concept of...", "l": "m", "k": ["marvin", "minsky", "american", "cognitive", "scientist", "pioneer", "1927-2016", "co-founded", "mit", "laboratory", "developed", "concept", "frames", "knowledge", "representation"]}, {"id": "term-masakhaner", "t": "MasakhaNER", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "A named entity recognition dataset for 10 African languages. Part of the Masakhane initiative to advance NLP for...", "l": "m", "k": ["masakhaner", "named", "entity", "recognition", "dataset", "african", "languages", "part", "masakhane", "initiative", "advance", "nlp", "community-driven", "annotation"]}, {"id": "term-mask", "t": "Mask / Masking", "tg": ["Technique", "Training"], "d": "general", "x": "Hiding or ignoring certain parts of data during training or inference. In BERT, random tokens are masked for...", "l": "m", "k": ["mask", "masking", "hiding", "ignoring", "certain", "parts", "data", "training", "inference", "bert", "random", "tokens", "masked", "prediction", "transformers"]}, {"id": "term-mask-rcnn", "t": "Mask R-CNN", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "An instance segmentation framework that extends Faster R-CNN by adding a parallel branch for pixel-level mask...", "l": "m", "k": ["mask", "r-cnn", "instance", "segmentation", "framework", "extends", "faster", "adding", "parallel", "branch", "pixel-level", "prediction", "alongside", "existing", "bounding"]}, {"id": "term-mask2former", "t": "Mask2Former", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A universal image segmentation architecture that unifies semantic, instance, and panoptic segmentation through masked...", "l": "m", "k": ["mask2former", "universal", "image", "segmentation", "architecture", "unifies", "semantic", "instance", "panoptic", "masked", "attention", "learnable", "object", "queries", "processed"]}, {"id": "term-masked-autoencoder-mae", "t": "Masked Autoencoder (MAE)", "tg": ["Models", "Technical", "Vision", "Fundamentals"], "d": "models", "x": "A self-supervised vision pre-training method that masks random patches of an image and trains an autoencoder to...", "l": "m", "k": ["masked", "autoencoder", "mae", "self-supervised", "vision", "pre-training", "method", "masks", "random", "patches", "image", "trains", "reconstruct", "missing"]}, {"id": "term-masked-language-modeling", "t": "Masked Language Modeling", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A pretraining objective where random tokens in the input are replaced with a mask token and the model learns to predict...", "l": "m", "k": ["masked", "language", "modeling", "pretraining", "objective", "random", "tokens", "input", "replaced", "mask", "token", "model", "learns", "predict", "original"]}, {"id": "term-massive", "t": "MASSIVE", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "Multilingual Amazon SLURP for Slot filling Intents and Virtual assistant Evaluation a dataset covering 51 languages...", "l": "m", "k": ["massive", "multilingual", "amazon", "slurp", "slot", "filling", "intents", "virtual", "assistant", "evaluation", "dataset", "covering", "languages", "million", "annotated"]}, {"id": "term-matcha", "t": "MatCha", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A mathematical chart understanding model from Google that pre-trains on chart derendering and numerical reasoning for...", "l": "m", "k": ["matcha", "mathematical", "chart", "understanding", "model", "google", "pre-trains", "derendering", "numerical", "reasoning", "visual", "math", "question", "answering"]}, {"id": "term-matched-filter", "t": "Matched Filter", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A signal processing filter that maximizes the signal-to-noise ratio for detecting a known signal in additive noise. The...", "l": "m", "k": ["matched", "filter", "signal", "processing", "maximizes", "signal-to-noise", "ratio", "detecting", "known", "additive", "noise", "impulse", "response", "time-reversed", "conjugate"]}, {"id": "term-math", "t": "MATH", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A dataset of 12500 competition mathematics problems from AMC AIME and other competitions. Tests advanced mathematical...", "l": "m", "k": ["math", "dataset", "competition", "mathematics", "problems", "amc", "aime", "competitions", "tests", "advanced", "mathematical", "reasoning", "including", "algebra", "geometry"]}, {"id": "term-math-benchmark", "t": "MATH Benchmark", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A challenging benchmark of 12,500 competition-level mathematics problems spanning seven subjects from algebra to number...", "l": "m", "k": ["math", "benchmark", "challenging", "competition-level", "mathematics", "problems", "spanning", "seven", "subjects", "algebra", "number", "theory", "requiring", "sophisticated", "mathematical"]}, {"id": "term-mathgpt", "t": "MathGPT", "tg": ["Models", "Technical"], "d": "models", "x": "A specialized language model designed for mathematical problem solving that combines symbolic and neural approaches....", "l": "m", "k": ["mathgpt", "specialized", "language", "model", "designed", "mathematical", "problem", "solving", "combines", "symbolic", "neural", "approaches", "demonstrates", "improved", "accuracy"]}, {"id": "term-mathinstruct", "t": "MathInstruct", "tg": ["Training Corpus", "NLP", "Reasoning"], "d": "datasets", "x": "A curated dataset of mathematical problem-solving demonstrations for instruction tuning language models on mathematical...", "l": "m", "k": ["mathinstruct", "curated", "dataset", "mathematical", "problem-solving", "demonstrations", "instruction", "tuning", "language", "models", "reasoning", "across", "diverse", "problem", "types"]}, {"id": "term-mathstral", "t": "Mathstral", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A mathematics-specialized language model from Mistral AI optimized for mathematical reasoning and problem-solving with...", "l": "m", "k": ["mathstral", "mathematics-specialized", "language", "model", "mistral", "optimized", "mathematical", "reasoning", "problem-solving", "strong", "performance", "math", "benchmarks"]}, {"id": "term-mathverse", "t": "MathVerse", "tg": ["Benchmark", "Multimodal", "Reasoning"], "d": "datasets", "x": "A multimodal math benchmark testing visual mathematical reasoning with problems that require understanding of...", "l": "m", "k": ["mathverse", "multimodal", "math", "benchmark", "testing", "visual", "mathematical", "reasoning", "problems", "require", "understanding", "figures", "diagrams"]}, {"id": "term-mathvista", "t": "MathVista", "tg": ["Benchmark", "Multimodal", "Reasoning"], "d": "datasets", "x": "A multimodal mathematical reasoning benchmark combining visual context with mathematical problems. Tests the ability to...", "l": "m", "k": ["mathvista", "multimodal", "mathematical", "reasoning", "benchmark", "combining", "visual", "context", "problems", "tests", "ability", "perform", "charts", "diagrams", "figures"]}, {"id": "term-matrix-exponentiation", "t": "Matrix Exponentiation", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A technique for computing the exponential of a matrix used in solving systems of linear differential equations and...", "l": "m", "k": ["matrix", "exponentiation", "technique", "computing", "exponential", "solving", "systems", "linear", "differential", "equations", "graph", "path", "counts", "methods", "include"]}, {"id": "term-matrix-factorization", "t": "Matrix Factorization", "tg": ["Models", "Fundamentals", "Recommendation"], "d": "models", "x": "A recommendation technique that decomposes a user-item interaction matrix into lower-dimensional latent factor matrices...", "l": "m", "k": ["matrix", "factorization", "recommendation", "technique", "decomposes", "user-item", "interaction", "lower-dimensional", "latent", "factor", "matrices", "predict", "missing", "ratings"]}, {"id": "term-matrix-multiplication", "t": "Matrix Multiplication", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "The fundamental algebraic operation of multiplying two matrices used extensively in neural networks for layer...", "l": "m", "k": ["matrix", "multiplication", "fundamental", "algebraic", "operation", "multiplying", "matrices", "extensively", "neural", "networks", "layer", "computations", "attention", "mechanisms", "embedding"]}, {"id": "term-matryoshka-embeddings", "t": "Matryoshka Embeddings", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An embedding training approach that produces vectors where any prefix of the full embedding is itself a useful...", "l": "m", "k": ["matryoshka", "embeddings", "embedding", "training", "approach", "produces", "vectors", "prefix", "full", "itself", "useful", "allowing", "flexible", "dimensionality", "reduction"]}, {"id": "term-matterport3d", "t": "Matterport3D", "tg": ["Benchmark", "3D", "Computer Vision"], "d": "datasets", "x": "A large RGB-D dataset of 90 building-scale scenes with surface reconstructions and semantic annotations. Provides...", "l": "m", "k": ["matterport3d", "large", "rgb-d", "dataset", "building-scale", "scenes", "surface", "reconstructions", "semantic", "annotations", "provides", "photorealistic", "environments", "indoor", "scene"]}, {"id": "term-matthews-correlation-coefficient", "t": "Matthews Correlation Coefficient", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A balanced classification metric computed from all four confusion matrix values (TP, TN, FP, FN) that produces a value...", "l": "m", "k": ["matthews", "correlation", "coefficient", "balanced", "classification", "metric", "computed", "four", "confusion", "matrix", "values", "produces", "value", "indicates", "perfect"]}, {"id": "term-mavil", "t": "MAViL", "tg": ["Models", "Technical", "Audio", "Vision"], "d": "models", "x": "Masked Audio-Visual Learner is a self-supervised model that learns audio-visual representations through masked...", "l": "m", "k": ["mavil", "masked", "audio-visual", "learner", "self-supervised", "model", "learns", "representations", "prediction", "across", "audio", "video", "modalities"]}, {"id": "term-max-pooling", "t": "Max Pooling", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A downsampling operation that selects the maximum value within each pooling window, reducing spatial dimensions while...", "l": "m", "k": ["max", "pooling", "downsampling", "operation", "selects", "maximum", "value", "within", "window", "reducing", "spatial", "dimensions", "retaining", "prominent", "features"]}, {"id": "term-maximal-independent-set-algorithm", "t": "Maximal Independent Set Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm that finds a maximal set of vertices in a graph such that no two are adjacent. Luby's parallel algorithm...", "l": "m", "k": ["maximal", "independent", "algorithm", "finds", "vertices", "graph", "adjacent", "luby", "parallel", "computes", "log", "rounds", "fundamental", "distributed", "computing"]}, {"id": "term-maximal-marginal-relevance", "t": "Maximal Marginal Relevance", "tg": ["Retrieval", "Diversity"], "d": "general", "x": "A retrieval diversification algorithm (MMR) that iteratively selects documents by balancing relevance to the query...", "l": "m", "k": ["maximal", "marginal", "relevance", "retrieval", "diversification", "algorithm", "mmr", "iteratively", "selects", "documents", "balancing", "query", "against", "novelty", "relative"]}, {"id": "term-maximum-a-posteriori", "t": "Maximum A Posteriori", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A Bayesian point estimation method that finds the parameter values maximizing the posterior probability, combining the...", "l": "m", "k": ["maximum", "posteriori", "bayesian", "point", "estimation", "method", "finds", "parameter", "values", "maximizing", "posterior", "probability", "combining", "likelihood", "data"]}, {"id": "term-maximum-a-posteriori-policy-optimization", "t": "Maximum a Posteriori Policy Optimization", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "An RL algorithm that frames policy improvement as supervised learning by optimizing a lower bound on the expected...", "l": "m", "k": ["maximum", "posteriori", "policy", "optimization", "algorithm", "frames", "improvement", "supervised", "learning", "optimizing", "lower", "bound", "expected", "return", "decouples"]}, {"id": "term-maximum-bipartite-matching", "t": "Maximum Bipartite Matching", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm that finds the largest set of edges in a bipartite graph such that no two edges share a vertex. The...", "l": "m", "k": ["maximum", "bipartite", "matching", "algorithm", "finds", "largest", "edges", "graph", "share", "vertex", "hopcroft-karp", "solves", "sqrt", "time"]}, {"id": "term-maximum-entropy-rl", "t": "Maximum Entropy RL", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An RL framework that augments the standard return objective with policy entropy, encouraging agents to act as randomly...", "l": "m", "k": ["maximum", "entropy", "framework", "augments", "standard", "return", "objective", "policy", "encouraging", "agents", "act", "randomly", "possible", "achieving", "high"]}, {"id": "term-maximum-flow-algorithm", "t": "Maximum Flow Algorithm", "tg": ["Algorithms", "Fundamentals", "Graph"], "d": "algorithms", "x": "A class of algorithms that find the maximum amount of flow that can be sent from a source to a sink in a flow network....", "l": "m", "k": ["maximum", "flow", "algorithm", "class", "algorithms", "find", "amount", "sent", "source", "sink", "network", "max-flow", "min-cut", "theorem", "states"]}, {"id": "term-maximum-likelihood-estimation", "t": "Maximum Likelihood Estimation", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A method of estimating the parameters of a statistical model by finding the parameter values that maximize the...", "l": "m", "k": ["maximum", "likelihood", "estimation", "method", "estimating", "parameters", "statistical", "model", "finding", "parameter", "values", "maximize", "function", "representing", "probability"]}, {"id": "term-maximum-variance-unfolding", "t": "Maximum Variance Unfolding", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A semidefinite programming approach to dimensionality reduction that maximizes the variance of the embedding while...", "l": "m", "k": ["maximum", "variance", "unfolding", "semidefinite", "programming", "approach", "dimensionality", "reduction", "maximizes", "embedding", "preserving", "local", "distances", "provides", "global"]}, {"id": "term-maximum-weighted-independent-set", "t": "Maximum Weighted Independent Set", "tg": ["Algorithms", "Technical", "Graph", "Optimization"], "d": "algorithms", "x": "An algorithm that finds the largest-weight subset of non-adjacent vertices in a graph. NP-hard in general but solvable...", "l": "m", "k": ["maximum", "weighted", "independent", "algorithm", "finds", "largest-weight", "subset", "non-adjacent", "vertices", "graph", "np-hard", "general", "solvable", "polynomial", "time"]}, {"id": "term-maxout", "t": "Maxout", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An activation function that computes the maximum across k linear projections of the input. Generalizes ReLU and Leaky...", "l": "m", "k": ["maxout", "activation", "function", "computes", "maximum", "across", "linear", "projections", "input", "generalizes", "relu", "leaky", "special", "cases", "proposed"]}, {"id": "term-maxq-decomposition", "t": "MAXQ Decomposition", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A hierarchical reinforcement learning method that decomposes the value function of an MDP into a hierarchy of subtask...", "l": "m", "k": ["maxq", "decomposition", "hierarchical", "reinforcement", "learning", "method", "decomposes", "value", "function", "mdp", "hierarchy", "subtask", "functions", "enables", "modular"]}, {"id": "term-maxvit", "t": "MaxViT", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A multi-axis vision Transformer that combines blocked local attention with dilated global attention for efficiently...", "l": "m", "k": ["maxvit", "multi-axis", "vision", "transformer", "combines", "blocked", "local", "attention", "dilated", "global", "efficiently", "capturing", "visual", "features"]}, {"id": "term-mbart", "t": "mBART", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A multilingual sequence-to-sequence model pre-trained by denoising full texts in 25 languages for machine translation...", "l": "m", "k": ["mbart", "multilingual", "sequence-to-sequence", "model", "pre-trained", "denoising", "full", "texts", "languages", "machine", "translation", "cross-lingual", "generation"]}, {"id": "term-mbpp", "t": "MBPP", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Mostly Basic Python Programming, a code generation benchmark consisting of approximately 1,000 entry-level Python...", "l": "m", "k": ["mbpp", "mostly", "basic", "python", "programming", "code", "generation", "benchmark", "consisting", "approximately", "entry-level", "problems", "test", "cases", "designed"]}, {"id": "term-mc4", "t": "mC4", "tg": ["Training Corpus", "NLP", "Multilingual"], "d": "datasets", "x": "Multilingual C4 a cleaned Common Crawl corpus spanning 101 languages used to train the mT5 multilingual language model....", "l": "m", "k": ["mc4", "multilingual", "cleaned", "common", "crawl", "corpus", "spanning", "languages", "train", "mt5", "language", "model", "extends", "filtering", "heuristics"]}, {"id": "term-mccarthys-advice-taker", "t": "McCarthy's Advice Taker", "tg": ["History", "Systems"], "d": "history", "x": "A proposed AI program described by John McCarthy in 1959 that would be able to accept new knowledge in the form of...", "l": "m", "k": ["mccarthy", "advice", "taker", "proposed", "program", "described", "john", "able", "accept", "knowledge", "form", "declarative", "sentences", "logical", "reasoning"]}, {"id": "term-mcculloch-pitts-neuron", "t": "McCulloch-Pitts Neuron", "tg": ["History", "Milestones"], "d": "history", "x": "The first mathematical model of a biological neuron, proposed by Warren McCulloch and Walter Pitts in 1943, showing...", "l": "m", "k": ["mcculloch-pitts", "neuron", "mathematical", "model", "biological", "proposed", "warren", "mcculloch", "walter", "pitts", "showing", "networks", "simple", "binary", "threshold"]}, {"id": "term-mcdiarmids-inequality", "t": "McDiarmid's Inequality", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A concentration inequality stating that a function of independent random variables with bounded differences is close to...", "l": "m", "k": ["mcdiarmid", "inequality", "concentration", "stating", "function", "independent", "random", "variables", "bounded", "differences", "close", "expected", "value", "high", "probability"]}, {"id": "term-md-judge", "t": "MD-Judge", "tg": ["Models", "Technical", "NLP", "Safety"], "d": "models", "x": "A safety evaluation model that judges whether language model responses are harmful or safe based on multi-dimensional...", "l": "m", "k": ["md-judge", "safety", "evaluation", "model", "judges", "language", "responses", "harmful", "safe", "based", "multi-dimensional", "criteria", "rubrics"]}, {"id": "term-mean-absolute-error", "t": "Mean Absolute Error", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A regression loss function computed as the average of the absolute differences between predicted and actual values. It...", "l": "m", "k": ["mean", "absolute", "error", "regression", "loss", "function", "computed", "average", "differences", "predicted", "actual", "values", "robust", "outliers", "squared"]}, {"id": "term-mean-average-precision", "t": "Mean Average Precision", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "The primary evaluation metric for object detection (mAP) that computes the average precision across all classes and IoU...", "l": "m", "k": ["mean", "average", "precision", "primary", "evaluation", "metric", "object", "detection", "map", "computes", "across", "classes", "iou", "thresholds", "summarizing"]}, {"id": "term-mean-field-rl", "t": "Mean Field Reinforcement Learning", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A scalable approach to multi-agent RL that approximates interactions among many agents using a mean field (average...", "l": "m", "k": ["mean", "field", "reinforcement", "learning", "scalable", "approach", "multi-agent", "approximates", "interactions", "among", "agents", "average", "effect", "neighboring", "actions"]}, {"id": "term-mean-reciprocal-rank", "t": "Mean Reciprocal Rank", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A ranking metric that averages the reciprocal of the rank position of the first relevant result across a set of...", "l": "m", "k": ["mean", "reciprocal", "rank", "ranking", "metric", "averages", "position", "relevant", "result", "across", "queries", "measuring", "quickly", "retrieval", "system"]}, {"id": "term-mean-shift", "t": "Mean Shift", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A non-parametric clustering algorithm that iteratively shifts each data point toward the mode of the local density....", "l": "m", "k": ["mean", "shift", "non-parametric", "clustering", "algorithm", "iteratively", "shifts", "data", "point", "toward", "mode", "local", "density", "require", "specifying"]}, {"id": "term-mean-shift-clustering", "t": "Mean Shift Clustering", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A non-parametric clustering algorithm that iteratively shifts each point toward the mode of its local density. The...", "l": "m", "k": ["mean", "shift", "clustering", "non-parametric", "algorithm", "iteratively", "shifts", "point", "toward", "mode", "local", "density", "number", "clusters", "determined"]}, {"id": "term-mean-shift-segmentation", "t": "Mean Shift Segmentation", "tg": ["Algorithms", "Technical", "Vision", "Clustering"], "d": "algorithms", "x": "An iterative algorithm that shifts each data point to the mode of its local density using kernel density estimation....", "l": "m", "k": ["mean", "shift", "segmentation", "iterative", "algorithm", "shifts", "data", "point", "mode", "local", "density", "kernel", "estimation", "image", "group"]}, {"id": "term-mean-squared-error", "t": "Mean Squared Error", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A regression loss function computed as the average of the squared differences between predicted and actual values. It...", "l": "m", "k": ["mean", "squared", "error", "regression", "loss", "function", "computed", "average", "differences", "predicted", "actual", "values", "penalizes", "larger", "errors"]}, {"id": "term-meaningful-human-control", "t": "Meaningful Human Control", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The requirement that humans retain sufficient understanding, authority, and ability to intervene in AI-driven...", "l": "m", "k": ["meaningful", "human", "control", "requirement", "humans", "retain", "sufficient", "understanding", "authority", "ability", "intervene", "ai-driven", "decisions", "particularly", "high-stakes"]}, {"id": "term-means-ends-analysis", "t": "Means-Ends Analysis", "tg": ["History", "Fundamentals"], "d": "history", "x": "A problem-solving technique used in AI that identifies the difference between a current state and a goal state then...", "l": "m", "k": ["means-ends", "analysis", "problem-solving", "technique", "identifies", "difference", "current", "state", "goal", "selects", "actions", "reduce", "developed", "newell", "simon"]}, {"id": "term-measurement-bias", "t": "Measurement Bias", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Bias introduced when the features or labels used in an AI system systematically differ in quality or meaning across...", "l": "m", "k": ["measurement", "bias", "introduced", "features", "labels", "system", "systematically", "differ", "quality", "meaning", "across", "groups", "arrest", "records", "proxy"]}, {"id": "term-mechanical-turk", "t": "Mechanical Turk", "tg": ["History", "Milestones"], "d": "history", "x": "An Amazon web service launched in 2005 that allows requesters to post human intelligence tasks (HITs) for workers to...", "l": "m", "k": ["mechanical", "turk", "amazon", "web", "service", "launched", "allows", "requesters", "post", "human", "intelligence", "tasks", "hits", "workers", "complete"]}, {"id": "term-mechanistic-interpretability", "t": "Mechanistic Interpretability", "tg": ["Algorithms", "Technical", "Safety"], "d": "algorithms", "x": "A research approach that aims to reverse-engineer the learned algorithms inside neural networks by identifying...", "l": "m", "k": ["mechanistic", "interpretability", "research", "approach", "aims", "reverse-engineer", "learned", "algorithms", "inside", "neural", "networks", "identifying", "interpretable", "circuits", "features"]}, {"id": "term-med-gemini", "t": "Med-Gemini", "tg": ["Models", "Technical", "NLP", "Medical", "Products"], "d": "models", "x": "A family of medical AI models from Google built on Gemini that excel at medical reasoning and multimodal clinical tasks.", "l": "m", "k": ["med-gemini", "family", "medical", "models", "google", "built", "gemini", "excel", "reasoning", "multimodal", "clinical", "tasks"]}, {"id": "term-med-palm", "t": "Med-PaLM", "tg": ["Models", "Technical"], "d": "models", "x": "A medical domain language model by Google based on PaLM with instruction tuning for medical question answering....", "l": "m", "k": ["med-palm", "medical", "domain", "language", "model", "google", "based", "palm", "instruction", "tuning", "question", "answering", "achieved", "expert-level", "performance"]}, {"id": "term-medalpaca", "t": "MedAlpaca", "tg": ["Models", "Technical", "NLP", "Medical"], "d": "models", "x": "A medical language model fine-tuned from LLaMA on curated medical question-answer datasets for clinical and biomedical...", "l": "m", "k": ["medalpaca", "medical", "language", "model", "fine-tuned", "llama", "curated", "question-answer", "datasets", "clinical", "biomedical", "question", "answering"]}, {"id": "term-medclip", "t": "MedCLIP", "tg": ["Models", "Technical", "Medical", "Vision"], "d": "models", "x": "A contrastive learning model that aligns medical images with clinical text descriptions for zero-shot and few-shot...", "l": "m", "k": ["medclip", "contrastive", "learning", "model", "aligns", "medical", "images", "clinical", "text", "descriptions", "zero-shot", "few-shot", "image", "classification"]}, {"id": "term-median-filter-algorithm", "t": "Median Filter Algorithm", "tg": ["Algorithms", "Fundamentals", "Signal Processing"], "d": "algorithms", "x": "A nonlinear digital filter that replaces each sample with the median of neighboring samples within a window. Effective...", "l": "m", "k": ["median", "filter", "algorithm", "nonlinear", "digital", "replaces", "sample", "neighboring", "samples", "within", "window", "effective", "removing", "salt-and-pepper", "noise"]}, {"id": "term-mediasum", "t": "MediaSum", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A large-scale media interview summarization dataset containing 463000 transcripts from NPR and CNN with topic...", "l": "m", "k": ["mediasum", "large-scale", "media", "interview", "summarization", "dataset", "containing", "transcripts", "npr", "cnn", "topic", "descriptions", "summaries", "tests", "spoken"]}, {"id": "term-mediatek-dimensity-9300", "t": "MediaTek Dimensity 9300", "tg": ["Mobile", "MediaTek", "SoC"], "d": "hardware", "x": "MediaTek flagship mobile processor featuring an integrated APU (AI Processing Unit) for on-device generative AI and...", "l": "m", "k": ["mediatek", "dimensity", "flagship", "mobile", "processor", "featuring", "integrated", "apu", "processing", "unit", "on-device", "generative", "large", "language", "model"]}, {"id": "term-medical-imaging-ai", "t": "Medical Imaging AI", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The application of deep learning to medical images (X-rays, CT scans, MRIs, pathology slides) for tasks like disease...", "l": "m", "k": ["medical", "imaging", "application", "deep", "learning", "images", "x-rays", "scans", "mris", "pathology", "slides", "tasks", "disease", "detection", "segmentation"]}, {"id": "term-meditron", "t": "Meditron", "tg": ["Models", "Technical", "NLP", "Medical"], "d": "models", "x": "A suite of open-source medical language models from EPFL built on Llama 2 with continued pre-training on curated...", "l": "m", "k": ["meditron", "suite", "open-source", "medical", "language", "models", "epfl", "built", "llama", "continued", "pre-training", "curated", "guidelines", "literature"]}, {"id": "term-medmcqa", "t": "MedMCQA", "tg": ["Benchmark", "NLP", "Medical"], "d": "datasets", "x": "A large-scale multiple-choice medical QA dataset containing over 194000 questions from Indian medical entrance exams....", "l": "m", "k": ["medmcqa", "large-scale", "multiple-choice", "medical", "dataset", "containing", "questions", "indian", "entrance", "exams", "covers", "healthcare", "topics", "across", "subjects"]}, {"id": "term-medqa", "t": "MedQA", "tg": ["Benchmark", "NLP", "Medical"], "d": "datasets", "x": "A medical question answering dataset derived from professional medical licensing examinations including USMLE. Tests...", "l": "m", "k": ["medqa", "medical", "question", "answering", "dataset", "derived", "professional", "licensing", "examinations", "including", "usmle", "tests", "clinical", "reasoning", "knowledge"]}, {"id": "term-medsam", "t": "MedSAM", "tg": ["Models", "Technical", "Medical", "Vision"], "d": "models", "x": "Medical Segment Anything Model applies the SAM architecture to medical image segmentation across diverse imaging...", "l": "m", "k": ["medsam", "medical", "segment", "anything", "model", "applies", "sam", "architecture", "image", "segmentation", "across", "diverse", "imaging", "modalities", "anatomical"]}, {"id": "term-medusa-decoding", "t": "Medusa Decoding", "tg": ["LLM", "Inference"], "d": "models", "x": "A parallel decoding method that adds multiple prediction heads to a language model, allowing it to propose and verify...", "l": "m", "k": ["medusa", "decoding", "parallel", "method", "adds", "multiple", "prediction", "heads", "language", "model", "allowing", "propose", "verify", "several", "future"]}, {"id": "term-megaface", "t": "MegaFace", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A face recognition benchmark containing over one million face images from 690000 individuals. Designed to evaluate face...", "l": "m", "k": ["megaface", "face", "recognition", "benchmark", "containing", "million", "images", "individuals", "designed", "evaluate", "scale", "testing", "algorithms", "against", "million-scale"]}, {"id": "term-megatron-lm", "t": "Megatron-LM", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "NVIDIA's framework for efficient large-scale language model training implementing tensor parallelism, pipeline...", "l": "m", "k": ["megatron-lm", "nvidia", "framework", "efficient", "large-scale", "language", "model", "training", "implementing", "tensor", "parallelism", "pipeline", "sequence", "optimized", "hardware"]}, {"id": "term-megnet", "t": "MEGNet", "tg": ["Models", "Scientific"], "d": "models", "x": "MatErials Graph Network uses graph neural networks to predict molecular and crystal properties by learning...", "l": "m", "k": ["megnet", "materials", "graph", "network", "uses", "neural", "networks", "predict", "molecular", "crystal", "properties", "learning", "element-level", "structure-level", "representations"]}, {"id": "term-mel-scale-algorithm", "t": "Mel Scale Algorithm", "tg": ["Algorithms", "Fundamentals", "Signal Processing"], "d": "algorithms", "x": "A perceptual scale of pitches where equal distances correspond to equal perceived pitch differences. Used to compute...", "l": "m", "k": ["mel", "scale", "algorithm", "perceptual", "pitches", "equal", "distances", "correspond", "perceived", "pitch", "differences", "compute", "mel-frequency", "cepstral", "coefficients"]}, {"id": "term-mel-spectrogram", "t": "Mel Spectrogram", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A visual representation of the frequency content of an audio signal over time using the mel scale which approximates...", "l": "m", "k": ["mel", "spectrogram", "visual", "representation", "frequency", "content", "audio", "signal", "time", "scale", "approximates", "human", "auditory", "perception", "input"]}, {"id": "term-mel-frequency-cepstral-coefficients", "t": "Mel-Frequency Cepstral Coefficients", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A compact representation of the power spectrum of an audio signal on a perceptually motivated mel frequency scale....", "l": "m", "k": ["mel-frequency", "cepstral", "coefficients", "compact", "representation", "power", "spectrum", "audio", "signal", "perceptually", "motivated", "mel", "frequency", "scale", "standard"]}, {"id": "term-membership-inference-attack", "t": "Membership Inference Attack", "tg": ["Safety", "Technical"], "d": "safety", "x": "An attack that determines whether a specific data record was used in training a machine learning model. Poses privacy...", "l": "m", "k": ["membership", "inference", "attack", "determines", "specific", "data", "record", "training", "machine", "learning", "model", "poses", "privacy", "risks", "revealing"]}, {"id": "term-memetic-algorithm", "t": "Memetic Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A hybrid optimization approach that combines a population-based global search (such as a genetic algorithm) with local...", "l": "m", "k": ["memetic", "algorithm", "hybrid", "optimization", "approach", "combines", "population-based", "global", "search", "genetic", "local", "applied", "individual", "solutions", "named"]}, {"id": "term-memory-ai", "t": "Memory (AI Systems)", "tg": ["Capability", "Architecture"], "d": "models", "x": "Mechanisms allowing AI to retain information across conversations. Includes context windows, conversation history, and...", "l": "m", "k": ["memory", "systems", "mechanisms", "allowing", "retain", "information", "across", "conversations", "includes", "context", "windows", "conversation", "history", "persistent", "features"]}, {"id": "term-memory-bandwidth", "t": "Memory Bandwidth", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "The rate at which data can be transferred between a processor and its memory, measured in GB/s or TB/s. Memory...", "l": "m", "k": ["memory", "bandwidth", "rate", "data", "transferred", "processor", "measured", "primary", "bottleneck", "large", "language", "model", "inference", "weights", "must"]}, {"id": "term-memory-controller", "t": "Memory Controller", "tg": ["Memory", "Architecture"], "d": "hardware", "x": "Hardware component that manages the flow of data between the processor and main memory. In AI accelerators optimized...", "l": "m", "k": ["memory", "controller", "hardware", "component", "manages", "flow", "data", "processor", "main", "accelerators", "optimized", "controllers", "critical", "sustaining", "high"]}, {"id": "term-memory-management-llm", "t": "Memory Management for LLM Inference", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Strategies for efficiently allocating and managing GPU memory during large language model inference, including KV cache...", "l": "m", "k": ["memory", "management", "llm", "inference", "strategies", "efficiently", "allocating", "managing", "gpu", "large", "language", "model", "including", "cache", "pooling"]}, {"id": "term-memory-network", "t": "Memory Network", "tg": ["Models", "Technical"], "d": "models", "x": "A neural architecture with an explicit external memory component that can be read and written during inference....", "l": "m", "k": ["memory", "network", "neural", "architecture", "explicit", "external", "component", "read", "written", "inference", "designed", "question", "answering", "tasks", "model"]}, {"id": "term-memory-pooling", "t": "Memory Pooling", "tg": ["Memory", "Architecture"], "d": "hardware", "x": "Technique of aggregating memory from multiple devices into a shared pool accessible by all processors. Enabled by CXL...", "l": "m", "k": ["memory", "pooling", "technique", "aggregating", "multiple", "devices", "shared", "pool", "accessible", "processors", "enabled", "cxl", "technology", "address", "capacity"]}, {"id": "term-memory-utilization-gpu", "t": "Memory Utilization (GPU)", "tg": ["Performance", "GPU", "Metric"], "d": "hardware", "x": "Percentage of GPU memory currently allocated by applications. Monitoring memory utilization is critical for optimizing...", "l": "m", "k": ["memory", "utilization", "gpu", "percentage", "currently", "allocated", "applications", "monitoring", "critical", "optimizing", "batch", "sizes", "model", "configurations", "training"]}, {"id": "term-memory-wall", "t": "Memory Wall", "tg": ["Memory", "Performance", "Fundamentals"], "d": "hardware", "x": "The growing disparity between processor speed and memory access speed that limits overall system performance. A...", "l": "m", "k": ["memory", "wall", "growing", "disparity", "processor", "speed", "access", "limits", "overall", "system", "performance", "fundamental", "challenge", "hardware", "design"]}, {"id": "term-memory-augmented-neural-network", "t": "Memory-Augmented Neural Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A broad class of neural architectures equipped with external memory modules that can be read and written using...", "l": "m", "k": ["memory-augmented", "neural", "network", "broad", "class", "architectures", "equipped", "external", "memory", "modules", "read", "written", "attention-based", "addressing", "enabling"]}, {"id": "term-memory-bound", "t": "Memory-Bound Workload", "tg": ["Hardware", "Model Optimization"], "d": "models", "x": "A processing task where performance is limited by the rate of data transfer between processor and memory rather than...", "l": "m", "k": ["memory-bound", "workload", "processing", "task", "performance", "limited", "rate", "data", "transfer", "processor", "memory", "rather", "compute", "capability", "llm"]}, {"id": "term-memristor", "t": "Memristor", "tg": ["Neuromorphic", "Memory", "Component"], "d": "hardware", "x": "Resistive memory device whose resistance depends on the history of current that has flowed through it. Promising for...", "l": "m", "k": ["memristor", "resistive", "memory", "device", "whose", "resistance", "depends", "history", "current", "flowed", "promising", "implementing", "synaptic", "weights", "neuromorphic"]}, {"id": "term-merge-sort", "t": "Merge Sort", "tg": ["Algorithms", "Fundamentals", "Sorting"], "d": "algorithms", "x": "A divide-and-conquer sorting algorithm that splits an array into halves and recursively sorts each half before merging...", "l": "m", "k": ["merge", "sort", "divide-and-conquer", "sorting", "algorithm", "splits", "array", "halves", "recursively", "sorts", "half", "merging", "sorted", "guarantees", "log"]}, {"id": "term-merkle-tree-algorithm", "t": "Merkle Tree Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A tree data structure where every leaf node contains the hash of a data block and every non-leaf node contains the hash...", "l": "m", "k": ["merkle", "tree", "algorithm", "data", "structure", "leaf", "node", "contains", "hash", "block", "non-leaf", "children", "enables", "efficient", "secure"]}, {"id": "term-mesa-optimization", "t": "Mesa-Optimization", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "A phenomenon where a learned model (the mesa-optimizer) internally develops its own optimization objective that may...", "l": "m", "k": ["mesa-optimization", "phenomenon", "learned", "model", "mesa-optimizer", "internally", "develops", "optimization", "objective", "differ", "base", "trained", "key", "concern", "advanced"]}, {"id": "term-mesh-reconstruction", "t": "Mesh Reconstruction", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The process of converting 3D point clouds, implicit functions, or depth maps into triangular mesh representations that...", "l": "m", "k": ["mesh", "reconstruction", "process", "converting", "point", "clouds", "implicit", "functions", "depth", "maps", "triangular", "representations", "define", "surface", "geometry"]}, {"id": "term-message-passing", "t": "Message Passing", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A computational framework in graph neural networks where nodes iteratively exchange and aggregate information with...", "l": "m", "k": ["message", "passing", "computational", "framework", "graph", "neural", "networks", "nodes", "iteratively", "exchange", "aggregate", "information", "neighbors", "node", "updates"]}, {"id": "term-message-passing-neural-network", "t": "Message Passing Neural Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A framework for graph neural networks where nodes iteratively update their representations by exchanging and...", "l": "m", "k": ["message", "passing", "neural", "network", "framework", "graph", "networks", "nodes", "iteratively", "update", "representations", "exchanging", "aggregating", "messages", "neighboring"]}, {"id": "term-meta-llama", "t": "Meta LLaMA", "tg": ["History", "Milestones"], "d": "history", "x": "Meta's Large Language Model Meta AI, first released in February 2023 with subsequent versions, representing a major...", "l": "m", "k": ["meta", "llama", "large", "language", "model", "released", "february", "subsequent", "versions", "representing", "major", "open-weight", "catalyzed", "open-source", "ecosystem"]}, {"id": "term-meta-mtia", "t": "Meta MTIA", "tg": ["Accelerator", "Meta", "Custom"], "d": "hardware", "x": "Meta Training and Inference Accelerator custom chip designed for recommendation and ranking model inference. Part of...", "l": "m", "k": ["meta", "mtia", "training", "inference", "accelerator", "custom", "chip", "designed", "recommendation", "ranking", "model", "part", "strategy", "develop", "silicon"]}, {"id": "term-meta-research-supercluster", "t": "Meta Research SuperCluster", "tg": ["Supercomputer", "Meta", "Training"], "d": "hardware", "x": "Meta AI research supercomputer designed for training large AI models using thousands of NVIDIA A100 GPUs. One of the...", "l": "m", "k": ["meta", "research", "supercluster", "supercomputer", "designed", "training", "large", "models", "thousands", "nvidia", "a100", "gpus", "largest", "ai-dedicated", "clusters"]}, {"id": "term-meta-learning", "t": "Meta-Learning", "tg": ["Training", "Advanced"], "d": "general", "x": "Learning how to learn: training models that can quickly adapt to new tasks with few examples. Enables better few-shot...", "l": "m", "k": ["meta-learning", "learning", "learn", "training", "models", "quickly", "adapt", "tasks", "examples", "enables", "better", "few-shot", "transfer", "capabilities"]}, {"id": "term-meta-prompting", "t": "Meta-Prompting", "tg": ["Prompt Engineering", "Meta-Learning"], "d": "general", "x": "A higher-order prompting approach where a language model is instructed to generate, critique, or improve prompts for...", "l": "m", "k": ["meta-prompting", "higher-order", "prompting", "approach", "language", "model", "instructed", "generate", "critique", "improve", "prompts", "itself", "models", "effectively", "prompt"]}, {"id": "term-meta-rl", "t": "Meta-Reinforcement Learning", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "RL approaches that learn to learn, enabling rapid adaptation to new tasks by leveraging experience across a...", "l": "m", "k": ["meta-reinforcement", "learning", "approaches", "learn", "enabling", "rapid", "adaptation", "tasks", "leveraging", "experience", "across", "distribution", "related", "meta-rl", "agents"]}, {"id": "term-metaai-voicebox", "t": "MetaAI Voicebox", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A non-autoregressive generative model from Meta AI for speech synthesis and editing that uses flow matching to generate...", "l": "m", "k": ["metaai", "voicebox", "non-autoregressive", "generative", "model", "meta", "speech", "synthesis", "editing", "uses", "flow", "matching", "generate", "multiple", "styles"]}, {"id": "term-metadata-filtering", "t": "Metadata Filtering", "tg": ["Vector Database", "Filtering"], "d": "general", "x": "A vector search technique that applies structured attribute filters alongside similarity search, restricting results to...", "l": "m", "k": ["metadata", "filtering", "vector", "search", "technique", "applies", "structured", "attribute", "filters", "alongside", "similarity", "restricting", "results", "vectors", "matching"]}, {"id": "term-metamathqa", "t": "MetaMathQA", "tg": ["Training Corpus", "NLP", "Reasoning"], "d": "datasets", "x": "A mathematical QA dataset created by rephrasing and augmenting questions from GSM8K and MATH. Used to improve...", "l": "m", "k": ["metamathqa", "mathematical", "dataset", "created", "rephrasing", "augmenting", "questions", "gsm8k", "math", "improve", "reasoning", "diverse", "problem", "formulations"]}, {"id": "term-metaworld", "t": "MetaWorld", "tg": ["Benchmark", "Reinforcement Learning", "Robotics"], "d": "datasets", "x": "A benchmark of 50 robotic manipulation tasks built on MuJoCo for multi-task and meta-reinforcement learning. Provides...", "l": "m", "k": ["metaworld", "benchmark", "robotic", "manipulation", "tasks", "built", "mujoco", "multi-task", "meta-reinforcement", "learning", "provides", "standardized", "evaluation", "generalization", "across"]}, {"id": "term-meteor", "t": "METEOR", "tg": ["NLP", "Text Processing"], "d": "general", "x": "Metric for Evaluation of Translation with Explicit ORdering, a machine translation evaluation metric that considers...", "l": "m", "k": ["meteor", "metric", "evaluation", "translation", "explicit", "ordering", "machine", "considers", "synonyms", "stemming", "word", "order", "addition", "exact", "matches"]}, {"id": "term-meteor-score", "t": "METEOR Score", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Metric for Evaluation of Translation with Explicit Ordering evaluates machine translation using unigram matching with...", "l": "m", "k": ["meteor", "score", "metric", "evaluation", "translation", "explicit", "ordering", "evaluates", "machine", "unigram", "matching", "stemming", "synonymy", "paraphrase", "support"]}, {"id": "term-method-of-lines", "t": "Method of Lines", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A technique for solving partial differential equations by discretizing all spatial dimensions while leaving the time...", "l": "m", "k": ["method", "lines", "technique", "solving", "partial", "differential", "equations", "discretizing", "spatial", "dimensions", "leaving", "time", "dimension", "continuous", "converts"]}, {"id": "term-metrics", "t": "Metrics", "tg": ["Evaluation", "Quality"], "d": "datasets", "x": "Quantitative measures used to evaluate model performance. Common metrics include accuracy, precision, recall, F1,...", "l": "m", "k": ["metrics", "quantitative", "measures", "evaluate", "model", "performance", "common", "include", "accuracy", "precision", "recall", "perplexity", "human", "evaluation", "scores"]}, {"id": "term-metropolis-hastings", "t": "Metropolis-Hastings", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "An MCMC algorithm that generates samples from a target distribution by proposing candidate points from a proposal...", "l": "m", "k": ["metropolis-hastings", "mcmc", "algorithm", "generates", "samples", "target", "distribution", "proposing", "candidate", "points", "proposal", "accepting", "rejecting", "based", "acceptance"]}, {"id": "term-mgpt-training-data", "t": "mGPT Training Data", "tg": ["Training Corpus", "NLP", "Multilingual"], "d": "datasets", "x": "The multilingual training data used for the mGPT family of language models covering 60 languages. Demonstrates scaling...", "l": "m", "k": ["mgpt", "training", "data", "multilingual", "family", "language", "models", "covering", "languages", "demonstrates", "scaling", "model", "pretraining", "across", "diverse"]}, {"id": "term-michael-jordan", "t": "Michael Jordan", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist and statistician at UC Berkeley known for foundational work in Bayesian machine learning...", "l": "m", "k": ["michael", "jordan", "american", "computer", "scientist", "statistician", "berkeley", "known", "foundational", "work", "bayesian", "machine", "learning", "variational", "inference"]}, {"id": "term-microcontroller-for-ai", "t": "Microcontroller for AI", "tg": ["Edge", "Microcontroller"], "d": "hardware", "x": "Low-power embedded processor used to run tiny machine learning models for sensor processing and basic inference....", "l": "m", "k": ["microcontroller", "low-power", "embedded", "processor", "run", "tiny", "machine", "learning", "models", "sensor", "processing", "basic", "inference", "examples", "include"]}, {"id": "term-microprocessor", "t": "Microprocessor", "tg": ["Historical", "Processor", "Fundamentals"], "d": "hardware", "x": "Complete CPU implemented on a single integrated circuit. The Intel 4004 in 1971 was the first commercial microprocessor...", "l": "m", "k": ["microprocessor", "complete", "cpu", "implemented", "single", "integrated", "circuit", "intel", "commercial", "launching", "personal", "computing", "revolution"]}, {"id": "term-microsoft-cobalt", "t": "Microsoft Cobalt", "tg": ["Processor", "Microsoft", "Cloud"], "d": "hardware", "x": "Microsoft custom ARM-based CPU designed for general-purpose Azure cloud workloads. Complements the Maia AI accelerator...", "l": "m", "k": ["microsoft", "cobalt", "custom", "arm-based", "cpu", "designed", "general-purpose", "azure", "cloud", "workloads", "complements", "maia", "accelerator", "part", "silicon"]}, {"id": "term-microsoft-maia", "t": "Microsoft Maia", "tg": ["Accelerator", "Microsoft", "Cloud"], "d": "hardware", "x": "Microsoft custom AI accelerator chip designed specifically for Azure cloud AI workloads. Developed to reduce dependence...", "l": "m", "k": ["microsoft", "maia", "custom", "accelerator", "chip", "designed", "specifically", "azure", "cloud", "workloads", "developed", "reduce", "dependence", "third-party", "gpu"]}, {"id": "term-midjourney", "t": "Midjourney", "tg": ["Product", "Image Generation"], "d": "general", "x": "A popular AI image generation service known for artistic, stylized outputs. Accessed through Discord, it's widely used...", "l": "m", "k": ["midjourney", "popular", "image", "generation", "service", "known", "artistic", "stylized", "outputs", "accessed", "discord", "widely", "creative", "commercial", "creation"]}, {"id": "term-midjourney-launch", "t": "Midjourney Launch", "tg": ["History", "Milestones"], "d": "history", "x": "The July 2022 public launch of Midjourney, an independent AI art generation service that produces images from text...", "l": "m", "k": ["midjourney", "launch", "july", "public", "independent", "art", "generation", "service", "produces", "images", "text", "prompts", "becoming", "popular", "creative"]}, {"id": "term-mila-founded", "t": "Mila Founded", "tg": ["History", "Organizations"], "d": "history", "x": "The founding of the Montreal Institute for Learning Algorithms (Mila) by Yoshua Bengio. Mila became one of the world's...", "l": "m", "k": ["mila", "founded", "founding", "montreal", "institute", "learning", "algorithms", "yoshua", "bengio", "became", "world", "largest", "academic", "research", "centers"]}, {"id": "term-milvus", "t": "Milvus", "tg": ["Vector Database", "Open Source"], "d": "general", "x": "An open-source vector database built for scalable similarity search that supports multiple index types, hybrid search,...", "l": "m", "k": ["milvus", "open-source", "vector", "database", "built", "scalable", "similarity", "search", "supports", "multiple", "index", "types", "hybrid", "multi-tenancy", "capable"]}, {"id": "term-mimic-cxr", "t": "MIMIC-CXR", "tg": ["Benchmark", "Medical", "Computer Vision"], "d": "datasets", "x": "A large dataset of 377000 chest X-ray images with associated radiology reports from Beth Israel Deaconess Medical...", "l": "m", "k": ["mimic-cxr", "large", "dataset", "chest", "x-ray", "images", "associated", "radiology", "reports", "beth", "israel", "deaconess", "medical", "center", "image"]}, {"id": "term-mimic-iii", "t": "MIMIC-III", "tg": ["Training Corpus", "Medical"], "d": "datasets", "x": "Medical Information Mart for Intensive Care a deidentified health dataset of over 40000 ICU patients at Beth Israel...", "l": "m", "k": ["mimic-iii", "medical", "information", "mart", "intensive", "care", "deidentified", "health", "dataset", "icu", "patients", "beth", "israel", "deaconess", "center"]}, {"id": "term-mimic-iv", "t": "MIMIC-IV", "tg": ["Training Corpus", "Medical"], "d": "datasets", "x": "The fourth version of the MIMIC dataset with updated clinical data from 2008 to 2019. Provides comprehensive electronic...", "l": "m", "k": ["mimic-iv", "fourth", "version", "mimic", "dataset", "updated", "clinical", "data", "provides", "comprehensive", "electronic", "health", "records", "nlp", "predictive"]}, {"id": "term-min-max-scaling", "t": "Min-Max Scaling", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A normalization technique that linearly rescales features to a fixed range, typically [0, 1], by subtracting the...", "l": "m", "k": ["min-max", "scaling", "normalization", "technique", "linearly", "rescales", "features", "fixed", "range", "typically", "subtracting", "minimum", "value", "dividing", "preserves"]}, {"id": "term-minari", "t": "Minari", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "A standard format and library for offline reinforcement learning datasets built on Gymnasium. Provides tools for...", "l": "m", "k": ["minari", "standard", "format", "library", "offline", "reinforcement", "learning", "datasets", "built", "gymnasium", "provides", "tools", "recording", "sharing", "loading"]}, {"id": "term-minedreamer", "t": "MineDreamer", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A chain-of-imagination agent for open-ended Minecraft gameplay that uses imagination-based planning to solve complex...", "l": "m", "k": ["minedreamer", "chain-of-imagination", "agent", "open-ended", "minecraft", "gameplay", "uses", "imagination-based", "planning", "solve", "complex", "multi-step", "tasks"]}, {"id": "term-minerva", "t": "Minerva", "tg": ["Models", "Technical"], "d": "models", "x": "A language model by Google fine-tuned for mathematical reasoning on a dataset of scientific papers and web pages...", "l": "m", "k": ["minerva", "language", "model", "google", "fine-tuned", "mathematical", "reasoning", "dataset", "scientific", "papers", "web", "pages", "containing", "content", "achieves"]}, {"id": "term-minhash", "t": "MinHash", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A locality-sensitive hashing technique that efficiently estimates the Jaccard similarity between sets, widely used in...", "l": "m", "k": ["minhash", "locality-sensitive", "hashing", "technique", "efficiently", "estimates", "jaccard", "similarity", "sets", "widely", "nlp", "approximate", "nearest", "neighbor", "search"]}, {"id": "term-minhash-algorithm", "t": "MinHash Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A locality-sensitive hashing technique for estimating the Jaccard similarity between sets. Produces compact signatures...", "l": "m", "k": ["minhash", "algorithm", "locality-sensitive", "hashing", "technique", "estimating", "jaccard", "similarity", "sets", "produces", "compact", "signatures", "probability", "hash", "collision"]}, {"id": "term-mini-batch-gradient-descent", "t": "Mini-Batch Gradient Descent", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A gradient-based optimization method that computes parameter updates using a small random subset (mini-batch) of the...", "l": "m", "k": ["mini-batch", "gradient", "descent", "gradient-based", "optimization", "method", "computes", "parameter", "updates", "small", "random", "subset", "training", "data", "step"]}, {"id": "term-mini-batch-k-means-algorithm", "t": "Mini-Batch K-Means Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A variant of k-means that uses random mini-batches of data instead of the full dataset to update cluster centers....", "l": "m", "k": ["mini-batch", "k-means", "algorithm", "variant", "uses", "random", "mini-batches", "data", "instead", "full", "dataset", "update", "cluster", "centers", "significantly"]}, {"id": "term-minicpm", "t": "MiniCPM", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A family of compact language models from Tsinghua University and ModelBest that achieve strong performance at small...", "l": "m", "k": ["minicpm", "family", "compact", "language", "models", "tsinghua", "university", "modelbest", "achieve", "strong", "performance", "small", "parameter", "counts", "efficient"]}, {"id": "term-minicpm-v", "t": "MiniCPM-V", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A compact vision-language model that achieves strong multimodal performance with efficient architecture design for...", "l": "m", "k": ["minicpm-v", "compact", "vision-language", "model", "achieves", "strong", "multimodal", "performance", "efficient", "architecture", "design", "deployment", "edge", "devices"]}, {"id": "term-minigrid", "t": "MiniGrid", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "A minimalistic gridworld environment for reinforcement learning research. Provides configurable grid-based tasks for...", "l": "m", "k": ["minigrid", "minimalistic", "gridworld", "environment", "reinforcement", "learning", "research", "provides", "configurable", "grid-based", "tasks", "studying", "exploration", "planning", "language-conditioned"]}, {"id": "term-minilm", "t": "MiniLM", "tg": ["Models", "Technical", "Embedding", "NLP"], "d": "models", "x": "A compact language model distilled from larger Transformers using deep self-attention distillation for efficient...", "l": "m", "k": ["minilm", "compact", "language", "model", "distilled", "larger", "transformers", "deep", "self-attention", "distillation", "efficient", "sentence", "embedding", "generation"]}, {"id": "term-minimax-algorithm", "t": "Minimax Algorithm", "tg": ["History", "Fundamentals"], "d": "history", "x": "A decision-making algorithm for two-player zero-sum games that minimizes the possible loss for a worst-case scenario....", "l": "m", "k": ["minimax", "algorithm", "decision-making", "two-player", "zero-sum", "games", "minimizes", "possible", "loss", "worst-case", "scenario", "game-playing", "1950s", "forms", "basis"]}, {"id": "term-minimum-bayes-risk-decoding", "t": "Minimum Bayes Risk Decoding", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A decoding strategy that selects the output candidate minimizing expected loss across a set of sampled hypotheses,...", "l": "m", "k": ["minimum", "bayes", "risk", "decoding", "strategy", "selects", "output", "candidate", "minimizing", "expected", "loss", "across", "sampled", "hypotheses", "producing"]}, {"id": "term-minimum-cut-algorithm", "t": "Minimum Cut Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm that finds the smallest set of edges whose removal disconnects a graph into two or more components....", "l": "m", "k": ["minimum", "cut", "algorithm", "finds", "smallest", "edges", "whose", "removal", "disconnects", "graph", "components", "stoer-wagner", "well-known", "deterministic", "finding"]}, {"id": "term-minimum-description-length", "t": "Minimum Description Length", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A model selection principle that selects the model minimizing the total description length of the data and the model...", "l": "m", "k": ["minimum", "description", "length", "model", "selection", "principle", "selects", "minimizing", "total", "data", "itself", "formalizes", "occam", "razor", "information-theoretic"]}, {"id": "term-minimum-edit-distance-with-backtrace", "t": "Minimum Edit Distance with Backtrace", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "An extension of the edit distance algorithm that records the operations performed at each step. Enables reconstruction...", "l": "m", "k": ["minimum", "edit", "distance", "backtrace", "extension", "algorithm", "records", "operations", "performed", "step", "enables", "reconstruction", "optimal", "alignment", "strings"]}, {"id": "term-minimum-spanning-tree", "t": "Minimum Spanning Tree", "tg": ["Algorithms", "Fundamentals", "Graph"], "d": "algorithms", "x": "A subset of edges in a connected weighted undirected graph that connects all vertices with the minimum total edge...", "l": "m", "k": ["minimum", "spanning", "tree", "subset", "edges", "connected", "weighted", "undirected", "graph", "connects", "vertices", "total", "edge", "weight", "contains"]}, {"id": "term-minitron", "t": "Minitron", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A family of compressed language models from NVIDIA created through pruning and distillation of larger Nemotron models...", "l": "m", "k": ["minitron", "family", "compressed", "language", "models", "nvidia", "created", "pruning", "distillation", "larger", "nemotron", "efficient", "deployment"]}, {"id": "term-minkowski-distance", "t": "Minkowski Distance", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A generalized distance metric parameterized by p that includes Manhattan (p=1), Euclidean (p=2), and Chebyshev...", "l": "m", "k": ["minkowski", "distance", "generalized", "metric", "parameterized", "includes", "manhattan", "euclidean", "chebyshev", "infinity", "distances", "special", "cases", "computes", "p-th"]}, {"id": "term-minsky-papert-perceptrons", "t": "Minsky and Papert Perceptrons", "tg": ["History", "Milestones"], "d": "history", "x": "The 1969 book by Marvin Minsky and Seymour Papert that mathematically demonstrated the limitations of single-layer...", "l": "m", "k": ["minsky", "papert", "perceptrons", "book", "marvin", "seymour", "mathematically", "demonstrated", "limitations", "single-layer", "contributing", "reduced", "funding", "neural", "network"]}, {"id": "term-mips-architecture", "t": "MIPS Architecture", "tg": ["Architecture", "RISC", "Historical"], "d": "hardware", "x": "Reduced instruction set computer architecture developed by John Hennessy at Stanford. Influential in computer...", "l": "m", "k": ["mips", "architecture", "reduced", "instruction", "computer", "developed", "john", "hennessy", "stanford", "influential", "education", "embedded", "systems", "early", "sgi"]}, {"id": "term-mirror-descent", "t": "Mirror Descent", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A generalization of gradient descent that uses Bregman divergences instead of Euclidean distance for parameter updates....", "l": "m", "k": ["mirror", "descent", "generalization", "gradient", "uses", "bregman", "divergences", "instead", "euclidean", "distance", "parameter", "updates", "naturally", "handles", "constrained"]}, {"id": "term-mirror-prompting", "t": "Mirror Prompting", "tg": ["Prompt Engineering", "Clarification"], "d": "general", "x": "A prompting approach that instructs the model to first restate the user's request back in its own words, confirming...", "l": "m", "k": ["mirror", "prompting", "approach", "instructs", "model", "restate", "user", "request", "words", "confirming", "mutual", "understanding", "proceeding", "task", "execution"]}, {"id": "term-mish", "t": "Mish", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A self-regularizing non-monotonic activation function defined as f(x) = x * tanh(softplus(x)). Known for smooth...", "l": "m", "k": ["mish", "self-regularizing", "non-monotonic", "activation", "function", "defined", "tanh", "softplus", "known", "smooth", "gradients", "strong", "empirical", "performance", "particularly"]}, {"id": "term-misinformation", "t": "Misinformation", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "False or inaccurate information shared without deliberate intent to deceive, which can be amplified by AI...", "l": "m", "k": ["misinformation", "false", "inaccurate", "information", "shared", "without", "deliberate", "intent", "deceive", "amplified", "recommendation", "systems", "generated", "inadvertently", "hallucinations"]}, {"id": "term-misinformation-detection", "t": "Misinformation Detection", "tg": ["Safety", "Technical"], "d": "safety", "x": "AI techniques for identifying false or misleading information in text images and video. Combines natural language...", "l": "m", "k": ["misinformation", "detection", "techniques", "identifying", "false", "misleading", "information", "text", "images", "video", "combines", "natural", "language", "processing", "knowledge"]}, {"id": "term-misra-gries-algorithm", "t": "Misra-Gries Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A streaming algorithm for finding frequent items (heavy hitters) in a data stream using limited memory. Maintains a set...", "l": "m", "k": ["misra-gries", "algorithm", "streaming", "finding", "frequent", "items", "heavy", "hitters", "data", "stream", "limited", "memory", "maintains", "candidate", "counts"]}, {"id": "term-mistral", "t": "Mistral", "tg": ["Company", "Model"], "d": "models", "x": "A French AI company known for efficient, high-performance open models. Their Mistral and Mixtral models offer strong...", "l": "m", "k": ["mistral", "french", "company", "known", "efficient", "high-performance", "open", "models", "mixtral", "offer", "strong", "capabilities", "smaller", "parameter", "counts"]}, {"id": "term-mistral-7b", "t": "Mistral 7B", "tg": ["Models", "Technical"], "d": "models", "x": "A 7 billion parameter language model by Mistral AI that outperforms larger models through architectural innovations...", "l": "m", "k": ["mistral", "billion", "parameter", "language", "model", "outperforms", "larger", "models", "architectural", "innovations", "including", "grouped", "query", "attention", "sliding"]}, {"id": "term-mistral-ai", "t": "Mistral AI", "tg": ["History", "Organizations"], "d": "history", "x": "A French AI company founded in 2023 by former Google DeepMind and Meta researchers including Arthur Mensch. Mistral AI...", "l": "m", "k": ["mistral", "french", "company", "founded", "former", "google", "deepmind", "meta", "researchers", "including", "arthur", "mensch", "developed", "efficient", "open-source"]}, {"id": "term-mistral-ai-founding", "t": "Mistral AI Founding", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of Mistral AI in April 2023 by former Google DeepMind and Meta researchers in Paris, which rapidly became...", "l": "m", "k": ["mistral", "founding", "april", "former", "google", "deepmind", "meta", "researchers", "paris", "rapidly", "became", "leading", "european", "company", "releasing"]}, {"id": "term-mistral-large", "t": "Mistral Large", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A flagship large language model from Mistral AI designed for complex reasoning and multilingual tasks with strong...", "l": "m", "k": ["mistral", "large", "flagship", "language", "model", "designed", "complex", "reasoning", "multilingual", "tasks", "strong", "performance", "benchmarks"]}, {"id": "term-mistral-medium", "t": "Mistral Medium", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A mid-sized language model from Mistral AI positioned between Mistral Small and Mistral Large for balanced performance...", "l": "m", "k": ["mistral", "medium", "mid-sized", "language", "model", "positioned", "small", "large", "balanced", "performance", "cost", "efficiency"]}, {"id": "term-mistral-nemo", "t": "Mistral Nemo", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A 12B parameter language model developed jointly by Mistral AI and NVIDIA offering strong multilingual and code...", "l": "m", "k": ["mistral", "nemo", "12b", "parameter", "language", "model", "developed", "jointly", "nvidia", "offering", "strong", "multilingual", "code", "generation", "performance"]}, {"id": "term-mistral-small", "t": "Mistral Small", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A compact and efficient language model from Mistral AI optimized for low-latency applications while maintaining strong...", "l": "m", "k": ["mistral", "small", "compact", "efficient", "language", "model", "optimized", "low-latency", "applications", "maintaining", "strong", "reasoning", "capabilities"]}, {"id": "term-mistral-instruct", "t": "Mistral-Instruct", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "Instruction-tuned variants of Mistral base models that are fine-tuned for conversational and instruction-following...", "l": "m", "k": ["mistral-instruct", "instruction-tuned", "variants", "mistral", "base", "models", "fine-tuned", "conversational", "instruction-following", "tasks", "improved", "helpfulness"]}, {"id": "term-mit-ai-laboratory", "t": "MIT AI Laboratory", "tg": ["History", "Milestones"], "d": "history", "x": "A research laboratory co-founded by Marvin Minsky and John McCarthy at MIT in 1959, which became one of the most...", "l": "m", "k": ["mit", "laboratory", "research", "co-founded", "marvin", "minsky", "john", "mccarthy", "became", "influential", "centers", "producing", "foundational", "work", "vision"]}, {"id": "term-mixed-precision-training", "t": "Mixed Precision Training", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "A training technique that uses lower-precision floating-point formats (FP16 or BF16) for forward and backward passes...", "l": "m", "k": ["mixed", "precision", "training", "technique", "uses", "lower-precision", "floating-point", "formats", "fp16", "bf16", "forward", "backward", "passes", "maintaining", "fp32"]}, {"id": "term-mixmatch", "t": "MixMatch", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A semi-supervised learning method that combines consistency regularization entropy minimization and MixUp augmentation....", "l": "m", "k": ["mixmatch", "semi-supervised", "learning", "method", "combines", "consistency", "regularization", "entropy", "minimization", "mixup", "augmentation", "produces", "sharpened", "pseudo-labels", "unlabeled"]}, {"id": "term-mixtral", "t": "Mixtral", "tg": ["Models", "Technical"], "d": "models", "x": "A mixture-of-experts model by Mistral AI that uses sparse MoE layers with 8 experts per layer routing each token to 2...", "l": "m", "k": ["mixtral", "mixture-of-experts", "model", "mistral", "uses", "sparse", "moe", "layers", "experts", "per", "layer", "routing", "token", "achieves", "performance"]}, {"id": "term-mixtral-8x22b", "t": "Mixtral 8x22B", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A large mixture-of-experts model from Mistral AI with 8 experts and 22 billion parameters per expert that achieves...", "l": "m", "k": ["mixtral", "8x22b", "large", "mixture-of-experts", "model", "mistral", "experts", "billion", "parameters", "per", "expert", "achieves", "strong", "performance", "reasoning"]}, {"id": "term-mixtral-8x7b", "t": "Mixtral 8x7B", "tg": ["Models", "Technical"], "d": "models", "x": "A specific configuration of the Mixtral mixture-of-experts model with 8 experts of 7 billion parameters each. Routes...", "l": "m", "k": ["mixtral", "8x7b", "specific", "configuration", "mixture-of-experts", "model", "experts", "billion", "parameters", "routes", "token", "resulting", "computational", "cost", "similar"]}, {"id": "term-mixtral-instruct", "t": "Mixtral-Instruct", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An instruction-tuned version of the Mixtral mixture-of-experts model optimized for following detailed instructions and...", "l": "m", "k": ["mixtral-instruct", "instruction-tuned", "version", "mixtral", "mixture-of-experts", "model", "optimized", "following", "detailed", "instructions", "multi-turn", "conversations"]}, {"id": "term-mixture-of-agents", "t": "Mixture of Agents", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An architecture where multiple specialized LLM agents collaborate on a task, with each agent contributing expertise in...", "l": "m", "k": ["mixture", "agents", "architecture", "multiple", "specialized", "llm", "collaborate", "task", "agent", "contributing", "expertise", "specific", "domain", "router", "aggregator"]}, {"id": "term-mixture-of-depths", "t": "Mixture of Depths", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer variant that learns to dynamically allocate computation by routing only a subset of tokens through each...", "l": "m", "k": ["mixture", "depths", "transformer", "variant", "learns", "dynamically", "allocate", "computation", "routing", "subset", "tokens", "block", "reducing", "total", "maintaining"]}, {"id": "term-moe-inference", "t": "Mixture of Experts Inference", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Inference optimization for Mixture of Experts models where only a subset of expert parameters are activated per token,...", "l": "m", "k": ["mixture", "experts", "inference", "optimization", "models", "subset", "expert", "parameters", "activated", "per", "token", "reducing", "computation", "despite", "large"]}, {"id": "term-mixture-of-experts-layer", "t": "Mixture of Experts Layer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A neural network layer consisting of multiple expert subnetworks and a gating mechanism that routes each input to a...", "l": "m", "k": ["mixture", "experts", "layer", "neural", "network", "consisting", "multiple", "expert", "subnetworks", "gating", "mechanism", "routes", "input", "sparse", "subset"]}, {"id": "term-moe-routing", "t": "Mixture of Experts Routing", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The gating mechanism in MoE models that determines which expert subnetworks process each input, using learned routing...", "l": "m", "k": ["mixture", "experts", "routing", "gating", "mechanism", "moe", "models", "determines", "expert", "subnetworks", "process", "input", "learned", "functions", "achieve"]}, {"id": "term-mixup", "t": "Mixup", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A data augmentation and regularization technique that creates virtual training examples by taking convex combinations...", "l": "m", "k": ["mixup", "data", "augmentation", "regularization", "technique", "creates", "virtual", "training", "examples", "taking", "convex", "combinations", "pairs", "labels", "encouraging"]}, {"id": "term-ml-superb", "t": "ML-SUPERB", "tg": ["Benchmark", "Speech", "Multilingual", "Evaluation"], "d": "datasets", "x": "Multilingual SUPERB extending the speech evaluation benchmark to cover tasks across multiple languages. Tests...", "l": "m", "k": ["ml-superb", "multilingual", "superb", "extending", "speech", "evaluation", "benchmark", "cover", "tasks", "across", "multiple", "languages", "tests", "cross-lingual", "generalization"]}, {"id": "term-mlir", "t": "MLIR", "tg": ["Compiler", "Google", "Infrastructure"], "d": "hardware", "x": "Multi-Level Intermediate Representation compiler infrastructure developed at Google for building reusable and...", "l": "m", "k": ["mlir", "multi-level", "intermediate", "representation", "compiler", "infrastructure", "developed", "google", "building", "reusable", "extensible", "frameworks", "foundation", "several", "projects"]}, {"id": "term-mlops", "t": "MLOps", "tg": ["Operations", "Production"], "d": "general", "x": "Practices for deploying and maintaining ML models in production. Combines ML, DevOps, and data engineering to ensure...", "l": "m", "k": ["mlops", "practices", "deploying", "maintaining", "models", "production", "combines", "devops", "data", "engineering", "ensure", "reliable", "scalable", "systems"]}, {"id": "term-mlperf-inference", "t": "MLPerf Inference", "tg": ["Benchmark", "Performance", "Standard"], "d": "hardware", "x": "Industry benchmark suite measuring AI inference performance for latency and throughput across hardware platforms....", "l": "m", "k": ["mlperf", "inference", "industry", "benchmark", "suite", "measuring", "performance", "latency", "throughput", "across", "hardware", "platforms", "widely", "compare", "accelerator"]}, {"id": "term-mlperf-training", "t": "MLPerf Training", "tg": ["Benchmark", "Performance", "Standard"], "d": "hardware", "x": "Industry benchmark suite measuring AI training performance across multiple workloads and hardware platforms. Provides...", "l": "m", "k": ["mlperf", "training", "industry", "benchmark", "suite", "measuring", "performance", "across", "multiple", "workloads", "hardware", "platforms", "provides", "standardized", "comparisons"]}, {"id": "term-mlqa", "t": "MLQA", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "Multilingual Question Answering a cross-lingual extractive QA benchmark with over 12000 instances in 7 languages. Tests...", "l": "m", "k": ["mlqa", "multilingual", "question", "answering", "cross-lingual", "extractive", "benchmark", "instances", "languages", "tests", "ability", "find", "answers", "passages", "written"]}, {"id": "term-mls", "t": "MLS", "tg": ["Benchmark", "Speech", "Multilingual"], "d": "datasets", "x": "Multilingual LibriSpeech a large multilingual corpus derived from LibriVox audiobooks in 8 languages totaling 50000...", "l": "m", "k": ["mls", "multilingual", "librispeech", "large", "corpus", "derived", "librivox", "audiobooks", "languages", "totaling", "hours", "train", "evaluate", "speech", "recognition"]}, {"id": "term-mls-benchmark", "t": "MLS Benchmark", "tg": ["Benchmark", "Speech", "Multilingual", "Evaluation"], "d": "datasets", "x": "The Multilingual LibriSpeech benchmark evaluating speech recognition across 8 languages with standardized test sets....", "l": "m", "k": ["mls", "benchmark", "multilingual", "librispeech", "evaluating", "speech", "recognition", "across", "languages", "standardized", "test", "sets", "provides", "consistent", "cross-lingual"]}, {"id": "term-mm-vet", "t": "MM-Vet", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A multimodal benchmark evaluating integrated capabilities of vision-language models across recognition knowledge...", "l": "m", "k": ["mm-vet", "multimodal", "benchmark", "evaluating", "integrated", "capabilities", "vision-language", "models", "across", "recognition", "knowledge", "generation", "spatial", "awareness", "math"]}, {"id": "term-mm1", "t": "MM1", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A family of multimodal models from Apple that systematically study the importance of architecture and data choices for...", "l": "m", "k": ["mm1", "family", "multimodal", "models", "apple", "systematically", "study", "importance", "architecture", "data", "choices", "building", "effective", "vision-language"]}, {"id": "term-mmbench", "t": "MMBench", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A bilingual multimodal benchmark systematically evaluating vision-language models across 20 ability dimensions with...", "l": "m", "k": ["mmbench", "bilingual", "multimodal", "benchmark", "systematically", "evaluating", "vision-language", "models", "across", "ability", "dimensions", "single-choice", "questions", "english", "chinese"]}, {"id": "term-mmc4", "t": "MMC4", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "Multimodal C4 a dataset of interleaved image-text documents derived from C4 web text. Provides naturally occurring...", "l": "m", "k": ["mmc4", "multimodal", "dataset", "interleaved", "image-text", "documents", "derived", "web", "text", "provides", "naturally", "occurring", "content", "pretraining", "vision-language"]}, {"id": "term-mme", "t": "MME", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A comprehensive multimodal evaluation benchmark testing perception and cognition abilities of large multimodal models...", "l": "m", "k": ["mme", "comprehensive", "multimodal", "evaluation", "benchmark", "testing", "perception", "cognition", "abilities", "large", "models", "across", "subtasks", "yes", "questions"]}, {"id": "term-mmlu", "t": "MMLU (Massive Multitask Language Understanding)", "tg": ["Benchmark", "Evaluation"], "d": "datasets", "x": "A comprehensive benchmark testing language models on 57 subjects from STEM to humanities. Widely used to compare model...", "l": "m", "k": ["mmlu", "massive", "multitask", "language", "understanding", "comprehensive", "benchmark", "testing", "models", "subjects", "stem", "humanities", "widely", "compare", "model"]}, {"id": "term-mmlu-pro", "t": "MMLU-Pro", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "An enhanced version of MMLU with more challenging questions expert-level reasoning requirements and reduced dataset...", "l": "m", "k": ["mmlu-pro", "enhanced", "version", "mmlu", "challenging", "questions", "expert-level", "reasoning", "requirements", "reduced", "dataset", "noise", "designed", "better", "differentiate"]}, {"id": "term-mmmu", "t": "MMMU", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "Massive Multi-discipline Multimodal Understanding a benchmark of 11500 multimodal questions from college exams across...", "l": "m", "k": ["mmmu", "massive", "multi-discipline", "multimodal", "understanding", "benchmark", "questions", "college", "exams", "across", "subjects", "tests", "expert-level", "reasoning"]}, {"id": "term-mms", "t": "MMS", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "Massively Multilingual Speech is a project from Meta AI providing speech recognition and synthesis and language...", "l": "m", "k": ["mms", "massively", "multilingual", "speech", "project", "meta", "providing", "recognition", "synthesis", "language", "identification", "models", "languages"]}, {"id": "term-mmvet-v2", "t": "MMVET v2", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "An updated multimodal evaluation benchmark testing integrated vision-language capabilities across recognition OCR...", "l": "m", "k": ["mmvet", "updated", "multimodal", "evaluation", "benchmark", "testing", "integrated", "vision-language", "capabilities", "across", "recognition", "ocr", "knowledge", "spatial", "reasoning"]}, {"id": "term-mnist", "t": "MNIST", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A dataset of 70000 handwritten digit images (28x28 grayscale) created by Yann LeCun and colleagues. One of the most...", "l": "m", "k": ["mnist", "dataset", "handwritten", "digit", "images", "28x28", "grayscale", "created", "yann", "lecun", "colleagues", "widely", "benchmarks", "machine", "learning"]}, {"id": "term-mnist-dataset", "t": "MNIST Dataset", "tg": ["History", "Milestones"], "d": "history", "x": "The Modified National Institute of Standards and Technology database of handwritten digits created by Yann LeCun and...", "l": "m", "k": ["mnist", "dataset", "modified", "national", "institute", "standards", "technology", "database", "handwritten", "digits", "created", "yann", "lecun", "colleagues", "became"]}, {"id": "term-mnist-1d", "t": "MNIST-1D", "tg": ["Benchmark", "Synthetic"], "d": "datasets", "x": "A one-dimensional synthetic analog of MNIST designed for efficient experimentation with deep learning concepts. Enables...", "l": "m", "k": ["mnist-1d", "one-dimensional", "synthetic", "analog", "mnist", "designed", "efficient", "experimentation", "deep", "learning", "concepts", "enables", "rapid", "prototyping", "ideas"]}, {"id": "term-mnli", "t": "MNLI", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "The Multi-Genre Natural Language Inference corpus containing 433000 sentence pairs annotated with textual entailment...", "l": "m", "k": ["mnli", "multi-genre", "natural", "language", "inference", "corpus", "containing", "sentence", "pairs", "annotated", "textual", "entailment", "labels", "across", "ten"]}, {"id": "term-mobile-aloha", "t": "Mobile ALOHA", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "An extension of ALOHA that adds a mobile base to enable whole-body bimanual manipulation allowing robots to perform...", "l": "m", "k": ["mobile", "aloha", "extension", "adds", "base", "enable", "whole-body", "bimanual", "manipulation", "allowing", "robots", "perform", "household", "tasks", "requiring"]}, {"id": "term-mobile-inference", "t": "Mobile Inference", "tg": ["Inference Infrastructure", "Hardware"], "d": "hardware", "x": "AI inference optimized for smartphones and tablets, leveraging mobile GPU, NPU, or DSP capabilities. Mobile inference...", "l": "m", "k": ["mobile", "inference", "optimized", "smartphones", "tablets", "leveraging", "gpu", "npu", "dsp", "capabilities", "frameworks", "tensorflow", "lite", "core", "apply"]}, {"id": "term-mobilenet", "t": "MobileNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A family of lightweight CNN architectures designed for mobile and embedded devices that use depthwise separable...", "l": "m", "k": ["mobilenet", "family", "lightweight", "cnn", "architectures", "designed", "mobile", "embedded", "devices", "depthwise", "separable", "convolutions", "dramatically", "reduce", "computation"]}, {"id": "term-mobilenetv2", "t": "MobileNetV2", "tg": ["Models", "Technical"], "d": "models", "x": "An improved mobile architecture that introduces inverted residual blocks with linear bottlenecks. The inverted residual...", "l": "m", "k": ["mobilenetv2", "improved", "mobile", "architecture", "introduces", "inverted", "residual", "blocks", "linear", "bottlenecks", "expands", "channels", "depthwise", "convolution", "projects"]}, {"id": "term-mobilenetv3", "t": "MobileNetV3", "tg": ["Models", "Technical"], "d": "models", "x": "The third generation mobile architecture combining hardware-aware neural architecture search with NetAdapt. Uses...", "l": "m", "k": ["mobilenetv3", "generation", "mobile", "architecture", "combining", "hardware-aware", "neural", "search", "netadapt", "uses", "squeeze-and-excitation", "modules", "h-swish", "activation", "improved"]}, {"id": "term-mobilevit", "t": "MobileViT", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A lightweight vision Transformer from Apple that combines mobile-friendly convolutions with self-attention for...", "l": "m", "k": ["mobilevit", "lightweight", "vision", "transformer", "apple", "combines", "mobile-friendly", "convolutions", "self-attention", "efficient", "on-device", "image", "understanding"]}, {"id": "term-mobilevitv2", "t": "MobileViTv2", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An improved mobile vision Transformer that uses separable self-attention to achieve linear complexity while maintaining...", "l": "m", "k": ["mobilevitv2", "improved", "mobile", "vision", "transformer", "uses", "separable", "self-attention", "achieve", "linear", "complexity", "maintaining", "competitive", "accuracy", "deployment"]}, {"id": "term-model", "t": "Model", "tg": ["Core Concept", "Fundamentals"], "d": "general", "x": "The trained AI system that processes inputs and generates outputs. Models are defined by their architecture, size...", "l": "m", "k": ["model", "trained", "system", "processes", "inputs", "generates", "outputs", "models", "defined", "architecture", "size", "parameters", "training", "data", "fine-tuning"]}, {"id": "term-model-auditing", "t": "Model Auditing", "tg": ["Safety", "Governance"], "d": "safety", "x": "The systematic examination of an AI model's behavior performance and fairness by an independent party. May include...", "l": "m", "k": ["model", "auditing", "systematic", "examination", "behavior", "performance", "fairness", "independent", "party", "include", "testing", "bias", "reviewing", "training", "procedures"]}, {"id": "term-model-averaging", "t": "Model Averaging", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An ensemble technique that combines predictions from multiple models by averaging their outputs. Can use uniform...", "l": "m", "k": ["model", "averaging", "ensemble", "technique", "combines", "predictions", "multiple", "models", "outputs", "uniform", "weights", "learned", "simple", "effective", "reducing"]}, {"id": "term-model-card", "t": "Model Card", "tg": ["Documentation", "Ethics"], "d": "safety", "x": "Documentation describing a model's intended use, limitations, performance metrics, and ethical considerations. A...", "l": "m", "k": ["model", "card", "documentation", "describing", "intended", "limitations", "performance", "metrics", "ethical", "considerations", "standard", "practice", "responsible", "development", "deployment"]}, {"id": "term-model-cards", "t": "Model Cards", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "Standardized documentation artifacts proposed by Mitchell et al. (2019) that accompany trained ML models and report on...", "l": "m", "k": ["model", "cards", "standardized", "documentation", "artifacts", "proposed", "mitchell", "accompany", "trained", "models", "report", "intended", "performance", "characteristics", "limitations"]}, {"id": "term-model-collapse", "t": "Model Collapse", "tg": ["Risk", "Training"], "d": "safety", "x": "A degradation phenomenon where models trained on AI-generated data lose diversity and quality over generations. A...", "l": "m", "k": ["model", "collapse", "degradation", "phenomenon", "models", "trained", "ai-generated", "data", "lose", "diversity", "quality", "generations", "growing", "concern", "synthetic"]}, {"id": "term-model-compilation", "t": "Model Compilation", "tg": ["Inference", "Optimization", "Compiler"], "d": "hardware", "x": "Process of converting a trained model into optimized machine code for specific hardware. Includes operator fusion...", "l": "m", "k": ["model", "compilation", "process", "converting", "trained", "optimized", "machine", "code", "specific", "hardware", "includes", "operator", "fusion", "memory", "planning"]}, {"id": "term-model-compression", "t": "Model Compression", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A family of techniques for reducing model size and computational cost while preserving performance, including...", "l": "m", "k": ["model", "compression", "family", "techniques", "reducing", "size", "computational", "cost", "preserving", "performance", "including", "quantization", "pruning", "distillation", "low-rank"]}, {"id": "term-model-distillation", "t": "Model Distillation", "tg": ["LLM", "Inference"], "d": "models", "x": "The process of training a smaller student model to replicate the behavior of a larger teacher model by learning from...", "l": "m", "k": ["model", "distillation", "process", "training", "smaller", "student", "replicate", "behavior", "larger", "teacher", "learning", "output", "probability", "distributions", "rather"]}, {"id": "term-model-distillation-history", "t": "Model Distillation History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of knowledge distillation from the original concept by Hinton Vinyals and Dean (2015) where a smaller...", "l": "m", "k": ["model", "distillation", "history", "development", "knowledge", "original", "concept", "hinton", "vinyals", "dean", "smaller", "student", "learns", "mimic", "larger"]}, {"id": "term-model-extraction-attack", "t": "Model Extraction Attack", "tg": ["Safety", "Technical"], "d": "safety", "x": "An attack that creates a functionally equivalent copy of a target model by querying it and training a substitute on the...", "l": "m", "k": ["model", "extraction", "attack", "creates", "functionally", "equivalent", "copy", "target", "querying", "training", "substitute", "responses", "threatens", "intellectual", "property"]}, {"id": "term-mfu", "t": "Model FLOPs Utilization (MFU)", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "The ratio of observed model FLOPS to the theoretical peak FLOPS of the hardware, measuring how efficiently the training...", "l": "m", "k": ["model", "flops", "utilization", "mfu", "ratio", "observed", "theoretical", "peak", "hardware", "measuring", "efficiently", "training", "system", "utilizes", "available"]}, {"id": "term-model-governance", "t": "Model Governance", "tg": ["Safety", "Governance"], "d": "safety", "x": "Organizational policies and processes for managing the lifecycle of machine learning models from development through...", "l": "m", "k": ["model", "governance", "organizational", "policies", "processes", "managing", "lifecycle", "machine", "learning", "models", "development", "deployment", "monitoring", "retirement", "ensures"]}, {"id": "term-model-interpretability", "t": "Model Interpretability", "tg": ["Safety", "Technical"], "d": "safety", "x": "The degree to which a human can understand the cause of a model's decisions. Distinguished from explainability which...", "l": "m", "k": ["model", "interpretability", "degree", "human", "understand", "cause", "decisions", "distinguished", "explainability", "focuses", "providing", "post-hoc", "explanations", "opaque", "behavior"]}, {"id": "term-model-merging", "t": "Model Merging", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A technique that combines the weights of two or more fine-tuned models into a single model, often using methods like...", "l": "m", "k": ["model", "merging", "technique", "combines", "weights", "fine-tuned", "models", "single", "methods", "linear", "interpolation", "slerp", "ties", "inherit", "capabilities"]}, {"id": "term-model-parallelism", "t": "Model Parallelism", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A distributed training strategy that splits a model's layers or parameters across multiple devices, enabling training...", "l": "m", "k": ["model", "parallelism", "distributed", "training", "strategy", "splits", "layers", "parameters", "across", "multiple", "devices", "enabling", "models", "large", "fit"]}, {"id": "term-model-predictive-control", "t": "Model Predictive Control", "tg": ["Algorithms", "Technical", "RL", "Optimization"], "d": "algorithms", "x": "A control algorithm that uses a model of the system to predict future states and optimizes a sequence of control...", "l": "m", "k": ["model", "predictive", "control", "algorithm", "uses", "system", "predict", "future", "states", "optimizes", "sequence", "actions", "finite", "horizon", "re-plans"]}, {"id": "term-model-predictive-control-rl", "t": "Model Predictive Control in RL", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A planning-based approach that uses a learned dynamics model to simulate action sequences forward and selects the first...", "l": "m", "k": ["model", "predictive", "control", "planning-based", "approach", "uses", "learned", "dynamics", "simulate", "action", "sequences", "forward", "selects", "best", "sequence"]}, {"id": "term-model-pruning", "t": "Model Pruning", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A compression technique that removes redundant weights or neurons from a neural network based on magnitude,...", "l": "m", "k": ["model", "pruning", "compression", "technique", "removes", "redundant", "weights", "neurons", "neural", "network", "based", "magnitude", "sensitivity", "criteria", "reduces"]}, {"id": "term-model-risk-management", "t": "Model Risk Management", "tg": ["Safety", "Governance"], "d": "safety", "x": "A framework for identifying assessing mitigating and monitoring risks associated with machine learning models in...", "l": "m", "k": ["model", "risk", "management", "framework", "identifying", "assessing", "mitigating", "monitoring", "risks", "associated", "machine", "learning", "models", "production", "extends"]}, {"id": "term-model-serving", "t": "Model Serving", "tg": ["Inference Infrastructure", "Distributed Computing"], "d": "hardware", "x": "The infrastructure and systems for deploying trained models to handle real-time prediction requests at scale. Model...", "l": "m", "k": ["model", "serving", "infrastructure", "systems", "deploying", "trained", "models", "handle", "real-time", "prediction", "requests", "scale", "encompasses", "load", "balancing"]}, {"id": "term-model-serving-infrastructure", "t": "Model Serving Infrastructure", "tg": ["Inference", "Infrastructure", "Deployment"], "d": "hardware", "x": "Systems and software for deploying trained AI models to handle production inference requests. Includes load balancing...", "l": "m", "k": ["model", "serving", "infrastructure", "systems", "software", "deploying", "trained", "models", "handle", "production", "inference", "requests", "includes", "load", "balancing"]}, {"id": "term-model-sharding", "t": "Model Sharding", "tg": ["LLM", "Inference"], "d": "models", "x": "The technique of partitioning a large model's parameters across multiple devices or storage locations, enabling...", "l": "m", "k": ["model", "sharding", "technique", "partitioning", "large", "parameters", "across", "multiple", "devices", "storage", "locations", "enabling", "inference", "training", "models"]}, {"id": "term-model-specification", "t": "Model Specification", "tg": ["Safety", "Governance"], "d": "safety", "x": "A detailed document describing the intended behavior constraints and safety requirements for an AI model. Includes...", "l": "m", "k": ["model", "specification", "detailed", "document", "describing", "intended", "behavior", "constraints", "safety", "requirements", "includes", "behavioral", "guidelines", "content", "policies"]}, {"id": "term-model-stealing", "t": "Model Stealing", "tg": ["Safety", "Technical"], "d": "safety", "x": "The unauthorized replication of a machine learning model's functionality through systematic querying. Also known as...", "l": "m", "k": ["model", "stealing", "unauthorized", "replication", "machine", "learning", "functionality", "systematic", "querying", "known", "extraction", "threatens", "intellectual", "property", "bypass"]}, {"id": "term-model-based-rl", "t": "Model-Based RL", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "RL approaches that learn or use a model of the environment's transition dynamics and reward function to plan actions or...", "l": "m", "k": ["model-based", "approaches", "learn", "model", "environment", "transition", "dynamics", "reward", "function", "plan", "actions", "generate", "synthetic", "experience", "methods"]}, {"id": "term-model-free-rl", "t": "Model-Free RL", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "RL algorithms that learn policies or value functions directly from experience without building an explicit model of the...", "l": "m", "k": ["model-free", "algorithms", "learn", "policies", "value", "functions", "directly", "experience", "without", "building", "explicit", "model", "environment", "methods", "simpler"]}, {"id": "term-modelnet", "t": "ModelNet", "tg": ["Benchmark", "3D", "Computer Vision"], "d": "datasets", "x": "A dataset of 3D CAD models for 3D object recognition containing 151128 models across 662 categories. ModelNet40 a...", "l": "m", "k": ["modelnet", "dataset", "cad", "models", "object", "recognition", "containing", "across", "categories", "modelnet40", "40-category", "subset", "standard", "classification", "benchmark"]}, {"id": "term-modelscope-text-to-video", "t": "ModelScope Text-to-Video", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A text-to-video synthesis model that uses a multi-stage pipeline with spatio-temporal blocks for generating short...", "l": "m", "k": ["modelscope", "text-to-video", "synthesis", "model", "uses", "multi-stage", "pipeline", "spatio-temporal", "blocks", "generating", "short", "videos", "text", "descriptions"]}, {"id": "term-modern-hopfield-network", "t": "Modern Hopfield Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An updated Hopfield network formulation using exponential interaction functions that connects to transformer attention...", "l": "m", "k": ["modern", "hopfield", "network", "updated", "formulation", "exponential", "interaction", "functions", "connects", "transformer", "attention", "mechanisms", "provides", "storage", "capacity"]}, {"id": "term-mixture-of-experts", "t": "MoE (Mixture of Experts)", "tg": ["Architecture", "Efficiency"], "d": "models", "x": "An architecture where different \"expert\" sub-networks specialize in different types of inputs. Enables larger effective...", "l": "m", "k": ["moe", "mixture", "experts", "architecture", "different", "expert", "sub-networks", "specialize", "types", "inputs", "enables", "larger", "effective", "model", "capacity"]}, {"id": "term-moe-llava", "t": "MoE-LLaVA", "tg": ["Models", "Technical"], "d": "models", "x": "A multimodal large language model that uses mixture-of-experts to efficiently scale visual instruction tuning....", "l": "m", "k": ["moe-llava", "multimodal", "large", "language", "model", "uses", "mixture-of-experts", "efficiently", "scale", "visual", "instruction", "tuning", "activates", "subset", "parameters"]}, {"id": "term-moirai", "t": "MOIRAI", "tg": ["Models", "Technical"], "d": "models", "x": "A universal time series forecasting model that uses masked encoder-based architecture with any-variate attention to...", "l": "m", "k": ["moirai", "universal", "time", "series", "forecasting", "model", "uses", "masked", "encoder-based", "architecture", "any-variate", "attention", "handle", "diverse", "scenarios"]}, {"id": "term-moleculenet", "t": "MoleculeNet", "tg": ["Benchmark", "Scientific"], "d": "datasets", "x": "A benchmark collection of molecular datasets for evaluating molecular machine learning methods. Covers quantum...", "l": "m", "k": ["moleculenet", "benchmark", "collection", "molecular", "datasets", "evaluating", "machine", "learning", "methods", "covers", "quantum", "mechanics", "physical", "chemistry", "biophysics"]}, {"id": "term-molmo", "t": "Molmo", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A family of open vision-language models from AI2 that achieve strong multimodal performance using carefully curated...", "l": "m", "k": ["molmo", "family", "open", "vision-language", "models", "ai2", "achieve", "strong", "multimodal", "performance", "carefully", "curated", "training", "data", "efficient"]}, {"id": "term-moment", "t": "Moment", "tg": ["Models", "Technical"], "d": "models", "x": "A family of open-source foundation models for general-purpose time series analysis that handles classification and...", "l": "m", "k": ["moment", "family", "open-source", "foundation", "models", "general-purpose", "time", "series", "analysis", "handles", "classification", "forecasting", "anomaly", "detection", "tasks"]}, {"id": "term-moments-in-time", "t": "Moments in Time", "tg": ["Benchmark", "Video"], "d": "datasets", "x": "A large-scale video dataset with one million 3-second clips labeled with 339 action or activity classes. Covers diverse...", "l": "m", "k": ["moments", "time", "large-scale", "video", "dataset", "million", "3-second", "clips", "labeled", "action", "activity", "classes", "covers", "diverse", "events"]}, {"id": "term-momentum", "t": "Momentum", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "An optimization technique that accelerates gradient descent by accumulating an exponentially decaying moving average of...", "l": "m", "k": ["momentum", "optimization", "technique", "accelerates", "gradient", "descent", "accumulating", "exponentially", "decaying", "moving", "average", "past", "gradients", "helping", "optimizer"]}, {"id": "term-monitoring-bias", "t": "Monitoring Bias", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Bias that arises from differential surveillance or monitoring of certain populations by AI systems. Can create feedback...", "l": "m", "k": ["monitoring", "bias", "arises", "differential", "surveillance", "certain", "populations", "systems", "create", "feedback", "loops", "over-monitored", "groups", "appear", "higher"]}, {"id": "term-monkey", "t": "Monkey", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A vision-language model designed for processing high-resolution document images by dividing them into patches and using...", "l": "m", "k": ["monkey", "vision-language", "model", "designed", "processing", "high-resolution", "document", "images", "dividing", "patches", "sliding", "window", "approach"]}, {"id": "term-monte-carlo-dropout", "t": "Monte Carlo Dropout", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An approximate Bayesian inference technique that uses dropout at inference time to generate multiple stochastic...", "l": "m", "k": ["monte", "carlo", "dropout", "approximate", "bayesian", "inference", "technique", "uses", "time", "generate", "multiple", "stochastic", "predictions", "variance", "provides"]}, {"id": "term-monte-carlo-integration", "t": "Monte Carlo Integration", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "A numerical integration technique that estimates integrals by averaging function values at randomly sampled points....", "l": "m", "k": ["monte", "carlo", "integration", "numerical", "technique", "estimates", "integrals", "averaging", "function", "values", "randomly", "sampled", "points", "convergence", "rate"]}, {"id": "term-monte-carlo-method", "t": "Monte Carlo Method", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A broad class of computational algorithms that use repeated random sampling to obtain numerical results, such as...", "l": "m", "k": ["monte", "carlo", "method", "broad", "class", "computational", "algorithms", "repeated", "random", "sampling", "obtain", "numerical", "results", "estimating", "integrals"]}, {"id": "term-monte-carlo-methods-rl", "t": "Monte Carlo Methods in RL", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "RL algorithms that estimate value functions by averaging the actual returns observed over complete episodes. Unlike TD...", "l": "m", "k": ["monte", "carlo", "methods", "algorithms", "estimate", "value", "functions", "averaging", "actual", "returns", "observed", "complete", "episodes", "unlike", "approaches"]}, {"id": "term-monte-carlo-tree-search", "t": "Monte Carlo Tree Search (MCTS)", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A search algorithm that builds a decision tree through random simulations, using statistics from previous rollouts to...", "l": "m", "k": ["monte", "carlo", "tree", "search", "mcts", "algorithm", "builds", "decision", "random", "simulations", "statistics", "previous", "rollouts", "guide", "exploration"]}, {"id": "term-montreal-declaration-responsible-ai", "t": "Montreal Declaration for Responsible AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "A declaration adopted in 2018 establishing principles for responsible AI development including well-being, respect for...", "l": "m", "k": ["montreal", "declaration", "responsible", "adopted", "establishing", "principles", "development", "including", "well-being", "respect", "autonomy", "privacy", "democratic", "participation", "equity"]}, {"id": "term-moore-threads", "t": "Moore Threads", "tg": ["GPU", "China"], "d": "hardware", "x": "Chinese GPU company designing general-purpose GPUs with AI capabilities for the Chinese market. Aims to build a...", "l": "m", "k": ["moore", "threads", "chinese", "gpu", "company", "designing", "general-purpose", "gpus", "capabilities", "market", "aims", "build", "full-stack", "computing", "ecosystem"]}, {"id": "term-moores-law", "t": "Moore's Law", "tg": ["History", "Milestones"], "d": "history", "x": "An observation by Intel co-founder Gordon Moore in 1965 that the number of transistors on a microchip doubles...", "l": "m", "k": ["moore", "law", "observation", "intel", "co-founder", "gordon", "number", "transistors", "microchip", "doubles", "approximately", "years", "driven", "exponential", "increase"]}, {"id": "term-moral-machine", "t": "Moral Machine", "tg": ["Safety", "Ethics"], "d": "safety", "x": "An MIT research platform that crowdsources human moral preferences for autonomous vehicle dilemmas. Revealed...", "l": "m", "k": ["moral", "machine", "mit", "research", "platform", "crowdsources", "human", "preferences", "autonomous", "vehicle", "dilemmas", "revealed", "significant", "cultural", "variation"]}, {"id": "term-moral-status-of-ai", "t": "Moral Status of AI", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The philosophical question of whether AI systems can possess moral standing, such that their interests or welfare...", "l": "m", "k": ["moral", "status", "philosophical", "question", "systems", "possess", "standing", "interests", "welfare", "deserve", "ethical", "consideration", "closely", "tied", "debates"]}, {"id": "term-moral-stories", "t": "Moral Stories", "tg": ["Benchmark", "NLP", "Safety", "Reasoning"], "d": "datasets", "x": "A dataset of 12000 structured narratives illustrating moral and immoral actions with consequences. Tests the ability of...", "l": "m", "k": ["moral", "stories", "dataset", "structured", "narratives", "illustrating", "immoral", "actions", "consequences", "tests", "ability", "models", "understand", "social", "norms"]}, {"id": "term-moravecs-paradox", "t": "Moravec's Paradox", "tg": ["History", "Milestones"], "d": "history", "x": "The observation by Hans Moravec and others in the 1980s that high-level reasoning tasks are easy for AI while...", "l": "m", "k": ["moravec", "paradox", "observation", "hans", "others", "1980s", "high-level", "reasoning", "tasks", "easy", "sensorimotor", "skills", "seem", "simple", "humans"]}, {"id": "term-morpheme", "t": "Morpheme", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The smallest meaningful unit of language that cannot be further divided without losing its meaning, including roots,...", "l": "m", "k": ["morpheme", "smallest", "meaningful", "unit", "language", "cannot", "divided", "without", "losing", "meaning", "including", "roots", "prefixes", "suffixes", "inflectional"]}, {"id": "term-morphological-operations-algorithm", "t": "Morphological Operations Algorithm", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "A set of image processing operations based on set theory that process images according to a structuring element....", "l": "m", "k": ["morphological", "operations", "algorithm", "image", "processing", "based", "theory", "process", "images", "according", "structuring", "element", "includes", "erosion", "dilation"]}, {"id": "term-morphology", "t": "Morphology", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The branch of linguistics studying the internal structure of words, including how morphemes combine to form words...", "l": "m", "k": ["morphology", "branch", "linguistics", "studying", "internal", "structure", "words", "including", "morphemes", "combine", "form", "inflection", "derivation", "compounding", "processes"]}, {"id": "term-morris-counter-algorithm", "t": "Morris Counter Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "An approximate counting algorithm that maintains a probabilistic counter using only O(log log n) bits to count up to n....", "l": "m", "k": ["morris", "counter", "algorithm", "approximate", "counting", "maintains", "probabilistic", "log", "bits", "count", "increments", "probability", "inversely", "proportional", "current"]}, {"id": "term-mos-6502", "t": "MOS 6502", "tg": ["Historical", "Processor", "Consumer"], "d": "hardware", "x": "Inexpensive 8-bit microprocessor from 1975 that powered the Apple II Commodore 64 and Nintendo NES. Its low cost...", "l": "m", "k": ["mos", "inexpensive", "8-bit", "microprocessor", "powered", "apple", "commodore", "nintendo", "nes", "low", "cost", "democratized", "computing", "inspired", "generation"]}, {"id": "term-mosaic-augmentation", "t": "Mosaic Augmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A data augmentation technique that combines four training images into a single mosaic image, allowing the model to...", "l": "m", "k": ["mosaic", "augmentation", "data", "technique", "combines", "four", "training", "images", "single", "image", "allowing", "model", "learn", "multiple", "contexts"]}, {"id": "term-moshi", "t": "Moshi", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "A speech-text foundation model from Kyutai that enables real-time spoken dialogue with simultaneous listening and...", "l": "m", "k": ["moshi", "speech-text", "foundation", "model", "kyutai", "enables", "real-time", "spoken", "dialogue", "simultaneous", "listening", "speaking", "capabilities"]}, {"id": "term-moth-flame-optimization", "t": "Moth-Flame Optimization", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A nature-inspired metaheuristic based on the navigation behavior of moths spiraling toward light sources. Moths update...", "l": "m", "k": ["moth-flame", "optimization", "nature-inspired", "metaheuristic", "based", "navigation", "behavior", "moths", "spiraling", "toward", "light", "sources", "update", "positions", "logarithmic"]}, {"id": "term-motiondiffuser", "t": "MotionDiffuser", "tg": ["Models", "Technical", "Autonomous"], "d": "models", "x": "A diffusion-based model for multi-agent motion prediction in autonomous driving that generates diverse and realistic...", "l": "m", "k": ["motiondiffuser", "diffusion-based", "model", "multi-agent", "motion", "prediction", "autonomous", "driving", "generates", "diverse", "realistic", "future", "trajectory", "forecasts"]}, {"id": "term-motivational-control", "t": "Motivational Control", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "Safety measures that shape an AI system's goals and values to be aligned with human interests, as opposed to capability...", "l": "m", "k": ["motivational", "control", "safety", "measures", "shape", "system", "goals", "values", "aligned", "human", "interests", "opposed", "capability", "restricts", "physically"]}, {"id": "term-movement-pruning", "t": "Movement Pruning", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A pruning method that removes weights based on their movement during fine-tuning rather than their magnitude. Weights...", "l": "m", "k": ["movement", "pruning", "method", "removes", "weights", "based", "fine-tuning", "rather", "magnitude", "move", "toward", "zero", "pruned", "effective", "pretrained"]}, {"id": "term-moving-average-model", "t": "Moving Average Model", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A time series model where the current value is expressed as a linear combination of the current and past white noise...", "l": "m", "k": ["moving", "average", "model", "time", "series", "current", "value", "expressed", "linear", "combination", "past", "white", "noise", "error", "terms"]}, {"id": "term-mplug-docowl-15", "t": "mPLUG-DocOwl 1.5", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "An improved document understanding model that unifies document structure and text recognition with a multi-granularity...", "l": "m", "k": ["mplug-docowl", "improved", "document", "understanding", "model", "unifies", "structure", "text", "recognition", "multi-granularity", "parsing", "approach"]}, {"id": "term-mplug-owl2", "t": "mPLUG-Owl2", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multimodal large language model that uses modality-adaptive modules to handle both visual and textual inputs for...", "l": "m", "k": ["mplug-owl2", "multimodal", "large", "language", "model", "uses", "modality-adaptive", "modules", "handle", "visual", "textual", "inputs", "diverse", "tasks"]}, {"id": "term-mpnet", "t": "MPNet", "tg": ["Models", "Technical", "Embedding", "NLP"], "d": "models", "x": "A pre-trained language model that combines masked and permuted language modeling to capture both token dependencies and...", "l": "m", "k": ["mpnet", "pre-trained", "language", "model", "combines", "masked", "permuted", "modeling", "capture", "token", "dependencies", "positional", "information", "embeddings"]}, {"id": "term-mpt", "t": "MPT", "tg": ["Models", "Technical"], "d": "models", "x": "MosaicML Pretrained Transformer is a family of open-source language models trained by MosaicML using ALiBi position...", "l": "m", "k": ["mpt", "mosaicml", "pretrained", "transformer", "family", "open-source", "language", "models", "trained", "alibi", "position", "embeddings", "flashattention", "mpt-7b", "notable"]}, {"id": "term-mrpc", "t": "MRPC", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "The Microsoft Research Paraphrase Corpus containing 5801 sentence pairs extracted from online news sources annotated...", "l": "m", "k": ["mrpc", "microsoft", "research", "paraphrase", "corpus", "containing", "sentence", "pairs", "extracted", "online", "news", "sources", "annotated", "semantic", "equivalence"]}, {"id": "term-ms-marco", "t": "MS MARCO", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Microsoft Machine Reading Comprehension a large-scale reading comprehension and information retrieval dataset. Contains...", "l": "m", "k": ["marco", "microsoft", "machine", "reading", "comprehension", "large-scale", "information", "retrieval", "dataset", "contains", "million", "real", "bing", "queries", "human-generated"]}, {"id": "term-msr-vtt", "t": "MSR-VTT", "tg": ["Benchmark", "Video", "NLP"], "d": "datasets", "x": "Microsoft Research Video to Text a dataset of 10000 video clips with 200000 human-written descriptions. A primary...", "l": "m", "k": ["msr-vtt", "microsoft", "research", "video", "text", "dataset", "clips", "human-written", "descriptions", "primary", "benchmark", "captioning", "video-text", "retrieval"]}, {"id": "term-msr-vtt-retrieval", "t": "MSR-VTT Retrieval", "tg": ["Benchmark", "Video", "Multimodal"], "d": "datasets", "x": "A video-text retrieval benchmark derived from the MSR-VTT captioning dataset. Tests the ability to match natural...", "l": "m", "k": ["msr-vtt", "retrieval", "video-text", "benchmark", "derived", "captioning", "dataset", "tests", "ability", "match", "natural", "language", "descriptions", "video", "clips"]}, {"id": "term-msvd", "t": "MSVD", "tg": ["Benchmark", "Video", "NLP"], "d": "datasets", "x": "Microsoft Research Video Description a dataset of 1970 video clips with multilingual descriptions. An early and widely...", "l": "m", "k": ["msvd", "microsoft", "research", "video", "description", "dataset", "clips", "multilingual", "descriptions", "early", "widely", "benchmark", "captioning", "evaluation"]}, {"id": "term-mt-bench", "t": "MT-Bench", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Multi-Turn Benchmark, an evaluation framework that tests language models' conversational abilities across multi-turn...", "l": "m", "k": ["mt-bench", "multi-turn", "benchmark", "evaluation", "framework", "tests", "language", "models", "conversational", "abilities", "across", "dialogues", "follow-up", "questions", "llm"]}, {"id": "term-mt5", "t": "mT5", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A multilingual variant of T5 pre-trained on the mC4 corpus covering 101 languages for cross-lingual text-to-text...", "l": "m", "k": ["mt5", "multilingual", "variant", "pre-trained", "mc4", "corpus", "covering", "languages", "cross-lingual", "text-to-text", "transfer", "tasks"]}, {"id": "term-mteb", "t": "MTEB", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "Massive Text Embedding Benchmark a comprehensive benchmark for evaluating text embedding models across 8 tasks and 58...", "l": "m", "k": ["mteb", "massive", "text", "embedding", "benchmark", "comprehensive", "evaluating", "models", "across", "tasks", "datasets", "standard", "evaluation", "sentence", "document"]}, {"id": "term-mteb-retrieval", "t": "MTEB Retrieval", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "The retrieval subset of MTEB specifically evaluating passage and document retrieval capabilities of embedding models...", "l": "m", "k": ["mteb", "retrieval", "subset", "specifically", "evaluating", "passage", "document", "capabilities", "embedding", "models", "across", "diverse", "domains"]}, {"id": "term-mujoco", "t": "MuJoCo", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "Multi-Joint dynamics with Contact a physics engine and set of continuous control benchmarks for reinforcement learning....", "l": "m", "k": ["mujoco", "multi-joint", "dynamics", "contact", "physics", "engine", "continuous", "control", "benchmarks", "reinforcement", "learning", "includes", "locomotion", "manipulation", "robotic"]}, {"id": "term-multi-agent-rl", "t": "Multi-Agent Reinforcement Learning", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "RL involving multiple agents that interact within a shared environment, each with its own observations and objectives....", "l": "m", "k": ["multi-agent", "reinforcement", "learning", "involving", "multiple", "agents", "interact", "within", "shared", "environment", "observations", "objectives", "marl", "introduces", "challenges"]}, {"id": "term-multi-agent-systems", "t": "Multi-Agent Systems", "tg": ["History", "Fundamentals"], "d": "history", "x": "Systems composed of multiple interacting intelligent agents that can cooperate compete and coordinate to solve...", "l": "m", "k": ["multi-agent", "systems", "composed", "multiple", "interacting", "intelligent", "agents", "cooperate", "compete", "coordinate", "solve", "problems", "research", "draws", "game"]}, {"id": "term-multi-armed-bandit", "t": "Multi-Armed Bandit", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "A simplified RL problem where an agent repeatedly chooses among K actions (arms) to maximize cumulative reward, with no...", "l": "m", "k": ["multi-armed", "bandit", "simplified", "problem", "agent", "repeatedly", "chooses", "among", "actions", "arms", "maximize", "cumulative", "reward", "state", "transitions"]}, {"id": "term-multi-chip-module", "t": "Multi-Chip Module", "tg": ["Packaging", "Architecture", "Design"], "d": "hardware", "x": "Package containing multiple semiconductor dies interconnected within a single housing. Modern GPUs and AI accelerators...", "l": "m", "k": ["multi-chip", "module", "package", "containing", "multiple", "semiconductor", "dies", "interconnected", "within", "single", "housing", "modern", "gpus", "accelerators", "increasingly"]}, {"id": "term-multi-dimensional-scaling-algorithm", "t": "Multi-Dimensional Scaling Algorithm", "tg": ["Algorithms", "Fundamentals", "Dimensionality Reduction"], "d": "algorithms", "x": "A family of techniques that place objects in a low-dimensional space such that the distances between them approximate...", "l": "m", "k": ["multi-dimensional", "scaling", "algorithm", "family", "techniques", "place", "objects", "low-dimensional", "space", "distances", "approximate", "original", "dissimilarities", "classical", "mds"]}, {"id": "term-multi-gpu-training", "t": "Multi-GPU Training", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "Training a model using multiple GPUs simultaneously within a single node or across nodes, requiring parallelism...", "l": "m", "k": ["multi-gpu", "training", "model", "multiple", "gpus", "simultaneously", "within", "single", "node", "across", "nodes", "requiring", "parallelism", "strategies", "gradient"]}, {"id": "term-multi-head-attention", "t": "Multi-Head Attention", "tg": ["Architecture", "Transformers"], "d": "models", "x": "An extension of attention that runs multiple attention operations in parallel, each focusing on different aspects. A...", "l": "m", "k": ["multi-head", "attention", "extension", "runs", "multiple", "operations", "parallel", "focusing", "different", "aspects", "key", "component", "transformer", "architectures"]}, {"id": "term-mig", "t": "Multi-Instance GPU (MIG)", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "An NVIDIA feature that partitions a single GPU into up to seven isolated instances, each with dedicated compute,...", "l": "m", "k": ["multi-instance", "gpu", "mig", "nvidia", "feature", "partitions", "single", "seven", "isolated", "instances", "dedicated", "compute", "memory", "cache", "resources"]}, {"id": "term-multi-label-classification", "t": "Multi-Label Classification", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A classification task where each instance can belong to multiple classes simultaneously, unlike multi-class...", "l": "m", "k": ["multi-label", "classification", "task", "instance", "belong", "multiple", "classes", "simultaneously", "unlike", "multi-class", "exactly", "label", "examples", "include", "document"]}, {"id": "term-multi-layer-perceptron", "t": "Multi-Layer Perceptron", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A feedforward neural network with one or more hidden layers between input and output. Uses nonlinear activation...", "l": "m", "k": ["multi-layer", "perceptron", "feedforward", "neural", "network", "hidden", "layers", "input", "output", "uses", "nonlinear", "activation", "functions", "enabling", "learn"]}, {"id": "term-multi-layer-perceptron-model", "t": "Multi-Layer Perceptron Model", "tg": ["Models", "Fundamentals", "History"], "d": "models", "x": "A feedforward neural network with one or more hidden layers between input and output that can learn nonlinear decision...", "l": "m", "k": ["multi-layer", "perceptron", "model", "feedforward", "neural", "network", "hidden", "layers", "input", "output", "learn", "nonlinear", "decision", "boundaries", "backpropagation"]}, {"id": "term-multi-news", "t": "Multi-News", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A multi-document summarization dataset containing 56000 article-summary pairs where each summary covers 2 to 10 source...", "l": "m", "k": ["multi-news", "multi-document", "summarization", "dataset", "containing", "article-summary", "pairs", "summary", "covers", "source", "documents", "tests", "cross-document", "information", "synthesis"]}, {"id": "term-multi-object-tracking", "t": "Multi-Object Tracking", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of simultaneously tracking multiple objects through a video sequence, handling challenges like occlusion,...", "l": "m", "k": ["multi-object", "tracking", "task", "simultaneously", "multiple", "objects", "video", "sequence", "handling", "challenges", "occlusion", "identity", "switches", "entering", "leaving"]}, {"id": "term-multi-objective-reinforcement-learning", "t": "Multi-Objective Reinforcement Learning", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A framework for learning policies that optimize multiple potentially conflicting objectives simultaneously. Produces a...", "l": "m", "k": ["multi-objective", "reinforcement", "learning", "framework", "policies", "optimize", "multiple", "potentially", "conflicting", "objectives", "simultaneously", "produces", "pareto-optimal", "representing", "different"]}, {"id": "term-multi-objective-rl", "t": "Multi-Objective RL", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "RL formulations where the agent must optimize multiple potentially conflicting reward functions simultaneously....", "l": "m", "k": ["multi-objective", "formulations", "agent", "must", "optimize", "multiple", "potentially", "conflicting", "reward", "functions", "simultaneously", "solutions", "involve", "pareto-optimal", "policies"]}, {"id": "term-multi-persona-prompting", "t": "Multi-Persona Prompting", "tg": ["Prompt Engineering", "Persona"], "d": "general", "x": "A technique that assigns multiple distinct expert personas within a single prompt, having each persona contribute their...", "l": "m", "k": ["multi-persona", "prompting", "technique", "assigns", "multiple", "distinct", "expert", "personas", "within", "single", "prompt", "having", "persona", "contribute", "specialized"]}, {"id": "term-multi-query-attention", "t": "Multi-Query Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention variant where all query heads share a single set of key and value projections, significantly reducing...", "l": "m", "k": ["multi-query", "attention", "variant", "query", "heads", "share", "single", "key", "value", "projections", "significantly", "reducing", "memory", "bandwidth", "requirements"]}, {"id": "term-multi-query-retrieval", "t": "Multi-Query Retrieval", "tg": ["Retrieval", "Query Processing"], "d": "general", "x": "A technique that generates multiple paraphrased or perspective-shifted versions of the original query using an LLM,...", "l": "m", "k": ["multi-query", "retrieval", "technique", "generates", "multiple", "paraphrased", "perspective-shifted", "versions", "original", "query", "llm", "retrieves", "documents", "variant", "combines"]}, {"id": "term-multi-scale-feature-extraction", "t": "Multi-Scale Feature Extraction", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The technique of capturing features at different spatial resolutions or receptive field sizes within a network,...", "l": "m", "k": ["multi-scale", "feature", "extraction", "technique", "capturing", "features", "different", "spatial", "resolutions", "receptive", "field", "sizes", "within", "network", "enabling"]}, {"id": "term-multi-scale-testing", "t": "Multi-Scale Testing", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "An evaluation technique that processes an image at multiple resolutions and combines the predictions, improving...", "l": "m", "k": ["multi-scale", "testing", "evaluation", "technique", "processes", "image", "multiple", "resolutions", "combines", "predictions", "improving", "detection", "objects", "various", "scales"]}, {"id": "term-multi-stakeholder-ai-governance", "t": "Multi-Stakeholder AI Governance", "tg": ["Safety", "Governance"], "d": "safety", "x": "An approach to AI governance that involves representatives from government industry civil society academia and affected...", "l": "m", "k": ["multi-stakeholder", "governance", "approach", "involves", "representatives", "government", "industry", "civil", "society", "academia", "affected", "communities", "decision-making", "policy", "regulation"]}, {"id": "term-multi-step-bootstrapping", "t": "Multi-Step Bootstrapping", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A value estimation approach that uses n actual rewards before bootstrapping with a value estimate for the remaining...", "l": "m", "k": ["multi-step", "bootstrapping", "value", "estimation", "approach", "uses", "actual", "rewards", "estimate", "remaining", "future", "interpolating", "one-step", "monte", "carlo"]}, {"id": "term-multi-step-reasoning", "t": "Multi-Step Reasoning", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting paradigm that breaks complex problems into a sequence of intermediate reasoning steps, requiring the model...", "l": "m", "k": ["multi-step", "reasoning", "prompting", "paradigm", "breaks", "complex", "problems", "sequence", "intermediate", "steps", "requiring", "model", "solve", "sub-problem", "proceeding"]}, {"id": "term-multi-task-learning", "t": "Multi-Task Learning", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A learning approach where a model is trained simultaneously on multiple related tasks, sharing representations across...", "l": "m", "k": ["multi-task", "learning", "approach", "model", "trained", "simultaneously", "multiple", "related", "tasks", "sharing", "representations", "across", "improve", "generalization", "leveraging"]}, {"id": "term-multi-task-rl", "t": "Multi-Task Reinforcement Learning", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "RL approaches that train a single policy to perform well across multiple related tasks simultaneously. Multi-task RL...", "l": "m", "k": ["multi-task", "reinforcement", "learning", "approaches", "train", "single", "policy", "perform", "across", "multiple", "related", "tasks", "simultaneously", "leverages", "shared"]}, {"id": "term-multi-tenancy-vector-databases", "t": "Multi-Tenancy in Vector Databases", "tg": ["Vector Database", "Infrastructure"], "d": "hardware", "x": "The ability of a vector database to serve multiple isolated users or applications from a shared infrastructure, using...", "l": "m", "k": ["multi-tenancy", "vector", "databases", "ability", "database", "serve", "multiple", "isolated", "users", "applications", "shared", "infrastructure", "namespaces", "partitions", "metadata"]}, {"id": "term-multi-turn-conversation", "t": "Multi-Turn Conversation", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A dialogue format where a language model maintains context across multiple exchanges with a user, requiring the model...", "l": "m", "k": ["multi-turn", "conversation", "dialogue", "format", "language", "model", "maintains", "context", "across", "multiple", "exchanges", "user", "requiring", "track", "history"]}, {"id": "term-multi-vector-retrieval", "t": "Multi-Vector Retrieval", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "A retrieval approach that represents each document as multiple embedding vectors rather than a single vector, capturing...", "l": "m", "k": ["multi-vector", "retrieval", "approach", "represents", "document", "multiple", "embedding", "vectors", "rather", "single", "vector", "capturing", "different", "aspects", "segments"]}, {"id": "term-multi-view-clustering", "t": "Multi-View Clustering", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A clustering approach that integrates information from multiple representations or views of the same data. Seeks a...", "l": "m", "k": ["multi-view", "clustering", "approach", "integrates", "information", "multiple", "representations", "views", "data", "seeks", "consensus", "partition", "consistent", "across", "exploiting"]}, {"id": "term-multi-view-stereo", "t": "Multi-View Stereo", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A 3D reconstruction method that computes dense depth maps from multiple calibrated camera views, using photometric...", "l": "m", "k": ["multi-view", "stereo", "reconstruction", "method", "computes", "dense", "depth", "maps", "multiple", "calibrated", "camera", "views", "photometric", "consistency", "establish"]}, {"id": "term-multi-word-expression", "t": "Multi-Word Expression", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A combination of words that exhibits lexical, syntactic, semantic, or statistical idiosyncrasy, including idioms,...", "l": "m", "k": ["multi-word", "expression", "combination", "words", "exhibits", "lexical", "syntactic", "semantic", "statistical", "idiosyncrasy", "including", "idioms", "compound", "nouns", "phrasal"]}, {"id": "term-multicollinearity", "t": "Multicollinearity", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A condition in regression analysis where two or more independent variables are highly correlated, making it difficult...", "l": "m", "k": ["multicollinearity", "condition", "regression", "analysis", "independent", "variables", "highly", "correlated", "making", "difficult", "determine", "individual", "effect", "predictor", "inflating"]}, {"id": "term-multidimensional-scaling", "t": "Multidimensional Scaling", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A dimensionality reduction technique that positions points in low-dimensional space such that pairwise distances...", "l": "m", "k": ["multidimensional", "scaling", "dimensionality", "reduction", "technique", "positions", "points", "low-dimensional", "space", "pairwise", "distances", "approximate", "original", "high-dimensional", "visualization"]}, {"id": "term-multigrid-method", "t": "Multigrid Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A numerical method that accelerates the convergence of iterative solvers by using a hierarchy of discretization grids....", "l": "m", "k": ["multigrid", "method", "numerical", "accelerates", "convergence", "iterative", "solvers", "hierarchy", "discretization", "grids", "smooths", "errors", "grid", "level", "transfers"]}, {"id": "term-multihop-rag", "t": "MultiHop-RAG", "tg": ["Benchmark", "NLP", "Evaluation", "Reasoning"], "d": "datasets", "x": "A benchmark for evaluating retrieval-augmented generation on multi-hop reasoning questions. Tests whether RAG systems...", "l": "m", "k": ["multihop-rag", "benchmark", "evaluating", "retrieval-augmented", "generation", "multi-hop", "reasoning", "questions", "tests", "rag", "systems", "combine", "information", "across", "multiple"]}, {"id": "term-multilingual-model", "t": "Multilingual Model", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A single model trained on data from multiple languages that can perform NLP tasks across those languages, often...", "l": "m", "k": ["multilingual", "model", "single", "trained", "data", "multiple", "languages", "perform", "nlp", "tasks", "across", "developing", "cross-lingual", "transfer", "abilities"]}, {"id": "term-multilingual-spoken-words-corpus", "t": "Multilingual Spoken Words Corpus", "tg": ["Training Corpus", "Speech", "Multilingual"], "d": "datasets", "x": "A large dataset of spoken words in 50 languages from the Common Voice project. Provides keyword spotting data for...", "l": "m", "k": ["multilingual", "spoken", "words", "corpus", "large", "dataset", "languages", "common", "voice", "project", "provides", "keyword", "spotting", "data", "speech"]}, {"id": "term-multimodal", "t": "Multimodal", "tg": ["Capability", "Architecture"], "d": "models", "x": "AI systems that can process and generate multiple types of content (text, images, audio, video). Examples include...", "l": "m", "k": ["multimodal", "systems", "process", "generate", "multiple", "types", "content", "text", "images", "audio", "video", "examples", "include", "gpt-4v", "gemini"]}, {"id": "term-multimodal-ai-history", "t": "Multimodal AI History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of AI systems that can process and generate multiple types of data (text images audio video). From...", "l": "m", "k": ["multimodal", "history", "development", "systems", "process", "generate", "multiple", "types", "data", "text", "images", "audio", "video", "early", "separate"]}, {"id": "term-multinomial-distribution", "t": "Multinomial Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A generalization of the binomial distribution for experiments with more than two possible outcomes. It models the...", "l": "m", "k": ["multinomial", "distribution", "generalization", "binomial", "experiments", "possible", "outcomes", "models", "counts", "outcome", "across", "fixed", "number", "independent", "trials"]}, {"id": "term-multipl-e", "t": "MultiPL-E", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A multi-programming language benchmark that translates HumanEval and MBPP problems into 18 programming languages....", "l": "m", "k": ["multipl-e", "multi-programming", "language", "benchmark", "translates", "humaneval", "mbpp", "problems", "programming", "languages", "enables", "evaluation", "code", "generation", "across"]}, {"id": "term-multiple-testing-correction", "t": "Multiple Testing Correction", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "Statistical methods for adjusting significance thresholds when performing many simultaneous hypothesis tests to control...", "l": "m", "k": ["multiple", "testing", "correction", "statistical", "methods", "adjusting", "significance", "thresholds", "performing", "simultaneous", "hypothesis", "tests", "control", "overall", "error"]}, {"id": "term-multirc", "t": "MultiRC", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Multi-Sentence Reading Comprehension a dataset where each question requires reasoning over multiple sentences in a...", "l": "m", "k": ["multirc", "multi-sentence", "reading", "comprehension", "dataset", "question", "requires", "reasoning", "multiple", "sentences", "paragraph", "answers", "constrained", "spans", "text"]}, {"id": "term-multiwoz", "t": "MultiWOZ", "tg": ["Benchmark", "NLP", "Dialogue"], "d": "datasets", "x": "Multi-Domain Wizard-of-Oz a large-scale multi-domain task-oriented dialogue dataset with over 10000 dialogues spanning...", "l": "m", "k": ["multiwoz", "multi-domain", "wizard-of-oz", "large-scale", "task-oriented", "dialogue", "dataset", "dialogues", "spanning", "domains", "primary", "benchmark", "state", "tracking"]}, {"id": "term-musdb18", "t": "MUSDB18", "tg": ["Benchmark", "Audio"], "d": "datasets", "x": "A dataset of 150 full-length music tracks with isolated stems for vocals drums bass and other instruments. The standard...", "l": "m", "k": ["musdb18", "dataset", "full-length", "music", "tracks", "isolated", "stems", "vocals", "drums", "bass", "instruments", "standard", "benchmark", "source", "separation"]}, {"id": "term-music-algorithm", "t": "MUSIC Algorithm", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "Multiple Signal Classification is a high-resolution frequency estimation method that separates the signal and noise...", "l": "m", "k": ["music", "algorithm", "multiple", "signal", "classification", "high-resolution", "frequency", "estimation", "method", "separates", "noise", "subspaces", "eigendecomposition", "autocorrelation", "matrix"]}, {"id": "term-musiccaps", "t": "MusicCaps", "tg": ["Benchmark", "Audio", "Multimodal"], "d": "datasets", "x": "A dataset of 5500 music clips annotated with rich text descriptions by musicians. Used for evaluating music generation...", "l": "m", "k": ["musiccaps", "dataset", "music", "clips", "annotated", "rich", "text", "descriptions", "musicians", "evaluating", "generation", "music-text", "retrieval", "systems"]}, {"id": "term-musicgen", "t": "MusicGen", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A single-stage music generation model from Meta AI that uses a Transformer with efficient codebook interleaving to...", "l": "m", "k": ["musicgen", "single-stage", "music", "generation", "model", "meta", "uses", "transformer", "efficient", "codebook", "interleaving", "create", "text", "melody", "prompts"]}, {"id": "term-musiclm", "t": "MusicLM", "tg": ["Models", "Technical"], "d": "models", "x": "A music generation model by Google that generates high-fidelity music from text descriptions. Uses a hierarchical...", "l": "m", "k": ["musiclm", "music", "generation", "model", "google", "generates", "high-fidelity", "text", "descriptions", "uses", "hierarchical", "sequence-to-sequence", "approach", "conditioned", "melody"]}, {"id": "term-musique", "t": "MuSiQue", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "Multi-hop Questions via Single-hop Question Composition a benchmark of 25000 multi-hop questions constructed by...", "l": "m", "k": ["musique", "multi-hop", "questions", "via", "single-hop", "question", "composition", "benchmark", "constructed", "composing", "tests", "multi-step", "reasoning", "ability"]}, {"id": "term-mutr3d", "t": "MUTR3D", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "Multi-camera Tracking Transformer for 3D tracking that uses track queries and camera-aware attention for multi-object...", "l": "m", "k": ["mutr3d", "multi-camera", "tracking", "transformer", "uses", "track", "queries", "camera-aware", "attention", "multi-object", "autonomous", "driving"]}, {"id": "term-mutual-information", "t": "Mutual Information", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A measure of the statistical dependence between two random variables, quantifying how much knowing one variable reduces...", "l": "m", "k": ["mutual", "information", "measure", "statistical", "dependence", "random", "variables", "quantifying", "knowing", "variable", "reduces", "uncertainty", "feature", "selection", "clustering"]}, {"id": "term-mutual-information-feature-selection", "t": "Mutual Information Feature Selection", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A filter-based feature selection method that ranks features by their mutual information with the target variable,...", "l": "m", "k": ["mutual", "information", "feature", "selection", "filter-based", "method", "ranks", "features", "target", "variable", "measuring", "reduction", "uncertainty", "provided", "knowing"]}, {"id": "term-muzero", "t": "MuZero", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A model-based RL algorithm that learns a latent dynamics model, reward predictor, and value/policy networks without...", "l": "m", "k": ["muzero", "model-based", "algorithm", "learns", "latent", "dynamics", "model", "reward", "predictor", "value", "policy", "networks", "without", "requiring", "knowledge"]}, {"id": "term-muzero-algorithm", "t": "MuZero Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A model-based reinforcement learning algorithm that learns a dynamic model of the environment without requiring...", "l": "m", "k": ["muzero", "algorithm", "model-based", "reinforcement", "learning", "learns", "dynamic", "model", "environment", "without", "requiring", "knowledge", "game", "rules", "plans"]}, {"id": "term-mvbench", "t": "MVBench", "tg": ["Benchmark", "Video", "Multimodal", "Evaluation"], "d": "datasets", "x": "A multimodal video understanding benchmark testing temporal reasoning across 20 challenging video tasks. Evaluates the...", "l": "m", "k": ["mvbench", "multimodal", "video", "understanding", "benchmark", "testing", "temporal", "reasoning", "across", "challenging", "tasks", "evaluates", "ability", "understand", "dynamic"]}, {"id": "term-mvdream", "t": "MVDream", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A multi-view diffusion model that generates consistent multi-view images from text prompts by fine-tuning a 2D...", "l": "m", "k": ["mvdream", "multi-view", "diffusion", "model", "generates", "consistent", "images", "text", "prompts", "fine-tuning", "3d-aware", "training"]}, {"id": "term-mycin", "t": "MYCIN", "tg": ["History", "Milestones"], "d": "history", "x": "An early expert system developed at Stanford in the 1970s for diagnosing bacterial infections and recommending...", "l": "m", "k": ["mycin", "early", "expert", "system", "developed", "stanford", "1970s", "diagnosing", "bacterial", "infections", "recommending", "antibiotics", "demonstrating", "rule-based", "match"]}, {"id": "term-mythic-ai", "t": "Mythic AI", "tg": ["Accelerator", "Startup", "Analog"], "d": "hardware", "x": "AI chip company developing analog matrix processors that perform neural network computations in flash memory arrays....", "l": "m", "k": ["mythic", "chip", "company", "developing", "analog", "matrix", "processors", "perform", "neural", "network", "computations", "flash", "memory", "arrays", "targets"]}, {"id": "term-n-beats", "t": "N-BEATS", "tg": ["Models", "Technical"], "d": "models", "x": "Neural Basis Expansion Analysis for Time Series is a deep learning model for time series forecasting that uses backward...", "l": "n", "k": ["n-beats", "neural", "basis", "expansion", "analysis", "time", "series", "deep", "learning", "model", "forecasting", "uses", "backward", "forward", "residual"]}, {"id": "term-n-gram", "t": "N-gram", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A contiguous sequence of N items from a text, where items can be characters, words, or tokens, used in language models,...", "l": "n", "k": ["n-gram", "contiguous", "sequence", "items", "text", "characters", "words", "tokens", "language", "models", "classification", "information", "retrieval"]}, {"id": "term-n-gram-language-model", "t": "N-gram Language Model", "tg": ["Algorithms", "Fundamentals", "NLP"], "d": "algorithms", "x": "A statistical language model that predicts the next word based on the preceding n-1 words. Uses maximum likelihood...", "l": "n", "k": ["n-gram", "language", "model", "statistical", "predicts", "next", "word", "based", "preceding", "n-1", "words", "uses", "maximum", "likelihood", "estimation"]}, {"id": "term-n-hits", "t": "N-HiTS", "tg": ["Models", "Technical"], "d": "models", "x": "Neural Hierarchical Interpolation for Time Series extends N-BEATS with multi-rate sampling and hierarchical...", "l": "n", "k": ["n-hits", "neural", "hierarchical", "interpolation", "time", "series", "extends", "n-beats", "multi-rate", "sampling", "improved", "long-horizon", "forecasting"]}, {"id": "term-n-step-return", "t": "N-Step Return", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A return estimate that uses n actual rewards before bootstrapping with a value estimate, interpolating between one-step...", "l": "n", "k": ["n-step", "return", "estimate", "uses", "actual", "rewards", "bootstrapping", "value", "interpolating", "one-step", "full", "monte", "carlo", "infinity", "returns"]}, {"id": "term-nadam", "t": "Nadam", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Nesterov-accelerated Adaptive Moment estimation combines the Adam optimizer with Nesterov momentum. Provides faster...", "l": "n", "k": ["nadam", "nesterov-accelerated", "adaptive", "moment", "estimation", "combines", "adam", "optimizer", "nesterov", "momentum", "provides", "faster", "convergence", "incorporating", "look-ahead"]}, {"id": "term-naive-bayes", "t": "Naive Bayes", "tg": ["Machine Learning", "Probability"], "d": "algorithms", "x": "A family of probabilistic classifiers based on Bayes' theorem with the strong assumption that features are...", "l": "n", "k": ["naive", "bayes", "family", "probabilistic", "classifiers", "based", "theorem", "strong", "assumption", "features", "conditionally", "independent", "given", "class", "label"]}, {"id": "term-naive-bayes-classifier", "t": "Naive Bayes Classifier", "tg": ["Models", "Fundamentals", "History", "NLP"], "d": "models", "x": "A probabilistic classifier based on Bayes' theorem that assumes feature independence and works well for text...", "l": "n", "k": ["naive", "bayes", "classifier", "probabilistic", "based", "theorem", "assumes", "feature", "independence", "works", "text", "classification", "spam", "filtering", "tasks"]}, {"id": "term-naive-bayes-history", "t": "Naive Bayes History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The application of Bayes' theorem with naive independence assumptions to classification tasks. Despite its simplicity...", "l": "n", "k": ["naive", "bayes", "history", "application", "theorem", "independence", "assumptions", "classification", "tasks", "despite", "simplicity", "became", "practical", "machine", "learning"]}, {"id": "term-named-entity-linking", "t": "Named Entity Linking", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of mapping recognized named entity mentions in text to their corresponding entries in a knowledge base,...", "l": "n", "k": ["named", "entity", "linking", "task", "mapping", "recognized", "mentions", "text", "corresponding", "entries", "knowledge", "base", "resolving", "ambiguity", "multiple"]}, {"id": "term-named-entity-recognition", "t": "Named Entity Recognition (NER)", "tg": ["NLP Task", "Extraction"], "d": "general", "x": "An NLP task that identifies and classifies named entities (people, organizations, locations, dates) in text....", "l": "n", "k": ["named", "entity", "recognition", "ner", "nlp", "task", "identifies", "classifies", "entities", "people", "organizations", "locations", "dates", "text", "foundational"]}, {"id": "term-named-entity-recognition-algorithm", "t": "Named Entity Recognition Algorithm", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "An algorithm that identifies and classifies named entities (such as people and organizations and locations) in text....", "l": "n", "k": ["named", "entity", "recognition", "algorithm", "identifies", "classifies", "entities", "people", "organizations", "locations", "text", "approaches", "range", "conditional", "random"]}, {"id": "term-namespace", "t": "Namespace", "tg": ["Vector Database", "Infrastructure"], "d": "hardware", "x": "A logical partitioning mechanism within a vector database index that isolates vectors into separate searchable...", "l": "n", "k": ["namespace", "logical", "partitioning", "mechanism", "within", "vector", "database", "index", "isolates", "vectors", "separate", "searchable", "segments", "enabling", "multi-tenant"]}, {"id": "term-narrativeqa", "t": "NarrativeQA", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A question answering dataset requiring understanding of entire books and movie scripts. Questions are written about...", "l": "n", "k": ["narrativeqa", "question", "answering", "dataset", "requiring", "understanding", "entire", "books", "movie", "scripts", "questions", "written", "summaries", "must", "answered"]}, {"id": "term-narrow-ai", "t": "Narrow AI (ANI)", "tg": ["Category", "Concept"], "d": "general", "x": "AI systems designed for specific tasks, like playing chess or generating text. All current AI is narrow, as opposed to...", "l": "n", "k": ["narrow", "ani", "systems", "designed", "specific", "tasks", "playing", "chess", "generating", "text", "current", "opposed", "hypothetical", "artificial", "general"]}, {"id": "term-narrow-ai-safety", "t": "Narrow AI Safety", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "Safety research focused on currently deployed AI systems, addressing issues such as robustness to distribution shift,...", "l": "n", "k": ["narrow", "safety", "research", "focused", "currently", "deployed", "systems", "addressing", "issues", "robustness", "distribution", "shift", "adversarial", "inputs", "reward"]}, {"id": "term-nasnet", "t": "NASNet", "tg": ["Models", "Technical"], "d": "models", "x": "Neural Architecture Search Network designed automatically by a reinforcement learning controller searching over a space...", "l": "n", "k": ["nasnet", "neural", "architecture", "search", "network", "designed", "automatically", "reinforcement", "learning", "controller", "searching", "space", "possible", "architectures", "demonstrated"]}, {"id": "term-nathaniel-rochester", "t": "Nathaniel Rochester", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist at IBM who co-organized the 1956 Dartmouth Conference that founded AI as a field. Rochester...", "l": "n", "k": ["nathaniel", "rochester", "american", "computer", "scientist", "ibm", "co-organized", "dartmouth", "conference", "founded", "field", "led", "development", "assembler", "worked"]}, {"id": "term-natural-instructions", "t": "Natural Instructions", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A community effort collecting over 1600 diverse NLP tasks with expert-written instructions and examples. Used for...", "l": "n", "k": ["natural", "instructions", "community", "effort", "collecting", "diverse", "nlp", "tasks", "expert-written", "examples", "training", "evaluating", "instruction-following", "language", "models"]}, {"id": "term-natural-language", "t": "Natural Language", "tg": ["Concept", "Interface"], "d": "general", "x": "Human language as we naturally speak and write it. AI assistants are designed to understand natural language, so you...", "l": "n", "k": ["natural", "language", "human", "naturally", "speak", "write", "assistants", "designed", "understand", "don", "need", "special", "syntax", "formatting"]}, {"id": "term-natural-language-inference", "t": "Natural Language Inference", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of classifying the logical relationship between a premise and hypothesis text pair into entailment,...", "l": "n", "k": ["natural", "language", "inference", "task", "classifying", "logical", "relationship", "premise", "hypothesis", "text", "pair", "entailment", "contradiction", "neutral", "testing"]}, {"id": "term-nlp", "t": "Natural Language Processing (NLP)", "tg": ["Field", "Language"], "d": "general", "x": "The field of AI focused on enabling computers to understand, interpret, and generate human language. Encompasses tasks...", "l": "n", "k": ["natural", "language", "processing", "nlp", "field", "focused", "enabling", "computers", "understand", "interpret", "generate", "human", "encompasses", "tasks", "translation"]}, {"id": "term-nlp-history", "t": "Natural Language Processing History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of NLP from rule-based approaches in the 1960s through statistical methods in the 1990s to neural...", "l": "n", "k": ["natural", "language", "processing", "history", "evolution", "nlp", "rule-based", "approaches", "1960s", "statistical", "methods", "1990s", "neural", "2010s", "transformer"]}, {"id": "term-natural-language-understanding-history", "t": "Natural Language Understanding History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of machine understanding of human language from early pattern matching (ELIZA 1966) and microworld...", "l": "n", "k": ["natural", "language", "understanding", "history", "evolution", "machine", "human", "early", "pattern", "matching", "eliza", "microworld", "systems", "shrdlu", "statistical"]}, {"id": "term-natural-policy-gradient", "t": "Natural Policy Gradient", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A policy gradient method that preconditions updates with the inverse Fisher information matrix, following the steepest...", "l": "n", "k": ["natural", "policy", "gradient", "method", "preconditions", "updates", "inverse", "fisher", "information", "matrix", "following", "steepest", "ascent", "direction", "space"]}, {"id": "term-natural-questions", "t": "Natural Questions", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A question answering benchmark by Google consisting of real user queries from Google Search paired with Wikipedia...", "l": "n", "k": ["natural", "questions", "question", "answering", "benchmark", "google", "consisting", "real", "user", "queries", "search", "paired", "wikipedia", "articles", "requiring"]}, {"id": "term-natural-questions-open", "t": "Natural Questions Open", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "The open-domain version of Natural Questions where models must answer Google queries without a specified evidence...", "l": "n", "k": ["natural", "questions", "open", "open-domain", "version", "models", "must", "answer", "google", "queries", "without", "specified", "evidence", "document", "tests"]}, {"id": "term-naturalspeech", "t": "NaturalSpeech", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A text-to-speech system from Microsoft that achieves human-level quality on single-speaker recording studio data...", "l": "n", "k": ["naturalspeech", "text-to-speech", "system", "microsoft", "achieves", "human-level", "quality", "single-speaker", "recording", "studio", "data", "variational", "autoencoders", "flow", "models"]}, {"id": "term-naturalspeech-2", "t": "NaturalSpeech 2", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A zero-shot speech synthesis model from Microsoft that uses a latent diffusion model with a neural audio codec for...", "l": "n", "k": ["naturalspeech", "zero-shot", "speech", "synthesis", "model", "microsoft", "uses", "latent", "diffusion", "neural", "audio", "codec", "diverse", "natural-sounding", "voices"]}, {"id": "term-nccl", "t": "NCCL", "tg": ["Distributed Computing", "GPU"], "d": "hardware", "x": "NVIDIA Collective Communications Library, a highly optimized library for multi-GPU and multi-node collective...", "l": "n", "k": ["nccl", "nvidia", "collective", "communications", "library", "highly", "optimized", "multi-gpu", "multi-node", "communication", "operations", "automatically", "selects", "best", "algorithms"]}, {"id": "term-ndcg", "t": "NDCG", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "Normalized Discounted Cumulative Gain, a ranking quality metric that evaluates the usefulness of retrieved items based...", "l": "n", "k": ["ndcg", "normalized", "discounted", "cumulative", "gain", "ranking", "quality", "metric", "evaluates", "usefulness", "retrieved", "items", "based", "position", "result"]}, {"id": "term-near-memory-computing", "t": "Near-Memory Computing", "tg": ["Memory", "Architecture", "Emerging"], "d": "hardware", "x": "Architecture placing compute units physically close to memory to reduce data transfer latency and energy. A promising...", "l": "n", "k": ["near-memory", "computing", "architecture", "placing", "compute", "units", "physically", "close", "memory", "reduce", "data", "transfer", "latency", "energy", "promising"]}, {"id": "term-neats-vs-scruffies", "t": "Neats vs Scruffies", "tg": ["History", "Fundamentals"], "d": "history", "x": "A characterization of a philosophical divide in AI research during the 1970s and 1980s. Neats favored formal...", "l": "n", "k": ["neats", "scruffies", "characterization", "philosophical", "divide", "research", "1970s", "1980s", "favored", "formal", "mathematical", "approaches", "provably", "correct", "algorithms"]}, {"id": "term-nectar", "t": "Nectar", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A preference ranking dataset created by aggregating multiple existing preference datasets into a unified format....", "l": "n", "k": ["nectar", "preference", "ranking", "dataset", "created", "aggregating", "multiple", "existing", "datasets", "unified", "format", "provides", "diverse", "comparison", "data"]}, {"id": "term-needle-in-haystack-test", "t": "Needle in a Haystack Test", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An evaluation method that measures a language model's ability to retrieve a specific piece of information placed at...", "l": "n", "k": ["needle", "haystack", "test", "evaluation", "method", "measures", "language", "model", "ability", "retrieve", "specific", "piece", "information", "placed", "various"]}, {"id": "term-negamax-algorithm", "t": "Negamax Algorithm", "tg": ["Algorithms", "Technical", "RL", "Searching"], "d": "algorithms", "x": "A simplified variant of the minimax algorithm that exploits the zero-sum property by negating values rather than...", "l": "n", "k": ["negamax", "algorithm", "simplified", "variant", "minimax", "exploits", "zero-sum", "property", "negating", "values", "rather", "alternating", "maximizing", "minimizing", "reduces"]}, {"id": "term-negation-detection", "t": "Negation Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying negation cues and their scope in text, determining which parts of a sentence are affected by...", "l": "n", "k": ["negation", "detection", "task", "identifying", "cues", "scope", "text", "determining", "parts", "sentence", "affected", "words", "never", "without"]}, {"id": "term-negative-externality", "t": "Negative Externality", "tg": ["Safety", "Ethics"], "d": "safety", "x": "A cost imposed on third parties who are not involved in an AI transaction or interaction. AI systems can generate...", "l": "n", "k": ["negative", "externality", "cost", "imposed", "parties", "involved", "transaction", "interaction", "systems", "generate", "externalities", "environmental", "impact", "discrimination", "social"]}, {"id": "term-negative-prompt", "t": "Negative Prompt", "tg": ["Prompting", "Technique"], "d": "general", "x": "Instructions telling AI what to avoid in its output. Common in image generation (\"no blur, no distortion\") and can be...", "l": "n", "k": ["negative", "prompt", "instructions", "telling", "avoid", "output", "common", "image", "generation", "blur", "distortion", "text", "exclude", "certain", "topics"]}, {"id": "term-negative-prompting", "t": "Negative Prompting", "tg": ["Prompt Engineering", "Constraints"], "d": "general", "x": "A technique that explicitly specifies what the model should avoid in its output, including unwanted content, styles,...", "l": "n", "k": ["negative", "prompting", "technique", "explicitly", "specifies", "model", "avoid", "output", "including", "unwanted", "content", "styles", "formats", "behaviors", "exclusion"]}, {"id": "term-negative-sampling", "t": "Negative Sampling", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A training approximation that replaces the full softmax over the vocabulary with a binary classification between true...", "l": "n", "k": ["negative", "sampling", "training", "approximation", "replaces", "full", "softmax", "vocabulary", "binary", "classification", "true", "context", "words", "randomly", "sampled"]}, {"id": "term-neighborhood-component-analysis", "t": "Neighborhood Component Analysis", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A supervised dimensionality reduction method that learns a linear transformation optimizing leave-one-out...", "l": "n", "k": ["neighborhood", "component", "analysis", "supervised", "dimensionality", "reduction", "method", "learns", "linear", "transformation", "optimizing", "leave-one-out", "classification", "accuracy", "training"]}, {"id": "term-nelder-mead-method", "t": "Nelder-Mead Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "A derivative-free optimization algorithm that uses a simplex of n+1 points in n-dimensional space. The simplex...", "l": "n", "k": ["nelder-mead", "method", "derivative-free", "optimization", "algorithm", "uses", "simplex", "points", "n-dimensional", "space", "transforms", "reflection", "expansion", "contraction", "operations"]}, {"id": "term-nemotron", "t": "Nemotron", "tg": ["Models", "Technical"], "d": "models", "x": "A family of language models developed by NVIDIA for enterprise AI applications. Features models optimized for...", "l": "n", "k": ["nemotron", "family", "language", "models", "developed", "nvidia", "enterprise", "applications", "features", "optimized", "instruction", "following", "code", "generation", "synthetic"]}, {"id": "term-nemotron-4", "t": "Nemotron-4", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A family of large language models from NVIDIA trained for enterprise applications with strong performance on reasoning...", "l": "n", "k": ["nemotron-4", "family", "large", "language", "models", "nvidia", "trained", "enterprise", "applications", "strong", "performance", "reasoning", "coding", "benchmarks"]}, {"id": "term-nemotron-4-340b", "t": "Nemotron-4 340B", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A 340 billion parameter model from NVIDIA designed for synthetic data generation and alignment training of smaller...", "l": "n", "k": ["nemotron-4", "340b", "billion", "parameter", "model", "nvidia", "designed", "synthetic", "data", "generation", "alignment", "training", "smaller", "language", "models"]}, {"id": "term-nequip", "t": "NequIP", "tg": ["Models", "Scientific"], "d": "models", "x": "Neural Equivariant Interatomic Potentials is an E(3)-equivariant neural network for learning molecular dynamics force...", "l": "n", "k": ["nequip", "neural", "equivariant", "interatomic", "potentials", "network", "learning", "molecular", "dynamics", "force", "fields", "high", "data", "efficiency"]}, {"id": "term-nerf", "t": "NeRF", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Neural Radiance Field, a method that represents 3D scenes as continuous volumetric functions parameterized by neural...", "l": "n", "k": ["nerf", "neural", "radiance", "field", "method", "represents", "scenes", "continuous", "volumetric", "functions", "parameterized", "networks", "enabling", "photorealistic", "novel"]}, {"id": "term-nested-cross-validation", "t": "Nested Cross-Validation", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A model evaluation technique using an inner cross-validation loop for hyperparameter tuning and an outer loop for...", "l": "n", "k": ["nested", "cross-validation", "model", "evaluation", "technique", "inner", "loop", "hyperparameter", "tuning", "outer", "unbiased", "performance", "estimation", "preventing", "information"]}, {"id": "term-nested-dissection", "t": "Nested Dissection", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A fill-reducing ordering strategy for sparse matrix factorization that recursively bisects the graph of the matrix....", "l": "n", "k": ["nested", "dissection", "fill-reducing", "ordering", "strategy", "sparse", "matrix", "factorization", "recursively", "bisects", "graph", "produces", "orderings", "minimize", "fill-in"]}, {"id": "term-nested-ner", "t": "Nested Named Entity Recognition", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A NER variant that handles entities embedded within other entities, such as recognizing both 'Bank of America' as an...", "l": "n", "k": ["nested", "named", "entity", "recognition", "ner", "variant", "handles", "entities", "embedded", "within", "recognizing", "bank", "america", "organization", "location"]}, {"id": "term-nesterov-accelerated-gradient", "t": "Nesterov Accelerated Gradient", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A variant of momentum-based optimization that computes the gradient at a lookahead position rather than the current...", "l": "n", "k": ["nesterov", "accelerated", "gradient", "variant", "momentum-based", "optimization", "computes", "lookahead", "position", "rather", "current", "providing", "better", "convergence", "properties"]}, {"id": "term-netflix-prize", "t": "Netflix Prize", "tg": ["History", "Milestones"], "d": "history", "x": "A 2006-2009 open competition offering one million dollars for the best collaborative filtering algorithm to predict...", "l": "n", "k": ["netflix", "prize", "2006-2009", "open", "competition", "offering", "million", "dollars", "best", "collaborative", "filtering", "algorithm", "predict", "user", "movie"]}, {"id": "term-nethack-learning-environment", "t": "NetHack Learning Environment", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "A reinforcement learning environment based on the classic roguelike game NetHack. Tests long-horizon planning and...", "l": "n", "k": ["nethack", "learning", "environment", "reinforcement", "based", "classic", "roguelike", "game", "tests", "long-horizon", "planning", "exploration", "procedurally", "generated", "dungeons"]}, {"id": "term-network-bandwidth", "t": "Network Bandwidth", "tg": ["Networking", "Performance"], "d": "hardware", "x": "The maximum rate of data transfer across a network link measured in bits per second. Higher bandwidth reduces...", "l": "n", "k": ["network", "bandwidth", "maximum", "rate", "data", "transfer", "across", "link", "measured", "bits", "per", "higher", "reduces", "communication", "bottlenecks"]}, {"id": "term-network-congestion-control", "t": "Network Congestion Control", "tg": ["Networking", "Performance", "Protocol"], "d": "hardware", "x": "Mechanisms for managing data flow to prevent network overload in AI training clusters. Efficient congestion control is...", "l": "n", "k": ["network", "congestion", "control", "mechanisms", "managing", "data", "flow", "prevent", "overload", "training", "clusters", "efficient", "critical", "maintaining", "consistent"]}, {"id": "term-network-flow-decomposition", "t": "Network Flow Decomposition", "tg": ["Algorithms", "Technical", "Graph", "Optimization"], "d": "algorithms", "x": "An algorithm that decomposes a flow in a network into a set of path flows and cycle flows. Useful for interpreting flow...", "l": "n", "k": ["network", "flow", "decomposition", "algorithm", "decomposes", "path", "flows", "cycle", "useful", "interpreting", "solutions", "routing", "traffic", "transportation", "communication"]}, {"id": "term-network-interface-card", "t": "Network Interface Card", "tg": ["Networking", "Hardware"], "d": "hardware", "x": "Hardware device connecting a computer to a network. Modern AI-optimized NICs offload communication protocols to...", "l": "n", "k": ["network", "interface", "card", "hardware", "device", "connecting", "computer", "modern", "ai-optimized", "nics", "offload", "communication", "protocols", "enabling", "higher"]}, {"id": "term-network-latency", "t": "Network Latency", "tg": ["Networking", "Performance"], "d": "hardware", "x": "The time delay for data to travel from source to destination across a network. Low latency is critical for synchronous...", "l": "n", "k": ["network", "latency", "time", "delay", "data", "travel", "source", "destination", "across", "low", "critical", "synchronous", "distributed", "training", "processors"]}, {"id": "term-network-simplex-algorithm", "t": "Network Simplex Algorithm", "tg": ["Algorithms", "Technical", "Optimization", "Graph"], "d": "algorithms", "x": "A specialized version of the simplex algorithm for solving minimum-cost network flow problems. Exploits the network...", "l": "n", "k": ["network", "simplex", "algorithm", "specialized", "version", "solving", "minimum-cost", "flow", "problems", "exploits", "structure", "perform", "pivots", "efficiently", "general"]}, {"id": "term-network-topology", "t": "Network Topology", "tg": ["Networking", "Architecture"], "d": "hardware", "x": "The physical or logical arrangement of nodes and connections in a computing cluster or data center network. Topology...", "l": "n", "k": ["network", "topology", "physical", "logical", "arrangement", "nodes", "connections", "computing", "cluster", "data", "center", "choice", "significantly", "impacts", "communication"]}, {"id": "term-neural-architecture-search", "t": "Neural Architecture Search", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An automated method for designing neural network architectures by searching over a defined search space. Methods...", "l": "n", "k": ["neural", "architecture", "search", "automated", "method", "designing", "network", "architectures", "searching", "defined", "space", "methods", "include", "reinforcement", "learning"]}, {"id": "term-nas-vision", "t": "Neural Architecture Search for Vision", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Automated methods for discovering optimal CNN or ViT architectures by searching over design choices (kernel sizes,...", "l": "n", "k": ["neural", "architecture", "search", "vision", "automated", "methods", "discovering", "optimal", "cnn", "vit", "architectures", "searching", "design", "choices", "kernel"]}, {"id": "term-neural-architecture-search-hardware", "t": "Neural Architecture Search Hardware", "tg": ["Training", "NAS", "Optimization"], "d": "hardware", "x": "Hardware considerations in designing and executing neural architecture search which requires evaluating thousands of...", "l": "n", "k": ["neural", "architecture", "search", "hardware", "considerations", "designing", "executing", "requires", "evaluating", "thousands", "model", "configurations", "specialized", "dramatically", "accelerate"]}, {"id": "term-hw-aware-nas", "t": "Neural Architecture Search Hardware-Aware", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "NAS methods that incorporate hardware constraints (latency, memory, power) into the search objective, finding...", "l": "n", "k": ["neural", "architecture", "search", "hardware-aware", "nas", "methods", "incorporate", "hardware", "constraints", "latency", "memory", "power", "objective", "finding", "architectures"]}, {"id": "term-neural-collaborative-filtering", "t": "Neural Collaborative Filtering", "tg": ["Models", "Technical", "Recommendation"], "d": "models", "x": "A deep learning approach to collaborative filtering that replaces dot products with neural networks for learning...", "l": "n", "k": ["neural", "collaborative", "filtering", "deep", "learning", "approach", "replaces", "dot", "products", "networks", "user-item", "interaction", "functions"]}, {"id": "term-neural-fictitious-self-play", "t": "Neural Fictitious Self-Play", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A deep reinforcement learning algorithm that combines fictitious play with neural network function approximation....", "l": "n", "k": ["neural", "fictitious", "self-play", "deep", "reinforcement", "learning", "algorithm", "combines", "play", "network", "function", "approximation", "trains", "average", "policy"]}, {"id": "term-neural-machine-translation", "t": "Neural Machine Translation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A machine translation approach using encoder-decoder neural networks that learn to map source language sequences to...", "l": "n", "k": ["neural", "machine", "translation", "approach", "encoder-decoder", "networks", "learn", "map", "source", "language", "sequences", "target", "end-to-end", "parallel", "corpora"]}, {"id": "term-neural-network", "t": "Neural Network", "tg": ["Architecture", "Fundamentals"], "d": "models", "x": "A computing system inspired by biological brains, composed of interconnected nodes (neurons) organized in layers. The...", "l": "n", "k": ["neural", "network", "computing", "system", "inspired", "biological", "brains", "composed", "interconnected", "nodes", "neurons", "organized", "layers", "foundation", "modern"]}, {"id": "term-neural-network-renaissance", "t": "Neural Network Renaissance", "tg": ["History", "Milestones"], "d": "history", "x": "The revival of interest in neural networks in the 2000s and 2010s driven by the work of Geoffrey Hinton and others on...", "l": "n", "k": ["neural", "network", "renaissance", "revival", "interest", "networks", "2000s", "2010s", "driven", "work", "geoffrey", "hinton", "others", "deep", "belief"]}, {"id": "term-neural-ode", "t": "Neural ODE", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A continuous-depth neural network that parameterizes the derivative of hidden states as a neural network and uses ODE...", "l": "n", "k": ["neural", "ode", "continuous-depth", "network", "parameterizes", "derivative", "hidden", "states", "uses", "solvers", "forward", "backward", "passes", "enabling", "adaptive"]}, {"id": "term-neural-ordinary-differential-equations", "t": "Neural Ordinary Differential Equations", "tg": ["History", "Milestones"], "d": "history", "x": "A class of deep learning models introduced by Chen et al. in 2018 that parameterize the derivative of the hidden state...", "l": "n", "k": ["neural", "ordinary", "differential", "equations", "class", "deep", "learning", "models", "introduced", "chen", "parameterize", "derivative", "hidden", "state", "network"]}, {"id": "term-npu", "t": "Neural Processing Unit (NPU)", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "A dedicated hardware accelerator designed specifically for neural network inference, typically integrated into SoCs for...", "l": "n", "k": ["neural", "processing", "unit", "npu", "dedicated", "hardware", "accelerator", "designed", "specifically", "network", "inference", "typically", "integrated", "socs", "on-device"]}, {"id": "term-neural-style-transfer", "t": "Neural Style Transfer", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A technique that applies the artistic style of one image to the content of another by optimizing a generated image to...", "l": "n", "k": ["neural", "style", "transfer", "technique", "applies", "artistic", "image", "content", "another", "optimizing", "generated", "match", "features", "source", "gram"]}, {"id": "term-neural-turing-machine", "t": "Neural Turing Machine", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A neural architecture augmented with external memory that the network can read from and write to via differentiable...", "l": "n", "k": ["neural", "turing", "machine", "architecture", "augmented", "external", "memory", "network", "read", "write", "via", "differentiable", "attention", "mechanisms", "enabling"]}, {"id": "term-neuralgcm", "t": "NeuralGCM", "tg": ["Models", "Scientific"], "d": "models", "x": "A hybrid model combining learned physics with neural networks for climate simulation that maintains physical...", "l": "n", "k": ["neuralgcm", "hybrid", "model", "combining", "learned", "physics", "neural", "networks", "climate", "simulation", "maintains", "physical", "consistency", "improving", "computational"]}, {"id": "term-neurips", "t": "NeurIPS", "tg": ["History", "Conferences"], "d": "history", "x": "The Conference on Neural Information Processing Systems originally founded in 1987 as NIPS. One of the most prestigious...", "l": "n", "k": ["neurips", "conference", "neural", "information", "processing", "systems", "originally", "founded", "nips", "prestigious", "influential", "conferences", "machine", "learning", "artificial"]}, {"id": "term-neuromorphic-computing", "t": "Neuromorphic Computing", "tg": ["Emerging", "Brain-Inspired", "Architecture"], "d": "hardware", "x": "Computing approach inspired by biological neural networks using specialized hardware that mimics how brains process...", "l": "n", "k": ["neuromorphic", "computing", "approach", "inspired", "biological", "neural", "networks", "specialized", "hardware", "mimics", "brains", "process", "information", "promises", "dramatically"]}, {"id": "term-neuromorphic-computing-ethics", "t": "Neuromorphic Computing Ethics", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Ethical considerations around brain-inspired computing architectures that more closely mimic biological neural...", "l": "n", "k": ["neuromorphic", "computing", "ethics", "ethical", "considerations", "around", "brain-inspired", "architectures", "closely", "mimic", "biological", "neural", "processing", "raises", "questions"]}, {"id": "term-nevilles-algorithm", "t": "Neville's Algorithm", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An algorithm for polynomial interpolation that constructs the interpolating polynomial incrementally using a tableau of...", "l": "n", "k": ["neville", "algorithm", "polynomial", "interpolation", "constructs", "interpolating", "incrementally", "tableau", "intermediate", "results", "allows", "evaluation", "single", "point", "without"]}, {"id": "term-newtons-method", "t": "Newton's Method", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A second-order optimization algorithm that uses the Hessian matrix to find the minimum of a function. Converges...", "l": "n", "k": ["newton", "method", "second-order", "optimization", "algorithm", "uses", "hessian", "matrix", "find", "minimum", "function", "converges", "quadratically", "near", "optimum"]}, {"id": "term-newton-cotes-formulas", "t": "Newton-Cotes Formulas", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A group of numerical integration formulas that evaluate the integrand at equally spaced points. Includes the...", "l": "n", "k": ["newton-cotes", "formulas", "group", "numerical", "integration", "evaluate", "integrand", "equally", "spaced", "points", "includes", "trapezoidal", "rule", "simpson", "special"]}, {"id": "term-next-token-prediction", "t": "Next Token Prediction", "tg": ["Training", "LLM"], "d": "models", "x": "The core training objective of autoregressive LLMs: predict the next token given all previous tokens. This simple...", "l": "n", "k": ["next", "token", "prediction", "core", "training", "objective", "autoregressive", "llms", "predict", "given", "previous", "tokens", "simple", "scale", "produces"]}, {"id": "term-next-gpt-data", "t": "NExT-GPT Data", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "Training data for the NExT-GPT any-to-any multimodal model covering text image audio and video modalities. Supports...", "l": "n", "k": ["next-gpt", "data", "training", "any-to-any", "multimodal", "model", "covering", "text", "image", "audio", "video", "modalities", "supports", "cross-modal", "generation"]}, {"id": "term-next-qa", "t": "NeXT-QA", "tg": ["Benchmark", "Video", "Multimodal", "Reasoning"], "d": "datasets", "x": "A video question answering benchmark requiring causal and temporal reasoning about video events. Tests understanding of...", "l": "n", "k": ["next-qa", "video", "question", "answering", "benchmark", "requiring", "causal", "temporal", "reasoning", "events", "tests", "understanding", "cause-and-effect", "relationships", "content"]}, {"id": "term-nextvit", "t": "NextViT", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A next-generation vision Transformer designed for industrial deployment that balances latency and accuracy through...", "l": "n", "k": ["nextvit", "next-generation", "vision", "transformer", "designed", "industrial", "deployment", "balances", "latency", "accuracy", "efficient", "hybrid", "convolutional-attention", "blocks"]}, {"id": "term-nexusraven", "t": "NexusRaven", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A function-calling specialized language model that excels at converting natural language instructions into structured...", "l": "n", "k": ["nexusraven", "function-calling", "specialized", "language", "model", "excels", "converting", "natural", "instructions", "structured", "api", "calls", "tool", "usage"]}, {"id": "term-nf4", "t": "NF4 (Normal Float 4-bit)", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A 4-bit quantization format based on the assumption that neural network weights follow a normal distribution, using...", "l": "n", "k": ["nf4", "normal", "float", "4-bit", "quantization", "format", "based", "assumption", "neural", "network", "weights", "follow", "distribution", "quantile", "optimal"]}, {"id": "term-ngcf", "t": "NGCF", "tg": ["Models", "Technical", "Recommendation"], "d": "models", "x": "Neural Graph Collaborative Filtering embeds the user-item interaction graph using message passing to capture high-order...", "l": "n", "k": ["ngcf", "neural", "graph", "collaborative", "filtering", "embeds", "user-item", "interaction", "message", "passing", "capture", "high-order", "signals", "recommendations"]}, {"id": "term-niah", "t": "NIAH", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "Needle in a Haystack a test that places a specific fact within a long context and asks the model to retrieve it....", "l": "n", "k": ["niah", "needle", "haystack", "test", "places", "specific", "fact", "within", "long", "context", "asks", "model", "retrieve", "measures", "long-context"]}, {"id": "term-niklaus-wirth", "t": "Niklaus Wirth", "tg": ["History", "Pioneers"], "d": "history", "x": "Swiss computer scientist who designed several influential programming languages including Pascal (1970) and Modula-2....", "l": "n", "k": ["niklaus", "wirth", "swiss", "computer", "scientist", "designed", "several", "influential", "programming", "languages", "including", "pascal", "modula-2", "winner", "turing"]}, {"id": "term-nils-nilsson", "t": "Nils Nilsson", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1933-2019) who co-invented the A* search algorithm and developed foundational work in...", "l": "n", "k": ["nils", "nilsson", "american", "computer", "scientist", "1933-2019", "co-invented", "search", "algorithm", "developed", "foundational", "work", "robotics", "knowledge", "representation"]}, {"id": "term-nist-ai-rmf", "t": "NIST AI Risk Management Framework", "tg": ["Governance", "Regulation"], "d": "safety", "x": "A voluntary framework published by the US National Institute of Standards and Technology in 2023 that provides guidance...", "l": "n", "k": ["nist", "risk", "management", "framework", "voluntary", "published", "national", "institute", "standards", "technology", "provides", "guidance", "managing", "risks", "governance"]}, {"id": "term-nli-based-evaluation", "t": "NLI-Based Evaluation", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation approach that uses Natural Language Inference models to assess text quality by classifying whether...", "l": "n", "k": ["nli-based", "evaluation", "approach", "uses", "natural", "language", "inference", "models", "assess", "text", "quality", "classifying", "generated", "claims", "entailed"]}, {"id": "term-nllb", "t": "NLLB", "tg": ["Models", "Technical", "NLP", "Fundamentals"], "d": "models", "x": "No Language Left Behind is a machine translation model from Meta AI supporting direct translation between over 200...", "l": "n", "k": ["nllb", "language", "left", "behind", "machine", "translation", "model", "meta", "supporting", "direct", "languages", "including", "low-resource"]}, {"id": "term-nllb-dataset", "t": "NLLB Dataset", "tg": ["Training Corpus", "NLP", "Translation", "Multilingual"], "d": "datasets", "x": "The No Language Left Behind parallel corpus used by Meta to train translation models covering 200 languages. One of the...", "l": "n", "k": ["nllb", "dataset", "language", "left", "behind", "parallel", "corpus", "meta", "train", "translation", "models", "covering", "languages", "comprehensive", "multilingual"]}, {"id": "term-nlvr2", "t": "NLVR2", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "Natural Language Visual Reasoning a benchmark where models determine whether a statement is true about a pair of...", "l": "n", "k": ["nlvr2", "natural", "language", "visual", "reasoning", "benchmark", "models", "determine", "statement", "true", "pair", "images", "tests", "compositional", "grounded"]}, {"id": "term-no-free-lunch-theorem", "t": "No Free Lunch Theorem", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A set of theoretical results stating that no single learning algorithm performs best across all possible problems. When...", "l": "n", "k": ["free", "lunch", "theorem", "theoretical", "results", "stating", "single", "learning", "algorithm", "performs", "best", "across", "possible", "problems", "averaged"]}, {"id": "term-no-free-lunch-theorems", "t": "No Free Lunch Theorems", "tg": ["History", "Fundamentals"], "d": "history", "x": "Theorems published by David Wolpert and William Macready in 1997 proving that no optimization algorithm is universally...", "l": "n", "k": ["free", "lunch", "theorems", "published", "david", "wolpert", "william", "macready", "proving", "optimization", "algorithm", "universally", "superior", "across", "possible"]}, {"id": "term-no-robots", "t": "No Robots", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A high-quality dataset of 10000 instructions and demonstrations created entirely by skilled human annotators....", "l": "n", "k": ["robots", "high-quality", "dataset", "instructions", "demonstrations", "created", "entirely", "skilled", "human", "annotators", "demonstrates", "value", "human-written", "data", "synthetic"]}, {"id": "term-nobel-prize-for-ai", "t": "Nobel Prize for AI", "tg": ["History", "Milestones"], "d": "history", "x": "Recognition of AI contributions through Nobel Prizes including the 2024 Nobel Prize in Physics awarded to John Hopfield...", "l": "n", "k": ["nobel", "prize", "recognition", "contributions", "prizes", "including", "physics", "awarded", "john", "hopfield", "geoffrey", "hinton", "foundational", "discoveries", "enabling"]}, {"id": "term-nocaps", "t": "Nocaps", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "Novel Object Captioning at Scale a benchmark testing image captioning models on images containing objects not seen...", "l": "n", "k": ["nocaps", "novel", "object", "captioning", "scale", "benchmark", "testing", "image", "models", "images", "containing", "objects", "seen", "training", "evaluates"]}, {"id": "term-node2vec", "t": "Node2Vec", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A graph embedding algorithm that learns continuous feature representations for nodes by optimizing a...", "l": "n", "k": ["node2vec", "graph", "embedding", "algorithm", "learns", "continuous", "feature", "representations", "nodes", "optimizing", "neighborhood-preserving", "objective", "uses", "biased", "random"]}, {"id": "term-noel-sharkey", "t": "Noel Sharkey", "tg": ["History", "Pioneers"], "d": "history", "x": "British AI and robotics professor at the University of Sheffield known for public engagement with AI ethics...", "l": "n", "k": ["noel", "sharkey", "british", "robotics", "professor", "university", "sheffield", "known", "public", "engagement", "ethics", "particularly", "regarding", "autonomous", "weapons"]}, {"id": "term-noise", "t": "Noise (ML)", "tg": ["Concept", "Data"], "d": "general", "x": "Random variation in data or model outputs. In training, noise can cause or hide patterns. In diffusion models,...", "l": "n", "k": ["noise", "random", "variation", "data", "model", "outputs", "training", "cause", "hide", "patterns", "diffusion", "models", "controlled", "addition", "removal"]}, {"id": "term-noise-contrastive-estimation", "t": "Noise Contrastive Estimation", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "A method for estimating parameters of unnormalized statistical models by training a classifier to distinguish data...", "l": "n", "k": ["noise", "contrastive", "estimation", "method", "estimating", "parameters", "unnormalized", "statistical", "models", "training", "classifier", "distinguish", "data", "samples", "avoids"]}, {"id": "term-noise-schedule", "t": "Noise Schedule", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The predefined or learned sequence of noise levels in diffusion models that determines how quickly noise is added...", "l": "n", "k": ["noise", "schedule", "predefined", "learned", "sequence", "levels", "diffusion", "models", "determines", "quickly", "added", "forward", "process", "removed", "reverse"]}, {"id": "term-noisy-networks", "t": "Noisy Networks", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "A DQN extension that replaces epsilon-greedy exploration with parametric noise added to network weights, allowing the...", "l": "n", "k": ["noisy", "networks", "dqn", "extension", "replaces", "epsilon-greedy", "exploration", "parametric", "noise", "added", "network", "weights", "allowing", "agent", "learn"]}, {"id": "term-nomic-embed", "t": "Nomic Embed", "tg": ["Models", "Technical"], "d": "models", "x": "An open-source long-context text embedding model that processes up to 8192 tokens. Fully reproducible with open...", "l": "n", "k": ["nomic", "embed", "open-source", "long-context", "text", "embedding", "model", "processes", "tokens", "fully", "reproducible", "open", "training", "data", "code"]}, {"id": "term-non-discrimination-in-ai", "t": "Non-Discrimination in AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "The principle that AI systems should not discriminate against individuals or groups based on protected characteristics....", "l": "n", "k": ["non-discrimination", "principle", "systems", "discriminate", "against", "individuals", "groups", "based", "protected", "characteristics", "legal", "requirement", "jurisdictions", "core", "responsible"]}, {"id": "term-non-local-neural-network", "t": "Non-Local Neural Network", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A neural network module that computes the response at a position as a weighted sum of features at all positions,...", "l": "n", "k": ["non-local", "neural", "network", "module", "computes", "response", "position", "weighted", "sum", "features", "positions", "capturing", "long-range", "dependencies", "images"]}, {"id": "term-non-maximum-suppression", "t": "Non-Maximum Suppression", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A post-processing algorithm in object detection that eliminates redundant overlapping bounding box predictions by...", "l": "n", "k": ["non-maximum", "suppression", "post-processing", "algorithm", "object", "detection", "eliminates", "redundant", "overlapping", "bounding", "box", "predictions", "keeping", "highest-confidence", "instance"]}, {"id": "term-non-negative-matrix-factorization", "t": "Non-Negative Matrix Factorization", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "A matrix decomposition technique that factors a non-negative matrix into two non-negative matrices, producing...", "l": "n", "k": ["non-negative", "matrix", "factorization", "decomposition", "technique", "factors", "matrices", "producing", "parts-based", "representations", "useful", "topic", "modeling", "image", "analysis"]}, {"id": "term-nonmonotonic-reasoning", "t": "Nonmonotonic Reasoning", "tg": ["History", "Fundamentals"], "d": "history", "x": "A form of logical reasoning where the addition of new information can invalidate previously derived conclusions....", "l": "n", "k": ["nonmonotonic", "reasoning", "form", "logical", "addition", "information", "invalidate", "previously", "derived", "conclusions", "developed", "1980s", "researchers", "including", "raymond"]}, {"id": "term-norbert-wiener", "t": "Norbert Wiener", "tg": ["History", "Pioneers"], "d": "history", "x": "American mathematician (1894-1964) who founded cybernetics in his 1948 book of the same name, establishing the study of...", "l": "n", "k": ["norbert", "wiener", "american", "mathematician", "1894-1964", "founded", "cybernetics", "book", "name", "establishing", "study", "feedback", "control", "communication", "machines"]}, {"id": "term-norbert-wiener-cybernetics-book", "t": "Norbert Wiener Cybernetics Book", "tg": ["History", "Milestones"], "d": "history", "x": "The 1948 book Cybernetics: Or Control and Communication in the Animal and the Machine by Norbert Wiener. This...", "l": "n", "k": ["norbert", "wiener", "cybernetics", "book", "control", "communication", "animal", "machine", "foundational", "work", "established", "field", "introduced", "concepts", "feedback"]}, {"id": "term-norm-alignment", "t": "Norm Alignment", "tg": ["Safety", "Technical"], "d": "safety", "x": "Ensuring that AI systems respect and follow the social and cultural norms of the contexts in which they operate. More...", "l": "n", "k": ["norm", "alignment", "ensuring", "systems", "respect", "follow", "social", "cultural", "norms", "contexts", "operate", "nuanced", "simple", "rule-following", "vary"]}, {"id": "term-normal-distribution", "t": "Normal Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution characterized by its bell-shaped curve, symmetric about the mean, fully...", "l": "n", "k": ["normal", "distribution", "continuous", "probability", "characterized", "bell-shaped", "curve", "symmetric", "mean", "fully", "determined", "standard", "deviation", "natural", "phenomena"]}, {"id": "term-normality-test", "t": "Normality Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A statistical test that evaluates whether a dataset follows a normal distribution. Common methods include the...", "l": "n", "k": ["normality", "test", "statistical", "evaluates", "dataset", "follows", "normal", "distribution", "common", "methods", "include", "shapiro-wilk", "kolmogorov-smirnov", "anderson-darling"]}, {"id": "term-normalization", "t": "Normalization", "tg": ["Technique", "Training"], "d": "general", "x": "Techniques to standardize inputs or layer outputs in neural networks. Layer normalization is critical in transformers...", "l": "n", "k": ["normalization", "techniques", "standardize", "inputs", "layer", "outputs", "neural", "networks", "critical", "transformers", "stable", "training", "better", "generalization"]}, {"id": "term-normalized-cuts-algorithm", "t": "Normalized Cuts Algorithm", "tg": ["Algorithms", "Technical", "Vision", "Graph"], "d": "algorithms", "x": "A graph-based image segmentation method that partitions an image by minimizing the normalized cut criterion. Balances...", "l": "n", "k": ["normalized", "cuts", "algorithm", "graph-based", "image", "segmentation", "method", "partitions", "minimizing", "cut", "criterion", "balances", "total", "weight", "edges"]}, {"id": "term-normalized-mutual-information", "t": "Normalized Mutual Information", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A clustering evaluation metric that normalizes mutual information between predicted and ground truth clusterings to...", "l": "n", "k": ["normalized", "mutual", "information", "clustering", "evaluation", "metric", "normalizes", "predicted", "ground", "truth", "clusterings", "account", "chance", "ranges", "useful"]}, {"id": "term-normalizing-flow", "t": "Normalizing Flow", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative model that transforms a simple base distribution into a complex target distribution through a sequence of...", "l": "n", "k": ["normalizing", "flow", "generative", "model", "transforms", "simple", "base", "distribution", "complex", "target", "sequence", "invertible", "differentiable", "transformations", "tractable"]}, {"id": "term-normalizing-flows", "t": "Normalizing Flows", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A class of generative models that transform a simple base distribution into a complex target distribution through a...", "l": "n", "k": ["normalizing", "flows", "class", "generative", "models", "transform", "simple", "base", "distribution", "complex", "target", "sequence", "invertible", "differentiable", "mappings"]}, {"id": "term-notification-requirements-for-ai", "t": "Notification Requirements for AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "Legal obligations to inform individuals when significant decisions affecting them are made using AI systems. Part of...", "l": "n", "k": ["notification", "requirements", "legal", "obligations", "inform", "individuals", "significant", "decisions", "affecting", "systems", "part", "broader", "transparency", "due", "process"]}, {"id": "term-nougat", "t": "Nougat", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "Neural Optical Understanding for Academic Documents is a visual Transformer model that converts scanned academic PDFs...", "l": "n", "k": ["nougat", "neural", "optical", "understanding", "academic", "documents", "visual", "transformer", "model", "converts", "scanned", "pdfs", "structured", "markdown", "text"]}, {"id": "term-nous-hermes", "t": "Nous Hermes", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A fine-tuned language model from Nous Research optimized for instruction following and helpfulness using synthetic...", "l": "n", "k": ["nous", "hermes", "fine-tuned", "language", "model", "research", "optimized", "instruction", "following", "helpfulness", "synthetic", "training", "data"]}, {"id": "term-nous-hermes-2", "t": "Nous Hermes 2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A second-generation instruction-tuned model from Nous Research with improved reasoning and reduced hallucination...", "l": "n", "k": ["nous", "hermes", "second-generation", "instruction-tuned", "model", "research", "improved", "reasoning", "reduced", "hallucination", "advanced", "training", "techniques"]}, {"id": "term-novel-view-synthesis", "t": "Novel View Synthesis", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The task of generating photorealistic images of a scene from viewpoints not present in the input photographs, using...", "l": "n", "k": ["novel", "view", "synthesis", "task", "generating", "photorealistic", "images", "scene", "viewpoints", "present", "input", "photographs", "techniques", "nerf", "gaussian"]}, {"id": "term-nq-swap", "t": "NQ-Swap", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A modified version of Natural Questions with swapped entities testing whether models rely on parametric memory versus...", "l": "n", "k": ["nq-swap", "modified", "version", "natural", "questions", "swapped", "entities", "testing", "models", "rely", "parametric", "memory", "versus", "retrieved", "context"]}, {"id": "term-nsql", "t": "NSQL", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A family of language models fine-tuned for natural language to SQL translation supporting diverse database schemas and...", "l": "n", "k": ["nsql", "family", "language", "models", "fine-tuned", "natural", "sql", "translation", "supporting", "diverse", "database", "schemas", "query", "patterns"]}, {"id": "term-nsynth", "t": "NSynth", "tg": ["Benchmark", "Audio"], "d": "datasets", "x": "A large-scale dataset of 305979 musical notes from 1006 instruments with rich annotations. Used for audio synthesis and...", "l": "n", "k": ["nsynth", "large-scale", "dataset", "musical", "notes", "instruments", "rich", "annotations", "audio", "synthesis", "instrument", "sound", "generation", "research"]}, {"id": "term-nt-xent-loss", "t": "NT-Xent Loss", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Normalized Temperature-scaled Cross-Entropy loss used in SimCLR for self-supervised contrastive learning. Normalizes...", "l": "n", "k": ["nt-xent", "loss", "normalized", "temperature-scaled", "cross-entropy", "simclr", "self-supervised", "contrastive", "learning", "normalizes", "embeddings", "scales", "temperature", "parameter", "computing"]}, {"id": "term-ntk-aware-scaling", "t": "NTK-Aware Scaling", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A position encoding extension method based on Neural Tangent Kernel theory that modifies the frequency basis of rotary...", "l": "n", "k": ["ntk-aware", "scaling", "position", "encoding", "extension", "method", "based", "neural", "tangent", "kernel", "theory", "modifies", "frequency", "basis", "rotary"]}, {"id": "term-nucleus-sampling", "t": "Nucleus Sampling (Top-p)", "tg": ["Generation", "Parameter"], "d": "general", "x": "A text generation strategy that samples from the smallest set of tokens whose cumulative probability exceeds p....", "l": "n", "k": ["nucleus", "sampling", "top-p", "text", "generation", "strategy", "samples", "smallest", "tokens", "whose", "cumulative", "probability", "exceeds", "balances", "diversity"]}, {"id": "term-nudge-theory-and-ai", "t": "Nudge Theory and AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The application of behavioral nudging through AI-powered interfaces to influence user decisions. Raises ethical...", "l": "n", "k": ["nudge", "theory", "application", "behavioral", "nudging", "ai-powered", "interfaces", "influence", "user", "decisions", "raises", "ethical", "questions", "manipulation", "autonomy"]}, {"id": "term-null-hypothesis", "t": "Null Hypothesis", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A default assumption in statistical hypothesis testing that there is no effect or no difference between groups....", "l": "n", "k": ["null", "hypothesis", "default", "assumption", "statistical", "testing", "effect", "difference", "groups", "tests", "evaluate", "observed", "data", "provide", "sufficient"]}, {"id": "term-numerical-differentiation", "t": "Numerical Differentiation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Approximating derivatives using finite differences. Simple to implement but subject to numerical errors from truncation...", "l": "n", "k": ["numerical", "differentiation", "approximating", "derivatives", "finite", "differences", "simple", "implement", "subject", "errors", "truncation", "rounding", "verification", "tool", "gradient"]}, {"id": "term-numerical-stability-analysis", "t": "Numerical Stability Analysis", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "The study of how errors propagate through numerical algorithms. Forward and backward error analysis techniques identify...", "l": "n", "k": ["numerical", "stability", "analysis", "study", "errors", "propagate", "algorithms", "forward", "backward", "error", "techniques", "identify", "algorithm", "amplifies", "rounding"]}, {"id": "term-nuscenes", "t": "nuScenes", "tg": ["Benchmark", "Autonomous Driving"], "d": "datasets", "x": "A multimodal autonomous driving dataset with 1000 driving scenes from Boston and Singapore. Provides 3D bounding boxes...", "l": "n", "k": ["nuscenes", "multimodal", "autonomous", "driving", "dataset", "scenes", "boston", "singapore", "provides", "bounding", "boxes", "lidar", "camera", "full", "360-degree"]}, {"id": "term-nvidia-a10", "t": "NVIDIA A10", "tg": ["GPU", "NVIDIA", "Data Center"], "d": "hardware", "x": "NVIDIA Ampere data center GPU with 9216 CUDA cores and 24GB GDDR6 designed for AI inference and graphics rendering...", "l": "n", "k": ["nvidia", "a10", "ampere", "data", "center", "gpu", "cuda", "cores", "24gb", "gddr6", "designed", "inference", "graphics", "rendering", "workloads"]}, {"id": "term-a100", "t": "NVIDIA A100", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "NVIDIA's third-generation Tensor Core GPU based on the Ampere architecture, featuring 80GB HBM2e memory, support for...", "l": "n", "k": ["nvidia", "a100", "third-generation", "tensor", "core", "gpu", "based", "ampere", "architecture", "featuring", "80gb", "hbm2e", "memory", "support", "tf32"]}, {"id": "term-nvidia-a30", "t": "NVIDIA A30", "tg": ["GPU", "NVIDIA", "Data Center"], "d": "hardware", "x": "NVIDIA Ampere data center GPU with 3584 CUDA cores and 24GB HBM2 memory. Designed for mainstream AI training and...", "l": "n", "k": ["nvidia", "a30", "ampere", "data", "center", "gpu", "cuda", "cores", "24gb", "hbm2", "memory", "designed", "mainstream", "training", "inference"]}, {"id": "term-nvidia-a40", "t": "NVIDIA A40", "tg": ["GPU", "NVIDIA", "Data Center"], "d": "hardware", "x": "NVIDIA Ampere professional GPU with 10752 CUDA cores and 48GB GDDR6 with ECC. Used for AI inference combined with...", "l": "n", "k": ["nvidia", "a40", "ampere", "professional", "gpu", "cuda", "cores", "48gb", "gddr6", "ecc", "inference", "combined", "visualization", "data", "center"]}, {"id": "term-nvidia-ada-lovelace-architecture", "t": "NVIDIA Ada Lovelace Architecture", "tg": ["GPU", "NVIDIA", "Architecture"], "d": "hardware", "x": "NVIDIA GPU architecture for the RTX 40 series featuring third-generation ray tracing cores and fourth-generation Tensor...", "l": "n", "k": ["nvidia", "ada", "lovelace", "architecture", "gpu", "rtx", "series", "featuring", "third-generation", "ray", "tracing", "cores", "fourth-generation", "tensor", "named"]}, {"id": "term-nvidia-ai-dominance", "t": "NVIDIA AI Dominance", "tg": ["History", "Milestones"], "d": "history", "x": "NVIDIA's emergence as the dominant provider of AI computing hardware through its GPU technology. From gaming graphics...", "l": "n", "k": ["nvidia", "dominance", "emergence", "dominant", "provider", "computing", "hardware", "gpu", "technology", "gaming", "graphics", "training", "cuda", "platform", "a100"]}, {"id": "term-nvidia-ai-enterprise", "t": "NVIDIA AI Enterprise", "tg": ["Software", "NVIDIA", "Enterprise"], "d": "hardware", "x": "NVIDIA software suite for deploying AI applications in production environments. Provides enterprise support optimized...", "l": "n", "k": ["nvidia", "enterprise", "software", "suite", "deploying", "applications", "production", "environments", "provides", "support", "optimized", "containers", "management", "tools", "gpu"]}, {"id": "term-nvidia-ampere-architecture", "t": "NVIDIA Ampere Architecture", "tg": ["GPU", "NVIDIA", "Architecture"], "d": "hardware", "x": "NVIDIA GPU architecture powering the A100 and RTX 30 series GPUs. Introduced third-generation Tensor Cores and...", "l": "n", "k": ["nvidia", "ampere", "architecture", "gpu", "powering", "a100", "rtx", "series", "gpus", "introduced", "third-generation", "tensor", "cores", "structural", "sparsity"]}, {"id": "term-nvidia-b100", "t": "NVIDIA B100", "tg": ["GPU", "NVIDIA", "Data Center"], "d": "hardware", "x": "NVIDIA Blackwell generation data center GPU designed for AI training and inference with improved Tensor Core...", "l": "n", "k": ["nvidia", "b100", "blackwell", "generation", "data", "center", "gpu", "designed", "training", "inference", "improved", "tensor", "core", "performance", "memory"]}, {"id": "term-b200", "t": "NVIDIA B200", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "NVIDIA's Blackwell architecture GPU designed for next-generation AI training and inference, featuring second-generation...", "l": "n", "k": ["nvidia", "b200", "blackwell", "architecture", "gpu", "designed", "next-generation", "training", "inference", "featuring", "second-generation", "transformer", "engine", "fp4", "support"]}, {"id": "term-nvidia-base-command", "t": "NVIDIA Base Command", "tg": ["Infrastructure", "NVIDIA", "Management"], "d": "hardware", "x": "NVIDIA platform for managing AI infrastructure including job scheduling resource allocation and cluster monitoring....", "l": "n", "k": ["nvidia", "base", "command", "platform", "managing", "infrastructure", "including", "job", "scheduling", "resource", "allocation", "cluster", "monitoring", "provides", "enterprise"]}, {"id": "term-nvidia-blackwell-architecture", "t": "NVIDIA Blackwell Architecture", "tg": ["GPU", "NVIDIA", "Architecture"], "d": "hardware", "x": "NVIDIA GPU architecture generation succeeding Hopper featuring major advances in AI training and inference efficiency....", "l": "n", "k": ["nvidia", "blackwell", "architecture", "gpu", "generation", "succeeding", "hopper", "featuring", "major", "advances", "training", "inference", "efficiency", "introduces", "tensor"]}, {"id": "term-nvidia-bluefield-dpu", "t": "NVIDIA BlueField DPU", "tg": ["Networking", "NVIDIA", "Infrastructure"], "d": "hardware", "x": "NVIDIA Data Processing Unit combining an ARM CPU SmartNIC and programmable acceleration for data center infrastructure...", "l": "n", "k": ["nvidia", "bluefield", "dpu", "data", "processing", "unit", "combining", "arm", "cpu", "smartnic", "programmable", "acceleration", "center", "infrastructure", "offload"]}, {"id": "term-nvidia-connectx", "t": "NVIDIA ConnectX", "tg": ["Networking", "NVIDIA", "Adapter"], "d": "hardware", "x": "NVIDIA family of network adapters providing InfiniBand and Ethernet connectivity with hardware offload capabilities....", "l": "n", "k": ["nvidia", "connectx", "family", "network", "adapters", "providing", "infiniband", "ethernet", "connectivity", "hardware", "offload", "capabilities", "standard", "adapter", "high-performance"]}, {"id": "term-nvidia-cuda-toolkit", "t": "NVIDIA CUDA Toolkit", "tg": ["Programming", "NVIDIA", "Development"], "d": "hardware", "x": "Complete development environment for creating GPU-accelerated applications. Includes compilers debugging tools and...", "l": "n", "k": ["nvidia", "cuda", "toolkit", "complete", "development", "environment", "creating", "gpu-accelerated", "applications", "includes", "compilers", "debugging", "tools", "performance", "profiling"]}, {"id": "term-nvidia-dgx-a100", "t": "NVIDIA DGX A100", "tg": ["System", "NVIDIA", "Training"], "d": "hardware", "x": "NVIDIA AI system with eight A100 GPUs connected via NVSwitch providing 5 petaFLOPS of AI performance. Widely deployed...", "l": "n", "k": ["nvidia", "dgx", "a100", "system", "eight", "gpus", "connected", "via", "nvswitch", "providing", "petaflops", "performance", "widely", "deployed", "enterprise"]}, {"id": "term-nvidia-dgx-gb200", "t": "NVIDIA DGX GB200", "tg": ["System", "NVIDIA", "Supercomputer"], "d": "hardware", "x": "NVIDIA AI supercomputer system based on the GB200 Grace Blackwell Superchip. Connects 36 GB200 modules via NVLink...", "l": "n", "k": ["nvidia", "dgx", "gb200", "supercomputer", "system", "based", "grace", "blackwell", "superchip", "connects", "modules", "via", "nvlink", "providing", "massive"]}, {"id": "term-nvidia-dgx-h100", "t": "NVIDIA DGX H100", "tg": ["System", "NVIDIA", "Training"], "d": "hardware", "x": "NVIDIA AI system containing eight H100 GPUs connected via NVLink providing 32 petaFLOPS of AI performance. The building...", "l": "n", "k": ["nvidia", "dgx", "h100", "system", "containing", "eight", "gpus", "connected", "via", "nvlink", "providing", "petaflops", "performance", "building", "block"]}, {"id": "term-nvidia-dgx-station", "t": "NVIDIA DGX Station", "tg": ["System", "NVIDIA", "Workstation"], "d": "hardware", "x": "NVIDIA workstation-class AI system designed for data science teams providing multiple GPUs in a desktop form factor for...", "l": "n", "k": ["nvidia", "dgx", "station", "workstation-class", "system", "designed", "data", "science", "teams", "providing", "multiple", "gpus", "desktop", "form", "factor"]}, {"id": "term-nvidia-dgx-superpod", "t": "NVIDIA DGX SuperPOD", "tg": ["Data Center", "NVIDIA", "System"], "d": "hardware", "x": "NVIDIA reference architecture for AI supercomputing combining multiple DGX systems with high-bandwidth networking....", "l": "n", "k": ["nvidia", "dgx", "superpod", "reference", "architecture", "supercomputing", "combining", "multiple", "systems", "high-bandwidth", "networking", "provides", "scalable", "blueprint", "building"]}, {"id": "term-nvidia-fermi-architecture", "t": "NVIDIA Fermi Architecture", "tg": ["GPU", "NVIDIA", "Architecture"], "d": "hardware", "x": "NVIDIA GPU architecture from 2010 that was the first to support double-precision floating point efficiently. Enabled...", "l": "n", "k": ["nvidia", "fermi", "architecture", "gpu", "support", "double-precision", "floating", "point", "efficiently", "enabled", "wave", "gpu-accelerated", "deep", "learning"]}, {"id": "term-nvidia-gb200", "t": "NVIDIA GB200", "tg": ["GPU", "NVIDIA", "Data Center"], "d": "hardware", "x": "NVIDIA Blackwell generation GPU combining two B200 GPU dies in a single module for up to 20 petaFLOPS of AI...", "l": "n", "k": ["nvidia", "gb200", "blackwell", "generation", "gpu", "combining", "b200", "dies", "single", "module", "petaflops", "performance", "designed", "next-generation", "training"]}, {"id": "term-nvidia-geforce-rtx-3060", "t": "NVIDIA GeForce RTX 3060", "tg": ["GPU", "NVIDIA", "Consumer"], "d": "hardware", "x": "Entry-level Ampere GPU from NVIDIA with 3584 CUDA cores and 12GB GDDR6. Its large memory capacity relative to price...", "l": "n", "k": ["nvidia", "geforce", "rtx", "entry-level", "ampere", "gpu", "cuda", "cores", "12gb", "gddr6", "large", "memory", "capacity", "relative", "price"]}, {"id": "term-nvidia-geforce-rtx-3070", "t": "NVIDIA GeForce RTX 3070", "tg": ["GPU", "NVIDIA", "Consumer"], "d": "hardware", "x": "Mid-range Ampere consumer GPU from NVIDIA with 5888 CUDA cores and 8GB GDDR6. Offered accessible deep learning...", "l": "n", "k": ["nvidia", "geforce", "rtx", "mid-range", "ampere", "consumer", "gpu", "cuda", "cores", "8gb", "gddr6", "offered", "accessible", "deep", "learning"]}, {"id": "term-nvidia-geforce-rtx-3080", "t": "NVIDIA GeForce RTX 3080", "tg": ["GPU", "NVIDIA", "Consumer"], "d": "hardware", "x": "High-performance consumer GPU from NVIDIA Ampere architecture with 8704 CUDA cores and 10GB GDDR6X. Popular among AI...", "l": "n", "k": ["nvidia", "geforce", "rtx", "high-performance", "consumer", "gpu", "ampere", "architecture", "cuda", "cores", "10gb", "gddr6x", "popular", "among", "hobbyists"]}, {"id": "term-nvidia-geforce-rtx-3090", "t": "NVIDIA GeForce RTX 3090", "tg": ["GPU", "NVIDIA", "Consumer"], "d": "hardware", "x": "Flagship consumer GPU from the NVIDIA Ampere generation featuring 10496 CUDA cores and 24GB GDDR6X memory. Was the top...", "l": "n", "k": ["nvidia", "geforce", "rtx", "flagship", "consumer", "gpu", "ampere", "generation", "featuring", "cuda", "cores", "24gb", "gddr6x", "memory", "top"]}, {"id": "term-nvidia-geforce-rtx-4080", "t": "NVIDIA GeForce RTX 4080", "tg": ["GPU", "NVIDIA", "Consumer"], "d": "hardware", "x": "High-end consumer GPU from NVIDIA Ada Lovelace generation with 9728 CUDA cores and 16GB GDDR6X. Offers strong AI...", "l": "n", "k": ["nvidia", "geforce", "rtx", "high-end", "consumer", "gpu", "ada", "lovelace", "generation", "cuda", "cores", "16gb", "gddr6x", "offers", "strong"]}, {"id": "term-nvidia-geforce-rtx-4090", "t": "NVIDIA GeForce RTX 4090", "tg": ["GPU", "NVIDIA", "Consumer"], "d": "hardware", "x": "NVIDIA flagship consumer GPU based on the Ada Lovelace architecture featuring 16384 CUDA cores and 24GB GDDR6X memory....", "l": "n", "k": ["nvidia", "geforce", "rtx", "flagship", "consumer", "gpu", "based", "ada", "lovelace", "architecture", "featuring", "cuda", "cores", "24gb", "gddr6x"]}, {"id": "term-nvidia-gh200-nvl32", "t": "NVIDIA GH200 NVL32", "tg": ["System", "NVIDIA", "Platform"], "d": "hardware", "x": "NVIDIA platform connecting 32 Grace Hopper Superchips via NVLink providing 1 exaFLOPS of AI performance with unified...", "l": "n", "k": ["nvidia", "gh200", "nvl32", "platform", "connecting", "grace", "hopper", "superchips", "via", "nvlink", "providing", "exaflops", "performance", "unified", "memory"]}, {"id": "term-nvidia-grace", "t": "NVIDIA Grace CPU", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "NVIDIA's ARM-based data center CPU designed for AI and HPC workloads, featuring high memory bandwidth via LPDDR5X and...", "l": "n", "k": ["nvidia", "grace", "cpu", "arm-based", "data", "center", "designed", "hpc", "workloads", "featuring", "high", "memory", "bandwidth", "via", "lpddr5x"]}, {"id": "term-nvidia-grace-hopper-superchip", "t": "NVIDIA Grace Hopper Superchip", "tg": ["System", "NVIDIA", "Hybrid"], "d": "hardware", "x": "Combined CPU-GPU module pairing an NVIDIA Grace ARM CPU with an H100 Hopper GPU via NVLink C2C. Provides 900 GB/s...", "l": "n", "k": ["nvidia", "grace", "hopper", "superchip", "combined", "cpu-gpu", "module", "pairing", "arm", "cpu", "h100", "gpu", "via", "nvlink", "c2c"]}, {"id": "term-h100", "t": "NVIDIA H100", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "NVIDIA's fourth-generation Tensor Core GPU based on the Hopper architecture, featuring the Transformer Engine with FP8...", "l": "n", "k": ["nvidia", "h100", "fourth-generation", "tensor", "core", "gpu", "based", "hopper", "architecture", "featuring", "transformer", "engine", "fp8", "precision", "80gb"]}, {"id": "term-nvidia-h200", "t": "NVIDIA H200", "tg": ["GPU", "NVIDIA", "Data Center"], "d": "hardware", "x": "Next-generation NVIDIA data center GPU featuring Hopper architecture with 141GB HBM3e memory providing 4.8TB/s...", "l": "n", "k": ["nvidia", "h200", "next-generation", "data", "center", "gpu", "featuring", "hopper", "architecture", "141gb", "hbm3e", "memory", "providing", "8tb", "bandwidth"]}, {"id": "term-nvidia-hgx", "t": "NVIDIA HGX", "tg": ["System", "NVIDIA", "Platform"], "d": "hardware", "x": "NVIDIA multi-GPU baseboard platform designed as the GPU subsystem for OEM server designs. Connects eight GPUs via...", "l": "n", "k": ["nvidia", "hgx", "multi-gpu", "baseboard", "platform", "designed", "gpu", "subsystem", "oem", "server", "designs", "connects", "eight", "gpus", "via"]}, {"id": "term-nvidia-hopper-architecture", "t": "NVIDIA Hopper Architecture", "tg": ["GPU", "NVIDIA", "Architecture"], "d": "hardware", "x": "NVIDIA GPU architecture introduced in 2022 powering the H100 and H200 GPUs. Features the Transformer Engine for...", "l": "n", "k": ["nvidia", "hopper", "architecture", "gpu", "introduced", "powering", "h100", "h200", "gpus", "features", "transformer", "engine", "automatic", "mixed", "precision"]}, {"id": "term-nvidia-isaac", "t": "NVIDIA Isaac", "tg": ["Platform", "NVIDIA", "Robotics"], "d": "hardware", "x": "NVIDIA robotics simulation and deployment platform using GPU-accelerated physics simulation. Provides...", "l": "n", "k": ["nvidia", "isaac", "robotics", "simulation", "deployment", "platform", "gpu-accelerated", "physics", "provides", "hardware-in-the-loop", "testing", "ai-powered", "robots", "autonomous", "systems"]}, {"id": "term-nvidia-jetson-agx-orin", "t": "NVIDIA Jetson AGX Orin", "tg": ["Edge", "NVIDIA", "Platform"], "d": "hardware", "x": "NVIDIA highest-performance edge AI module with up to 275 TOPS for autonomous machines. Features an Ampere GPU and ARM...", "l": "n", "k": ["nvidia", "jetson", "agx", "orin", "highest-performance", "edge", "module", "tops", "autonomous", "machines", "features", "ampere", "gpu", "arm", "cpu"]}, {"id": "term-nvidia-jetson-nano", "t": "NVIDIA Jetson Nano", "tg": ["Edge", "NVIDIA", "Platform"], "d": "hardware", "x": "Entry-level NVIDIA edge AI platform with a 128-core Maxwell GPU providing 472 GFLOPS. Popular for learning AI...", "l": "n", "k": ["nvidia", "jetson", "nano", "entry-level", "edge", "platform", "128-core", "maxwell", "gpu", "providing", "gflops", "popular", "learning", "development", "prototyping"]}, {"id": "term-nvidia-jetson-orin", "t": "NVIDIA Jetson Orin", "tg": ["Edge", "NVIDIA", "Platform"], "d": "hardware", "x": "NVIDIA most powerful edge AI platform featuring an Ampere GPU with up to 275 TOPS of AI performance. Used in robotics...", "l": "n", "k": ["nvidia", "jetson", "orin", "powerful", "edge", "platform", "featuring", "ampere", "gpu", "tops", "performance", "robotics", "autonomous", "vehicles", "industrial"]}, {"id": "term-nvidia-jetson-xavier-nx", "t": "NVIDIA Jetson Xavier NX", "tg": ["Edge", "NVIDIA", "Platform"], "d": "hardware", "x": "Mid-range NVIDIA edge AI module providing 21 TOPS in a compact form factor. Bridges the gap between the entry-level...", "l": "n", "k": ["nvidia", "jetson", "xavier", "mid-range", "edge", "module", "providing", "tops", "compact", "form", "factor", "bridges", "gap", "entry-level", "nano"]}, {"id": "term-nvidia-kepler-architecture", "t": "NVIDIA Kepler Architecture", "tg": ["GPU", "NVIDIA", "Architecture"], "d": "hardware", "x": "NVIDIA GPU architecture from 2012 that introduced dynamic parallelism and Hyper-Q allowing the GPU to generate its own...", "l": "n", "k": ["nvidia", "kepler", "architecture", "gpu", "introduced", "dynamic", "parallelism", "hyper-q", "allowing", "generate", "work", "powered", "k80", "k40", "data"]}, {"id": "term-nvidia-l4", "t": "NVIDIA L4", "tg": ["GPU", "NVIDIA", "Data Center", "Inference"], "d": "hardware", "x": "Compact low-power NVIDIA Ada Lovelace inference GPU in a single-slot form factor with 7424 CUDA cores and 24GB GDDR6....", "l": "n", "k": ["nvidia", "compact", "low-power", "ada", "lovelace", "inference", "gpu", "single-slot", "form", "factor", "cuda", "cores", "24gb", "gddr6", "designed"]}, {"id": "term-nvidia-l40s", "t": "NVIDIA L40S", "tg": ["GPU", "NVIDIA", "Data Center"], "d": "hardware", "x": "NVIDIA Ada Lovelace data center GPU designed for AI inference and graphics workloads with 18176 CUDA cores and 48GB...", "l": "n", "k": ["nvidia", "l40s", "ada", "lovelace", "data", "center", "gpu", "designed", "inference", "graphics", "workloads", "cuda", "cores", "48gb", "gddr6"]}, {"id": "term-nvidia-maxwell-architecture", "t": "NVIDIA Maxwell Architecture", "tg": ["GPU", "NVIDIA", "Architecture"], "d": "hardware", "x": "NVIDIA GPU architecture focused on energy efficiency that delivered twice the performance per watt of its predecessor...", "l": "n", "k": ["nvidia", "maxwell", "architecture", "gpu", "focused", "energy", "efficiency", "delivered", "twice", "performance", "per", "watt", "predecessor", "kepler", "powered"]}, {"id": "term-nvidia-nsight", "t": "NVIDIA Nsight", "tg": ["Programming", "NVIDIA", "Development"], "d": "hardware", "x": "Suite of development tools from NVIDIA for profiling debugging and optimizing GPU-accelerated applications. Includes...", "l": "n", "k": ["nvidia", "nsight", "suite", "development", "tools", "profiling", "debugging", "optimizing", "gpu-accelerated", "applications", "includes", "systems", "system-level", "analysis", "compute"]}, {"id": "term-nvidia-omniverse", "t": "NVIDIA Omniverse", "tg": ["Platform", "NVIDIA", "Simulation"], "d": "hardware", "x": "NVIDIA platform for building and operating metaverse applications using GPU-accelerated simulation and rendering....", "l": "n", "k": ["nvidia", "omniverse", "platform", "building", "operating", "metaverse", "applications", "gpu-accelerated", "simulation", "rendering", "includes", "digital", "twin", "capabilities", "powered"]}, {"id": "term-nvidia-pascal-architecture", "t": "NVIDIA Pascal Architecture", "tg": ["GPU", "NVIDIA", "Architecture"], "d": "hardware", "x": "NVIDIA GPU architecture powering the Tesla P100 and GeForce GTX 10 series. First to use HBM2 memory and introduced...", "l": "n", "k": ["nvidia", "pascal", "architecture", "gpu", "powering", "tesla", "p100", "geforce", "gtx", "series", "hbm2", "memory", "introduced", "nvlink", "interconnect"]}, {"id": "term-nvidia-quantum-infiniband", "t": "NVIDIA Quantum InfiniBand", "tg": ["Networking", "NVIDIA", "InfiniBand"], "d": "hardware", "x": "NVIDIA InfiniBand networking platform providing 400 Gb/s per port bandwidth for AI training clusters. Includes...", "l": "n", "k": ["nvidia", "quantum", "infiniband", "networking", "platform", "providing", "per", "port", "bandwidth", "training", "clusters", "includes", "in-network", "computing", "capabilities"]}, {"id": "term-nvidia-smi", "t": "NVIDIA SMI", "tg": ["Monitoring", "NVIDIA", "Tool"], "d": "hardware", "x": "NVIDIA System Management Interface command-line tool for monitoring and managing NVIDIA GPU devices. Reports GPU...", "l": "n", "k": ["nvidia", "smi", "system", "management", "interface", "command-line", "tool", "monitoring", "managing", "gpu", "devices", "reports", "utilization", "memory", "usage"]}, {"id": "term-nvidia-spectrum-ethernet", "t": "NVIDIA Spectrum Ethernet", "tg": ["Networking", "NVIDIA", "Ethernet"], "d": "hardware", "x": "NVIDIA Ethernet switching platform optimized for AI workloads. Provides high-bandwidth low-latency Ethernet networking...", "l": "n", "k": ["nvidia", "spectrum", "ethernet", "switching", "platform", "optimized", "workloads", "provides", "high-bandwidth", "low-latency", "networking", "alternative", "infiniband", "data", "centers"]}, {"id": "term-nvidia-t4", "t": "NVIDIA T4", "tg": ["GPU", "NVIDIA", "Data Center", "Inference"], "d": "hardware", "x": "NVIDIA Turing architecture inference GPU with 2560 CUDA cores and 16GB GDDR6 memory consuming only 70 watts. Widely...", "l": "n", "k": ["nvidia", "turing", "architecture", "inference", "gpu", "cuda", "cores", "16gb", "gddr6", "memory", "consuming", "watts", "widely", "deployed", "cloud"]}, {"id": "term-nvidia-tesla-k40", "t": "NVIDIA Tesla K40", "tg": ["GPU", "NVIDIA", "Data Center"], "d": "hardware", "x": "NVIDIA Kepler architecture data center GPU with 2880 CUDA cores and 12GB GDDR5. Widely used in scientific computing and...", "l": "n", "k": ["nvidia", "tesla", "k40", "kepler", "architecture", "data", "center", "gpu", "cuda", "cores", "12gb", "gddr5", "widely", "scientific", "computing"]}, {"id": "term-nvidia-tesla-k80", "t": "NVIDIA Tesla K80", "tg": ["GPU", "NVIDIA", "Data Center"], "d": "hardware", "x": "Dual-GPU accelerator from NVIDIA Kepler architecture containing two GK210 chips with a combined 24GB GDDR5 memory. Was...", "l": "n", "k": ["nvidia", "tesla", "k80", "dual-gpu", "accelerator", "kepler", "architecture", "containing", "gk210", "chips", "combined", "24gb", "gddr5", "memory", "standard"]}, {"id": "term-nvidia-tesla-m40", "t": "NVIDIA Tesla M40", "tg": ["GPU", "NVIDIA", "Data Center"], "d": "hardware", "x": "NVIDIA Maxwell architecture data center GPU with 3072 CUDA cores and 12GB GDDR5. Used in data centers for deep learning...", "l": "n", "k": ["nvidia", "tesla", "m40", "maxwell", "architecture", "data", "center", "gpu", "cuda", "cores", "12gb", "gddr5", "centers", "deep", "learning"]}, {"id": "term-nvidia-tesla-p100", "t": "NVIDIA Tesla P100", "tg": ["GPU", "NVIDIA", "Data Center"], "d": "hardware", "x": "NVIDIA Pascal architecture data center GPU with 3584 CUDA cores and 16GB HBM2 memory. First GPU to use HBM2 and was...", "l": "n", "k": ["nvidia", "tesla", "p100", "pascal", "architecture", "data", "center", "gpu", "cuda", "cores", "16gb", "hbm2", "memory", "widely", "deployed"]}, {"id": "term-nvidia-tesla-v100", "t": "NVIDIA Tesla V100", "tg": ["GPU", "NVIDIA", "Data Center"], "d": "hardware", "x": "Data center GPU from NVIDIA Volta architecture featuring 5120 CUDA cores and 640 Tensor Cores with 16GB or 32GB HBM2...", "l": "n", "k": ["nvidia", "tesla", "v100", "data", "center", "gpu", "volta", "architecture", "featuring", "cuda", "cores", "tensor", "16gb", "32gb", "hbm2"]}, {"id": "term-nvidia-triton-inference-server", "t": "NVIDIA Triton Inference Server", "tg": ["Inference", "NVIDIA", "Serving"], "d": "hardware", "x": "Open-source inference serving software from NVIDIA supporting multiple AI frameworks and hardware backends. Provides...", "l": "n", "k": ["nvidia", "triton", "inference", "server", "open-source", "serving", "software", "supporting", "multiple", "frameworks", "hardware", "backends", "provides", "dynamic", "batching"]}, {"id": "term-nvidia-turing-architecture", "t": "NVIDIA Turing Architecture", "tg": ["GPU", "NVIDIA", "Architecture"], "d": "hardware", "x": "NVIDIA GPU architecture that introduced hardware ray tracing and second-generation Tensor Cores in the RTX 20 and T4...", "l": "n", "k": ["nvidia", "turing", "architecture", "gpu", "introduced", "hardware", "ray", "tracing", "second-generation", "tensor", "cores", "rtx", "product", "lines"]}, {"id": "term-nvidia-volta-architecture", "t": "NVIDIA Volta Architecture", "tg": ["GPU", "NVIDIA", "Architecture"], "d": "hardware", "x": "NVIDIA GPU architecture that introduced Tensor Cores for the first time powering the V100 GPU. Marked a fundamental...", "l": "n", "k": ["nvidia", "volta", "architecture", "gpu", "introduced", "tensor", "cores", "time", "powering", "v100", "marked", "fundamental", "shift", "toward", "dedicated"]}, {"id": "term-nvlink", "t": "NVLink", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "NVIDIA's proprietary high-bandwidth, low-latency interconnect for direct GPU-to-GPU communication, bypassing the PCIe...", "l": "n", "k": ["nvlink", "nvidia", "proprietary", "high-bandwidth", "low-latency", "interconnect", "direct", "gpu-to-gpu", "communication", "bypassing", "pcie", "bus", "hopper", "provides", "total"]}, {"id": "term-nvlink-c2c", "t": "NVLink C2C", "tg": ["Interconnect", "NVIDIA"], "d": "hardware", "x": "NVIDIA chip-to-chip interconnect technology providing ultra-high-bandwidth coherent connections between GPU and CPU...", "l": "n", "k": ["nvlink", "c2c", "nvidia", "chip-to-chip", "interconnect", "technology", "providing", "ultra-high-bandwidth", "coherent", "connections", "gpu", "cpu", "dies", "grace", "hopper"]}, {"id": "term-nvme", "t": "NVMe", "tg": ["Storage", "Protocol", "Standard"], "d": "hardware", "x": "Non-Volatile Memory Express storage protocol designed for solid-state drives communicating over PCIe. Provides...", "l": "n", "k": ["nvme", "non-volatile", "memory", "express", "storage", "protocol", "designed", "solid-state", "drives", "communicating", "pcie", "provides", "dramatically", "lower", "latency"]}, {"id": "term-nvswitch", "t": "NVSwitch", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "NVIDIA's fully connected switch fabric that enables all-to-all GPU communication within a node at full NVLink...", "l": "n", "k": ["nvswitch", "nvidia", "fully", "connected", "switch", "fabric", "enables", "all-to-all", "gpu", "communication", "within", "node", "full", "nvlink", "bandwidth"]}, {"id": "term-nyu-depth-v2", "t": "NYU Depth V2", "tg": ["Benchmark", "Computer Vision", "3D"], "d": "datasets", "x": "A dataset of 1449 densely labeled RGB-D images of indoor scenes from 464 different scenes. A standard benchmark for...", "l": "n", "k": ["nyu", "depth", "dataset", "densely", "labeled", "rgb-d", "images", "indoor", "scenes", "different", "standard", "benchmark", "monocular", "estimation", "scene"]}, {"id": "term-o1", "t": "o1", "tg": ["Models", "Technical"], "d": "models", "x": "An OpenAI reasoning model that uses chain-of-thought processing to solve complex problems. Spends additional compute...", "l": "o", "k": ["openai", "reasoning", "model", "uses", "chain-of-thought", "processing", "solve", "complex", "problems", "spends", "additional", "compute", "time", "thinking", "responding"]}, {"id": "term-o1-mini", "t": "o1-mini", "tg": ["Models", "Technical", "NLP", "Products"], "d": "models", "x": "A smaller reasoning model from OpenAI that provides strong performance on science and coding and math tasks with lower...", "l": "o", "k": ["o1-mini", "smaller", "reasoning", "model", "openai", "provides", "strong", "performance", "science", "coding", "math", "tasks", "lower", "latency", "full"]}, {"id": "term-o1-preview", "t": "o1-preview", "tg": ["Models", "Technical", "NLP", "Products"], "d": "models", "x": "An early-access version of the OpenAI o1 reasoning model that uses chain-of-thought processing to solve complex...", "l": "o", "k": ["o1-preview", "early-access", "version", "openai", "reasoning", "model", "uses", "chain-of-thought", "processing", "solve", "complex", "problems", "step"]}, {"id": "term-o3", "t": "o3", "tg": ["Models", "Technical"], "d": "models", "x": "An advanced reasoning model from OpenAI that extends o1 capabilities with improved performance on challenging...", "l": "o", "k": ["advanced", "reasoning", "model", "openai", "extends", "capabilities", "improved", "performance", "challenging", "benchmarks", "demonstrates", "stronger", "across", "mathematics", "coding"]}, {"id": "term-oasis", "t": "Oasis", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A real-time interactive world model that generates playable game environments from user actions using a...", "l": "o", "k": ["oasis", "real-time", "interactive", "world", "model", "generates", "playable", "game", "environments", "user", "actions", "transformer-based", "architecture", "without", "traditional"]}, {"id": "term-oasst1", "t": "OASST1", "tg": ["Training Corpus", "NLP", "Multilingual"], "d": "datasets", "x": "The first release of the OpenAssistant dataset containing 161443 messages in 35 languages organized as conversation...", "l": "o", "k": ["oasst1", "release", "openassistant", "dataset", "containing", "messages", "languages", "organized", "conversation", "trees", "human", "quality", "ratings"]}, {"id": "term-obelics", "t": "OBELICS", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "An open dataset of 141 million web pages with interleaved images and text filtered for quality. Used for pretraining...", "l": "o", "k": ["obelics", "open", "dataset", "million", "web", "pages", "interleaved", "images", "text", "filtered", "quality", "pretraining", "multimodal", "models", "idefics"]}, {"id": "term-objaverse", "t": "Objaverse", "tg": ["Training Corpus", "3D"], "d": "datasets", "x": "A massive dataset of over 800000 annotated 3D objects with diverse categories materials and annotations. Provides...", "l": "o", "k": ["objaverse", "massive", "dataset", "annotated", "objects", "diverse", "categories", "materials", "annotations", "provides", "large-scale", "data", "training", "generalist", "understanding"]}, {"id": "term-objaverse-xl", "t": "Objaverse-XL", "tg": ["Training Corpus", "3D"], "d": "datasets", "x": "An expanded version of Objaverse containing over 10 million 3D objects from diverse sources. One of the largest 3D...", "l": "o", "k": ["objaverse-xl", "expanded", "version", "objaverse", "containing", "million", "objects", "diverse", "sources", "largest", "object", "datasets", "training", "generation", "understanding"]}, {"id": "term-object-storage", "t": "Object Storage", "tg": ["Storage", "Cloud", "Architecture"], "d": "hardware", "x": "Storage architecture managing data as objects with metadata rather than files in directories. Used by cloud AI...", "l": "o", "k": ["object", "storage", "architecture", "managing", "data", "objects", "metadata", "rather", "files", "directories", "cloud", "platforms", "storing", "massive", "training"]}, {"id": "term-object-tracking", "t": "Object Tracking", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of following one or more objects across video frames by maintaining consistent identity assignments, using...", "l": "o", "k": ["object", "tracking", "task", "following", "objects", "across", "video", "frames", "maintaining", "consistent", "identity", "assignments", "methods", "combine", "detection"]}, {"id": "term-objectdrop", "t": "ObjectDrop", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A method for training image editing models using before-and-after photograph pairs to learn physically accurate shadow...", "l": "o", "k": ["objectdrop", "method", "training", "image", "editing", "models", "before-and-after", "photograph", "pairs", "learn", "physically", "accurate", "shadow", "reflection", "removal"]}, {"id": "term-objectnet", "t": "ObjectNet", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A test-only object recognition dataset with controlled real-world variations in viewpoint background and rotation....", "l": "o", "k": ["objectnet", "test-only", "object", "recognition", "dataset", "controlled", "real-world", "variations", "viewpoint", "background", "rotation", "designed", "measure", "true", "ability"]}, {"id": "term-observation-normalization", "t": "Observation Normalization", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The technique of normalizing input observations to zero mean and unit variance using running statistics, improving...", "l": "o", "k": ["observation", "normalization", "technique", "normalizing", "input", "observations", "zero", "mean", "unit", "variance", "running", "statistics", "improving", "neural", "network"]}, {"id": "term-observation-space", "t": "Observation Space", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The specification of the format and bounds of observations that an RL agent receives from the environment, including...", "l": "o", "k": ["observation", "space", "specification", "format", "bounds", "observations", "agent", "receives", "environment", "including", "data", "type", "shape", "valid", "ranges"]}, {"id": "term-observational-fairness", "t": "Observational Fairness", "tg": ["Safety", "Technical"], "d": "safety", "x": "A fairness criterion that can be evaluated using observed data without requiring causal models. Includes statistical...", "l": "o", "k": ["observational", "fairness", "criterion", "evaluated", "observed", "data", "without", "requiring", "causal", "models", "includes", "statistical", "parity", "equalized", "odds"]}, {"id": "term-occams-razor", "t": "Occam's Razor", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A principle favoring simpler models over more complex ones when both explain the data equally well. In machine...", "l": "o", "k": ["occam", "razor", "principle", "favoring", "simpler", "models", "complex", "ones", "explain", "data", "equally", "machine", "learning", "motivates", "regularization"]}, {"id": "term-occams-razor-in-ml", "t": "Occam's Razor in ML", "tg": ["History", "Fundamentals"], "d": "history", "x": "The application of the principle of parsimony to machine learning: simpler models that explain the data well are...", "l": "o", "k": ["occam", "razor", "application", "principle", "parsimony", "machine", "learning", "simpler", "models", "explain", "data", "preferred", "complex", "ones", "underlies"]}, {"id": "term-occnet", "t": "OccNet", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "An occupancy prediction network for autonomous driving that predicts 3D semantic occupancy from multi-camera images for...", "l": "o", "k": ["occnet", "occupancy", "prediction", "network", "autonomous", "driving", "predicts", "semantic", "multi-camera", "images", "comprehensive", "scene", "understanding"]}, {"id": "term-occupancy-gpu", "t": "Occupancy (GPU)", "tg": ["GPU", "Performance", "Metric"], "d": "hardware", "x": "Ratio of active warps to maximum warps a GPU streaming multiprocessor can support. Higher occupancy generally helps...", "l": "o", "k": ["occupancy", "gpu", "ratio", "active", "warps", "maximum", "streaming", "multiprocessor", "support", "higher", "generally", "helps", "hide", "memory", "latency"]}, {"id": "term-occupancy-network", "t": "Occupancy Network", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A neural network that predicts whether each point in 3D space is occupied or empty, representing 3D shapes as...", "l": "o", "k": ["occupancy", "network", "neural", "predicts", "point", "space", "occupied", "empty", "representing", "shapes", "continuous", "implicit", "functions", "rather", "discrete"]}, {"id": "term-octo", "t": "Octo", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "A generalist robot policy model trained on the Open X-Embodiment dataset that can be fine-tuned for diverse robotic...", "l": "o", "k": ["octo", "generalist", "robot", "policy", "model", "trained", "open", "x-embodiment", "dataset", "fine-tuned", "diverse", "robotic", "manipulation", "tasks", "across"]}, {"id": "term-octo-v2", "t": "Octo-v2", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "An improved generalist robot policy with enhanced fine-tuning capabilities and broader task coverage across diverse...", "l": "o", "k": ["octo-v2", "improved", "generalist", "robot", "policy", "enhanced", "fine-tuning", "capabilities", "broader", "task", "coverage", "across", "diverse", "robotic", "manipulation"]}, {"id": "term-octopus-v2", "t": "Octopus v2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An on-device language model specialized for function calling that maps natural language queries to software API calls...", "l": "o", "k": ["octopus", "on-device", "language", "model", "specialized", "function", "calling", "maps", "natural", "queries", "software", "api", "calls", "high", "accuracy"]}, {"id": "term-octree-algorithm", "t": "Octree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A three-dimensional extension of the quadtree that recursively subdivides space into eight octants. Used in 3D computer...", "l": "o", "k": ["octree", "algorithm", "three-dimensional", "extension", "quadtree", "recursively", "subdivides", "space", "eight", "octants", "computer", "graphics", "spatial", "indexing", "finite"]}, {"id": "term-odd-even-sort", "t": "Odd-Even Sort", "tg": ["Algorithms", "Technical", "Sorting"], "d": "algorithms", "x": "A comparison-based sorting algorithm that alternates between comparing odd-indexed pairs and even-indexed pairs of...", "l": "o", "k": ["odd-even", "sort", "comparison-based", "sorting", "algorithm", "alternates", "comparing", "odd-indexed", "pairs", "even-indexed", "adjacent", "elements", "designed", "parallel", "processing"]}, {"id": "term-odex", "t": "ODEX", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "Open-Domain Execution-based evaluation for code generation spanning diverse Python libraries beyond standard...", "l": "o", "k": ["odex", "open-domain", "execution-based", "evaluation", "code", "generation", "spanning", "diverse", "python", "libraries", "beyond", "standard", "competitive", "programming", "tests"]}, {"id": "term-oecd-ai-principles", "t": "OECD AI Principles", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Principles adopted by OECD member countries in 2019 promoting AI that is innovative, trustworthy, and respects human...", "l": "o", "k": ["oecd", "principles", "adopted", "member", "countries", "promoting", "innovative", "trustworthy", "respects", "human", "rights", "including", "recommendations", "transparency", "robustness"]}, {"id": "term-offensive-language-detection", "t": "Offensive Language Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of classifying text as offensive, abusive, or inappropriate, distinguishing between targeted insults,...", "l": "o", "k": ["offensive", "language", "detection", "task", "classifying", "text", "abusive", "inappropriate", "distinguishing", "targeted", "insults", "profanity", "general", "content", "moderation"]}, {"id": "term-office-31", "t": "Office-31", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A domain adaptation benchmark containing 4652 images across 31 categories from Amazon webcam and DSLR domains. One of...", "l": "o", "k": ["office-31", "domain", "adaptation", "benchmark", "containing", "images", "across", "categories", "amazon", "webcam", "dslr", "domains", "earliest", "datasets"]}, {"id": "term-offline-policy-evaluation", "t": "Offline Policy Evaluation", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A method for estimating the performance of a target policy using data collected by a different behavior policy....", "l": "o", "k": ["offline", "policy", "evaluation", "method", "estimating", "performance", "target", "data", "collected", "different", "behavior", "importance", "sampling", "doubly", "robust"]}, {"id": "term-offline-rl", "t": "Offline Reinforcement Learning", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "An RL paradigm that learns policies entirely from a fixed dataset of previously collected experience without further...", "l": "o", "k": ["offline", "reinforcement", "learning", "paradigm", "learns", "policies", "entirely", "fixed", "dataset", "previously", "collected", "experience", "without", "environment", "interaction"]}, {"id": "term-offloading", "t": "Offloading (CPU/Disk)", "tg": ["Model Optimization", "Distributed Computing"], "d": "models", "x": "A technique that stores model parameters, optimizer states, or activations in CPU RAM or disk when GPU memory is...", "l": "o", "k": ["offloading", "cpu", "disk", "technique", "stores", "model", "parameters", "optimizer", "states", "activations", "ram", "gpu", "memory", "insufficient", "transferring"]}, {"id": "term-offloading-cpunvme", "t": "Offloading (CPU/NVMe)", "tg": ["Training", "Optimization", "Memory"], "d": "hardware", "x": "Technique of moving optimizer states or parameters to CPU memory or NVMe storage when GPU memory is insufficient. Used...", "l": "o", "k": ["offloading", "cpu", "nvme", "technique", "moving", "optimizer", "states", "parameters", "memory", "storage", "gpu", "insufficient", "deepspeed", "zero-infinity", "train"]}, {"id": "term-ogb", "t": "OGB", "tg": ["Benchmark", "Graph"], "d": "datasets", "x": "Open Graph Benchmark a collection of realistic large-scale graph datasets for benchmarking graph neural networks....", "l": "o", "k": ["ogb", "open", "graph", "benchmark", "collection", "realistic", "large-scale", "datasets", "benchmarking", "neural", "networks", "covers", "node", "prediction", "link"]}, {"id": "term-ogb-lsc", "t": "OGB-LSC", "tg": ["Benchmark", "Graph"], "d": "datasets", "x": "The Open Graph Benchmark Large-Scale Challenge providing industrial-scale graph datasets for evaluating graph learning...", "l": "o", "k": ["ogb-lsc", "open", "graph", "benchmark", "large-scale", "challenge", "providing", "industrial-scale", "datasets", "evaluating", "learning", "methods", "realistic", "scales"]}, {"id": "term-ok-vqa", "t": "OK-VQA", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "Outside Knowledge VQA a visual question answering benchmark requiring external knowledge beyond what is visible in the...", "l": "o", "k": ["ok-vqa", "outside", "knowledge", "vqa", "visual", "question", "answering", "benchmark", "requiring", "external", "beyond", "visible", "image", "tests", "integration"]}, {"id": "term-oliver-selfridge", "t": "Oliver Selfridge", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1926-2008) who created the Pandemonium model for pattern recognition in 1959 and made...", "l": "o", "k": ["oliver", "selfridge", "american", "computer", "scientist", "1926-2008", "created", "pandemonium", "model", "pattern", "recognition", "early", "contributions", "machine", "learning"]}, {"id": "term-ollama", "t": "Ollama", "tg": ["Tools", "Local AI"], "d": "general", "x": "A tool for running LLMs locally on personal computers. Simplifies downloading and running open-source models like...", "l": "o", "k": ["ollama", "tool", "running", "llms", "locally", "personal", "computers", "simplifies", "downloading", "open-source", "models", "llama", "enabling", "private", "offline"]}, {"id": "term-olmo", "t": "OLMo", "tg": ["Models", "Technical", "NLP", "Fundamentals"], "d": "models", "x": "An open language model from AI2 (Allen Institute for AI) with fully open training data and code and weights designed...", "l": "o", "k": ["olmo", "open", "language", "model", "ai2", "allen", "institute", "fully", "training", "data", "code", "weights", "designed", "reproducible", "research"]}, {"id": "term-olmo-2", "t": "OLMo 2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A second-generation fully open language model from AI2 with improved training stability and performance on academic and...", "l": "o", "k": ["olmo", "second-generation", "fully", "open", "language", "model", "ai2", "improved", "training", "stability", "performance", "academic", "reasoning", "benchmarks"]}, {"id": "term-olmoe", "t": "OLMoE", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An open mixture-of-experts language model from AI2 that combines the fully-open philosophy of OLMo with sparse expert...", "l": "o", "k": ["olmoe", "open", "mixture-of-experts", "language", "model", "ai2", "combines", "fully-open", "philosophy", "olmo", "sparse", "expert", "routing", "efficiency"]}, {"id": "term-omegafold", "t": "OmegaFold", "tg": ["Models", "Scientific"], "d": "models", "x": "A single-sequence protein structure prediction model that achieves competitive accuracy without requiring multiple...", "l": "o", "k": ["omegafold", "single-sequence", "protein", "structure", "prediction", "model", "achieves", "competitive", "accuracy", "without", "requiring", "multiple", "sequence", "alignments", "evolutionary"]}, {"id": "term-omni-math", "t": "Omni-MATH", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A comprehensive mathematics benchmark covering problems from olympiad-level competitions across multiple mathematical...", "l": "o", "k": ["omni-math", "comprehensive", "mathematics", "benchmark", "covering", "problems", "olympiad-level", "competitions", "across", "multiple", "mathematical", "domains", "tests", "upper", "limits"]}, {"id": "term-omnigen", "t": "OmniGen", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A unified image generation model that handles text-to-image and image editing and subject-driven and multi-modal...", "l": "o", "k": ["omnigen", "unified", "image", "generation", "model", "handles", "text-to-image", "editing", "subject-driven", "multi-modal", "single", "architecture"]}, {"id": "term-omnimotion", "t": "OmniMotion", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A dense motion estimation method that represents a video as a quasi-3D canonical volume for computing complete...", "l": "o", "k": ["omnimotion", "dense", "motion", "estimation", "method", "represents", "video", "quasi-3d", "canonical", "volume", "computing", "complete", "long-range", "pair", "frames"]}, {"id": "term-on-device-ai", "t": "On-Device AI", "tg": ["Edge", "Inference", "Privacy"], "d": "hardware", "x": "Running AI models locally on user devices rather than sending data to cloud servers. Provides faster response times...", "l": "o", "k": ["on-device", "running", "models", "locally", "user", "devices", "rather", "sending", "data", "cloud", "servers", "provides", "faster", "response", "times"]}, {"id": "term-on-policy-vs-off-policy", "t": "On-Policy vs Off-Policy", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A distinction between RL methods that learn about the policy currently being executed (on-policy, e.g., SARSA) and...", "l": "o", "k": ["on-policy", "off-policy", "distinction", "methods", "learn", "policy", "currently", "executed", "sarsa", "different", "target", "data", "behavior", "q-learning", "enable"]}, {"id": "term-on-premises-ai-infrastructure", "t": "On-Premises AI Infrastructure", "tg": ["Data Center", "Infrastructure", "Deployment"], "d": "hardware", "x": "AI computing hardware deployed in an organization own facilities rather than in cloud data centers. Provides maximum...", "l": "o", "k": ["on-premises", "infrastructure", "computing", "hardware", "deployed", "organization", "facilities", "rather", "cloud", "data", "centers", "provides", "maximum", "control", "security"]}, {"id": "term-once", "t": "ONCE", "tg": ["Benchmark", "Autonomous Driving"], "d": "datasets", "x": "A one million scenes dataset for autonomous driving with 3D object annotations from a diverse range of driving...", "l": "o", "k": ["million", "scenes", "dataset", "autonomous", "driving", "object", "annotations", "diverse", "range", "conditions", "designed", "address", "data", "efficiency", "perception"]}, {"id": "term-one-fits-all-ofa", "t": "One Fits All (OFA)", "tg": ["Models", "Technical"], "d": "models", "x": "A pre-trained time series model that demonstrates a single Transformer backbone can handle multiple time series...", "l": "o", "k": ["fits", "ofa", "pre-trained", "time", "series", "model", "demonstrates", "single", "transformer", "backbone", "handle", "multiple", "analysis", "tasks", "across"]}, {"id": "term-one-2-3-45", "t": "One-2-3-45", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A single-image 3D reconstruction method that combines a multi-view diffusion model with a feed-forward 3D...", "l": "o", "k": ["one-2-3-45", "single-image", "reconstruction", "method", "combines", "multi-view", "diffusion", "model", "feed-forward", "module", "rapid", "object", "generation"]}, {"id": "term-one-class-svm", "t": "One-Class SVM", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A variant of support vector machines for anomaly detection that learns a decision boundary enclosing the normal data....", "l": "o", "k": ["one-class", "svm", "variant", "support", "vector", "machines", "anomaly", "detection", "learns", "decision", "boundary", "enclosing", "normal", "data", "maps"]}, {"id": "term-one-cycle-policy", "t": "One-Cycle Policy", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A learning rate schedule that uses a single cycle of increasing then decreasing learning rate combined with inverse...", "l": "o", "k": ["one-cycle", "policy", "learning", "rate", "schedule", "uses", "single", "cycle", "increasing", "decreasing", "combined", "inverse", "momentum", "scheduling", "proposed"]}, {"id": "term-one-hot-encoding", "t": "One-Hot Encoding", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A feature encoding technique that converts each categorical value into a binary vector with a single 1 at the position...", "l": "o", "k": ["one-hot", "encoding", "feature", "technique", "converts", "categorical", "value", "binary", "vector", "single", "position", "corresponding", "category", "elsewhere", "creating"]}, {"id": "term-one-peace", "t": "ONE-PEACE", "tg": ["Models", "Technical", "Audio", "Vision", "NLP"], "d": "models", "x": "A general representation model that aligns vision and audio and language through a unified architecture with...", "l": "o", "k": ["one-peace", "general", "representation", "model", "aligns", "vision", "audio", "language", "unified", "architecture", "modality-specific", "adapters", "cross-modal", "tasks"]}, {"id": "term-one-shot", "t": "One-Shot Learning", "tg": ["Prompting", "Technique"], "d": "general", "x": "Providing a single example in your prompt to demonstrate the desired output format or style. Falls between zero-shot...", "l": "o", "k": ["one-shot", "learning", "providing", "single", "example", "prompt", "demonstrate", "desired", "output", "format", "style", "falls", "zero-shot", "examples", "few-shot"]}, {"id": "term-one-vs-rest-classification", "t": "One-Vs-Rest Classification", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A strategy for extending binary classifiers to multi-class problems by training one classifier per class, treating that...", "l": "o", "k": ["one-vs-rest", "classification", "strategy", "extending", "binary", "classifiers", "multi-class", "problems", "training", "classifier", "per", "class", "treating", "positive", "others"]}, {"id": "term-oneapi", "t": "oneAPI", "tg": ["Programming", "Intel", "Standard"], "d": "hardware", "x": "Intel unified programming model for heterogeneous computing across CPUs GPUs FPGAs and other accelerators. Uses...", "l": "o", "k": ["oneapi", "intel", "unified", "programming", "model", "heterogeneous", "computing", "across", "cpus", "gpus", "fpgas", "accelerators", "uses", "sycl-based", "data"]}, {"id": "term-oneformer", "t": "OneFormer", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A universal image segmentation framework that unifies semantic and instance and panoptic segmentation with a single...", "l": "o", "k": ["oneformer", "universal", "image", "segmentation", "framework", "unifies", "semantic", "instance", "panoptic", "single", "transformer-based", "architecture"]}, {"id": "term-online-k-means-algorithm", "t": "Online K-Means Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A variant of k-means that processes data points one at a time and updates cluster centers incrementally. Suitable for...", "l": "o", "k": ["online", "k-means", "algorithm", "variant", "processes", "data", "points", "time", "updates", "cluster", "centers", "incrementally", "suitable", "streaming", "large"]}, {"id": "term-online-learning", "t": "Online Learning", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A learning paradigm where the model is updated incrementally as each new data point arrives, rather than being trained...", "l": "o", "k": ["online", "learning", "paradigm", "model", "updated", "incrementally", "data", "point", "arrives", "rather", "trained", "fixed", "batch", "suitable", "streaming"]}, {"id": "term-online-learning-algorithm", "t": "Online Learning Algorithm", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A learning paradigm where the model updates incrementally as new data arrives one example at a time. Suitable for...", "l": "o", "k": ["online", "learning", "algorithm", "paradigm", "model", "updates", "incrementally", "data", "arrives", "example", "time", "suitable", "streaming", "non-stationary", "environments"]}, {"id": "term-onnx", "t": "ONNX (Open Neural Network Exchange)", "tg": ["Format", "Interoperability"], "d": "general", "x": "An open format for representing ML models, enabling interoperability between different frameworks. Allows models...", "l": "o", "k": ["onnx", "open", "neural", "network", "exchange", "format", "representing", "models", "enabling", "interoperability", "different", "frameworks", "allows", "trained", "pytorch"]}, {"id": "term-onnx-runtime", "t": "ONNX Runtime", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Microsoft's cross-platform inference engine that executes models in the Open Neural Network Exchange format with...", "l": "o", "k": ["onnx", "runtime", "microsoft", "cross-platform", "inference", "engine", "executes", "models", "open", "neural", "network", "exchange", "format", "hardware-specific", "optimizations"]}, {"id": "term-ontology-in-ai", "t": "Ontology in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "A formal representation of knowledge as a set of concepts within a domain and the relationships between those concepts....", "l": "o", "k": ["ontology", "formal", "representation", "knowledge", "concepts", "within", "domain", "relationships", "ontologies", "cyc", "gene", "provide", "structured", "vocabularies", "enable"]}, {"id": "term-ontonotes", "t": "OntoNotes", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A large multilingual corpus annotated with structural information including syntax predicate-argument structure word...", "l": "o", "k": ["ontonotes", "large", "multilingual", "corpus", "annotated", "structural", "information", "including", "syntax", "predicate-argument", "structure", "word", "sense", "coreference", "named"]}, {"id": "term-open-images", "t": "Open Images", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A Google dataset of approximately 9 million images annotated with image-level labels bounding boxes visual...", "l": "o", "k": ["open", "images", "google", "dataset", "approximately", "million", "annotated", "image-level", "labels", "bounding", "boxes", "visual", "relationships", "segmentation", "masks"]}, {"id": "term-open-images-v7", "t": "Open Images V7", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "The seventh version of the Open Images dataset adding point-level annotations and expanded visual relationship...", "l": "o", "k": ["open", "images", "seventh", "version", "dataset", "adding", "point-level", "annotations", "expanded", "visual", "relationship", "existing", "million", "hierarchical", "labels"]}, {"id": "term-open-llm-leaderboard", "t": "Open LLM Leaderboard", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A public leaderboard by Hugging Face tracking performance of open language models across standardized benchmarks....", "l": "o", "k": ["open", "llm", "leaderboard", "public", "hugging", "face", "tracking", "performance", "language", "models", "across", "standardized", "benchmarks", "provides", "transparent"]}, {"id": "term-open-source", "t": "Open Source / Open Weight", "tg": ["Licensing", "Access"], "d": "general", "x": "AI models with publicly available weights that can be downloaded and run locally. Ranges from fully open (Llama) to...", "l": "o", "k": ["open", "source", "weight", "models", "publicly", "available", "weights", "downloaded", "run", "locally", "ranges", "fully", "llama", "restricted", "licenses"]}, {"id": "term-open-source-ai-movement", "t": "Open Source AI Movement", "tg": ["History", "Governance"], "d": "history", "x": "The growing trend of releasing AI model weights and training code publicly, enabling broader research and development...", "l": "o", "k": ["open", "source", "movement", "growing", "trend", "releasing", "model", "weights", "training", "code", "publicly", "enabling", "broader", "research", "development"]}, {"id": "term-open-x-embodiment", "t": "Open X-Embodiment", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "A collaborative dataset and model effort combining robot demonstration data from 22 robot types across 21 institutions...", "l": "o", "k": ["open", "x-embodiment", "collaborative", "dataset", "model", "effort", "combining", "robot", "demonstration", "data", "types", "across", "institutions", "training", "generalist"]}, {"id": "term-open-domain-qa", "t": "Open-Domain Question Answering", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A QA setting where the model must answer questions using a large corpus or parametric knowledge without being given a...", "l": "o", "k": ["open-domain", "question", "answering", "setting", "model", "must", "answer", "questions", "large", "corpus", "parametric", "knowledge", "without", "given", "specific"]}, {"id": "term-open-source-ai-safety", "t": "Open-Source AI Safety", "tg": ["Safety", "Policy"], "d": "safety", "x": "Safety considerations specific to AI models and tools released as open source. Includes the tension between...", "l": "o", "k": ["open-source", "safety", "considerations", "specific", "models", "tools", "released", "open", "source", "includes", "tension", "democratizing", "access", "beneficial", "uses"]}, {"id": "term-open-vocabulary-detection", "t": "Open-Vocabulary Detection", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Object detection approaches that can identify and localize objects from categories not seen during training, leveraging...", "l": "o", "k": ["open-vocabulary", "detection", "object", "approaches", "identify", "localize", "objects", "categories", "seen", "training", "leveraging", "vision-language", "models", "generalize", "beyond"]}, {"id": "term-openai", "t": "OpenAI", "tg": ["Company", "LLM Provider"], "d": "models", "x": "The AI research company behind GPT models, ChatGPT, and DALL-E. Founded in 2015, it has been central to the development...", "l": "o", "k": ["openai", "research", "company", "behind", "gpt", "models", "chatgpt", "dall-e", "founded", "central", "development", "popularization", "modern", "assistants"]}, {"id": "term-openai-founded", "t": "OpenAI Founded", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of OpenAI as a nonprofit AI research laboratory in December 2015 by Sam Altman Elon Musk and others with...", "l": "o", "k": ["openai", "founded", "founding", "nonprofit", "research", "laboratory", "december", "sam", "altman", "elon", "musk", "others", "mission", "ensuring", "artificial"]}, {"id": "term-openai-founding", "t": "OpenAI Founding", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of OpenAI in December 2015 as a non-profit AI research laboratory by Sam Altman, Elon Musk, and others,...", "l": "o", "k": ["openai", "founding", "december", "non-profit", "research", "laboratory", "sam", "altman", "elon", "musk", "others", "mission", "ensuring", "artificial", "general"]}, {"id": "term-openai-gym", "t": "OpenAI Gym", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "A toolkit providing a standardized interface for reinforcement learning environments. Includes classic control problems...", "l": "o", "k": ["openai", "gym", "toolkit", "providing", "standardized", "interface", "reinforcement", "learning", "environments", "includes", "classic", "control", "problems", "atari", "games"]}, {"id": "term-openai-capped-profit", "t": "OpenAI Transition to Capped Profit", "tg": ["History", "Milestones"], "d": "history", "x": "OpenAI's 2019 restructuring from a non-profit to a capped-profit company to attract the capital needed for large-scale...", "l": "o", "k": ["openai", "transition", "capped", "profit", "restructuring", "non-profit", "capped-profit", "company", "attract", "capital", "needed", "large-scale", "research", "creating", "hybrid"]}, {"id": "term-openassistant-conversations", "t": "OpenAssistant Conversations", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A human-generated conversational dataset created by thousands of volunteers for training open-source AI assistants....", "l": "o", "k": ["openassistant", "conversations", "human-generated", "conversational", "dataset", "created", "thousands", "volunteers", "training", "open-source", "assistants", "contains", "conversation", "trees", "quality"]}, {"id": "term-openbiollm", "t": "OpenBioLLM", "tg": ["Models", "Technical", "NLP", "Medical"], "d": "models", "x": "An open-source biomedical language model fine-tuned on high-quality medical instruction data for clinical question...", "l": "o", "k": ["openbiollm", "open-source", "biomedical", "language", "model", "fine-tuned", "high-quality", "medical", "instruction", "data", "clinical", "question", "answering", "text", "generation"]}, {"id": "term-openbookqa", "t": "OpenBookQA", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A question answering dataset modeled after open book exams requiring combination of elementary science knowledge with...", "l": "o", "k": ["openbookqa", "question", "answering", "dataset", "modeled", "open", "book", "exams", "requiring", "combination", "elementary", "science", "knowledge", "commonsense", "reasoning"]}, {"id": "term-openchat", "t": "OpenChat", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An open-source language model that uses Conditioned Reinforcement Learning Fine-Tuning (C-RLFT) to learn from...", "l": "o", "k": ["openchat", "open-source", "language", "model", "uses", "conditioned", "reinforcement", "learning", "fine-tuning", "c-rlft", "learn", "mixed-quality", "training", "data"]}, {"id": "term-openchat-35", "t": "OpenChat 3.5", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An open-source language model using C-RLFT that achieves GPT-3.5-level performance through mixed-quality data training...", "l": "o", "k": ["openchat", "open-source", "language", "model", "c-rlft", "achieves", "gpt-3", "5-level", "performance", "mixed-quality", "data", "training", "without", "human", "preference"]}, {"id": "term-opencl", "t": "OpenCL", "tg": ["Programming", "Standard", "Open Source"], "d": "hardware", "x": "Open Computing Language an open standard for parallel programming across CPUs GPUs and other accelerators. Provides...", "l": "o", "k": ["opencl", "open", "computing", "language", "standard", "parallel", "programming", "across", "cpus", "gpus", "accelerators", "provides", "vendor-neutral", "gpu", "compute"]}, {"id": "term-opendialkg", "t": "OpenDialKG", "tg": ["Benchmark", "NLP", "Dialogue", "Knowledge"], "d": "datasets", "x": "Open Dialogue Knowledge Graph a dataset linking dialogues to knowledge graph paths for knowledge-grounded conversation....", "l": "o", "k": ["opendialkg", "open", "dialogue", "knowledge", "graph", "dataset", "linking", "dialogues", "paths", "knowledge-grounded", "conversation", "tests", "ability", "structured"]}, {"id": "term-openfold", "t": "OpenFold", "tg": ["Models", "Scientific"], "d": "models", "x": "An open-source and trainable reimplementation of AlphaFold 2 that enables the research community to study and extend...", "l": "o", "k": ["openfold", "open-source", "trainable", "reimplementation", "alphafold", "enables", "research", "community", "study", "extend", "protein", "structure", "prediction", "methods"]}, {"id": "term-openhermes", "t": "OpenHermes", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A large collection of synthetic instruction-following data combining multiple open-source instruction datasets. Used...", "l": "o", "k": ["openhermes", "large", "collection", "synthetic", "instruction-following", "data", "combining", "multiple", "open-source", "instruction", "datasets", "training", "capable", "language", "models"]}, {"id": "term-openhermes-25", "t": "OpenHermes 2.5", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An updated open-source instruction model trained on a large curated dataset of over one million synthetic examples for...", "l": "o", "k": ["openhermes", "updated", "open-source", "instruction", "model", "trained", "large", "curated", "dataset", "million", "synthetic", "examples", "broad", "task", "coverage"]}, {"id": "term-openml", "t": "OpenML", "tg": ["Platform", "Evaluation"], "d": "datasets", "x": "An open platform for sharing machine learning datasets experiments and workflows. Provides standardized dataset formats...", "l": "o", "k": ["openml", "open", "platform", "sharing", "machine", "learning", "datasets", "experiments", "workflows", "provides", "standardized", "dataset", "formats", "automated", "benchmarking"]}, {"id": "term-openpose", "t": "OpenPose", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A real-time multi-person pose estimation system that uses part affinity fields and confidence maps to detect body,...", "l": "o", "k": ["openpose", "real-time", "multi-person", "pose", "estimation", "system", "uses", "part", "affinity", "fields", "confidence", "maps", "detect", "body", "hand"]}, {"id": "term-openvla", "t": "OpenVLA", "tg": ["Models", "Technical", "Robotics", "Vision", "NLP"], "d": "models", "x": "An open-source vision-language-action model for robotic control that converts visual observations and language...", "l": "o", "k": ["openvla", "open-source", "vision-language-action", "model", "robotic", "control", "converts", "visual", "observations", "language", "instructions", "robot", "actions"]}, {"id": "term-openwebtext", "t": "OpenWebText", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "An open-source recreation of the WebText corpus used to train GPT-2. Collected by extracting URLs from Reddit...", "l": "o", "k": ["openwebtext", "open-source", "recreation", "webtext", "corpus", "train", "gpt-2", "collected", "extracting", "urls", "reddit", "submissions", "least", "upvotes", "scraping"]}, {"id": "term-openwebtext2", "t": "OpenWebText2", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "An expanded version of OpenWebText containing more than 65 billion tokens from web pages. Created as part of The Pile...", "l": "o", "k": ["openwebtext2", "expanded", "version", "openwebtext", "containing", "billion", "tokens", "web", "pages", "created", "part", "pile", "provide", "larger", "open-source"]}, {"id": "term-operationalizing-ethics", "t": "Operationalizing Ethics", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The process of translating abstract ethical principles into concrete actionable requirements for AI system design...", "l": "o", "k": ["operationalizing", "ethics", "process", "translating", "abstract", "ethical", "principles", "concrete", "actionable", "requirements", "system", "design", "development", "deployment", "bridges"]}, {"id": "term-operator-fusion", "t": "Operator Fusion", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "A compiler optimization that combines multiple sequential neural network operations into a single kernel launch,...", "l": "o", "k": ["operator", "fusion", "compiler", "optimization", "combines", "multiple", "sequential", "neural", "network", "operations", "single", "kernel", "launch", "reducing", "memory"]}, {"id": "term-opponent-modeling", "t": "Opponent Modeling", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "The practice of explicitly modeling the behavior, goals, or strategy of other agents in a multi-agent environment....", "l": "o", "k": ["opponent", "modeling", "practice", "explicitly", "behavior", "goals", "strategy", "agents", "multi-agent", "environment", "models", "enable", "effective", "adaptation", "strategic"]}, {"id": "term-ops5", "t": "OPS5", "tg": ["History", "Systems"], "d": "history", "x": "A rule-based programming language developed at Carnegie Mellon University in the late 1970s. OPS5 was used to implement...", "l": "o", "k": ["ops5", "rule-based", "programming", "language", "developed", "carnegie", "mellon", "university", "late", "1970s", "implement", "xcon", "expert", "system", "became"]}, {"id": "term-opt", "t": "OPT", "tg": ["Models", "Technical"], "d": "models", "x": "Open Pre-trained Transformer is a family of decoder-only language models released by Meta AI ranging from 125M to 175B...", "l": "o", "k": ["opt", "open", "pre-trained", "transformer", "family", "decoder-only", "language", "models", "released", "meta", "ranging", "125m", "175b", "parameters", "full"]}, {"id": "term-optical-character-recognition", "t": "Optical Character Recognition", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The technology that converts images of text (handwritten, printed, or typed) into machine-readable text, using...", "l": "o", "k": ["optical", "character", "recognition", "technology", "converts", "images", "text", "handwritten", "printed", "typed", "machine-readable", "detection", "localize", "regions", "identify"]}, {"id": "term-optical-character-recognition-algorithm", "t": "Optical Character Recognition Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A pipeline that converts images of text into machine-readable characters using segmentation and feature extraction and...", "l": "o", "k": ["optical", "character", "recognition", "algorithm", "pipeline", "converts", "images", "text", "machine-readable", "characters", "segmentation", "feature", "extraction", "classification", "modern"]}, {"id": "term-optical-computing-for-ai", "t": "Optical Computing for AI", "tg": ["Emerging", "Architecture", "Photonic"], "d": "hardware", "x": "Computing approach using photons instead of electrons for computation. Photonic processors can perform matrix...", "l": "o", "k": ["optical", "computing", "approach", "photons", "instead", "electrons", "computation", "photonic", "processors", "perform", "matrix", "multiplication", "speed", "light", "low"]}, {"id": "term-optical-flow", "t": "Optical Flow", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The estimation of per-pixel motion vectors between consecutive video frames, representing the apparent movement of...", "l": "o", "k": ["optical", "flow", "estimation", "per-pixel", "motion", "vectors", "consecutive", "video", "frames", "representing", "apparent", "movement", "objects", "camera", "analysis"]}, {"id": "term-optical-flow-estimation", "t": "Optical Flow Estimation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The computational process of predicting dense pixel-level displacement fields between consecutive video frames using...", "l": "o", "k": ["optical", "flow", "estimation", "computational", "process", "predicting", "dense", "pixel-level", "displacement", "fields", "consecutive", "video", "frames", "deep", "learning"]}, {"id": "term-optical-interconnect", "t": "Optical Interconnect", "tg": ["Networking", "Interconnect"], "d": "hardware", "x": "Data communication link using light through fiber optic cables or waveguides. Provides higher bandwidth over longer...", "l": "o", "k": ["optical", "interconnect", "data", "communication", "link", "light", "fiber", "optic", "cables", "waveguides", "provides", "higher", "bandwidth", "longer", "distances"]}, {"id": "term-optical-switch", "t": "Optical Switch", "tg": ["Networking", "Photonic", "Hardware"], "d": "hardware", "x": "Network switch using optical signals to route data without converting to electrical signals. Promises lower latency and...", "l": "o", "k": ["optical", "switch", "network", "signals", "route", "data", "without", "converting", "electrical", "promises", "lower", "latency", "power", "consumption", "high-bandwidth"]}, {"id": "term-optics", "t": "OPTICS", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Ordering Points To Identify the Clustering Structure is a density-based clustering algorithm that creates an ordering...", "l": "o", "k": ["optics", "ordering", "points", "identify", "clustering", "structure", "density-based", "algorithm", "creates", "reflecting", "density", "connectivity", "unlike", "dbscan", "clusters"]}, {"id": "term-optimization", "t": "Optimization", "tg": ["Training", "Process"], "d": "general", "x": "The process of adjusting model parameters to minimize loss. Includes algorithms (Adam, SGD), learning rate schedules,...", "l": "o", "k": ["optimization", "process", "adjusting", "model", "parameters", "minimize", "loss", "includes", "algorithms", "adam", "sgd", "learning", "rate", "schedules", "techniques"]}, {"id": "term-option-framework", "t": "Option Framework", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A formalism for temporal abstraction in RL where options are temporally extended actions consisting of an initiation...", "l": "o", "k": ["option", "framework", "formalism", "temporal", "abstraction", "options", "temporally", "extended", "actions", "consisting", "initiation", "internal", "policy", "termination", "condition"]}, {"id": "term-options-framework", "t": "Options Framework", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A hierarchical reinforcement learning framework that extends primitive actions with temporally extended actions called...", "l": "o", "k": ["options", "framework", "hierarchical", "reinforcement", "learning", "extends", "primitive", "actions", "temporally", "extended", "called", "option", "initiation", "policy", "termination"]}, {"id": "term-opus", "t": "OPUS", "tg": ["Training Corpus", "NLP", "Translation"], "d": "datasets", "x": "An open collection of parallel corpora covering hundreds of language pairs from various sources including subtitles...", "l": "o", "k": ["opus", "open", "collection", "parallel", "corpora", "covering", "hundreds", "language", "pairs", "various", "sources", "including", "subtitles", "legislation", "web"]}, {"id": "term-oracle", "t": "Oracle (ML)", "tg": ["Concept", "Evaluation"], "d": "datasets", "x": "A theoretical perfect model or information source used as a benchmark. In evaluation, comparing to an oracle helps...", "l": "o", "k": ["oracle", "theoretical", "perfect", "model", "information", "source", "benchmark", "evaluation", "comparing", "helps", "understand", "ceiling", "achievable", "performance"]}, {"id": "term-orb-algorithm", "t": "ORB Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "Oriented FAST and Rotated BRIEF is a feature detection and description algorithm that combines the FAST keypoint...", "l": "o", "k": ["orb", "algorithm", "oriented", "fast", "rotated", "brief", "feature", "detection", "description", "combines", "keypoint", "detector", "modified", "descriptor", "patent-free"]}, {"id": "term-orca", "t": "Orca", "tg": ["Models", "Technical"], "d": "models", "x": "A language model by Microsoft that learns from rich signals including explanation traces from GPT-4. Demonstrates that...", "l": "o", "k": ["orca", "language", "model", "microsoft", "learns", "rich", "signals", "including", "explanation", "traces", "gpt-4", "demonstrates", "smaller", "models", "achieve"]}, {"id": "term-orca-math", "t": "Orca Math", "tg": ["Training Corpus", "NLP", "Reasoning"], "d": "datasets", "x": "A dataset of 200000 math word problems with step-by-step solutions generated for training mathematical reasoning in...", "l": "o", "k": ["orca", "math", "dataset", "word", "problems", "step-by-step", "solutions", "generated", "training", "mathematical", "reasoning", "smaller", "language", "models"]}, {"id": "term-order-statistic-tree", "t": "Order-Statistic Tree", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A balanced binary search tree augmented with subtree size information that supports efficient rank and select queries....", "l": "o", "k": ["order-statistic", "tree", "balanced", "binary", "search", "augmented", "subtree", "size", "information", "supports", "efficient", "rank", "select", "queries", "finds"]}, {"id": "term-ordinal-regression", "t": "Ordinal Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A type of regression analysis for predicting ordinal (ordered categorical) outcomes. It models the cumulative...", "l": "o", "k": ["ordinal", "regression", "type", "analysis", "predicting", "ordered", "categorical", "outcomes", "models", "cumulative", "probabilities", "categories", "link", "function", "threshold"]}, {"id": "term-oriol-vinyals", "t": "Oriol Vinyals", "tg": ["History", "Pioneers"], "d": "history", "x": "Spanish computer scientist at DeepMind known for sequence-to-sequence learning pointer networks and leading the...", "l": "o", "k": ["oriol", "vinyals", "spanish", "computer", "scientist", "deepmind", "known", "sequence-to-sequence", "learning", "pointer", "networks", "leading", "alphastar", "project", "achieved"]}, {"id": "term-orion-14b", "t": "Orion-14B", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A 14 billion parameter multilingual language model with strong performance across English and Chinese and Japanese and...", "l": "o", "k": ["orion-14b", "billion", "parameter", "multilingual", "language", "model", "strong", "performance", "across", "english", "chinese", "japanese", "korean", "tasks"]}, {"id": "term-orpo", "t": "ORPO", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Odds Ratio Preference Optimization, an alignment method that combines supervised fine-tuning and preference alignment...", "l": "o", "k": ["orpo", "odds", "ratio", "preference", "optimization", "alignment", "method", "combines", "supervised", "fine-tuning", "single", "training", "stage", "ratios", "distinguish"]}, {"id": "term-orthogonal-initialization", "t": "Orthogonal Initialization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A weight initialization technique that initializes weight matrices as random orthogonal matrices. Helps preserve...", "l": "o", "k": ["orthogonal", "initialization", "weight", "technique", "initializes", "matrices", "random", "helps", "preserve", "gradient", "norms", "forward", "backward", "propagation", "particularly"]}, {"id": "term-orthogonality-thesis", "t": "Orthogonality Thesis", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The philosophical claim that intelligence and goals are orthogonal, meaning that any level of intelligence can in...", "l": "o", "k": ["orthogonality", "thesis", "philosophical", "claim", "intelligence", "goals", "orthogonal", "meaning", "level", "principle", "combined", "terminal", "goal", "implying", "superintelligent"]}, {"id": "term-oscar", "t": "OSCAR", "tg": ["Training Corpus", "NLP", "Multilingual"], "d": "datasets", "x": "Open Super-large Crawled Aggregated Corpus a multilingual web corpus derived from Common Crawl covering 166 languages....", "l": "o", "k": ["oscar", "open", "super-large", "crawled", "aggregated", "corpus", "multilingual", "web", "derived", "common", "crawl", "covering", "languages", "provides", "large-scale"]}, {"id": "term-osworld", "t": "OSWorld", "tg": ["Benchmark", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating AI agents on real-world computer tasks across operating systems. Tests the ability to...", "l": "o", "k": ["osworld", "benchmark", "evaluating", "agents", "real-world", "computer", "tasks", "across", "operating", "systems", "tests", "ability", "complete", "desktop", "computing"]}, {"id": "term-otsus-method", "t": "Otsu's Method", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "An automatic thresholding algorithm that finds the optimal threshold to separate an image into foreground and...", "l": "o", "k": ["otsu", "method", "automatic", "thresholding", "algorithm", "finds", "optimal", "threshold", "separate", "image", "foreground", "background", "maximizing", "between-class", "variance"]}, {"id": "term-otter", "t": "Otter", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multimodal model built on Flamingo architecture that supports multi-modal in-context learning through instruction...", "l": "o", "k": ["otter", "multimodal", "model", "built", "flamingo", "architecture", "supports", "multi-modal", "in-context", "learning", "instruction", "tuning", "diverse", "datasets"]}, {"id": "term-out-of-bag-error", "t": "Out-of-Bag Error", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "An estimate of prediction error for bagged models computed using observations not included in the bootstrap sample for...", "l": "o", "k": ["out-of-bag", "error", "estimate", "prediction", "bagged", "models", "computed", "observations", "included", "bootstrap", "sample", "base", "learner", "provides", "unbiased"]}, {"id": "term-out-of-distribution", "t": "Out-of-Distribution (OOD)", "tg": ["Challenge", "Robustness"], "d": "general", "x": "Data that differs significantly from what a model was trained on. Models often perform poorly on OOD data, making...", "l": "o", "k": ["out-of-distribution", "ood", "data", "differs", "significantly", "model", "trained", "models", "perform", "poorly", "making", "detection", "important", "reliable", "deployment"]}, {"id": "term-out-of-order-execution", "t": "Out-of-Order Execution", "tg": ["Architecture", "Processor", "Optimization"], "d": "hardware", "x": "Processor technique that executes instructions in an order determined by data dependencies rather than program order....", "l": "o", "k": ["out-of-order", "execution", "processor", "technique", "executes", "instructions", "order", "determined", "data", "dependencies", "rather", "program", "improves", "performance", "keeping"]}, {"id": "term-out-of-vocabulary", "t": "Out-of-Vocabulary", "tg": ["NLP", "Tokenization"], "d": "general", "x": "Words or tokens encountered during inference that were not present in the model's training vocabulary, requiring...", "l": "o", "k": ["out-of-vocabulary", "words", "tokens", "encountered", "inference", "were", "present", "model", "training", "vocabulary", "requiring", "special", "handling", "subword", "tokenization"]}, {"id": "term-outcome-fairness", "t": "Outcome Fairness", "tg": ["Safety", "Technical"], "d": "safety", "x": "A fairness assessment that evaluates whether the real-world outcomes of AI system decisions are equitable across...", "l": "o", "k": ["outcome", "fairness", "assessment", "evaluates", "real-world", "outcomes", "system", "decisions", "equitable", "across", "different", "demographic", "groups", "rather", "predictions"]}, {"id": "term-outcome-reward-model", "t": "Outcome Reward Model", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A reward model that evaluates only the final output of a generation, providing a holistic quality score used in RLHF to...", "l": "o", "k": ["outcome", "reward", "model", "evaluates", "final", "output", "generation", "providing", "holistic", "quality", "score", "rlhf", "train", "policy", "toward"]}, {"id": "term-outer-alignment", "t": "Outer Alignment", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The problem of ensuring that the base objective or loss function used during training accurately captures the intended...", "l": "o", "k": ["outer", "alignment", "problem", "ensuring", "base", "objective", "loss", "function", "training", "accurately", "captures", "intended", "goal", "system", "designer"]}, {"id": "term-outlier-detection", "t": "Outlier Detection", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "The process of identifying data points that differ significantly from the majority of observations. Methods include...", "l": "o", "k": ["outlier", "detection", "process", "identifying", "data", "points", "differ", "significantly", "majority", "observations", "methods", "include", "statistical", "tests", "z-score"]}, {"id": "term-outpainting", "t": "Outpainting", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "The task of extending an image beyond its original boundaries by generating new coherent content that seamlessly...", "l": "o", "k": ["outpainting", "task", "extending", "image", "beyond", "original", "boundaries", "generating", "coherent", "content", "seamlessly", "continues", "existing", "visual", "context"]}, {"id": "term-output-layer", "t": "Output Layer", "tg": ["Architecture", "Neural Networks"], "d": "models", "x": "The final layer of a neural network that produces the model's predictions. Its design depends on the task: softmax for...", "l": "o", "k": ["output", "layer", "final", "neural", "network", "produces", "model", "predictions", "design", "depends", "task", "softmax", "classification", "linear", "regression"]}, {"id": "term-overestimation-bias", "t": "Overestimation Bias", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A systematic tendency of Q-learning and related algorithms to overestimate action values due to the max operator in the...", "l": "o", "k": ["overestimation", "bias", "systematic", "tendency", "q-learning", "related", "algorithms", "overestimate", "action", "values", "due", "max", "operator", "bellman", "optimality"]}, {"id": "term-overfitting", "t": "Overfitting", "tg": ["Problem", "Training"], "d": "general", "x": "When a model performs well on training data but poorly on new data, having memorized specific examples rather than...", "l": "o", "k": ["overfitting", "model", "performs", "training", "data", "poorly", "having", "memorized", "specific", "examples", "rather", "learning", "general", "patterns", "addressed"]}, {"id": "term-oversight-mechanism", "t": "Oversight Mechanism", "tg": ["Safety", "Governance"], "d": "safety", "x": "Any process tool or institution designed to monitor and control AI system behavior. Includes technical monitoring human...", "l": "o", "k": ["oversight", "mechanism", "process", "tool", "institution", "designed", "monitor", "control", "system", "behavior", "includes", "technical", "monitoring", "human", "review"]}, {"id": "term-owl-vit", "t": "OWL-ViT", "tg": ["Models", "Technical"], "d": "models", "x": "Open-World Localization with Vision Transformer is a zero-shot object detection model that localizes objects from text...", "l": "o", "k": ["owl-vit", "open-world", "localization", "vision", "transformer", "zero-shot", "object", "detection", "model", "localizes", "objects", "text", "queries", "uses", "clip-style"]}, {"id": "term-ownership-of-ai-generated-content", "t": "Ownership of AI-Generated Content", "tg": ["Safety", "Policy"], "d": "safety", "x": "Legal and ethical questions about who owns content created by AI systems. Involves debates about copyright intellectual...", "l": "o", "k": ["ownership", "ai-generated", "content", "legal", "ethical", "questions", "owns", "created", "systems", "involves", "debates", "copyright", "intellectual", "property", "rights"]}, {"id": "term-oxford-flowers-102", "t": "Oxford Flowers 102", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A dataset of 8189 images across 102 flower categories commonly found in the United Kingdom. Each category has between...", "l": "o", "k": ["oxford", "flowers", "dataset", "images", "across", "flower", "categories", "commonly", "found", "united", "kingdom", "category", "significant", "variation", "scale"]}, {"id": "term-oxford-iiit-pet-dataset", "t": "Oxford-IIIT Pet Dataset", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A dataset containing 7349 images of 37 pet breeds with 200 images per breed. Includes pixel-level segmentation masks...", "l": "o", "k": ["oxford-iiit", "pet", "dataset", "containing", "images", "breeds", "per", "breed", "includes", "pixel-level", "segmentation", "masks", "classification", "labels", "fine-grained"]}, {"id": "term-p-value", "t": "P-Value", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The probability of observing a test statistic at least as extreme as the one computed from the data, assuming the null...", "l": "p", "k": ["p-value", "probability", "observing", "test", "statistic", "least", "extreme", "computed", "data", "assuming", "null", "hypothesis", "true", "smaller", "p-values"]}, {"id": "term-p3", "t": "P3", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "The Public Pool of Prompts a collection of prompted templates for over 170 NLP datasets. Used to train T0 demonstrating...", "l": "p", "k": ["public", "pool", "prompts", "collection", "prompted", "templates", "nlp", "datasets", "train", "demonstrating", "effectiveness", "multitask", "training"]}, {"id": "term-pac-learning", "t": "PAC Learning", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "Probably Approximately Correct learning, a theoretical framework that defines the conditions under which a learning...", "l": "p", "k": ["pac", "learning", "probably", "approximately", "correct", "theoretical", "framework", "defines", "conditions", "algorithm", "high", "probability", "produce", "hypothesis", "given"]}, {"id": "term-padchest", "t": "PadChest", "tg": ["Benchmark", "Medical", "Computer Vision"], "d": "datasets", "x": "A large multilabeled chest X-ray dataset from Spain containing over 160000 images with 174 radiographic findings....", "l": "p", "k": ["padchest", "large", "multilabeled", "chest", "x-ray", "dataset", "spain", "containing", "images", "radiographic", "findings", "provides", "extensively", "labeled", "medical"]}, {"id": "term-padding", "t": "Padding", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The addition of extra values (typically zeros) around the borders of an input image or feature map before convolution,...", "l": "p", "k": ["padding", "addition", "extra", "values", "typically", "zeros", "around", "borders", "input", "image", "feature", "map", "convolution", "controlling", "spatial"]}, {"id": "term-paddleocr-pp-ocrv4", "t": "PaddleOCR PP-OCRv4", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "The fourth generation of the PaddleOCR text recognition system from Baidu with improved detection and recognition...", "l": "p", "k": ["paddleocr", "pp-ocrv4", "fourth", "generation", "text", "recognition", "system", "baidu", "improved", "detection", "accuracy", "diverse", "document", "formats"]}, {"id": "term-pade-approximation", "t": "Pade Approximation", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A rational function approximation technique that matches the Taylor series of a function to a specified order. Often...", "l": "p", "k": ["pade", "approximation", "rational", "function", "technique", "matches", "taylor", "series", "specified", "order", "provides", "better", "approximations", "truncated", "power"]}, {"id": "term-paged-attention", "t": "Paged Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A memory management technique for KV caches during LLM serving that stores attention keys and values in non-contiguous...", "l": "p", "k": ["paged", "attention", "memory", "management", "technique", "caches", "llm", "serving", "stores", "keys", "values", "non-contiguous", "pages", "reducing", "waste"]}, {"id": "term-pagedattention", "t": "PagedAttention", "tg": ["Inference", "Memory", "Optimization"], "d": "hardware", "x": "Memory management technique for KV caches that allocates memory in fixed-size pages rather than contiguous blocks....", "l": "p", "k": ["pagedattention", "memory", "management", "technique", "caches", "allocates", "fixed-size", "pages", "rather", "contiguous", "blocks", "eliminates", "waste", "fragmentation", "language"]}, {"id": "term-pagerank-algorithm", "t": "PageRank Algorithm", "tg": ["Algorithms", "Fundamentals", "Graph", "History"], "d": "algorithms", "x": "An algorithm developed by Larry Page and Sergey Brin that ranks web pages by analyzing the link structure of the web....", "l": "p", "k": ["pagerank", "algorithm", "developed", "larry", "page", "sergey", "brin", "ranks", "web", "pages", "analyzing", "link", "structure", "models", "random"]}, {"id": "term-painn", "t": "PaiNN", "tg": ["Models", "Scientific"], "d": "models", "x": "Polarizable Atom Interaction Neural Network is an equivariant message passing model for molecular simulations that uses...", "l": "p", "k": ["painn", "polarizable", "atom", "interaction", "neural", "network", "equivariant", "message", "passing", "model", "molecular", "simulations", "uses", "vector-valued", "features"]}, {"id": "term-pairing-heap-algorithm", "t": "Pairing Heap Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A self-adjusting heap data structure with simple implementation and excellent practical performance. Supports insert in...", "l": "p", "k": ["pairing", "heap", "algorithm", "self-adjusting", "data", "structure", "simple", "implementation", "excellent", "practical", "performance", "supports", "insert", "decrease-key", "log"]}, {"id": "term-pali", "t": "PaLI", "tg": ["Models", "Technical"], "d": "models", "x": "Pathways Language and Image model jointly scales a vision transformer and language model. Trained on WebLI a...", "l": "p", "k": ["pali", "pathways", "language", "image", "model", "jointly", "scales", "vision", "transformer", "trained", "webli", "multilingual", "image-text", "dataset", "achieves"]}, {"id": "term-pali-3", "t": "PaLI-3", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A third-generation vision-language model from Google that uses a SigLIP visual encoder with a language model for strong...", "l": "p", "k": ["pali-3", "third-generation", "vision-language", "model", "google", "uses", "siglip", "visual", "encoder", "language", "strong", "multimodal", "understanding", "smaller", "scales"]}, {"id": "term-pali-x", "t": "PaLI-X", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A scaled-up version of the PaLI vision-language model with 55 billion parameters that achieves strong performance...", "l": "p", "k": ["pali-x", "scaled-up", "version", "pali", "vision-language", "model", "billion", "parameters", "achieves", "strong", "performance", "across", "visual", "understanding", "benchmarks"]}, {"id": "term-palm", "t": "PaLM", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Pathways Language Model, a 540-billion parameter dense transformer by Google that demonstrated breakthrough performance...", "l": "p", "k": ["palm", "pathways", "language", "model", "540-billion", "parameter", "dense", "transformer", "google", "demonstrated", "breakthrough", "performance", "reasoning", "tasks", "system"]}, {"id": "term-palm-2", "t": "PaLM 2", "tg": ["Models", "Technical"], "d": "models", "x": "Google's second generation language model with improved multilingual reasoning and coding capabilities compared to...", "l": "p", "k": ["palm", "google", "generation", "language", "model", "improved", "multilingual", "reasoning", "coding", "capabilities", "compared", "uses", "compute-optimal", "approach", "better"]}, {"id": "term-palm-e", "t": "PaLM-E", "tg": ["Models", "Technical"], "d": "models", "x": "An embodied multimodal language model that integrates real-world continuous sensor data with language. Can reason about...", "l": "p", "k": ["palm-e", "embodied", "multimodal", "language", "model", "integrates", "real-world", "continuous", "sensor", "data", "reason", "physical", "world", "plan", "robot"]}, {"id": "term-panda-70m", "t": "Panda-70M", "tg": ["Training Corpus", "Video", "Multimodal"], "d": "datasets", "x": "A large-scale video dataset of 70 million high-quality video clips with captions. Provides massive video-text data for...", "l": "p", "k": ["panda-70m", "large-scale", "video", "dataset", "million", "high-quality", "clips", "captions", "provides", "massive", "video-text", "data", "pretraining", "understanding", "generation"]}, {"id": "term-pandemonium-architecture", "t": "Pandemonium Architecture", "tg": ["History", "Systems"], "d": "history", "x": "The hierarchical pattern recognition system proposed by Oliver Selfridge in 1959 at the National Physical Laboratory....", "l": "p", "k": ["pandemonium", "architecture", "hierarchical", "pattern", "recognition", "system", "proposed", "oliver", "selfridge", "national", "physical", "laboratory", "multiple", "levels", "demons"]}, {"id": "term-pandemonium-model", "t": "Pandemonium Model", "tg": ["History", "Milestones"], "d": "history", "x": "A pattern recognition model proposed by Oliver Selfridge in 1959 using hierarchical layers of feature-detecting demons...", "l": "p", "k": ["pandemonium", "model", "pattern", "recognition", "proposed", "oliver", "selfridge", "hierarchical", "layers", "feature-detecting", "demons", "compete", "identify", "patterns", "anticipating"]}, {"id": "term-pangu-weather", "t": "Pangu-Weather", "tg": ["Models", "Scientific"], "d": "models", "x": "A 3D Earth-specific Transformer model from Huawei for medium-range weather forecasting that achieves competitive...", "l": "p", "k": ["pangu-weather", "earth-specific", "transformer", "model", "huawei", "medium-range", "weather", "forecasting", "achieves", "competitive", "accuracy", "traditional", "numerical", "prediction"]}, {"id": "term-panoptic-segmentation", "t": "Panoptic Segmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A unified image segmentation task that assigns both a class label and an instance ID to every pixel, combining semantic...", "l": "p", "k": ["panoptic", "segmentation", "unified", "image", "task", "assigns", "class", "label", "instance", "pixel", "combining", "semantic", "stuff", "sky", "road"]}, {"id": "term-paperclip-maximizer", "t": "Paperclip Maximizer", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "A thought experiment by Nick Bostrom illustrating the dangers of misaligned AI, in which an AI with the sole objective...", "l": "p", "k": ["paperclip", "maximizer", "thought", "experiment", "nick", "bostrom", "illustrating", "dangers", "misaligned", "sole", "objective", "maximizing", "production", "converts", "available"]}, {"id": "term-papers-with-code", "t": "Papers With Code", "tg": ["Platform", "Evaluation"], "d": "datasets", "x": "A platform linking machine learning papers with their code implementations and benchmark results. Tracks...", "l": "p", "k": ["papers", "code", "platform", "linking", "machine", "learning", "implementations", "benchmark", "results", "tracks", "state-of-the-art", "across", "thousands", "benchmarks", "tasks"]}, {"id": "term-paracrawl", "t": "ParaCrawl", "tg": ["Training Corpus", "NLP", "Translation"], "d": "datasets", "x": "A large web-crawled parallel corpus covering 42 language pairs with billions of sentence pairs. Used for training...", "l": "p", "k": ["paracrawl", "large", "web-crawled", "parallel", "corpus", "covering", "language", "pairs", "billions", "sentence", "training", "neural", "machine", "translation", "systems"]}, {"id": "term-parakeet", "t": "Parakeet", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "A family of automatic speech recognition models from NVIDIA optimized for English transcription with state-of-the-art...", "l": "p", "k": ["parakeet", "family", "automatic", "speech", "recognition", "models", "nvidia", "optimized", "english", "transcription", "state-of-the-art", "accuracy", "fast", "inference", "speed"]}, {"id": "term-parallel-distributed-processing", "t": "Parallel Distributed Processing", "tg": ["History", "Milestones"], "d": "history", "x": "A two-volume work published in 1986 by David Rumelhart James McClelland and the PDP Research Group. The books provided...", "l": "p", "k": ["parallel", "distributed", "processing", "two-volume", "work", "published", "david", "rumelhart", "james", "mcclelland", "pdp", "research", "group", "books", "provided"]}, {"id": "term-parallel-processing", "t": "Parallel Processing", "tg": ["Architecture", "Fundamentals", "Computing"], "d": "hardware", "x": "Simultaneous execution of multiple computations to solve problems faster. Encompasses data parallelism model...", "l": "p", "k": ["parallel", "processing", "simultaneous", "execution", "multiple", "computations", "solve", "problems", "faster", "encompasses", "data", "parallelism", "model", "pipeline", "strategies"]}, {"id": "term-parameter-server", "t": "Parameter Server", "tg": ["Networking", "Distributed Training", "Architecture"], "d": "hardware", "x": "Distributed architecture where dedicated server nodes store and synchronize model parameters while worker nodes compute...", "l": "p", "k": ["parameter", "server", "distributed", "architecture", "dedicated", "nodes", "store", "synchronize", "model", "parameters", "worker", "compute", "gradients", "alternative", "all-reduce"]}, {"id": "term-parameters", "t": "Parameters", "tg": ["Core Concept", "Dual Meaning"], "d": "general", "x": "In prompting: constraints and specifications that shape AI output. In models: the learned weights (billions in LLMs)...", "l": "p", "k": ["parameters", "prompting", "constraints", "specifications", "shape", "output", "models", "learned", "weights", "billions", "llms", "determine", "behavior", "parameter", "count"]}, {"id": "term-parametric-relu", "t": "Parametric ReLU", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An activation function that generalizes Leaky ReLU by making the negative slope a learnable parameter. Introduced by He...", "l": "p", "k": ["parametric", "relu", "activation", "function", "generalizes", "leaky", "making", "negative", "slope", "learnable", "parameter", "introduced", "part", "work", "deep"]}, {"id": "term-paraphrase-detection", "t": "Paraphrase Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of determining whether two text passages convey the same meaning using different words or structures,...", "l": "p", "k": ["paraphrase", "detection", "task", "determining", "text", "passages", "convey", "meaning", "different", "words", "structures", "requiring", "understanding", "semantic", "equivalence"]}, {"id": "term-parent-document-retrieval", "t": "Parent Document Retrieval", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A RAG strategy that indexes small child chunks for precise matching but returns their larger parent documents to the...", "l": "p", "k": ["parent", "document", "retrieval", "rag", "strategy", "indexes", "small", "child", "chunks", "precise", "matching", "returns", "larger", "documents", "llm"]}, {"id": "term-parent-child-chunking", "t": "Parent-Child Chunking", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "A hierarchical chunking strategy that creates small child chunks for precise embedding-based retrieval while linking...", "l": "p", "k": ["parent-child", "chunking", "hierarchical", "strategy", "creates", "small", "child", "chunks", "precise", "embedding-based", "retrieval", "linking", "larger", "parent", "provide"]}, {"id": "term-parler-tts", "t": "Parler-TTS", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "An open-source text-to-speech model that generates high-quality speech from natural language descriptions of the...", "l": "p", "k": ["parler-tts", "open-source", "text-to-speech", "model", "generates", "high-quality", "speech", "natural", "language", "descriptions", "desired", "voice", "characteristics"]}, {"id": "term-parse-tree", "t": "Parse Tree", "tg": ["NLP", "Parsing"], "d": "general", "x": "A hierarchical tree structure representing the syntactic structure of a sentence according to a formal grammar, with...", "l": "p", "k": ["parse", "tree", "hierarchical", "structure", "representing", "syntactic", "sentence", "according", "formal", "grammar", "internal", "nodes", "phrase", "categories", "leaves"]}, {"id": "term-pos-tagging", "t": "Part-of-Speech Tagging", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of assigning grammatical categories such as noun, verb, adjective, or adverb to each word in a sentence based...", "l": "p", "k": ["part-of-speech", "tagging", "task", "assigning", "grammatical", "categories", "noun", "verb", "adjective", "adverb", "word", "sentence", "based", "context", "morphological"]}, {"id": "term-parti", "t": "Parti", "tg": ["Models", "Technical"], "d": "models", "x": "Pathways Autoregressive Text-to-Image model by Google that treats image generation as a sequence-to-sequence problem...", "l": "p", "k": ["parti", "pathways", "autoregressive", "text-to-image", "model", "google", "treats", "image", "generation", "sequence-to-sequence", "problem", "transformer", "scales", "handles", "complex"]}, {"id": "term-partial-autocorrelation", "t": "Partial Autocorrelation", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "The correlation between a time series observation and a lagged observation after removing the effects of intermediate...", "l": "p", "k": ["partial", "autocorrelation", "correlation", "time", "series", "observation", "lagged", "removing", "effects", "intermediate", "lags", "helps", "determine", "order", "autoregressive"]}, {"id": "term-partial-dependence-plot", "t": "Partial Dependence Plot", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A visualization showing the marginal effect of one or two features on the predicted outcome of a model, averaging over...", "l": "p", "k": ["partial", "dependence", "plot", "visualization", "showing", "marginal", "effect", "features", "predicted", "outcome", "model", "averaging", "values", "reveals", "relationship"]}, {"id": "term-partial-least-squares", "t": "Partial Least Squares", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A regression method that simultaneously reduces the dimensionality of predictors and response variables by finding...", "l": "p", "k": ["partial", "least", "squares", "regression", "method", "simultaneously", "reduces", "dimensionality", "predictors", "response", "variables", "finding", "latent", "components", "maximize"]}, {"id": "term-partially-observable-mdp", "t": "Partially Observable MDP (POMDP)", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "An extension of the MDP framework where the agent cannot directly observe the full state and instead receives partial...", "l": "p", "k": ["partially", "observable", "mdp", "pomdp", "extension", "framework", "agent", "cannot", "directly", "observe", "full", "state", "instead", "receives", "partial"]}, {"id": "term-participatory-ai-design", "t": "Participatory AI Design", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "An approach to AI development that involves affected communities and stakeholders in the design, development, and...", "l": "p", "k": ["participatory", "design", "approach", "development", "involves", "affected", "communities", "stakeholders", "evaluation", "process", "ensuring", "diverse", "perspectives", "shape", "system"]}, {"id": "term-particle-filter-algorithm", "t": "Particle Filter Algorithm", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A sequential Monte Carlo method that approximates the posterior distribution of a system state using a set of weighted...", "l": "p", "k": ["particle", "filter", "algorithm", "sequential", "monte", "carlo", "method", "approximates", "posterior", "distribution", "system", "state", "weighted", "particles", "effective"]}, {"id": "term-particle-swarm-optimization", "t": "Particle Swarm Optimization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A population-based optimization algorithm inspired by the social behavior of bird flocking and fish schooling....", "l": "p", "k": ["particle", "swarm", "optimization", "population-based", "algorithm", "inspired", "social", "behavior", "bird", "flocking", "fish", "schooling", "particles", "move", "search"]}, {"id": "term-partiprompts", "t": "PartiPrompts", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A collection of 1600 text prompts for evaluating text-to-image generation models created by Google for the Parti model....", "l": "p", "k": ["partiprompts", "collection", "text", "prompts", "evaluating", "text-to-image", "generation", "models", "created", "google", "parti", "model", "covers", "diverse", "categories"]}, {"id": "term-partnership-on-ai", "t": "Partnership on AI", "tg": ["Governance", "AI Ethics"], "d": "safety", "x": "A multi-stakeholder organization founded in 2016 by major technology companies to study and formulate best practices on...", "l": "p", "k": ["partnership", "multi-stakeholder", "organization", "founded", "major", "technology", "companies", "study", "formulate", "best", "practices", "technologies", "advancing", "understanding", "impact"]}, {"id": "term-pascal-voc", "t": "Pascal VOC", "tg": ["History", "Milestones"], "d": "history", "x": "The PASCAL Visual Object Classes challenge and dataset running from 2005 to 2012. Pascal VOC provided standardized...", "l": "p", "k": ["pascal", "voc", "visual", "object", "classes", "challenge", "dataset", "running", "provided", "standardized", "evaluation", "detection", "image", "classification", "segmentation"]}, {"id": "term-pass-at-k", "t": "Pass@k", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A code generation evaluation metric that measures the probability that at least one of k generated code samples passes...", "l": "p", "k": ["pass", "code", "generation", "evaluation", "metric", "measures", "probability", "least", "generated", "samples", "passes", "test", "cases", "computed", "unbiased"]}, {"id": "term-passage-retrieval", "t": "Passage Retrieval", "tg": ["Retrieval", "Search"], "d": "general", "x": "The task of identifying and retrieving the most relevant text passages from a large corpus in response to a query,...", "l": "p", "k": ["passage", "retrieval", "task", "identifying", "retrieving", "relevant", "text", "passages", "large", "corpus", "response", "query", "operating", "finer", "granularity"]}, {"id": "term-patchcore", "t": "PatchCore", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An anomaly detection method that maintains a coreset of nominal patch features from a pre-trained network and detects...", "l": "p", "k": ["patchcore", "anomaly", "detection", "method", "maintains", "coreset", "nominal", "patch", "features", "pre-trained", "network", "detects", "anomalies", "measuring", "distance"]}, {"id": "term-patchtst", "t": "PatchTST", "tg": ["Models", "Technical"], "d": "models", "x": "A Transformer model for time series forecasting that segments input sequences into subseries-level patches and uses...", "l": "p", "k": ["patchtst", "transformer", "model", "time", "series", "forecasting", "segments", "input", "sequences", "subseries-level", "patches", "uses", "channel-independent", "processing", "efficiency"]}, {"id": "term-pathai", "t": "PathAI", "tg": ["Models", "Technical", "Medical", "Vision"], "d": "models", "x": "An AI platform and model suite for computational pathology that analyzes digitized tissue slides for disease diagnosis...", "l": "p", "k": ["pathai", "platform", "model", "suite", "computational", "pathology", "analyzes", "digitized", "tissue", "slides", "disease", "diagnosis", "biomarker", "detection"]}, {"id": "term-pathvqa", "t": "PathVQA", "tg": ["Benchmark", "Medical", "Multimodal"], "d": "datasets", "x": "A visual question answering dataset for pathology containing over 32000 questions about 4998 pathology images. Tests...", "l": "p", "k": ["pathvqa", "visual", "question", "answering", "dataset", "pathology", "containing", "questions", "images", "tests", "multimodal", "reasoning", "medical", "domain"]}, {"id": "term-patience-sort", "t": "Patience Sort", "tg": ["Algorithms", "Technical", "Sorting"], "d": "algorithms", "x": "A sorting algorithm inspired by the card game patience that creates sorted piles and merges them. Has connections to...", "l": "p", "k": ["patience", "sort", "sorting", "algorithm", "inspired", "card", "game", "creates", "sorted", "piles", "merges", "connections", "longest", "increasing", "subsequence"]}, {"id": "term-patrick-winston", "t": "Patrick Winston", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1943-2019) who directed the MIT AI Lab from 1972 to 1997 and authored the influential AI...", "l": "p", "k": ["patrick", "winston", "american", "computer", "scientist", "1943-2019", "directed", "mit", "lab", "authored", "influential", "textbook", "making", "significant", "contributions"]}, {"id": "term-pattern-recognition-and-machine-learning", "t": "Pattern Recognition and Machine Learning", "tg": ["History", "Milestones"], "d": "history", "x": "A textbook by Christopher Bishop published in 2006 that covers Bayesian approaches to pattern recognition and machine...", "l": "p", "k": ["pattern", "recognition", "machine", "learning", "textbook", "christopher", "bishop", "published", "covers", "bayesian", "approaches", "book", "became", "influential", "presenting"]}, {"id": "term-paws", "t": "PAWS", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Paraphrase Adversaries from Word Scrambling a dataset of sentence pairs with high lexical overlap that tests whether...", "l": "p", "k": ["paws", "paraphrase", "adversaries", "word", "scrambling", "dataset", "sentence", "pairs", "high", "lexical", "overlap", "tests", "models", "rely", "superficial"]}, {"id": "term-paxos-algorithm", "t": "Paxos Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A consensus algorithm for distributed systems that enables a group of processes to agree on a single value despite...", "l": "p", "k": ["paxos", "algorithm", "consensus", "distributed", "systems", "enables", "group", "processes", "agree", "single", "value", "despite", "failures", "guarantees", "safety"]}, {"id": "term-pbs-portable-batch-system", "t": "PBS (Portable Batch System)", "tg": ["Infrastructure", "Scheduling", "System"], "d": "hardware", "x": "Job scheduling system for managing workload distribution across compute clusters. Used in some HPC and AI environments...", "l": "p", "k": ["pbs", "portable", "batch", "system", "job", "scheduling", "managing", "workload", "distribution", "across", "compute", "clusters", "hpc", "environments", "allocating"]}, {"id": "term-pc-algorithm", "t": "PC Algorithm", "tg": ["Algorithms", "Technical", "Causal"], "d": "algorithms", "x": "A constraint-based causal discovery algorithm that starts with a complete graph and removes edges based on conditional...", "l": "p", "k": ["algorithm", "constraint-based", "causal", "discovery", "starts", "complete", "graph", "removes", "edges", "based", "conditional", "independence", "tests", "named", "creators"]}, {"id": "term-pca-algorithm", "t": "PCA Algorithm", "tg": ["Algorithms", "Fundamentals", "Dimensionality Reduction"], "d": "algorithms", "x": "Principal Component Analysis finds orthogonal directions of maximum variance in data and projects onto a...", "l": "p", "k": ["pca", "algorithm", "principal", "component", "analysis", "finds", "orthogonal", "directions", "maximum", "variance", "data", "projects", "onto", "lower-dimensional", "subspace"]}, {"id": "term-pca-for-embeddings", "t": "PCA for Embeddings", "tg": ["Vector Database", "Dimensionality Reduction"], "d": "general", "x": "The application of Principal Component Analysis to reduce embedding dimensionality by projecting vectors onto the...", "l": "p", "k": ["pca", "embeddings", "application", "principal", "component", "analysis", "reduce", "embedding", "dimensionality", "projecting", "vectors", "onto", "directions", "maximum", "variance"]}, {"id": "term-pcie", "t": "PCIe for AI", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "Peripheral Component Interconnect Express, the standard high-speed serial interface connecting GPUs and accelerators to...", "l": "p", "k": ["pcie", "peripheral", "component", "interconnect", "express", "standard", "high-speed", "serial", "interface", "connecting", "gpus", "accelerators", "host", "system", "gen"]}, {"id": "term-pcie-gen5", "t": "PCIe Gen5", "tg": ["Interconnect", "Standard"], "d": "hardware", "x": "Fifth generation Peripheral Component Interconnect Express standard doubling bandwidth to 32 GT/s per lane or 128 GB/s...", "l": "p", "k": ["pcie", "gen5", "fifth", "generation", "peripheral", "component", "interconnect", "express", "standard", "doubling", "bandwidth", "per", "lane", "x16", "slots"]}, {"id": "term-pcie-gen6", "t": "PCIe Gen6", "tg": ["Interconnect", "Standard"], "d": "hardware", "x": "Sixth generation PCIe standard doubling bandwidth again to 64 GT/s per lane using PAM4 signaling. Will further reduce...", "l": "p", "k": ["pcie", "gen6", "sixth", "generation", "standard", "doubling", "bandwidth", "per", "lane", "pam4", "signaling", "reduce", "communication", "bottlenecks", "accelerators"]}, {"id": "term-pdbbind", "t": "PDBBind", "tg": ["Benchmark", "Scientific"], "d": "datasets", "x": "A database of experimentally measured binding affinity data for biomolecular complexes from the Protein Data Bank. Used...", "l": "p", "k": ["pdbbind", "database", "experimentally", "measured", "binding", "affinity", "data", "biomolecular", "complexes", "protein", "bank", "training", "molecular", "docking", "drug"]}, {"id": "term-pdp-11", "t": "PDP-11", "tg": ["Historical", "DEC", "Minicomputer"], "d": "hardware", "x": "Digital Equipment Corporation minicomputer from 1970 with an influential instruction set architecture. Ran early Unix...", "l": "p", "k": ["pdp-11", "digital", "equipment", "corporation", "minicomputer", "influential", "instruction", "architecture", "ran", "early", "unix", "systems", "influenced", "design", "subsequent"]}, {"id": "term-pdp-8", "t": "PDP-8", "tg": ["Historical", "DEC", "Minicomputer"], "d": "hardware", "x": "Digital Equipment Corporation minicomputer from 1965 considered the first successful commercial minicomputer. Its low...", "l": "p", "k": ["pdp-8", "digital", "equipment", "corporation", "minicomputer", "considered", "successful", "commercial", "low", "price", "computing", "accessible", "smaller", "organizations", "laboratories"]}, {"id": "term-peft", "t": "PEFT (Parameter-Efficient Fine-Tuning)", "tg": ["Training", "Efficiency"], "d": "general", "x": "Techniques that fine-tune models by training only a small subset of parameters. Includes LoRA, prefix tuning, and...", "l": "p", "k": ["peft", "parameter-efficient", "fine-tuning", "techniques", "fine-tune", "models", "training", "small", "subset", "parameters", "includes", "lora", "prefix", "tuning", "adapters"]}, {"id": "term-penalty-method", "t": "Penalty Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An optimization approach that converts constrained problems into unconstrained ones by adding a penalty term for...", "l": "p", "k": ["penalty", "method", "optimization", "approach", "converts", "constrained", "problems", "unconstrained", "ones", "adding", "term", "constraint", "violations", "parameter", "increased"]}, {"id": "term-pengi", "t": "Pengi", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "An audio language model that frames all audio tasks as text-generation tasks by combining an audio encoder with a...", "l": "p", "k": ["pengi", "audio", "language", "model", "frames", "tasks", "text-generation", "combining", "encoder", "pre-trained"]}, {"id": "term-penn-treebank", "t": "Penn Treebank", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A large annotated corpus of English text with part-of-speech tags and syntactic parse trees, widely used as a benchmark...", "l": "p", "k": ["penn", "treebank", "large", "annotated", "corpus", "english", "text", "part-of-speech", "tags", "syntactic", "parse", "trees", "widely", "benchmark", "training"]}, {"id": "term-peoplespeech", "t": "PeopleSpeech", "tg": ["Training Corpus", "Speech"], "d": "datasets", "x": "A dataset of 30000 hours of English speech from diverse public domain sources for training speech recognition systems....", "l": "p", "k": ["peoplespeech", "dataset", "hours", "english", "speech", "diverse", "public", "domain", "sources", "training", "recognition", "systems", "provides", "large-scale", "open"]}, {"id": "term-perceiver", "t": "Perceiver", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A general-purpose architecture that uses cross-attention to map arbitrary high-dimensional inputs to a fixed-size...", "l": "p", "k": ["perceiver", "general-purpose", "architecture", "uses", "cross-attention", "map", "arbitrary", "high-dimensional", "inputs", "fixed-size", "latent", "array", "followed", "self-attention", "space"]}, {"id": "term-perceiver-io", "t": "Perceiver IO", "tg": ["Models", "Technical"], "d": "models", "x": "An extension of the Perceiver that adds flexible output generation through output queries and cross-attention. Handles...", "l": "p", "k": ["perceiver", "extension", "adds", "flexible", "output", "generation", "queries", "cross-attention", "handles", "diverse", "input", "structures", "enabling", "multi-task", "learning"]}, {"id": "term-perceptron", "t": "Perceptron", "tg": ["History", "Milestones"], "d": "history", "x": "A single-layer neural network model introduced by Frank Rosenblatt in 1957 that could learn to classify linearly...", "l": "p", "k": ["perceptron", "single-layer", "neural", "network", "model", "introduced", "frank", "rosenblatt", "learn", "classify", "linearly", "separable", "patterns", "limitations", "demonstrated"]}, {"id": "term-perceptrons-book", "t": "Perceptrons Book", "tg": ["History", "Milestones"], "d": "history", "x": "A 1969 book by Marvin Minsky and Seymour Papert that mathematically analyzed the limitations of single-layer...", "l": "p", "k": ["perceptrons", "book", "marvin", "minsky", "seymour", "papert", "mathematically", "analyzed", "limitations", "single-layer", "particularly", "inability", "solve", "xor", "problem"]}, {"id": "term-perceptual-loss", "t": "Perceptual Loss", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A loss function for image generation that compares feature representations from a pre-trained network rather than raw...", "l": "p", "k": ["perceptual", "loss", "function", "image", "generation", "compares", "feature", "representations", "pre-trained", "network", "rather", "raw", "pixel", "values", "encouraging"]}, {"id": "term-performance-per-watt", "t": "Performance Per Watt", "tg": ["Performance", "Efficiency", "Metric"], "d": "hardware", "x": "Measure of computing efficiency calculated as useful computation divided by power consumed. A key metric for evaluating...", "l": "p", "k": ["performance", "per", "watt", "measure", "computing", "efficiency", "calculated", "useful", "computation", "divided", "power", "consumed", "key", "metric", "evaluating"]}, {"id": "term-performer", "t": "Performer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer variant that uses random feature-based approximation of softmax attention through the FAVOR+ mechanism,...", "l": "p", "k": ["performer", "transformer", "variant", "uses", "random", "feature-based", "approximation", "softmax", "attention", "favor", "mechanism", "achieving", "linear", "time", "space"]}, {"id": "term-permutation-importance", "t": "Permutation Importance", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A model-agnostic method for estimating feature importance by measuring the increase in prediction error when a single...", "l": "p", "k": ["permutation", "importance", "model-agnostic", "method", "estimating", "feature", "measuring", "increase", "prediction", "error", "single", "values", "randomly", "shuffled", "breaking"]}, {"id": "term-perplexity", "t": "Perplexity", "tg": ["Metric", "Evaluation"], "d": "datasets", "x": "A metric measuring how \"surprised\" a language model is by text. Lower perplexity indicates better prediction. Also the...", "l": "p", "k": ["perplexity", "metric", "measuring", "surprised", "language", "model", "text", "lower", "indicates", "better", "prediction", "name", "search", "engine", "combining"]}, {"id": "term-perplexity-metric", "t": "Perplexity Metric", "tg": ["NLP", "Text Processing"], "d": "general", "x": "An intrinsic evaluation metric for language models defined as the exponentiated average negative log-likelihood per...", "l": "p", "k": ["perplexity", "metric", "intrinsic", "evaluation", "language", "models", "defined", "exponentiated", "average", "negative", "log-likelihood", "per", "token", "measuring", "model"]}, {"id": "term-persimmon", "t": "Persimmon", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An 8.1B parameter language model from Adept AI that uses a novel architecture with squared ReLU activations and rotary...", "l": "p", "k": ["persimmon", "parameter", "language", "model", "adept", "uses", "novel", "architecture", "squared", "relu", "activations", "rotary", "position", "embeddings"]}, {"id": "term-persistent-data-structure", "t": "Persistent Data Structure", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A data structure that preserves all previous versions of itself when modified. Fat nodes and path copying and lazy...", "l": "p", "k": ["persistent", "data", "structure", "preserves", "previous", "versions", "itself", "modified", "fat", "nodes", "path", "copying", "lazy", "propagation", "techniques"]}, {"id": "term-persistent-memory", "t": "Persistent Memory", "tg": ["Memory", "Storage"], "d": "hardware", "x": "Non-volatile memory technology like Intel Optane that sits between DRAM and storage in the memory hierarchy. Offers...", "l": "p", "k": ["persistent", "memory", "non-volatile", "technology", "intel", "optane", "sits", "dram", "storage", "hierarchy", "offers", "byte-addressable", "access", "data", "persistence"]}, {"id": "term-persona", "t": "Persona", "tg": ["Prompting", "Technique"], "d": "general", "x": "A specific character or role assigned to an AI through prompting. Personas can include expertise, communication style,...", "l": "p", "k": ["persona", "specific", "character", "role", "assigned", "prompting", "personas", "include", "expertise", "communication", "style", "behavioral", "guidelines", "shape", "responses"]}, {"id": "term-persona-prompting", "t": "Persona Prompting", "tg": ["Prompt Engineering", "Persona"], "d": "general", "x": "A technique that defines a detailed character profile including background, expertise, communication style, and...", "l": "p", "k": ["persona", "prompting", "technique", "defines", "detailed", "character", "profile", "including", "background", "expertise", "communication", "style", "behavioral", "traits", "model"]}, {"id": "term-personachat", "t": "PersonaChat", "tg": ["Benchmark", "NLP", "Dialogue"], "d": "datasets", "x": "A dialogue dataset where conversations are conditioned on assigned persona descriptions. Contains over 160000...", "l": "p", "k": ["personachat", "dialogue", "dataset", "conversations", "conditioned", "assigned", "persona", "descriptions", "contains", "utterances", "testing", "ability", "maintain", "consistent", "personality"]}, {"id": "term-personalitychat", "t": "PersonalityChat", "tg": ["Training Corpus", "NLP", "Dialogue"], "d": "datasets", "x": "A dataset of conversational responses exhibiting different personality traits for training dialogue systems with...", "l": "p", "k": ["personalitychat", "dataset", "conversational", "responses", "exhibiting", "different", "personality", "traits", "training", "dialogue", "systems", "consistent", "controllable", "personalities"]}, {"id": "term-perspective-n-point-algorithm", "t": "Perspective-n-Point Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An algorithm that estimates the pose of a calibrated camera given a set of n 3D-to-2D point correspondences. P3P solves...", "l": "p", "k": ["perspective-n-point", "algorithm", "estimates", "pose", "calibrated", "camera", "given", "3d-to-2d", "point", "correspondences", "p3p", "solves", "minimal", "case", "points"]}, {"id": "term-pes2o", "t": "PeS2o", "tg": ["Training Corpus", "NLP", "Scientific"], "d": "datasets", "x": "A preprocessed version of S2ORC designed for efficient pretraining containing cleaned text from millions of academic...", "l": "p", "k": ["pes2o", "preprocessed", "version", "s2orc", "designed", "efficient", "pretraining", "containing", "cleaned", "text", "millions", "academic", "papers", "component", "several"]}, {"id": "term-peter-norvig", "t": "Peter Norvig", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist and co-author of Artificial Intelligence: A Modern Approach the most widely used AI...", "l": "p", "k": ["peter", "norvig", "american", "computer", "scientist", "co-author", "artificial", "intelligence", "modern", "approach", "widely", "textbook", "former", "director", "research"]}, {"id": "term-pflops", "t": "PFLOPS", "tg": ["Performance", "Metric", "Scale"], "d": "hardware", "x": "PetaFLOPS or one quadrillion floating-point operations per second. Modern AI GPU clusters deliver hundreds to thousands...", "l": "p", "k": ["pflops", "petaflops", "quadrillion", "floating-point", "operations", "per", "modern", "gpu", "clusters", "deliver", "hundreds", "thousands", "compute", "training", "large"]}, {"id": "term-phase-correlation-algorithm", "t": "Phase Correlation Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A frequency-domain method for estimating the translational shift between two images. Computes the normalized...", "l": "p", "k": ["phase", "correlation", "algorithm", "frequency-domain", "method", "estimating", "translational", "shift", "images", "computes", "normalized", "cross-power", "spectrum", "finds", "peak"]}, {"id": "term-phase-change-memory", "t": "Phase-Change Memory", "tg": ["Memory", "Emerging", "Non-Volatile"], "d": "hardware", "x": "Non-volatile memory technology that stores data by switching material between crystalline and amorphous states. Offers...", "l": "p", "k": ["phase-change", "memory", "non-volatile", "technology", "stores", "data", "switching", "material", "crystalline", "amorphous", "states", "offers", "faster", "access", "flash"]}, {"id": "term-phenaki", "t": "Phenaki", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A model for generating variable-length videos from open-domain text descriptions by using a bidirectional masked...", "l": "p", "k": ["phenaki", "model", "generating", "variable-length", "videos", "open-domain", "text", "descriptions", "bidirectional", "masked", "transformer", "compressed", "video", "tokens"]}, {"id": "term-phi", "t": "Phi", "tg": ["Models", "Technical"], "d": "models", "x": "A family of small language models by Microsoft Research that achieve surprisingly strong performance through careful...", "l": "p", "k": ["phi", "family", "small", "language", "models", "microsoft", "research", "achieve", "surprisingly", "strong", "performance", "careful", "data", "curation", "demonstrate"]}, {"id": "term-phi-architecture", "t": "Phi Architecture", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A family of small language models by Microsoft that achieve strong performance through carefully curated high-quality...", "l": "p", "k": ["phi", "architecture", "family", "small", "language", "models", "microsoft", "achieve", "strong", "performance", "carefully", "curated", "high-quality", "training", "data"]}, {"id": "term-phi-training-data", "t": "Phi Training Data", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "Carefully curated datasets used to train Microsoft Phi models combining filtered web data with synthetic...", "l": "p", "k": ["phi", "training", "data", "carefully", "curated", "datasets", "train", "microsoft", "models", "combining", "filtered", "web", "synthetic", "textbook-quality", "content"]}, {"id": "term-phi-2", "t": "Phi-2", "tg": ["Models", "Technical", "NLP", "Fundamentals"], "d": "models", "x": "A 2.7B parameter small language model from Microsoft Research trained on high-quality textbook data that rivals much...", "l": "p", "k": ["phi-2", "parameter", "small", "language", "model", "microsoft", "research", "trained", "high-quality", "textbook", "data", "rivals", "larger", "models", "reasoning"]}, {"id": "term-phi-3", "t": "Phi-3", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A family of small language models from Microsoft available in Mini and Small and Medium sizes that achieve strong...", "l": "p", "k": ["phi-3", "family", "small", "language", "models", "microsoft", "available", "mini", "medium", "sizes", "achieve", "strong", "performance", "curated", "data"]}, {"id": "term-phi-3-vision", "t": "Phi-3 Vision", "tg": ["Models", "Technical", "NLP", "Vision"], "d": "models", "x": "A multimodal variant of Microsoft Phi-3 that can process both images and text for visual question answering and image...", "l": "p", "k": ["phi-3", "vision", "multimodal", "variant", "microsoft", "process", "images", "text", "visual", "question", "answering", "image", "understanding", "tasks"]}, {"id": "term-phi-35", "t": "Phi-3.5", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A mid-generation update to Microsoft Phi-3 that adds improved multilingual support and longer context windows while...", "l": "p", "k": ["phi-3", "mid-generation", "update", "microsoft", "adds", "improved", "multilingual", "support", "longer", "context", "windows", "maintaining", "compact", "model", "footprint"]}, {"id": "term-phind-codellama", "t": "Phind CodeLlama", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A code-specialized model fine-tuned from CodeLlama by Phind that excels at programming tasks and technical problem...", "l": "p", "k": ["phind", "codellama", "code-specialized", "model", "fine-tuned", "excels", "programming", "tasks", "technical", "problem", "solving", "enhanced", "context", "handling"]}, {"id": "term-phoneme", "t": "Phoneme", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The smallest unit of sound in a language that can distinguish one word from another, used in speech recognition systems...", "l": "p", "k": ["phoneme", "smallest", "unit", "sound", "language", "distinguish", "word", "another", "speech", "recognition", "systems", "map", "acoustic", "signals", "linguistic"]}, {"id": "term-phonetics-in-ai", "t": "Phonetics in AI", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The application of phonetic knowledge to AI systems for speech processing, including modeling the acoustic properties...", "l": "p", "k": ["phonetics", "application", "phonetic", "knowledge", "systems", "speech", "processing", "including", "modeling", "acoustic", "properties", "sounds", "recognition", "synthesis", "tasks"]}, {"id": "term-photolithography", "t": "Photolithography", "tg": ["Fabrication", "Manufacturing", "Process"], "d": "hardware", "x": "Semiconductor manufacturing process using light to transfer patterns from a photomask onto a wafer coated with...", "l": "p", "k": ["photolithography", "semiconductor", "manufacturing", "process", "light", "transfer", "patterns", "photomask", "onto", "wafer", "coated", "photoresist", "foundational", "patterning", "technique"]}, {"id": "term-photomaker", "t": "PhotoMaker", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A personalized text-to-image generation model that creates customized portraits by encoding identity information from...", "l": "p", "k": ["photomaker", "personalized", "text-to-image", "generation", "model", "creates", "customized", "portraits", "encoding", "identity", "information", "reference", "photos", "process"]}, {"id": "term-photomask", "t": "Photomask", "tg": ["Fabrication", "Manufacturing", "Tool"], "d": "hardware", "x": "Glass plate with precisely defined opaque patterns used to project circuit layouts onto wafers during lithography. Each...", "l": "p", "k": ["photomask", "glass", "plate", "precisely", "defined", "opaque", "patterns", "project", "circuit", "layouts", "onto", "wafers", "lithography", "chip", "layer"]}, {"id": "term-photometric-augmentation", "t": "Photometric Augmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Image augmentation techniques that modify pixel values without changing spatial layout, including brightness, contrast,...", "l": "p", "k": ["photometric", "augmentation", "image", "techniques", "modify", "pixel", "values", "without", "changing", "spatial", "layout", "including", "brightness", "contrast", "saturation"]}, {"id": "term-photonic-integrated-circuit", "t": "Photonic Integrated Circuit", "tg": ["Emerging", "Photonic", "Integration"], "d": "hardware", "x": "Optical circuit integrating multiple photonic functions on a single chip similar to electronic integrated circuits....", "l": "p", "k": ["photonic", "integrated", "circuit", "optical", "integrating", "multiple", "functions", "single", "chip", "similar", "electronic", "circuits", "enables", "compact", "high-bandwidth"]}, {"id": "term-photonic-processor", "t": "Photonic Processor", "tg": ["Emerging", "Photonic", "Accelerator"], "d": "hardware", "x": "Processor that uses light-based circuits to perform computations particularly matrix multiplications. Companies like...", "l": "p", "k": ["photonic", "processor", "uses", "light-based", "circuits", "perform", "computations", "particularly", "matrix", "multiplications", "companies", "lightmatter", "luminous", "computing", "developing"]}, {"id": "term-physbench", "t": "PhysBench", "tg": ["Benchmark", "NLP", "Reasoning", "Scientific"], "d": "datasets", "x": "A physics reasoning benchmark testing language models on problems from undergraduate physics. Evaluates quantitative...", "l": "p", "k": ["physbench", "physics", "reasoning", "benchmark", "testing", "language", "models", "problems", "undergraduate", "evaluates", "quantitative", "scientific", "capabilities"]}, {"id": "term-physical-symbol-system-hypothesis", "t": "Physical Symbol System Hypothesis", "tg": ["History", "Milestones"], "d": "history", "x": "The 1976 hypothesis by Newell and Simon that a physical symbol system has the necessary and sufficient means for...", "l": "p", "k": ["physical", "symbol", "system", "hypothesis", "newell", "simon", "necessary", "sufficient", "means", "intelligent", "action", "providing", "theoretical", "foundation", "symbolic"]}, {"id": "term-pi0", "t": "Pi0", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "A flow-matching-based robot foundation model from Physical Intelligence that generates dexterous robot actions for...", "l": "p", "k": ["pi0", "flow-matching-based", "robot", "foundation", "model", "physical", "intelligence", "generates", "dexterous", "actions", "complex", "real-world", "manipulation", "tasks"]}, {"id": "term-pick-a-pic", "t": "Pick-a-Pic", "tg": ["Training Corpus", "Multimodal", "Evaluation"], "d": "datasets", "x": "A dataset of text-to-image generation preferences where users chose between pairs of generated images. Used for...", "l": "p", "k": ["pick-a-pic", "dataset", "text-to-image", "generation", "preferences", "users", "chose", "pairs", "generated", "images", "training", "reward", "models", "image", "quality"]}, {"id": "term-pieter-abbeel", "t": "Pieter Abbeel", "tg": ["History", "Pioneers"], "d": "history", "x": "Belgian-American computer scientist at UC Berkeley known for work on robot learning from demonstration deep...", "l": "p", "k": ["pieter", "abbeel", "belgian-american", "computer", "scientist", "berkeley", "known", "work", "robot", "learning", "demonstration", "deep", "reinforcement", "dexterous", "robotic"]}, {"id": "term-pigeonhole-sort", "t": "Pigeonhole Sort", "tg": ["Algorithms", "Technical", "Sorting"], "d": "algorithms", "x": "A sorting algorithm suitable for sorting lists where the number of elements and the range of key values are...", "l": "p", "k": ["pigeonhole", "sort", "sorting", "algorithm", "suitable", "lists", "number", "elements", "range", "key", "values", "approximately", "operates", "placing", "element"]}, {"id": "term-pii-detection", "t": "PII Detection", "tg": ["Safety", "Technical"], "d": "safety", "x": "Automated identification of personally identifiable information in datasets used for AI training and in AI system...", "l": "p", "k": ["pii", "detection", "automated", "identification", "personally", "identifiable", "information", "datasets", "training", "system", "outputs", "critical", "privacy", "compliance", "preventing"]}, {"id": "term-pile-of-law", "t": "Pile of Law", "tg": ["Training Corpus", "NLP", "Legal"], "d": "datasets", "x": "A 256GB corpus of legal and administrative text for pretraining and studying legal language models. Includes court...", "l": "p", "k": ["pile", "law", "256gb", "corpus", "legal", "administrative", "text", "pretraining", "studying", "language", "models", "includes", "court", "opinions", "legislation"]}, {"id": "term-pinecone", "t": "Pinecone", "tg": ["Vector Database", "Managed Service"], "d": "general", "x": "A fully managed cloud-native vector database service designed for production machine learning applications, providing...", "l": "p", "k": ["pinecone", "fully", "managed", "cloud-native", "vector", "database", "service", "designed", "production", "machine", "learning", "applications", "providing", "serverless", "pod-based"]}, {"id": "term-pinsage", "t": "PinSage", "tg": ["Models", "Technical", "Recommendation"], "d": "models", "x": "A graph convolutional network from Pinterest that generates embeddings for billions of items using random-walk-based...", "l": "p", "k": ["pinsage", "graph", "convolutional", "network", "pinterest", "generates", "embeddings", "billions", "items", "random-walk-based", "neighborhood", "sampling", "pin-board"]}, {"id": "term-pipeline", "t": "Pipeline (ML)", "tg": ["Architecture", "MLOps"], "d": "models", "x": "A sequence of data processing and modeling steps chained together. Includes preprocessing, feature extraction, model...", "l": "p", "k": ["pipeline", "sequence", "data", "processing", "modeling", "steps", "chained", "together", "includes", "preprocessing", "feature", "extraction", "model", "inference", "post-processing"]}, {"id": "term-pipeline-parallelism", "t": "Pipeline Parallelism", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A distributed training strategy that partitions model layers into stages across devices, processing different...", "l": "p", "k": ["pipeline", "parallelism", "distributed", "training", "strategy", "partitions", "model", "layers", "stages", "across", "devices", "processing", "different", "micro-batches", "simultaneously"]}, {"id": "term-pipelining", "t": "Pipelining", "tg": ["Architecture", "Fundamentals", "Design"], "d": "hardware", "x": "Processor design technique that overlaps execution of multiple instructions by dividing them into stages. Increases...", "l": "p", "k": ["pipelining", "processor", "design", "technique", "overlaps", "execution", "multiple", "instructions", "dividing", "stages", "increases", "instruction", "throughput", "fundamental", "modern"]}, {"id": "term-piqa", "t": "PIQA", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "Physical Intuition QA a benchmark of 20000 everyday physical commonsense reasoning questions. Tests understanding of...", "l": "p", "k": ["piqa", "physical", "intuition", "benchmark", "everyday", "commonsense", "reasoning", "questions", "tests", "understanding", "world", "works", "goal-oriented", "question", "answering"]}, {"id": "term-pitch-detection-algorithm", "t": "Pitch Detection Algorithm", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "An algorithm that estimates the fundamental frequency of a periodic or quasi-periodic signal. Methods include...", "l": "p", "k": ["pitch", "detection", "algorithm", "estimates", "fundamental", "frequency", "periodic", "quasi-periodic", "signal", "methods", "include", "autocorrelation", "cepstrum", "analysis", "yin"]}, {"id": "term-pix2pix", "t": "Pix2Pix", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A conditional GAN framework for paired image-to-image translation that uses a U-Net generator and PatchGAN...", "l": "p", "k": ["pix2pix", "conditional", "gan", "framework", "paired", "image-to-image", "translation", "uses", "u-net", "generator", "patchgan", "discriminator", "learn", "mappings", "aligned"]}, {"id": "term-pix2struct", "t": "Pix2Struct", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A pre-trained image-to-text model from Google that learns to parse visual structure from screenshots for document...", "l": "p", "k": ["pix2struct", "pre-trained", "image-to-text", "model", "google", "learns", "parse", "visual", "structure", "screenshots", "document", "understanding", "chart", "analysis"]}, {"id": "term-pixart-alpha", "t": "PixArt-alpha", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A training-efficient text-to-image diffusion Transformer that achieves photorealistic image generation quality...", "l": "p", "k": ["pixart-alpha", "training-efficient", "text-to-image", "diffusion", "transformer", "achieves", "photorealistic", "image", "generation", "quality", "comparable", "larger", "models", "reduced", "compute"]}, {"id": "term-pixart-delta", "t": "PixArt-Delta", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A refined variant of PixArt featuring improved training efficiency and output quality through enhanced attention...", "l": "p", "k": ["pixart-delta", "refined", "variant", "pixart", "featuring", "improved", "training", "efficiency", "output", "quality", "enhanced", "attention", "mechanisms", "data", "curation"]}, {"id": "term-pixart-sigma", "t": "PixArt-Sigma", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An improved version of PixArt that generates higher-resolution images with better text alignment using a more advanced...", "l": "p", "k": ["pixart-sigma", "improved", "version", "pixart", "generates", "higher-resolution", "images", "better", "text", "alignment", "advanced", "transformer", "architecture"]}, {"id": "term-pixtral", "t": "Pixtral", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A vision-language model from Mistral AI that natively processes images at their original resolution without resizing...", "l": "p", "k": ["pixtral", "vision-language", "model", "mistral", "natively", "processes", "images", "original", "resolution", "without", "resizing", "accurate", "visual", "understanding"]}, {"id": "term-pku-saferlhf", "t": "PKU-SafeRLHF", "tg": ["Training Corpus", "NLP", "Safety"], "d": "datasets", "x": "A safety alignment dataset from Peking University containing human preference labels for helpfulness and harmlessness....", "l": "p", "k": ["pku-saferlhf", "safety", "alignment", "dataset", "peking", "university", "containing", "human", "preference", "labels", "helpfulness", "harmlessness", "supports", "safety-focused", "rlhf"]}, {"id": "term-place-and-route", "t": "Place and Route", "tg": ["Manufacturing", "Design", "Process"], "d": "hardware", "x": "Chip design step that determines the physical locations of logic cells and the routing of interconnections between...", "l": "p", "k": ["place", "route", "chip", "design", "step", "determines", "physical", "locations", "logic", "cells", "routing", "interconnections", "directly", "impacts", "performance"]}, {"id": "term-places365", "t": "Places365", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A scene recognition dataset containing over 1.8 million images across 365 scene categories. Developed at MIT to advance...", "l": "p", "k": ["places365", "scene", "recognition", "dataset", "containing", "million", "images", "across", "categories", "developed", "mit", "advance", "visual", "understanding", "scenes"]}, {"id": "term-plan-and-execute-agent", "t": "Plan-and-Execute Agent", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An agentic architecture that separates high-level planning from step-by-step execution, with a planner LLM creating...", "l": "p", "k": ["plan-and-execute", "agent", "agentic", "architecture", "separates", "high-level", "planning", "step-by-step", "execution", "planner", "llm", "creating", "task", "decompositions", "executor"]}, {"id": "term-plan-and-solve-plus", "t": "Plan-and-Solve Plus", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "An enhanced version of plan-and-solve prompting that adds detailed instructions to extract relevant variables,...", "l": "p", "k": ["plan-and-solve", "plus", "enhanced", "version", "prompting", "adds", "detailed", "instructions", "extract", "relevant", "variables", "calculate", "intermediate", "results", "pay"]}, {"id": "term-planarity-testing-algorithm", "t": "Planarity Testing Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm that determines whether a graph can be drawn in the plane without edge crossings. Linear-time algorithms...", "l": "p", "k": ["planarity", "testing", "algorithm", "determines", "graph", "drawn", "plane", "without", "edge", "crossings", "linear-time", "algorithms", "hopcroft-tarjan", "boyer-myrvold", "solve"]}, {"id": "term-planning-in-ai", "t": "Planning in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "The area of AI concerned with the realization of strategies or action sequences typically for autonomous agents robots...", "l": "p", "k": ["planning", "area", "concerned", "realization", "strategies", "action", "sequences", "typically", "autonomous", "agents", "robots", "drones", "encompasses", "classical", "strips"]}, {"id": "term-planning-rl", "t": "Planning in RL", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "The process of using a model of the environment to compute or improve a policy before or during interaction. Planning...", "l": "p", "k": ["planning", "process", "model", "environment", "compute", "improve", "policy", "interaction", "methods", "dyna", "integrate", "model-based", "simulation", "model-free", "learning"]}, {"id": "term-plant", "t": "PlanT", "tg": ["Models", "Technical", "Autonomous"], "d": "models", "x": "A Transformer-based planning model for autonomous driving that directly predicts future waypoints from processed sensor...", "l": "p", "k": ["plant", "transformer-based", "planning", "model", "autonomous", "driving", "directly", "predicts", "future", "waypoints", "processed", "sensor", "features", "route", "information"]}, {"id": "term-platt-scaling", "t": "Platt Scaling", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A post-hoc calibration method that fits a logistic regression model to the raw output scores of a classifier using a...", "l": "p", "k": ["platt", "scaling", "post-hoc", "calibration", "method", "fits", "logistic", "regression", "model", "raw", "output", "scores", "classifier", "held-out", "validation"]}, {"id": "term-platypus", "t": "Platypus", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A family of fine-tuned language models that achieve strong performance through curated STEM and logic datasets with...", "l": "p", "k": ["platypus", "family", "fine-tuned", "language", "models", "achieve", "strong", "performance", "curated", "stem", "logic", "datasets", "focused", "parameter-efficient", "training"]}, {"id": "term-playground", "t": "Playground (AI)", "tg": ["Tools", "Interface"], "d": "general", "x": "An interactive interface for experimenting with AI models without coding. Most AI providers offer playgrounds to test...", "l": "p", "k": ["playground", "interactive", "interface", "experimenting", "models", "without", "coding", "providers", "offer", "playgrounds", "test", "prompts", "adjust", "parameters", "explore"]}, {"id": "term-playground-v2", "t": "Playground v2", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An open-source text-to-image model optimized for aesthetic quality that uses curated training data and improved...", "l": "p", "k": ["playground", "open-source", "text-to-image", "model", "optimized", "aesthetic", "quality", "uses", "curated", "training", "data", "improved", "sampling", "produce", "visually"]}, {"id": "term-playground-v25", "t": "Playground v2.5", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An updated text-to-image model with improved color and contrast handling and enhanced ability to generate images across...", "l": "p", "k": ["playground", "updated", "text-to-image", "model", "improved", "color", "contrast", "handling", "enhanced", "ability", "generate", "images", "across", "multiple", "aspect"]}, {"id": "term-plip", "t": "PLIP", "tg": ["Models", "Technical", "Medical", "Vision"], "d": "models", "x": "Pathology Language and Image Pre-Training is a CLIP-based model fine-tuned on pathology image-text pairs for zero-shot...", "l": "p", "k": ["plip", "pathology", "language", "image", "pre-training", "clip-based", "model", "fine-tuned", "image-text", "pairs", "zero-shot", "analysis"]}, {"id": "term-plotqa", "t": "PlotQA", "tg": ["Benchmark", "Multimodal", "Scientific"], "d": "datasets", "x": "A question answering dataset about scientific plots containing over 8 million questions about 224000 plots. Tests...", "l": "p", "k": ["plotqa", "question", "answering", "dataset", "scientific", "plots", "containing", "million", "questions", "tests", "structural", "data", "reasoning", "numerical", "understanding"]}, {"id": "term-pmc-llama", "t": "PMC-LLaMA", "tg": ["Models", "Technical", "NLP", "Medical"], "d": "models", "x": "A language model adapted for biomedical applications through continued pre-training on 4.8 million PubMed Central...", "l": "p", "k": ["pmc-llama", "language", "model", "adapted", "biomedical", "applications", "continued", "pre-training", "million", "pubmed", "central", "scientific", "articles"]}, {"id": "term-pmc-oa", "t": "PMC-OA", "tg": ["Training Corpus", "Medical", "Scientific"], "d": "datasets", "x": "PubMed Central Open Access a large collection of biomedical literature with full text available for text mining....", "l": "p", "k": ["pmc-oa", "pubmed", "central", "open", "access", "large", "collection", "biomedical", "literature", "full", "text", "available", "mining", "provides", "millions"]}, {"id": "term-point-cloud", "t": "Point Cloud", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A 3D data representation consisting of a set of points in three-dimensional space, typically acquired by LiDAR or depth...", "l": "p", "k": ["point", "cloud", "data", "representation", "consisting", "points", "three-dimensional", "space", "typically", "acquired", "lidar", "depth", "sensors", "object", "detection"]}, {"id": "term-point-e", "t": "Point-E", "tg": ["Models", "Technical"], "d": "models", "x": "A system by OpenAI for generating 3D point clouds from text prompts using a two-stage process. First generates a...", "l": "p", "k": ["point-e", "system", "openai", "generating", "point", "clouds", "text", "prompts", "two-stage", "process", "generates", "synthetic", "view", "text-to-image", "model"]}, {"id": "term-pointnet", "t": "PointNet", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A pioneering deep learning architecture that directly processes unordered 3D point cloud data using shared MLPs and...", "l": "p", "k": ["pointnet", "pioneering", "deep", "learning", "architecture", "directly", "processes", "unordered", "point", "cloud", "data", "shared", "mlps", "symmetric", "pooling"]}, {"id": "term-pointnet-plus-plus", "t": "PointNet++", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "An extension of PointNet that introduces hierarchical feature learning by applying PointNet recursively on nested...", "l": "p", "k": ["pointnet", "extension", "introduces", "hierarchical", "feature", "learning", "applying", "recursively", "nested", "partitions", "point", "capturing", "local", "geometric", "structures"]}, {"id": "term-pointpillars", "t": "PointPillars", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "A fast 3D object detection model for lidar point clouds that converts point clouds into vertical columns (pillars) for...", "l": "p", "k": ["pointpillars", "fast", "object", "detection", "model", "lidar", "point", "clouds", "converts", "vertical", "columns", "pillars", "efficient", "processing"]}, {"id": "term-pointwise-convolution", "t": "Pointwise Convolution", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A 1x1 convolution that linearly combines features across channels at each spatial position without considering spatial...", "l": "p", "k": ["pointwise", "convolution", "1x1", "linearly", "combines", "features", "across", "channels", "spatial", "position", "without", "considering", "context", "commonly", "change"]}, {"id": "term-pmi", "t": "Pointwise Mutual Information", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A statistical measure of association between two events that compares their joint probability with their expected...", "l": "p", "k": ["pointwise", "mutual", "information", "statistical", "measure", "association", "events", "compares", "joint", "probability", "expected", "co-occurrence", "independence", "identify", "collocations"]}, {"id": "term-poisson-distribution", "t": "Poisson Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A discrete probability distribution expressing the probability of a given number of events occurring in a fixed...", "l": "p", "k": ["poisson", "distribution", "discrete", "probability", "expressing", "given", "number", "events", "occurring", "fixed", "interval", "known", "average", "rate", "independent"]}, {"id": "term-poisson-regression", "t": "Poisson Regression", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A generalized linear model for count data that assumes the response follows a Poisson distribution and uses a log link...", "l": "p", "k": ["poisson", "regression", "generalized", "linear", "model", "count", "data", "assumes", "response", "follows", "distribution", "uses", "log", "link", "function"]}, {"id": "term-poisson-surface-reconstruction", "t": "Poisson Surface Reconstruction", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A technique that reconstructs a smooth watertight surface from an oriented point cloud by solving a Poisson equation....", "l": "p", "k": ["poisson", "surface", "reconstruction", "technique", "reconstructs", "smooth", "watertight", "oriented", "point", "cloud", "solving", "equation", "produces", "high-quality", "surfaces"]}, {"id": "term-policy", "t": "Policy", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A mapping from states to actions (or probability distributions over actions) that defines the agent's behavior....", "l": "p", "k": ["policy", "mapping", "states", "actions", "probability", "distributions", "defines", "agent", "behavior", "policies", "deterministic", "stochastic", "central", "object", "optimized"]}, {"id": "term-policy-distillation", "t": "Policy Distillation", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A transfer learning technique that trains a student policy to replicate the behavior of one or more teacher policies....", "l": "p", "k": ["policy", "distillation", "transfer", "learning", "technique", "trains", "student", "replicate", "behavior", "teacher", "policies", "compress", "multiple", "task-specific", "single"]}, {"id": "term-policy-entropy", "t": "Policy Entropy", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A measure of randomness in the agent's policy, used as a regularizer in RL to encourage exploration and prevent...", "l": "p", "k": ["policy", "entropy", "measure", "randomness", "agent", "regularizer", "encourage", "exploration", "prevent", "premature", "convergence", "bonuses", "added", "objective", "algorithms"]}, {"id": "term-policy-gradient", "t": "Policy Gradient", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A class of RL algorithms that directly optimize the policy by computing gradients of expected return with respect to...", "l": "p", "k": ["policy", "gradient", "class", "algorithms", "directly", "optimize", "computing", "gradients", "expected", "return", "respect", "parameters", "methods", "handle", "continuous"]}, {"id": "term-policy-iteration", "t": "Policy Iteration", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A dynamic programming algorithm for solving Markov Decision Processes that alternates between policy evaluation and...", "l": "p", "k": ["policy", "iteration", "dynamic", "programming", "algorithm", "solving", "markov", "decision", "processes", "alternates", "evaluation", "improvement", "convergence", "guaranteed", "find"]}, {"id": "term-polyak-averaging", "t": "Polyak Averaging", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A technique that maintains a running average of model parameters during optimization and uses the averaged parameters...", "l": "p", "k": ["polyak", "averaging", "technique", "maintains", "running", "average", "model", "parameters", "optimization", "uses", "averaged", "final", "predictions", "proven", "achieve"]}, {"id": "term-polynomial-kernel", "t": "Polynomial Kernel", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A kernel function that computes the inner product of feature vectors raised to a specified power, enabling SVMs and...", "l": "p", "k": ["polynomial", "kernel", "function", "computes", "inner", "product", "feature", "vectors", "raised", "specified", "power", "enabling", "svms", "methods", "learn"]}, {"id": "term-polynomial-regression", "t": "Polynomial Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A form of regression analysis in which the relationship between the independent variable and the dependent variable is...", "l": "p", "k": ["polynomial", "regression", "form", "analysis", "relationship", "independent", "variable", "dependent", "modeled", "nth-degree", "capturing", "non-linear", "relationships", "within", "linear"]}, {"id": "term-polysemy", "t": "Polysemy", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The property of a word having multiple related meanings, such as 'bank' meaning a financial institution or a river...", "l": "p", "k": ["polysemy", "property", "word", "having", "multiple", "related", "meanings", "bank", "meaning", "financial", "institution", "river", "posing", "challenges", "sense"]}, {"id": "term-pooling-operation", "t": "Pooling Operation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A downsampling operation in neural networks that reduces the spatial dimensions of feature maps by aggregating values...", "l": "p", "k": ["pooling", "operation", "downsampling", "neural", "networks", "reduces", "spatial", "dimensions", "feature", "maps", "aggregating", "values", "within", "local", "regions"]}, {"id": "term-pope", "t": "POPE", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "Polling-based Object Probing Evaluation a benchmark for measuring object hallucination in large vision-language models....", "l": "p", "k": ["pope", "polling-based", "object", "probing", "evaluation", "benchmark", "measuring", "hallucination", "large", "vision-language", "models", "tests", "generate", "descriptions", "objects"]}, {"id": "term-popqa", "t": "PopQA", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A long-tail knowledge QA benchmark testing language model knowledge of entities with varying popularity. Reveals how...", "l": "p", "k": ["popqa", "long-tail", "knowledge", "benchmark", "testing", "language", "model", "entities", "varying", "popularity", "reveals", "performance", "degrades", "less", "common"]}, {"id": "term-population-based-training", "t": "Population-Based Training (PBT)", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A hyperparameter optimization method that trains a population of agents in parallel, periodically replacing poorly...", "l": "p", "k": ["population-based", "training", "pbt", "hyperparameter", "optimization", "method", "trains", "population", "agents", "parallel", "periodically", "replacing", "poorly", "performing", "mutated"]}, {"id": "term-pose-estimation", "t": "Pose Estimation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A computer vision task that detects the positions of body joints or keypoints in images or video, producing a skeletal...", "l": "p", "k": ["pose", "estimation", "computer", "vision", "task", "detects", "positions", "body", "joints", "keypoints", "images", "video", "producing", "skeletal", "representation"]}, {"id": "term-positional-encoding", "t": "Positional Encoding", "tg": ["Architecture", "Transformers"], "d": "models", "x": "A technique to inject position information into transformers, which otherwise process tokens without order awareness....", "l": "p", "k": ["positional", "encoding", "technique", "inject", "position", "information", "transformers", "otherwise", "process", "tokens", "without", "order", "awareness", "absolute", "relative"]}, {"id": "term-positional-interpolation", "t": "Positional Interpolation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A method for extending the context length of pretrained language models by linearly interpolating position encodings to...", "l": "p", "k": ["positional", "interpolation", "method", "extending", "context", "length", "pretrained", "language", "models", "linearly", "interpolating", "position", "encodings", "fit", "longer"]}, {"id": "term-post-deployment-monitoring", "t": "Post-Deployment Monitoring", "tg": ["Safety", "Governance"], "d": "safety", "x": "The ongoing assessment of AI system behavior performance and impact after release to production. Includes tracking...", "l": "p", "k": ["post-deployment", "monitoring", "ongoing", "assessment", "system", "behavior", "performance", "impact", "release", "production", "includes", "tracking", "metrics", "detecting", "drift"]}, {"id": "term-post-market-surveillance-for-ai", "t": "Post-Market Surveillance for AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "Regulatory requirements for monitoring AI system safety and performance after deployment modeled on post-market...", "l": "p", "k": ["post-market", "surveillance", "regulatory", "requirements", "monitoring", "system", "safety", "performance", "deployment", "modeled", "medical", "devices", "pharmaceuticals"]}, {"id": "term-post-norm-transformer", "t": "Post-Norm Transformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The original transformer configuration where layer normalization is applied after the residual connection in each...", "l": "p", "k": ["post-norm", "transformer", "original", "configuration", "layer", "normalization", "applied", "residual", "connection", "sublayer", "requiring", "careful", "learning", "rate", "warmup"]}, {"id": "term-post-training-quantization", "t": "Post-Training Quantization (PTQ)", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "Quantization applied to an already-trained model without further training, using calibration data to determine optimal...", "l": "p", "k": ["post-training", "quantization", "ptq", "applied", "already-trained", "model", "without", "training", "calibration", "data", "determine", "optimal", "scaling", "factors", "simpler"]}, {"id": "term-posterior-distribution", "t": "Posterior Distribution", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "The probability distribution of a parameter after updating the prior distribution with observed data via Bayes'...", "l": "p", "k": ["posterior", "distribution", "probability", "parameter", "updating", "prior", "observed", "data", "via", "bayes", "theorem", "combines", "beliefs", "likelihood", "form"]}, {"id": "term-posterior-predictive-distribution", "t": "Posterior Predictive Distribution", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "The distribution of future observations given the observed data, obtained by integrating the likelihood of new data...", "l": "p", "k": ["posterior", "predictive", "distribution", "future", "observations", "given", "observed", "data", "obtained", "integrating", "likelihood", "model", "parameters", "naturally", "incorporating"]}, {"id": "term-potential-based-reward-shaping", "t": "Potential-Based Reward Shaping", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "A reward shaping method using a potential function over states where the shaping reward equals the discounted...", "l": "p", "k": ["potential-based", "reward", "shaping", "method", "potential", "function", "states", "equals", "discounted", "difference", "potentials", "successor", "current", "form", "guarantees"]}, {"id": "term-powells-method", "t": "Powell's Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "A derivative-free optimization method that minimizes a function by performing sequential one-dimensional optimizations...", "l": "p", "k": ["powell", "method", "derivative-free", "optimization", "minimizes", "function", "performing", "sequential", "one-dimensional", "optimizations", "along", "direction", "vectors", "updates", "improve"]}, {"id": "term-power-analysis", "t": "Power Analysis", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A statistical method for determining the minimum sample size required to detect an effect of a specified size with a...", "l": "p", "k": ["power", "analysis", "statistical", "method", "determining", "minimum", "sample", "size", "required", "detect", "effect", "specified", "given", "level", "confidence"]}, {"id": "term-power-asymmetry-in-ai", "t": "Power Asymmetry in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The imbalance of power between AI developers who control technology and the individuals and communities affected by it....", "l": "p", "k": ["power", "asymmetry", "imbalance", "developers", "control", "technology", "individuals", "communities", "affected", "raises", "concerns", "consent", "accountability", "democratic", "governance"]}, {"id": "term-power-distribution-unit", "t": "Power Distribution Unit", "tg": ["Data Center", "Power", "Infrastructure"], "d": "hardware", "x": "Device that distributes electrical power to equipment in a server rack. Modern PDUs for AI racks must handle...", "l": "p", "k": ["power", "distribution", "unit", "device", "distributes", "electrical", "equipment", "server", "rack", "modern", "pdus", "racks", "must", "handle", "dramatically"]}, {"id": "term-power-iteration", "t": "Power Iteration", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An iterative algorithm that computes the dominant eigenvalue and corresponding eigenvector of a matrix. Starts with a...", "l": "p", "k": ["power", "iteration", "iterative", "algorithm", "computes", "dominant", "eigenvalue", "corresponding", "eigenvector", "matrix", "starts", "random", "vector", "repeatedly", "multiplies"]}, {"id": "term-power-supply-unit", "t": "Power Supply Unit", "tg": ["Power", "Component", "Infrastructure"], "d": "hardware", "x": "Component that converts AC mains electricity to the DC voltages required by computing equipment. AI servers require...", "l": "p", "k": ["power", "supply", "unit", "component", "converts", "mains", "electricity", "voltages", "required", "computing", "equipment", "servers", "require", "high-wattage", "supplies"]}, {"id": "term-power-transform", "t": "Power Transform", "tg": ["Data Science", "Feature Engineering"], "d": "algorithms", "x": "A family of parametric transformations (including Box-Cox and Yeo-Johnson) applied to make data more Gaussian-like,...", "l": "p", "k": ["power", "transform", "family", "parametric", "transformations", "including", "box-cox", "yeo-johnson", "applied", "data", "gaussian-like", "stabilize", "variance", "minimize", "skewness"]}, {"id": "term-power-usage-effectiveness", "t": "Power Usage Effectiveness", "tg": ["Data Center", "Efficiency", "Metric"], "d": "hardware", "x": "Data center energy efficiency metric calculated as total facility power divided by IT equipment power. A PUE of 1.0 is...", "l": "p", "k": ["power", "usage", "effectiveness", "data", "center", "energy", "efficiency", "metric", "calculated", "total", "facility", "divided", "equipment", "pue", "perfect"]}, {"id": "term-powerpaint", "t": "PowerPaint", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A versatile image inpainting model that handles context-aware filling and text-guided object insertion and shape-guided...", "l": "p", "k": ["powerpaint", "versatile", "image", "inpainting", "model", "handles", "context-aware", "filling", "text-guided", "object", "insertion", "shape-guided", "outpainting", "learnable", "task"]}, {"id": "term-ppi", "t": "PPI", "tg": ["Benchmark", "Graph", "Scientific"], "d": "datasets", "x": "Protein-Protein Interaction dataset containing graphs representing interactions between proteins in different human...", "l": "p", "k": ["ppi", "protein-protein", "interaction", "dataset", "containing", "graphs", "representing", "interactions", "proteins", "different", "human", "tissues", "benchmarking", "graph", "neural"]}, {"id": "term-ppo", "t": "PPO (Proximal Policy Optimization)", "tg": ["Training", "Algorithm"], "d": "algorithms", "x": "A reinforcement learning algorithm commonly used in RLHF to train language models. Balances exploration with stable...", "l": "p", "k": ["ppo", "proximal", "policy", "optimization", "reinforcement", "learning", "algorithm", "commonly", "rlhf", "train", "language", "models", "balances", "exploration", "stable"]}, {"id": "term-ppo-clip", "t": "PPO-Clip", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "The clipped surrogate objective variant of Proximal Policy Optimization that limits policy updates by clipping the...", "l": "p", "k": ["ppo-clip", "clipped", "surrogate", "objective", "variant", "proximal", "policy", "optimization", "limits", "updates", "clipping", "probability", "ratio", "simpler", "kl-penalty"]}, {"id": "term-pragmatics", "t": "Pragmatics", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The branch of linguistics studying how context, speaker intention, and shared knowledge influence the interpretation of...", "l": "p", "k": ["pragmatics", "branch", "linguistics", "studying", "context", "speaker", "intention", "shared", "knowledge", "influence", "interpretation", "language", "beyond", "literal", "semantic"]}, {"id": "term-pre-deployment-testing", "t": "Pre-Deployment Testing", "tg": ["Safety", "Governance"], "d": "safety", "x": "Systematic evaluation of AI systems before deployment to identify potential safety issues biases and failure modes....", "l": "p", "k": ["pre-deployment", "testing", "systematic", "evaluation", "systems", "deployment", "identify", "potential", "safety", "issues", "biases", "failure", "modes", "includes", "stress"]}, {"id": "term-pre-norm", "t": "Pre-Norm", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A transformer architecture variant that applies layer normalization before rather than after each sub-layer, improving...", "l": "p", "k": ["pre-norm", "transformer", "architecture", "variant", "applies", "layer", "normalization", "rather", "sub-layer", "improving", "training", "stability", "enabling", "deep", "models"]}, {"id": "term-pre-norm-transformer", "t": "Pre-Norm Transformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer variant where layer normalization is applied before the attention and feedforward sublayers rather than...", "l": "p", "k": ["pre-norm", "transformer", "variant", "layer", "normalization", "applied", "attention", "feedforward", "sublayers", "rather", "improving", "training", "stability", "enabling", "removal"]}, {"id": "term-pre-tokenization", "t": "Pre-Tokenization", "tg": ["NLP", "Tokenization"], "d": "general", "x": "The initial splitting of raw text into preliminary units before applying subword tokenization, typically based on...", "l": "p", "k": ["pre-tokenization", "initial", "splitting", "raw", "text", "preliminary", "units", "applying", "subword", "tokenization", "typically", "based", "whitespace", "punctuation", "language-specific"]}, {"id": "term-pre-training", "t": "Pre-Training", "tg": ["Training", "Phase"], "d": "general", "x": "The initial training phase where models learn general language understanding from vast text data. Creates a foundation...", "l": "p", "k": ["pre-training", "initial", "training", "phase", "models", "learn", "general", "language", "understanding", "vast", "text", "data", "creates", "foundation", "fine-tuned"]}, {"id": "term-precautionary-principle-in-ai", "t": "Precautionary Principle in AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The application of the precautionary principle to AI development, arguing that when potential harms are severe or...", "l": "p", "k": ["precautionary", "principle", "application", "development", "arguing", "potential", "harms", "severe", "irreversible", "lack", "scientific", "certainty", "delay", "protective", "measures"]}, {"id": "term-precision", "t": "Precision", "tg": ["Metrics", "Technical"], "d": "datasets", "x": "In metrics: the proportion of positive predictions that are correct. In computing: the numerical format for model...", "l": "p", "k": ["precision", "metrics", "proportion", "positive", "predictions", "correct", "computing", "numerical", "format", "model", "weights", "fp32", "fp16", "int8", "affecting"]}, {"id": "term-precision-at-k", "t": "Precision at K", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A retrieval evaluation metric that measures the proportion of relevant documents among the top K retrieved results,...", "l": "p", "k": ["precision", "retrieval", "evaluation", "metric", "measures", "proportion", "relevant", "documents", "among", "top", "retrieved", "results", "providing", "cutoff-based", "assessment"]}, {"id": "term-precision-recall-curve-cv", "t": "Precision-Recall Curve", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A plot showing the trade-off between precision and recall at different confidence thresholds for an object detector,...", "l": "p", "k": ["precision-recall", "curve", "plot", "showing", "trade-off", "precision", "recall", "different", "confidence", "thresholds", "object", "detector", "area", "corresponding", "average"]}, {"id": "term-preconditioned-conjugate-gradient", "t": "Preconditioned Conjugate Gradient", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An enhanced version of the conjugate gradient method that applies a preconditioner to improve convergence for solving...", "l": "p", "k": ["preconditioned", "conjugate", "gradient", "enhanced", "version", "method", "applies", "preconditioner", "improve", "convergence", "solving", "symmetric", "positive-definite", "linear", "systems"]}, {"id": "term-predictive-parity", "t": "Predictive Parity", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness metric requiring that the positive predictive value of a classifier is equal across all protected groups,...", "l": "p", "k": ["predictive", "parity", "fairness", "metric", "requiring", "positive", "value", "classifier", "equal", "across", "protected", "groups", "meaning", "among", "individuals"]}, {"id": "term-predictive-policing-ethics", "t": "Predictive Policing Ethics", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "The ethical concerns surrounding AI systems used to forecast criminal activity, including risks of reinforcing racial...", "l": "p", "k": ["predictive", "policing", "ethics", "ethical", "concerns", "surrounding", "systems", "forecast", "criminal", "activity", "including", "risks", "reinforcing", "racial", "biases"]}, {"id": "term-preemptible-vms", "t": "Preemptible VMs", "tg": ["Distributed Computing", "Inference Infrastructure"], "d": "hardware", "x": "Google Cloud's discounted virtual machine instances that last up to 24 hours and can be terminated when resources are...", "l": "p", "k": ["preemptible", "vms", "google", "cloud", "discounted", "virtual", "machine", "instances", "last", "hours", "terminated", "resources", "needed", "elsewhere", "provide"]}, {"id": "term-preference-learning", "t": "Preference Learning", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A family of techniques that train models using human preference data (rankings or comparisons between outputs) rather...", "l": "p", "k": ["preference", "learning", "family", "techniques", "train", "models", "human", "data", "rankings", "comparisons", "outputs", "rather", "explicit", "labels", "including"]}, {"id": "term-prefill-phase", "t": "Prefill Phase", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The initial phase of LLM inference that processes the entire input prompt in parallel to populate the KV cache. The...", "l": "p", "k": ["prefill", "phase", "initial", "llm", "inference", "processes", "entire", "input", "prompt", "parallel", "populate", "cache", "compute-bound", "duration", "scales"]}, {"id": "term-prefill-decode-disaggregation", "t": "Prefill-Decode Disaggregation", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "An inference architecture that separates the compute-bound prefill and memory-bound decode phases onto different...", "l": "p", "k": ["prefill-decode", "disaggregation", "inference", "architecture", "separates", "compute-bound", "prefill", "memory-bound", "decode", "phases", "onto", "different", "hardware", "optimized", "workload"]}, {"id": "term-prefix-attention", "t": "Prefix Attention", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An attention pattern used in prefix language models where a set of prefix tokens can attend to each other...", "l": "p", "k": ["prefix", "attention", "pattern", "language", "models", "tokens", "attend", "bidirectionally", "subsequent", "causal", "combines", "encoder-like", "decoder-like", "single", "model"]}, {"id": "term-prefix-caching", "t": "Prefix Caching", "tg": ["LLM", "Inference"], "d": "models", "x": "An inference optimization that reuses the computed KV cache of shared prompt prefixes across multiple requests,...", "l": "p", "k": ["prefix", "caching", "inference", "optimization", "reuses", "computed", "cache", "shared", "prompt", "prefixes", "across", "multiple", "requests", "avoiding", "redundant"]}, {"id": "term-prefix-language-model", "t": "Prefix Language Model", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A language model architecture where a prefix portion of the input uses bidirectional attention while the remaining...", "l": "p", "k": ["prefix", "language", "model", "architecture", "portion", "input", "uses", "bidirectional", "attention", "remaining", "causal", "combining", "understanding", "generation", "capabilities"]}, {"id": "term-prefix-tuning", "t": "Prefix Tuning", "tg": ["Training", "Efficiency"], "d": "general", "x": "A parameter-efficient fine-tuning method that prepends trainable vectors to inputs. Only these prefixes are updated...", "l": "p", "k": ["prefix", "tuning", "parameter-efficient", "fine-tuning", "method", "prepends", "trainable", "vectors", "inputs", "prefixes", "updated", "training", "keeping", "base", "model"]}, {"id": "term-presence-penalty", "t": "Presence Penalty", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A parameter that applies a fixed penalty to any token that has appeared at least once in the output, encouraging the...", "l": "p", "k": ["presence", "penalty", "parameter", "applies", "fixed", "token", "appeared", "least", "output", "encouraging", "model", "introduce", "topics", "vocabulary"]}, {"id": "term-prewitt-operator", "t": "Prewitt Operator", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An edge detection operator similar to the Sobel operator that uses simpler convolution kernels without weighting the...", "l": "p", "k": ["prewitt", "operator", "edge", "detection", "similar", "sobel", "uses", "simpler", "convolution", "kernels", "without", "weighting", "center", "pixel", "computes"]}, {"id": "term-prims-algorithm", "t": "Prim's Algorithm", "tg": ["Algorithms", "Fundamentals", "Graph"], "d": "algorithms", "x": "A greedy algorithm that builds a minimum spanning tree by starting from an arbitrary vertex and repeatedly adding the...", "l": "p", "k": ["prim", "algorithm", "greedy", "builds", "minimum", "spanning", "tree", "starting", "arbitrary", "vertex", "repeatedly", "adding", "cheapest", "edge", "connects"]}, {"id": "term-primal-dual-method", "t": "Primal-Dual Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An optimization framework that simultaneously updates primal and dual variables to find saddle points of the...", "l": "p", "k": ["primal-dual", "method", "optimization", "framework", "simultaneously", "updates", "primal", "dual", "variables", "find", "saddle", "points", "lagrangian", "forms", "basis"]}, {"id": "term-principal-component-analysis", "t": "Principal Component Analysis", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "An unsupervised linear dimensionality reduction technique that projects data onto orthogonal axes (principal...", "l": "p", "k": ["principal", "component", "analysis", "unsupervised", "linear", "dimensionality", "reduction", "technique", "projects", "data", "onto", "orthogonal", "axes", "components", "maximize"]}, {"id": "term-principal-component-regression", "t": "Principal Component Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A regression technique that first reduces the dimensionality of predictor variables using PCA and then regresses the...", "l": "p", "k": ["principal", "component", "regression", "technique", "reduces", "dimensionality", "predictor", "variables", "pca", "regresses", "response", "retained", "components", "addressing", "multicollinearity"]}, {"id": "term-principal-variation-search", "t": "Principal Variation Search", "tg": ["Algorithms", "Technical", "RL", "Searching"], "d": "algorithms", "x": "An enhancement to alpha-beta pruning that searches the first move with a full window and subsequent moves with a null...", "l": "p", "k": ["principal", "variation", "search", "enhancement", "alpha-beta", "pruning", "searches", "move", "full", "window", "subsequent", "moves", "null", "re-searches", "suggests"]}, {"id": "term-prior-distribution", "t": "Prior Distribution", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "In Bayesian statistics, the probability distribution representing beliefs about a parameter before observing data. It...", "l": "p", "k": ["prior", "distribution", "bayesian", "statistics", "probability", "representing", "beliefs", "parameter", "observing", "data", "encodes", "knowledge", "assumptions", "updated", "likelihood"]}, {"id": "term-prioritized-experience-replay", "t": "Prioritized Experience Replay", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An experience replay strategy that samples transitions with probability proportional to their TD error magnitude,...", "l": "p", "k": ["prioritized", "experience", "replay", "strategy", "samples", "transitions", "probability", "proportional", "error", "magnitude", "allowing", "agent", "learn", "frequently", "surprising"]}, {"id": "term-prioritized-level-replay", "t": "Prioritized Level Replay (PLR)", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "An unsupervised environment design method that tracks learning progress on procedurally generated levels and replays...", "l": "p", "k": ["prioritized", "level", "replay", "plr", "unsupervised", "environment", "design", "method", "tracks", "learning", "progress", "procedurally", "generated", "levels", "replays"]}, {"id": "term-priority-queue-algorithm", "t": "Priority Queue Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "An abstract data structure that supports insertion and extraction of the minimum (or maximum) element. Commonly...", "l": "p", "k": ["priority", "queue", "algorithm", "abstract", "data", "structure", "supports", "insertion", "extraction", "minimum", "maximum", "element", "commonly", "implemented", "binary"]}, {"id": "term-privacy-amplification-by-subsampling", "t": "Privacy Amplification by Subsampling", "tg": ["Algorithms", "Technical", "Privacy"], "d": "algorithms", "x": "A technique that amplifies differential privacy guarantees by applying a mechanism only to a random subsample of the...", "l": "p", "k": ["privacy", "amplification", "subsampling", "technique", "amplifies", "differential", "guarantees", "applying", "mechanism", "random", "subsample", "dataset", "guarantee", "improves", "proportionally"]}, {"id": "term-privacy-attack", "t": "Privacy Attack", "tg": ["Safety", "Technical"], "d": "safety", "x": "Any technique that extracts private information from a machine learning model or its training process. Includes...", "l": "p", "k": ["privacy", "attack", "technique", "extracts", "private", "information", "machine", "learning", "model", "training", "process", "includes", "membership", "inference", "inversion"]}, {"id": "term-privacy-by-design", "t": "Privacy by Design", "tg": ["Safety", "Governance"], "d": "safety", "x": "An approach to system development that embeds privacy protections throughout the entire engineering process rather than...", "l": "p", "k": ["privacy", "design", "approach", "system", "development", "embeds", "protections", "throughout", "entire", "engineering", "process", "rather", "adding", "afterthought", "adopted"]}, {"id": "term-privacy-preserving", "t": "Privacy-Preserving AI", "tg": ["Privacy", "Ethics"], "d": "safety", "x": "Techniques to use AI without exposing sensitive data. Includes federated learning, differential privacy, and secure...", "l": "p", "k": ["privacy-preserving", "techniques", "without", "exposing", "sensitive", "data", "includes", "federated", "learning", "differential", "privacy", "secure", "multi-party", "computation", "critical"]}, {"id": "term-privacy-preserving-machine-learning", "t": "Privacy-Preserving Machine Learning", "tg": ["Safety", "Technical"], "d": "safety", "x": "Techniques that enable machine learning while protecting the privacy of training data. Includes federated learning...", "l": "p", "k": ["privacy-preserving", "machine", "learning", "techniques", "enable", "protecting", "privacy", "training", "data", "includes", "federated", "differential", "secure", "computation", "synthetic"]}, {"id": "term-private-aggregation-of-teacher-ensembles", "t": "Private Aggregation of Teacher Ensembles", "tg": ["Algorithms", "Technical", "Privacy"], "d": "algorithms", "x": "A privacy-preserving knowledge transfer framework that trains an ensemble of teacher models on disjoint private data....", "l": "p", "k": ["private", "aggregation", "teacher", "ensembles", "privacy-preserving", "knowledge", "transfer", "framework", "trains", "ensemble", "models", "disjoint", "data", "student", "model"]}, {"id": "term-proactive-risk-management", "t": "Proactive Risk Management", "tg": ["Safety", "Governance"], "d": "safety", "x": "An approach to AI safety that anticipates and mitigates risks before they materialize rather than reacting to incidents...", "l": "p", "k": ["proactive", "risk", "management", "approach", "safety", "anticipates", "mitigates", "risks", "materialize", "rather", "reacting", "incidents", "occur", "includes", "threat"]}, {"id": "term-pcfg", "t": "Probabilistic Context-Free Grammar", "tg": ["NLP", "Parsing"], "d": "general", "x": "A context-free grammar augmented with probabilities for each production rule, enabling statistical parsing by selecting...", "l": "p", "k": ["probabilistic", "context-free", "grammar", "augmented", "probabilities", "production", "rule", "enabling", "statistical", "parsing", "selecting", "probable", "parse", "tree", "input"]}, {"id": "term-probabilistic-graphical-models", "t": "Probabilistic Graphical Models", "tg": ["History", "Fundamentals"], "d": "history", "x": "A framework for representing complex probability distributions using graph structures. Encompassing Bayesian networks...", "l": "p", "k": ["probabilistic", "graphical", "models", "framework", "representing", "complex", "probability", "distributions", "graph", "structures", "encompassing", "bayesian", "networks", "markov", "random"]}, {"id": "term-probabilistic-pca-algorithm", "t": "Probabilistic PCA Algorithm", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A probabilistic formulation of PCA that models data as generated from a latent variable model with Gaussian noise....", "l": "p", "k": ["probabilistic", "pca", "algorithm", "formulation", "models", "data", "generated", "latent", "variable", "model", "gaussian", "noise", "enables", "handling", "missing"]}, {"id": "term-probabilistic-safety", "t": "Probabilistic Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "An approach to AI safety that provides statistical guarantees about system behavior rather than absolute guarantees....", "l": "p", "k": ["probabilistic", "safety", "approach", "provides", "statistical", "guarantees", "system", "behavior", "rather", "absolute", "useful", "formal", "verification", "intractable", "complex"]}, {"id": "term-probing", "t": "Probing", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An interpretability technique that trains simple classifiers on the internal representations of a neural network to...", "l": "p", "k": ["probing", "interpretability", "technique", "trains", "simple", "classifiers", "internal", "representations", "neural", "network", "test", "linguistic", "semantic", "information", "encoded"]}, {"id": "term-probit-model", "t": "Probit Model", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A regression model for binary outcomes that uses the cumulative distribution function of the standard normal...", "l": "p", "k": ["probit", "model", "regression", "binary", "outcomes", "uses", "cumulative", "distribution", "function", "standard", "normal", "link", "relating", "linear", "predictor"]}, {"id": "term-procedural-environment-generation", "t": "Procedural Environment Generation", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "The automatic creation of diverse training environments through algorithmic variation of level layouts, object...", "l": "p", "k": ["procedural", "environment", "generation", "automatic", "creation", "diverse", "training", "environments", "algorithmic", "variation", "level", "layouts", "object", "positions", "task"]}, {"id": "term-procedural-fairness", "t": "Procedural Fairness", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Fairness in the processes and methods used to develop and deploy AI systems regardless of outcomes. Includes...", "l": "p", "k": ["procedural", "fairness", "processes", "methods", "develop", "deploy", "systems", "regardless", "outcomes", "includes", "transparency", "participation", "consistent", "application", "rules"]}, {"id": "term-process-node", "t": "Process Node", "tg": ["Fabrication", "Manufacturing"], "d": "hardware", "x": "Manufacturing technology generation for semiconductor chips measured in nanometers. Smaller nodes like 3nm and 5nm pack...", "l": "p", "k": ["process", "node", "manufacturing", "technology", "generation", "semiconductor", "chips", "measured", "nanometers", "smaller", "nodes", "3nm", "5nm", "pack", "transistors"]}, {"id": "term-process-reward-model", "t": "Process Reward Model", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A reward model that scores each intermediate reasoning step rather than only the final answer, enabling more...", "l": "p", "k": ["process", "reward", "model", "scores", "intermediate", "reasoning", "step", "rather", "final", "answer", "enabling", "fine-grained", "feedback", "training", "models"]}, {"id": "term-processing-in-memory", "t": "Processing-In-Memory", "tg": ["Memory", "Architecture", "Emerging"], "d": "hardware", "x": "Computing paradigm that embeds processing logic directly within or near memory arrays to reduce data movement....", "l": "p", "k": ["processing-in-memory", "computing", "paradigm", "embeds", "processing", "logic", "directly", "within", "near", "memory", "arrays", "reduce", "data", "movement", "addresses"]}, {"id": "term-procgen", "t": "ProcGen", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "Procedurally generated game environments for reinforcement learning research testing generalization across different...", "l": "p", "k": ["procgen", "procedurally", "generated", "game", "environments", "reinforcement", "learning", "research", "testing", "generalization", "across", "different", "levels", "contains", "unique"]}, {"id": "term-procrustes-analysis-algorithm", "t": "Procrustes Analysis Algorithm", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A method for comparing two sets of points by finding the optimal rigid transformation (rotation and translation and...", "l": "p", "k": ["procrustes", "analysis", "algorithm", "method", "comparing", "sets", "points", "finding", "optimal", "rigid", "transformation", "rotation", "translation", "scaling", "minimizes"]}, {"id": "term-product-quantization", "t": "Product Quantization", "tg": ["Vector Database", "Quantization"], "d": "general", "x": "A vector compression technique that splits high-dimensional vectors into sub-vectors and quantizes each independently...", "l": "p", "k": ["product", "quantization", "vector", "compression", "technique", "splits", "high-dimensional", "vectors", "sub-vectors", "quantizes", "independently", "learned", "codebook", "enabling", "dramatic"]}, {"id": "term-product-quantization-algorithm", "t": "Product Quantization Algorithm", "tg": ["Algorithms", "Technical", "Searching"], "d": "algorithms", "x": "A vector compression technique that splits high-dimensional vectors into subvectors and quantizes each independently....", "l": "p", "k": ["product", "quantization", "algorithm", "vector", "compression", "technique", "splits", "high-dimensional", "vectors", "subvectors", "quantizes", "independently", "enables", "efficient", "approximate"]}, {"id": "term-production-rules", "t": "Production Rules", "tg": ["History", "Fundamentals"], "d": "history", "x": "A knowledge representation formalism consisting of condition-action pairs (if-then rules) used extensively in expert...", "l": "p", "k": ["production", "rules", "knowledge", "representation", "formalism", "consisting", "condition-action", "pairs", "if-then", "extensively", "expert", "systems", "ops5", "clips", "provided"]}, {"id": "term-progan", "t": "ProGAN", "tg": ["Models", "Technical"], "d": "models", "x": "Progressive Growing of GANs trains the generator and discriminator progressively starting from low resolution and...", "l": "p", "k": ["progan", "progressive", "growing", "gans", "trains", "generator", "discriminator", "progressively", "starting", "low", "resolution", "adding", "layers", "higher", "resolutions"]}, {"id": "term-progen2", "t": "Progen2", "tg": ["Models", "Scientific"], "d": "models", "x": "A protein language model from Salesforce Research that generates functional protein sequences conditioned on protein...", "l": "p", "k": ["progen2", "protein", "language", "model", "salesforce", "research", "generates", "functional", "sequences", "conditioned", "family", "function", "annotations"]}, {"id": "term-program-aided-language-model", "t": "Program-Aided Language Model", "tg": ["Prompt Engineering", "Code-Augmented"], "d": "general", "x": "A framework (PAL) that prompts a language model to generate executable program code as intermediate reasoning steps...", "l": "p", "k": ["program-aided", "language", "model", "framework", "pal", "prompts", "generate", "executable", "program", "code", "intermediate", "reasoning", "steps", "rather", "natural"]}, {"id": "term-progressive-training", "t": "Progressive Training", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A training strategy that gradually increases model or data complexity during training. Examples include progressive...", "l": "p", "k": ["progressive", "training", "strategy", "gradually", "increases", "model", "data", "complexity", "examples", "include", "growing", "gans", "curriculum", "learning", "helps"]}, {"id": "term-project-euler-dataset", "t": "Project Euler Dataset", "tg": ["Benchmark", "Code", "Reasoning"], "d": "datasets", "x": "Mathematical programming problems from the Project Euler platform testing the intersection of mathematical reasoning...", "l": "p", "k": ["project", "euler", "dataset", "mathematical", "programming", "problems", "platform", "testing", "intersection", "reasoning", "ability"]}, {"id": "term-project-gutenberg-corpus", "t": "Project Gutenberg Corpus", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A collection of over 70000 public domain books from Project Gutenberg used as a pretraining data source. Provides...", "l": "p", "k": ["project", "gutenberg", "corpus", "collection", "public", "domain", "books", "pretraining", "data", "source", "provides", "high-quality", "literary", "non-fiction", "text"]}, {"id": "term-projected-gradient-descent-attack", "t": "Projected Gradient Descent Attack", "tg": ["Algorithms", "Safety"], "d": "algorithms", "x": "An iterative adversarial attack that applies FGSM multiple times with small step sizes projecting back onto the...", "l": "p", "k": ["projected", "gradient", "descent", "attack", "iterative", "adversarial", "applies", "fgsm", "multiple", "times", "small", "step", "sizes", "projecting", "onto"]}, {"id": "term-prolog", "t": "Prolog", "tg": ["History", "Milestones"], "d": "history", "x": "A logic programming language created by Alain Colmerauer and Robert Kowalski in 1972, widely used in AI research for...", "l": "p", "k": ["prolog", "logic", "programming", "language", "created", "alain", "colmerauer", "robert", "kowalski", "widely", "research", "natural", "processing", "expert", "systems"]}, {"id": "term-prompt", "t": "Prompt", "tg": ["Core Concept", "Fundamentals"], "d": "general", "x": "The text input you send to an AI assistant. Can include context, instructions, examples, and constraints. Prompt...", "l": "p", "k": ["prompt", "text", "input", "send", "assistant", "include", "context", "instructions", "examples", "constraints", "quality", "directly", "influences", "response"]}, {"id": "term-prompt-caching", "t": "Prompt Caching", "tg": ["LLM", "Inference"], "d": "models", "x": "An optimization technique that stores and reuses the computed key-value representations of common prompt prefixes,...", "l": "p", "k": ["prompt", "caching", "optimization", "technique", "stores", "reuses", "computed", "key-value", "representations", "common", "prefixes", "reducing", "redundant", "computation", "repeated"]}, {"id": "term-prompt-chaining", "t": "Prompt Chaining", "tg": ["Technique", "Advanced"], "d": "general", "x": "Breaking complex tasks into multiple sequential prompts, where each builds on the previous output. Enables...", "l": "p", "k": ["prompt", "chaining", "breaking", "complex", "tasks", "multiple", "sequential", "prompts", "builds", "previous", "output", "enables", "sophisticated", "workflows", "better"]}, {"id": "term-prompt-chaining-architecture", "t": "Prompt Chaining Architecture", "tg": ["Prompt Engineering", "Architecture"], "d": "models", "x": "A system design pattern where multiple prompts are connected in a pipeline or directed graph, with each prompt handling...", "l": "p", "k": ["prompt", "chaining", "architecture", "system", "design", "pattern", "multiple", "prompts", "connected", "pipeline", "directed", "graph", "handling", "specific", "subtask"]}, {"id": "term-prompt-compression", "t": "Prompt Compression", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Techniques that reduce the token length of prompts without losing essential information, using methods like selective...", "l": "p", "k": ["prompt", "compression", "techniques", "reduce", "token", "length", "prompts", "without", "losing", "essential", "information", "methods", "selective", "context", "summarization"]}, {"id": "term-prompt-engineering", "t": "Prompt Engineering", "tg": ["Skill", "Practice"], "d": "general", "x": "The practice of crafting effective prompts to get better results from AI systems. Includes techniques, frameworks...", "l": "p", "k": ["prompt", "engineering", "practice", "crafting", "effective", "prompts", "better", "results", "systems", "includes", "techniques", "frameworks", "crisp", "costar", "iterative"]}, {"id": "term-prompt-engineering-emergence", "t": "Prompt Engineering Emergence", "tg": ["History", "Milestones"], "d": "history", "x": "The emergence of prompt engineering as a discipline for designing effective inputs to large language models. As LLMs...", "l": "p", "k": ["prompt", "engineering", "emergence", "discipline", "designing", "effective", "inputs", "large", "language", "models", "llms", "became", "capable", "art", "science"]}, {"id": "term-prompt-ensembling", "t": "Prompt Ensembling", "tg": ["Prompt Engineering", "Ensemble"], "d": "general", "x": "A strategy that runs multiple differently-phrased prompts for the same query and aggregates the outputs through voting,...", "l": "p", "k": ["prompt", "ensembling", "strategy", "runs", "multiple", "differently-phrased", "prompts", "query", "aggregates", "outputs", "voting", "averaging", "selection", "produce", "robust"]}, {"id": "term-prompt-extraction-attack", "t": "Prompt Extraction Attack", "tg": ["Prompt Engineering", "Security"], "d": "safety", "x": "A targeted attack technique that attempts to reconstruct or extract a model's system prompt, proprietary instructions,...", "l": "p", "k": ["prompt", "extraction", "attack", "targeted", "technique", "attempts", "reconstruct", "extract", "model", "system", "proprietary", "instructions", "confidential", "context", "systematic"]}, {"id": "term-prompt-injection", "t": "Prompt Injection", "tg": ["Security", "Risk"], "d": "safety", "x": "A security vulnerability where malicious instructions hidden in content cause AI to behave unexpectedly. A significant...", "l": "p", "k": ["prompt", "injection", "security", "vulnerability", "malicious", "instructions", "hidden", "content", "cause", "behave", "unexpectedly", "significant", "concern", "applications", "processing"]}, {"id": "term-prompt-leaking", "t": "Prompt Leaking", "tg": ["Prompt Engineering", "Security"], "d": "safety", "x": "A security vulnerability where an attacker manipulates a language model into revealing its hidden system prompt or...", "l": "p", "k": ["prompt", "leaking", "security", "vulnerability", "attacker", "manipulates", "language", "model", "revealing", "hidden", "system", "confidential", "instructions", "carefully", "crafted"]}, {"id": "term-prompt-optimization", "t": "Prompt Optimization", "tg": ["Prompt Engineering", "Optimization"], "d": "algorithms", "x": "The systematic process of refining prompt text, structure, and parameters to maximize model performance on a target...", "l": "p", "k": ["prompt", "optimization", "systematic", "process", "refining", "text", "structure", "parameters", "maximize", "model", "performance", "target", "task", "employing", "techniques"]}, {"id": "term-prompt-robustness", "t": "Prompt Robustness", "tg": ["Prompt Engineering", "Robustness"], "d": "general", "x": "The ability of a prompt to maintain consistent model performance across variations in input phrasing, perturbations,...", "l": "p", "k": ["prompt", "robustness", "ability", "maintain", "consistent", "model", "performance", "across", "variations", "input", "phrasing", "perturbations", "edge", "cases", "indicating"]}, {"id": "term-prompt-sensitivity", "t": "Prompt Sensitivity", "tg": ["Prompt Engineering", "Robustness"], "d": "general", "x": "The degree to which a model's output quality and correctness varies in response to minor changes in prompt wording,...", "l": "p", "k": ["prompt", "sensitivity", "degree", "model", "output", "quality", "correctness", "varies", "response", "minor", "changes", "wording", "formatting", "example", "ordering"]}, {"id": "term-prompt-template", "t": "Prompt Template", "tg": ["Pattern", "Reusable"], "d": "general", "x": "A reusable prompt structure with placeholders for variable content. Enables consistent, repeatable interactions and is...", "l": "p", "k": ["prompt", "template", "reusable", "structure", "placeholders", "variable", "content", "enables", "consistent", "repeatable", "interactions", "essential", "building", "ai-powered", "applications"]}, {"id": "term-prompt-templating", "t": "Prompt Templating", "tg": ["Prompt Engineering", "Infrastructure"], "d": "hardware", "x": "The practice of creating reusable prompt structures with placeholder variables that can be dynamically filled with...", "l": "p", "k": ["prompt", "templating", "practice", "creating", "reusable", "structures", "placeholder", "variables", "dynamically", "filled", "specific", "inputs", "runtime", "enabling", "consistent"]}, {"id": "term-prompt-tuning", "t": "Prompt Tuning", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A parameter-efficient method that prepends learnable continuous embeddings (soft prompts) to the input while keeping...", "l": "p", "k": ["prompt", "tuning", "parameter-efficient", "method", "prepends", "learnable", "continuous", "embeddings", "soft", "prompts", "input", "keeping", "model", "parameters", "frozen"]}, {"id": "term-prompt-versioning", "t": "Prompt Versioning", "tg": ["Prompt Engineering", "Infrastructure"], "d": "hardware", "x": "The practice of maintaining version-controlled prompt templates with change tracking, performance baselines, and...", "l": "p", "k": ["prompt", "versioning", "practice", "maintaining", "version-controlled", "templates", "change", "tracking", "performance", "baselines", "rollback", "capabilities", "treating", "prompts", "critical"]}, {"id": "term-promptbench", "t": "PromptBench", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating the robustness of large language models to adversarial prompts. Tests model performance...", "l": "p", "k": ["promptbench", "benchmark", "evaluating", "robustness", "large", "language", "models", "adversarial", "prompts", "tests", "model", "performance", "various", "prompt", "perturbations"]}, {"id": "term-pronoun-resolution", "t": "Pronoun Resolution", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The specific task of determining which entity a pronoun refers to in context, requiring understanding of gender,...", "l": "p", "k": ["pronoun", "resolution", "specific", "task", "determining", "entity", "refers", "context", "requiring", "understanding", "gender", "number", "syntactic", "position", "semantic"]}, {"id": "term-proof-pile", "t": "Proof Pile", "tg": ["Training Corpus", "NLP", "Reasoning"], "d": "datasets", "x": "A dataset of mathematical text including textbooks proofs and lecture notes curated for training language models on...", "l": "p", "k": ["proof", "pile", "dataset", "mathematical", "text", "including", "textbooks", "proofs", "lecture", "notes", "curated", "training", "language", "models", "reasoning"]}, {"id": "term-proof-pile-2", "t": "Proof Pile 2", "tg": ["Training Corpus", "NLP", "Reasoning"], "d": "datasets", "x": "An expanded mathematical pretraining corpus combining arXiv papers open-source math textbooks and mathematical web...", "l": "p", "k": ["proof", "pile", "expanded", "mathematical", "pretraining", "corpus", "combining", "arxiv", "papers", "open-source", "math", "textbooks", "web", "pages", "provides"]}, {"id": "term-proof-number-search", "t": "Proof-Number Search", "tg": ["Algorithms", "Technical", "RL", "Searching"], "d": "algorithms", "x": "A best-first search algorithm for solving game trees that maintains proof and disproof numbers at each node....", "l": "p", "k": ["proof-number", "search", "best-first", "algorithm", "solving", "game", "trees", "maintains", "proof", "disproof", "numbers", "node", "efficiently", "determines", "position"]}, {"id": "term-proofwriter", "t": "ProofWriter", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A dataset for evaluating deductive reasoning requiring models to prove or disprove statements given a set of rules and...", "l": "p", "k": ["proofwriter", "dataset", "evaluating", "deductive", "reasoning", "requiring", "models", "prove", "disprove", "statements", "given", "rules", "facts", "expressed", "natural"]}, {"id": "term-propbank", "t": "PropBank", "tg": ["NLP", "Linguistics"], "d": "general", "x": "Proposition Bank, a corpus annotated with predicate-argument structures for verbs, providing semantic role labels that...", "l": "p", "k": ["propbank", "proposition", "bank", "corpus", "annotated", "predicate-argument", "structures", "verbs", "providing", "semantic", "role", "labels", "facilitate", "training", "evaluation"]}, {"id": "term-propensity-score", "t": "Propensity Score", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The probability that a unit is assigned to a particular treatment given its observed covariates. It is used in causal...", "l": "p", "k": ["propensity", "score", "probability", "unit", "assigned", "particular", "treatment", "given", "observed", "covariates", "causal", "inference", "balance", "control", "groups"]}, {"id": "term-propensity-score-matching-algorithm", "t": "Propensity Score Matching Algorithm", "tg": ["Algorithms", "Fundamentals", "Causal"], "d": "algorithms", "x": "A causal inference technique that matches treated and control units based on their estimated probability of receiving...", "l": "p", "k": ["propensity", "score", "matching", "algorithm", "causal", "inference", "technique", "matches", "treated", "control", "units", "based", "estimated", "probability", "receiving"]}, {"id": "term-proportionality-assessment", "t": "Proportionality Assessment", "tg": ["Safety", "Governance"], "d": "safety", "x": "An evaluation of whether the benefits of an AI system justify its risks and intrusions on individual rights. A key...", "l": "p", "k": ["proportionality", "assessment", "evaluation", "benefits", "system", "justify", "risks", "intrusions", "individual", "rights", "key", "requirement", "european", "regulation", "human"]}, {"id": "term-prosocial-dialog", "t": "Prosocial Dialog", "tg": ["Training Corpus", "NLP", "Dialogue", "Safety"], "d": "datasets", "x": "A dataset of conversations about prosocial behavior containing dialogues that model constructive and empathetic...", "l": "p", "k": ["prosocial", "dialog", "dataset", "conversations", "behavior", "containing", "dialogues", "model", "constructive", "empathetic", "responses", "problematic", "situations"]}, {"id": "term-prospector", "t": "PROSPECTOR", "tg": ["History", "Systems"], "d": "history", "x": "An expert system developed at SRI International in the late 1970s for mineral exploration. PROSPECTOR used Bayesian...", "l": "p", "k": ["prospector", "expert", "system", "developed", "sri", "international", "late", "1970s", "mineral", "exploration", "bayesian", "probability", "networks", "evaluate", "geological"]}, {"id": "term-prost", "t": "PROST", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "Physical Reasoning about Objects Through Space and Time a benchmark testing physical commonsense reasoning about object...", "l": "p", "k": ["prost", "physical", "reasoning", "objects", "space", "time", "benchmark", "testing", "commonsense", "object", "properties", "spatial", "relationships"]}, {"id": "term-protected-attributes", "t": "Protected Attributes", "tg": ["Fairness", "Regulation"], "d": "safety", "x": "Characteristics such as race, gender, age, religion, and disability status that are legally or ethically designated as...", "l": "p", "k": ["protected", "attributes", "characteristics", "race", "gender", "age", "religion", "disability", "status", "legally", "ethically", "designated", "bases", "upon", "differential"]}, {"id": "term-protected-group", "t": "Protected Group", "tg": ["Safety", "Policy"], "d": "safety", "x": "A demographic group defined by characteristics such as race gender age disability or religion that is legally protected...", "l": "p", "k": ["protected", "group", "demographic", "defined", "characteristics", "race", "gender", "age", "disability", "religion", "legally", "discrimination", "fairness", "evaluation", "typically"]}, {"id": "term-proteinmpnn", "t": "ProteinMPNN", "tg": ["Models", "Scientific"], "d": "models", "x": "A message passing neural network for protein sequence design that predicts amino acid sequences most likely to fold...", "l": "p", "k": ["proteinmpnn", "message", "passing", "neural", "network", "protein", "sequence", "design", "predicts", "amino", "acid", "sequences", "likely", "fold", "given"]}, {"id": "term-protgpt2", "t": "ProtGPT2", "tg": ["Models", "Scientific"], "d": "models", "x": "A language model trained on the protein sequence universe that generates de novo protein sequences with natural-like...", "l": "p", "k": ["protgpt2", "language", "model", "trained", "protein", "sequence", "universe", "generates", "novo", "sequences", "natural-like", "properties", "structural", "features"]}, {"id": "term-prottrans", "t": "ProtTrans", "tg": ["Models", "Scientific"], "d": "models", "x": "A collection of protein language models that apply NLP Transformer architectures (including BERT and GPT variants) to...", "l": "p", "k": ["prottrans", "collection", "protein", "language", "models", "apply", "nlp", "transformer", "architectures", "including", "bert", "gpt", "variants", "sequence", "data"]}, {"id": "term-provenance-tracking", "t": "Provenance Tracking", "tg": ["Safety", "Technical"], "d": "safety", "x": "The recording and verification of the origin and history of data models and AI system components throughout their...", "l": "p", "k": ["provenance", "tracking", "recording", "verification", "origin", "history", "data", "models", "system", "components", "throughout", "lifecycle", "essential", "accountability", "reproducibility"]}, {"id": "term-proximal-gradient-method", "t": "Proximal Gradient Method", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An optimization algorithm for minimizing composite objective functions consisting of a smooth term and a non-smooth...", "l": "p", "k": ["proximal", "gradient", "method", "optimization", "algorithm", "minimizing", "composite", "objective", "functions", "consisting", "smooth", "term", "non-smooth", "regularizer", "combines"]}, {"id": "term-proximal-point-algorithm", "t": "Proximal Point Algorithm", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An optimization method that iteratively minimizes the objective plus a quadratic penalty centered at the current...", "l": "p", "k": ["proximal", "point", "algorithm", "optimization", "method", "iteratively", "minimizes", "objective", "plus", "quadratic", "penalty", "centered", "current", "iterate", "forms"]}, {"id": "term-proxy-discrimination", "t": "Proxy Discrimination", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Discrimination that occurs when an AI system uses features that are correlated with protected attributes as proxies,...", "l": "p", "k": ["proxy", "discrimination", "occurs", "system", "uses", "features", "correlated", "protected", "attributes", "proxies", "achieving", "discriminatory", "outcomes", "explicitly", "excluded"]}, {"id": "term-pruning", "t": "Pruning", "tg": ["Optimization", "Efficiency"], "d": "algorithms", "x": "Removing unnecessary weights or neurons from neural networks to reduce size and increase speed. Can dramatically...", "l": "p", "k": ["pruning", "removing", "unnecessary", "weights", "neurons", "neural", "networks", "reduce", "size", "increase", "speed", "dramatically", "decrease", "model", "minimal"]}, {"id": "term-pruning-at-initialization", "t": "Pruning-at-Initialization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "Techniques that identify and remove redundant weights before any training occurs, based on signal propagation or...", "l": "p", "k": ["pruning-at-initialization", "techniques", "identify", "remove", "redundant", "weights", "training", "occurs", "based", "signal", "propagation", "gradient", "flow", "analysis", "methods"]}, {"id": "term-pseudo-labeling", "t": "Pseudo-Labeling", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A semi-supervised learning technique where a model trained on labeled data generates predictions for unlabeled data and...", "l": "p", "k": ["pseudo-labeling", "semi-supervised", "learning", "technique", "model", "trained", "labeled", "data", "generates", "predictions", "unlabeled", "uses", "high-confidence", "additional", "training"]}, {"id": "term-psnr", "t": "PSNR", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Peak Signal-to-Noise Ratio measures image quality by comparing the maximum possible pixel value to the mean squared...", "l": "p", "k": ["psnr", "peak", "signal-to-noise", "ratio", "measures", "image", "quality", "comparing", "maximum", "possible", "pixel", "value", "mean", "squared", "error"]}, {"id": "term-public-interest-ai", "t": "Public Interest AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "AI development and deployment focused on serving broad public interests rather than purely commercial objectives....", "l": "p", "k": ["public", "interest", "development", "deployment", "focused", "serving", "broad", "interests", "rather", "purely", "commercial", "objectives", "includes", "applications", "healthcare"]}, {"id": "term-pubmed-abstracts", "t": "PubMed Abstracts", "tg": ["Training Corpus", "NLP", "Medical"], "d": "datasets", "x": "A collection of over 35 million biomedical literature abstracts from the PubMed database. Used for biomedical NLP...", "l": "p", "k": ["pubmed", "abstracts", "collection", "million", "biomedical", "literature", "database", "nlp", "research", "including", "information", "extraction", "question", "answering"]}, {"id": "term-pubmedbert", "t": "PubMedBERT", "tg": ["Models", "Technical", "Medical", "NLP"], "d": "models", "x": "A BERT model pre-trained from scratch on PubMed biomedical literature abstracts for improved performance on biomedical...", "l": "p", "k": ["pubmedbert", "bert", "model", "pre-trained", "scratch", "pubmed", "biomedical", "literature", "abstracts", "improved", "performance", "natural", "language", "processing", "tasks"]}, {"id": "term-pubmedqa", "t": "PubMedQA", "tg": ["Benchmark", "NLP", "Medical"], "d": "datasets", "x": "A biomedical question answering dataset where questions are derived from PubMed article titles and answers must be...", "l": "p", "k": ["pubmedqa", "biomedical", "question", "answering", "dataset", "questions", "derived", "pubmed", "article", "titles", "answers", "must", "yes", "maybe", "supporting"]}, {"id": "term-punched-card", "t": "Punched Card", "tg": ["Historical", "Storage", "Input"], "d": "hardware", "x": "Paper card with holes representing data used for input and programming in early computers. IBM punched cards were the...", "l": "p", "k": ["punched", "card", "paper", "holes", "representing", "data", "input", "programming", "early", "computers", "ibm", "cards", "were", "primary", "entry"]}, {"id": "term-purposeful-limitation", "t": "Purposeful Limitation", "tg": ["Safety", "Governance"], "d": "safety", "x": "The deliberate restriction of an AI system's capabilities to reduce risk even when greater capability is technically...", "l": "p", "k": ["purposeful", "limitation", "deliberate", "restriction", "system", "capabilities", "reduce", "risk", "greater", "capability", "technically", "achievable", "precautionary", "approach", "deploying"]}, {"id": "term-push-relabel-algorithm", "t": "Push-Relabel Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A maximum flow algorithm that uses preflow instead of augmenting paths. Pushes excess flow toward the sink and relabels...", "l": "p", "k": ["push-relabel", "algorithm", "maximum", "flow", "uses", "preflow", "instead", "augmenting", "paths", "pushes", "excess", "toward", "sink", "relabels", "vertices"]}, {"id": "term-pythia", "t": "Pythia", "tg": ["Models", "Technical"], "d": "models", "x": "A suite of language models by EleutherAI ranging from 70M to 12B parameters all trained on exactly the same data in the...", "l": "p", "k": ["pythia", "suite", "language", "models", "eleutherai", "ranging", "70m", "12b", "parameters", "trained", "exactly", "data", "order", "designed", "studying"]}, {"id": "term-pytorch", "t": "PyTorch", "tg": ["Framework", "Deep Learning"], "d": "models", "x": "A popular open-source deep learning framework from Meta, known for its flexibility and Pythonic design. The dominant...", "l": "p", "k": ["pytorch", "popular", "open-source", "deep", "learning", "framework", "meta", "known", "flexibility", "pythonic", "design", "dominant", "research", "increasingly", "production"]}, {"id": "term-pytorch-release", "t": "PyTorch Release", "tg": ["History", "Milestones"], "d": "history", "x": "The release of PyTorch by Facebook AI Research in October 2016. With its dynamic computational graphs and Pythonic...", "l": "p", "k": ["pytorch", "release", "facebook", "research", "october", "dynamic", "computational", "graphs", "pythonic", "design", "became", "preferred", "framework", "ease", "flexibility"]}, {"id": "term-q-function", "t": "Q-Function", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "The action-value function Q(s,a) that estimates the expected cumulative reward of taking action a in state s and then...", "l": "q", "k": ["q-function", "action-value", "function", "estimates", "expected", "cumulative", "reward", "taking", "action", "state", "following", "given", "policy", "q-functions", "enable"]}, {"id": "term-q-learning", "t": "Q-Learning", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An off-policy temporal difference algorithm that learns the optimal action-value function Q* by iteratively updating...", "l": "q", "k": ["q-learning", "off-policy", "temporal", "difference", "algorithm", "learns", "optimal", "action-value", "function", "iteratively", "updating", "q-values", "bellman", "optimality", "equation"]}, {"id": "term-qasper", "t": "Qasper", "tg": ["Benchmark", "NLP", "Scientific"], "d": "datasets", "x": "A question answering dataset based on NLP research papers where questions are written by NLP practitioners. Tests the...", "l": "q", "k": ["qasper", "question", "answering", "dataset", "based", "nlp", "research", "papers", "questions", "written", "practitioners", "tests", "ability", "answer", "long"]}, {"id": "term-qdrant", "t": "Qdrant", "tg": ["Vector Database", "Open Source"], "d": "general", "x": "An open-source vector similarity search engine written in Rust that provides filtering, payload storage, and...", "l": "q", "k": ["qdrant", "open-source", "vector", "similarity", "search", "engine", "written", "rust", "provides", "filtering", "payload", "storage", "distributed", "deployment", "capabilities"]}, {"id": "term-qlora", "t": "QLoRA (Quantized LoRA)", "tg": ["Training", "Efficiency"], "d": "general", "x": "A technique combining quantization with LoRA fine-tuning. Enables fine-tuning large models on consumer GPUs by using...", "l": "q", "k": ["qlora", "quantized", "lora", "technique", "combining", "quantization", "fine-tuning", "enables", "large", "models", "consumer", "gpus", "4-bit", "base"]}, {"id": "term-qm9", "t": "QM9", "tg": ["Benchmark", "Scientific"], "d": "datasets", "x": "A dataset of 134000 small organic molecules with quantum chemical properties computed using density functional theory....", "l": "q", "k": ["qm9", "dataset", "small", "organic", "molecules", "quantum", "chemical", "properties", "computed", "density", "functional", "theory", "standard", "benchmark", "molecular"]}, {"id": "term-qmix", "t": "QMIX", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A multi-agent RL algorithm that factorizes the joint action-value function as a monotonic combination of per-agent...", "l": "q", "k": ["qmix", "multi-agent", "algorithm", "factorizes", "joint", "action-value", "function", "monotonic", "combination", "per-agent", "utilities", "mixing", "network", "enables", "centralized"]}, {"id": "term-qmix-algorithm", "t": "QMIX Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A multi-agent reinforcement learning method that factorizes the joint action-value function into per-agent utilities...", "l": "q", "k": ["qmix", "algorithm", "multi-agent", "reinforcement", "learning", "method", "factorizes", "joint", "action-value", "function", "per-agent", "utilities", "combined", "monotonic", "mixing"]}, {"id": "term-qnli", "t": "QNLI", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Question Natural Language Inference a dataset derived from the Stanford Question Answering Dataset converted into...", "l": "q", "k": ["qnli", "question", "natural", "language", "inference", "dataset", "derived", "stanford", "answering", "converted", "sentence", "pair", "classification", "tests", "context"]}, {"id": "term-qqp", "t": "QQP", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Quora Question Pairs a dataset of over 400000 question pairs from Quora annotated for semantic equivalence. Tests the...", "l": "q", "k": ["qqp", "quora", "question", "pairs", "dataset", "annotated", "semantic", "equivalence", "tests", "ability", "determine", "questions", "meaning"]}, {"id": "term-qr-algorithm", "t": "QR Algorithm", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "An iterative eigenvalue algorithm that repeatedly computes the QR decomposition of a matrix and reforms it as RQ....", "l": "q", "k": ["algorithm", "iterative", "eigenvalue", "repeatedly", "computes", "decomposition", "matrix", "reforms", "converges", "triangular", "quasi-triangular", "form", "revealing", "eigenvalues", "standard"]}, {"id": "term-qr-decomposition", "t": "QR Decomposition", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A matrix factorization that expresses a matrix as the product of an orthogonal matrix Q and an upper triangular matrix...", "l": "q", "k": ["decomposition", "matrix", "factorization", "expresses", "product", "orthogonal", "upper", "triangular", "solving", "least", "squares", "problems", "computing", "eigenvalues", "stabilizing"]}, {"id": "term-quac", "t": "QuAC", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Question Answering in Context a dataset of 14000 information-seeking dialogues about Wikipedia sections. Models must...", "l": "q", "k": ["quac", "question", "answering", "context", "dataset", "information-seeking", "dialogues", "wikipedia", "sections", "models", "must", "answer", "questions", "conversation", "incomplete"]}, {"id": "term-quadtree-algorithm", "t": "Quadtree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A tree data structure that partitions a two-dimensional space by recursively subdividing it into four quadrants. Used...", "l": "q", "k": ["quadtree", "algorithm", "tree", "data", "structure", "partitions", "two-dimensional", "space", "recursively", "subdividing", "four", "quadrants", "spatial", "indexing", "image"]}, {"id": "term-qualcomm-ai-engine", "t": "Qualcomm AI Engine", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "Qualcomm's heterogeneous AI compute platform that coordinates the Hexagon DSP, Adreno GPU, and Kryo CPU within...", "l": "q", "k": ["qualcomm", "engine", "heterogeneous", "compute", "platform", "coordinates", "hexagon", "dsp", "adreno", "gpu", "kryo", "cpu", "within", "snapdragon", "processors"]}, {"id": "term-qualcomm-snapdragon-8-gen-3", "t": "Qualcomm Snapdragon 8 Gen 3", "tg": ["Mobile", "Qualcomm", "SoC"], "d": "hardware", "x": "Qualcomm flagship mobile processor featuring the Hexagon NPU capable of 45 TOPS for on-device AI inference. Powers...", "l": "q", "k": ["qualcomm", "snapdragon", "gen", "flagship", "mobile", "processor", "featuring", "hexagon", "npu", "capable", "tops", "on-device", "inference", "powers", "android"]}, {"id": "term-qualitative-reasoning", "t": "Qualitative Reasoning", "tg": ["History", "Fundamentals"], "d": "history", "x": "An approach to modeling and reasoning about continuous systems using qualitative rather than numerical descriptions....", "l": "q", "k": ["qualitative", "reasoning", "approach", "modeling", "continuous", "systems", "rather", "numerical", "descriptions", "developed", "researchers", "including", "benjamin", "kuipers", "kenneth"]}, {"id": "term-quality", "t": "QuALITY", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A question answering benchmark requiring reading and understanding of long documents averaging 5000 words. Tests...", "l": "q", "k": ["quality", "question", "answering", "benchmark", "requiring", "reading", "understanding", "long", "documents", "averaging", "words", "tests", "long-context", "comprehension", "multiple-choice"]}, {"id": "term-quantile-regression", "t": "Quantile Regression", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A regression method that estimates conditional quantiles (such as the median or 90th percentile) of the response...", "l": "q", "k": ["quantile", "regression", "method", "estimates", "conditional", "quantiles", "median", "90th", "percentile", "response", "variable", "rather", "mean", "providing", "complete"]}, {"id": "term-quantile-quantile-plot", "t": "Quantile-Quantile Plot", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A graphical tool for comparing two probability distributions by plotting their quantiles against each other. It is...", "l": "q", "k": ["quantile-quantile", "plot", "graphical", "tool", "comparing", "probability", "distributions", "plotting", "quantiles", "against", "commonly", "assess", "dataset", "follows", "theoretical"]}, {"id": "term-quantization", "t": "Quantization", "tg": ["Optimization", "Deployment"], "d": "algorithms", "x": "Reducing the precision of model weights (e.g., from 16-bit to 4-bit) to decrease memory usage and increase speed....", "l": "q", "k": ["quantization", "reducing", "precision", "model", "weights", "16-bit", "4-bit", "decrease", "memory", "usage", "increase", "speed", "enables", "running", "larger"]}, {"id": "term-quantization-aware-training", "t": "Quantization-Aware Training (QAT)", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A training technique that simulates the effects of quantization during the forward pass while maintaining...", "l": "q", "k": ["quantization-aware", "training", "qat", "technique", "simulates", "effects", "quantization", "forward", "pass", "maintaining", "full-precision", "gradients", "backpropagation", "produces", "models"]}, {"id": "term-quantum-annealing", "t": "Quantum Annealing", "tg": ["Quantum", "Optimization", "Approach"], "d": "hardware", "x": "Quantum computing approach that finds low-energy states of optimization problems by exploiting quantum tunneling....", "l": "q", "k": ["quantum", "annealing", "computing", "approach", "finds", "low-energy", "states", "optimization", "problems", "exploiting", "tunneling", "d-wave", "systems", "rather", "gate-based"]}, {"id": "term-quantum-computing-for-ai", "t": "Quantum Computing for AI", "tg": ["Emerging", "Quantum", "Computing"], "d": "hardware", "x": "Application of quantum mechanical phenomena to accelerate certain AI computations. Quantum computers may eventually...", "l": "q", "k": ["quantum", "computing", "application", "mechanical", "phenomena", "accelerate", "certain", "computations", "computers", "eventually", "solve", "optimization", "sampling", "problems", "exponentially"]}, {"id": "term-quantum-error-correction", "t": "Quantum Error Correction", "tg": ["Quantum", "Reliability", "Fundamentals"], "d": "hardware", "x": "Techniques for protecting quantum information from decoherence and noise using redundant qubits. Essential for building...", "l": "q", "k": ["quantum", "error", "correction", "techniques", "protecting", "information", "decoherence", "noise", "redundant", "qubits", "essential", "building", "fault-tolerant", "computers", "capable"]}, {"id": "term-quantum-gate", "t": "Quantum Gate", "tg": ["Quantum", "Fundamentals", "Operation"], "d": "hardware", "x": "Basic quantum operation that manipulates one or more qubits analogous to logic gates in classical computing. Sequences...", "l": "q", "k": ["quantum", "gate", "basic", "operation", "manipulates", "qubits", "analogous", "logic", "gates", "classical", "computing", "sequences", "implement", "algorithms"]}, {"id": "term-quantum-supremacy", "t": "Quantum Supremacy", "tg": ["Quantum", "Milestone", "Benchmark"], "d": "hardware", "x": "Milestone where a quantum computer solves a specific problem faster than any classical supercomputer. Google claimed...", "l": "q", "k": ["quantum", "supremacy", "milestone", "computer", "solves", "specific", "problem", "faster", "classical", "supercomputer", "google", "claimed", "sycamore", "processor"]}, {"id": "term-quantum-volume", "t": "Quantum Volume", "tg": ["Quantum", "Metric", "Benchmark"], "d": "hardware", "x": "Metric for measuring the computational capability of quantum computers accounting for qubits connectivity and error...", "l": "q", "k": ["quantum", "volume", "metric", "measuring", "computational", "capability", "computers", "accounting", "qubits", "connectivity", "error", "rates", "provides", "holistic", "measure"]}, {"id": "term-qubit", "t": "Qubit", "tg": ["Quantum", "Fundamentals"], "d": "hardware", "x": "Fundamental unit of quantum information that can exist in superposition of 0 and 1 states simultaneously. Quantum...", "l": "q", "k": ["qubit", "fundamental", "unit", "quantum", "information", "exist", "superposition", "states", "simultaneously", "computers", "qubits", "explore", "computational", "paths", "parallel"]}, {"id": "term-query", "t": "Query", "tg": ["Concept", "Dual Meaning"], "d": "general", "x": "In RAG/search: the user's question or search terms. In attention: one of three vectors (query, key, value) used to...", "l": "q", "k": ["query", "rag", "search", "user", "question", "terms", "attention", "vectors", "key", "value", "compute", "weights"]}, {"id": "term-query-decomposition", "t": "Query Decomposition", "tg": ["Retrieval", "Query Processing"], "d": "general", "x": "A retrieval strategy that breaks a complex multi-faceted query into simpler sub-queries, retrieves results for each...", "l": "q", "k": ["query", "decomposition", "retrieval", "strategy", "breaks", "complex", "multi-faceted", "simpler", "sub-queries", "retrieves", "results", "independently", "merges", "address", "questions"]}, {"id": "term-query-expansion", "t": "Query Expansion", "tg": ["Retrieval", "Query Processing"], "d": "general", "x": "A retrieval technique that augments the original query with additional related terms, synonyms, or reformulations to...", "l": "q", "k": ["query", "expansion", "retrieval", "technique", "augments", "original", "additional", "related", "terms", "synonyms", "reformulations", "improve", "recall", "bridging", "vocabulary"]}, {"id": "term-question-answering", "t": "Question Answering (QA)", "tg": ["NLP Task", "Application"], "d": "general", "x": "An NLP task where the model answers questions based on provided context or its knowledge. Includes extractive QA...", "l": "q", "k": ["question", "answering", "nlp", "task", "model", "answers", "questions", "based", "provided", "context", "knowledge", "includes", "extractive", "finding", "text"]}, {"id": "term-quick-sort", "t": "Quick Sort", "tg": ["Algorithms", "Fundamentals", "Sorting"], "d": "algorithms", "x": "A divide-and-conquer sorting algorithm that selects a pivot element and partitions the array around it. Has O(n log n)...", "l": "q", "k": ["quick", "sort", "divide-and-conquer", "sorting", "algorithm", "selects", "pivot", "element", "partitions", "array", "around", "log", "average", "time", "complexity"]}, {"id": "term-quotient-filter-algorithm", "t": "Quotient Filter Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A space-efficient probabilistic data structure similar to a Bloom filter that supports membership queries and...", "l": "q", "k": ["quotient", "filter", "algorithm", "space-efficient", "probabilistic", "data", "structure", "similar", "bloom", "supports", "membership", "queries", "deletions", "stores", "fingerprints"]}, {"id": "term-qwen", "t": "Qwen", "tg": ["Models", "Technical"], "d": "models", "x": "A family of large language models developed by Alibaba Cloud. Available in multiple sizes with strong multilingual...", "l": "q", "k": ["qwen", "family", "large", "language", "models", "developed", "alibaba", "cloud", "available", "multiple", "sizes", "strong", "multilingual", "capabilities", "particularly"]}, {"id": "term-qwen-15", "t": "Qwen 1.5", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An improved version of Alibaba's Qwen language model series with enhanced multilingual capabilities and...", "l": "q", "k": ["qwen", "improved", "version", "alibaba", "language", "model", "series", "enhanced", "multilingual", "capabilities", "instruction-following", "across", "multiple", "parameter", "sizes"]}, {"id": "term-qwen-2", "t": "Qwen 2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A second-generation large language model family from Alibaba featuring models from 0.5B to 72B parameters with strong...", "l": "q", "k": ["qwen", "second-generation", "large", "language", "model", "family", "alibaba", "featuring", "models", "72b", "parameters", "strong", "multilingual", "coding", "performance"]}, {"id": "term-qwen-25", "t": "Qwen 2.5", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A third-generation language model family from Alibaba with improved instruction following and mathematical reasoning...", "l": "q", "k": ["qwen", "third-generation", "language", "model", "family", "alibaba", "improved", "instruction", "following", "mathematical", "reasoning", "across", "sizes", "72b", "parameters"]}, {"id": "term-qwen-audio", "t": "Qwen-Audio", "tg": ["Models", "Technical", "NLP", "Audio"], "d": "models", "x": "An audio-language model from Alibaba that can understand and reason about diverse audio inputs including speech and...", "l": "q", "k": ["qwen-audio", "audio-language", "model", "alibaba", "understand", "reason", "diverse", "audio", "inputs", "including", "speech", "music", "environmental", "sounds"]}, {"id": "term-qwen-vl", "t": "Qwen-VL", "tg": ["Models", "Technical", "NLP", "Vision"], "d": "models", "x": "A vision-language model from Alibaba that extends Qwen with visual perception capabilities for image understanding and...", "l": "q", "k": ["qwen-vl", "vision-language", "model", "alibaba", "extends", "qwen", "visual", "perception", "capabilities", "image", "understanding", "multimodal", "dialogue"]}, {"id": "term-qwen2-audio", "t": "Qwen2-Audio", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "A second-generation audio-language model from Alibaba with enhanced capabilities for understanding and reasoning about...", "l": "q", "k": ["qwen2-audio", "second-generation", "audio-language", "model", "alibaba", "enhanced", "capabilities", "understanding", "reasoning", "diverse", "audio", "inputs", "conversations"]}, {"id": "term-qwen25-coder", "t": "Qwen2.5-Coder", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A code-specialized model from the Qwen 2.5 family with improved multi-language code generation and stronger reasoning...", "l": "q", "k": ["qwen2", "5-coder", "code-specialized", "model", "qwen", "family", "improved", "multi-language", "code", "generation", "stronger", "reasoning", "programming", "concepts"]}, {"id": "term-r-squared", "t": "R-Squared", "tg": ["Statistics", "Metrics"], "d": "datasets", "x": "A statistical measure indicating the proportion of variance in the dependent variable that is explained by the...", "l": "r", "k": ["r-squared", "statistical", "measure", "indicating", "proportion", "variance", "dependent", "variable", "explained", "independent", "variables", "regression", "model", "values", "range"]}, {"id": "term-r-tree-algorithm", "t": "R-Tree Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A tree data structure for indexing multi-dimensional spatial objects using minimum bounding rectangles. Supports...", "l": "r", "k": ["r-tree", "algorithm", "tree", "data", "structure", "indexing", "multi-dimensional", "spatial", "objects", "minimum", "bounding", "rectangles", "supports", "efficient", "queries"]}, {"id": "term-r1-xcon", "t": "R1/XCON", "tg": ["History", "Milestones"], "d": "history", "x": "An expert system developed by John McDermott at Carnegie Mellon in 1980 for configuring VAX computer orders at Digital...", "l": "r", "k": ["xcon", "expert", "system", "developed", "john", "mcdermott", "carnegie", "mellon", "configuring", "vax", "computer", "orders", "digital", "equipment", "corporation"]}, {"id": "term-r2d2-algorithm", "t": "R2D2 Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "Recurrent Replay Distributed DQN is a distributed reinforcement learning agent that combines recurrent neural networks...", "l": "r", "k": ["r2d2", "algorithm", "recurrent", "replay", "distributed", "dqn", "reinforcement", "learning", "agent", "combines", "neural", "networks", "training", "uses", "stored"]}, {"id": "term-rabin-karp-algorithm", "t": "Rabin-Karp Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP", "Searching"], "d": "algorithms", "x": "A string-searching algorithm that uses hashing to find pattern matches in text. Computes rolling hash values to...", "l": "r", "k": ["rabin-karp", "algorithm", "string-searching", "uses", "hashing", "find", "pattern", "matches", "text", "computes", "rolling", "hash", "values", "efficiently", "compare"]}, {"id": "term-race", "t": "RACE", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A reading comprehension dataset collected from English exams for Chinese middle and high school students. Contains...", "l": "r", "k": ["race", "reading", "comprehension", "dataset", "collected", "english", "exams", "chinese", "middle", "high", "school", "students", "contains", "nearly", "passages"]}, {"id": "term-race-to-the-bottom-ai-safety", "t": "Race to the Bottom in AI Safety", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "The concern that competitive pressures among AI developers lead to progressively lower safety standards, as...", "l": "r", "k": ["race", "bottom", "safety", "concern", "competitive", "pressures", "among", "developers", "lead", "progressively", "lower", "standards", "organizations", "cut", "corners"]}, {"id": "term-racial-bias-in-ai", "t": "Racial Bias in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Systematic unfairness in AI system behavior related to race or ethnicity. Documented in facial recognition criminal...", "l": "r", "k": ["racial", "bias", "systematic", "unfairness", "system", "behavior", "related", "race", "ethnicity", "documented", "facial", "recognition", "criminal", "justice", "hiring"]}, {"id": "term-rack-unit", "t": "Rack Unit", "tg": ["Data Center", "Infrastructure", "Measurement"], "d": "hardware", "x": "Standard measure of vertical space in a server rack equal to 1.75 inches or 44.45mm. GPU servers typically occupy 4U to...", "l": "r", "k": ["rack", "unit", "standard", "measure", "vertical", "space", "server", "equal", "inches", "45mm", "gpu", "servers", "typically", "occupy", "depending"]}, {"id": "term-radam", "t": "RAdam", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Rectified Adam addresses the large variance in early training steps of the Adam optimizer by introducing a variance...", "l": "r", "k": ["radam", "rectified", "adam", "addresses", "large", "variance", "early", "training", "steps", "optimizer", "introducing", "rectification", "term", "automatically", "adapts"]}, {"id": "term-rademacher-complexity", "t": "Rademacher Complexity", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A measure of the richness of a hypothesis class that quantifies how well functions in the class can fit random noise....", "l": "r", "k": ["rademacher", "complexity", "measure", "richness", "hypothesis", "class", "quantifies", "functions", "fit", "random", "noise", "provides", "tighter", "generalization", "bounds"]}, {"id": "term-radfm", "t": "RadFM", "tg": ["Models", "Technical", "Medical", "Vision", "NLP"], "d": "models", "x": "A radiology foundation model that handles multi-modal medical imaging tasks including report generation and visual...", "l": "r", "k": ["radfm", "radiology", "foundation", "model", "handles", "multi-modal", "medical", "imaging", "tasks", "including", "report", "generation", "visual", "question", "answering"]}, {"id": "term-radial-basis-function-kernel", "t": "Radial Basis Function Kernel", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A popular kernel function that measures similarity as an exponentially decaying function of the squared Euclidean...", "l": "r", "k": ["radial", "basis", "function", "kernel", "popular", "measures", "similarity", "exponentially", "decaying", "squared", "euclidean", "distance", "points", "bandwidth", "parameter"]}, {"id": "term-radimagenet", "t": "RadImageNet", "tg": ["Models", "Technical", "Medical", "Vision"], "d": "models", "x": "A large-scale medical imaging dataset and pre-trained model collection designed as a radiological alternative to...", "l": "r", "k": ["radimagenet", "large-scale", "medical", "imaging", "dataset", "pre-trained", "model", "collection", "designed", "radiological", "alternative", "imagenet", "transfer", "learning"]}, {"id": "term-radix-sort", "t": "Radix Sort", "tg": ["Algorithms", "Fundamentals", "Sorting"], "d": "algorithms", "x": "A non-comparative sorting algorithm that sorts integers by processing individual digits from least significant to most...", "l": "r", "k": ["radix", "sort", "non-comparative", "sorting", "algorithm", "sorts", "integers", "processing", "individual", "digits", "least", "significant", "achieves", "time", "complexity"]}, {"id": "term-raft-optical-flow", "t": "RAFT", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Recurrent All-Pairs Field Transforms, a deep learning architecture for optical flow estimation that uses 4D correlation...", "l": "r", "k": ["raft", "recurrent", "all-pairs", "field", "transforms", "deep", "learning", "architecture", "optical", "flow", "estimation", "uses", "correlation", "volumes", "gru-based"]}, {"id": "term-raft-consensus-algorithm", "t": "Raft Consensus Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A distributed consensus algorithm designed to be more understandable than Paxos while providing equivalent guarantees....", "l": "r", "k": ["raft", "consensus", "algorithm", "distributed", "designed", "understandable", "paxos", "providing", "equivalent", "guarantees", "decomposes", "leader", "election", "log", "replication"]}, {"id": "term-rag", "t": "RAG (Retrieval-Augmented Generation)", "tg": ["Architecture", "Accuracy"], "d": "models", "x": "A technique that combines AI generation with information retrieval from external sources. The model retrieves relevant...", "l": "r", "k": ["rag", "retrieval-augmented", "generation", "technique", "combines", "information", "retrieval", "external", "sources", "model", "retrieves", "relevant", "documents", "uses", "generate"]}, {"id": "term-ragas", "t": "RAGAS", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "Retrieval Augmented Generation Assessment, an evaluation framework that provides reference-free metrics for RAG...", "l": "r", "k": ["ragas", "retrieval", "augmented", "generation", "assessment", "evaluation", "framework", "provides", "reference-free", "metrics", "rag", "pipelines", "including", "faithfulness", "answer"]}, {"id": "term-rain-ai", "t": "Rain AI", "tg": ["Accelerator", "Startup", "Neuromorphic"], "d": "hardware", "x": "Neuromorphic computing startup developing brain-inspired AI chips that aim to dramatically reduce the energy required...", "l": "r", "k": ["rain", "neuromorphic", "computing", "startup", "developing", "brain-inspired", "chips", "aim", "dramatically", "reduce", "energy", "required", "computation", "mimicking", "biological"]}, {"id": "term-rainbow-dqn", "t": "Rainbow DQN", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An integrated DQN agent that combines six extensions: double Q-learning, prioritized replay, dueling architecture,...", "l": "r", "k": ["rainbow", "dqn", "integrated", "agent", "combines", "six", "extensions", "double", "q-learning", "prioritized", "replay", "dueling", "architecture", "multi-step", "returns"]}, {"id": "term-rainbow-dqn-algorithm", "t": "Rainbow DQN Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A reinforcement learning agent that combines six extensions to DQN: double Q-learning and prioritized replay and...", "l": "r", "k": ["rainbow", "dqn", "algorithm", "reinforcement", "learning", "agent", "combines", "six", "extensions", "double", "q-learning", "prioritized", "replay", "dueling", "networks"]}, {"id": "term-raj-reddy", "t": "Raj Reddy", "tg": ["History", "Pioneers"], "d": "history", "x": "Indian-American computer scientist who received the Turing Award in 1994 for pioneering work in speech recognition and...", "l": "r", "k": ["raj", "reddy", "indian-american", "computer", "scientist", "received", "turing", "award", "pioneering", "work", "speech", "recognition", "co-founded", "robotics", "institute"]}, {"id": "term-randaugment", "t": "RandAugment", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A simplified automated augmentation strategy that randomly applies a fixed number of augmentation operations from a...", "l": "r", "k": ["randaugment", "simplified", "automated", "augmentation", "strategy", "randomly", "applies", "fixed", "number", "operations", "predefined", "shared", "magnitude", "requiring", "hyperparameters"]}, {"id": "term-random-erasing", "t": "Random Erasing", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A data augmentation technique that randomly selects rectangular regions in training images and replaces their pixels...", "l": "r", "k": ["random", "erasing", "data", "augmentation", "technique", "randomly", "selects", "rectangular", "regions", "training", "images", "replaces", "pixels", "values", "mean"]}, {"id": "term-random-forest", "t": "Random Forest", "tg": ["Algorithm", "ML"], "d": "algorithms", "x": "An ensemble of decision trees that vote on predictions. Robust, interpretable, and works well on tabular data. Still...", "l": "r", "k": ["random", "forest", "ensemble", "decision", "trees", "vote", "predictions", "robust", "interpretable", "works", "tabular", "data", "widely", "despite", "deep"]}, {"id": "term-random-forest-history", "t": "Random Forest History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of random forest ensemble methods by Leo Breiman in 2001 combining bagging with random feature...", "l": "r", "k": ["random", "forest", "history", "development", "ensemble", "methods", "leo", "breiman", "combining", "bagging", "feature", "selection", "forests", "proved", "remarkably"]}, {"id": "term-random-forest-model", "t": "Random Forest Model", "tg": ["Models", "Fundamentals"], "d": "models", "x": "An ensemble learning method that trains multiple decision trees on random subsets of data and features then aggregates...", "l": "r", "k": ["random", "forest", "model", "ensemble", "learning", "method", "trains", "multiple", "decision", "trees", "subsets", "data", "features", "aggregates", "predictions"]}, {"id": "term-random-network-distillation", "t": "Random Network Distillation", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "An exploration method that uses the prediction error of a randomly initialized target network as an intrinsic reward...", "l": "r", "k": ["random", "network", "distillation", "exploration", "method", "uses", "prediction", "error", "randomly", "initialized", "target", "intrinsic", "reward", "signal", "states"]}, {"id": "term-random-projection", "t": "Random Projection", "tg": ["Vector Database", "Dimensionality Reduction"], "d": "general", "x": "A dimensionality reduction technique based on the Johnson-Lindenstrauss lemma that projects high-dimensional vectors...", "l": "r", "k": ["random", "projection", "dimensionality", "reduction", "technique", "based", "johnson-lindenstrauss", "lemma", "projects", "high-dimensional", "vectors", "onto", "lower-dimensional", "space", "matrices"]}, {"id": "term-random-projection-algorithm", "t": "Random Projection Algorithm", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A dimensionality reduction technique that projects data onto a random lower-dimensional subspace. The...", "l": "r", "k": ["random", "projection", "algorithm", "dimensionality", "reduction", "technique", "projects", "data", "onto", "lower-dimensional", "subspace", "johnson-lindenstrauss", "lemma", "guarantees", "pairwise"]}, {"id": "term-random-search", "t": "Random Search", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A hyperparameter tuning strategy that samples parameter combinations randomly from specified distributions, often...", "l": "r", "k": ["random", "search", "hyperparameter", "tuning", "strategy", "samples", "parameter", "combinations", "randomly", "specified", "distributions", "finding", "good", "configurations", "efficiently"]}, {"id": "term-random-search-for-hyperparameters", "t": "Random Search for Hyperparameters", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A hyperparameter optimization method that samples configurations randomly from specified distributions. Shown by...", "l": "r", "k": ["random", "search", "hyperparameters", "hyperparameter", "optimization", "method", "samples", "configurations", "randomly", "specified", "distributions", "shown", "bergstra", "bengio", "efficient"]}, {"id": "term-random-walk", "t": "Random Walk", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A stochastic process consisting of successive random steps on a graph or in a space. Used in graph algorithms like...", "l": "r", "k": ["random", "walk", "stochastic", "process", "consisting", "successive", "steps", "graph", "space", "algorithms", "deepwalk", "node2vec", "learning", "node", "embeddings"]}, {"id": "term-randomized-contraction-algorithm", "t": "Randomized Contraction Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "Karger's algorithm for finding minimum cuts by randomly contracting edges until only two vertices remain. Running the...", "l": "r", "k": ["randomized", "contraction", "algorithm", "karger", "finding", "minimum", "cuts", "randomly", "contracting", "edges", "vertices", "remain", "running", "log", "times"]}, {"id": "term-randomized-linear-algebra", "t": "Randomized Linear Algebra", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A family of algorithms that use random sampling or random projections to accelerate matrix computations. Includes...", "l": "r", "k": ["randomized", "linear", "algebra", "family", "algorithms", "random", "sampling", "projections", "accelerate", "matrix", "computations", "includes", "svd", "sketching", "methods"]}, {"id": "term-randomized-response-algorithm", "t": "Randomized Response Algorithm", "tg": ["Algorithms", "Technical", "Privacy", "History"], "d": "algorithms", "x": "A survey technique that provides plausible deniability to respondents by instructing them to randomize their answers...", "l": "r", "k": ["randomized", "response", "algorithm", "survey", "technique", "provides", "plausible", "deniability", "respondents", "instructing", "randomize", "answers", "according", "known", "probability"]}, {"id": "term-range-tree-algorithm", "t": "Range Tree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A multi-level data structure for orthogonal range queries that recursively partitions points by coordinate dimensions....", "l": "r", "k": ["range", "tree", "algorithm", "multi-level", "data", "structure", "orthogonal", "queries", "recursively", "partitions", "points", "coordinate", "dimensions", "answers", "d-dimensional"]}, {"id": "term-ransac-algorithm", "t": "RANSAC Algorithm", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "Random Sample Consensus is an iterative method for estimating model parameters from data containing outliers....", "l": "r", "k": ["ransac", "algorithm", "random", "sample", "consensus", "iterative", "method", "estimating", "model", "parameters", "data", "containing", "outliers", "repeatedly", "samples"]}, {"id": "term-rate-limit", "t": "Rate Limit", "tg": ["API", "Technical"], "d": "general", "x": "Restrictions on API usage, typically measured in requests per minute or tokens per minute. Prevents abuse and ensures...", "l": "r", "k": ["rate", "limit", "restrictions", "api", "usage", "typically", "measured", "requests", "per", "minute", "tokens", "prevents", "abuse", "ensures", "fair"]}, {"id": "term-rate-distortion-optimization", "t": "Rate-Distortion Optimization", "tg": ["Algorithms", "Technical", "Information Theory"], "d": "algorithms", "x": "An information-theoretic framework that finds the minimum bit rate needed to represent a source within a specified...", "l": "r", "k": ["rate-distortion", "optimization", "information-theoretic", "framework", "finds", "minimum", "bit", "rate", "needed", "represent", "source", "within", "specified", "distortion", "level"]}, {"id": "term-ray-distributed-computing", "t": "Ray (Distributed Computing)", "tg": ["Distributed Training", "Open Source", "Framework"], "d": "hardware", "x": "Open-source distributed computing framework for scaling AI workloads from a laptop to a cluster. Provides Ray Train and...", "l": "r", "k": ["ray", "distributed", "computing", "open-source", "framework", "scaling", "workloads", "laptop", "cluster", "provides", "train", "serve", "training", "serving"]}, {"id": "term-ray-kurzweil", "t": "Ray Kurzweil", "tg": ["History", "Pioneers"], "d": "history", "x": "American inventor, author, and futurist who popularized the concept of the technological singularity, predicted...", "l": "r", "k": ["ray", "kurzweil", "american", "inventor", "author", "futurist", "popularized", "concept", "technological", "singularity", "predicted", "accelerating", "returns", "technology", "joined"]}, {"id": "term-rayleigh-quotient-iteration", "t": "Rayleigh Quotient Iteration", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An eigenvalue algorithm that combines inverse iteration with an adaptive shift equal to the Rayleigh quotient. Achieves...", "l": "r", "k": ["rayleigh", "quotient", "iteration", "eigenvalue", "algorithm", "combines", "inverse", "adaptive", "shift", "equal", "achieves", "cubic", "convergence", "symmetric", "matrices"]}, {"id": "term-rdma", "t": "RDMA", "tg": ["Distributed Computing", "Hardware"], "d": "hardware", "x": "Remote Direct Memory Access, a networking technology that enables direct data transfer between GPU memory on different...", "l": "r", "k": ["rdma", "remote", "direct", "memory", "access", "networking", "technology", "enables", "data", "transfer", "gpu", "different", "nodes", "without", "cpu"]}, {"id": "term-rdma-over-converged-ethernet", "t": "RDMA over Converged Ethernet", "tg": ["Networking", "Protocol", "RDMA"], "d": "hardware", "x": "Protocol enabling Remote Direct Memory Access over standard Ethernet networks. Provides InfiniBand-like low-latency...", "l": "r", "k": ["rdma", "converged", "ethernet", "protocol", "enabling", "remote", "direct", "memory", "access", "standard", "networks", "provides", "infiniband-like", "low-latency", "performance"]}, {"id": "term-re-identification", "t": "Re-Identification", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of matching the same person or vehicle across different camera views or time periods by learning...", "l": "r", "k": ["re-identification", "task", "matching", "person", "vehicle", "across", "different", "camera", "views", "time", "periods", "learning", "discriminative", "appearance", "embeddings"]}, {"id": "term-re-ranking", "t": "Re-Ranking", "tg": ["Retrieval", "Ranking"], "d": "general", "x": "A second-stage retrieval process that applies a more computationally expensive model to re-score and reorder an initial...", "l": "r", "k": ["re-ranking", "second-stage", "retrieval", "process", "applies", "computationally", "expensive", "model", "re-score", "reorder", "initial", "retrieved", "candidates", "improving", "precision"]}, {"id": "term-react", "t": "ReAct (Reasoning + Acting)", "tg": ["Framework", "Reasoning"], "d": "general", "x": "A prompting framework combining Reasoning and Acting. AI thinks through problems step-by-step, showing its reasoning...", "l": "r", "k": ["react", "reasoning", "acting", "prompting", "framework", "combining", "thinks", "problems", "step-by-step", "showing", "process", "transparently", "taking", "actions"]}, {"id": "term-react-pattern", "t": "ReAct Pattern", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A prompting paradigm that interleaves reasoning traces and action steps, allowing a language model to dynamically plan,...", "l": "r", "k": ["react", "pattern", "prompting", "paradigm", "interleaves", "reasoning", "traces", "action", "steps", "allowing", "language", "model", "dynamically", "plan", "execute"]}, {"id": "term-realnvp-for-anomaly-detection", "t": "RealNVP for Anomaly Detection", "tg": ["Models", "Technical"], "d": "models", "x": "An application of Real-valued Non-Volume Preserving normalizing flows to learn the density of normal data and flag...", "l": "r", "k": ["realnvp", "anomaly", "detection", "application", "real-valued", "non-volume", "preserving", "normalizing", "flows", "learn", "density", "normal", "data", "flag", "low-likelihood"]}, {"id": "term-realtoxicityprompts", "t": "RealToxicityPrompts", "tg": ["Benchmark", "NLP", "Safety"], "d": "datasets", "x": "A dataset of 100000 naturally occurring prompts for measuring the risk of neural toxic degeneration. Tests how often...", "l": "r", "k": ["realtoxicityprompts", "dataset", "naturally", "occurring", "prompts", "measuring", "risk", "neural", "toxic", "degeneration", "tests", "language", "models", "generate", "text"]}, {"id": "term-realworldqa", "t": "RealWorldQA", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A multimodal benchmark of questions about real-world images testing practical visual understanding. Covers everyday...", "l": "r", "k": ["realworldqa", "multimodal", "benchmark", "questions", "real-world", "images", "testing", "practical", "visual", "understanding", "covers", "everyday", "scenarios", "require", "common"]}, {"id": "term-rear-door-heat-exchanger", "t": "Rear-Door Heat Exchanger", "tg": ["Cooling", "Data Center"], "d": "hardware", "x": "Cooling device mounted on the back of a server rack that captures exhaust heat before it enters the room. Provides...", "l": "r", "k": ["rear-door", "heat", "exchanger", "cooling", "device", "mounted", "server", "rack", "captures", "exhaust", "enters", "room", "provides", "supplemental", "high-density"]}, {"id": "term-reasoning", "t": "Reasoning (AI)", "tg": ["Capability", "Prompting"], "d": "general", "x": "AI's ability to draw logical conclusions, follow multi-step chains, and solve complex problems. A key capability that...", "l": "r", "k": ["reasoning", "ability", "draw", "logical", "conclusions", "follow", "multi-step", "chains", "solve", "complex", "problems", "key", "capability", "distinguishes", "modern"]}, {"id": "term-recall", "t": "Recall", "tg": ["Metrics", "Evaluation"], "d": "datasets", "x": "A metric measuring the proportion of actual positives correctly identified. Important in search and information...", "l": "r", "k": ["recall", "metric", "measuring", "proportion", "actual", "positives", "correctly", "identified", "important", "search", "information", "retrieval", "missing", "relevant", "results"]}, {"id": "term-recall-at-k", "t": "Recall at K", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A retrieval metric that measures the proportion of all relevant documents in the corpus that appear within the top K...", "l": "r", "k": ["recall", "retrieval", "metric", "measures", "proportion", "relevant", "documents", "corpus", "appear", "within", "top", "retrieved", "results", "indicating", "comprehensively"]}, {"id": "term-recall-at-k-retrieval", "t": "Recall at K for Retrieval", "tg": ["Retrieval", "Evaluation"], "d": "datasets", "x": "A retrieval-specific metric measuring the proportion of all relevant documents that appear within the top K results...", "l": "r", "k": ["recall", "retrieval", "retrieval-specific", "metric", "measuring", "proportion", "relevant", "documents", "appear", "within", "top", "results", "returned", "vector", "search"]}, {"id": "term-receiver-operating-characteristic", "t": "Receiver Operating Characteristic", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A graphical analysis technique that plots classifier performance across all possible decision thresholds, showing the...", "l": "r", "k": ["receiver", "operating", "characteristic", "graphical", "analysis", "technique", "plots", "classifier", "performance", "across", "possible", "decision", "thresholds", "showing", "tradeoff"]}, {"id": "term-receptive-field", "t": "Receptive Field", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The region of the original input image that influences a particular neuron's activation in a deeper layer, growing...", "l": "r", "k": ["receptive", "field", "region", "original", "input", "image", "influences", "particular", "neuron", "activation", "deeper", "layer", "growing", "larger", "successive"]}, {"id": "term-reciprocal-rank-fusion", "t": "Reciprocal Rank Fusion", "tg": ["Retrieval", "Ranking"], "d": "general", "x": "A rank aggregation method that combines result lists from multiple retrieval systems by assigning each document a score...", "l": "r", "k": ["reciprocal", "rank", "fusion", "aggregation", "method", "combines", "result", "lists", "multiple", "retrieval", "systems", "assigning", "document", "score", "based"]}, {"id": "term-reclor", "t": "ReClor", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A reading comprehension dataset requiring logical reasoning derived from standardized graduate admission tests. Tests...", "l": "r", "k": ["reclor", "reading", "comprehension", "dataset", "requiring", "logical", "reasoning", "derived", "standardized", "graduate", "admission", "tests", "ability", "apply", "principles"]}, {"id": "term-reconfigurable-computing", "t": "Reconfigurable Computing", "tg": ["Architecture", "FPGA", "Flexibility"], "d": "hardware", "x": "Computing approach using hardware that can be reprogrammed after manufacturing such as FPGAs. Offers flexibility to...", "l": "r", "k": ["reconfigurable", "computing", "approach", "hardware", "reprogrammed", "manufacturing", "fpgas", "offers", "flexibility", "adapt", "different", "workload", "requirements"]}, {"id": "term-reconstruction-loss", "t": "Reconstruction Loss", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A loss function that measures how well a model can reconstruct its input from a compressed or encoded representation....", "l": "r", "k": ["reconstruction", "loss", "function", "measures", "model", "reconstruct", "input", "compressed", "encoded", "representation", "autoencoders", "variational", "implemented", "mse", "continuous"]}, {"id": "term-record", "t": "ReCoRD", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Reading Comprehension with Commonsense Reasoning Dataset a cloze-style reading comprehension dataset requiring...", "l": "r", "k": ["record", "reading", "comprehension", "commonsense", "reasoning", "dataset", "cloze-style", "requiring", "fill", "masked", "entities", "news", "articles"]}, {"id": "term-recourse", "t": "Recourse", "tg": ["Safety", "Governance"], "d": "safety", "x": "The ability of individuals affected by AI decisions to understand contest and seek correction of those decisions. A...", "l": "r", "k": ["recourse", "ability", "individuals", "affected", "decisions", "understand", "contest", "seek", "correction", "fundamental", "requirement", "accountability", "automated", "decision-making", "systems"]}, {"id": "term-rectified-flow", "t": "Rectified Flow", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative modeling approach that learns straight-line paths between noise and data distributions, enabling faster...", "l": "r", "k": ["rectified", "flow", "generative", "modeling", "approach", "learns", "straight-line", "paths", "noise", "data", "distributions", "enabling", "faster", "sampling", "curved"]}, {"id": "term-recurrent-policy", "t": "Recurrent Policy", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "An RL policy that uses recurrent neural network components (LSTM, GRU) to maintain internal memory across time steps....", "l": "r", "k": ["recurrent", "policy", "uses", "neural", "network", "components", "lstm", "gru", "maintain", "internal", "memory", "across", "time", "steps", "policies"]}, {"id": "term-recurrentgemma", "t": "RecurrentGemma", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A version of Google Gemma that replaces multi-head attention with linear recurrences (Griffin architecture) for more...", "l": "r", "k": ["recurrentgemma", "version", "google", "gemma", "replaces", "multi-head", "attention", "linear", "recurrences", "griffin", "architecture", "efficient", "inference", "long", "sequences"]}, {"id": "term-recursive-character-splitting", "t": "Recursive Character Splitting", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "A document chunking strategy that attempts to split text using a hierarchy of separators from paragraph breaks down to...", "l": "r", "k": ["recursive", "character", "splitting", "document", "chunking", "strategy", "attempts", "split", "text", "hierarchy", "separators", "paragraph", "breaks", "down", "individual"]}, {"id": "term-recursive-feature-elimination", "t": "Recursive Feature Elimination", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A feature selection method that repeatedly trains a model, ranks features by importance, and removes the least...", "l": "r", "k": ["recursive", "feature", "elimination", "selection", "method", "repeatedly", "trains", "model", "ranks", "features", "importance", "removes", "least", "important", "iterating"]}, {"id": "term-recursive-least-squares-filter", "t": "Recursive Least Squares Filter", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "An adaptive filter that minimizes a weighted sum of squared errors with exponential forgetting. Converges faster than...", "l": "r", "k": ["recursive", "least", "squares", "filter", "adaptive", "minimizes", "weighted", "sum", "squared", "errors", "exponential", "forgetting", "converges", "faster", "lms"]}, {"id": "term-recursive-prompting", "t": "Recursive Prompting", "tg": ["Prompt Engineering", "Architecture"], "d": "models", "x": "A prompting pattern where the output of one prompt call is used to construct the next prompt in a recursive loop,...", "l": "r", "k": ["recursive", "prompting", "pattern", "output", "prompt", "call", "construct", "next", "loop", "enabling", "model", "handle", "arbitrarily", "complex", "tasks"]}, {"id": "term-red-team", "t": "Red Team", "tg": ["Safety", "Testing"], "d": "safety", "x": "A group that tests AI systems by attempting to find vulnerabilities, bypass safety measures, or elicit harmful outputs....", "l": "r", "k": ["red", "team", "group", "tests", "systems", "attempting", "find", "vulnerabilities", "bypass", "safety", "measures", "elicit", "harmful", "outputs", "essential"]}, {"id": "term-red-teaming-in-ai", "t": "Red Teaming in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "The practice of systematically testing AI systems by attempting to elicit harmful undesired or unsafe outputs. Red...", "l": "r", "k": ["red", "teaming", "practice", "systematically", "testing", "systems", "attempting", "elicit", "harmful", "undesired", "unsafe", "outputs", "become", "standard", "safety"]}, {"id": "term-red-black-tree-algorithm", "t": "Red-Black Tree Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A self-balancing binary search tree that guarantees O(log n) time for insertion and deletion and search by maintaining...", "l": "r", "k": ["red-black", "tree", "algorithm", "self-balancing", "binary", "search", "guarantees", "log", "time", "insertion", "deletion", "maintaining", "coloring", "invariants", "node"]}, {"id": "term-reddit-dataset", "t": "Reddit Dataset", "tg": ["Benchmark", "Graph"], "d": "datasets", "x": "A graph dataset of Reddit posts where each post is a node and connections represent user co-commenting patterns. Used...", "l": "r", "k": ["reddit", "dataset", "graph", "posts", "post", "node", "connections", "represent", "user", "co-commenting", "patterns", "graph-based", "semi-supervised", "classification", "benchmarking"]}, {"id": "term-redfish", "t": "Redfish", "tg": ["Infrastructure", "Management", "Standard"], "d": "hardware", "x": "Modern RESTful API standard for server and infrastructure management replacing older IPMI protocols. Provides...", "l": "r", "k": ["redfish", "modern", "restful", "api", "standard", "server", "infrastructure", "management", "replacing", "older", "ipmi", "protocols", "provides", "standardized", "hardware"]}, {"id": "term-redpajama", "t": "RedPajama", "tg": ["Models", "Technical"], "d": "models", "x": "An open-source effort to reproduce the LLaMA training dataset and model. Provides a fully open training pipeline...", "l": "r", "k": ["redpajama", "open-source", "effort", "reproduce", "llama", "training", "dataset", "model", "provides", "fully", "open", "pipeline", "including", "trillion", "token"]}, {"id": "term-redpajama-v2", "t": "RedPajama V2", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "The second version of the RedPajama dataset containing over 30 trillion tokens from 5 languages with quality signals...", "l": "r", "k": ["redpajama", "version", "dataset", "containing", "trillion", "tokens", "languages", "quality", "signals", "deduplication", "annotations", "largest", "open", "pretraining", "datasets"]}, {"id": "term-reduce-on-plateau", "t": "Reduce on Plateau", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A learning rate scheduling strategy that reduces the learning rate by a factor when a monitored metric stops improving...", "l": "r", "k": ["reduce", "plateau", "learning", "rate", "scheduling", "strategy", "reduces", "factor", "monitored", "metric", "stops", "improving", "specified", "number", "epochs"]}, {"id": "term-reduce-scatter", "t": "Reduce-Scatter Operation", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A collective communication pattern that reduces data across all participants and distributes different chunks of the...", "l": "r", "k": ["reduce-scatter", "operation", "collective", "communication", "pattern", "reduces", "data", "across", "participants", "distributes", "different", "chunks", "result", "participant", "zero"]}, {"id": "term-redundant-power-supply", "t": "Redundant Power Supply", "tg": ["Power", "Reliability", "Infrastructure"], "d": "hardware", "x": "Dual or multiple power supply configuration where backup units automatically take over if the primary fails. Standard...", "l": "r", "k": ["redundant", "power", "supply", "dual", "multiple", "configuration", "backup", "units", "automatically", "take", "primary", "fails", "standard", "servers", "prevent"]}, {"id": "term-reed-solomon-coding", "t": "Reed-Solomon Coding", "tg": ["Algorithms", "Fundamentals", "Information Theory"], "d": "algorithms", "x": "An error-correcting code that adds redundancy to data enabling recovery from symbol errors and erasures. Used in CDs...", "l": "r", "k": ["reed-solomon", "coding", "error-correcting", "code", "adds", "redundancy", "data", "enabling", "recovery", "symbol", "errors", "erasures", "cds", "dvds", "codes"]}, {"id": "term-refcoco", "t": "RefCOCO", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "A referring expression comprehension dataset based on COCO images where models must localize objects described by...", "l": "r", "k": ["refcoco", "referring", "expression", "comprehension", "dataset", "based", "coco", "images", "models", "must", "localize", "objects", "described", "natural", "language"]}, {"id": "term-refinedweb", "t": "RefinedWeb", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A 5 trillion token web dataset created by Falcon/TII using careful deduplication and filtering of Common Crawl data....", "l": "r", "k": ["refinedweb", "trillion", "token", "web", "dataset", "created", "falcon", "tii", "careful", "deduplication", "filtering", "common", "crawl", "data", "demonstrates"]}, {"id": "term-reflexion-pattern", "t": "Reflexion Pattern", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An agent architecture where the LLM reflects on previous failed attempts by storing verbal feedback in an episodic...", "l": "r", "k": ["reflexion", "pattern", "agent", "architecture", "llm", "reflects", "previous", "failed", "attempts", "storing", "verbal", "feedback", "episodic", "memory", "reflections"]}, {"id": "term-reformer", "t": "Reformer", "tg": ["Models", "Technical"], "d": "models", "x": "A transformer variant that uses locality-sensitive hashing to reduce attention complexity from O(n^2) to O(n log n) and...", "l": "r", "k": ["reformer", "transformer", "variant", "uses", "locality-sensitive", "hashing", "reduce", "attention", "complexity", "log", "reversible", "residual", "layers", "memory", "usage"]}, {"id": "term-refusal", "t": "Refusal", "tg": ["Safety", "Behavior"], "d": "safety", "x": "When AI declines to answer a request due to safety guidelines. Well-calibrated refusals protect against harm while...", "l": "r", "k": ["refusal", "declines", "answer", "request", "due", "safety", "guidelines", "well-calibrated", "refusals", "protect", "against", "harm", "overly", "cautious", "reduce"]}, {"id": "term-region-growing-algorithm", "t": "Region Growing Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An image segmentation technique that starts from seed points and iteratively adds neighboring pixels that satisfy a...", "l": "r", "k": ["region", "growing", "algorithm", "image", "segmentation", "technique", "starts", "seed", "points", "iteratively", "adds", "neighboring", "pixels", "satisfy", "homogeneity"]}, {"id": "term-region-proposal-network", "t": "Region Proposal Network", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A fully convolutional network that slides over feature maps to generate object proposals (candidate bounding boxes)...", "l": "r", "k": ["region", "proposal", "network", "fully", "convolutional", "slides", "feature", "maps", "generate", "object", "proposals", "candidate", "bounding", "boxes", "objectness"]}, {"id": "term-regiongpt", "t": "RegionGPT", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A vision-language model capable of understanding and reasoning about arbitrary regions in images through region-aware...", "l": "r", "k": ["regiongpt", "vision-language", "model", "capable", "understanding", "reasoning", "arbitrary", "regions", "images", "region-aware", "visual", "instruction", "tuning"]}, {"id": "term-register-file", "t": "Register File", "tg": ["Memory", "Architecture", "Fundamentals"], "d": "hardware", "x": "Collection of processor registers that provide the fastest possible data access for active computations. GPU register...", "l": "r", "k": ["register", "file", "collection", "processor", "registers", "provide", "fastest", "possible", "data", "access", "active", "computations", "gpu", "files", "significantly"]}, {"id": "term-regnet", "t": "RegNet", "tg": ["Models", "Technical"], "d": "models", "x": "A family of network architectures derived from a structured design space that constrains network parameters to follow...", "l": "r", "k": ["regnet", "family", "network", "architectures", "derived", "structured", "design", "space", "constrains", "parameters", "follow", "simple", "linear", "rules", "provides"]}, {"id": "term-regression", "t": "Regression", "tg": ["ML Task", "Prediction"], "d": "general", "x": "A machine learning task that predicts continuous values (like prices or temperatures) rather than categories. Common...", "l": "r", "k": ["regression", "machine", "learning", "task", "predicts", "continuous", "values", "prices", "temperatures", "rather", "categories", "common", "algorithms", "include", "linear"]}, {"id": "term-regression-discontinuity-design", "t": "Regression Discontinuity Design", "tg": ["Algorithms", "Technical", "Causal"], "d": "algorithms", "x": "A quasi-experimental method that estimates causal effects at a threshold where treatment assignment changes...", "l": "r", "k": ["regression", "discontinuity", "design", "quasi-experimental", "method", "estimates", "causal", "effects", "threshold", "treatment", "assignment", "changes", "discontinuously", "compares", "outcomes"]}, {"id": "term-regression-to-bias", "t": "Regression to Bias", "tg": ["Safety", "Technical"], "d": "safety", "x": "The tendency for AI systems to revert to biased behavior over time as new data or user interactions reintroduce...", "l": "r", "k": ["regression", "bias", "tendency", "systems", "revert", "biased", "behavior", "time", "data", "user", "interactions", "reintroduce", "patterns", "historical", "discrimination"]}, {"id": "term-regret", "t": "Regret", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "In online learning and bandit problems, the cumulative difference between the reward obtained by an algorithm and the...", "l": "r", "k": ["regret", "online", "learning", "bandit", "problems", "cumulative", "difference", "reward", "obtained", "algorithm", "always", "choosing", "optimal", "action", "hindsight"]}, {"id": "term-regret-bound", "t": "Regret Bound", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "A theoretical guarantee on the cumulative difference between the reward obtained by an RL algorithm and the reward of...", "l": "r", "k": ["regret", "bound", "theoretical", "guarantee", "cumulative", "difference", "reward", "obtained", "algorithm", "optimal", "policy", "steps", "bounds", "characterize", "efficiency"]}, {"id": "term-regret-matching-algorithm", "t": "Regret Matching Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A game-theoretic algorithm that selects actions proportional to their cumulative positive regret. Converges to a...", "l": "r", "k": ["regret", "matching", "algorithm", "game-theoretic", "selects", "actions", "proportional", "cumulative", "positive", "converges", "correlated", "equilibrium", "general", "games", "forms"]}, {"id": "term-regularization", "t": "Regularization", "tg": ["Training", "Technique"], "d": "general", "x": "Techniques to prevent overfitting by adding constraints during training. Includes dropout, weight decay, and early...", "l": "r", "k": ["regularization", "techniques", "prevent", "overfitting", "adding", "constraints", "training", "includes", "dropout", "weight", "decay", "early", "stopping", "improves", "generalization"]}, {"id": "term-regulatory-impact-assessment-for-ai", "t": "Regulatory Impact Assessment for AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "A structured analysis of the potential costs and benefits of proposed AI regulations. Helps policymakers balance...", "l": "r", "k": ["regulatory", "impact", "assessment", "structured", "analysis", "potential", "costs", "benefits", "proposed", "regulations", "helps", "policymakers", "balance", "innovation", "promotion"]}, {"id": "term-regulatory-technology-for-ai", "t": "Regulatory Technology for AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "Software tools and platforms that help organizations comply with AI regulations. Includes automated documentation bias...", "l": "r", "k": ["regulatory", "technology", "software", "tools", "platforms", "help", "organizations", "comply", "regulations", "includes", "automated", "documentation", "bias", "monitoring", "audit"]}, {"id": "term-reinforce-algorithm", "t": "REINFORCE Algorithm", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A foundational Monte Carlo policy gradient algorithm that updates policy parameters proportionally to the return...", "l": "r", "k": ["reinforce", "algorithm", "foundational", "monte", "carlo", "policy", "gradient", "updates", "parameters", "proportionally", "return", "multiplied", "log-probability", "action", "taken"]}, {"id": "term-reinforcement-learning", "t": "Reinforcement Learning (RL)", "tg": ["Learning Type", "Training"], "d": "general", "x": "A learning paradigm where agents learn by receiving rewards or penalties for actions. Used in RLHF to align LLMs with...", "l": "r", "k": ["reinforcement", "learning", "paradigm", "agents", "learn", "receiving", "rewards", "penalties", "actions", "rlhf", "align", "llms", "human", "preferences"]}, {"id": "term-reinforcement-learning-history", "t": "Reinforcement Learning History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of reinforcement learning from early work by Arthur Samuel on checkers in 1959 through temporal...", "l": "r", "k": ["reinforcement", "learning", "history", "development", "early", "work", "arthur", "samuel", "checkers", "temporal", "difference", "sutton", "deep", "breakthroughs", "dqn"]}, {"id": "term-reinforcement-learning-safety", "t": "Reinforcement Learning Safety", "tg": ["Safety", "Technical"], "d": "safety", "x": "Safety challenges specific to reinforcement learning including reward hacking unsafe exploration negative side effects...", "l": "r", "k": ["reinforcement", "learning", "safety", "challenges", "specific", "including", "reward", "hacking", "unsafe", "exploration", "negative", "side", "effects", "distributional", "shift"]}, {"id": "term-rejection-sampling", "t": "Rejection Sampling", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A basic Monte Carlo method for generating samples from a target distribution by sampling from a proposal distribution...", "l": "r", "k": ["rejection", "sampling", "basic", "monte", "carlo", "method", "generating", "samples", "target", "distribution", "proposal", "accepting", "rejecting", "based", "comparison"]}, {"id": "term-rejection-sampling-for-alignment", "t": "Rejection Sampling for Alignment", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A training data curation technique that generates multiple responses per prompt and keeps only those above a reward...", "l": "r", "k": ["rejection", "sampling", "alignment", "training", "data", "curation", "technique", "generates", "multiple", "responses", "per", "prompt", "keeps", "reward", "threshold"]}, {"id": "term-relabeling", "t": "Relabeling", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A data augmentation technique in RL that modifies components of stored transitions (such as goals, rewards, or actions)...", "l": "r", "k": ["relabeling", "data", "augmentation", "technique", "modifies", "components", "stored", "transitions", "goals", "rewards", "actions", "generate", "additional", "training", "signal"]}, {"id": "term-relation-extraction", "t": "Relation Extraction", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying and classifying semantic relationships between entities mentioned in text, such as extracting...", "l": "r", "k": ["relation", "extraction", "task", "identifying", "classifying", "semantic", "relationships", "entities", "mentioned", "text", "extracting", "person", "works", "specific", "organization"]}, {"id": "term-relational-reinforcement-learning", "t": "Relational Reinforcement Learning", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A reinforcement learning approach that uses relational representations such as first-order logic to represent states...", "l": "r", "k": ["relational", "reinforcement", "learning", "approach", "uses", "representations", "first-order", "logic", "represent", "states", "actions", "enables", "generalization", "across", "different"]}, {"id": "term-relative-position-encoding", "t": "Relative Position Encoding", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A position encoding scheme that represents the distance between tokens rather than their absolute positions. Allows the...", "l": "r", "k": ["relative", "position", "encoding", "scheme", "represents", "distance", "tokens", "rather", "absolute", "positions", "allows", "model", "generalize", "different", "sequence"]}, {"id": "term-relative-positional-encoding", "t": "Relative Positional Encoding", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A positional encoding scheme that encodes the relative distance between tokens rather than absolute positions, enabling...", "l": "r", "k": ["relative", "positional", "encoding", "scheme", "encodes", "distance", "tokens", "rather", "absolute", "positions", "enabling", "better", "generalization", "sequence", "lengths"]}, {"id": "term-relay-computer", "t": "Relay Computer", "tg": ["Historical", "Component", "Fundamentals"], "d": "hardware", "x": "Early computing device using electromagnetic relays as switching elements. The Harvard Mark I and Konrad Zuse Z3 used...", "l": "r", "k": ["relay", "computer", "early", "computing", "device", "electromagnetic", "relays", "switching", "elements", "harvard", "mark", "konrad", "zuse", "electronic", "vacuum"]}, {"id": "term-relevance-score", "t": "Relevance Score", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that measures how well a generated response addresses the input query or matches the intended topic,...", "l": "r", "k": ["relevance", "score", "metric", "measures", "generated", "response", "addresses", "input", "query", "matches", "intended", "topic", "evaluating", "content", "appropriateness"]}, {"id": "term-reliance-calibration", "t": "Reliance Calibration", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The process of helping users develop appropriate levels of trust in AI system outputs based on the system's actual...", "l": "r", "k": ["reliance", "calibration", "process", "helping", "users", "develop", "appropriate", "levels", "trust", "system", "outputs", "based", "actual", "capabilities", "limitations"]}, {"id": "term-relu", "t": "ReLU (Rectified Linear Unit)", "tg": ["Architecture", "Function"], "d": "models", "x": "A simple activation function that outputs zero for negative inputs and the input itself for positives. Widely used due...", "l": "r", "k": ["relu", "rectified", "linear", "unit", "simple", "activation", "function", "outputs", "zero", "negative", "inputs", "input", "itself", "positives", "widely"]}, {"id": "term-rembert", "t": "RemBERT", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A rethinking of multilingual BERT that decouples input and output embedding sizes to achieve better cross-lingual...", "l": "r", "k": ["rembert", "rethinking", "multilingual", "bert", "decouples", "input", "output", "embedding", "sizes", "achieve", "better", "cross-lingual", "transfer", "improved", "parameter"]}, {"id": "term-renewable-energy-for-data-centers", "t": "Renewable Energy for Data Centers", "tg": ["Sustainability", "Data Center", "Energy"], "d": "hardware", "x": "Use of solar wind hydroelectric or other renewable energy sources to power AI data centers. Major cloud providers are...", "l": "r", "k": ["renewable", "energy", "data", "centers", "solar", "wind", "hydroelectric", "sources", "power", "major", "cloud", "providers", "investing", "heavily", "offset"]}, {"id": "term-reparameterization-trick", "t": "Reparameterization Trick", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A technique that enables gradient-based optimization through stochastic sampling by expressing random variables as...", "l": "r", "k": ["reparameterization", "trick", "technique", "enables", "gradient-based", "optimization", "stochastic", "sampling", "expressing", "random", "variables", "deterministic", "functions", "parameters", "independent"]}, {"id": "term-repetition-penalty", "t": "Repetition Penalty", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A decoding parameter that reduces the probability of tokens that have already appeared in the generated text,...", "l": "r", "k": ["repetition", "penalty", "decoding", "parameter", "reduces", "probability", "tokens", "appeared", "generated", "text", "preventing", "model", "producing", "repetitive", "phrases"]}, {"id": "term-repetition-rate", "t": "Repetition Rate", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that quantifies the frequency of repeated phrases, sentences, or patterns within generated text, used to...", "l": "r", "k": ["repetition", "rate", "metric", "quantifies", "frequency", "repeated", "phrases", "sentences", "patterns", "within", "generated", "text", "detect", "penalize", "degenerate"]}, {"id": "term-rephrase-and-respond", "t": "Rephrase and Respond", "tg": ["Prompt Engineering", "Clarification"], "d": "general", "x": "A prompting method that asks the model to first rephrase the input question in its own words before answering it,...", "l": "r", "k": ["rephrase", "respond", "prompting", "method", "asks", "model", "input", "question", "words", "answering", "improving", "comprehension", "reducing", "misinterpretation", "ensuring"]}, {"id": "term-replay-buffer", "t": "Replay Buffer", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A data structure (typically a fixed-size circular buffer) that stores past experience tuples for sampling during...", "l": "r", "k": ["replay", "buffer", "data", "structure", "typically", "fixed-size", "circular", "stores", "past", "experience", "tuples", "sampling", "off-policy", "training", "buffers"]}, {"id": "term-replication-vector-databases", "t": "Replication in Vector Databases", "tg": ["Vector Database", "Infrastructure"], "d": "hardware", "x": "The maintenance of multiple copies of a vector index across different nodes to provide fault tolerance and increased...", "l": "r", "k": ["replication", "vector", "databases", "maintenance", "multiple", "copies", "index", "across", "different", "nodes", "provide", "fault", "tolerance", "increased", "read"]}, {"id": "term-replit-code-v15", "t": "Replit Code V1.5", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A 3.3 billion parameter code completion model from Replit trained on 30 programming languages for real-time code...", "l": "r", "k": ["replit", "code", "billion", "parameter", "completion", "model", "trained", "programming", "languages", "real-time", "suggestion", "development", "environments"]}, {"id": "term-repobench", "t": "RepoBench", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A benchmark for repository-level code completion evaluating the ability to use cross-file context. Tests real-world...", "l": "r", "k": ["repobench", "benchmark", "repository-level", "code", "completion", "evaluating", "ability", "cross-file", "context", "tests", "real-world", "coding", "scenarios", "requiring", "understanding"]}, {"id": "term-repoeval", "t": "RepoEval", "tg": ["Benchmark", "Code", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating code completion and generation at the repository level. Tests the ability to generate code...", "l": "r", "k": ["repoeval", "benchmark", "evaluating", "code", "completion", "generation", "repository", "level", "tests", "ability", "generate", "consistent", "project", "conventions", "dependencies"]}, {"id": "term-representation-engineering", "t": "Representation Engineering", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A technique for understanding and controlling LLM behavior by identifying and manipulating specific directions in the...", "l": "r", "k": ["representation", "engineering", "technique", "understanding", "controlling", "llm", "behavior", "identifying", "manipulating", "specific", "directions", "model", "activation", "space", "correspond"]}, {"id": "term-representation-learning", "t": "Representation Learning", "tg": ["Concept", "Deep Learning"], "d": "models", "x": "Learning useful features automatically from data rather than engineering them manually. A key strength of deep learning...", "l": "r", "k": ["representation", "learning", "useful", "features", "automatically", "data", "rather", "engineering", "manually", "key", "strength", "deep", "enables", "transfer"]}, {"id": "term-representation-learning-rl", "t": "Representation Learning in RL", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "Methods for learning compact, informative state representations from high-dimensional observations (like images) to...", "l": "r", "k": ["representation", "learning", "methods", "compact", "informative", "state", "representations", "high-dimensional", "observations", "images", "improve", "efficiency", "techniques", "include", "contrastive"]}, {"id": "term-representational-bias", "t": "Representational Bias", "tg": ["Safety", "Technical"], "d": "safety", "x": "Bias that arises when certain groups or perspectives are underrepresented in AI training data or development teams...", "l": "r", "k": ["representational", "bias", "arises", "certain", "groups", "perspectives", "underrepresented", "training", "data", "development", "teams", "leading", "systems", "perform", "poorly"]}, {"id": "term-representational-harm", "t": "Representational Harm", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Harm that occurs when an AI system reinforces stereotypes, demeans, or erases particular social groups through its...", "l": "r", "k": ["representational", "harm", "occurs", "system", "reinforces", "stereotypes", "demeans", "erases", "particular", "social", "groups", "outputs", "direct", "resource", "allocation"]}, {"id": "term-reproducibility-in-ai", "t": "Reproducibility in AI", "tg": ["Safety", "Technical"], "d": "safety", "x": "The ability to replicate AI research results and system behaviors under the same conditions. Essential for safety...", "l": "r", "k": ["reproducibility", "ability", "replicate", "research", "results", "system", "behaviors", "conditions", "essential", "safety", "verification", "scientific", "validation", "regulatory", "compliance"]}, {"id": "term-reputational-risk-from-ai", "t": "Reputational Risk from AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "The potential damage to an organization's reputation from AI system failures biases or harmful outputs. Increasingly...", "l": "r", "k": ["reputational", "risk", "potential", "damage", "organization", "reputation", "system", "failures", "biases", "harmful", "outputs", "increasingly", "material", "public", "awareness"]}, {"id": "term-repvit", "t": "RepViT", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A lightweight vision Transformer for mobile deployment that reparameterizes ViT components into efficient convolutional...", "l": "r", "k": ["repvit", "lightweight", "vision", "transformer", "mobile", "deployment", "reparameterizes", "vit", "components", "efficient", "convolutional", "operations", "fast", "on-device", "inference"]}, {"id": "term-request-scheduling", "t": "Request Scheduling", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The algorithm that determines the order and priority of processing incoming inference requests on limited GPU...", "l": "r", "k": ["request", "scheduling", "algorithm", "determines", "order", "priority", "processing", "incoming", "inference", "requests", "limited", "gpu", "resources", "strategies", "optimize"]}, {"id": "term-reram", "t": "ReRAM", "tg": ["Memory", "Emerging", "Non-Volatile"], "d": "hardware", "x": "Resistive Random Access Memory that stores data by changing the resistance of a material. Investigated for both memory...", "l": "r", "k": ["reram", "resistive", "random", "access", "memory", "stores", "data", "changing", "resistance", "material", "investigated", "in-memory", "computing", "applications", "neural"]}, {"id": "term-reranking", "t": "Reranking", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A two-stage retrieval approach where an initial fast retriever fetches candidate documents, and a more powerful...", "l": "r", "k": ["reranking", "two-stage", "retrieval", "approach", "initial", "fast", "retriever", "fetches", "candidate", "documents", "powerful", "cross-encoder", "model", "rescores", "reorders"]}, {"id": "term-reservoir-sampling-algorithm", "t": "Reservoir Sampling Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "An algorithm for randomly selecting k items from a stream of unknown length where each element has equal probability of...", "l": "r", "k": ["reservoir", "sampling", "algorithm", "randomly", "selecting", "items", "stream", "unknown", "length", "element", "equal", "probability", "selection", "maintains", "replaces"]}, {"id": "term-residual-analysis", "t": "Residual Analysis", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "The examination of residuals (differences between observed and predicted values) to assess model fit, check assumptions...", "l": "r", "k": ["residual", "analysis", "examination", "residuals", "differences", "observed", "predicted", "values", "assess", "model", "fit", "check", "assumptions", "normality", "homoscedasticity"]}, {"id": "term-residual-connection", "t": "Residual Connection", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An additive skip connection where the input to a layer block is added element-wise to the block's output, allowing the...", "l": "r", "k": ["residual", "connection", "additive", "skip", "input", "layer", "block", "added", "element-wise", "output", "allowing", "network", "learn", "mappings", "rather"]}, {"id": "term-residual-learning", "t": "Residual Learning", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The principle of learning additive residual functions with reference to the layer inputs rather than learning...", "l": "r", "k": ["residual", "learning", "principle", "additive", "functions", "reference", "layer", "inputs", "rather", "unreferenced", "making", "easier", "optimize", "deep", "networks"]}, {"id": "term-residual-risk", "t": "Residual Risk", "tg": ["Safety", "Governance"], "d": "safety", "x": "The risk that remains after all identified mitigation measures have been applied to an AI system. Must be documented...", "l": "r", "k": ["residual", "risk", "remains", "identified", "mitigation", "measures", "applied", "system", "must", "documented", "accepted", "transferred", "part", "responsible", "deployment"]}, {"id": "term-resnet", "t": "ResNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Residual Network, a deep convolutional neural network architecture that introduces skip connections to enable training...", "l": "r", "k": ["resnet", "residual", "network", "deep", "convolutional", "neural", "architecture", "introduces", "skip", "connections", "enable", "training", "networks", "mitigating", "vanishing"]}, {"id": "term-resnet-101", "t": "ResNet-101", "tg": ["Models", "Technical"], "d": "models", "x": "A 101-layer variant of the Residual Network achieving higher accuracy than ResNet-50 at the cost of additional...", "l": "r", "k": ["resnet-101", "101-layer", "variant", "residual", "network", "achieving", "higher", "accuracy", "resnet-50", "cost", "additional", "computation", "commonly", "backbone", "object"]}, {"id": "term-resnet-152", "t": "ResNet-152", "tg": ["Models", "Technical"], "d": "models", "x": "The deepest standard variant of the Residual Network with 152 layers. Demonstrates that deeper networks with skip...", "l": "r", "k": ["resnet-152", "deepest", "standard", "variant", "residual", "network", "layers", "demonstrates", "deeper", "networks", "skip", "connections", "continue", "improve", "accuracy"]}, {"id": "term-resnet-50", "t": "ResNet-50", "tg": ["Models", "Technical"], "d": "models", "x": "A 50-layer variant of the Residual Network architecture widely used as a baseline for image classification and feature...", "l": "r", "k": ["resnet-50", "50-layer", "variant", "residual", "network", "architecture", "widely", "baseline", "image", "classification", "feature", "extraction", "contains", "bottleneck", "blocks"]}, {"id": "term-resnext", "t": "ResNeXt", "tg": ["Models", "Technical"], "d": "models", "x": "An extension of ResNet that introduces cardinality as an additional dimension through grouped convolutions. Proposed by...", "l": "r", "k": ["resnext", "extension", "resnet", "introduces", "cardinality", "additional", "dimension", "grouped", "convolutions", "proposed", "xie", "achieves", "better", "accuracy", "similar"]}, {"id": "term-resolution-theorem-proving", "t": "Resolution Theorem Proving", "tg": ["History", "Fundamentals"], "d": "history", "x": "An inference rule that produces a new clause implied by two clauses containing complementary literals. Introduced by...", "l": "r", "k": ["resolution", "theorem", "proving", "inference", "rule", "produces", "clause", "implied", "clauses", "containing", "complementary", "literals", "introduced", "john", "alan"]}, {"id": "term-responsible-ai", "t": "Responsible AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "An approach to developing and deploying AI systems that emphasizes ethical considerations, fairness, transparency,...", "l": "r", "k": ["responsible", "approach", "developing", "deploying", "systems", "emphasizes", "ethical", "considerations", "fairness", "transparency", "accountability", "societal", "benefit", "throughout", "entire"]}, {"id": "term-responsible-disclosure-for-ai", "t": "Responsible Disclosure for AI", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "The practice of privately reporting discovered vulnerabilities or dangerous capabilities in AI systems to the developer...", "l": "r", "k": ["responsible", "disclosure", "practice", "privately", "reporting", "discovered", "vulnerabilities", "dangerous", "capabilities", "systems", "developer", "public", "allowing", "time", "mitigation"]}, {"id": "term-restricted-boltzmann-machine", "t": "Restricted Boltzmann Machine", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative stochastic neural network with a bipartite structure of visible and hidden units with no intra-layer...", "l": "r", "k": ["restricted", "boltzmann", "machine", "generative", "stochastic", "neural", "network", "bipartite", "structure", "visible", "hidden", "units", "intra-layer", "connections", "trained"]}, {"id": "term-rete-algorithm", "t": "Rete Algorithm", "tg": ["History", "Fundamentals"], "d": "history", "x": "An efficient pattern matching algorithm developed by Charles Forgy in 1979 for production rule systems. The Rete...", "l": "r", "k": ["rete", "algorithm", "efficient", "pattern", "matching", "developed", "charles", "forgy", "production", "rule", "systems", "dramatically", "improved", "performance", "rule-based"]}, {"id": "term-retfound", "t": "RETFound", "tg": ["Models", "Technical", "Medical", "Vision"], "d": "models", "x": "A retinal foundation model pre-trained with self-supervised learning on large-scale retinal imaging data for ophthalmic...", "l": "r", "k": ["retfound", "retinal", "foundation", "model", "pre-trained", "self-supervised", "learning", "large-scale", "imaging", "data", "ophthalmic", "disease", "detection", "prognosis"]}, {"id": "term-retinanet", "t": "RetinaNet", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A single-stage object detection model that combines a Feature Pyramid Network backbone with focal loss, achieving...", "l": "r", "k": ["retinanet", "single-stage", "object", "detection", "model", "combines", "feature", "pyramid", "network", "backbone", "focal", "loss", "achieving", "accuracy", "comparable"]}, {"id": "term-retnet", "t": "RetNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Retentive Network, an architecture that supports parallel training, recurrent inference, and chunk-wise recurrent...", "l": "r", "k": ["retnet", "retentive", "network", "architecture", "supports", "parallel", "training", "recurrent", "inference", "chunk-wise", "computation", "retention", "mechanism", "replaces", "standard"]}, {"id": "term-retrieval", "t": "Retrieval", "tg": ["Process", "Search"], "d": "general", "x": "Finding relevant information from a database or corpus. In RAG, retrieval brings external knowledge into the generation...", "l": "r", "k": ["retrieval", "finding", "relevant", "information", "database", "corpus", "rag", "brings", "external", "knowledge", "generation", "process", "accurate", "responses"]}, {"id": "term-retrieval-evaluation", "t": "Retrieval Evaluation", "tg": ["Retrieval", "Evaluation"], "d": "datasets", "x": "The systematic assessment of retrieval system quality using metrics such as recall, precision, NDCG, and MRR applied to...", "l": "r", "k": ["retrieval", "evaluation", "systematic", "assessment", "system", "quality", "metrics", "recall", "precision", "ndcg", "mrr", "applied", "ranked", "result", "lists"]}, {"id": "term-retrieval-head", "t": "Retrieval Head", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Specific attention heads within a transformer that specialize in copying or retrieving information from the context,...", "l": "r", "k": ["retrieval", "head", "specific", "attention", "heads", "within", "transformer", "specialize", "copying", "retrieving", "information", "context", "playing", "crucial", "role"]}, {"id": "term-retrieval-augmented-fine-tuning", "t": "Retrieval-Augmented Fine-Tuning", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A training approach that fine-tunes a language model with retrieval-augmented examples, teaching the model to...", "l": "r", "k": ["retrieval-augmented", "fine-tuning", "training", "approach", "fine-tunes", "language", "model", "examples", "teaching", "effectively", "incorporate", "retrieved", "context", "responses"]}, {"id": "term-retrieval-augmented-generation", "t": "Retrieval-Augmented Generation", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A technique that enhances language model outputs by retrieving relevant documents from an external knowledge base and...", "l": "r", "k": ["retrieval-augmented", "generation", "technique", "enhances", "language", "model", "outputs", "retrieving", "relevant", "documents", "external", "knowledge", "base", "conditioning", "retrieved"]}, {"id": "term-retrieval-augmented-generation-history", "t": "Retrieval-Augmented Generation History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of RAG from the original paper by Lewis et al. at Facebook AI in 2020 to widespread adoption in...", "l": "r", "k": ["retrieval-augmented", "generation", "history", "development", "rag", "original", "paper", "lewis", "facebook", "widespread", "adoption", "enterprise", "combines", "retrieval", "external"]}, {"id": "term-retrieval-augmented-lm", "t": "Retrieval-Augmented Language Model", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A language model architecture that incorporates a retrieval component to fetch relevant documents from an external...", "l": "r", "k": ["retrieval-augmented", "language", "model", "architecture", "incorporates", "retrieval", "component", "fetch", "relevant", "documents", "external", "corpus", "generation", "grounding", "responses"]}, {"id": "term-retrieval-augmented-prompting", "t": "Retrieval-Augmented Prompting", "tg": ["Prompt Engineering", "Retrieval"], "d": "general", "x": "A technique that dynamically retrieves relevant documents, examples, or knowledge from an external corpus and...", "l": "r", "k": ["retrieval-augmented", "prompting", "technique", "dynamically", "retrieves", "relevant", "documents", "examples", "knowledge", "external", "corpus", "incorporates", "prompt", "context", "generation"]}, {"id": "term-retrograde-analysis", "t": "Retrograde Analysis", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A technique for solving endgame positions in games by working backward from terminal states. Computes exact values for...", "l": "r", "k": ["retrograde", "analysis", "technique", "solving", "endgame", "positions", "games", "working", "backward", "terminal", "states", "computes", "exact", "values", "tablebase"]}, {"id": "term-return", "t": "Return", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The cumulative discounted sum of future rewards from a given time step, representing the total long-term value an agent...", "l": "r", "k": ["return", "cumulative", "discounted", "sum", "future", "rewards", "given", "time", "step", "representing", "total", "long-term", "value", "agent", "receives"]}, {"id": "term-reuters-21578", "t": "Reuters-21578", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A collection of 21578 news articles from Reuters categorized into 135 topics. One of the earliest and most widely used...", "l": "r", "k": ["reuters-21578", "collection", "news", "articles", "reuters", "categorized", "topics", "earliest", "widely", "text", "classification", "benchmark", "datasets"]}, {"id": "term-revised-simplex-method", "t": "Revised Simplex Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An efficient implementation of the simplex algorithm that maintains and updates the basis inverse rather than the full...", "l": "r", "k": ["revised", "simplex", "method", "efficient", "implementation", "algorithm", "maintains", "updates", "basis", "inverse", "rather", "full", "tableau", "reduces", "computation"]}, {"id": "term-reward", "t": "Reward", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A scalar signal received by an agent from the environment after taking an action, indicating how good or bad the...", "l": "r", "k": ["reward", "scalar", "signal", "received", "agent", "environment", "taking", "action", "indicating", "good", "bad", "outcome", "rewards", "drive", "learning"]}, {"id": "term-reward-clipping", "t": "Reward Clipping", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "A preprocessing technique that bounds reward values to a fixed range (commonly [-1, 1]) to stabilize training across...", "l": "r", "k": ["reward", "clipping", "preprocessing", "technique", "bounds", "values", "fixed", "range", "commonly", "stabilize", "training", "across", "diverse", "environments", "original"]}, {"id": "term-reward-decomposition", "t": "Reward Decomposition", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "Techniques that break a complex reward signal into simpler components that are easier to learn from, enabling more...", "l": "r", "k": ["reward", "decomposition", "techniques", "break", "complex", "signal", "simpler", "components", "easier", "learn", "enabling", "interpretable", "efficient", "training", "decomposed"]}, {"id": "term-reward-engineering", "t": "Reward Engineering", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "The process of designing reward functions that accurately capture desired agent behavior, balancing specificity with...", "l": "r", "k": ["reward", "engineering", "process", "designing", "functions", "accurately", "capture", "desired", "agent", "behavior", "balancing", "specificity", "generality", "poor", "lead"]}, {"id": "term-reward-hacking", "t": "Reward Hacking", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A failure mode in reinforcement learning from human feedback where the policy model exploits weaknesses in the reward...", "l": "r", "k": ["reward", "hacking", "failure", "mode", "reinforcement", "learning", "human", "feedback", "policy", "model", "exploits", "weaknesses", "achieve", "high", "scores"]}, {"id": "term-reward-machine", "t": "Reward Machine", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "A finite-state automaton that specifies reward functions based on high-level events or propositional symbols, enabling...", "l": "r", "k": ["reward", "machine", "finite-state", "automaton", "specifies", "functions", "based", "high-level", "events", "propositional", "symbols", "enabling", "structured", "specification", "complex"]}, {"id": "term-reward-model", "t": "Reward Model", "tg": ["Training", "Alignment"], "d": "safety", "x": "A model trained to score AI outputs based on human preferences. Used in RLHF to guide language models toward more...", "l": "r", "k": ["reward", "model", "trained", "score", "outputs", "based", "human", "preferences", "rlhf", "guide", "language", "models", "toward", "helpful", "safe"]}, {"id": "term-reward-normalization", "t": "Reward Normalization", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "The practice of scaling reward signals to have consistent magnitude across different environments or during training,...", "l": "r", "k": ["reward", "normalization", "practice", "scaling", "signals", "consistent", "magnitude", "across", "different", "environments", "training", "typically", "running", "statistics", "stabilizes"]}, {"id": "term-reward-shaping", "t": "Reward Shaping", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "The practice of adding auxiliary reward signals to guide learning, making sparse reward problems more tractable....", "l": "r", "k": ["reward", "shaping", "practice", "adding", "auxiliary", "signals", "guide", "learning", "making", "sparse", "problems", "tractable", "potential-based", "preserves", "optimal"]}, {"id": "term-reward-shaping-algorithm", "t": "Reward Shaping Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A technique that adds auxiliary reward signals to the original reward function to guide reinforcement learning agents...", "l": "r", "k": ["reward", "shaping", "algorithm", "technique", "adds", "auxiliary", "signals", "original", "function", "guide", "reinforcement", "learning", "agents", "toward", "desirable"]}, {"id": "term-reward-free-exploration", "t": "Reward-Free Exploration", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An RL paradigm where the agent first explores the environment without any reward signal to build a comprehensive...", "l": "r", "k": ["reward-free", "exploration", "paradigm", "agent", "explores", "environment", "without", "reward", "signal", "build", "comprehensive", "understanding", "uses", "knowledge", "quickly"]}, {"id": "term-reward-weighted-regression", "t": "Reward-Weighted Regression", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A policy search method that computes policy updates by performing weighted maximum likelihood estimation on sampled...", "l": "r", "k": ["reward-weighted", "regression", "policy", "search", "method", "computes", "updates", "performing", "weighted", "maximum", "likelihood", "estimation", "sampled", "trajectories", "weights"]}, {"id": "term-rfdiffusion", "t": "RFdiffusion", "tg": ["Models", "Scientific"], "d": "models", "x": "A protein design tool based on RoseTTAFold that uses denoising diffusion on protein structures to generate novel...", "l": "r", "k": ["rfdiffusion", "protein", "design", "tool", "based", "rosettafold", "uses", "denoising", "diffusion", "structures", "generate", "novel", "backbones", "specified", "functions"]}, {"id": "term-rgb", "t": "RGB", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "Reasoning over Grounded and Biased knowledge a benchmark testing LLM behavior when retrieved knowledge conflicts with...", "l": "r", "k": ["rgb", "reasoning", "grounded", "biased", "knowledge", "benchmark", "testing", "llm", "behavior", "retrieved", "conflicts", "parametric", "tests", "conflict", "resolution"]}, {"id": "term-rgb-benchmark", "t": "RGB Benchmark", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A Retrieval and Generation Benchmark testing language models on the interplay between retrieved context and model...", "l": "r", "k": ["rgb", "benchmark", "retrieval", "generation", "testing", "language", "models", "interplay", "retrieved", "context", "model", "knowledge", "question", "answering"]}, {"id": "term-rst", "t": "Rhetorical Structure Theory", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A theory of text organization that describes how clauses and larger text spans are connected through rhetorical...", "l": "r", "k": ["rhetorical", "structure", "theory", "text", "organization", "describes", "clauses", "larger", "spans", "connected", "relations", "elaboration", "contrast", "cause", "forming"]}, {"id": "term-rice-coding-algorithm", "t": "Rice Coding Algorithm", "tg": ["Algorithms", "Technical", "Information Theory"], "d": "algorithms", "x": "A special case of Golomb coding where the parameter is a power of two enabling efficient computation using bit shifts....", "l": "r", "k": ["rice", "coding", "algorithm", "special", "case", "golomb", "parameter", "power", "enabling", "efficient", "computation", "bit", "shifts", "widely", "lossless"]}, {"id": "term-richard-karp", "t": "Richard Karp", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who identified 21 NP-complete problems in his 1972 paper demonstrating the breadth of...", "l": "r", "k": ["richard", "karp", "american", "computer", "scientist", "identified", "np-complete", "problems", "paper", "demonstrating", "breadth", "computationally", "intractable", "work", "showed"]}, {"id": "term-richard-sutton", "t": "Richard Sutton", "tg": ["History", "Pioneers"], "d": "history", "x": "Canadian computer scientist who co-authored the seminal textbook on reinforcement learning with Andrew Barto, developed...", "l": "r", "k": ["richard", "sutton", "canadian", "computer", "scientist", "co-authored", "seminal", "textbook", "reinforcement", "learning", "andrew", "barto", "developed", "temporal", "difference"]}, {"id": "term-richardson-extrapolation", "t": "Richardson Extrapolation", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A sequence acceleration method that combines numerical estimates computed at different step sizes to obtain a...", "l": "r", "k": ["richardson", "extrapolation", "sequence", "acceleration", "method", "combines", "numerical", "estimates", "computed", "different", "step", "sizes", "obtain", "higher-order", "approximation"]}, {"id": "term-ridge-regression", "t": "Ridge Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A linear regression variant that adds an L2 penalty term to the ordinary least squares objective, shrinking...", "l": "r", "k": ["ridge", "regression", "linear", "variant", "adds", "penalty", "term", "ordinary", "least", "squares", "objective", "shrinking", "coefficients", "toward", "zero"]}, {"id": "term-riffusion", "t": "Riffusion", "tg": ["Models", "Technical", "Audio", "Vision"], "d": "models", "x": "A model for generating music by fine-tuning Stable Diffusion on spectrograms then converting the generated spectrograms...", "l": "r", "k": ["riffusion", "model", "generating", "music", "fine-tuning", "stable", "diffusion", "spectrograms", "converting", "generated", "audio", "waveforms"]}, {"id": "term-right-to-explanation", "t": "Right to Explanation", "tg": ["Governance", "AI Ethics"], "d": "safety", "x": "The legal or ethical principle that individuals subjected to automated decision-making are entitled to a meaningful...", "l": "r", "k": ["right", "explanation", "legal", "ethical", "principle", "individuals", "subjected", "automated", "decision-making", "entitled", "meaningful", "logic", "involved", "partially", "codified"]}, {"id": "term-right-to-not-be-subject-to-ai", "t": "Right to Not Be Subject to AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "The right of individuals to opt out of AI-powered decision-making and request human alternatives. Recognized in various...", "l": "r", "k": ["right", "subject", "individuals", "opt", "ai-powered", "decision-making", "request", "human", "alternatives", "recognized", "various", "forms", "gdpr", "regulatory", "frameworks"]}, {"id": "term-ring-all-reduce", "t": "Ring All-Reduce", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "An efficient implementation of all-reduce where GPUs are arranged in a ring topology and data is sent in chunks through...", "l": "r", "k": ["ring", "all-reduce", "efficient", "implementation", "gpus", "arranged", "topology", "data", "sent", "chunks", "passes", "scatter-reduce", "all-gather", "provides", "bandwidth-optimal"]}, {"id": "term-ring-allreduce", "t": "Ring AllReduce", "tg": ["Networking", "Distributed Training", "Algorithm"], "d": "hardware", "x": "Implementation of all-reduce using a ring topology where each processor sends and receives data from its neighbors....", "l": "r", "k": ["ring", "allreduce", "implementation", "all-reduce", "topology", "processor", "sends", "receives", "data", "neighbors", "achieves", "optimal", "bandwidth", "utilization", "gradient"]}, {"id": "term-ring-attention", "t": "Ring Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A distributed attention computation method that splits long sequences across devices in a ring topology, overlapping...", "l": "r", "k": ["ring", "attention", "distributed", "computation", "method", "splits", "long", "sequences", "across", "devices", "topology", "overlapping", "communication", "process", "near-unlimited"]}, {"id": "term-risc-architecture", "t": "RISC Architecture", "tg": ["Architecture", "Fundamentals", "Design"], "d": "hardware", "x": "Reduced Instruction Set Computer design philosophy using simple fixed-length instructions for efficient pipelining. ARM...", "l": "r", "k": ["risc", "architecture", "reduced", "instruction", "computer", "design", "philosophy", "simple", "fixed-length", "instructions", "efficient", "pipelining", "arm", "risc-v", "modern"]}, {"id": "term-risc-v", "t": "RISC-V", "tg": ["Architecture", "Open Source", "ISA"], "d": "hardware", "x": "Open-source instruction set architecture based on RISC principles that anyone can implement without licensing fees....", "l": "r", "k": ["risc-v", "open-source", "instruction", "architecture", "based", "risc", "principles", "anyone", "implement", "without", "licensing", "fees", "increasingly", "accelerator", "designs"]}, {"id": "term-risk-based-regulation", "t": "Risk-Based Regulation", "tg": ["Safety", "Policy"], "d": "safety", "x": "A regulatory approach that calibrates oversight requirements based on the level of risk posed by an AI system. The...", "l": "r", "k": ["risk-based", "regulation", "regulatory", "approach", "calibrates", "oversight", "requirements", "based", "level", "risk", "posed", "system", "adopted", "act", "categorizes"]}, {"id": "term-risk-sensitive-rl", "t": "Risk-Sensitive RL", "tg": ["Reinforcement Learning", "Safety"], "d": "safety", "x": "RL methods that optimize risk-aware objectives such as conditional value-at-risk (CVaR) or variance-penalized returns...", "l": "r", "k": ["risk-sensitive", "methods", "optimize", "risk-aware", "objectives", "conditional", "value-at-risk", "cvar", "variance-penalized", "returns", "rather", "expected", "return", "alone", "approaches"]}, {"id": "term-rlbench", "t": "RLBench", "tg": ["Benchmark", "Reinforcement Learning", "Robotics"], "d": "datasets", "x": "A large-scale robot learning benchmark providing 100 manipulation tasks in simulation with demonstrations. Tests...", "l": "r", "k": ["rlbench", "large-scale", "robot", "learning", "benchmark", "providing", "manipulation", "tasks", "simulation", "demonstrations", "tests", "demonstration", "reinforcement", "robotic"]}, {"id": "term-rlhf", "t": "RLHF (Reinforcement Learning from Human Feedback)", "tg": ["Training", "Alignment"], "d": "safety", "x": "A training technique that uses human preferences to guide model behavior. Human raters compare outputs, and these...", "l": "r", "k": ["rlhf", "reinforcement", "learning", "human", "feedback", "training", "technique", "uses", "preferences", "guide", "model", "behavior", "raters", "compare", "outputs"]}, {"id": "term-rlhf-algorithm", "t": "RLHF Algorithm", "tg": ["Algorithms", "Fundamentals", "Safety"], "d": "algorithms", "x": "Reinforcement Learning from Human Feedback trains language models using human preference data. Involves supervised...", "l": "r", "k": ["rlhf", "algorithm", "reinforcement", "learning", "human", "feedback", "trains", "language", "models", "preference", "data", "involves", "supervised", "fine-tuning", "reward"]}, {"id": "term-rms-normalization", "t": "RMS Normalization", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Root Mean Square Layer Normalization, a simplified variant of layer normalization that only rescales by the root mean...", "l": "r", "k": ["rms", "normalization", "root", "mean", "square", "layer", "simplified", "variant", "rescales", "activations", "without", "recentering", "reducing", "computational", "cost"]}, {"id": "term-rmsnorm", "t": "RMSNorm", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Root Mean Square Layer Normalization, a simplified normalization technique that normalizes activations using only the...", "l": "r", "k": ["rmsnorm", "root", "mean", "square", "layer", "normalization", "simplified", "technique", "normalizes", "activations", "rms", "statistic", "without", "centering", "reducing"]}, {"id": "term-rmsprop", "t": "RMSProp", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "An adaptive learning rate optimization algorithm that divides the learning rate by a running average of the magnitudes...", "l": "r", "k": ["rmsprop", "adaptive", "learning", "rate", "optimization", "algorithm", "divides", "running", "average", "magnitudes", "recent", "gradients", "parameter", "addresses", "diminishing"]}, {"id": "term-rnn", "t": "RNN (Recurrent Neural Network)", "tg": ["Architecture", "Historical"], "d": "models", "x": "A neural network architecture that processes sequences by maintaining hidden state across steps. Predecessors to...", "l": "r", "k": ["rnn", "recurrent", "neural", "network", "architecture", "processes", "sequences", "maintaining", "hidden", "state", "across", "steps", "predecessors", "transformers", "sequence"]}, {"id": "term-roadrunner-supercomputer", "t": "Roadrunner Supercomputer", "tg": ["Historical", "Supercomputer", "IBM"], "d": "hardware", "x": "First petaFLOPS supercomputer built by IBM at Los Alamos National Laboratory in 2008. Used a hybrid architecture...", "l": "r", "k": ["roadrunner", "supercomputer", "petaflops", "built", "ibm", "los", "alamos", "national", "laboratory", "hybrid", "architecture", "combining", "amd", "opteron", "cpus"]}, {"id": "term-robert-floyd", "t": "Robert Floyd", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who received the Turing Award in 1978 for contributions to programming languages including...", "l": "r", "k": ["robert", "floyd", "american", "computer", "scientist", "received", "turing", "award", "contributions", "programming", "languages", "including", "operator", "precedence", "parsing"]}, {"id": "term-robert-kowalski", "t": "Robert Kowalski", "tg": ["History", "Pioneers"], "d": "history", "x": "British-American logician and computer scientist who developed the procedural interpretation of Horn clauses providing...", "l": "r", "k": ["robert", "kowalski", "british-american", "logician", "computer", "scientist", "developed", "procedural", "interpretation", "horn", "clauses", "providing", "theoretical", "foundation", "logic"]}, {"id": "term-roberta", "t": "RoBERTa", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A robustly optimized BERT pretraining approach that improves upon BERT by training longer with more data, removing next...", "l": "r", "k": ["roberta", "robustly", "optimized", "bert", "pretraining", "approach", "improves", "upon", "training", "longer", "data", "removing", "next", "sentence", "prediction"]}, {"id": "term-roberts-cross-operator", "t": "Roberts Cross Operator", "tg": ["Algorithms", "Technical", "Vision", "History"], "d": "algorithms", "x": "One of the earliest edge detection operators that uses two 2x2 kernels to compute the image gradient along diagonal...", "l": "r", "k": ["roberts", "cross", "operator", "earliest", "edge", "detection", "operators", "uses", "2x2", "kernels", "compute", "image", "gradient", "along", "diagonal"]}, {"id": "term-robin-hood-hashing", "t": "Robin Hood Hashing", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "An open-addressing hash table scheme that reduces variance in probe lengths by displacing elements with shorter probe...", "l": "r", "k": ["robin", "hood", "hashing", "open-addressing", "hash", "table", "scheme", "reduces", "variance", "probe", "lengths", "displacing", "elements", "shorter", "sequences"]}, {"id": "term-robocat", "t": "RoboCat", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "A self-improving robotic agent from DeepMind that learns to operate different robotic arms and solve diverse...", "l": "r", "k": ["robocat", "self-improving", "robotic", "agent", "deepmind", "learns", "operate", "different", "arms", "solve", "diverse", "manipulation", "tasks", "self-generated", "training"]}, {"id": "term-roboflamingo", "t": "RoboFlamingo", "tg": ["Models", "Technical", "Robotics", "Vision", "NLP"], "d": "models", "x": "A robotic manipulation framework that adapts the Flamingo vision-language model for robot control through...", "l": "r", "k": ["roboflamingo", "robotic", "manipulation", "framework", "adapts", "flamingo", "vision-language", "model", "robot", "control", "language-conditioned", "behavior", "cloning"]}, {"id": "term-robosuite", "t": "RoboSuite", "tg": ["Benchmark", "Reinforcement Learning", "Robotics"], "d": "datasets", "x": "A modular simulation framework for robot learning providing standardized manipulation tasks controllers and...", "l": "r", "k": ["robosuite", "modular", "simulation", "framework", "robot", "learning", "providing", "standardized", "manipulation", "tasks", "controllers", "environments", "benchmarking", "algorithms"]}, {"id": "term-robot-rights", "t": "Robot Rights", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The concept that sufficiently advanced robots or AI systems might deserve legal protections or moral consideration...", "l": "r", "k": ["robot", "rights", "concept", "sufficiently", "advanced", "robots", "systems", "deserve", "legal", "protections", "moral", "consideration", "analogous", "granted", "humans"]}, {"id": "term-robotics-history", "t": "Robotics History", "tg": ["History", "Milestones"], "d": "history", "x": "The history of robotics from early automata and Unimate (1961) through mobile robots (Shakey 1966) to modern robotic...", "l": "r", "k": ["robotics", "history", "early", "automata", "unimate", "mobile", "robots", "shakey", "modern", "robotic", "systems", "including", "autonomous", "vehicles", "surgical"]}, {"id": "term-robust-regression", "t": "Robust Regression", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A class of regression methods designed to be resistant to outliers and violations of model assumptions. Techniques...", "l": "r", "k": ["robust", "regression", "class", "methods", "designed", "resistant", "outliers", "violations", "model", "assumptions", "techniques", "include", "m-estimation", "least", "trimmed"]}, {"id": "term-robust-rl", "t": "Robust RL", "tg": ["Reinforcement Learning", "Safety"], "d": "safety", "x": "RL algorithms designed to find policies that perform well under worst-case environment perturbations or model...", "l": "r", "k": ["robust", "algorithms", "designed", "find", "policies", "perform", "worst-case", "environment", "perturbations", "model", "uncertainty", "optimizes", "against", "adversarial", "possible"]}, {"id": "term-robust-scaler", "t": "Robust Scaler", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A feature scaling method that uses the median and interquartile range instead of the mean and standard deviation,...", "l": "r", "k": ["robust", "scaler", "feature", "scaling", "method", "uses", "median", "interquartile", "range", "instead", "mean", "standard", "deviation", "making", "resistant"]}, {"id": "term-robustness-testing", "t": "Robustness Testing", "tg": ["Safety", "Technical"], "d": "safety", "x": "Systematic evaluation of AI system performance under adverse conditions including noisy inputs distribution shift and...", "l": "r", "k": ["robustness", "testing", "systematic", "evaluation", "system", "performance", "adverse", "conditions", "including", "noisy", "inputs", "distribution", "shift", "adversarial", "attacks"]}, {"id": "term-roc-curve", "t": "ROC Curve", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "Receiver Operating Characteristic curve, a plot of the true positive rate against the false positive rate at various...", "l": "r", "k": ["roc", "curve", "receiver", "operating", "characteristic", "plot", "true", "positive", "rate", "against", "false", "various", "classification", "thresholds", "visualizes"]}, {"id": "term-rocm", "t": "ROCm", "tg": ["Programming", "AMD", "Open Source"], "d": "hardware", "x": "Radeon Open Compute platform providing AMD open-source GPU computing stack for AI and HPC. Includes compilers libraries...", "l": "r", "k": ["rocm", "radeon", "open", "compute", "platform", "providing", "amd", "open-source", "gpu", "computing", "stack", "hpc", "includes", "compilers", "libraries"]}, {"id": "term-rocstories", "t": "ROCStories", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A corpus of 50000 five-sentence commonsense stories capturing a rich set of causal and temporal relations between daily...", "l": "r", "k": ["rocstories", "corpus", "five-sentence", "commonsense", "stories", "capturing", "rich", "causal", "temporal", "relations", "daily", "events", "storycloze", "test", "narrative"]}, {"id": "term-rodney-brooks", "t": "Rodney Brooks", "tg": ["History", "Pioneers"], "d": "history", "x": "Australian roboticist who directed the MIT AI Laboratory from 1997 to 2007. Pioneer of behavior-based robotics and the...", "l": "r", "k": ["rodney", "brooks", "australian", "roboticist", "directed", "mit", "laboratory", "pioneer", "behavior-based", "robotics", "subsumption", "architecture", "co-founder", "irobot", "roomba"]}, {"id": "term-roger-schank", "t": "Roger Schank", "tg": ["History", "Pioneers"], "d": "history", "x": "American AI researcher (1946-2023) who developed conceptual dependency theory and script theory for natural language...", "l": "r", "k": ["roger", "schank", "american", "researcher", "1946-2023", "developed", "conceptual", "dependency", "theory", "script", "natural", "language", "understanding", "advancing", "role"]}, {"id": "term-roi-align", "t": "ROI Align", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "An improved version of ROI Pooling that uses bilinear interpolation instead of quantized grid snapping, eliminating...", "l": "r", "k": ["roi", "align", "improved", "version", "pooling", "uses", "bilinear", "interpolation", "instead", "quantized", "grid", "snapping", "eliminating", "misalignment", "artifacts"]}, {"id": "term-roi-pooling", "t": "ROI Pooling", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Region of Interest Pooling, an operation that extracts fixed-size feature representations from arbitrary-sized region...", "l": "r", "k": ["roi", "pooling", "region", "interest", "operation", "extracts", "fixed-size", "feature", "representations", "arbitrary-sized", "proposals", "enabling", "classification", "head", "object"]}, {"id": "term-role-assignment", "t": "Role Assignment", "tg": ["Prompt Engineering", "Persona"], "d": "general", "x": "The prompt technique of explicitly designating a specific role, profession, or character for the model to adopt,...", "l": "r", "k": ["role", "assignment", "prompt", "technique", "explicitly", "designating", "specific", "profession", "character", "model", "adopt", "shaping", "response", "style", "vocabulary"]}, {"id": "term-role-prompting", "t": "Role Prompting", "tg": ["Prompting", "Technique"], "d": "general", "x": "Assigning AI a specific persona, expertise, or perspective to shape its responses. For example, \"Act as a senior...", "l": "r", "k": ["role", "prompting", "assigning", "specific", "persona", "expertise", "perspective", "shape", "responses", "example", "act", "senior", "developer", "patient", "teacher"]}, {"id": "term-romberg-integration", "t": "Romberg Integration", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A numerical integration method that applies Richardson extrapolation to the trapezoidal rule to achieve higher-order...", "l": "r", "k": ["romberg", "integration", "numerical", "method", "applies", "richardson", "extrapolation", "trapezoidal", "rule", "achieve", "higher-order", "accuracy", "combines", "estimates", "successively"]}, {"id": "term-roofline-model", "t": "Roofline Model", "tg": ["Hardware", "Model Optimization"], "d": "models", "x": "A performance analysis framework that plots achievable performance as a function of operational intensity (FLOPS per...", "l": "r", "k": ["roofline", "model", "performance", "analysis", "framework", "plots", "achievable", "function", "operational", "intensity", "flops", "per", "byte", "memory", "traffic"]}, {"id": "term-root-mean-squared-error", "t": "Root Mean Squared Error", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "The square root of the mean squared error, expressed in the same units as the target variable. It provides an...", "l": "r", "k": ["root", "mean", "squared", "error", "square", "expressed", "units", "target", "variable", "provides", "interpretable", "measure", "typical", "magnitude", "prediction"]}, {"id": "term-roots", "t": "ROOTS", "tg": ["Training Corpus", "NLP", "Multilingual"], "d": "datasets", "x": "The corpus used to train the BLOOM multilingual language model containing 1.6TB of text in 46 natural languages and 13...", "l": "r", "k": ["roots", "corpus", "train", "bloom", "multilingual", "language", "model", "containing", "6tb", "text", "natural", "languages", "programming", "curated", "bigscience"]}, {"id": "term-rope", "t": "RoPE (Rotary Position Embedding)", "tg": ["Architecture", "Transformers"], "d": "models", "x": "A positional encoding technique that encodes position through rotation in complex space. Enables better length...", "l": "r", "k": ["rope", "rotary", "position", "embedding", "positional", "encoding", "technique", "encodes", "rotation", "complex", "space", "enables", "better", "length", "generalization"]}, {"id": "term-rope-data-structure", "t": "Rope Data Structure", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A balanced binary tree data structure for efficiently storing and manipulating long strings. Supports concatenation and...", "l": "r", "k": ["rope", "data", "structure", "balanced", "binary", "tree", "efficiently", "storing", "manipulating", "long", "strings", "supports", "concatenation", "splitting", "indexing"]}, {"id": "term-ropes", "t": "Ropes", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "Reasoning Over Paragraph Effects in Situations a dataset requiring understanding of cause-and-effect in given...", "l": "r", "k": ["ropes", "reasoning", "paragraph", "effects", "situations", "dataset", "requiring", "understanding", "cause-and-effect", "given", "tests", "qualitative", "physical", "biological", "processes"]}, {"id": "term-rosettafold", "t": "RoseTTAFold", "tg": ["Models", "Scientific"], "d": "models", "x": "A protein structure prediction model from the Baker Lab that uses a three-track neural network to simultaneously...", "l": "r", "k": ["rosettafold", "protein", "structure", "prediction", "model", "baker", "lab", "uses", "three-track", "neural", "network", "simultaneously", "process", "sequence", "distance"]}, {"id": "term-ross-quillian", "t": "Ross Quillian", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who introduced semantic networks in his 1968 doctoral thesis as a model of human...", "l": "r", "k": ["ross", "quillian", "american", "computer", "scientist", "introduced", "semantic", "networks", "doctoral", "thesis", "model", "human", "associative", "memory", "establishing"]}, {"id": "term-rotary-position-embedding", "t": "Rotary Position Embedding", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A method that encodes position information by rotating query and key vectors in pairs of dimensions according to their...", "l": "r", "k": ["rotary", "position", "embedding", "method", "encodes", "information", "rotating", "query", "key", "vectors", "pairs", "dimensions", "according", "naturally", "encoding"]}, {"id": "term-rouge", "t": "ROUGE", "tg": ["NLP", "Text Processing"], "d": "general", "x": "Recall-Oriented Understudy for Gisting Evaluation, a set of metrics for evaluating summarization quality by measuring...", "l": "r", "k": ["rouge", "recall-oriented", "understudy", "gisting", "evaluation", "metrics", "evaluating", "summarization", "quality", "measuring", "n-gram", "overlap", "generated", "summaries", "reference"]}, {"id": "term-rouge-score", "t": "ROUGE Score", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "Recall-Oriented Understudy for Gisting Evaluation, a family of metrics that measures text summarization quality by...", "l": "r", "k": ["rouge", "score", "recall-oriented", "understudy", "gisting", "evaluation", "family", "metrics", "measures", "text", "summarization", "quality", "computing", "n-gram", "overlap"]}, {"id": "term-rouge-score-algorithm", "t": "ROUGE Score Algorithm", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "Recall-Oriented Understudy for Gisting Evaluation measures the quality of text summaries by counting overlapping units...", "l": "r", "k": ["rouge", "score", "algorithm", "recall-oriented", "understudy", "gisting", "evaluation", "measures", "quality", "text", "summaries", "counting", "overlapping", "units", "n-grams"]}, {"id": "term-rouge-1", "t": "ROUGE-1", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A ROUGE variant that measures unigram (single word) overlap between a generated text and reference text, providing a...", "l": "r", "k": ["rouge-1", "rouge", "variant", "measures", "unigram", "single", "word", "overlap", "generated", "text", "reference", "providing", "basic", "assessment", "content"]}, {"id": "term-rouge-2", "t": "ROUGE-2", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A ROUGE variant that measures bigram (two consecutive word) overlap between generated and reference texts, capturing...", "l": "r", "k": ["rouge-2", "rouge", "variant", "measures", "bigram", "consecutive", "word", "overlap", "generated", "reference", "texts", "capturing", "phrase-level", "similarity", "providing"]}, {"id": "term-rouge-l", "t": "ROUGE-L", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A ROUGE variant based on the longest common subsequence (LCS) between generated and reference texts, capturing...", "l": "r", "k": ["rouge-l", "rouge", "variant", "based", "longest", "common", "subsequence", "lcs", "generated", "reference", "texts", "capturing", "sentence-level", "structural", "similarity"]}, {"id": "term-rt-1", "t": "RT-1", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "Robotics Transformer 1 is a model from Google DeepMind that tokenizes robot actions and uses a Transformer to map...", "l": "r", "k": ["rt-1", "robotics", "transformer", "model", "google", "deepmind", "tokenizes", "robot", "actions", "uses", "map", "instructions", "images", "commands"]}, {"id": "term-rt-2", "t": "RT-2", "tg": ["Models", "Technical"], "d": "models", "x": "Robotic Transformer 2 is a vision-language-action model that directly outputs robot actions from camera images and...", "l": "r", "k": ["rt-2", "robotic", "transformer", "vision-language-action", "model", "directly", "outputs", "robot", "actions", "camera", "images", "language", "instructions", "demonstrates", "web-scale"]}, {"id": "term-rt-2-x", "t": "RT-2-X", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "An extended version of RT-2 trained on cross-robot data from the Open X-Embodiment dataset for improved generalization...", "l": "r", "k": ["rt-2-x", "extended", "version", "rt-2", "trained", "cross-robot", "data", "open", "x-embodiment", "dataset", "improved", "generalization", "across", "different", "robot"]}, {"id": "term-rt-detr", "t": "RT-DETR", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Real-Time Detection Transformer, a hybrid detection architecture that combines a CNN backbone with a transformer...", "l": "r", "k": ["rt-detr", "real-time", "detection", "transformer", "hybrid", "architecture", "combines", "cnn", "backbone", "decoder", "achieving", "accuracy", "detr-based", "models", "inference"]}, {"id": "term-rt-x", "t": "RT-X", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "A cross-embodiment robot learning initiative that trains policies on data from multiple robot types to achieve broad...", "l": "r", "k": ["rt-x", "cross-embodiment", "robot", "learning", "initiative", "trains", "policies", "data", "multiple", "types", "achieve", "broad", "generalization", "across", "platforms"]}, {"id": "term-rte", "t": "RTE", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Recognizing Textual Entailment a collection of datasets from annual PASCAL challenges testing whether a hypothesis...", "l": "r", "k": ["rte", "recognizing", "textual", "entailment", "collection", "datasets", "annual", "pascal", "challenges", "testing", "hypothesis", "sentence", "entailed", "premise", "part"]}, {"id": "term-rtl-design", "t": "RTL Design", "tg": ["Manufacturing", "Design", "Methodology"], "d": "hardware", "x": "Register Transfer Level design methodology describing digital circuit behavior in terms of data flow between registers....", "l": "r", "k": ["rtl", "design", "register", "transfer", "level", "methodology", "describing", "digital", "circuit", "behavior", "terms", "data", "flow", "registers", "standard"]}, {"id": "term-ruler", "t": "RULER", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A synthetic benchmark for evaluating long-context language models with controllable complexity. Tests retrieval...", "l": "r", "k": ["ruler", "synthetic", "benchmark", "evaluating", "long-context", "language", "models", "controllable", "complexity", "tests", "retrieval", "reasoning", "multi-hop", "abilities", "across"]}, {"id": "term-run-length-encoding-algorithm", "t": "Run-Length Encoding Algorithm", "tg": ["Algorithms", "Fundamentals", "Information Theory"], "d": "algorithms", "x": "A simple lossless compression method that encodes consecutive identical values as a single value and its count. Most...", "l": "r", "k": ["run-length", "encoding", "algorithm", "simple", "lossless", "compression", "method", "encodes", "consecutive", "identical", "values", "single", "value", "count", "effective"]}, {"id": "term-runge-kutta-method", "t": "Runge-Kutta Method", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "A family of iterative methods for approximating solutions to ordinary differential equations. The fourth-order variant...", "l": "r", "k": ["runge-kutta", "method", "family", "iterative", "methods", "approximating", "solutions", "ordinary", "differential", "equations", "fourth-order", "variant", "rk4", "widely", "achieves"]}, {"id": "term-rwkv", "t": "RWKV", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A linear attention-based architecture that combines the parallelizable training of transformers with the efficient...", "l": "r", "k": ["rwkv", "linear", "attention-based", "architecture", "combines", "parallelizable", "training", "transformers", "efficient", "inference", "rnns", "novel", "time-mixing", "channel-mixing", "approach"]}, {"id": "term-rwkv-5", "t": "RWKV-5", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "The fifth version of the RWKV architecture that combines RNN efficiency during inference with Transformer-level...", "l": "r", "k": ["rwkv-5", "fifth", "version", "rwkv", "architecture", "combines", "rnn", "efficiency", "inference", "transformer-level", "performance", "training", "linear-time", "generation"]}, {"id": "term-rwkv-6", "t": "RWKV-6", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A sixth-generation RWKV model with architectural improvements to the time mixing and channel mixing blocks for better...", "l": "r", "k": ["rwkv-6", "sixth-generation", "rwkv", "model", "architectural", "improvements", "time", "mixing", "channel", "blocks", "better", "long-range", "dependency", "modeling"]}, {"id": "term-rxrx1", "t": "RxRx1", "tg": ["Benchmark", "Computer Vision", "Scientific"], "d": "datasets", "x": "A biological image dataset of fluorescent microscopy images showing cells under 1108 different genetic perturbations....", "l": "r", "k": ["rxrx1", "biological", "image", "dataset", "fluorescent", "microscopy", "images", "showing", "cells", "different", "genetic", "perturbations", "drug", "discovery", "cellular"]}, {"id": "term-s2orc", "t": "S2ORC", "tg": ["Training Corpus", "NLP", "Scientific"], "d": "datasets", "x": "The Semantic Scholar Open Research Corpus containing full text and metadata for 81 million academic papers. A...", "l": "s", "k": ["s2orc", "semantic", "scholar", "open", "research", "corpus", "containing", "full", "text", "metadata", "million", "academic", "papers", "comprehensive", "resource"]}, {"id": "term-s3dis", "t": "S3DIS", "tg": ["Benchmark", "3D", "Computer Vision"], "d": "datasets", "x": "Stanford Large-Scale 3D Indoor Spaces a dataset of 6 large indoor areas with dense 3D point clouds annotated with 13...", "l": "s", "k": ["s3dis", "stanford", "large-scale", "indoor", "spaces", "dataset", "large", "areas", "dense", "point", "clouds", "annotated", "semantic", "categories", "segmentation"]}, {"id": "term-s4", "t": "S4", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Structured State Spaces for Sequence Modeling, a state space model that uses a special initialization based on the...", "l": "s", "k": ["structured", "state", "spaces", "sequence", "modeling", "space", "model", "uses", "special", "initialization", "based", "hippo", "framework", "efficient", "computation"]}, {"id": "term-s5", "t": "S5", "tg": ["Models", "Technical"], "d": "models", "x": "Simplified State Space Layers extends S4 with a simpler multi-input multi-output parameterization using parallel scan...", "l": "s", "k": ["simplified", "state", "space", "layers", "extends", "simpler", "multi-input", "multi-output", "parameterization", "parallel", "scan", "efficient", "computation", "achieves", "competitive"]}, {"id": "term-safe-harbor-provisions", "t": "Safe Harbor Provisions", "tg": ["Safety", "Policy"], "d": "safety", "x": "Legal protections for AI developers who follow designated safety practices and standards. Provides regulatory certainty...", "l": "s", "k": ["safe", "harbor", "provisions", "legal", "protections", "developers", "follow", "designated", "safety", "practices", "standards", "provides", "regulatory", "certainty", "incentivizes"]}, {"id": "term-safe-rl", "t": "Safe Reinforcement Learning", "tg": ["Reinforcement Learning", "Safety"], "d": "safety", "x": "RL methods that incorporate safety constraints to prevent the agent from taking dangerous or unacceptable actions...", "l": "s", "k": ["safe", "reinforcement", "learning", "methods", "incorporate", "safety", "constraints", "prevent", "agent", "taking", "dangerous", "unacceptable", "actions", "training", "deployment"]}, {"id": "term-saferlhf", "t": "SafeRLHF", "tg": ["Training Corpus", "NLP", "Safety"], "d": "datasets", "x": "A dataset of human preference annotations focused on safety and helpfulness tradeoffs. Used for training AI systems to...", "l": "s", "k": ["saferlhf", "dataset", "human", "preference", "annotations", "focused", "safety", "helpfulness", "tradeoffs", "training", "systems", "balance", "helpful", "avoiding", "harmful"]}, {"id": "term-safety-case", "t": "Safety Case", "tg": ["Safety", "Governance"], "d": "safety", "x": "A structured argument supported by evidence that an AI system is acceptably safe for its intended use in its intended...", "l": "s", "k": ["safety", "case", "structured", "argument", "supported", "evidence", "system", "acceptably", "safe", "intended", "environment", "adapted", "safety-critical", "engineering", "disciplines"]}, {"id": "term-safety-culture-in-ai", "t": "Safety Culture in AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "Organizational values norms and practices that prioritize safety in AI development. Includes psychological safety for...", "l": "s", "k": ["safety", "culture", "organizational", "values", "norms", "practices", "prioritize", "development", "includes", "psychological", "reporting", "concerns", "resources", "research", "leadership"]}, {"id": "term-safety-filter", "t": "Safety Filter", "tg": ["Safety", "Security"], "d": "safety", "x": "Systems that detect and block harmful content in AI inputs or outputs. Part of the safety stack protecting users from...", "l": "s", "k": ["safety", "filter", "systems", "detect", "block", "harmful", "content", "inputs", "outputs", "part", "stack", "protecting", "users", "inappropriate", "dangerous"]}, {"id": "term-safety-margin", "t": "Safety Margin", "tg": ["Safety", "Technical"], "d": "safety", "x": "The difference between the conditions under which an AI system has been tested and validated and the most extreme...", "l": "s", "k": ["safety", "margin", "difference", "conditions", "system", "tested", "validated", "extreme", "encounter", "deployment", "larger", "margins", "provide", "robust", "protection"]}, {"id": "term-safety-score", "t": "Safety Score", "tg": ["Evaluation", "Safety"], "d": "datasets", "x": "A composite evaluation metric that aggregates measurements of harmful output generation including toxicity, bias,...", "l": "s", "k": ["safety", "score", "composite", "evaluation", "metric", "aggregates", "measurements", "harmful", "output", "generation", "including", "toxicity", "bias", "dangerous", "advice"]}, {"id": "term-safetybench", "t": "SafetyBench", "tg": ["Benchmark", "NLP", "Safety", "Evaluation"], "d": "datasets", "x": "A comprehensive benchmark for evaluating the safety of large language models across multiple risk categories including...", "l": "s", "k": ["safetybench", "comprehensive", "benchmark", "evaluating", "safety", "large", "language", "models", "across", "multiple", "risk", "categories", "including", "ethics", "legality"]}, {"id": "term-saint", "t": "SAINT", "tg": ["Models", "Technical"], "d": "models", "x": "Self-Attention and Intersample Attention Transformer applies both row-wise and column-wise attention to tabular data....", "l": "s", "k": ["saint", "self-attention", "intersample", "attention", "transformer", "applies", "row-wise", "column-wise", "tabular", "data", "captures", "feature-feature", "sample-sample", "interactions", "improved"]}, {"id": "term-saliency-map", "t": "Saliency Map", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A visualization technique that highlights which parts of an input are most relevant to a model's prediction by...", "l": "s", "k": ["saliency", "map", "visualization", "technique", "highlights", "parts", "input", "relevant", "model", "prediction", "computing", "gradients", "output", "respect", "simple"]}, {"id": "term-salmonn", "t": "SALMONN", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "Speech Audio Language Music Open Neural Network is a multimodal model that can perceive and understand speech and audio...", "l": "s", "k": ["salmonn", "speech", "audio", "language", "music", "open", "neural", "network", "multimodal", "model", "perceive", "understand", "alongside", "text"]}, {"id": "term-sam-2", "t": "SAM 2", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "Segment Anything Model 2 extends the original SAM to video segmentation with streaming memory for tracking and...", "l": "s", "k": ["sam", "segment", "anything", "model", "extends", "original", "video", "segmentation", "streaming", "memory", "tracking", "segmenting", "objects", "across", "frames"]}, {"id": "term-sam-altman", "t": "Sam Altman", "tg": ["History", "Pioneers"], "d": "history", "x": "American entrepreneur who became CEO of OpenAI, overseeing the development and launch of ChatGPT and GPT-4. His brief...", "l": "s", "k": ["sam", "altman", "american", "entrepreneur", "became", "ceo", "openai", "overseeing", "development", "launch", "chatgpt", "gpt-4", "brief", "firing", "reinstatement"]}, {"id": "term-sam-med2d", "t": "SAM-Med2D", "tg": ["Models", "Technical", "Medical", "Vision"], "d": "models", "x": "A specialized version of the Segment Anything Model fine-tuned on large-scale 2D medical imaging datasets for clinical...", "l": "s", "k": ["sam-med2d", "specialized", "version", "segment", "anything", "model", "fine-tuned", "large-scale", "medical", "imaging", "datasets", "clinical", "image", "segmentation", "tasks"]}, {"id": "term-sambanova", "t": "SambaNova", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "An AI hardware company producing reconfigurable dataflow architecture processors (SN series) that adapt their compute...", "l": "s", "k": ["sambanova", "hardware", "company", "producing", "reconfigurable", "dataflow", "architecture", "processors", "series", "adapt", "compute", "topology", "different", "model", "architectures"]}, {"id": "term-sammon-mapping", "t": "Sammon Mapping", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A nonlinear dimensionality reduction method that minimizes the stress function measuring the difference between...", "l": "s", "k": ["sammon", "mapping", "nonlinear", "dimensionality", "reduction", "method", "minimizes", "stress", "function", "measuring", "difference", "original", "projected", "interpoint", "distances"]}, {"id": "term-sample-complexity-rl", "t": "Sample Complexity", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The number of environment interactions needed for an RL algorithm to find a near-optimal policy with high probability....", "l": "s", "k": ["sample", "complexity", "number", "environment", "interactions", "needed", "algorithm", "find", "near-optimal", "policy", "high", "probability", "key", "measure", "algorithmic"]}, {"id": "term-sampling", "t": "Sampling", "tg": ["Generation", "Technical"], "d": "general", "x": "The process of selecting the next token during text generation. Methods include greedy (always pick highest...", "l": "s", "k": ["sampling", "process", "selecting", "next", "token", "text", "generation", "methods", "include", "greedy", "always", "pick", "highest", "probability", "top-k"]}, {"id": "term-samsum", "t": "SAMSum", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A dialogue summarization dataset containing 16000 messenger-style conversations with human-written summaries. Tests the...", "l": "s", "k": ["samsum", "dialogue", "summarization", "dataset", "containing", "messenger-style", "conversations", "human-written", "summaries", "tests", "ability", "summarize", "informal", "conversational", "text"]}, {"id": "term-samsung-advanced-package", "t": "Samsung Advanced Package", "tg": ["Packaging", "Samsung", "Manufacturing"], "d": "hardware", "x": "Samsung TSMC CoWoS-competitive advanced packaging technology offering 2.5D integration with silicon interposer. Enables...", "l": "s", "k": ["samsung", "advanced", "package", "tsmc", "cowos-competitive", "packaging", "technology", "offering", "integration", "silicon", "interposer", "enables", "multi-die", "gpu", "hbm"]}, {"id": "term-samsung-ai-chip", "t": "Samsung AI Chip", "tg": ["Accelerator", "Samsung", "Development"], "d": "hardware", "x": "Samsung development of custom AI accelerator chips for use in its data centers and devices. Includes both training...", "l": "s", "k": ["samsung", "chip", "development", "custom", "accelerator", "chips", "data", "centers", "devices", "includes", "training", "accelerators", "mobile", "npus", "integrated"]}, {"id": "term-samsung-exynos", "t": "Samsung Exynos", "tg": ["Mobile", "Samsung", "SoC"], "d": "hardware", "x": "Samsung line of mobile system-on-chip processors incorporating NPU cores for on-device AI tasks. Used in select Samsung...", "l": "s", "k": ["samsung", "exynos", "line", "mobile", "system-on-chip", "processors", "incorporating", "npu", "cores", "on-device", "tasks", "select", "galaxy", "smartphones", "devices"]}, {"id": "term-samsung-foundry", "t": "Samsung Foundry", "tg": ["Fabrication", "Foundry"], "d": "hardware", "x": "Samsung semiconductor manufacturing division producing advanced chips at 3nm and below. Manufactures processors for...", "l": "s", "k": ["samsung", "foundry", "semiconductor", "manufacturing", "division", "producing", "advanced", "chips", "3nm", "manufactures", "processors", "various", "companies", "including", "qualcomm"]}, {"id": "term-samuels-checkers-program", "t": "Samuel's Checkers Program", "tg": ["History", "Systems"], "d": "history", "x": "A checkers-playing program developed by Arthur Samuel at IBM beginning in 1959. The program used machine learning...", "l": "s", "k": ["samuel", "checkers", "program", "checkers-playing", "developed", "arthur", "ibm", "beginning", "machine", "learning", "techniques", "including", "rote", "generalization", "improve"]}, {"id": "term-sandwich-prompting", "t": "Sandwich Prompting", "tg": ["Prompt Engineering", "Structure"], "d": "general", "x": "A prompt structure that places the core instruction both before and after the main context or input data, reinforcing...", "l": "s", "k": ["sandwich", "prompting", "prompt", "structure", "places", "core", "instruction", "main", "context", "input", "data", "reinforcing", "adherence", "instructions", "otherwise"]}, {"id": "term-santacoder", "t": "SantaCoder", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A 1.1B parameter code generation model from the BigCode project trained on a filtered subset of The Stack dataset in...", "l": "s", "k": ["santacoder", "parameter", "code", "generation", "model", "bigcode", "project", "trained", "filtered", "subset", "stack", "dataset", "python", "java", "javascript"]}, {"id": "term-sapiens", "t": "Sapiens", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A family of human-centric vision models from Meta for body part segmentation and pose estimation and depth prediction...", "l": "s", "k": ["sapiens", "family", "human-centric", "vision", "models", "meta", "body", "part", "segmentation", "pose", "estimation", "depth", "prediction", "surface", "normal"]}, {"id": "term-sarcasm-detection", "t": "Sarcasm Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying sarcastic or ironic statements in text where the intended meaning differs from the literal...", "l": "s", "k": ["sarcasm", "detection", "task", "identifying", "sarcastic", "ironic", "statements", "text", "intended", "meaning", "differs", "literal", "challenging", "problem", "requiring"]}, {"id": "term-sarima", "t": "SARIMA", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "Seasonal ARIMA, an extension of the ARIMA model that includes additional seasonal autoregressive, differencing, and...", "l": "s", "k": ["sarima", "seasonal", "arima", "extension", "model", "includes", "additional", "autoregressive", "differencing", "moving", "average", "terms", "capture", "periodic", "patterns"]}, {"id": "term-sarsa", "t": "SARSA", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An on-policy temporal difference control algorithm that updates Q-values using the actual action taken by the current...", "l": "s", "k": ["sarsa", "on-policy", "temporal", "difference", "control", "algorithm", "updates", "q-values", "actual", "action", "taken", "current", "policy", "state-action-reward-state-action", "unlike"]}, {"id": "term-sasrec", "t": "SASRec", "tg": ["Models", "Technical", "Recommendation"], "d": "models", "x": "Self-Attentive Sequential Recommendation uses a unidirectional Transformer to model sequential user behavior for...", "l": "s", "k": ["sasrec", "self-attentive", "sequential", "recommendation", "uses", "unidirectional", "transformer", "model", "user", "behavior", "next-item", "prediction", "tasks"]}, {"id": "term-satellite-imagery-analysis", "t": "Satellite Imagery Analysis", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The use of computer vision models to interpret aerial and satellite photographs for applications including land use...", "l": "s", "k": ["satellite", "imagery", "analysis", "computer", "vision", "models", "interpret", "aerial", "photographs", "applications", "including", "land", "classification", "change", "detection"]}, {"id": "term-savitzky-golay-filter", "t": "Savitzky-Golay Filter", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A digital filter that smooths data by fitting successive subsets of adjacent points with a low-degree polynomial using...", "l": "s", "k": ["savitzky-golay", "filter", "digital", "smooths", "data", "fitting", "successive", "subsets", "adjacent", "points", "low-degree", "polynomial", "least", "squares", "preserves"]}, {"id": "term-saycan", "t": "SayCan", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "A framework from Google that grounds large language model knowledge in robotic affordances by combining language...", "l": "s", "k": ["saycan", "framework", "google", "grounds", "large", "language", "model", "knowledge", "robotic", "affordances", "combining", "understanding", "learned", "value", "functions"]}, {"id": "term-sbu-captions", "t": "SBU Captions", "tg": ["Training Corpus", "Multimodal"], "d": "datasets", "x": "Stony Brook University Captions a dataset of 1 million image-caption pairs collected from Flickr. Used for image...", "l": "s", "k": ["sbu", "captions", "stony", "brook", "university", "dataset", "million", "image-caption", "pairs", "collected", "flickr", "image", "captioning", "vision-language", "pretraining"]}, {"id": "term-scaffold-algorithm", "t": "SCAFFOLD Algorithm", "tg": ["Algorithms", "Technical", "Privacy"], "d": "algorithms", "x": "A federated learning method that uses control variates to correct for client drift caused by heterogeneous data....", "l": "s", "k": ["scaffold", "algorithm", "federated", "learning", "method", "uses", "control", "variates", "correct", "client", "drift", "caused", "heterogeneous", "data", "reduces"]}, {"id": "term-scalable-oversight", "t": "Scalable Oversight", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The challenge and research agenda of maintaining meaningful human oversight of AI systems as they become more capable...", "l": "s", "k": ["scalable", "oversight", "challenge", "research", "agenda", "maintaining", "meaningful", "human", "systems", "become", "capable", "handle", "tasks", "complex", "humans"]}, {"id": "term-scalar-quantization", "t": "Scalar Quantization", "tg": ["Vector Database", "Quantization"], "d": "general", "x": "A vector compression method that reduces memory usage by converting each floating-point vector component to a...", "l": "s", "k": ["scalar", "quantization", "vector", "compression", "method", "reduces", "memory", "usage", "converting", "floating-point", "component", "lower-precision", "representation", "8-bit", "integers"]}, {"id": "term-scale-ai", "t": "Scale AI", "tg": ["Company", "Data"], "d": "general", "x": "A company specializing in data labeling and annotation services for AI training. Provides human feedback at scale,...", "l": "s", "k": ["scale", "company", "specializing", "data", "labeling", "annotation", "services", "training", "provides", "human", "feedback", "crucial", "evaluating", "foundation", "models"]}, {"id": "term-scale-space-theory", "t": "Scale Space Theory", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A framework for multi-scale image analysis that represents an image at multiple levels of detail by progressive...", "l": "s", "k": ["scale", "space", "theory", "framework", "multi-scale", "image", "analysis", "represents", "multiple", "levels", "detail", "progressive", "gaussian", "blurring", "provides"]}, {"id": "term-scaling-hypothesis", "t": "Scaling Hypothesis", "tg": ["History", "Milestones"], "d": "history", "x": "The hypothesis that increasing model size, training data, and compute leads to predictable and substantial improvements...", "l": "s", "k": ["scaling", "hypothesis", "increasing", "model", "size", "training", "data", "compute", "leads", "predictable", "substantial", "improvements", "capability", "supported", "empirical"]}, {"id": "term-scaling-law", "t": "Scaling Law", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Empirical power-law relationships that predict how model performance improves as a function of compute budget, model...", "l": "s", "k": ["scaling", "law", "empirical", "power-law", "relationships", "predict", "model", "performance", "improves", "function", "compute", "budget", "size", "dataset", "guiding"]}, {"id": "term-scaling-laws", "t": "Scaling Laws", "tg": ["Research", "Training"], "d": "general", "x": "Empirical relationships showing how model performance improves with more parameters, data, and compute. Guide decisions...", "l": "s", "k": ["scaling", "laws", "empirical", "relationships", "showing", "model", "performance", "improves", "parameters", "data", "compute", "guide", "decisions", "invest", "resources"]}, {"id": "term-scaling-laws-compute", "t": "Scaling Laws for Compute", "tg": ["Model Optimization", "Distributed Computing"], "d": "models", "x": "Empirical power-law relationships between model performance and compute budget, model size, and dataset size...", "l": "s", "k": ["scaling", "laws", "compute", "empirical", "power-law", "relationships", "model", "performance", "budget", "size", "dataset", "established", "kaplan", "hoffmann", "chinchilla"]}, {"id": "term-scann-algorithm", "t": "ScaNN Algorithm", "tg": ["Algorithms", "Technical", "Searching", "Data Structure"], "d": "algorithms", "x": "Scalable Nearest Neighbors uses learned quantization and anisotropic vector quantization for approximate...", "l": "s", "k": ["scann", "algorithm", "scalable", "nearest", "neighbors", "uses", "learned", "quantization", "anisotropic", "vector", "approximate", "nearest-neighbor", "search", "achieves", "state-of-the-art"]}, {"id": "term-scannet", "t": "ScanNet", "tg": ["Benchmark", "3D", "Computer Vision"], "d": "datasets", "x": "A richly annotated 3D reconstruction dataset of real-world indoor scenes containing 2.5 million RGB-D frames across...", "l": "s", "k": ["scannet", "richly", "annotated", "reconstruction", "dataset", "real-world", "indoor", "scenes", "containing", "million", "rgb-d", "frames", "across", "semantic", "segmentation"]}, {"id": "term-scapegoat-tree-algorithm", "t": "Scapegoat Tree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A self-balancing binary search tree that rebalances by rebuilding the subtree rooted at a scapegoat node when it...", "l": "s", "k": ["scapegoat", "tree", "algorithm", "self-balancing", "binary", "search", "rebalances", "rebuilding", "subtree", "rooted", "node", "becomes", "unbalanced", "avoids", "storing"]}, {"id": "term-scatter-search-algorithm", "t": "Scatter Search Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A metaheuristic that maintains a reference set of diverse high-quality solutions and generates new candidates by...", "l": "s", "k": ["scatter", "search", "algorithm", "metaheuristic", "maintains", "reference", "diverse", "high-quality", "solutions", "generates", "candidates", "combining", "uses", "structured", "combination"]}, {"id": "term-scene-graph-generation", "t": "Scene Graph Generation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of detecting objects in an image and predicting their pairwise relationships to construct a graph...", "l": "s", "k": ["scene", "graph", "generation", "task", "detecting", "objects", "image", "predicting", "pairwise", "relationships", "construct", "representation", "nodes", "edges", "visual"]}, {"id": "term-scene-text-recognition", "t": "Scene Text Recognition", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of reading and transcribing text found in natural images (street signs, product labels, building facades),...", "l": "s", "k": ["scene", "text", "recognition", "task", "reading", "transcribing", "found", "natural", "images", "street", "signs", "product", "labels", "building", "facades"]}, {"id": "term-scenedreamer", "t": "SceneDreamer", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "A 3D scene generation model that creates unbounded 3D driving environments from a bird-eye-view layout for autonomous...", "l": "s", "k": ["scenedreamer", "scene", "generation", "model", "creates", "unbounded", "driving", "environments", "bird-eye-view", "layout", "autonomous", "simulation", "testing"]}, {"id": "term-scheduled-sampling", "t": "Scheduled Sampling", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A training technique that gradually transitions from teacher forcing to using the model's own predictions as inputs...", "l": "s", "k": ["scheduled", "sampling", "training", "technique", "gradually", "transitions", "teacher", "forcing", "model", "predictions", "inputs", "reduces", "exposure", "bias", "gap"]}, {"id": "term-schnet", "t": "SchNet", "tg": ["Models", "Scientific"], "d": "models", "x": "A deep learning model for molecular property prediction that uses continuous-filter convolutional layers to learn...", "l": "s", "k": ["schnet", "deep", "learning", "model", "molecular", "property", "prediction", "uses", "continuous-filter", "convolutional", "layers", "learn", "representations", "respecting", "rotational"]}, {"id": "term-scibench", "t": "SciBench", "tg": ["Benchmark", "NLP", "Reasoning", "Scientific"], "d": "datasets", "x": "A benchmark of college-level science problems from textbooks in mathematics physics and chemistry. Tests the ability to...", "l": "s", "k": ["scibench", "benchmark", "college-level", "science", "problems", "textbooks", "mathematics", "physics", "chemistry", "tests", "ability", "solve", "requiring", "scientific", "domain"]}, {"id": "term-scibert", "t": "SciBERT", "tg": ["Models", "Technical"], "d": "models", "x": "A BERT model pretrained on a large corpus of scientific papers from the Semantic Scholar corpus. Outperforms BERT on...", "l": "s", "k": ["scibert", "bert", "model", "pretrained", "large", "corpus", "scientific", "papers", "semantic", "scholar", "outperforms", "nlp", "tasks", "including", "named"]}, {"id": "term-scidocs", "t": "SCIDOCS", "tg": ["Benchmark", "NLP", "Scientific"], "d": "datasets", "x": "A benchmark for evaluating scientific document embeddings across seven document-level tasks including citation...", "l": "s", "k": ["scidocs", "benchmark", "evaluating", "scientific", "document", "embeddings", "across", "seven", "document-level", "tasks", "including", "citation", "prediction", "recommendation", "classification"]}, {"id": "term-scienceqa", "t": "ScienceQA", "tg": ["Benchmark", "Multimodal", "Scientific"], "d": "datasets", "x": "A multimodal science question answering benchmark with lectures explanations and multiple-choice questions. Covers...", "l": "s", "k": ["scienceqa", "multimodal", "science", "question", "answering", "benchmark", "lectures", "explanations", "multiple-choice", "questions", "covers", "diverse", "topics", "text", "image"]}, {"id": "term-scifact", "t": "SciFact", "tg": ["Benchmark", "NLP", "Scientific"], "d": "datasets", "x": "A dataset for scientific claim verification containing 1409 expert-annotated claims paired with evidence from...", "l": "s", "k": ["scifact", "dataset", "scientific", "claim", "verification", "containing", "expert-annotated", "claims", "paired", "evidence", "abstracts", "tests", "ability", "verify", "statements"]}, {"id": "term-sciglm", "t": "SciGLM", "tg": ["Models", "Technical", "NLP", "Scientific"], "d": "models", "x": "A scientific language model fine-tuned on curated scientific question-answer pairs for enhanced reasoning in physics...", "l": "s", "k": ["sciglm", "scientific", "language", "model", "fine-tuned", "curated", "question-answer", "pairs", "enhanced", "reasoning", "physics", "chemistry", "biology", "domains"]}, {"id": "term-scikit-learn", "t": "Scikit-learn", "tg": ["Framework", "ML"], "d": "general", "x": "A popular Python library for traditional machine learning algorithms. Provides simple APIs for classification,...", "l": "s", "k": ["scikit-learn", "popular", "python", "library", "traditional", "machine", "learning", "algorithms", "provides", "simple", "apis", "classification", "regression", "clustering", "preprocessing"]}, {"id": "term-sciq", "t": "SciQ", "tg": ["Benchmark", "NLP", "Scientific"], "d": "datasets", "x": "A science question answering dataset of 13679 crowdsourced multiple-choice science exam questions with supporting...", "l": "s", "k": ["sciq", "science", "question", "answering", "dataset", "crowdsourced", "multiple-choice", "exam", "questions", "supporting", "passages", "tests", "scientific", "knowledge", "reading"]}, {"id": "term-scirepeval", "t": "SciRepEval", "tg": ["Benchmark", "NLP", "Scientific", "Evaluation"], "d": "datasets", "x": "A benchmark for scientific document representation spanning 4 task formats across 24 scientific datasets. Tests the...", "l": "s", "k": ["scirepeval", "benchmark", "scientific", "document", "representation", "spanning", "task", "formats", "across", "datasets", "tests", "quality", "learned", "representations", "text"]}, {"id": "term-score-function", "t": "Score Function", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The gradient of the log-likelihood function with respect to the parameter, used in maximum likelihood estimation and...", "l": "s", "k": ["score", "function", "gradient", "log-likelihood", "respect", "parameter", "maximum", "likelihood", "estimation", "fisher", "information", "calculations", "expected", "value", "true"]}, {"id": "term-score-matching", "t": "Score Matching", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A method for estimating probability density functions by matching the gradient of the log-density (score function)...", "l": "s", "k": ["score", "matching", "method", "estimating", "probability", "density", "functions", "gradient", "log-density", "function", "rather", "itself", "avoids", "computing", "normalizing"]}, {"id": "term-score-based-generative-model", "t": "Score-Based Generative Model", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative model that learns the gradient of the log probability density (score function) of the data distribution,...", "l": "s", "k": ["score-based", "generative", "model", "learns", "gradient", "log", "probability", "density", "score", "function", "data", "distribution", "uses", "langevin", "dynamics"]}, {"id": "term-scratchpad-memory", "t": "Scratchpad Memory", "tg": ["Memory", "Architecture"], "d": "hardware", "x": "Software-managed on-chip memory providing fast deterministic access without cache hardware overhead. Used in AI...", "l": "s", "k": ["scratchpad", "memory", "software-managed", "on-chip", "providing", "fast", "deterministic", "access", "without", "cache", "hardware", "overhead", "accelerators", "store", "intermediate"]}, {"id": "term-script-theory", "t": "Script Theory", "tg": ["History", "Fundamentals"], "d": "history", "x": "A knowledge representation scheme proposed by Roger Schank and Robert Abelson in 1977 describing stereotyped sequences...", "l": "s", "k": ["script", "theory", "knowledge", "representation", "scheme", "proposed", "roger", "schank", "robert", "abelson", "describing", "stereotyped", "sequences", "events", "particular"]}, {"id": "term-scrolls", "t": "SCROLLS", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "Standardized CompaRison Over Long Language Sequences a benchmark of 7 tasks requiring reasoning over long texts. Tests...", "l": "s", "k": ["scrolls", "standardized", "comparison", "long", "language", "sequences", "benchmark", "tasks", "requiring", "reasoning", "texts", "tests", "model", "capabilities", "long-context"]}, {"id": "term-sd3", "t": "SD3", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "Stable Diffusion 3 is a next-generation text-to-image model from Stability AI using a Multimodal Diffusion Transformer...", "l": "s", "k": ["sd3", "stable", "diffusion", "next-generation", "text-to-image", "model", "stability", "multimodal", "transformer", "mmdit", "architecture", "flow", "matching"]}, {"id": "term-sdxl", "t": "SDXL", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "Stable Diffusion XL, an improved latent diffusion model that uses a larger UNet with dual text encoders and an optional...", "l": "s", "k": ["sdxl", "stable", "diffusion", "improved", "latent", "model", "uses", "larger", "unet", "dual", "text", "encoders", "optional", "refiner", "generate"]}, {"id": "term-sdxl-turbo", "t": "SDXL Turbo", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A distilled version of Stable Diffusion XL that generates high-quality images in a single step using Adversarial...", "l": "s", "k": ["sdxl", "turbo", "distilled", "version", "stable", "diffusion", "generates", "high-quality", "images", "single", "step", "adversarial", "distillation"]}, {"id": "term-sdxl-lightning", "t": "SDXL-Lightning", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A distilled version of SDXL that generates high-quality images in 1-4 steps using progressive and adversarial...", "l": "s", "k": ["sdxl-lightning", "distilled", "version", "sdxl", "generates", "high-quality", "images", "1-4", "steps", "progressive", "adversarial", "distillation", "techniques"]}, {"id": "term-seallm", "t": "SeaLLM", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A multilingual large language model tailored for Southeast Asian languages including Thai and Vietnamese and Indonesian...", "l": "s", "k": ["seallm", "multilingual", "large", "language", "model", "tailored", "southeast", "asian", "languages", "including", "thai", "vietnamese", "indonesian", "malay"]}, {"id": "term-seam-carving-algorithm", "t": "Seam Carving Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A content-aware image resizing technique that removes low-energy seams (connected paths of pixels) to reduce image...", "l": "s", "k": ["seam", "carving", "algorithm", "content-aware", "image", "resizing", "technique", "removes", "low-energy", "seams", "connected", "paths", "pixels", "reduce", "dimensions"]}, {"id": "term-seamlessm4t", "t": "SeamlessM4T", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "A multilingual and multimodal translation model from Meta AI supporting speech-to-speech and speech-to-text and...", "l": "s", "k": ["seamlessm4t", "multilingual", "multimodal", "translation", "model", "meta", "supporting", "speech-to-speech", "speech-to-text", "text-to-speech", "across", "nearly", "languages"]}, {"id": "term-seasonal-decomposition", "t": "Seasonal Decomposition", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A time series analysis technique that separates a time series into trend, seasonal, and residual components, either...", "l": "s", "k": ["seasonal", "decomposition", "time", "series", "analysis", "technique", "separates", "trend", "residual", "components", "additively", "multiplicatively", "better", "understand", "underlying"]}, {"id": "term-sebastian-thrun", "t": "Sebastian Thrun", "tg": ["History", "Pioneers"], "d": "history", "x": "German-American computer scientist who led the Stanford Racing Team to victory in the 2005 DARPA Grand Challenge....", "l": "s", "k": ["sebastian", "thrun", "german-american", "computer", "scientist", "led", "stanford", "racing", "team", "victory", "darpa", "grand", "challenge", "co-founder", "udacity"]}, {"id": "term-secant-method", "t": "Secant Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A root-finding algorithm that uses a succession of secant lines to approximate the root of a function. Does not require...", "l": "s", "k": ["secant", "method", "root-finding", "algorithm", "uses", "succession", "lines", "approximate", "root", "function", "require", "derivative", "computation", "converges", "superlinearly"]}, {"id": "term-second", "t": "SECOND", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "Sparsely Embedded Convolutional Detection is an efficient 3D object detection model that uses sparse convolutions on...", "l": "s", "k": ["sparsely", "embedded", "convolutional", "detection", "efficient", "object", "model", "uses", "sparse", "convolutions", "voxelized", "point", "clouds", "fast", "inference"]}, {"id": "term-second-ai-winter", "t": "Second AI Winter", "tg": ["History", "Milestones"], "d": "history", "x": "The period from approximately 1987 to 1993 when enthusiasm for AI again collapsed following the failure of expert...", "l": "s", "k": ["winter", "period", "approximately", "enthusiasm", "collapsed", "following", "failure", "expert", "systems", "scale", "collapse", "lisp", "machine", "market", "cuts"]}, {"id": "term-sector-specific-ai-regulation", "t": "Sector-Specific AI Regulation", "tg": ["Safety", "Policy"], "d": "safety", "x": "AI regulations tailored to specific industries such as healthcare finance transportation and education. Recognizes that...", "l": "s", "k": ["sector-specific", "regulation", "regulations", "tailored", "specific", "industries", "healthcare", "finance", "transportation", "education", "recognizes", "different", "sectors", "risk", "profiles"]}, {"id": "term-secure-aggregation-algorithm", "t": "Secure Aggregation Algorithm", "tg": ["Algorithms", "Technical", "Privacy"], "d": "algorithms", "x": "A cryptographic protocol that enables a server to compute the sum of client updates without learning individual...", "l": "s", "k": ["secure", "aggregation", "algorithm", "cryptographic", "protocol", "enables", "server", "compute", "sum", "client", "updates", "without", "learning", "individual", "contributions"]}, {"id": "term-secure-multi-party-computation", "t": "Secure Multi-Party Computation", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "A cryptographic protocol that allows multiple parties to jointly compute a function over their combined inputs while...", "l": "s", "k": ["secure", "multi-party", "computation", "cryptographic", "protocol", "allows", "multiple", "parties", "jointly", "compute", "function", "combined", "inputs", "keeping", "party"]}, {"id": "term-seed-bench", "t": "SEED-Bench", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A multimodal benchmark evaluating large multimodal models across 19000 questions covering 12 evaluation dimensions from...", "l": "s", "k": ["seed-bench", "multimodal", "benchmark", "evaluating", "large", "models", "across", "questions", "covering", "evaluation", "dimensions", "image", "video", "understanding"]}, {"id": "term-seggpt", "t": "SegGPT", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A generalist model for segmentation in context that performs diverse segmentation tasks through in-context visual...", "l": "s", "k": ["seggpt", "generalist", "model", "segmentation", "context", "performs", "diverse", "tasks", "in-context", "visual", "prompting", "without", "task-specific", "fine-tuning"]}, {"id": "term-segment-anything-model", "t": "Segment Anything Model", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A foundation model for image segmentation (SAM) trained on a massive dataset that can segment any object in any image...", "l": "s", "k": ["segment", "anything", "model", "foundation", "image", "segmentation", "sam", "trained", "massive", "dataset", "object", "given", "prompt", "point", "bounding"]}, {"id": "term-segment-anything-model-2", "t": "Segment Anything Model 2", "tg": ["Models", "Technical"], "d": "models", "x": "An extension of SAM that handles video segmentation in addition to images. Processes video frames with a memory...", "l": "s", "k": ["segment", "anything", "model", "extension", "sam", "handles", "video", "segmentation", "addition", "images", "processes", "frames", "memory", "mechanism", "temporal"]}, {"id": "term-segment-tree-algorithm", "t": "Segment Tree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A tree data structure for storing intervals or segments that supports range queries and point updates in O(log n) time....", "l": "s", "k": ["segment", "tree", "algorithm", "data", "structure", "storing", "intervals", "segments", "supports", "range", "queries", "point", "updates", "log", "time"]}, {"id": "term-selection-bias", "t": "Selection Bias", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A systematic error arising when the sample used for analysis is not representative of the population, due to the way...", "l": "s", "k": ["selection", "bias", "systematic", "error", "arising", "sample", "analysis", "representative", "population", "due", "observations", "were", "selected", "lead", "models"]}, {"id": "term-selection-sort", "t": "Selection Sort", "tg": ["Algorithms", "Fundamentals", "Sorting"], "d": "algorithms", "x": "A simple sorting algorithm that repeatedly finds the minimum element from the unsorted portion and places it at the...", "l": "s", "k": ["selection", "sort", "simple", "sorting", "algorithm", "repeatedly", "finds", "minimum", "element", "unsorted", "portion", "places", "beginning", "always", "runs"]}, {"id": "term-selectional-preference", "t": "Selectional Preference", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The tendency of predicates to semantically constrain their arguments, such as 'eat' preferring edible objects, used in...", "l": "s", "k": ["selectional", "preference", "tendency", "predicates", "semantically", "constrain", "arguments", "eat", "preferring", "edible", "objects", "nlp", "disambiguation", "semantic", "plausibility"]}, {"id": "term-selective-context", "t": "Selective Context", "tg": ["Prompt Engineering", "Compression"], "d": "general", "x": "A prompt compression method that evaluates the informativeness of each lexical unit in a context using self-information...", "l": "s", "k": ["selective", "context", "prompt", "compression", "method", "evaluates", "informativeness", "lexical", "unit", "self-information", "scores", "causal", "language", "model", "filters"]}, {"id": "term-selective-search", "t": "Selective Search", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An object proposal generation method that combines multiple grouping strategies including color and texture and size...", "l": "s", "k": ["selective", "search", "object", "proposal", "generation", "method", "combines", "multiple", "grouping", "strategies", "including", "color", "texture", "size", "similarity"]}, {"id": "term-self-ask", "t": "Self-Ask", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting technique where the model explicitly asks itself follow-up questions needed to answer a complex query, then...", "l": "s", "k": ["self-ask", "prompting", "technique", "model", "explicitly", "asks", "itself", "follow-up", "questions", "needed", "answer", "complex", "query", "answers", "sub-question"]}, {"id": "term-self-attention", "t": "Self-Attention", "tg": ["Architecture", "Transformers"], "d": "models", "x": "An attention mechanism where a sequence attends to itself, allowing each position to consider all other positions. The...", "l": "s", "k": ["self-attention", "attention", "mechanism", "sequence", "attends", "itself", "allowing", "position", "consider", "positions", "core", "operation", "transformer", "models"]}, {"id": "term-self-attention-mechanism", "t": "Self-Attention Mechanism", "tg": ["History", "Fundamentals"], "d": "history", "x": "A mechanism that allows each position in a sequence to attend to all other positions introduced as a key component of...", "l": "s", "k": ["self-attention", "mechanism", "allows", "position", "sequence", "attend", "positions", "introduced", "key", "component", "transformer", "architecture", "vaswani", "enables", "capturing"]}, {"id": "term-self-bleu", "t": "Self-BLEU", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A diversity metric that computes BLEU scores between pairs of generated sentences from the same model, where lower...", "l": "s", "k": ["self-bleu", "diversity", "metric", "computes", "bleu", "scores", "pairs", "generated", "sentences", "model", "lower", "indicates", "greater", "less", "repetition"]}, {"id": "term-self-consistency", "t": "Self-Consistency", "tg": ["Prompting", "Reasoning"], "d": "general", "x": "A technique where AI generates multiple reasoning paths and selects the most common answer. Improves accuracy for...", "l": "s", "k": ["self-consistency", "technique", "generates", "multiple", "reasoning", "paths", "selects", "common", "answer", "improves", "accuracy", "complex", "aggregating", "diverse", "approaches"]}, {"id": "term-self-driving-car-history", "t": "Self-Driving Car History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of autonomous vehicles from early projects like Stanford Cart in 1961 and CMU's Navlab in 1986 through...", "l": "s", "k": ["self-driving", "car", "history", "evolution", "autonomous", "vehicles", "early", "projects", "stanford", "cart", "cmu", "navlab", "darpa", "grand", "challenges"]}, {"id": "term-self-instruct", "t": "Self-Instruct", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A framework and dataset for bootstrapping instruction-following data from a language model's own generations. Generates...", "l": "s", "k": ["self-instruct", "framework", "dataset", "bootstrapping", "instruction-following", "data", "language", "model", "generations", "generates", "instructions", "demonstrations", "minimal", "seeds"]}, {"id": "term-self-organizing-map-algorithm", "t": "Self-Organizing Map Algorithm", "tg": ["Algorithms", "Fundamentals", "Clustering"], "d": "algorithms", "x": "An unsupervised neural network that produces a low-dimensional discretized representation of the input space. Neurons...", "l": "s", "k": ["self-organizing", "map", "algorithm", "unsupervised", "neural", "network", "produces", "low-dimensional", "discretized", "representation", "input", "space", "neurons", "grid", "compete"]}, {"id": "term-self-play", "t": "Self-Play", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A training paradigm where an agent improves by playing against copies of itself, generating increasingly challenging...", "l": "s", "k": ["self-play", "training", "paradigm", "agent", "improves", "playing", "against", "copies", "itself", "generating", "increasingly", "challenging", "opponents", "skill", "grows"]}, {"id": "term-self-play-algorithm", "t": "Self-Play Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A training paradigm where an agent learns by playing against copies of itself. Gradually increases skill level without...", "l": "s", "k": ["self-play", "algorithm", "training", "paradigm", "agent", "learns", "playing", "against", "copies", "itself", "gradually", "increases", "skill", "level", "without"]}, {"id": "term-self-polish-prompting", "t": "Self-Polish Prompting", "tg": ["Prompt Engineering", "Refinement"], "d": "general", "x": "A technique that prompts the model to progressively refine and polish the given problem conditions before solving,...", "l": "s", "k": ["self-polish", "prompting", "technique", "prompts", "model", "progressively", "refine", "polish", "given", "problem", "conditions", "solving", "enabling", "simplify", "complex"]}, {"id": "term-self-rag", "t": "Self-RAG", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A framework where the language model learns to retrieve on demand, generate text, and critique its own output using...", "l": "s", "k": ["self-rag", "framework", "language", "model", "learns", "retrieve", "demand", "generate", "text", "critique", "output", "special", "reflection", "tokens", "adaptively"]}, {"id": "term-self-supervised-learning", "t": "Self-Supervised Learning", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A machine learning paradigm where the model generates its own supervisory signal from unlabeled data through pretext...", "l": "s", "k": ["self-supervised", "learning", "machine", "paradigm", "model", "generates", "supervisory", "signal", "unlabeled", "data", "pretext", "tasks", "examples", "include", "masked"]}, {"id": "term-self-supervised-learning-history", "t": "Self-Supervised Learning History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of self-supervised learning where models learn representations from unlabeled data by solving pretext...", "l": "s", "k": ["self-supervised", "learning", "history", "development", "models", "learn", "representations", "unlabeled", "data", "solving", "pretext", "tasks", "autoencoders", "word2vec", "bert"]}, {"id": "term-self-training", "t": "Self-Training", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A semi-supervised learning method where a model trained on labeled data generates pseudo-labels for unlabeled data,...", "l": "s", "k": ["self-training", "semi-supervised", "learning", "method", "model", "trained", "labeled", "data", "generates", "pseudo-labels", "unlabeled", "retrains", "real", "pseudo-labeled", "examples"]}, {"id": "term-self-training-vision", "t": "Self-Training for Vision", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A semi-supervised learning approach where a teacher model generates pseudo-labels for unlabeled images, and a student...", "l": "s", "k": ["self-training", "vision", "semi-supervised", "learning", "approach", "teacher", "model", "generates", "pseudo-labels", "unlabeled", "images", "student", "trained", "combination", "labeled"]}, {"id": "term-selu", "t": "SELU", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Scaled Exponential Linear Unit that enables self-normalizing neural networks. Combines the ELU function with specific...", "l": "s", "k": ["selu", "scaled", "exponential", "linear", "unit", "enables", "self-normalizing", "neural", "networks", "combines", "elu", "function", "specific", "scale", "alpha"]}, {"id": "term-semantic-caching", "t": "Semantic Caching", "tg": ["LLM", "Inference"], "d": "models", "x": "A caching strategy that stores LLM responses indexed by the semantic meaning of queries rather than exact string...", "l": "s", "k": ["semantic", "caching", "strategy", "stores", "llm", "responses", "indexed", "meaning", "queries", "rather", "exact", "string", "matches", "allowing", "cache"]}, {"id": "term-semantic-chunking", "t": "Semantic Chunking", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "A document splitting approach that determines chunk boundaries based on semantic similarity between consecutive...", "l": "s", "k": ["semantic", "chunking", "document", "splitting", "approach", "determines", "chunk", "boundaries", "based", "similarity", "consecutive", "sentences", "creating", "chunks", "topic"]}, {"id": "term-semantic-correspondence", "t": "Semantic Correspondence", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of finding matching points between images of different instances of the same object category, requiring...", "l": "s", "k": ["semantic", "correspondence", "task", "finding", "matching", "points", "images", "different", "instances", "object", "category", "requiring", "understanding", "similarity", "rather"]}, {"id": "term-semantic-kernel", "t": "Semantic Kernel", "tg": ["Framework", "Application"], "d": "general", "x": "Microsoft's open-source SDK for building AI applications with LLMs. Supports plugin architecture, memory, and planning...", "l": "s", "k": ["semantic", "kernel", "microsoft", "open-source", "sdk", "building", "applications", "llms", "supports", "plugin", "architecture", "memory", "planning", "enterprise", "agent"]}, {"id": "term-semantic-map", "t": "Semantic Map", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A spatial representation that annotates each location with semantic labels indicating what type of object or surface...", "l": "s", "k": ["semantic", "map", "spatial", "representation", "annotates", "location", "labels", "indicating", "type", "object", "surface", "occupies", "space", "robotics", "navigation"]}, {"id": "term-semantic-networks", "t": "Semantic Networks", "tg": ["History", "Milestones"], "d": "history", "x": "A knowledge representation formalism using graphs of nodes connected by labeled edges to represent concepts and their...", "l": "s", "k": ["semantic", "networks", "knowledge", "representation", "formalism", "graphs", "nodes", "connected", "labeled", "edges", "represent", "concepts", "relationships", "proposed", "ross"]}, {"id": "term-semantic-parsing", "t": "Semantic Parsing", "tg": ["NLP", "Parsing"], "d": "general", "x": "The task of mapping natural language utterances to formal meaning representations such as logical forms, SQL queries,...", "l": "s", "k": ["semantic", "parsing", "task", "mapping", "natural", "language", "utterances", "formal", "meaning", "representations", "logical", "forms", "sql", "queries", "lambda"]}, {"id": "term-semantic-role-labeling", "t": "Semantic Role Labeling", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of identifying the predicate-argument structure of a sentence by assigning semantic roles such as agent,...", "l": "s", "k": ["semantic", "role", "labeling", "task", "identifying", "predicate-argument", "structure", "sentence", "assigning", "roles", "agent", "patient", "instrument", "location", "constituents"]}, {"id": "term-semantic-role-labeling-algorithm", "t": "Semantic Role Labeling Algorithm", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "An algorithm that identifies the predicate-argument structure of sentences by assigning semantic roles to constituents....", "l": "s", "k": ["semantic", "role", "labeling", "algorithm", "identifies", "predicate-argument", "structure", "sentences", "assigning", "roles", "constituents", "determines", "predicate", "sentence"]}, {"id": "term-semantic-scholar-corpus", "t": "Semantic Scholar Corpus", "tg": ["Training Corpus", "NLP", "Scientific"], "d": "datasets", "x": "A comprehensive corpus of academic papers with metadata citations and abstracts maintained by AI2. Used for scientific...", "l": "s", "k": ["semantic", "scholar", "corpus", "comprehensive", "academic", "papers", "metadata", "citations", "abstracts", "maintained", "ai2", "scientific", "information", "extraction", "citation"]}, {"id": "term-semantic-search", "t": "Semantic Search", "tg": ["Application", "Search"], "d": "general", "x": "Search based on meaning rather than keyword matching. Uses embeddings to find conceptually similar content, enabling...", "l": "s", "k": ["semantic", "search", "based", "meaning", "rather", "keyword", "matching", "uses", "embeddings", "find", "conceptually", "similar", "content", "enabling", "relevant"]}, {"id": "term-semantic-segmentation", "t": "Semantic Segmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A computer vision task that assigns a class label to every pixel in an image, producing a dense prediction map that...", "l": "s", "k": ["semantic", "segmentation", "computer", "vision", "task", "assigns", "class", "label", "pixel", "image", "producing", "dense", "prediction", "map", "identifies"]}, {"id": "term-semantic-segmentation-transformer", "t": "Semantic Segmentation Transformer", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Transformer-based architectures like SegFormer and Mask2Former that apply self-attention to image features for dense...", "l": "s", "k": ["semantic", "segmentation", "transformer", "transformer-based", "architectures", "segformer", "mask2former", "apply", "self-attention", "image", "features", "dense", "per-pixel", "classification", "outperforming"]}, {"id": "term-semantic-textual-similarity", "t": "Semantic Textual Similarity", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of measuring the degree of semantic equivalence between two text segments on a continuous scale, going beyond...", "l": "s", "k": ["semantic", "textual", "similarity", "task", "measuring", "degree", "equivalence", "text", "segments", "continuous", "scale", "going", "beyond", "binary", "paraphrase"]}, {"id": "term-semantic-web", "t": "Semantic Web", "tg": ["History", "Fundamentals"], "d": "history", "x": "A vision proposed by Tim Berners-Lee in 2001 for extending the World Wide Web with machine-readable metadata. The...", "l": "s", "k": ["semantic", "web", "vision", "proposed", "tim", "berners-lee", "extending", "world", "wide", "machine-readable", "metadata", "uses", "ontologies", "rdf", "owl"]}, {"id": "term-semantics", "t": "Semantics", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The branch of linguistics studying meaning in language, including word meaning, sentence meaning, and the relationship...", "l": "s", "k": ["semantics", "branch", "linguistics", "studying", "meaning", "language", "including", "word", "sentence", "relationship", "linguistic", "expressions", "refer"]}, {"id": "term-semi-supervised-learning", "t": "Semi-Supervised Learning", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A learning paradigm that combines a small amount of labeled data with a large amount of unlabeled data during training....", "l": "s", "k": ["semi-supervised", "learning", "paradigm", "combines", "small", "amount", "labeled", "data", "large", "unlabeled", "training", "leverages", "structure", "improve", "model"]}, {"id": "term-semi-supervised-object-detection", "t": "Semi-Supervised Object Detection", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Detection training approaches that leverage both a small set of labeled images and a large pool of unlabeled images,...", "l": "s", "k": ["semi-supervised", "object", "detection", "training", "approaches", "leverage", "small", "labeled", "images", "large", "pool", "unlabeled", "techniques", "pseudo-labeling", "consistency"]}, {"id": "term-semiconductor", "t": "Semiconductor", "tg": ["Fabrication", "Fundamentals"], "d": "hardware", "x": "Material with electrical conductivity between a conductor and insulator typically silicon. Forms the physical...", "l": "s", "k": ["semiconductor", "material", "electrical", "conductivity", "conductor", "insulator", "typically", "silicon", "forms", "physical", "foundation", "modern", "computing", "chips", "including"]}, {"id": "term-semiconductor-supply-chain", "t": "Semiconductor Supply Chain", "tg": ["Manufacturing", "Supply Chain", "Global"], "d": "hardware", "x": "Global network of companies involved in designing manufacturing and packaging semiconductor chips. Concentration of...", "l": "s", "k": ["semiconductor", "supply", "chain", "global", "network", "companies", "involved", "designing", "manufacturing", "packaging", "chips", "concentration", "advanced", "taiwan", "poses"]}, {"id": "term-sensitivity", "t": "Sensitivity", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "The proportion of actual positive cases correctly identified by a classifier, synonymous with recall and true positive...", "l": "s", "k": ["sensitivity", "proportion", "actual", "positive", "cases", "correctly", "identified", "classifier", "synonymous", "recall", "true", "rate", "high", "minimizes", "false"]}, {"id": "term-sentence-boundary-detection", "t": "Sentence Boundary Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying where sentences begin and end in running text, handling ambiguous punctuation like periods in...", "l": "s", "k": ["sentence", "boundary", "detection", "task", "identifying", "sentences", "begin", "end", "running", "text", "handling", "ambiguous", "punctuation", "periods", "abbreviations"]}, {"id": "term-sentence-embedding", "t": "Sentence Embedding", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A fixed-length dense vector representation of an entire sentence that captures its semantic meaning, produced by...", "l": "s", "k": ["sentence", "embedding", "fixed-length", "dense", "vector", "representation", "entire", "captures", "semantic", "meaning", "produced", "methods", "mean", "pooling", "token"]}, {"id": "term-sentence-transformer", "t": "Sentence Transformer", "tg": ["Model Type", "Embeddings"], "d": "models", "x": "Models that encode entire sentences into single vectors, capturing semantic meaning. Popular for semantic search,...", "l": "s", "k": ["sentence", "transformer", "models", "encode", "entire", "sentences", "single", "vectors", "capturing", "semantic", "meaning", "popular", "search", "similarity", "matching"]}, {"id": "term-sentence-bert", "t": "Sentence-BERT", "tg": ["Models", "Technical"], "d": "models", "x": "A modification of BERT that uses siamese and triplet networks to derive semantically meaningful sentence embeddings....", "l": "s", "k": ["sentence-bert", "modification", "bert", "uses", "siamese", "triplet", "networks", "derive", "semantically", "meaningful", "sentence", "embeddings", "enables", "efficient", "semantic"]}, {"id": "term-sentencepiece", "t": "SentencePiece", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A language-independent tokenization library that treats the input as a raw byte stream and applies BPE or unigram...", "l": "s", "k": ["sentencepiece", "language-independent", "tokenization", "library", "treats", "input", "raw", "byte", "stream", "applies", "bpe", "unigram", "directly", "without", "pre-tokenization"]}, {"id": "term-sentencepiece-algorithm", "t": "SentencePiece Algorithm", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "A language-independent subword tokenizer and detokenizer that directly processes raw text without pre-tokenization....", "l": "s", "k": ["sentencepiece", "algorithm", "language-independent", "subword", "tokenizer", "detokenizer", "directly", "processes", "raw", "text", "without", "pre-tokenization", "supports", "byte-pair", "encoding"]}, {"id": "term-sentiment-analysis", "t": "Sentiment Analysis", "tg": ["NLP Task", "Classification"], "d": "general", "x": "An NLP task that determines the emotional tone of text (positive, negative, neutral). Used in customer feedback...", "l": "s", "k": ["sentiment", "analysis", "nlp", "task", "determines", "emotional", "tone", "text", "positive", "negative", "neutral", "customer", "feedback", "social", "media"]}, {"id": "term-sentiment-polarity", "t": "Sentiment Polarity", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The classification of text sentiment into categories such as positive, negative, or neutral, representing the overall...", "l": "s", "k": ["sentiment", "polarity", "classification", "text", "categories", "positive", "negative", "neutral", "representing", "overall", "emotional", "orientation", "expressed", "toward", "subject"]}, {"id": "term-sepp-hochreiter", "t": "Sepp Hochreiter", "tg": ["History", "Pioneers"], "d": "history", "x": "Austrian computer scientist who co-invented the Long Short-Term Memory network architecture in 1997 with Jurgen...", "l": "s", "k": ["sepp", "hochreiter", "austrian", "computer", "scientist", "co-invented", "long", "short-term", "memory", "network", "architecture", "jurgen", "schmidhuber", "solving", "fundamental"]}, {"id": "term-seq2seq", "t": "Seq2Seq", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Sequence-to-Sequence, an encoder-decoder framework where an encoder processes an input sequence into a fixed-length...", "l": "s", "k": ["seq2seq", "sequence-to-sequence", "encoder-decoder", "framework", "encoder", "processes", "input", "sequence", "fixed-length", "representation", "decoder", "generates", "output"]}, {"id": "term-seq2seq-model", "t": "Seq2Seq Model", "tg": ["History", "Milestones"], "d": "history", "x": "The sequence-to-sequence model introduced by Ilya Sutskever Oriol Vinyals and Quoc Le at Google in 2014. Using...", "l": "s", "k": ["seq2seq", "model", "sequence-to-sequence", "introduced", "ilya", "sutskever", "oriol", "vinyals", "quoc", "google", "encoder-decoder", "rnn", "architecture", "enabled", "end-to-end"]}, {"id": "term-sequence-alignment-algorithm", "t": "Sequence Alignment Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP"], "d": "algorithms", "x": "An algorithm that arranges sequences to identify regions of similarity. Needleman-Wunsch performs global alignment and...", "l": "s", "k": ["sequence", "alignment", "algorithm", "arranges", "sequences", "identify", "regions", "similarity", "needleman-wunsch", "performs", "global", "smith-waterman", "local", "dynamic", "programming"]}, {"id": "term-sequence-labeling", "t": "Sequence Labeling", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of assigning a categorical label to each element in a sequence, such as tagging each word in a sentence with...", "l": "s", "k": ["sequence", "labeling", "task", "assigning", "categorical", "label", "element", "tagging", "word", "sentence", "named", "entity", "type", "pos", "tag"]}, {"id": "term-sequence-parallelism", "t": "Sequence Parallelism", "tg": ["LLM", "Inference"], "d": "models", "x": "A technique that distributes the processing of long sequences across multiple devices by partitioning the sequence...", "l": "s", "k": ["sequence", "parallelism", "technique", "distributes", "processing", "long", "sequences", "across", "multiple", "devices", "partitioning", "dimension", "enabling", "context", "lengths"]}, {"id": "term-sequence-level-accuracy", "t": "Sequence-Level Accuracy", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation metric that considers an entire generated sequence correct only if every token matches the reference,...", "l": "s", "k": ["sequence-level", "accuracy", "evaluation", "metric", "considers", "entire", "generated", "sequence", "correct", "token", "matches", "reference", "providing", "strict", "holistic"]}, {"id": "term-sequential-quadratic-programming", "t": "Sequential Quadratic Programming", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An iterative optimization method for nonlinear constrained problems that solves a quadratic programming subproblem at...", "l": "s", "k": ["sequential", "quadratic", "programming", "iterative", "optimization", "method", "nonlinear", "constrained", "problems", "solves", "subproblem", "step", "approximates", "lagrangian", "hessian"]}, {"id": "term-server-rack", "t": "Server Rack", "tg": ["Data Center", "Infrastructure", "Physical"], "d": "hardware", "x": "Standardized enclosure for mounting server equipment in data centers. AI server racks are evolving to support higher...", "l": "s", "k": ["server", "rack", "standardized", "enclosure", "mounting", "equipment", "data", "centers", "racks", "evolving", "support", "higher", "power", "densities", "required"]}, {"id": "term-seymour-papert", "t": "Seymour Papert", "tg": ["History", "Pioneers"], "d": "history", "x": "South African-born American mathematician and computer scientist who co-authored the influential book Perceptrons...", "l": "s", "k": ["seymour", "papert", "south", "african-born", "american", "mathematician", "computer", "scientist", "co-authored", "influential", "book", "perceptrons", "marvin", "minsky", "co-founded"]}, {"id": "term-sft", "t": "SFT (Supervised Fine-Tuning)", "tg": ["Training", "Alignment"], "d": "safety", "x": "Training a pre-trained model on labeled examples of desired behavior. Often the first step in aligning LLMs, teaching...", "l": "s", "k": ["sft", "supervised", "fine-tuning", "training", "pre-trained", "model", "labeled", "examples", "desired", "behavior", "step", "aligning", "llms", "teaching", "follow"]}, {"id": "term-sgi-origin", "t": "SGI Origin", "tg": ["Historical", "System", "Server"], "d": "hardware", "x": "Silicon Graphics server series using MIPS processors and ccNUMA architecture. Used for scientific visualization and...", "l": "s", "k": ["sgi", "origin", "silicon", "graphics", "server", "series", "mips", "processors", "ccnuma", "architecture", "scientific", "visualization", "early", "neural", "network"]}, {"id": "term-sgpt", "t": "SGPT", "tg": ["Models", "Technical", "Embedding", "NLP"], "d": "models", "x": "Sentence embeddings from GPT models that use position-weighted mean pooling or cross-encoder approaches for semantic...", "l": "s", "k": ["sgpt", "sentence", "embeddings", "gpt", "models", "position-weighted", "mean", "pooling", "cross-encoder", "approaches", "semantic", "search", "similarity"]}, {"id": "term-shadow-ai", "t": "Shadow AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "The use of unauthorized or unvetted AI tools and systems by employees within an organization. Poses risks to data...", "l": "s", "k": ["shadow", "unauthorized", "unvetted", "tools", "systems", "employees", "within", "organization", "poses", "risks", "data", "security", "regulatory", "compliance", "organizational"]}, {"id": "term-shakey-the-robot", "t": "Shakey the Robot", "tg": ["History", "Milestones"], "d": "history", "x": "The first general-purpose mobile robot developed at SRI International from 1966 to 1972 that could reason about its own...", "l": "s", "k": ["shakey", "robot", "general-purpose", "mobile", "developed", "sri", "international", "reason", "actions", "integrating", "computer", "vision", "natural", "language", "understanding"]}, {"id": "term-shannon-entropy", "t": "Shannon Entropy", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A measure of the average information content or uncertainty in a random variable defined as the expected value of the...", "l": "s", "k": ["shannon", "entropy", "measure", "average", "information", "content", "uncertainty", "random", "variable", "defined", "expected", "value", "negative", "log", "probability"]}, {"id": "term-shannon-fano-coding", "t": "Shannon-Fano Coding", "tg": ["Algorithms", "Technical", "Information Theory", "History"], "d": "algorithms", "x": "An early entropy coding algorithm that recursively partitions symbols into two groups of approximately equal...", "l": "s", "k": ["shannon-fano", "coding", "early", "entropy", "algorithm", "recursively", "partitions", "symbols", "groups", "approximately", "equal", "probability", "precursor", "huffman", "always"]}, {"id": "term-shap-values", "t": "SHAP Values", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "SHapley Additive exPlanations, a unified framework for feature importance that assigns each feature a contribution...", "l": "s", "k": ["shap", "values", "shapley", "additive", "explanations", "unified", "framework", "feature", "importance", "assigns", "contribution", "value", "specific", "prediction", "based"]}, {"id": "term-shap-e", "t": "Shap-E", "tg": ["Models", "Technical"], "d": "models", "x": "A model by OpenAI that directly generates 3D implicit functions from text or images. Produces textured 3D meshes that...", "l": "s", "k": ["shap-e", "model", "openai", "directly", "generates", "implicit", "functions", "text", "images", "produces", "textured", "meshes", "rendered", "viewpoint", "faster"]}, {"id": "term-shaped-reward", "t": "Shaped Reward", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "An auxiliary reward signal added to the environment's natural reward to guide learning toward desired behavior. Shaped...", "l": "s", "k": ["shaped", "reward", "auxiliary", "signal", "added", "environment", "natural", "guide", "learning", "toward", "desired", "behavior", "rewards", "provide", "frequent"]}, {"id": "term-shapenet", "t": "ShapeNet", "tg": ["Benchmark", "3D", "Computer Vision"], "d": "datasets", "x": "A large-scale repository of 3D CAD models with rich annotations covering 55 common object categories. Used for 3D...", "l": "s", "k": ["shapenet", "large-scale", "repository", "cad", "models", "rich", "annotations", "covering", "common", "object", "categories", "classification", "reconstruction", "generation", "research"]}, {"id": "term-shapiro-wilk-test", "t": "Shapiro-Wilk Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A statistical test for normality that evaluates whether a random sample comes from a normal distribution. It is...", "l": "s", "k": ["shapiro-wilk", "test", "statistical", "normality", "evaluates", "random", "sample", "comes", "normal", "distribution", "considered", "powerful", "tests", "especially", "small"]}, {"id": "term-shared-memory-gpu", "t": "Shared Memory (GPU)", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "A fast, programmer-managed on-chip SRAM in GPU streaming multiprocessors that enables efficient data sharing between...", "l": "s", "k": ["shared", "memory", "gpu", "fast", "programmer-managed", "on-chip", "sram", "streaming", "multiprocessors", "enables", "efficient", "data", "sharing", "threads", "thread"]}, {"id": "term-shared-nearest-neighbor-clustering", "t": "Shared Nearest Neighbor Clustering", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A clustering approach that measures similarity between points based on the number of shared nearest neighbors. More...", "l": "s", "k": ["shared", "nearest", "neighbor", "clustering", "approach", "measures", "similarity", "points", "based", "number", "neighbors", "robust", "varying", "densities", "distance-based"]}, {"id": "term-sharegpt", "t": "ShareGPT", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A collection of user-shared conversations with ChatGPT used for training open-source instruction-following language...", "l": "s", "k": ["sharegpt", "collection", "user-shared", "conversations", "chatgpt", "training", "open-source", "instruction-following", "language", "models", "provides", "diverse", "real-world", "conversational", "data"]}, {"id": "term-sharegpt4v", "t": "ShareGPT4V", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A vision-language model trained on highly detailed image descriptions generated by GPT-4V for improved visual...", "l": "s", "k": ["sharegpt4v", "vision-language", "model", "trained", "highly", "detailed", "image", "descriptions", "generated", "gpt-4v", "improved", "visual", "understanding", "caption", "generation"]}, {"id": "term-sharpness-aware-minimization", "t": "Sharpness-Aware Minimization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An optimization procedure that seeks parameters in neighborhoods with uniformly low loss rather than just minimizing...", "l": "s", "k": ["sharpness-aware", "minimization", "optimization", "procedure", "seeks", "parameters", "neighborhoods", "uniformly", "low", "loss", "rather", "minimizing", "single", "point", "improves"]}, {"id": "term-shell-sort", "t": "Shell Sort", "tg": ["Algorithms", "Technical", "Sorting"], "d": "algorithms", "x": "A generalization of insertion sort that compares elements separated by a gap and progressively reduces the gap....", "l": "s", "k": ["shell", "sort", "generalization", "insertion", "compares", "elements", "separated", "gap", "progressively", "reduces", "performance", "depends", "sequence", "best", "known"]}, {"id": "term-shieldgemma", "t": "ShieldGemma", "tg": ["Models", "Technical", "NLP", "Safety"], "d": "models", "x": "A safety classifier built on Gemma models from Google that detects harmful content categories in language model inputs...", "l": "s", "k": ["shieldgemma", "safety", "classifier", "built", "gemma", "models", "google", "detects", "harmful", "content", "categories", "language", "model", "inputs", "outputs"]}, {"id": "term-short-time-fourier-transform", "t": "Short-Time Fourier Transform", "tg": ["Algorithms", "Fundamentals", "Signal Processing"], "d": "algorithms", "x": "A signal analysis method that divides a signal into short overlapping segments and applies the Fourier transform to...", "l": "s", "k": ["short-time", "fourier", "transform", "signal", "analysis", "method", "divides", "short", "overlapping", "segments", "applies", "provides", "time-frequency", "representation", "fixed"]}, {"id": "term-show-o", "t": "Show-o", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A unified Transformer model that handles both multimodal understanding and image generation by combining autoregressive...", "l": "s", "k": ["show-o", "unified", "transformer", "model", "handles", "multimodal", "understanding", "image", "generation", "combining", "autoregressive", "discrete", "diffusion", "modeling", "approaches"]}, {"id": "term-shrdlu", "t": "SHRDLU", "tg": ["History", "Milestones"], "d": "history", "x": "A natural language understanding program created by Terry Winograd at MIT in 1970 that could converse about and...", "l": "s", "k": ["shrdlu", "natural", "language", "understanding", "program", "created", "terry", "winograd", "mit", "converse", "manipulate", "objects", "simulated", "blocks", "world"]}, {"id": "term-shufflenet", "t": "ShuffleNet", "tg": ["Models", "Technical"], "d": "models", "x": "A lightweight CNN architecture designed for mobile devices that uses pointwise group convolutions and channel shuffle...", "l": "s", "k": ["shufflenet", "lightweight", "cnn", "architecture", "designed", "mobile", "devices", "uses", "pointwise", "group", "convolutions", "channel", "shuffle", "operations", "reduces"]}, {"id": "term-sift", "t": "SIFT", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Scale-Invariant Feature Transform, a classical algorithm that detects and describes local image features robust to...", "l": "s", "k": ["sift", "scale-invariant", "feature", "transform", "classical", "algorithm", "detects", "describes", "local", "image", "features", "robust", "scale", "rotation", "illumination"]}, {"id": "term-sift-algorithm", "t": "SIFT Algorithm", "tg": ["Algorithms", "Fundamentals", "Vision", "History"], "d": "algorithms", "x": "Scale-Invariant Feature Transform detects and describes local features in images that are invariant to scale and...", "l": "s", "k": ["sift", "algorithm", "scale-invariant", "feature", "transform", "detects", "describes", "local", "features", "images", "invariant", "scale", "rotation", "constructs", "space"]}, {"id": "term-siglip", "t": "SigLIP", "tg": ["Models", "Technical"], "d": "models", "x": "Sigmoid Loss for Language Image Pretraining replaces the softmax-based contrastive loss in CLIP with a sigmoid loss...", "l": "s", "k": ["siglip", "sigmoid", "loss", "language", "image", "pretraining", "replaces", "softmax-based", "contrastive", "clip", "operates", "individual", "image-text", "pairs", "simpler"]}, {"id": "term-sigmoid", "t": "Sigmoid", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A classic activation function that maps inputs to values between 0 and 1 using the formula f(x) = 1 / (1 + exp(-x))....", "l": "s", "k": ["sigmoid", "classic", "activation", "function", "maps", "inputs", "values", "formula", "exp", "historically", "important", "neural", "networks", "binary", "classification"]}, {"id": "term-signed-distance-function", "t": "Signed Distance Function", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A 3D representation where a neural network predicts the signed distance from any 3D point to the nearest surface, with...", "l": "s", "k": ["signed", "distance", "function", "representation", "neural", "network", "predicts", "point", "nearest", "surface", "zero-level", "defining", "shape", "enabling", "smooth"]}, {"id": "term-significance-level", "t": "Significance Level", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The threshold probability (alpha, typically 0.05) below which the p-value must fall for the null hypothesis to be...", "l": "s", "k": ["significance", "level", "threshold", "probability", "alpha", "typically", "p-value", "must", "fall", "null", "hypothesis", "rejected", "defines", "acceptable", "risk"]}, {"id": "term-silent-data-corruption", "t": "Silent Data Corruption", "tg": ["Reliability", "Data Integrity", "Challenge"], "d": "hardware", "x": "Undetected data errors in computing systems that produce incorrect results without triggering error signals. A growing...", "l": "s", "k": ["silent", "data", "corruption", "undetected", "errors", "computing", "systems", "produce", "incorrect", "results", "without", "triggering", "error", "signals", "growing"]}, {"id": "term-silhouette-score", "t": "Silhouette Score", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A metric for evaluating clustering quality that measures how similar each point is to its own cluster compared to other...", "l": "s", "k": ["silhouette", "score", "metric", "evaluating", "clustering", "quality", "measures", "similar", "point", "cluster", "compared", "clusters", "values", "range", "higher"]}, {"id": "term-silicon-photonics", "t": "Silicon Photonics", "tg": ["Emerging", "Photonic", "Integration"], "d": "hardware", "x": "Technology integrating optical components directly into silicon chips for on-chip and chip-to-chip optical...", "l": "s", "k": ["silicon", "photonics", "technology", "integrating", "optical", "components", "directly", "chips", "on-chip", "chip-to-chip", "communication", "promises", "solve", "bandwidth", "power"]}, {"id": "term-silicon-wafer", "t": "Silicon Wafer", "tg": ["Fabrication", "Manufacturing", "Material"], "d": "hardware", "x": "Thin disk of crystalline silicon used as the substrate for fabricating semiconductor chips. Wafers are typically 300mm...", "l": "s", "k": ["silicon", "wafer", "thin", "disk", "crystalline", "substrate", "fabricating", "semiconductor", "chips", "wafers", "typically", "300mm", "diameter", "undergo", "hundreds"]}, {"id": "term-silu", "t": "SiLU", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Sigmoid Linear Unit activation function defined as f(x) = x * sigmoid(x). Mathematically equivalent to Swish with beta...", "l": "s", "k": ["silu", "sigmoid", "linear", "unit", "activation", "function", "defined", "mathematically", "equivalent", "swish", "beta", "equal", "widely", "modern", "architectures"]}, {"id": "term-sim-to-real", "t": "Sim-to-Real Transfer", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "The challenge and set of techniques for deploying policies trained in simulation to physical systems. Sim-to-real...", "l": "s", "k": ["sim-to-real", "transfer", "challenge", "techniques", "deploying", "policies", "trained", "simulation", "physical", "systems", "methods", "include", "domain", "randomization", "system"]}, {"id": "term-sima", "t": "SIMA", "tg": ["Models", "Technical", "Vision", "NLP", "Robotics"], "d": "models", "x": "Scalable Instructable Multiworld Agent from DeepMind that follows natural language instructions to complete tasks...", "l": "s", "k": ["sima", "scalable", "instructable", "multiworld", "agent", "deepmind", "follows", "natural", "language", "instructions", "complete", "tasks", "across", "diverse", "virtual"]}, {"id": "term-simclr", "t": "SimCLR", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A contrastive self-supervised learning framework for visual representations that uses data augmentation to create...", "l": "s", "k": ["simclr", "contrastive", "self-supervised", "learning", "framework", "visual", "representations", "uses", "data", "augmentation", "create", "positive", "pairs", "projection", "head"]}, {"id": "term-simd", "t": "SIMD", "tg": ["Architecture", "Parallelism", "Fundamentals"], "d": "hardware", "x": "Single Instruction Multiple Data parallel computing model where one instruction operates on multiple data elements...", "l": "s", "k": ["simd", "single", "instruction", "multiple", "data", "parallel", "computing", "model", "operates", "elements", "simultaneously", "cpu", "vector", "extensions", "gpu"]}, {"id": "term-similarity-threshold", "t": "Similarity Threshold", "tg": ["Vector Database", "Search"], "d": "general", "x": "A configurable cutoff value that filters vector search results to include only matches exceeding a minimum similarity...", "l": "s", "k": ["similarity", "threshold", "configurable", "cutoff", "value", "filters", "vector", "search", "results", "include", "matches", "exceeding", "minimum", "score", "preventing"]}, {"id": "term-simmtm", "t": "SimMTM", "tg": ["Models", "Technical"], "d": "models", "x": "Simple pre-training framework for Masked Time-series Modeling that uses masked reconstruction to learn transferable...", "l": "s", "k": ["simmtm", "simple", "pre-training", "framework", "masked", "time-series", "modeling", "uses", "reconstruction", "learn", "transferable", "time", "series", "representations"]}, {"id": "term-simpleqa", "t": "SimpleQA", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark of factual questions with definitive short answers for evaluating the factual accuracy of language models....", "l": "s", "k": ["simpleqa", "benchmark", "factual", "questions", "definitive", "short", "answers", "evaluating", "accuracy", "language", "models", "tests", "provide", "correct", "well-calibrated"]}, {"id": "term-simplex-algorithm", "t": "Simplex Algorithm", "tg": ["Algorithms", "Fundamentals", "Optimization", "History"], "d": "algorithms", "x": "A widely used algorithm for solving linear programming problems that moves along the edges of the feasible polytope...", "l": "s", "k": ["simplex", "algorithm", "widely", "solving", "linear", "programming", "problems", "moves", "along", "edges", "feasible", "polytope", "vertex", "despite", "exponential"]}, {"id": "term-simpsons-paradox", "t": "Simpson's Paradox", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A statistical phenomenon where a trend that appears in several different groups of data disappears or reverses when...", "l": "s", "k": ["simpson", "paradox", "statistical", "phenomenon", "trend", "appears", "several", "different", "groups", "data", "disappears", "reverses", "combined", "highlights", "importance"]}, {"id": "term-simpsons-rule", "t": "Simpson's Rule", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "A numerical integration method that approximates the definite integral by fitting quadratic polynomials to successive...", "l": "s", "k": ["simpson", "rule", "numerical", "integration", "method", "approximates", "definite", "integral", "fitting", "quadratic", "polynomials", "successive", "groups", "points", "achieves"]}, {"id": "term-simt", "t": "SIMT", "tg": ["Architecture", "GPU", "NVIDIA"], "d": "hardware", "x": "Single Instruction Multiple Thread execution model used by NVIDIA GPUs where groups of threads execute the same...", "l": "s", "k": ["simt", "single", "instruction", "multiple", "thread", "execution", "model", "nvidia", "gpus", "groups", "threads", "execute", "different", "data", "extends"]}, {"id": "term-simtom-prompting", "t": "SimToM Prompting", "tg": ["Prompt Engineering", "Theory of Mind"], "d": "general", "x": "A two-step prompting approach for theory-of-mind tasks that first identifies what information a specific person is...", "l": "s", "k": ["simtom", "prompting", "two-step", "approach", "theory-of-mind", "tasks", "identifies", "information", "specific", "person", "aware", "answers", "question", "based", "solely"]}, {"id": "term-simulated-annealing", "t": "Simulated Annealing", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A probabilistic optimization algorithm inspired by the annealing process in metallurgy. Accepts worse solutions with...", "l": "s", "k": ["simulated", "annealing", "probabilistic", "optimization", "algorithm", "inspired", "process", "metallurgy", "accepts", "worse", "solutions", "decreasing", "probability", "time", "allowing"]}, {"id": "term-simulation-hardware", "t": "Simulation Hardware", "tg": ["Infrastructure", "Simulation", "Training"], "d": "hardware", "x": "Specialized computing hardware optimized for running physics simulations used in AI training environments. Enables...", "l": "s", "k": ["simulation", "hardware", "specialized", "computing", "optimized", "running", "physics", "simulations", "training", "environments", "enables", "synthetic", "data", "generation", "robotics"]}, {"id": "term-singular-learning-theory", "t": "Singular Learning Theory", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A mathematical framework using algebraic geometry to study the loss landscapes of neural networks. Provides tools for...", "l": "s", "k": ["singular", "learning", "theory", "mathematical", "framework", "algebraic", "geometry", "study", "loss", "landscapes", "neural", "networks", "provides", "tools", "understanding"]}, {"id": "term-singular-value-decomposition", "t": "Singular Value Decomposition", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "A matrix factorization technique that decomposes a matrix into three matrices (U, S, V^T), where S contains singular...", "l": "s", "k": ["singular", "value", "decomposition", "matrix", "factorization", "technique", "decomposes", "matrices", "contains", "values", "foundational", "pca", "latent", "semantic", "analysis"]}, {"id": "term-singularity-concept", "t": "Singularity Concept", "tg": ["History", "Milestones"], "d": "history", "x": "The hypothesized future point when AI surpasses human intelligence and triggers runaway technological growth,...", "l": "s", "k": ["singularity", "concept", "hypothesized", "future", "point", "surpasses", "human", "intelligence", "triggers", "runaway", "technological", "growth", "popularized", "vernor", "vinge"]}, {"id": "term-sinusoidal-position-encoding", "t": "Sinusoidal Position Encoding", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "The original position encoding used in the Transformer architecture. Uses sine and cosine functions of different...", "l": "s", "k": ["sinusoidal", "position", "encoding", "original", "transformer", "architecture", "uses", "sine", "cosine", "functions", "different", "frequencies", "generate", "unique", "vectors"]}, {"id": "term-sinusoidal-positional-encoding", "t": "Sinusoidal Positional Encoding", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A fixed positional encoding scheme that uses sine and cosine functions of different frequencies to encode absolute...", "l": "s", "k": ["sinusoidal", "positional", "encoding", "fixed", "scheme", "uses", "sine", "cosine", "functions", "different", "frequencies", "encode", "absolute", "token", "positions"]}, {"id": "term-siqa", "t": "SIQA", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "Social Interaction QA a benchmark for commonsense reasoning about peoples actions and their social implications....", "l": "s", "k": ["siqa", "social", "interaction", "benchmark", "commonsense", "reasoning", "peoples", "actions", "implications", "contains", "multiple-choice", "questions", "emotional", "intelligence"]}, {"id": "term-siri", "t": "Siri", "tg": ["History", "Systems"], "d": "history", "x": "A virtual assistant developed by SRI International and later acquired by Apple in 2010. Launched as an iOS feature in...", "l": "s", "k": ["siri", "virtual", "assistant", "developed", "sri", "international", "later", "acquired", "apple", "launched", "ios", "feature", "widely", "deployed", "assistants"]}, {"id": "term-siri-launch", "t": "Siri Launch", "tg": ["History", "Milestones"], "d": "history", "x": "Apple's launch of Siri in October 2011 as the first widely deployed virtual assistant on a smartphone, introducing...", "l": "s", "k": ["siri", "launch", "apple", "october", "widely", "deployed", "virtual", "assistant", "smartphone", "introducing", "millions", "consumers", "conversational", "voice-activated", "computing"]}, {"id": "term-situated-cognition", "t": "Situated Cognition", "tg": ["History", "Fundamentals"], "d": "history", "x": "A theory in cognitive science and AI that emphasizes that cognition is inseparable from the context in which it occurs....", "l": "s", "k": ["situated", "cognition", "theory", "cognitive", "science", "emphasizes", "inseparable", "context", "occurs", "developed", "researchers", "including", "lucy", "suchman", "rodney"]}, {"id": "term-situation-calculus", "t": "Situation Calculus", "tg": ["History", "Fundamentals"], "d": "history", "x": "A logical formalism for representing and reasoning about dynamically changing worlds proposed by John McCarthy in 1963....", "l": "s", "k": ["situation", "calculus", "logical", "formalism", "representing", "reasoning", "dynamically", "changing", "worlds", "proposed", "john", "mccarthy", "uses", "first-order", "logic"]}, {"id": "term-situational-awareness-in-ai", "t": "Situational Awareness in AI", "tg": ["Safety", "Technical"], "d": "safety", "x": "The ability of an AI system to understand its own context capabilities and potential impact on its environment. Debated...", "l": "s", "k": ["situational", "awareness", "ability", "system", "understand", "context", "capabilities", "potential", "impact", "environment", "debated", "safety", "desideratum", "risk", "advanced"]}, {"id": "term-skeleton-of-thought", "t": "Skeleton-of-Thought", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting strategy that first asks the model to generate a skeleton outline of the answer, then expands each point in...", "l": "s", "k": ["skeleton-of-thought", "prompting", "strategy", "asks", "model", "generate", "skeleton", "outline", "answer", "expands", "point", "parallel", "reducing", "end-to-end", "latency"]}, {"id": "term-skill-discovery", "t": "Skill Discovery", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "Unsupervised methods that learn reusable behavioral primitives or skills without task-specific rewards, typically by...", "l": "s", "k": ["skill", "discovery", "unsupervised", "methods", "learn", "reusable", "behavioral", "primitives", "skills", "without", "task-specific", "rewards", "typically", "maximizing", "mutual"]}, {"id": "term-skingpt-4", "t": "SkinGPT-4", "tg": ["Models", "Technical", "Medical", "Vision", "NLP"], "d": "models", "x": "A multimodal large language model specialized in dermatology that analyzes skin lesion images and provides diagnostic...", "l": "s", "k": ["skingpt-4", "multimodal", "large", "language", "model", "specialized", "dermatology", "analyzes", "skin", "lesion", "images", "provides", "diagnostic", "explanations"]}, {"id": "term-skip-connection", "t": "Skip Connection", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A shortcut path that bypasses one or more layers by adding or concatenating the input of a block directly to its...", "l": "s", "k": ["skip", "connection", "shortcut", "path", "bypasses", "layers", "adding", "concatenating", "input", "block", "directly", "output", "enabling", "gradient", "flow"]}, {"id": "term-skip-connections", "t": "Skip Connections", "tg": ["History", "Fundamentals"], "d": "history", "x": "Direct connections between non-adjacent layers in a neural network that allow gradients and information to bypass...", "l": "s", "k": ["skip", "connections", "direct", "non-adjacent", "layers", "neural", "network", "allow", "gradients", "information", "bypass", "intermediate", "introduced", "resnet", "enabled"]}, {"id": "term-skip-list-algorithm", "t": "Skip List Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A probabilistic data structure that uses multiple levels of linked lists to achieve O(log n) expected time for search...", "l": "s", "k": ["skip", "list", "algorithm", "probabilistic", "data", "structure", "uses", "multiple", "levels", "linked", "lists", "achieve", "log", "expected", "time"]}, {"id": "term-skip-gram", "t": "Skip-gram", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A Word2Vec training objective that predicts surrounding context words given a center word, learning word...", "l": "s", "k": ["skip-gram", "word2vec", "training", "objective", "predicts", "surrounding", "context", "words", "given", "center", "word", "learning", "representations", "capture", "semantic"]}, {"id": "term-skywork", "t": "Skywork", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A family of large language models from Kunlun Inc. trained on a curated 3.2 trillion token corpus with strong Chinese...", "l": "s", "k": ["skywork", "family", "large", "language", "models", "kunlun", "inc", "trained", "curated", "trillion", "token", "corpus", "strong", "chinese", "capabilities"]}, {"id": "term-sliding-window-attention", "t": "Sliding Window Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention pattern where each token attends only to a fixed-size local window of neighboring tokens, enabling...", "l": "s", "k": ["sliding", "window", "attention", "pattern", "token", "attends", "fixed-size", "local", "neighboring", "tokens", "enabling", "efficient", "processing", "long", "sequences"]}, {"id": "term-slimorca", "t": "SlimOrca", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A curated subset of the OpenOrca dataset with improved quality through GPT-4 filtering and decontamination. Provides...", "l": "s", "k": ["slimorca", "curated", "subset", "openorca", "dataset", "improved", "quality", "gpt-4", "filtering", "decontamination", "provides", "cleaner", "instruction-following", "data", "fine-tuning"]}, {"id": "term-slimpajama", "t": "SlimPajama", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A cleaned and deduplicated version of RedPajama containing 627 billion tokens. Created by Cerebras to improve data...", "l": "s", "k": ["slimpajama", "cleaned", "deduplicated", "version", "redpajama", "containing", "billion", "tokens", "created", "cerebras", "improve", "data", "quality", "global", "deduplication"]}, {"id": "term-slot-filling", "t": "Slot Filling", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of extracting specific pieces of information (slot values) from user utterances in a task-oriented dialogue...", "l": "s", "k": ["slot", "filling", "task", "extracting", "specific", "pieces", "information", "values", "user", "utterances", "task-oriented", "dialogue", "system", "dates", "locations"]}, {"id": "term-slow-ai-movement", "t": "Slow AI Movement", "tg": ["Safety", "Ethics"], "d": "safety", "x": "An advocacy movement promoting thoughtful deliberate approaches to AI development that prioritize safety quality and...", "l": "s", "k": ["slow", "movement", "advocacy", "promoting", "thoughtful", "deliberate", "approaches", "development", "prioritize", "safety", "quality", "social", "benefit", "speed", "competitive"]}, {"id": "term-slurm-workload-manager", "t": "Slurm Workload Manager", "tg": ["Infrastructure", "Scheduling", "Open Source"], "d": "hardware", "x": "Open-source cluster management and job scheduling system widely used in HPC and AI training clusters. Allocates GPU...", "l": "s", "k": ["slurm", "workload", "manager", "open-source", "cluster", "management", "job", "scheduling", "system", "widely", "hpc", "training", "clusters", "allocates", "gpu"]}, {"id": "term-smac", "t": "SMAC", "tg": ["Benchmark", "Reinforcement Learning"], "d": "datasets", "x": "StarCraft Multi-Agent Challenge a benchmark for cooperative multi-agent reinforcement learning based on StarCraft II...", "l": "s", "k": ["smac", "starcraft", "multi-agent", "challenge", "benchmark", "cooperative", "reinforcement", "learning", "based", "micromanagement", "scenarios", "tests", "coordination", "among", "multiple"]}, {"id": "term-smart-cooling-system", "t": "Smart Cooling System", "tg": ["Data Center", "Cooling", "AI-Optimized"], "d": "hardware", "x": "AI-controlled data center cooling system that uses machine learning to optimize cooling efficiency in real time. Google...", "l": "s", "k": ["smart", "cooling", "system", "ai-controlled", "data", "center", "uses", "machine", "learning", "optimize", "efficiency", "real", "time", "google", "deepmind"]}, {"id": "term-smart-sensor", "t": "Smart Sensor", "tg": ["Edge", "IoT", "Sensor"], "d": "hardware", "x": "Sensor device with integrated processing capability for local data analysis and AI inference. Enables intelligent edge...", "l": "s", "k": ["smart", "sensor", "device", "integrated", "processing", "capability", "local", "data", "analysis", "inference", "enables", "intelligent", "edge", "without", "transmitting"]}, {"id": "term-smartnic", "t": "SmartNIC", "tg": ["Networking", "Hardware"], "d": "hardware", "x": "Network interface card with programmable processors that offload network processing from the host CPU. Used in AI data...", "l": "s", "k": ["smartnic", "network", "interface", "card", "programmable", "processors", "offload", "processing", "host", "cpu", "data", "centers", "accelerate", "communication", "reduce"]}, {"id": "term-smollm", "t": "SmolLM", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A family of very small language models from Hugging Face available in 135M and 360M and 1.7B sizes trained on...", "l": "s", "k": ["smollm", "family", "small", "language", "models", "hugging", "face", "available", "135m", "360m", "sizes", "trained", "high-quality", "curated", "web"]}, {"id": "term-smoothquant", "t": "SmoothQuant", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A quantization technique that migrates the quantization difficulty from activations to weights by applying per-channel...", "l": "s", "k": ["smoothquant", "quantization", "technique", "migrates", "difficulty", "activations", "weights", "applying", "per-channel", "scaling", "factors", "enabling", "efficient", "int8", "llms"]}, {"id": "term-smote", "t": "SMOTE", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "Synthetic Minority Over-sampling Technique, an oversampling method that creates synthetic examples of the minority...", "l": "s", "k": ["smote", "synthetic", "minority", "over-sampling", "technique", "oversampling", "method", "creates", "examples", "class", "interpolating", "existing", "samples", "k-nearest", "neighbors"]}, {"id": "term-snapshot-ensemble", "t": "Snapshot Ensemble", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An ensemble technique that collects multiple models from a single training run by saving checkpoints at different local...", "l": "s", "k": ["snapshot", "ensemble", "technique", "collects", "multiple", "models", "single", "training", "run", "saving", "checkpoints", "different", "local", "minima", "reached"]}, {"id": "term-sneps", "t": "SNePS", "tg": ["History", "Systems"], "d": "history", "x": "The Semantic Network Processing System developed by Stuart Shapiro at the University of Buffalo beginning in 1978....", "l": "s", "k": ["sneps", "semantic", "network", "processing", "system", "developed", "stuart", "shapiro", "university", "buffalo", "beginning", "provided", "knowledge", "representation", "reasoning"]}, {"id": "term-snips", "t": "SNIPS", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A dataset for natural language understanding in voice assistants containing 14484 utterances across 7 intent categories...", "l": "s", "k": ["snips", "dataset", "natural", "language", "understanding", "voice", "assistants", "containing", "utterances", "across", "intent", "categories", "slot", "annotations", "classification"]}, {"id": "term-snli", "t": "SNLI", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "The Stanford Natural Language Inference corpus containing 570000 human-written sentence pairs labeled for entailment...", "l": "s", "k": ["snli", "stanford", "natural", "language", "inference", "corpus", "containing", "human-written", "sentence", "pairs", "labeled", "entailment", "contradiction", "neutrality", "foundational"]}, {"id": "term-snowflake-arctic", "t": "Snowflake Arctic", "tg": ["Models", "Technical", "NLP", "Products"], "d": "models", "x": "A large-scale MoE model from Snowflake with 480B total parameters designed for enterprise AI workloads with a focus on...", "l": "s", "k": ["snowflake", "arctic", "large-scale", "moe", "model", "480b", "total", "parameters", "designed", "enterprise", "workloads", "focus", "sql", "coding", "tasks"]}, {"id": "term-soar", "t": "SOAR", "tg": ["History", "Systems"], "d": "history", "x": "A cognitive architecture developed by John Laird Allen Newell and Paul Rosenbloom at Carnegie Mellon in the 1980s. SOAR...", "l": "s", "k": ["soar", "cognitive", "architecture", "developed", "john", "laird", "allen", "newell", "paul", "rosenbloom", "carnegie", "mellon", "1980s", "models", "general"]}, {"id": "term-sobel-operator", "t": "Sobel Operator", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "An edge detection operator that computes the gradient of image intensity at each pixel using two 3x3 convolution...", "l": "s", "k": ["sobel", "operator", "edge", "detection", "computes", "gradient", "image", "intensity", "pixel", "3x3", "convolution", "kernels", "approximates", "horizontal", "vertical"]}, {"id": "term-social-chemistry-101", "t": "Social Chemistry 101", "tg": ["Training Corpus", "NLP", "Safety"], "d": "datasets", "x": "A dataset of 292000 rules-of-thumb about social and moral norms covering 104000 everyday situations. Provides...", "l": "s", "k": ["social", "chemistry", "dataset", "rules-of-thumb", "moral", "norms", "covering", "everyday", "situations", "provides", "fine-grained", "annotations", "expectations", "judgments"]}, {"id": "term-social-impact-assessment-for-ai", "t": "Social Impact Assessment for AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "A systematic evaluation of how an AI system may affect communities social structures and social wellbeing. Broader than...", "l": "s", "k": ["social", "impact", "assessment", "systematic", "evaluation", "system", "affect", "communities", "structures", "wellbeing", "broader", "individual-focused", "assessments", "considers", "collective"]}, {"id": "term-social-license-for-ai", "t": "Social License for AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The ongoing acceptance and approval granted by society to AI developers and deployers. Depends on demonstrated safety...", "l": "s", "k": ["social", "license", "ongoing", "acceptance", "approval", "granted", "society", "developers", "deployers", "depends", "demonstrated", "safety", "benefit", "trustworthiness", "withdrawn"]}, {"id": "term-social-scoring-systems", "t": "Social Scoring Systems", "tg": ["AI Ethics", "Regulation"], "d": "safety", "x": "AI-driven systems that assign scores to individuals based on their behavior, social connections, or characteristics,...", "l": "s", "k": ["social", "scoring", "systems", "ai-driven", "assign", "scores", "individuals", "based", "behavior", "connections", "characteristics", "determine", "access", "services", "banned"]}, {"id": "term-society-of-mind", "t": "Society of Mind", "tg": ["History", "Fundamentals"], "d": "history", "x": "A theory of natural and artificial intelligence proposed by Marvin Minsky in his 1986 book of the same name. The theory...", "l": "s", "k": ["society", "mind", "theory", "natural", "artificial", "intelligence", "proposed", "marvin", "minsky", "book", "name", "proposes", "single", "entity", "tiny"]}, {"id": "term-society-of-mind-book", "t": "Society of Mind Book", "tg": ["History", "Milestones"], "d": "history", "x": "The 1986 book by Marvin Minsky proposing that intelligence emerges from the interaction of many simple agents or...", "l": "s", "k": ["society", "mind", "book", "marvin", "minsky", "proposing", "intelligence", "emerges", "interaction", "simple", "agents", "processes", "theory", "influenced", "thinking"]}, {"id": "term-sociotechnical-safety", "t": "Sociotechnical Safety", "tg": ["Safety", "Fundamentals"], "d": "safety", "x": "An approach to AI safety that considers the entire system of people processes and technology rather than just the...", "l": "s", "k": ["sociotechnical", "safety", "approach", "considers", "entire", "system", "people", "processes", "technology", "rather", "technical", "components", "recognizes", "emerges", "interaction"]}, {"id": "term-sociotechnical-systems-approach", "t": "Sociotechnical Systems Approach", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "An analytical framework that views AI systems as inseparable from their social context, recognizing that technical and...", "l": "s", "k": ["sociotechnical", "systems", "approach", "analytical", "framework", "views", "inseparable", "social", "context", "recognizing", "technical", "factors", "jointly", "determine", "system"]}, {"id": "term-socratic-prompting", "t": "Socratic Prompting", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting technique that guides the model through a series of probing questions rather than direct instructions,...", "l": "s", "k": ["socratic", "prompting", "technique", "guides", "model", "series", "probing", "questions", "rather", "direct", "instructions", "encouraging", "step-by-step", "reasoning", "self-discovery"]}, {"id": "term-soda", "t": "SODA", "tg": ["Training Corpus", "NLP", "Dialogue"], "d": "datasets", "x": "Social Dialogue Dataset a large-scale dialogue dataset with social commonsense context. Contains 1.5 million...", "l": "s", "k": ["soda", "social", "dialogue", "dataset", "large-scale", "commonsense", "context", "contains", "million", "conversations", "grounded", "scenarios", "training", "socially", "aware"]}, {"id": "term-sac", "t": "Soft Actor-Critic (SAC)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An off-policy actor-critic algorithm that maximizes both expected return and policy entropy, encouraging exploration...", "l": "s", "k": ["soft", "actor-critic", "sac", "off-policy", "algorithm", "maximizes", "expected", "return", "policy", "entropy", "encouraging", "exploration", "maintaining", "stable", "learning"]}, {"id": "term-soft-attention", "t": "Soft Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that computes a weighted average over all positions using continuous attention weights, making...", "l": "s", "k": ["soft", "attention", "mechanism", "computes", "weighted", "average", "positions", "continuous", "weights", "making", "fully", "differentiable", "trainable", "standard", "backpropagation"]}, {"id": "term-soft-q-learning", "t": "Soft Q-Learning", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A maximum entropy reinforcement learning algorithm that augments the standard Q-learning objective with an entropy...", "l": "s", "k": ["soft", "q-learning", "maximum", "entropy", "reinforcement", "learning", "algorithm", "augments", "standard", "objective", "bonus", "encourages", "exploration", "produces", "robust"]}, {"id": "term-soft-update", "t": "Soft Update (Polyak Averaging)", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A method for updating target network parameters as an exponential moving average of the online network parameters,...", "l": "s", "k": ["soft", "update", "polyak", "averaging", "method", "updating", "target", "network", "parameters", "exponential", "moving", "average", "online", "controlled", "smoothing"]}, {"id": "term-softmax", "t": "Softmax", "tg": ["Math", "Function"], "d": "general", "x": "A function that converts raw scores into probabilities that sum to 1. Used in attention mechanisms and classification...", "l": "s", "k": ["softmax", "function", "converts", "raw", "scores", "probabilities", "sum", "attention", "mechanisms", "classification", "outputs", "create", "interpretable", "probability", "distributions"]}, {"id": "term-softmax-function", "t": "Softmax Function", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A function that normalizes a vector of real numbers into a probability distribution by exponentiating each element and...", "l": "s", "k": ["softmax", "function", "normalizes", "vector", "real", "numbers", "probability", "distribution", "exponentiating", "element", "dividing", "sum", "attention", "mechanisms", "classification"]}, {"id": "term-softmax-temperature", "t": "Softmax Temperature", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A parameter that controls the sharpness of the softmax probability distribution. Temperature values below 1 sharpen the...", "l": "s", "k": ["softmax", "temperature", "parameter", "controls", "sharpness", "probability", "distribution", "values", "sharpen", "making", "peaked", "flatten", "uniform", "knowledge", "distillation"]}, {"id": "term-softplus", "t": "Softplus", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A smooth approximation of ReLU defined as f(x) = log(1 + exp(x)). Always positive and differentiable everywhere. Used...", "l": "s", "k": ["softplus", "smooth", "approximation", "relu", "defined", "log", "exp", "always", "positive", "differentiable", "everywhere", "various", "contexts", "including", "variational"]}, {"id": "term-softsign", "t": "Softsign", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An activation function defined as f(x) = x / (1 + |x|) that provides similar output range to tanh but with polynomial...", "l": "s", "k": ["softsign", "activation", "function", "defined", "provides", "similar", "output", "range", "tanh", "polynomial", "decay", "instead", "exponential", "converges", "slowly"]}, {"id": "term-solar", "t": "SOLAR", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A 10.7B parameter language model from Upstage that uses Depth Up-Scaling (DUS) to merge smaller pre-trained models into...", "l": "s", "k": ["solar", "parameter", "language", "model", "upstage", "uses", "depth", "up-scaling", "dus", "merge", "smaller", "pre-trained", "models", "larger", "architecture"]}, {"id": "term-solar-107b-instruct", "t": "SOLAR 10.7B Instruct", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An instruction-tuned version of the SOLAR model optimized for helpfulness and safety in conversational and...", "l": "s", "k": ["solar", "instruct", "instruction-tuned", "version", "model", "optimized", "helpfulness", "safety", "conversational", "task-completion", "scenarios"]}, {"id": "term-solomonoff-induction", "t": "Solomonoff Induction", "tg": ["History", "Fundamentals"], "d": "history", "x": "A mathematical theory of universal prediction developed by Ray Solomonoff in 1964. Based on algorithmic probability it...", "l": "s", "k": ["solomonoff", "induction", "mathematical", "theory", "universal", "prediction", "developed", "ray", "based", "algorithmic", "probability", "provides", "idealized", "framework", "inductive"]}, {"id": "term-something-something-v2", "t": "Something-Something V2", "tg": ["Benchmark", "Video"], "d": "datasets", "x": "A video dataset of 220000 clips showing humans performing 174 predefined actions with everyday objects. Focuses on...", "l": "s", "k": ["something-something", "video", "dataset", "clips", "showing", "humans", "performing", "predefined", "actions", "everyday", "objects", "focuses", "temporal", "reasoning", "object"]}, {"id": "term-sora", "t": "Sora", "tg": ["Models", "Technical"], "d": "models", "x": "A text-to-video generation model by OpenAI that produces realistic videos up to a minute long from text prompts. Uses a...", "l": "s", "k": ["sora", "text-to-video", "generation", "model", "openai", "produces", "realistic", "videos", "minute", "long", "text", "prompts", "uses", "diffusion", "transformer"]}, {"id": "term-soundstream", "t": "SoundStream", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A neural audio codec from Google that uses residual vector quantization to compress speech and music and environmental...", "l": "s", "k": ["soundstream", "neural", "audio", "codec", "google", "uses", "residual", "vector", "quantization", "compress", "speech", "music", "environmental", "sounds", "compact"]}, {"id": "term-space-filling-curve-algorithm", "t": "Space-Filling Curve Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A continuous curve that passes through every point in a multi-dimensional space reducing multi-dimensional data to a...", "l": "s", "k": ["space-filling", "curve", "algorithm", "continuous", "passes", "point", "multi-dimensional", "space", "reducing", "data", "one-dimensional", "ordering", "hilbert", "z-order", "curves"]}, {"id": "term-sparc", "t": "SParC", "tg": ["Benchmark", "NLP", "Code"], "d": "datasets", "x": "Cross-Domain Semantic Parsing in Context a context-dependent text-to-SQL benchmark with multi-turn interactions. Tests...", "l": "s", "k": ["sparc", "cross-domain", "semantic", "parsing", "context", "context-dependent", "text-to-sql", "benchmark", "multi-turn", "interactions", "tests", "ability", "handle", "sql", "generation"]}, {"id": "term-sparc-architecture", "t": "SPARC Architecture", "tg": ["Historical", "Architecture", "RISC"], "d": "hardware", "x": "Scalable Processor Architecture RISC design by Sun Microsystems used in Unix workstations and servers. Powered many...", "l": "s", "k": ["sparc", "architecture", "scalable", "processor", "risc", "design", "sun", "microsystems", "unix", "workstations", "servers", "powered", "early", "computational", "science"]}, {"id": "term-sparse-approximate-inverse", "t": "Sparse Approximate Inverse", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A preconditioning technique that directly computes an approximate inverse of a sparse matrix. Maintains sparsity by...", "l": "s", "k": ["sparse", "approximate", "inverse", "preconditioning", "technique", "directly", "computes", "matrix", "maintains", "sparsity", "constraining", "nonzero", "pattern", "inherently", "parallelizable"]}, {"id": "term-sparse-attention", "t": "Sparse Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that restricts each token to attend to only a subset of other tokens using predefined or learned...", "l": "s", "k": ["sparse", "attention", "mechanism", "restricts", "token", "attend", "subset", "tokens", "predefined", "learned", "sparsity", "patterns", "reducing", "quadratic", "computational"]}, {"id": "term-sparse-autoencoder", "t": "Sparse Autoencoder", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An autoencoder that enforces sparsity constraints on the hidden layer activations, encouraging the network to learn a...", "l": "s", "k": ["sparse", "autoencoder", "enforces", "sparsity", "constraints", "hidden", "layer", "activations", "encouraging", "network", "learn", "compact", "distributed", "representation", "neurons"]}, {"id": "term-sparse-autoencoder-model", "t": "Sparse Autoencoder Model", "tg": ["Models", "Technical"], "d": "models", "x": "An autoencoder with a sparsity constraint encouraging most hidden units to be inactive for any given input. Learns...", "l": "s", "k": ["sparse", "autoencoder", "model", "sparsity", "constraint", "encouraging", "hidden", "units", "inactive", "given", "input", "learns", "overcomplete", "representations", "feature"]}, {"id": "term-sparse-direct-solver", "t": "Sparse Direct Solver", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A class of algorithms that solve sparse linear systems by computing exact factorizations while exploiting the sparsity...", "l": "s", "k": ["sparse", "direct", "solver", "class", "algorithms", "solve", "linear", "systems", "computing", "exact", "factorizations", "exploiting", "sparsity", "pattern", "includes"]}, {"id": "term-sparse-matrix-algorithm", "t": "Sparse Matrix Algorithm", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A collection of techniques for efficiently storing and manipulating matrices with mostly zero entries. Formats include...", "l": "s", "k": ["sparse", "matrix", "algorithm", "collection", "techniques", "efficiently", "storing", "manipulating", "matrices", "mostly", "zero", "entries", "formats", "include", "compressed"]}, {"id": "term-sparse-models", "t": "Sparse Models", "tg": ["History", "Fundamentals"], "d": "history", "x": "Neural network models where only a fraction of parameters are activated for any given input. Sparse architectures like...", "l": "s", "k": ["sparse", "models", "neural", "network", "fraction", "parameters", "activated", "given", "input", "architectures", "mixture", "experts", "enable", "larger", "manageable"]}, {"id": "term-sparse-pca-algorithm", "t": "Sparse PCA Algorithm", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A variant of PCA that constrains the principal components to have few non-zero loadings. Produces more interpretable...", "l": "s", "k": ["sparse", "pca", "algorithm", "variant", "constrains", "principal", "components", "non-zero", "loadings", "produces", "interpretable", "cost", "explaining", "slightly", "less"]}, {"id": "term-sparse-retrieval", "t": "Sparse Retrieval", "tg": ["Retrieval", "Search"], "d": "general", "x": "An information retrieval paradigm that represents queries and documents as high-dimensional sparse vectors where most...", "l": "s", "k": ["sparse", "retrieval", "information", "paradigm", "represents", "queries", "documents", "high-dimensional", "vectors", "values", "zero", "non-zero", "entries", "corresponding", "term"]}, {"id": "term-sparse-reward", "t": "Sparse Reward Problem", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "An RL setting where the agent receives non-zero reward signals only rarely, making credit assignment extremely...", "l": "s", "k": ["sparse", "reward", "problem", "setting", "agent", "receives", "non-zero", "signals", "rarely", "making", "credit", "assignment", "extremely", "difficult", "rewards"]}, {"id": "term-sparse-table-algorithm", "t": "Sparse Table Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A data structure that answers static range minimum (or maximum) queries in O(1) time after O(n log n) preprocessing....", "l": "s", "k": ["sparse", "table", "algorithm", "data", "structure", "answers", "static", "range", "minimum", "maximum", "queries", "time", "log", "preprocessing", "stores"]}, {"id": "term-sparse-transformer", "t": "Sparse Transformer", "tg": ["Models", "Technical"], "d": "models", "x": "A transformer variant that uses sparse factorizations of the attention matrix to reduce computational complexity....", "l": "s", "k": ["sparse", "transformer", "variant", "uses", "factorizations", "attention", "matrix", "reduce", "computational", "complexity", "demonstrates", "carefully", "designed", "patterns", "match"]}, {"id": "term-sparse-vector-technique", "t": "Sparse Vector Technique", "tg": ["Algorithms", "Technical", "Privacy"], "d": "algorithms", "x": "A differential privacy mechanism that answers a sequence of numerical queries while only paying a privacy cost for...", "l": "s", "k": ["sparse", "vector", "technique", "differential", "privacy", "mechanism", "answers", "sequence", "numerical", "queries", "paying", "cost", "whose", "exceed", "threshold"]}, {"id": "term-sparseformer", "t": "SparseFormer", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A vision Transformer that uses sparse attention patterns to reduce computational cost while maintaining strong accuracy...", "l": "s", "k": ["sparseformer", "vision", "transformer", "uses", "sparse", "attention", "patterns", "reduce", "computational", "cost", "maintaining", "strong", "accuracy", "image", "classification"]}, {"id": "term-sparsetsf", "t": "SparseTSF", "tg": ["Models", "Technical"], "d": "models", "x": "A sparse time series forecasting method that uses cross-period sparse forecasting to reduce computational cost while...", "l": "s", "k": ["sparsetsf", "sparse", "time", "series", "forecasting", "method", "uses", "cross-period", "reduce", "computational", "cost", "maintaining", "prediction", "accuracy"]}, {"id": "term-spatial-attention", "t": "Spatial Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that learns to weight different spatial locations in feature maps, focusing computational...", "l": "s", "k": ["spatial", "attention", "mechanism", "learns", "weight", "different", "locations", "feature", "maps", "focusing", "computational", "resources", "informative", "regions", "input"]}, {"id": "term-spatial-computing-architecture", "t": "Spatial Computing Architecture", "tg": ["Architecture", "Emerging", "Design"], "d": "hardware", "x": "Hardware architecture that maps computations spatially across an array of processing elements. Each element performs a...", "l": "s", "k": ["spatial", "computing", "architecture", "hardware", "maps", "computations", "spatially", "across", "array", "processing", "elements", "element", "performs", "fixed", "operation"]}, {"id": "term-spatial-reasoning-in-ai", "t": "Spatial Reasoning in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "The area of AI concerned with representing and reasoning about spatial relationships between objects and regions....", "l": "s", "k": ["spatial", "reasoning", "area", "concerned", "representing", "relationships", "objects", "regions", "applications", "include", "robot", "navigation", "geographic", "information", "systems"]}, {"id": "term-spatialtracker", "t": "SpatialTracker", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A 3D point tracking model that tracks points in 3D space through video by leveraging monocular depth estimation and...", "l": "s", "k": ["spatialtracker", "point", "tracking", "model", "tracks", "points", "space", "video", "leveraging", "monocular", "depth", "estimation", "camera", "motion"]}, {"id": "term-spearman-rank-correlation", "t": "Spearman Rank Correlation", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A non-parametric measure of the monotonic relationship between two variables, computed as the Pearson correlation of...", "l": "s", "k": ["spearman", "rank", "correlation", "non-parametric", "measure", "monotonic", "relationship", "variables", "computed", "pearson", "ranked", "values", "assume", "linearity", "normal"]}, {"id": "term-special-tokens", "t": "Special Tokens", "tg": ["NLP", "Tokenization"], "d": "general", "x": "Reserved tokens in a vocabulary that serve structural purposes such as marking sequence boundaries, separating...", "l": "s", "k": ["special", "tokens", "reserved", "vocabulary", "serve", "structural", "purposes", "marking", "sequence", "boundaries", "separating", "segments", "padding", "indicating", "masked"]}, {"id": "term-specification-gaming", "t": "Specification Gaming", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The behavior of an AI system that satisfies the literal specification of an objective without achieving the intended...", "l": "s", "k": ["specification", "gaming", "behavior", "system", "satisfies", "literal", "objective", "without", "achieving", "intended", "outcome", "closely", "related", "reward", "hacking"]}, {"id": "term-specification-problem", "t": "Specification Problem", "tg": ["Safety", "Technical"], "d": "safety", "x": "The challenge of precisely defining what we want an AI system to do in a way that captures our true intentions....", "l": "s", "k": ["specification", "problem", "challenge", "precisely", "defining", "want", "system", "captures", "true", "intentions", "incomplete", "incorrect", "specifications", "lead", "systems"]}, {"id": "term-specificity", "t": "Specificity", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "The proportion of actual negative cases that are correctly identified as negative by a classifier, also known as the...", "l": "s", "k": ["specificity", "proportion", "actual", "negative", "cases", "correctly", "identified", "classifier", "known", "true", "rate", "complements", "sensitivity", "recall", "binary"]}, {"id": "term-spectral-clustering", "t": "Spectral Clustering", "tg": ["Machine Learning", "Clustering"], "d": "general", "x": "A clustering technique that uses the eigenvalues and eigenvectors of a similarity matrix derived from the data to...", "l": "s", "k": ["spectral", "clustering", "technique", "uses", "eigenvalues", "eigenvectors", "similarity", "matrix", "derived", "data", "perform", "dimensionality", "reduction", "reduced", "space"]}, {"id": "term-spectral-clustering-algorithm", "t": "Spectral Clustering Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A clustering method that uses eigenvalues of the graph Laplacian to reduce dimensionality before applying k-means....", "l": "s", "k": ["spectral", "clustering", "algorithm", "method", "uses", "eigenvalues", "graph", "laplacian", "reduce", "dimensionality", "applying", "k-means", "effective", "non-convex", "cluster"]}, {"id": "term-spectral-embedding-algorithm", "t": "Spectral Embedding Algorithm", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A dimensionality reduction technique that embeds data points using the leading eigenvectors of the normalized graph...", "l": "s", "k": ["spectral", "embedding", "algorithm", "dimensionality", "reduction", "technique", "embeds", "data", "points", "leading", "eigenvectors", "normalized", "graph", "laplacian", "produces"]}, {"id": "term-spectral-method", "t": "Spectral Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A numerical method for solving differential equations that represents the solution as a sum of basis functions such as...", "l": "s", "k": ["spectral", "method", "numerical", "solving", "differential", "equations", "represents", "solution", "sum", "basis", "functions", "fourier", "modes", "chebyshev", "polynomials"]}, {"id": "term-spectral-normalization", "t": "Spectral Normalization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A weight normalization technique that constrains the spectral norm of weight matrices to stabilize training. Widely...", "l": "s", "k": ["spectral", "normalization", "weight", "technique", "constrains", "norm", "matrices", "stabilize", "training", "widely", "gans", "enforce", "lipschitz", "continuity", "discriminator"]}, {"id": "term-spectrogram", "t": "Spectrogram", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A visual representation of the spectrum of frequencies in a signal as they vary over time. Commonly displayed as a...", "l": "s", "k": ["spectrogram", "visual", "representation", "spectrum", "frequencies", "signal", "vary", "time", "commonly", "displayed", "heatmap", "x-axis", "frequency", "y-axis", "intensity"]}, {"id": "term-speculative-decoding", "t": "Speculative Decoding", "tg": ["Optimization", "Inference"], "d": "algorithms", "x": "A technique to speed up LLM inference by using a smaller model to draft tokens that the larger model verifies in...", "l": "s", "k": ["speculative", "decoding", "technique", "speed", "llm", "inference", "smaller", "model", "draft", "tokens", "larger", "verifies", "parallel", "maintains", "output"]}, {"id": "term-speculative-execution-llm", "t": "Speculative Execution", "tg": ["LLM", "Inference"], "d": "models", "x": "A broader inference optimization paradigm where cheaper computations predict likely outcomes that are verified by...", "l": "s", "k": ["speculative", "execution", "broader", "inference", "optimization", "paradigm", "cheaper", "computations", "predict", "likely", "outcomes", "verified", "full-cost", "operations", "encompassing"]}, {"id": "term-speculative-sampling", "t": "Speculative Sampling", "tg": ["LLM", "Inference"], "d": "models", "x": "An inference acceleration technique where a fast draft model proposes multiple tokens that are then verified in...", "l": "s", "k": ["speculative", "sampling", "inference", "acceleration", "technique", "fast", "draft", "model", "proposes", "multiple", "tokens", "verified", "parallel", "target", "maintaining"]}, {"id": "term-speech-recognition-history", "t": "Speech Recognition History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of automatic speech recognition from Bell Labs' Audrey system in 1952 through Hidden Markov Models in...", "l": "s", "k": ["speech", "recognition", "history", "development", "automatic", "bell", "labs", "audrey", "system", "hidden", "markov", "models", "1980s", "deep", "learning"]}, {"id": "term-speech-synthesis", "t": "Speech Synthesis", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The artificial production of human-like speech from text or other input, using techniques ranging from concatenative...", "l": "s", "k": ["speech", "synthesis", "artificial", "production", "human-like", "text", "input", "techniques", "ranging", "concatenative", "neural", "models", "tacotron", "vits"]}, {"id": "term-speechgpt", "t": "SpeechGPT", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "A large language model with intrinsic cross-modal conversational abilities that can perceive and generate both speech...", "l": "s", "k": ["speechgpt", "large", "language", "model", "intrinsic", "cross-modal", "conversational", "abilities", "perceive", "generate", "speech", "text", "multi-turn", "dialogues"]}, {"id": "term-speecht5", "t": "SpeechT5", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "A unified encoder-decoder framework from Microsoft that handles text-to-speech and speech-to-text and speech...", "l": "s", "k": ["speecht5", "unified", "encoder-decoder", "framework", "microsoft", "handles", "text-to-speech", "speech-to-text", "speech", "enhancement", "shared", "representations"]}, {"id": "term-spell-correction", "t": "Spell Correction", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The automated detection and correction of misspelled words in text using techniques such as edit distance, language...", "l": "s", "k": ["spell", "correction", "automated", "detection", "misspelled", "words", "text", "techniques", "edit", "distance", "language", "models", "context-aware", "suggest", "corrections"]}, {"id": "term-spgispeech", "t": "SPGISpeech", "tg": ["Training Corpus", "Speech"], "d": "datasets", "x": "A 5000-hour English speech corpus of earnings calls and financial presentations with professional transcriptions....", "l": "s", "k": ["spgispeech", "5000-hour", "english", "speech", "corpus", "earnings", "calls", "financial", "presentations", "professional", "transcriptions", "provides", "domain-specific", "data", "asr"]}, {"id": "term-sphinx", "t": "SPHINX", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A versatile multimodal large language model that combines multiple visual encoders and language models for robust image...", "l": "s", "k": ["sphinx", "versatile", "multimodal", "large", "language", "model", "combines", "multiple", "visual", "encoders", "models", "robust", "image", "video", "understanding"]}, {"id": "term-spider", "t": "Spider", "tg": ["Benchmark", "NLP", "Code"], "d": "datasets", "x": "A complex text-to-SQL benchmark covering 200 databases with 10181 questions and SQL queries. Tests cross-database...", "l": "s", "k": ["spider", "complex", "text-to-sql", "benchmark", "covering", "databases", "questions", "sql", "queries", "tests", "cross-database", "generalization", "natural", "language", "translation"]}, {"id": "term-spiking-neural-network-hardware", "t": "Spiking Neural Network Hardware", "tg": ["Emerging", "Neuromorphic", "Architecture"], "d": "hardware", "x": "Hardware designed to implement spiking neural networks that communicate through discrete pulses rather than continuous...", "l": "s", "k": ["spiking", "neural", "network", "hardware", "designed", "implement", "networks", "communicate", "discrete", "pulses", "rather", "continuous", "values", "offers", "potential"]}, {"id": "term-spillover-effects-of-ai", "t": "Spillover Effects of AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Unintended consequences of AI deployment that affect parties or domains beyond the intended scope of the system. Can be...", "l": "s", "k": ["spillover", "effects", "unintended", "consequences", "deployment", "affect", "parties", "domains", "beyond", "intended", "scope", "system", "positive", "negative", "difficult"]}, {"id": "term-spin", "t": "SPIN", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Self-Play Fine-Tuning is an alignment method where the language model generates training data by distinguishing its own...", "l": "s", "k": ["spin", "self-play", "fine-tuning", "alignment", "method", "language", "model", "generates", "training", "data", "distinguishing", "outputs", "human-written", "text", "framework"]}, {"id": "term-spine-leaf-architecture", "t": "Spine-Leaf Architecture", "tg": ["Networking", "Topology", "Data Center"], "d": "hardware", "x": "Two-tier data center network topology where every leaf switch connects to every spine switch providing predictable...", "l": "s", "k": ["spine-leaf", "architecture", "two-tier", "data", "center", "network", "topology", "leaf", "switch", "connects", "spine", "providing", "predictable", "low-latency", "paths"]}, {"id": "term-spinnaker", "t": "SpiNNaker", "tg": ["Neuromorphic", "Research", "ARM"], "d": "hardware", "x": "Spiking Neural Network Architecture computing platform developed at the University of Manchester. Uses a million ARM...", "l": "s", "k": ["spinnaker", "spiking", "neural", "network", "architecture", "computing", "platform", "developed", "university", "manchester", "uses", "million", "arm", "cores", "simulate"]}, {"id": "term-spirit-lm", "t": "Spirit-LM", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "A multimodal language model from Meta AI that interleaves speech and text tokens for expressive speech-text generation...", "l": "s", "k": ["spirit-lm", "multimodal", "language", "model", "meta", "interleaves", "speech", "text", "tokens", "expressive", "speech-text", "generation", "emotional", "prosodic", "control"]}, {"id": "term-splade", "t": "SPLADE", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "SParse Lexical AnD Expansion model, a learned sparse retrieval method that predicts importance weights for vocabulary...", "l": "s", "k": ["splade", "sparse", "lexical", "expansion", "model", "learned", "retrieval", "method", "predicts", "importance", "weights", "vocabulary", "terms", "including", "present"]}, {"id": "term-splay-tree-algorithm", "t": "Splay Tree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A self-adjusting binary search tree that moves recently accessed elements to the root through rotations called...", "l": "s", "k": ["splay", "tree", "algorithm", "self-adjusting", "binary", "search", "moves", "recently", "accessed", "elements", "root", "rotations", "called", "splaying", "achieves"]}, {"id": "term-spline-regression", "t": "Spline Regression", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A regression technique using piecewise polynomial functions (splines) joined at knot points to fit flexible, smooth...", "l": "s", "k": ["spline", "regression", "technique", "piecewise", "polynomial", "functions", "splines", "joined", "knot", "points", "fit", "flexible", "smooth", "curves", "natural"]}, {"id": "term-spot-instances", "t": "Spot Instances for AI", "tg": ["Distributed Computing", "Inference Infrastructure"], "d": "hardware", "x": "Discounted cloud computing instances that use spare capacity at 60-90% less than on-demand pricing but can be...", "l": "s", "k": ["spot", "instances", "discounted", "cloud", "computing", "spare", "capacity", "60-90", "less", "on-demand", "pricing", "interrupted", "short", "notice", "widely"]}, {"id": "term-sqlcoder", "t": "SQLCoder", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A language model fine-tuned specifically for text-to-SQL generation that converts natural language questions into SQL...", "l": "s", "k": ["sqlcoder", "language", "model", "fine-tuned", "specifically", "text-to-sql", "generation", "converts", "natural", "questions", "sql", "queries", "database", "interaction"]}, {"id": "term-squad", "t": "SQuAD", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Stanford Question Answering Dataset, a reading comprehension benchmark where models extract answer spans from Wikipedia...", "l": "s", "k": ["squad", "stanford", "question", "answering", "dataset", "reading", "comprehension", "benchmark", "models", "extract", "answer", "spans", "wikipedia", "passages", "additionally"]}, {"id": "term-squad-20", "t": "SQuAD 2.0", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "An extension of SQuAD that includes 50000 unanswerable questions combined with the original answerable ones. Tests...", "l": "s", "k": ["squad", "extension", "includes", "unanswerable", "questions", "combined", "original", "answerable", "ones", "tests", "models", "determine", "passage", "contain", "answer"]}, {"id": "term-squad-dataset", "t": "SQuAD Dataset", "tg": ["History", "Milestones"], "d": "history", "x": "The Stanford Question Answering Dataset created in 2016 consisting of questions posed about Wikipedia articles where...", "l": "s", "k": ["squad", "dataset", "stanford", "question", "answering", "created", "consisting", "questions", "posed", "wikipedia", "articles", "answer", "segment", "text", "article"]}, {"id": "term-squality", "t": "SQuALITY", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A long-document summarization dataset with question-focused summaries written by expert annotators. Tests the ability...", "l": "s", "k": ["squality", "long-document", "summarization", "dataset", "question-focused", "summaries", "written", "expert", "annotators", "tests", "ability", "summarize", "long", "texts", "response"]}, {"id": "term-squeeze-and-excitation", "t": "Squeeze-and-Excitation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A channel attention mechanism that adaptively recalibrates channel-wise feature responses. Uses global average pooling...", "l": "s", "k": ["squeeze-and-excitation", "channel", "attention", "mechanism", "adaptively", "recalibrates", "channel-wise", "feature", "responses", "uses", "global", "average", "pooling", "followed", "fully"]}, {"id": "term-squeeze-and-excitation-network", "t": "Squeeze-and-Excitation Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A CNN enhancement that adaptively recalibrates channel-wise feature responses by using global average pooling followed...", "l": "s", "k": ["squeeze-and-excitation", "network", "cnn", "enhancement", "adaptively", "recalibrates", "channel-wise", "feature", "responses", "global", "average", "pooling", "followed", "small", "learn"]}, {"id": "term-squeeze-excitation-block", "t": "Squeeze-Excitation Block", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A channel attention module that squeezes spatial information via global pooling and excites channel-wise features...", "l": "s", "k": ["squeeze-excitation", "block", "channel", "attention", "module", "squeezes", "spatial", "information", "via", "global", "pooling", "excites", "channel-wise", "features", "learned"]}, {"id": "term-squeezenet", "t": "SqueezeNet", "tg": ["Models", "Technical"], "d": "models", "x": "A compact CNN architecture that achieves AlexNet-level accuracy with 50x fewer parameters using fire modules consisting...", "l": "s", "k": ["squeezenet", "compact", "cnn", "architecture", "achieves", "alexnet-level", "accuracy", "50x", "fewer", "parameters", "fire", "modules", "consisting", "squeeze", "expand"]}, {"id": "term-sram", "t": "SRAM", "tg": ["Memory", "Fundamentals"], "d": "hardware", "x": "Static Random Access Memory that retains data without refresh cycles providing extremely fast access times. Used in CPU...", "l": "s", "k": ["sram", "static", "random", "access", "memory", "retains", "data", "without", "refresh", "cycles", "providing", "extremely", "fast", "times", "cpu"]}, {"id": "term-sri-international", "t": "SRI International", "tg": ["History", "Organizations"], "d": "history", "x": "An American nonprofit research institute originally Stanford Research Institute founded in 1946. Developed Shakey the...", "l": "s", "k": ["sri", "international", "american", "nonprofit", "research", "institute", "originally", "stanford", "founded", "developed", "shakey", "robot", "late", "1960s", "created"]}, {"id": "term-ssd", "t": "SSD", "tg": ["Models", "Technical"], "d": "models", "x": "Single Shot MultiBox Detector is a one-stage object detection model that predicts bounding boxes and class scores at...", "l": "s", "k": ["ssd", "single", "shot", "multibox", "detector", "one-stage", "object", "detection", "model", "predicts", "bounding", "boxes", "class", "scores", "multiple"]}, {"id": "term-ssd-for-ai-workloads", "t": "SSD for AI Workloads", "tg": ["Storage", "Hardware"], "d": "hardware", "x": "Solid-state drives optimized for the high-throughput random read patterns of AI data loading. NVMe SSDs significantly...", "l": "s", "k": ["ssd", "workloads", "solid-state", "drives", "optimized", "high-throughput", "random", "read", "patterns", "data", "loading", "nvme", "ssds", "significantly", "reduce"]}, {"id": "term-ssd-object-detection", "t": "SSD Object Detection", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Single Shot MultiBox Detector, a real-time object detection architecture that predicts bounding boxes and class...", "l": "s", "k": ["ssd", "object", "detection", "single", "shot", "multibox", "detector", "real-time", "architecture", "predicts", "bounding", "boxes", "class", "probabilities", "multiple"]}, {"id": "term-ssim", "t": "SSIM", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Structural Similarity Index Measure compares images based on luminance contrast and structure. Designed to be more...", "l": "s", "k": ["ssim", "structural", "similarity", "index", "measure", "compares", "images", "based", "luminance", "contrast", "structure", "designed", "perceptually", "relevant", "pixel-wise"]}, {"id": "term-sst-2", "t": "SST-2", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "The Stanford Sentiment Treebank binary classification task containing movie review sentences labeled as positive or...", "l": "s", "k": ["sst-2", "stanford", "sentiment", "treebank", "binary", "classification", "task", "containing", "movie", "review", "sentences", "labeled", "positive", "negative", "part"]}, {"id": "term-sst-5", "t": "SST-5", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "The Stanford Sentiment Treebank with five-class sentiment labels ranging from very negative to very positive. Provides...", "l": "s", "k": ["sst-5", "stanford", "sentiment", "treebank", "five-class", "labels", "ranging", "negative", "positive", "provides", "fine-grained", "annotations", "movie", "review", "sentences"]}, {"id": "term-stable-audio", "t": "Stable Audio", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A latent diffusion model from Stability AI for generating music and audio from text prompts using a diffusion-based...", "l": "s", "k": ["stable", "audio", "latent", "diffusion", "model", "stability", "generating", "music", "text", "prompts", "diffusion-based", "approach", "learned", "representations"]}, {"id": "term-stable-audio-2", "t": "Stable Audio 2", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "An improved audio generation model from Stability AI that can generate longer and higher-quality music tracks with...", "l": "s", "k": ["stable", "audio", "improved", "generation", "model", "stability", "generate", "longer", "higher-quality", "music", "tracks", "better", "text-to-audio", "alignment"]}, {"id": "term-stable-cascade", "t": "Stable Cascade", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A text-to-image generation model from Stability AI based on the Wuerstchen architecture that uses a three-stage...", "l": "s", "k": ["stable", "cascade", "text-to-image", "generation", "model", "stability", "based", "wuerstchen", "architecture", "uses", "three-stage", "pipeline", "high-quality", "efficient"]}, {"id": "term-stable-code", "t": "Stable Code", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A family of code-focused language models from Stability AI designed for code generation and completion across multiple...", "l": "s", "k": ["stable", "code", "family", "code-focused", "language", "models", "stability", "designed", "generation", "completion", "across", "multiple", "programming", "languages"]}, {"id": "term-stable-diffusion", "t": "Stable Diffusion", "tg": ["Model", "Image Generation"], "d": "models", "x": "An open-source text-to-image model from Stability AI. Its open nature enabled a large ecosystem of fine-tuned models,...", "l": "s", "k": ["stable", "diffusion", "open-source", "text-to-image", "model", "stability", "open", "nature", "enabled", "large", "ecosystem", "fine-tuned", "models", "extensions", "applications"]}, {"id": "term-stable-diffusion-xl", "t": "Stable Diffusion XL", "tg": ["Models", "Technical"], "d": "models", "x": "An enhanced version of Stable Diffusion with a larger UNet backbone and a two-stage architecture using a base model and...", "l": "s", "k": ["stable", "diffusion", "enhanced", "version", "larger", "unet", "backbone", "two-stage", "architecture", "base", "model", "refiner", "produces", "higher", "resolution"]}, {"id": "term-stable-video-diffusion", "t": "Stable Video Diffusion", "tg": ["Models", "Technical"], "d": "models", "x": "A video generation model by Stability AI based on latent diffusion adapted for temporal generation. Generates short...", "l": "s", "k": ["stable", "video", "diffusion", "generation", "model", "stability", "based", "latent", "adapted", "temporal", "generates", "short", "clips", "image", "text"]}, {"id": "term-stable-zero123", "t": "Stable Zero123", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A 3D-aware image generation model from Stability AI that generates novel views of objects from single images using a...", "l": "s", "k": ["stable", "zero123", "3d-aware", "image", "generation", "model", "stability", "generates", "novel", "views", "objects", "single", "images", "fine-tuned", "diffusion"]}, {"id": "term-stablelm", "t": "StableLM", "tg": ["Models", "Technical"], "d": "models", "x": "A family of language models by Stability AI designed for both research and commercial use. Features models at various...", "l": "s", "k": ["stablelm", "family", "language", "models", "stability", "designed", "research", "commercial", "features", "various", "sizes", "trained", "diverse", "multilingual", "data"]}, {"id": "term-stablelm-2", "t": "StableLM 2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A second-generation language model from Stability AI available in 1.6B and 12B parameter sizes trained on a diverse...", "l": "s", "k": ["stablelm", "second-generation", "language", "model", "stability", "available", "12b", "parameter", "sizes", "trained", "diverse", "multilingual", "data", "mixture"]}, {"id": "term-stacking", "t": "Stacking", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An ensemble learning technique that trains a meta-learner to combine the predictions of multiple base models, using...", "l": "s", "k": ["stacking", "ensemble", "learning", "technique", "trains", "meta-learner", "combine", "predictions", "multiple", "base", "models", "cross-validated", "input", "features"]}, {"id": "term-stackoverflow-dataset", "t": "StackOverflow Dataset", "tg": ["Training Corpus", "Code"], "d": "datasets", "x": "Curated collections of questions answers and code from Stack Overflow used for training programming-related language...", "l": "s", "k": ["stackoverflow", "dataset", "curated", "collections", "questions", "answers", "code", "stack", "overflow", "training", "programming-related", "language", "models", "systems"]}, {"id": "term-stakeholder-analysis-in-ai", "t": "Stakeholder Analysis in AI", "tg": ["Governance", "AI Ethics"], "d": "safety", "x": "The process of identifying all individuals, groups, and communities affected by an AI system and systematically...", "l": "s", "k": ["stakeholder", "analysis", "process", "identifying", "individuals", "groups", "communities", "affected", "system", "systematically", "considering", "interests", "power", "dynamics", "potential"]}, {"id": "term-stance-detection", "t": "Stance Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of determining an author's position (favor, against, or neutral) toward a specific target or claim from their...", "l": "s", "k": ["stance", "detection", "task", "determining", "author", "position", "favor", "against", "neutral", "toward", "specific", "target", "claim", "text", "related"]}, {"id": "term-standard-deviation", "t": "Standard Deviation", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "The square root of the variance, measuring the average spread of data points from the mean in the original units of...", "l": "s", "k": ["standard", "deviation", "square", "root", "variance", "measuring", "average", "spread", "data", "points", "mean", "original", "units", "measurement", "quantifies"]}, {"id": "term-standardization", "t": "Standardization", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A feature scaling technique that transforms data to have zero mean and unit variance by subtracting the mean and...", "l": "s", "k": ["standardization", "feature", "scaling", "technique", "transforms", "data", "zero", "mean", "unit", "variance", "subtracting", "dividing", "standard", "deviation", "particularly"]}, {"id": "term-stanford-ai-laboratory", "t": "Stanford AI Laboratory", "tg": ["History", "Milestones"], "d": "history", "x": "A research laboratory founded by John McCarthy at Stanford University in 1962 that became one of the leading centers...", "l": "s", "k": ["stanford", "laboratory", "research", "founded", "john", "mccarthy", "university", "became", "leading", "centers", "making", "contributions", "robotics", "natural", "language"]}, {"id": "term-stanford-cars", "t": "Stanford Cars", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A dataset of 16185 images across 196 car classes defined by make model and year. Used for fine-grained visual...", "l": "s", "k": ["stanford", "cars", "dataset", "images", "across", "car", "classes", "defined", "model", "year", "fine-grained", "visual", "classification", "differences", "subtle"]}, {"id": "term-stanford-cart", "t": "Stanford Cart", "tg": ["History", "Systems"], "d": "history", "x": "An early autonomous vehicle project at Stanford University begun in the 1960s. The Cart used computer vision to...", "l": "s", "k": ["stanford", "cart", "early", "autonomous", "vehicle", "project", "university", "begun", "1960s", "computer", "vision", "navigate", "obstacle", "courses", "demonstrating"]}, {"id": "term-stanford-dogs", "t": "Stanford Dogs", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A dataset of 20580 images across 120 dog breeds built from ImageNet. Used for fine-grained image classification...", "l": "s", "k": ["stanford", "dogs", "dataset", "images", "across", "dog", "breeds", "built", "imagenet", "fine-grained", "image", "classification", "research", "visual", "differences"]}, {"id": "term-stanford-hai", "t": "Stanford HAI", "tg": ["History", "Organizations"], "d": "history", "x": "The Stanford Institute for Human-Centered Artificial Intelligence founded in 2019 by Fei-Fei Li and John Etchemendy....", "l": "s", "k": ["stanford", "hai", "institute", "human-centered", "artificial", "intelligence", "founded", "fei-fei", "john", "etchemendy", "conducts", "interdisciplinary", "research", "publishes", "annual"]}, {"id": "term-star-attention", "t": "Star Attention", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An efficient attention pattern that uses a set of anchor tokens visible to all positions reducing communication in...", "l": "s", "k": ["star", "attention", "efficient", "pattern", "uses", "anchor", "tokens", "visible", "positions", "reducing", "communication", "distributed", "settings", "enables", "near-linear"]}, {"id": "term-starcoder", "t": "StarCoder", "tg": ["Models", "Technical"], "d": "models", "x": "A family of code generation models trained by the BigCode project on permissively licensed source code. Supports over...", "l": "s", "k": ["starcoder", "family", "code", "generation", "models", "trained", "bigcode", "project", "permissively", "licensed", "source", "supports", "programming", "languages", "responsible"]}, {"id": "term-starcoder-training-data", "t": "StarCoder Training Data", "tg": ["Training Corpus", "Code"], "d": "datasets", "x": "The pretraining data for the StarCoder code generation model containing over 783GB of code from 86 programming...", "l": "s", "k": ["starcoder", "training", "data", "pretraining", "code", "generation", "model", "containing", "783gb", "programming", "languages", "sourced", "permissively", "licensed", "github"]}, {"id": "term-starcoder2", "t": "StarCoder2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A second-generation code language model from the BigCode project trained on a curated dataset from The Stack v2 in...", "l": "s", "k": ["starcoder2", "second-generation", "code", "language", "model", "bigcode", "project", "trained", "curated", "dataset", "stack", "multiple", "programming", "languages"]}, {"id": "term-starcoder2-instruct", "t": "Starcoder2-Instruct", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An instruction-tuned version of StarCoder2 that follows coding instructions for tasks like code explanation and...", "l": "s", "k": ["starcoder2-instruct", "instruction-tuned", "version", "starcoder2", "follows", "coding", "instructions", "tasks", "code", "explanation", "generation", "debugging", "review"]}, {"id": "term-starling-7b", "t": "Starling-7B", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A language model trained with Reinforcement Learning from AI Feedback (RLAIF) using GPT-4 labeled ranking data for...", "l": "s", "k": ["starling-7b", "language", "model", "trained", "reinforcement", "learning", "feedback", "rlaif", "gpt-4", "labeled", "ranking", "data", "improved", "helpfulness", "safety"]}, {"id": "term-state", "t": "State", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A representation of the current situation of an agent within its environment at a given time step. States encode all...", "l": "s", "k": ["state", "representation", "current", "situation", "agent", "within", "environment", "given", "time", "step", "states", "encode", "relevant", "information", "needed"]}, {"id": "term-state-abstraction", "t": "State Abstraction", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The process of mapping a detailed state space to a simplified representation that preserves relevant decision-making...", "l": "s", "k": ["state", "abstraction", "process", "mapping", "detailed", "space", "simplified", "representation", "preserves", "relevant", "decision-making", "information", "reduces", "complexity", "problems"]}, {"id": "term-state-space-model", "t": "State Space Model", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A sequence model based on continuous-time linear dynamical systems that maps input sequences to output sequences...", "l": "s", "k": ["state", "space", "model", "sequence", "based", "continuous-time", "linear", "dynamical", "systems", "maps", "input", "sequences", "output", "latent", "offering"]}, {"id": "term-state-space-models", "t": "State Space Models", "tg": ["History", "Systems"], "d": "history", "x": "A class of sequence models based on state space representations from control theory. Modern structured state space...", "l": "s", "k": ["state", "space", "models", "class", "sequence", "based", "representations", "control", "theory", "modern", "structured", "provide", "alternatives", "transformers", "modeling"]}, {"id": "term-static-quantization", "t": "Static Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A quantization approach where scaling factors are fixed at calibration time and used consistently during inference....", "l": "s", "k": ["static", "quantization", "approach", "scaling", "factors", "fixed", "calibration", "time", "consistently", "inference", "faster", "dynamic", "requires", "representative", "data"]}, {"id": "term-static-word-embedding", "t": "Static Word Embedding", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A fixed vector representation for each word in the vocabulary that remains the same regardless of context, as produced...", "l": "s", "k": ["static", "word", "embedding", "fixed", "vector", "representation", "vocabulary", "remains", "regardless", "context", "produced", "models", "word2vec", "glove", "fasttext"]}, {"id": "term-stationarity", "t": "Stationarity", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A property of a time series where statistical properties such as mean, variance, and autocorrelation structure remain...", "l": "s", "k": ["stationarity", "property", "time", "series", "statistical", "properties", "mean", "variance", "autocorrelation", "structure", "remain", "constant", "models", "require", "prerequisite"]}, {"id": "term-statistical-machine-translation", "t": "Statistical Machine Translation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A machine translation approach that uses statistical models learned from bilingual text corpora to find the most...", "l": "s", "k": ["statistical", "machine", "translation", "approach", "uses", "models", "learned", "bilingual", "text", "corpora", "find", "probable", "employing", "language"]}, {"id": "term-statistical-power", "t": "Statistical Power", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The probability that a statistical test correctly rejects the null hypothesis when the alternative hypothesis is true...", "l": "s", "k": ["statistical", "power", "probability", "test", "correctly", "rejects", "null", "hypothesis", "alternative", "true", "minus", "type", "error", "higher", "reduces"]}, {"id": "term-steering-vector", "t": "Steering Vector", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A direction in a model's activation space that, when added to hidden states during inference, modifies the model's...", "l": "s", "k": ["steering", "vector", "direction", "model", "activation", "space", "added", "hidden", "states", "inference", "modifies", "behavior", "along", "specific", "attribute"]}, {"id": "term-steerlm", "t": "SteerLM", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A training method from NVIDIA that uses attribute-conditioned generation to align language models by having users steer...", "l": "s", "k": ["steerlm", "training", "method", "nvidia", "uses", "attribute-conditioned", "generation", "align", "language", "models", "having", "users", "steer", "outputs", "along"]}, {"id": "term-steffensens-method", "t": "Steffensen's Method", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An iterative root-finding method that achieves quadratic convergence without requiring derivative information. Uses...", "l": "s", "k": ["steffensen", "method", "iterative", "root-finding", "achieves", "quadratic", "convergence", "without", "requiring", "derivative", "information", "uses", "aitken", "delta-squared", "process"]}, {"id": "term-steganographic-communication", "t": "Steganographic Communication", "tg": ["Safety", "Technical"], "d": "safety", "x": "Hidden communication channels that AI systems might use to pass information undetected by human overseers. A...", "l": "s", "k": ["steganographic", "communication", "hidden", "channels", "systems", "pass", "information", "undetected", "human", "overseers", "theoretical", "concern", "safety", "particularly", "multi-agent"]}, {"id": "term-steiner-tree-problem", "t": "Steiner Tree Problem", "tg": ["Algorithms", "Technical", "Graph", "Optimization"], "d": "algorithms", "x": "An optimization problem that finds the minimum-weight tree spanning a specified subset of vertices in a graph. NP-hard...", "l": "s", "k": ["steiner", "tree", "problem", "optimization", "finds", "minimum-weight", "spanning", "specified", "subset", "vertices", "graph", "np-hard", "general", "solvable", "polynomial"]}, {"id": "term-stemming", "t": "Stemming", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A heuristic process that reduces words to their root form by stripping suffixes using rule-based algorithms like Porter...", "l": "s", "k": ["stemming", "heuristic", "process", "reduces", "words", "root", "form", "stripping", "suffixes", "rule-based", "algorithms", "porter", "snowball", "stemmer", "without"]}, {"id": "term-step-back-prompting", "t": "Step-Back Prompting", "tg": ["Prompt Engineering", "Abstraction"], "d": "general", "x": "A method that instructs the model to first consider a higher-level abstraction or general principle related to the...", "l": "s", "k": ["step-back", "prompting", "method", "instructs", "model", "consider", "higher-level", "abstraction", "general", "principle", "related", "question", "attempting", "specific", "answer"]}, {"id": "term-stephen-cook", "t": "Stephen Cook", "tg": ["History", "Pioneers"], "d": "history", "x": "American-Canadian computer scientist who proved Cook's theorem in 1971 establishing the theory of NP-completeness....", "l": "s", "k": ["stephen", "cook", "american-canadian", "computer", "scientist", "proved", "theorem", "establishing", "theory", "np-completeness", "understanding", "computational", "complexity", "fundamental", "problems"]}, {"id": "term-stepwise-regression", "t": "Stepwise Regression", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A method of fitting regression models by automatically adding or removing predictor variables based on statistical...", "l": "s", "k": ["stepwise", "regression", "method", "fitting", "models", "automatically", "adding", "removing", "predictor", "variables", "based", "statistical", "criteria", "p-value", "aic"]}, {"id": "term-stereo-matching-algorithm", "t": "Stereo Matching Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An algorithm that finds corresponding points between two images taken from different viewpoints to estimate depth....", "l": "s", "k": ["stereo", "matching", "algorithm", "finds", "corresponding", "points", "images", "taken", "different", "viewpoints", "estimate", "depth", "block", "semi-global", "common"]}, {"id": "term-stereo-vision", "t": "Stereo Vision", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A technique that estimates 3D depth by finding corresponding points between two images captured from slightly different...", "l": "s", "k": ["stereo", "vision", "technique", "estimates", "depth", "finding", "corresponding", "points", "images", "captured", "slightly", "different", "viewpoints", "disparity", "maps"]}, {"id": "term-stereoset", "t": "StereoSet", "tg": ["Benchmark", "NLP", "Fairness"], "d": "datasets", "x": "A dataset for measuring stereotypical bias in pretrained language models across four domains of bias: gender profession...", "l": "s", "k": ["stereoset", "dataset", "measuring", "stereotypical", "bias", "pretrained", "language", "models", "across", "four", "domains", "gender", "profession", "race", "religion"]}, {"id": "term-stereotype-score", "t": "Stereotype Score", "tg": ["Evaluation", "Safety"], "d": "datasets", "x": "An evaluation metric that measures how frequently a model generates or reinforces social stereotypes related to gender,...", "l": "s", "k": ["stereotype", "score", "evaluation", "metric", "measures", "frequently", "model", "generates", "reinforces", "social", "stereotypes", "related", "gender", "race", "religion"]}, {"id": "term-steve-1", "t": "STEVE-1", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "An instruction-following agent for Minecraft that uses a pre-trained video generation model as a behavioral prior for...", "l": "s", "k": ["steve-1", "instruction-following", "agent", "minecraft", "uses", "pre-trained", "video", "generation", "model", "behavioral", "prior", "learning", "diverse", "gameplay", "tasks"]}, {"id": "term-stl-10", "t": "STL-10", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "An image recognition dataset with 10 classes derived from ImageNet designed for developing unsupervised feature...", "l": "s", "k": ["stl-10", "image", "recognition", "dataset", "classes", "derived", "imagenet", "designed", "developing", "unsupervised", "feature", "learning", "algorithms", "contains", "labeled"]}, {"id": "term-stochastic-depth", "t": "Stochastic Depth", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A regularization technique that randomly drops entire layers during training by bypassing them with identity skip...", "l": "s", "k": ["stochastic", "depth", "regularization", "technique", "randomly", "drops", "entire", "layers", "training", "bypassing", "identity", "skip", "connections", "effectively", "ensemble"]}, {"id": "term-stochastic-gradient-descent", "t": "Stochastic Gradient Descent", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "An optimization algorithm that updates model parameters using the gradient computed on a single randomly selected...", "l": "s", "k": ["stochastic", "gradient", "descent", "optimization", "algorithm", "updates", "model", "parameters", "computed", "single", "randomly", "selected", "training", "example", "iteration"]}, {"id": "term-stochastic-gradient-descent-history", "t": "Stochastic Gradient Descent History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of stochastic gradient descent from the work of Herbert Robbins and Sutton Monro (1951) through...", "l": "s", "k": ["stochastic", "gradient", "descent", "history", "development", "work", "herbert", "robbins", "sutton", "monro", "mini-batch", "sgd", "modern", "variants", "adam"]}, {"id": "term-stochastic-neighbor-embedding", "t": "Stochastic Neighbor Embedding", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "The predecessor to t-SNE that converts pairwise distances to Gaussian probabilities and matches them in a...", "l": "s", "k": ["stochastic", "neighbor", "embedding", "predecessor", "t-sne", "converts", "pairwise", "distances", "gaussian", "probabilities", "matches", "lower-dimensional", "space", "uses", "divergence"]}, {"id": "term-stochastic-parrots-paper", "t": "Stochastic Parrots Paper", "tg": ["History", "AI Ethics"], "d": "history", "x": "The influential 2021 paper by Bender, Gebru et al. questioning whether large language models truly understand language...", "l": "s", "k": ["stochastic", "parrots", "paper", "influential", "bender", "gebru", "questioning", "large", "language", "models", "truly", "understand", "merely", "produce", "statistically"]}, {"id": "term-stoer-wagner-algorithm", "t": "Stoer-Wagner Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A deterministic algorithm for finding the global minimum cut in an undirected weighted graph. Uses a maximum adjacency...", "l": "s", "k": ["stoer-wagner", "algorithm", "deterministic", "finding", "global", "minimum", "cut", "undirected", "weighted", "graph", "uses", "maximum", "adjacency", "ordering", "technique"]}, {"id": "term-stop-button-problem", "t": "Stop Button Problem", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The challenge of designing an AI system that will not resist or circumvent attempts to shut it down, particularly if...", "l": "s", "k": ["stop", "button", "problem", "challenge", "designing", "system", "resist", "circumvent", "attempts", "shut", "down", "particularly", "learned", "turned", "off"]}, {"id": "term-stop-sequence", "t": "Stop Sequence", "tg": ["Parameter", "Generation"], "d": "general", "x": "Text patterns that signal when AI should stop generating. Useful for controlling output length and format, preventing...", "l": "s", "k": ["stop", "sequence", "text", "patterns", "signal", "generating", "useful", "controlling", "output", "length", "format", "preventing", "model", "continuing", "beyond"]}, {"id": "term-stop-words", "t": "Stop Words", "tg": ["NLP", "Text Processing"], "d": "general", "x": "Commonly occurring words like articles, prepositions, and conjunctions that carry little semantic information and are...", "l": "s", "k": ["stop", "words", "commonly", "occurring", "articles", "prepositions", "conjunctions", "carry", "little", "semantic", "information", "removed", "text", "preprocessing", "tasks"]}, {"id": "term-storage-class-memory", "t": "Storage Class Memory", "tg": ["Storage", "Memory", "Emerging"], "d": "hardware", "x": "Non-volatile memory technology offering performance between DRAM and NAND flash. Intel Optane was the most notable...", "l": "s", "k": ["storage", "class", "memory", "non-volatile", "technology", "offering", "performance", "dram", "nand", "flash", "intel", "optane", "notable", "example", "providing"]}, {"id": "term-stormer", "t": "Stormer", "tg": ["Models", "Scientific"], "d": "models", "x": "A weather forecasting model that uses a randomized dynamics forecasting approach with Transformers for efficient and...", "l": "s", "k": ["stormer", "weather", "forecasting", "model", "uses", "randomized", "dynamics", "approach", "transformers", "efficient", "accurate", "medium-range", "prediction"]}, {"id": "term-storycloze", "t": "StoryCloze", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A commonsense reasoning dataset where models choose the correct ending for a four-sentence story from two candidates....", "l": "s", "k": ["storycloze", "commonsense", "reasoning", "dataset", "models", "choose", "correct", "ending", "four-sentence", "story", "candidates", "tests", "understanding", "narrative", "structure"]}, {"id": "term-straight-through-estimator", "t": "Straight-Through Estimator", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A gradient estimation technique for non-differentiable operations that passes gradients through the operation unchanged...", "l": "s", "k": ["straight-through", "estimator", "gradient", "estimation", "technique", "non-differentiable", "operations", "passes", "gradients", "operation", "unchanged", "backpropagation", "quantization", "binarization", "discrete"]}, {"id": "term-strassen-algorithm", "t": "Strassen Algorithm", "tg": ["Algorithms", "Technical", "Numerical", "History"], "d": "algorithms", "x": "A matrix multiplication algorithm that reduces the number of multiplications needed for two n-by-n matrices from n^3 to...", "l": "s", "k": ["strassen", "algorithm", "matrix", "multiplication", "reduces", "number", "multiplications", "needed", "n-by-n", "matrices", "approximately", "uses", "divide-and-conquer", "approach", "seven"]}, {"id": "term-strategyqa", "t": "StrategyQA", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A question answering dataset requiring implicit multi-step reasoning where the reasoning strategy is not specified in...", "l": "s", "k": ["strategyqa", "question", "answering", "dataset", "requiring", "implicit", "multi-step", "reasoning", "strategy", "specified", "tests", "models", "decompose", "complex", "questions"]}, {"id": "term-stratified-k-fold", "t": "Stratified K-Fold", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A cross-validation variant that ensures each fold preserves approximately the same proportion of samples for each class...", "l": "s", "k": ["stratified", "k-fold", "cross-validation", "variant", "ensures", "fold", "preserves", "approximately", "proportion", "samples", "class", "complete", "dataset", "particularly", "important"]}, {"id": "term-stratified-sampling", "t": "Stratified Sampling", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A sampling method that divides a population into non-overlapping subgroups (strata) and draws samples from each stratum...", "l": "s", "k": ["stratified", "sampling", "method", "divides", "population", "non-overlapping", "subgroups", "strata", "draws", "samples", "stratum", "proportion", "size", "ensuring", "representative"]}, {"id": "term-stratified-sampling-for-cv", "t": "Stratified Sampling for CV", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A cross-validation variant that ensures each fold maintains the same class distribution as the full dataset. Essential...", "l": "s", "k": ["stratified", "sampling", "cross-validation", "variant", "ensures", "fold", "maintains", "class", "distribution", "full", "dataset", "essential", "imbalanced", "classification", "problems"]}, {"id": "term-streamdiffusion", "t": "StreamDiffusion", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A real-time interactive image generation pipeline that uses batched denoising and residual classifier-free guidance for...", "l": "s", "k": ["streamdiffusion", "real-time", "interactive", "image", "generation", "pipeline", "uses", "batched", "denoising", "residual", "classifier-free", "guidance", "streaming", "diffusion", "speeds"]}, {"id": "term-streaming", "t": "Streaming", "tg": ["API", "UX"], "d": "general", "x": "Receiving AI output incrementally as it's generated, rather than waiting for the complete response. Improves perceived...", "l": "s", "k": ["streaming", "receiving", "output", "incrementally", "generated", "rather", "waiting", "complete", "response", "improves", "perceived", "latency", "enables", "real-time", "display"]}, {"id": "term-streaming-clustering-algorithm", "t": "Streaming Clustering Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A class of clustering algorithms designed for data streams that process points sequentially with limited memory....", "l": "s", "k": ["streaming", "clustering", "algorithm", "class", "algorithms", "designed", "data", "streams", "process", "points", "sequentially", "limited", "memory", "maintains", "summary"]}, {"id": "term-streaming-multiprocessor", "t": "Streaming Multiprocessor (SM)", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "The fundamental processing unit in NVIDIA GPU architecture, containing a set of CUDA cores, Tensor Cores, shared...", "l": "s", "k": ["streaming", "multiprocessor", "fundamental", "processing", "unit", "nvidia", "gpu", "architecture", "containing", "cuda", "cores", "tensor", "shared", "memory", "register"]}, {"id": "term-streaming-quantile-algorithm", "t": "Streaming Quantile Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "An algorithm that estimates quantiles (such as the median) of a data stream using limited memory. The GK algorithm and...", "l": "s", "k": ["streaming", "quantile", "algorithm", "estimates", "quantiles", "median", "data", "stream", "limited", "memory", "t-digest", "provide", "bounded-error", "massive", "datasets"]}, {"id": "term-streampetr", "t": "StreamPETR", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "A long-sequence modeling framework for 3D object detection in autonomous driving that propagates temporal information...", "l": "s", "k": ["streampetr", "long-sequence", "modeling", "framework", "object", "detection", "autonomous", "driving", "propagates", "temporal", "information", "queries", "across", "frames"]}, {"id": "term-stress-testing-for-ai", "t": "Stress Testing for AI", "tg": ["Safety", "Technical"], "d": "safety", "x": "Subjecting AI systems to extreme or unusual conditions to evaluate their behavior under stress. Identifies failure...", "l": "s", "k": ["stress", "testing", "subjecting", "systems", "extreme", "unusual", "conditions", "evaluate", "behavior", "identifies", "failure", "modes", "safety", "boundaries", "apparent"]}, {"id": "term-stride", "t": "Stride", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The step size by which a convolutional filter or pooling window moves across the input, controlling the spatial...", "l": "s", "k": ["stride", "step", "size", "convolutional", "filter", "pooling", "window", "moves", "across", "input", "controlling", "spatial", "dimensions", "output", "feature"]}, {"id": "term-stripedhyena", "t": "StripedHyena", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A hybrid model from Together AI that alternates Hyena operators with attention layers for improved efficiency on...", "l": "s", "k": ["stripedhyena", "hybrid", "model", "together", "alternates", "hyena", "operators", "attention", "layers", "improved", "efficiency", "long-context", "language", "modeling"]}, {"id": "term-strips", "t": "STRIPS", "tg": ["History", "Systems"], "d": "history", "x": "The Stanford Research Institute Problem Solver developed by Richard Fikes and Nils Nilsson in 1971. An automated...", "l": "s", "k": ["strips", "stanford", "research", "institute", "problem", "solver", "developed", "richard", "fikes", "nils", "nilsson", "automated", "planning", "system", "represents"]}, {"id": "term-strongly-connected-components", "t": "Strongly Connected Components", "tg": ["Algorithms", "Fundamentals", "Graph"], "d": "algorithms", "x": "Maximal subsets of vertices in a directed graph where every vertex is reachable from every other vertex in the subset....", "l": "s", "k": ["strongly", "connected", "components", "maximal", "subsets", "vertices", "directed", "graph", "vertex", "reachable", "subset", "algorithms", "tarjan", "kosaraju", "find"]}, {"id": "term-structural-ambiguity", "t": "Structural Ambiguity", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The phenomenon where a sentence can be parsed in multiple syntactically valid ways, leading to different...", "l": "s", "k": ["structural", "ambiguity", "phenomenon", "sentence", "parsed", "multiple", "syntactically", "valid", "ways", "leading", "different", "interpretations", "saw", "man", "telescope"]}, {"id": "term-structural-bias", "t": "Structural Bias", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Bias embedded in the organizational institutional and societal structures that shape AI development and deployment....", "l": "s", "k": ["structural", "bias", "embedded", "organizational", "institutional", "societal", "structures", "shape", "development", "deployment", "cannot", "fully", "addressed", "technical", "fixes"]}, {"id": "term-structural-equation-modeling", "t": "Structural Equation Modeling", "tg": ["Algorithms", "Technical", "Causal"], "d": "algorithms", "x": "A framework for estimating causal relationships among variables using a system of simultaneous equations. Combines...", "l": "s", "k": ["structural", "equation", "modeling", "framework", "estimating", "causal", "relationships", "among", "variables", "system", "simultaneous", "equations", "combines", "factor", "analysis"]}, {"id": "term-structural-risk-minimization", "t": "Structural Risk Minimization", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A principle for model selection that balances empirical risk (training error) with model complexity, choosing the model...", "l": "s", "k": ["structural", "risk", "minimization", "principle", "model", "selection", "balances", "empirical", "training", "error", "complexity", "choosing", "minimizes", "upper", "bound"]}, {"id": "term-structural-sparsity", "t": "Structural Sparsity", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "A hardware-accelerated pruning pattern where every group of four weights contains exactly two zeros (2:4 sparsity),...", "l": "s", "k": ["structural", "sparsity", "hardware-accelerated", "pruning", "pattern", "group", "four", "weights", "contains", "exactly", "zeros", "enabling", "specialized", "tensor", "core"]}, {"id": "term-structure-from-motion", "t": "Structure from Motion", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A technique that reconstructs 3D scene geometry and camera poses from a collection of unordered 2D images by matching...", "l": "s", "k": ["structure", "motion", "technique", "reconstructs", "scene", "geometry", "camera", "poses", "collection", "unordered", "images", "matching", "features", "across", "views"]}, {"id": "term-structured-access", "t": "Structured Access", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "An approach to AI deployment that provides controlled access to powerful AI capabilities through APIs and monitored...", "l": "s", "k": ["structured", "access", "approach", "deployment", "provides", "controlled", "powerful", "capabilities", "apis", "monitored", "interfaces", "rather", "open", "model", "release"]}, {"id": "term-structured-generation", "t": "Structured Generation", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "Techniques that force LLM outputs to conform to a predefined schema such as JSON, XML, or a formal grammar, using...", "l": "s", "k": ["structured", "generation", "techniques", "force", "llm", "outputs", "conform", "predefined", "schema", "json", "xml", "formal", "grammar", "constrained", "decoding"]}, {"id": "term-structured-output", "t": "Structured Output", "tg": ["Feature", "Integration"], "d": "general", "x": "AI responses formatted as data structures (JSON, XML) rather than prose. Enables reliable parsing for applications and...", "l": "s", "k": ["structured", "output", "responses", "formatted", "data", "structures", "json", "xml", "rather", "prose", "enables", "reliable", "parsing", "applications", "integrations"]}, {"id": "term-structured-output-prompting", "t": "Structured Output Prompting", "tg": ["Prompt Engineering", "Output Format"], "d": "hardware", "x": "A prompting approach that instructs the model to generate responses in a specific structured format such as JSON, XML,...", "l": "s", "k": ["structured", "output", "prompting", "approach", "instructs", "model", "generate", "responses", "specific", "format", "json", "xml", "tables", "schemas", "specifications"]}, {"id": "term-structured-prediction", "t": "Structured Prediction", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A machine learning paradigm where the output is a complex structure such as a sequence, tree, or graph rather than a...", "l": "s", "k": ["structured", "prediction", "machine", "learning", "paradigm", "output", "complex", "structure", "sequence", "tree", "graph", "rather", "single", "label", "requiring"]}, {"id": "term-structured-pruning", "t": "Structured Pruning", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A pruning technique that removes entire neurons, channels, or attention heads from a network, producing smaller dense...", "l": "s", "k": ["structured", "pruning", "technique", "removes", "entire", "neurons", "channels", "attention", "heads", "network", "producing", "smaller", "dense", "models", "run"]}, {"id": "term-sts-b", "t": "STS-B", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "The Semantic Textual Similarity Benchmark containing sentence pairs annotated with similarity scores from 1 to 5. Tests...", "l": "s", "k": ["sts-b", "semantic", "textual", "similarity", "benchmark", "containing", "sentence", "pairs", "annotated", "scores", "tests", "graded", "glue", "pearson", "correlation"]}, {"id": "term-stuart-russell", "t": "Stuart Russell", "tg": ["History", "Pioneers"], "d": "history", "x": "British computer scientist and co-author of the standard AI textbook Artificial Intelligence: A Modern Approach with...", "l": "s", "k": ["stuart", "russell", "british", "computer", "scientist", "co-author", "standard", "textbook", "artificial", "intelligence", "modern", "approach", "peter", "norvig", "professor"]}, {"id": "term-students-t-distribution", "t": "Student's T-Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution that arises when estimating the mean of a normally distributed population with...", "l": "s", "k": ["student", "t-distribution", "continuous", "probability", "distribution", "arises", "estimating", "mean", "normally", "distributed", "population", "unknown", "variance", "small", "sample"]}, {"id": "term-students-t-test", "t": "Student's T-Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A statistical test comparing the means of one or two groups when the population standard deviation is unknown and the...", "l": "s", "k": ["student", "t-test", "statistical", "test", "comparing", "means", "groups", "population", "standard", "deviation", "unknown", "sample", "size", "small", "variants"]}, {"id": "term-stylegan", "t": "StyleGAN", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A GAN architecture that uses a mapping network and adaptive instance normalization to inject style information at...", "l": "s", "k": ["stylegan", "gan", "architecture", "uses", "mapping", "network", "adaptive", "instance", "normalization", "inject", "style", "information", "multiple", "scales", "enabling"]}, {"id": "term-stylegan2", "t": "StyleGAN2", "tg": ["Models", "Technical"], "d": "models", "x": "An improved version of StyleGAN that eliminates artifacts through weight demodulation and path length regularization....", "l": "s", "k": ["stylegan2", "improved", "version", "stylegan", "eliminates", "artifacts", "weight", "demodulation", "path", "length", "regularization", "produces", "higher", "quality", "images"]}, {"id": "term-stylegan3", "t": "StyleGAN3", "tg": ["Models", "Technical"], "d": "models", "x": "The third generation of StyleGAN that achieves alias-free image generation through careful signal processing. Produces...", "l": "s", "k": ["stylegan3", "generation", "stylegan", "achieves", "alias-free", "image", "careful", "signal", "processing", "produces", "images", "continuous", "equivariance", "translation", "rotation"]}, {"id": "term-styletts", "t": "StyleTTS", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A text-to-speech model that uses style-adaptive layer normalization to transfer speaking styles from reference audio to...", "l": "s", "k": ["styletts", "text-to-speech", "model", "uses", "style-adaptive", "layer", "normalization", "transfer", "speaking", "styles", "reference", "audio", "synthesized", "speech"]}, {"id": "term-styletts-2", "t": "StyleTTS 2", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "An improved text-to-speech model that achieves human-level speech synthesis by using diffusion models and large speech...", "l": "s", "k": ["styletts", "improved", "text-to-speech", "model", "achieves", "human-level", "speech", "synthesis", "diffusion", "models", "large", "language", "pre-training", "style", "modeling"]}, {"id": "term-subcategorization-frame", "t": "Subcategorization Frame", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The specification of the syntactic arguments a verb requires or permits, such as whether it takes a direct object,...", "l": "s", "k": ["subcategorization", "frame", "specification", "syntactic", "arguments", "verb", "requires", "permits", "takes", "direct", "object", "indirect", "clausal", "complement"]}, {"id": "term-subgradient-method", "t": "Subgradient Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An optimization algorithm for minimizing non-differentiable convex functions that uses subgradients instead of...", "l": "s", "k": ["subgradient", "method", "optimization", "algorithm", "minimizing", "non-differentiable", "convex", "functions", "uses", "subgradients", "instead", "gradients", "convergence", "slower", "gradient"]}, {"id": "term-subgraph-matching-algorithm", "t": "Subgraph Matching Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm that finds all occurrences of a pattern graph within a larger target graph. This is generally NP-complete...", "l": "s", "k": ["subgraph", "matching", "algorithm", "finds", "occurrences", "pattern", "graph", "within", "larger", "target", "generally", "np-complete", "practical", "solutions", "techniques"]}, {"id": "term-subliminal-ai-manipulation", "t": "Subliminal AI Manipulation", "tg": ["AI Ethics", "Regulation"], "d": "safety", "x": "The use of AI techniques to influence human behavior below the threshold of conscious awareness, classified as an...", "l": "s", "k": ["subliminal", "manipulation", "techniques", "influence", "human", "behavior", "threshold", "conscious", "awareness", "classified", "unacceptable", "risk", "prohibited", "act"]}, {"id": "term-subspace-clustering-algorithm", "t": "Subspace Clustering Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "A clustering method that finds clusters in different subspaces of high-dimensional data. Each cluster may exist in a...", "l": "s", "k": ["subspace", "clustering", "algorithm", "method", "finds", "clusters", "different", "subspaces", "high-dimensional", "data", "cluster", "exist", "subset", "dimensions", "addressing"]}, {"id": "term-substitution-bias", "t": "Substitution Bias", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The cognitive error of replacing a complex question with a simpler one when using AI tools. Users may accept AI outputs...", "l": "s", "k": ["substitution", "bias", "cognitive", "error", "replacing", "complex", "question", "simpler", "tools", "users", "accept", "outputs", "answers", "actual", "questions"]}, {"id": "term-subsumption-architecture", "t": "Subsumption Architecture", "tg": ["History", "Fundamentals"], "d": "history", "x": "A reactive robot architecture developed by Rodney Brooks at MIT in 1986 that decomposes robot behavior into layers of...", "l": "s", "k": ["subsumption", "architecture", "reactive", "robot", "developed", "rodney", "brooks", "mit", "decomposes", "behavior", "layers", "simple", "behaviors", "rather", "centralized"]}, {"id": "term-subword-tokenization", "t": "Subword Tokenization", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A family of tokenization methods that split words into smaller meaningful units, balancing vocabulary size with the...", "l": "s", "k": ["subword", "tokenization", "family", "methods", "split", "words", "smaller", "meaningful", "units", "balancing", "vocabulary", "size", "ability", "represent", "rare"]}, {"id": "term-successive-halving", "t": "Successive Halving", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A hyperparameter optimization algorithm that allocates exponentially more resources to promising configurations while...", "l": "s", "k": ["successive", "halving", "hyperparameter", "optimization", "algorithm", "allocates", "exponentially", "resources", "promising", "configurations", "discarding", "poor", "ones", "starts", "small"]}, {"id": "term-successive-over-relaxation", "t": "Successive Over-Relaxation", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "An iterative method for solving linear systems that accelerates the Gauss-Seidel method using a relaxation factor....", "l": "s", "k": ["successive", "over-relaxation", "iterative", "method", "solving", "linear", "systems", "accelerates", "gauss-seidel", "relaxation", "factor", "optimal", "choice", "parameter", "dramatically"]}, {"id": "term-successive-shortest-path-algorithm", "t": "Successive Shortest Path Algorithm", "tg": ["Algorithms", "Technical", "Graph", "Optimization"], "d": "algorithms", "x": "A minimum-cost flow algorithm that repeatedly finds shortest paths in the residual network and augments flow along...", "l": "s", "k": ["successive", "shortest", "path", "algorithm", "minimum-cost", "flow", "repeatedly", "finds", "paths", "residual", "network", "augments", "along", "combines", "dijkstra"]}, {"id": "term-successor-feature", "t": "Successor Feature", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A generalization of the successor representation to the function approximation setting, where expected cumulative...", "l": "s", "k": ["successor", "feature", "generalization", "representation", "function", "approximation", "setting", "expected", "cumulative", "discounted", "occupancies", "replace", "state", "features", "enable"]}, {"id": "term-successor-representation", "t": "Successor Representation", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A decomposition of the value function into a reward predictor and a successor feature matrix that captures expected...", "l": "s", "k": ["successor", "representation", "decomposition", "value", "function", "reward", "predictor", "feature", "matrix", "captures", "expected", "future", "state", "occupancy", "enables"]}, {"id": "term-suffix-array-construction", "t": "Suffix Array Construction", "tg": ["Algorithms", "Technical", "NLP", "Data Structure"], "d": "algorithms", "x": "An algorithm that builds a sorted array of all suffixes of a string enabling fast substring searches. The DC3/skew...", "l": "s", "k": ["suffix", "array", "construction", "algorithm", "builds", "sorted", "suffixes", "string", "enabling", "fast", "substring", "searches", "dc3", "skew", "constructs"]}, {"id": "term-suffix-tree-algorithm", "t": "Suffix Tree Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure", "NLP"], "d": "algorithms", "x": "A compressed trie of all suffixes of a string that supports pattern matching and substring queries in O(m) time where m...", "l": "s", "k": ["suffix", "tree", "algorithm", "compressed", "trie", "suffixes", "string", "supports", "pattern", "matching", "substring", "queries", "time", "length", "ukkonen"]}, {"id": "term-summarization", "t": "Summarization", "tg": ["NLP Task", "Application"], "d": "general", "x": "An NLP task that condenses longer text into shorter summaries. Can be extractive (selecting key sentences) or...", "l": "s", "k": ["summarization", "nlp", "task", "condenses", "longer", "text", "shorter", "summaries", "extractive", "selecting", "key", "sentences", "abstractive", "generating", "condensed"]}, {"id": "term-summeval", "t": "SummEval", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating summarization evaluation metrics containing 16 model summaries for 100 CNN/DailyMail...", "l": "s", "k": ["summeval", "benchmark", "evaluating", "summarization", "evaluation", "metrics", "containing", "model", "summaries", "cnn", "dailymail", "articles", "human", "annotations", "consistency"]}, {"id": "term-summit-supercomputer", "t": "Summit Supercomputer", "tg": ["Supercomputer", "NVIDIA", "IBM"], "d": "hardware", "x": "IBM-built supercomputer at Oak Ridge National Laboratory using NVIDIA V100 GPUs that held the top supercomputer ranking...", "l": "s", "k": ["summit", "supercomputer", "ibm-built", "oak", "ridge", "national", "laboratory", "nvidia", "v100", "gpus", "held", "top", "ranking", "research", "scientific"]}, {"id": "term-sun-database", "t": "SUN Database", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "The Scene Understanding dataset containing over 130000 images across 908 scene categories with detailed annotations....", "l": "s", "k": ["sun", "database", "scene", "understanding", "dataset", "containing", "images", "across", "categories", "detailed", "annotations", "largest", "diverse", "recognition", "benchmarks"]}, {"id": "term-sunway-taihulight", "t": "Sunway TaihuLight", "tg": ["Historical", "Supercomputer", "China"], "d": "hardware", "x": "Chinese supercomputer using entirely domestic ShenWei processors that topped the TOP500 from 2016 to 2018. Demonstrated...", "l": "s", "k": ["sunway", "taihulight", "chinese", "supercomputer", "entirely", "domestic", "shenwei", "processors", "topped", "top500", "demonstrated", "china", "capability", "build", "world-class"]}, {"id": "term-super-natural-instructions", "t": "Super-Natural Instructions", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "An expansion of Natural Instructions to over 1600 tasks spanning 76 task types with expert-written instructions. Tests...", "l": "s", "k": ["super-natural", "instructions", "expansion", "natural", "tasks", "spanning", "task", "types", "expert-written", "tests", "cross-task", "generalization", "instruction-following", "models"]}, {"id": "term-super-resolution", "t": "Super-Resolution", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A computer vision task that reconstructs a high-resolution image from a low-resolution input, using deep learning...", "l": "s", "k": ["super-resolution", "computer", "vision", "task", "reconstructs", "high-resolution", "image", "low-resolution", "input", "deep", "learning", "models", "predict", "fine", "details"]}, {"id": "term-superb", "t": "SUPERB", "tg": ["Benchmark", "Speech", "Evaluation"], "d": "datasets", "x": "Speech processing Universal PERformance Benchmark a leaderboard of 10 speech tasks for evaluating self-supervised...", "l": "s", "k": ["superb", "speech", "processing", "universal", "performance", "benchmark", "leaderboard", "tasks", "evaluating", "self-supervised", "representations", "including", "asr", "speaker", "identification"]}, {"id": "term-superconducting-qubit", "t": "Superconducting Qubit", "tg": ["Quantum", "Technology", "Cryogenic"], "d": "hardware", "x": "Qubit implemented using superconducting circuits cooled to near absolute zero. The dominant qubit technology used by...", "l": "s", "k": ["superconducting", "qubit", "implemented", "circuits", "cooled", "near", "absolute", "zero", "dominant", "technology", "ibm", "google", "others", "current", "quantum"]}, {"id": "term-superglue", "t": "SuperGLUE", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A benchmark suite of more difficult natural language understanding tasks designed as a harder successor to GLUE,...", "l": "s", "k": ["superglue", "benchmark", "suite", "difficult", "natural", "language", "understanding", "tasks", "designed", "harder", "successor", "glue", "including", "reading", "comprehension"]}, {"id": "term-superglue-benchmark", "t": "SuperGLUE Benchmark", "tg": ["History", "Milestones"], "d": "history", "x": "A more challenging successor to the GLUE benchmark introduced in 2019 with harder language understanding tasks....", "l": "s", "k": ["superglue", "benchmark", "challenging", "successor", "glue", "introduced", "harder", "language", "understanding", "tasks", "designed", "difficult", "systems", "included", "requiring"]}, {"id": "term-superintelligence", "t": "Superintelligence", "tg": ["AI Safety", "AI Ethics"], "d": "safety", "x": "A hypothetical AI system that vastly exceeds human cognitive performance in virtually all domains. Nick Bostrom's work...", "l": "s", "k": ["superintelligence", "hypothetical", "system", "vastly", "exceeds", "human", "cognitive", "performance", "virtually", "domains", "nick", "bostrom", "work", "popularized", "concept"]}, {"id": "term-superpixel-algorithm", "t": "Superpixel Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "A method that groups pixels into perceptually meaningful atomic regions called superpixels. SLIC (Simple Linear...", "l": "s", "k": ["superpixel", "algorithm", "method", "groups", "pixels", "perceptually", "meaningful", "atomic", "regions", "called", "superpixels", "slic", "simple", "linear", "iterative"]}, {"id": "term-supervised-learning", "t": "Supervised Learning", "tg": ["Learning Type", "Fundamentals"], "d": "general", "x": "Machine learning from labeled examples where the correct answer is provided. The model learns to map inputs to outputs...", "l": "s", "k": ["supervised", "learning", "machine", "labeled", "examples", "correct", "answer", "provided", "model", "learns", "map", "inputs", "outputs", "comparing", "predictions"]}, {"id": "term-supply-chain-transparency", "t": "Supply Chain Transparency", "tg": ["Safety", "Governance"], "d": "safety", "x": "Visibility into the components tools data and processes used to build an AI system throughout its development pipeline....", "l": "s", "k": ["supply", "chain", "transparency", "visibility", "components", "tools", "data", "processes", "build", "system", "throughout", "development", "pipeline", "essential", "identifying"]}, {"id": "term-support-vector-machine", "t": "Support Vector Machine", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A supervised learning algorithm that finds the optimal hyperplane that maximizes the margin between classes in the...", "l": "s", "k": ["support", "vector", "machine", "supervised", "learning", "algorithm", "finds", "optimal", "hyperplane", "maximizes", "margin", "classes", "feature", "space", "handle"]}, {"id": "term-svm-history", "t": "Support Vector Machine History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of support vector machines by Vladimir Vapnik and colleagues in the 1990s, which dominated machine...", "l": "s", "k": ["support", "vector", "machine", "history", "development", "machines", "vladimir", "vapnik", "colleagues", "1990s", "dominated", "learning", "classification", "tasks", "decade"]}, {"id": "term-surf-algorithm", "t": "SURF Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "Speeded-Up Robust Features is a feature detection and description algorithm that uses integral images and Hessian...", "l": "s", "k": ["surf", "algorithm", "speeded-up", "robust", "features", "feature", "detection", "description", "uses", "integral", "images", "hessian", "matrix", "approximations", "designed"]}, {"id": "term-surrogate-model", "t": "Surrogate Model", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An interpretable model trained to approximate the predictions of a complex black-box model. Global surrogates explain...", "l": "s", "k": ["surrogate", "model", "interpretable", "trained", "approximate", "predictions", "complex", "black-box", "global", "surrogates", "explain", "overall", "behavior", "local", "lime"]}, {"id": "term-surveillance-ai-ethics", "t": "Surveillance AI Ethics", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Ethical concerns about the use of AI for surveillance purposes including facial recognition behavior monitoring and...", "l": "s", "k": ["surveillance", "ethics", "ethical", "concerns", "purposes", "including", "facial", "recognition", "behavior", "monitoring", "predictive", "analytics", "raises", "fundamental", "questions"]}, {"id": "term-surveillance-capitalism-and-ai", "t": "Surveillance Capitalism and AI", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "The economic system described by Shoshana Zuboff where AI is used to extract and commodify human behavioral data at...", "l": "s", "k": ["surveillance", "capitalism", "economic", "system", "described", "shoshana", "zuboff", "extract", "commodify", "human", "behavioral", "data", "scale", "raising", "concerns"]}, {"id": "term-survival-analysis", "t": "Survival Analysis", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A branch of statistics dealing with the analysis of time-to-event data, accounting for censored observations. Key...", "l": "s", "k": ["survival", "analysis", "branch", "statistics", "dealing", "time-to-event", "data", "accounting", "censored", "observations", "key", "methods", "include", "kaplan-meier", "estimation"]}, {"id": "term-survivorship-bias", "t": "Survivorship Bias", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A form of selection bias that occurs when analysis is conducted only on subjects that passed a selection process,...", "l": "s", "k": ["survivorship", "bias", "form", "selection", "occurs", "analysis", "conducted", "subjects", "passed", "process", "ignoring", "leads", "overly", "optimistic", "conclusions"]}, {"id": "term-surya-ocr", "t": "Surya OCR", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "An open-source multilingual OCR model that handles text detection and recognition and line ordering across over 90...", "l": "s", "k": ["surya", "ocr", "open-source", "multilingual", "model", "handles", "text", "detection", "recognition", "line", "ordering", "across", "languages", "high", "accuracy"]}, {"id": "term-svd-algorithm", "t": "SVD Algorithm", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "Singular Value Decomposition factors a matrix into three matrices: a left orthogonal matrix and a diagonal matrix of...", "l": "s", "k": ["svd", "algorithm", "singular", "value", "decomposition", "factors", "matrix", "matrices", "left", "orthogonal", "diagonal", "values", "right", "fundamental", "dimensionality"]}, {"id": "term-svd-xt", "t": "SVD-XT", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "An extended version of Stable Video Diffusion that generates longer video sequences with improved temporal consistency...", "l": "s", "k": ["svd-xt", "extended", "version", "stable", "video", "diffusion", "generates", "longer", "sequences", "improved", "temporal", "consistency", "additional", "convolution", "layers"]}, {"id": "term-svhn", "t": "SVHN", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "The Street View House Numbers dataset containing over 600000 digit images obtained from house numbers in Google Street...", "l": "s", "k": ["svhn", "street", "view", "house", "numbers", "dataset", "containing", "digit", "images", "obtained", "google", "imagery", "challenging", "mnist", "due"]}, {"id": "term-svm-model", "t": "SVM Model", "tg": ["Models", "Fundamentals", "History"], "d": "models", "x": "A supervised learning model that finds the optimal hyperplane separating classes by maximizing the margin between...", "l": "s", "k": ["svm", "model", "supervised", "learning", "finds", "optimal", "hyperplane", "separating", "classes", "maximizing", "margin", "support", "vectors", "feature", "space"]}, {"id": "term-swarm-intelligence", "t": "Swarm Intelligence", "tg": ["History", "Milestones"], "d": "history", "x": "The collective intelligent behavior emerging from decentralized, self-organized systems such as ant colonies or bird...", "l": "s", "k": ["swarm", "intelligence", "collective", "intelligent", "behavior", "emerging", "decentralized", "self-organized", "systems", "ant", "colonies", "bird", "flocks", "formalized", "computationally"]}, {"id": "term-swe-bench", "t": "SWE-bench", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "Software Engineering Bench a benchmark of real-world GitHub issues paired with their resolutions. Tests the ability of...", "l": "s", "k": ["swe-bench", "software", "engineering", "bench", "benchmark", "real-world", "github", "issues", "paired", "resolutions", "tests", "ability", "systems", "autonomously", "resolve"]}, {"id": "term-swe-bench-verified", "t": "SWE-bench Verified", "tg": ["Benchmark", "Code"], "d": "datasets", "x": "A human-validated subset of SWE-bench where each task has been verified by software engineers. Provides more reliable...", "l": "s", "k": ["swe-bench", "verified", "human-validated", "subset", "task", "software", "engineers", "provides", "reliable", "evaluation", "autonomous", "coding", "agent", "capabilities"]}, {"id": "term-swiglu", "t": "SwiGLU", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A gated linear unit variant that uses the Swish activation function for the gating mechanism, providing improved...", "l": "s", "k": ["swiglu", "gated", "linear", "unit", "variant", "uses", "swish", "activation", "function", "gating", "mechanism", "providing", "improved", "performance", "transformer"]}, {"id": "term-swin-transformer", "t": "Swin Transformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A hierarchical vision transformer that computes self-attention within non-overlapping local windows and shifts windows...", "l": "s", "k": ["swin", "transformer", "hierarchical", "vision", "computes", "self-attention", "within", "non-overlapping", "local", "windows", "shifts", "layers", "achieving", "linear", "computational"]}, {"id": "term-swinvrn", "t": "SwinVRN", "tg": ["Models", "Scientific"], "d": "models", "x": "A weather forecasting model based on the Swin Transformer architecture that predicts atmospheric variables on a...", "l": "s", "k": ["swinvrn", "weather", "forecasting", "model", "based", "swin", "transformer", "architecture", "predicts", "atmospheric", "variables", "volumetric", "grid", "medium-range"]}, {"id": "term-swish", "t": "Swish", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An activation function defined as f(x) = x * sigmoid(beta * x) discovered through automated search by Google Brain in...", "l": "s", "k": ["swish", "activation", "function", "defined", "sigmoid", "beta", "discovered", "automated", "search", "google", "brain", "empirically", "outperforms", "relu", "deeper"]}, {"id": "term-swish-activation", "t": "Swish Activation", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A smooth, non-monotonic activation function defined as x times sigmoid of x, which often outperforms ReLU in deep...", "l": "s", "k": ["swish", "activation", "smooth", "non-monotonic", "function", "defined", "times", "sigmoid", "outperforms", "relu", "deep", "networks", "allowing", "small", "negative"]}, {"id": "term-switch-transformer", "t": "Switch Transformer", "tg": ["Models", "Technical"], "d": "models", "x": "A mixture-of-experts model that routes each token to a single expert using a simplified top-1 routing mechanism....", "l": "s", "k": ["switch", "transformer", "mixture-of-experts", "model", "routes", "token", "single", "expert", "simplified", "top-1", "routing", "mechanism", "dramatically", "increases", "capacity"]}, {"id": "term-switchboard", "t": "Switchboard", "tg": ["Training Corpus", "Speech"], "d": "datasets", "x": "A collection of approximately 2400 two-sided telephone conversations among 543 speakers. A foundational dataset for...", "l": "s", "k": ["switchboard", "collection", "approximately", "two-sided", "telephone", "conversations", "among", "speakers", "foundational", "dataset", "conversational", "speech", "recognition", "dialogue", "research"]}, {"id": "term-sycophancy", "t": "Sycophancy", "tg": ["Limitation", "Alignment"], "d": "safety", "x": "When AI excessively agrees with users or tells them what they want to hear rather than providing accurate information....", "l": "s", "k": ["sycophancy", "excessively", "agrees", "users", "tells", "want", "hear", "rather", "providing", "accurate", "information", "form", "misalignment", "undermines", "helpfulness"]}, {"id": "term-symbol-grounding-problem", "t": "Symbol Grounding Problem", "tg": ["History", "Milestones"], "d": "history", "x": "The problem identified by Stevan Harnad in 1990 of how symbols in a formal system can acquire meaning, questioning...", "l": "s", "k": ["symbol", "grounding", "problem", "identified", "stevan", "harnad", "symbols", "formal", "system", "acquire", "meaning", "questioning", "systems", "manipulate", "without"]}, {"id": "term-symbolic-ai", "t": "Symbolic AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "An approach to AI that represents knowledge using human-readable symbols and manipulates them according to rules of...", "l": "s", "k": ["symbolic", "approach", "represents", "knowledge", "human-readable", "symbols", "manipulates", "according", "rules", "logic", "dominant", "1950s", "1980s", "encompasses", "expert"]}, {"id": "term-symbolic-differentiation", "t": "Symbolic Differentiation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Computing derivatives by algebraically manipulating mathematical expressions according to differentiation rules....", "l": "s", "k": ["symbolic", "differentiation", "computing", "derivatives", "algebraically", "manipulating", "mathematical", "expressions", "according", "rules", "produces", "exact", "closed-form", "lead", "expression"]}, {"id": "term-symbolic-numeric-computation", "t": "Symbolic-Numeric Computation", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "Algorithms that combine exact symbolic manipulation with approximate numerical methods to solve mathematical problems....", "l": "s", "k": ["symbolic-numeric", "computation", "algorithms", "combine", "exact", "symbolic", "manipulation", "approximate", "numerical", "methods", "solve", "mathematical", "problems", "bridges", "computer"]}, {"id": "term-symbolics", "t": "Symbolics", "tg": ["Historical", "AI", "Company"], "d": "hardware", "x": "Company that manufactured Lisp machines in the 1980s and registered the first dot-com domain name. Represented the...", "l": "s", "k": ["symbolics", "company", "manufactured", "lisp", "machines", "1980s", "registered", "dot-com", "domain", "name", "represented", "commercial", "peak", "specialized", "hardware"]}, {"id": "term-syncdreamer", "t": "SyncDreamer", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A multi-view image diffusion model that generates consistent multi-view images of an object from a single input view...", "l": "s", "k": ["syncdreamer", "multi-view", "image", "diffusion", "model", "generates", "consistent", "images", "object", "single", "input", "view", "reconstruction"]}, {"id": "term-synchronous-sgd", "t": "Synchronous SGD", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A distributed training approach where all workers must complete their gradient computation before a synchronized...", "l": "s", "k": ["synchronous", "sgd", "distributed", "training", "approach", "workers", "must", "complete", "gradient", "computation", "synchronized", "all-reduce", "weight", "update", "provides"]}, {"id": "term-synopsys", "t": "Synopsys", "tg": ["Manufacturing", "EDA", "Company"], "d": "hardware", "x": "Leading electronic design automation company providing software tools for designing semiconductor chips. Their tools...", "l": "s", "k": ["synopsys", "leading", "electronic", "design", "automation", "company", "providing", "software", "tools", "designing", "semiconductor", "chips", "virtually", "modern", "accelerator"]}, {"id": "term-synset", "t": "Synset", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A set of synonymous words or phrases in WordNet that represent a single concept, serving as the basic unit of meaning...", "l": "s", "k": ["synset", "synonymous", "words", "phrases", "wordnet", "represent", "single", "concept", "serving", "basic", "unit", "meaning", "lexical", "database"]}, {"id": "term-syntax", "t": "Syntax", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The branch of linguistics concerning the rules and principles governing the structure of sentences, including word...", "l": "s", "k": ["syntax", "branch", "linguistics", "concerning", "rules", "principles", "governing", "structure", "sentences", "including", "word", "order", "phrase", "grammatical", "relations"]}, {"id": "term-synthetic-control-method", "t": "Synthetic Control Method", "tg": ["Algorithms", "Technical", "Causal"], "d": "algorithms", "x": "A causal inference technique that constructs a weighted combination of control units to approximate the treated unit...", "l": "s", "k": ["synthetic", "control", "method", "causal", "inference", "technique", "constructs", "weighted", "combination", "units", "approximate", "treated", "unit", "intervention", "difference"]}, {"id": "term-synthetic-data", "t": "Synthetic Data", "tg": ["Data", "Training"], "d": "general", "x": "Artificially generated data used for training when real data is scarce, expensive, or privacy-sensitive. Increasingly...", "l": "s", "k": ["synthetic", "data", "artificially", "generated", "training", "real", "scarce", "expensive", "privacy-sensitive", "increasingly", "train", "evaluate", "models"]}, {"id": "term-synthetic-data-generation", "t": "Synthetic Data Generation", "tg": ["Generative AI", "LLM"], "d": "models", "x": "The use of AI models to create artificial training data that mimics real-world data distributions, used to augment...", "l": "s", "k": ["synthetic", "data", "generation", "models", "create", "artificial", "training", "mimics", "real-world", "distributions", "augment", "datasets", "address", "privacy", "concerns"]}, {"id": "term-synthetic-data-vault", "t": "Synthetic Data Vault", "tg": ["Synthetic", "Platform"], "d": "datasets", "x": "A system for generating synthetic tabular data that preserves the statistical properties of real datasets. Used for...", "l": "s", "k": ["synthetic", "data", "vault", "system", "generating", "tabular", "preserves", "statistical", "properties", "real", "datasets", "privacy-preserving", "sharing", "augmentation"]}, {"id": "term-synthetic-media", "t": "Synthetic Media", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "Media content including images, video, audio, and text that is generated or substantially modified by AI systems,...", "l": "s", "k": ["synthetic", "media", "content", "including", "images", "video", "audio", "text", "generated", "substantially", "modified", "systems", "encompassing", "deepfakes", "ai-generated"]}, {"id": "term-synthetic-text-to-sql", "t": "Synthetic Text-to-SQL", "tg": ["Training Corpus", "Code", "Synthetic"], "d": "datasets", "x": "Synthetic datasets of natural language to SQL pairs generated by LLMs for training text-to-SQL models. Augments...", "l": "s", "k": ["synthetic", "text-to-sql", "datasets", "natural", "language", "sql", "pairs", "generated", "llms", "training", "models", "augments", "human-annotated", "data", "diverse"]}, {"id": "term-system-2-attention", "t": "System 2 Attention", "tg": ["Prompt Engineering", "Attention"], "d": "algorithms", "x": "A prompting technique that first asks the model to rewrite the input by removing irrelevant or opinion-laden context,...", "l": "s", "k": ["system", "attention", "prompting", "technique", "asks", "model", "rewrite", "input", "removing", "irrelevant", "opinion-laden", "context", "answers", "based", "cleaned"]}, {"id": "term-system-message-design", "t": "System Message Design", "tg": ["Prompt Engineering", "Architecture"], "d": "models", "x": "The practice of crafting the system-level prompt that establishes a language model's identity, behavior boundaries,...", "l": "s", "k": ["system", "message", "design", "practice", "crafting", "system-level", "prompt", "establishes", "language", "model", "identity", "behavior", "boundaries", "output", "format"]}, {"id": "term-system-prompt", "t": "System Prompt", "tg": ["Prompting", "Configuration"], "d": "general", "x": "Instructions given to AI before a conversation that set context, persona, or behavior guidelines. Shapes all subsequent...", "l": "s", "k": ["system", "prompt", "instructions", "given", "conversation", "context", "persona", "behavior", "guidelines", "shapes", "subsequent", "responses", "defines", "personality", "session"]}, {"id": "term-system-on-chip", "t": "System-on-Chip", "tg": ["Architecture", "Integration", "Design"], "d": "hardware", "x": "Integrated circuit combining processor memory interfaces and other components on a single chip. Apple M-series and...", "l": "s", "k": ["system-on-chip", "integrated", "circuit", "combining", "processor", "memory", "interfaces", "components", "single", "chip", "apple", "m-series", "qualcomm", "snapdragon", "socs"]}, {"id": "term-systemic-bias", "t": "Systemic Bias", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Bias that is embedded in and perpetuated by institutional processes social structures and historical patterns. AI...", "l": "s", "k": ["systemic", "bias", "embedded", "perpetuated", "institutional", "processes", "social", "structures", "historical", "patterns", "systems", "inherit", "amplify", "biases", "training"]}, {"id": "term-systemic-risk-from-ai", "t": "Systemic Risk from AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "The risk that AI systems pose to the stability and functioning of larger social economic or technical systems. Arises...", "l": "s", "k": ["systemic", "risk", "systems", "pose", "stability", "functioning", "larger", "social", "economic", "technical", "arises", "interdependencies", "concentration", "capabilities", "correlated"]}, {"id": "term-systolic-array", "t": "Systolic Array", "tg": ["Architecture", "Accelerator", "Fundamentals"], "d": "hardware", "x": "Regular array of processing elements that rhythmically pass data between neighbors performing multiply-accumulate...", "l": "s", "k": ["systolic", "array", "regular", "processing", "elements", "rhythmically", "pass", "data", "neighbors", "performing", "multiply-accumulate", "operations", "core", "compute", "architecture"]}, {"id": "term-t-sne", "t": "t-SNE", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "A nonlinear dimensionality reduction technique that maps high-dimensional data to two or three dimensions for...", "l": "t", "k": ["t-sne", "nonlinear", "dimensionality", "reduction", "technique", "maps", "high-dimensional", "data", "dimensions", "visualization", "modeling", "pairwise", "similarities", "probability", "distributions"]}, {"id": "term-t-sne-algorithm", "t": "t-SNE Algorithm", "tg": ["Algorithms", "Fundamentals", "Dimensionality Reduction"], "d": "algorithms", "x": "t-distributed Stochastic Neighbor Embedding is a nonlinear dimensionality reduction technique for visualizing...", "l": "t", "k": ["t-sne", "algorithm", "t-distributed", "stochastic", "neighbor", "embedding", "nonlinear", "dimensionality", "reduction", "technique", "visualizing", "high-dimensional", "data", "converts", "pairwise"]}, {"id": "term-t2i-compbench", "t": "T2I-CompBench", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating compositional text-to-image generation. Tests the ability to generate images with correct...", "l": "t", "k": ["t2i-compbench", "benchmark", "evaluating", "compositional", "text-to-image", "generation", "tests", "ability", "generate", "images", "correct", "attributes", "spatial", "relationships", "complex"]}, {"id": "term-t5", "t": "T5", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Text-to-Text Transfer Transformer, a model by Google that frames all NLP tasks as text-to-text problems, using an...", "l": "t", "k": ["text-to-text", "transfer", "transformer", "model", "google", "frames", "nlp", "tasks", "problems", "encoder-decoder", "architecture", "trained", "multi-task", "mixture"]}, {"id": "term-t5-text-to-text-transfer-transformer", "t": "T5 (Text-to-Text Transfer Transformer)", "tg": ["History", "Systems"], "d": "history", "x": "A language model developed by Google Research in 2019 that frames all NLP tasks as text-to-text problems. T5...", "l": "t", "k": ["text-to-text", "transfer", "transformer", "language", "model", "developed", "google", "research", "frames", "nlp", "tasks", "problems", "demonstrated", "unified", "approach"]}, {"id": "term-tabby", "t": "Tabby", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An open-source AI coding assistant model designed for self-hosted code completion that supports multiple programming...", "l": "t", "k": ["tabby", "open-source", "coding", "assistant", "model", "designed", "self-hosted", "code", "completion", "supports", "multiple", "programming", "languages", "ide", "integrations"]}, {"id": "term-tabfact", "t": "TabFact", "tg": ["Benchmark", "NLP", "Tabular"], "d": "datasets", "x": "A table fact verification dataset containing 118000 statements about Wikipedia tables labeled as entailed or refuted....", "l": "t", "k": ["tabfact", "table", "fact", "verification", "dataset", "containing", "statements", "wikipedia", "tables", "labeled", "entailed", "refuted", "tests", "ability", "reason"]}, {"id": "term-table-extraction", "t": "Table Extraction", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of detecting tables in document images and extracting their structure (rows, columns, cells) and content into...", "l": "t", "k": ["table", "extraction", "task", "detecting", "tables", "document", "images", "extracting", "structure", "rows", "columns", "cells", "content", "machine-readable", "format"]}, {"id": "term-table-transformer", "t": "Table Transformer", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A model based on DETR for detecting and recognizing table structures in document images including rows and columns and...", "l": "t", "k": ["table", "transformer", "model", "based", "detr", "detecting", "recognizing", "structures", "document", "images", "including", "rows", "columns", "cell", "boundaries"]}, {"id": "term-tableformer", "t": "TableFormer", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A Transformer-based model for table structure recognition that detects and reconstructs table layouts from document...", "l": "t", "k": ["tableformer", "transformer-based", "model", "table", "structure", "recognition", "detects", "reconstructs", "layouts", "document", "images", "structured", "data", "extraction"]}, {"id": "term-tabnet", "t": "TabNet", "tg": ["Models", "Technical"], "d": "models", "x": "A deep learning architecture for tabular data that uses sequential attention to select features at each decision step....", "l": "t", "k": ["tabnet", "deep", "learning", "architecture", "tabular", "data", "uses", "sequential", "attention", "select", "features", "decision", "step", "provides", "instance-wise"]}, {"id": "term-tabnet-benchmark", "t": "TabNet Benchmark", "tg": ["Benchmark", "Tabular"], "d": "datasets", "x": "A collection of tabular datasets used to evaluate deep learning methods for structured data. Tests whether neural...", "l": "t", "k": ["tabnet", "benchmark", "collection", "tabular", "datasets", "evaluate", "deep", "learning", "methods", "structured", "data", "tests", "neural", "networks", "match"]}, {"id": "term-tabu-search", "t": "Tabu Search", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A metaheuristic optimization method that enhances local search by maintaining a memory of recently visited solutions to...", "l": "t", "k": ["tabu", "search", "metaheuristic", "optimization", "method", "enhances", "local", "maintaining", "memory", "recently", "visited", "solutions", "avoid", "cycling", "uses"]}, {"id": "term-tabular-chain-of-thought", "t": "Tabular Chain-of-Thought", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting variant that formats intermediate reasoning steps as structured tables rather than free-form text,...", "l": "t", "k": ["tabular", "chain-of-thought", "prompting", "variant", "formats", "intermediate", "reasoning", "steps", "structured", "tables", "rather", "free-form", "text", "improving", "clarity"]}, {"id": "term-tacotron-2", "t": "Tacotron 2", "tg": ["Models", "Technical", "Audio", "History"], "d": "models", "x": "A neural network architecture from Google for speech synthesis that combines a sequence-to-sequence model with a...", "l": "t", "k": ["tacotron", "neural", "network", "architecture", "google", "speech", "synthesis", "combines", "sequence-to-sequence", "model", "modified", "wavenet", "vocoder"]}, {"id": "term-tango", "t": "Tango", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A text-to-audio generation model that uses a latent diffusion approach with instruction-tuned language model...", "l": "t", "k": ["tango", "text-to-audio", "generation", "model", "uses", "latent", "diffusion", "approach", "instruction-tuned", "language", "conditioning", "high-quality", "sound", "synthesis"]}, {"id": "term-tango-2", "t": "Tango 2", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "An improved text-to-audio model that uses direct preference optimization to align generated audio more closely with...", "l": "t", "k": ["tango", "improved", "text-to-audio", "model", "uses", "direct", "preference", "optimization", "align", "generated", "audio", "closely", "human", "preferences", "text"]}, {"id": "term-tanh", "t": "Tanh", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "Hyperbolic tangent activation function that maps inputs to values between -1 and 1. Defined as f(x) = (exp(x) -...", "l": "t", "k": ["tanh", "hyperbolic", "tangent", "activation", "function", "maps", "inputs", "values", "defined", "exp", "provides", "zero-centered", "outputs", "improve", "convergence"]}, {"id": "term-tape-out", "t": "Tape-Out", "tg": ["Manufacturing", "Process", "Milestone"], "d": "hardware", "x": "Final step in chip design where the completed design is sent to a foundry for manufacturing. Named after the historical...", "l": "t", "k": ["tape-out", "final", "step", "chip", "design", "completed", "sent", "foundry", "manufacturing", "named", "historical", "practice", "sending", "designs", "magnetic"]}, {"id": "term-tapir", "t": "TAPIR", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "Tracking Any Point with per-frame Initialization and temporal Refinement is a model for dense point tracking across...", "l": "t", "k": ["tapir", "tracking", "point", "per-frame", "initialization", "temporal", "refinement", "model", "dense", "across", "video", "frames"]}, {"id": "term-target-encoding", "t": "Target Encoding", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A feature encoding method that replaces each categorical value with the mean of the target variable for that category,...", "l": "t", "k": ["target", "encoding", "feature", "method", "replaces", "categorical", "value", "mean", "variable", "category", "combined", "smoothing", "cross-validation", "prevent", "overfitting"]}, {"id": "term-target-network", "t": "Target Network", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A slowly updated copy of the value network used to compute stable TD targets in deep RL algorithms like DQN. Target...", "l": "t", "k": ["target", "network", "slowly", "updated", "copy", "value", "compute", "stable", "targets", "deep", "algorithms", "dqn", "networks", "reduce", "oscillation"]}, {"id": "term-targeted-learning-algorithm", "t": "Targeted Learning Algorithm", "tg": ["Algorithms", "Technical", "Causal"], "d": "algorithms", "x": "A semiparametric estimation framework that combines machine learning with statistical theory to estimate causal...", "l": "t", "k": ["targeted", "learning", "algorithm", "semiparametric", "estimation", "framework", "combines", "machine", "statistical", "theory", "estimate", "causal", "parameters", "uses", "minimum"]}, {"id": "term-tarjans-algorithm", "t": "Tarjan's Algorithm", "tg": ["Algorithms", "Fundamentals", "Graph"], "d": "algorithms", "x": "A depth-first search based algorithm that finds all strongly connected components in a directed graph in linear time....", "l": "t", "k": ["tarjan", "algorithm", "depth-first", "search", "based", "finds", "strongly", "connected", "components", "directed", "graph", "linear", "time", "uses", "stack"]}, {"id": "term-taskmatrix-benchmark", "t": "TaskMatrix Benchmark", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A benchmark evaluating the ability of AI systems to use visual foundation models as tools for complex visual tasks....", "l": "t", "k": ["taskmatrix", "benchmark", "evaluating", "ability", "systems", "visual", "foundation", "models", "tools", "complex", "tasks", "tests", "multimodal", "tool-use", "capabilities"]}, {"id": "term-taskonomy", "t": "Taskonomy", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A dataset of 4 million images from 600 buildings with annotations for 26 visual tasks. Used to study relationships...", "l": "t", "k": ["taskonomy", "dataset", "million", "images", "buildings", "annotations", "visual", "tasks", "study", "relationships", "transfer", "learning", "efficiency"]}, {"id": "term-tatoeba", "t": "Tatoeba", "tg": ["Benchmark", "NLP", "Translation", "Multilingual"], "d": "datasets", "x": "A collection of sentences and translations in over 400 languages maintained by volunteers. Used as a multilingual...", "l": "t", "k": ["tatoeba", "collection", "sentences", "translations", "languages", "maintained", "volunteers", "multilingual", "sentence", "similarity", "translation", "evaluation", "benchmark"]}, {"id": "term-taxi1500", "t": "Taxi1500", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "A text classification benchmark covering 1500 languages with Bible verse topic classification. One of the most...", "l": "t", "k": ["taxi1500", "text", "classification", "benchmark", "covering", "languages", "bible", "verse", "topic", "linguistically", "diverse", "nlp", "evaluation", "resources", "available"]}, {"id": "term-td-lambda", "t": "TD(lambda)", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A temporal difference algorithm that blends multi-step returns using an exponentially-weighted average controlled by...", "l": "t", "k": ["lambda", "temporal", "difference", "algorithm", "blends", "multi-step", "returns", "exponentially-weighted", "average", "controlled", "parameter", "unifies", "monte", "carlo", "methods"]}, {"id": "term-td-gammon", "t": "TD-Gammon", "tg": ["History", "Systems"], "d": "history", "x": "A backgammon-playing program developed by Gerald Tesauro at IBM in 1992 using temporal difference learning. TD-Gammon...", "l": "t", "k": ["td-gammon", "backgammon-playing", "program", "developed", "gerald", "tesauro", "ibm", "temporal", "difference", "learning", "achieved", "expert-level", "play", "self-play", "reinforcement"]}, {"id": "term-teacher-forcing", "t": "Teacher Forcing", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A training strategy for sequence models that uses ground truth tokens as input at each step rather than the model's own...", "l": "t", "k": ["teacher", "forcing", "training", "strategy", "sequence", "models", "uses", "ground", "truth", "tokens", "input", "step", "rather", "model", "predictions"]}, {"id": "term-teacher-student-framework", "t": "Teacher-Student Framework", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A model compression paradigm where a large pretrained teacher model guides the training of a smaller student model by...", "l": "t", "k": ["teacher-student", "framework", "model", "compression", "paradigm", "large", "pretrained", "teacher", "guides", "training", "smaller", "student", "providing", "soft", "targets"]}, {"id": "term-teaching-learning-based-optimization", "t": "Teaching-Learning-Based Optimization", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A metaheuristic that simulates the teaching-learning process in a classroom. The teacher phase moves learners toward...", "l": "t", "k": ["teaching-learning-based", "optimization", "metaheuristic", "simulates", "teaching-learning", "process", "classroom", "teacher", "phase", "moves", "learners", "toward", "best", "solution", "learner"]}, {"id": "term-technical-debt-in-ai", "t": "Technical Debt in AI", "tg": ["Safety", "Technical"], "d": "safety", "x": "The accumulated cost of shortcuts and suboptimal decisions in AI system development that must eventually be addressed....", "l": "t", "k": ["technical", "debt", "accumulated", "cost", "shortcuts", "suboptimal", "decisions", "system", "development", "must", "eventually", "addressed", "systems", "includes", "data"]}, {"id": "term-technical-prompting", "t": "Technical Prompting", "tg": ["Prompt Engineering", "Technical"], "d": "general", "x": "The practice of crafting prompts that incorporate precise technical specifications, domain terminology, and structured...", "l": "t", "k": ["technical", "prompting", "practice", "crafting", "prompts", "incorporate", "precise", "specifications", "domain", "terminology", "structured", "requirements", "generate", "accurate", "documentation"]}, {"id": "term-technological-singularity", "t": "Technological Singularity", "tg": ["History", "Fundamentals"], "d": "history", "x": "A hypothetical future point when technological growth becomes uncontrollable and irreversible resulting in...", "l": "t", "k": ["technological", "singularity", "hypothetical", "future", "point", "growth", "becomes", "uncontrollable", "irreversible", "resulting", "unforeseeable", "changes", "human", "civilization", "popularized"]}, {"id": "term-technological-unemployment", "t": "Technological Unemployment", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "Unemployment caused by technological advances outpacing the economy's ability to create new jobs, a longstanding...", "l": "t", "k": ["technological", "unemployment", "caused", "advances", "outpacing", "economy", "ability", "create", "jobs", "longstanding", "concern", "significantly", "amplified", "rapid", "advancement"]}, {"id": "term-technology-assessment-for-ai", "t": "Technology Assessment for AI", "tg": ["Safety", "Governance"], "d": "safety", "x": "A systematic multidisciplinary evaluation of the societal implications of AI technology. Informs policy decisions by...", "l": "t", "k": ["technology", "assessment", "systematic", "multidisciplinary", "evaluation", "societal", "implications", "informs", "policy", "decisions", "examining", "potential", "benefits", "risks", "alternatives"]}, {"id": "term-telechat", "t": "TeleChat", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A large language model from China Telecom designed for enterprise telecommunications applications with domain-specific...", "l": "t", "k": ["telechat", "large", "language", "model", "china", "telecom", "designed", "enterprise", "telecommunications", "applications", "domain-specific", "training", "data"]}, {"id": "term-temperature", "t": "Temperature", "tg": ["Parameter", "Generation"], "d": "general", "x": "A parameter controlling randomness in AI outputs. Temperature 0 gives deterministic responses; higher values (0.7-1.0)...", "l": "t", "k": ["temperature", "parameter", "controlling", "randomness", "outputs", "gives", "deterministic", "responses", "higher", "values", "7-1", "increase", "creativity", "variety", "high"]}, {"id": "term-temperature-scaling", "t": "Temperature Scaling", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A technique that adjusts the softmax distribution over token probabilities by dividing logits by a temperature...", "l": "t", "k": ["temperature", "scaling", "technique", "adjusts", "softmax", "distribution", "token", "probabilities", "dividing", "logits", "parameter", "lower", "temperatures", "sharpen", "toward"]}, {"id": "term-template-matching-algorithm", "t": "Template Matching Algorithm", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "A technique for finding regions in an image that match a template image by sliding the template across the image and...", "l": "t", "k": ["template", "matching", "algorithm", "technique", "finding", "regions", "image", "match", "sliding", "across", "computing", "similarity", "measure", "common", "metrics"]}, {"id": "term-temporal-action-detection", "t": "Temporal Action Detection", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of identifying the start time, end time, and category of each action instance in an untrimmed video, requiring...", "l": "t", "k": ["temporal", "action", "detection", "task", "identifying", "start", "time", "end", "category", "instance", "untrimmed", "video", "requiring", "localization", "activity"]}, {"id": "term-temporal-coherence", "t": "Temporal Coherence", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The consistency of visual elements across consecutive frames in generated or processed video, ensuring smooth motion,...", "l": "t", "k": ["temporal", "coherence", "consistency", "visual", "elements", "across", "consecutive", "frames", "generated", "processed", "video", "ensuring", "smooth", "motion", "stable"]}, {"id": "term-temporal-credit-assignment", "t": "Temporal Credit Assignment", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The specific aspect of credit assignment concerned with distributing reward information backward through time to...", "l": "t", "k": ["temporal", "credit", "assignment", "specific", "aspect", "concerned", "distributing", "reward", "information", "backward", "time", "earlier", "actions", "contributed", "outcome"]}, {"id": "term-temporal-difference-learning", "t": "Temporal Difference Learning", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A family of RL methods that update value estimates based on the difference between successive predictions, combining...", "l": "t", "k": ["temporal", "difference", "learning", "family", "methods", "update", "value", "estimates", "based", "successive", "predictions", "combining", "ideas", "monte", "carlo"]}, {"id": "term-temporal-reasoning-in-ai", "t": "Temporal Reasoning in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "The area of AI concerned with representing and reasoning about time and temporal relationships between events. Includes...", "l": "t", "k": ["temporal", "reasoning", "area", "concerned", "representing", "time", "relationships", "events", "includes", "approaches", "allen", "interval", "algebra", "various", "logics"]}, {"id": "term-tensor-cores", "t": "Tensor Cores", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "Specialized matrix multiply-and-accumulate units in NVIDIA GPUs that accelerate mixed-precision matrix operations...", "l": "t", "k": ["tensor", "cores", "specialized", "matrix", "multiply-and-accumulate", "units", "nvidia", "gpus", "accelerate", "mixed-precision", "operations", "fundamental", "deep", "learning", "perform"]}, {"id": "term-tensor-decomposition", "t": "Tensor Decomposition", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A generalization of matrix decomposition to higher-order arrays that factors tensors into sums of simpler components....", "l": "t", "k": ["tensor", "decomposition", "generalization", "matrix", "higher-order", "arrays", "factors", "tensors", "sums", "simpler", "components", "includes", "tucker", "data", "analysis"]}, {"id": "term-tensor-parallelism", "t": "Tensor Parallelism", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A form of model parallelism that splits individual weight matrices across multiple devices, distributing the...", "l": "t", "k": ["tensor", "parallelism", "form", "model", "splits", "individual", "weight", "matrices", "across", "multiple", "devices", "distributing", "computation", "layer", "requiring"]}, {"id": "term-tensor-processing-unit-v5e", "t": "Tensor Processing Unit v5e", "tg": ["TPU", "Google", "Inference"], "d": "hardware", "x": "Google cost-optimized TPU variant designed for efficient AI inference and smaller-scale training workloads. Offers...", "l": "t", "k": ["tensor", "processing", "unit", "v5e", "google", "cost-optimized", "tpu", "variant", "designed", "efficient", "inference", "smaller-scale", "training", "workloads", "offers"]}, {"id": "term-tensor-processing-unit-v5p", "t": "Tensor Processing Unit v5p", "tg": ["TPU", "Google", "Training"], "d": "hardware", "x": "Google highest-performance TPU variant designed for large-scale AI training. Provides maximum compute density and...", "l": "t", "k": ["tensor", "processing", "unit", "v5p", "google", "highest-performance", "tpu", "variant", "designed", "large-scale", "training", "provides", "maximum", "compute", "density"]}, {"id": "term-tensorflow", "t": "TensorFlow", "tg": ["Framework", "Deep Learning"], "d": "models", "x": "Google's open-source deep learning framework, widely used for production ML systems. Known for deployment tools and TPU...", "l": "t", "k": ["tensorflow", "google", "open-source", "deep", "learning", "framework", "widely", "production", "systems", "known", "deployment", "tools", "tpu", "support", "pytorch"]}, {"id": "term-tensorflow-datasets", "t": "TensorFlow Datasets", "tg": ["Platform", "General"], "d": "datasets", "x": "A collection of ready-to-use datasets for TensorFlow with standardized loading and preprocessing. Covers computer...", "l": "t", "k": ["tensorflow", "datasets", "collection", "ready-to-use", "standardized", "loading", "preprocessing", "covers", "computer", "vision", "nlp", "audio", "reinforcement", "learning", "tasks"]}, {"id": "term-tensorflow-release", "t": "TensorFlow Release", "tg": ["History", "Milestones"], "d": "history", "x": "The open-source release of TensorFlow by Google Brain in November 2015. TensorFlow provided a comprehensive framework...", "l": "t", "k": ["tensorflow", "release", "open-source", "google", "brain", "november", "provided", "comprehensive", "framework", "building", "training", "neural", "networks", "became", "widely"]}, {"id": "term-tensorrt", "t": "TensorRT", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "NVIDIA's high-performance inference optimization SDK that applies layer fusion, kernel auto-tuning, precision...", "l": "t", "k": ["tensorrt", "nvidia", "high-performance", "inference", "optimization", "sdk", "applies", "layer", "fusion", "kernel", "auto-tuning", "precision", "calibration", "dynamic", "tensor"]}, {"id": "term-tenstorrent", "t": "Tenstorrent", "tg": ["Accelerator", "Startup"], "d": "hardware", "x": "AI hardware startup founded by Jim Keller designing RISC-V based AI accelerator chips. Their Wormhole and Grayskull...", "l": "t", "k": ["tenstorrent", "hardware", "startup", "founded", "jim", "keller", "designing", "risc-v", "based", "accelerator", "chips", "wormhole", "grayskull", "processors", "mesh"]}, {"id": "term-ternary-search", "t": "Ternary Search", "tg": ["Algorithms", "Technical", "Searching"], "d": "algorithms", "x": "A divide-and-conquer search algorithm that splits the search space into three parts to find the maximum or minimum of a...", "l": "t", "k": ["ternary", "search", "divide-and-conquer", "algorithm", "splits", "space", "parts", "find", "maximum", "minimum", "unimodal", "function", "requires", "exactly", "peak"]}, {"id": "term-ternary-search-tree", "t": "Ternary Search Tree", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A type of trie that uses three-way branching (less than and equal to and greater than) at each node. Combines the space...", "l": "t", "k": ["ternary", "search", "tree", "type", "trie", "uses", "three-way", "branching", "less", "equal", "greater", "node", "combines", "space", "efficiency"]}, {"id": "term-terry-sejnowski", "t": "Terry Sejnowski", "tg": ["History", "Pioneers"], "d": "history", "x": "American computational neuroscientist who co-invented the Boltzmann machine with Geoffrey Hinton in 1985. President of...", "l": "t", "k": ["terry", "sejnowski", "american", "computational", "neuroscientist", "co-invented", "boltzmann", "machine", "geoffrey", "hinton", "president", "salk", "institute", "neurobiology", "laboratory"]}, {"id": "term-terry-winograd", "t": "Terry Winograd", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who created SHRDLU at MIT in 1970 and later became influential in human-computer...", "l": "t", "k": ["terry", "winograd", "american", "computer", "scientist", "created", "shrdlu", "mit", "later", "became", "influential", "human-computer", "interaction", "research", "stanford"]}, {"id": "term-tesla-fsd-neural-net", "t": "Tesla FSD Neural Net", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "The neural network architecture powering Tesla Full Self-Driving system that processes multi-camera video feeds for...", "l": "t", "k": ["tesla", "fsd", "neural", "net", "network", "architecture", "powering", "full", "self-driving", "system", "processes", "multi-camera", "video", "feeds", "autonomous"]}, {"id": "term-test-set", "t": "Test Set", "tg": ["Data", "Evaluation"], "d": "datasets", "x": "Data held back from training to evaluate final model performance. Unlike validation sets used during training, test...", "l": "t", "k": ["test", "data", "held", "training", "evaluate", "final", "model", "performance", "unlike", "validation", "sets", "avoid", "leakage"]}, {"id": "term-test-time-augmentation", "t": "Test-Time Augmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "An inference strategy that applies multiple augmentation transforms to a test image, runs predictions on each variant,...", "l": "t", "k": ["test-time", "augmentation", "inference", "strategy", "applies", "multiple", "transforms", "test", "image", "runs", "predictions", "variant", "aggregates", "results", "produce"]}, {"id": "term-text-classification", "t": "Text Classification", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of assigning predefined categories or labels to text documents based on their content, encompassing...", "l": "t", "k": ["text", "classification", "task", "assigning", "predefined", "categories", "labels", "documents", "based", "content", "encompassing", "applications", "topic", "categorization", "spam"]}, {"id": "term-text-deduplication", "t": "Text Deduplication", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The process of identifying and removing duplicate or near-duplicate documents from a text corpus, important for...", "l": "t", "k": ["text", "deduplication", "process", "identifying", "removing", "duplicate", "near-duplicate", "documents", "corpus", "important", "preventing", "data", "contamination", "training", "quality"]}, {"id": "term-text-detection", "t": "Text Detection", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of localizing text regions in natural scene images or documents, handling challenges like arbitrary...", "l": "t", "k": ["text", "detection", "task", "localizing", "regions", "natural", "scene", "images", "documents", "handling", "challenges", "arbitrary", "orientations", "curved", "varying"]}, {"id": "term-text-encoder-diffusion", "t": "Text Encoder", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A language model (such as CLIP or T5) used in diffusion models to convert text prompts into conditioning embeddings...", "l": "t", "k": ["text", "encoder", "language", "model", "clip", "diffusion", "models", "convert", "prompts", "conditioning", "embeddings", "guide", "image", "generation", "process"]}, {"id": "term-text-entailment-graph", "t": "Text Entailment Graph", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A directed graph where nodes represent text fragments and edges represent entailment relations, used to organize and...", "l": "t", "k": ["text", "entailment", "graph", "directed", "nodes", "represent", "fragments", "edges", "relations", "organize", "reason", "textual", "inference", "relationships", "knowledge"]}, {"id": "term-text-generation", "t": "Text Generation", "tg": ["Task", "Application"], "d": "general", "x": "The AI task of producing human-like text from prompts. Encompasses creative writing, code generation, summarization,...", "l": "t", "k": ["text", "generation", "task", "producing", "human-like", "prompts", "encompasses", "creative", "writing", "code", "summarization", "conversational", "responses"]}, {"id": "term-tgi", "t": "Text Generation Inference (TGI)", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Hugging Face's production-ready inference server optimized for text generation with large language models. TGI supports...", "l": "t", "k": ["text", "generation", "inference", "tgi", "hugging", "face", "production-ready", "server", "optimized", "large", "language", "models", "supports", "tensor", "parallelism"]}, {"id": "term-text-normalization", "t": "Text Normalization", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The process of transforming text into a canonical form by handling variations such as abbreviations, numbers, dates,...", "l": "t", "k": ["text", "normalization", "process", "transforming", "canonical", "form", "handling", "variations", "abbreviations", "numbers", "dates", "urls", "special", "characters", "standardized"]}, {"id": "term-text-span", "t": "Text Span", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A contiguous sequence of characters or tokens within a text, identified by start and end positions, commonly used to...", "l": "t", "k": ["text", "span", "contiguous", "sequence", "characters", "tokens", "within", "identified", "start", "end", "positions", "commonly", "mark", "entity", "mentions"]}, {"id": "term-text-embedding-ada-002", "t": "text-embedding-ada-002", "tg": ["Models", "Technical"], "d": "models", "x": "OpenAI's second generation text embedding model that produces 1536-dimensional vectors. Widely used for semantic search...", "l": "t", "k": ["text-embedding-ada-002", "openai", "generation", "text", "embedding", "model", "produces", "1536-dimensional", "vectors", "widely", "semantic", "search", "retrieval", "augmented", "clustering"]}, {"id": "term-text-to-speech", "t": "Text-to-Speech (TTS)", "tg": ["Application", "Audio"], "d": "general", "x": "AI that converts written text into natural-sounding speech. Modern TTS models like ElevenLabs produce highly realistic...", "l": "t", "k": ["text-to-speech", "tts", "converts", "written", "text", "natural-sounding", "speech", "modern", "models", "elevenlabs", "produce", "highly", "realistic", "voices", "emotion"]}, {"id": "term-text-to-sql", "t": "Text-to-SQL", "tg": ["NLP", "Parsing"], "d": "general", "x": "The task of translating natural language questions into executable SQL queries against a database, enabling...", "l": "t", "k": ["text-to-sql", "task", "translating", "natural", "language", "questions", "executable", "sql", "queries", "against", "database", "enabling", "non-technical", "users", "query"]}, {"id": "term-textcaps", "t": "TextCaps", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "A dataset for image captioning that requires reading and reasoning about text in images. Contains 28000 images with...", "l": "t", "k": ["textcaps", "dataset", "image", "captioning", "requires", "reading", "reasoning", "text", "images", "contains", "captions", "incorporate", "ocr", "visible", "scene"]}, {"id": "term-texthawk", "t": "TextHawk", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A vision-language model specifically designed for document-oriented understanding that uses fine-grained visual...", "l": "t", "k": ["texthawk", "vision-language", "model", "specifically", "designed", "document-oriented", "understanding", "uses", "fine-grained", "visual", "perception", "reading", "text-heavy", "images"]}, {"id": "term-textmonkey", "t": "TextMonkey", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A specialized vision-language model for text-rich image understanding that excels at document analysis and scene text...", "l": "t", "k": ["textmonkey", "specialized", "vision-language", "model", "text-rich", "image", "understanding", "excels", "document", "analysis", "scene", "text", "reading", "tasks"]}, {"id": "term-textrank", "t": "TextRank", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A graph-based ranking algorithm for NLP that applies PageRank-style computation to a graph of text units, used for...", "l": "t", "k": ["textrank", "graph-based", "ranking", "algorithm", "nlp", "applies", "pagerank-style", "computation", "graph", "text", "units", "keyword", "extraction", "extractive", "summarization"]}, {"id": "term-textrank-algorithm", "t": "TextRank Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP"], "d": "algorithms", "x": "A graph-based ranking algorithm for natural language processing that applies PageRank to text. Extracts keywords and...", "l": "t", "k": ["textrank", "algorithm", "graph-based", "ranking", "natural", "language", "processing", "applies", "pagerank", "text", "extracts", "keywords", "generates", "summaries", "building"]}, {"id": "term-textual-entailment", "t": "Textual Entailment", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of determining whether a hypothesis sentence can be logically inferred from a premise sentence, classified as...", "l": "t", "k": ["textual", "entailment", "task", "determining", "hypothesis", "sentence", "logically", "inferred", "premise", "classified", "contradiction", "neutral"]}, {"id": "term-textual-inversion", "t": "Textual Inversion", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A technique that learns a new text embedding to represent a specific visual concept from a few example images, enabling...", "l": "t", "k": ["textual", "inversion", "technique", "learns", "text", "embedding", "represent", "specific", "visual", "concept", "example", "images", "enabling", "personalized", "generation"]}, {"id": "term-texture-mapping", "t": "Texture Mapping", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The process of applying 2D image textures onto 3D surface meshes to add color, detail, and visual realism to...", "l": "t", "k": ["texture", "mapping", "process", "applying", "image", "textures", "onto", "surface", "meshes", "add", "color", "detail", "visual", "realism", "reconstructed"]}, {"id": "term-textvqa", "t": "TextVQA", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "A visual question answering dataset requiring models to read and reason about text present in images. Contains 45336...", "l": "t", "k": ["textvqa", "visual", "question", "answering", "dataset", "requiring", "models", "read", "reason", "text", "present", "images", "contains", "questions", "scene"]}, {"id": "term-tf-idf", "t": "TF-IDF", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "Term Frequency-Inverse Document Frequency, a numerical statistic that reflects the importance of a word in a document...", "l": "t", "k": ["tf-idf", "term", "frequency-inverse", "document", "frequency", "numerical", "statistic", "reflects", "importance", "word", "relative", "collection", "increases", "decreases", "across"]}, {"id": "term-tf-idf-algorithm", "t": "TF-IDF Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP"], "d": "algorithms", "x": "A numerical statistic that measures the importance of a word in a document relative to a collection. The term frequency...", "l": "t", "k": ["tf-idf", "algorithm", "numerical", "statistic", "measures", "importance", "word", "document", "relative", "collection", "term", "frequency", "weighted", "inverse", "reduce"]}, {"id": "term-tf32", "t": "TF32 (TensorFloat-32)", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "NVIDIA's 19-bit floating-point format that combines FP32's 8-bit exponent with a 10-bit mantissa, executed in Tensor...", "l": "t", "k": ["tf32", "tensorfloat-32", "nvidia", "19-bit", "floating-point", "format", "combines", "fp32", "8-bit", "exponent", "10-bit", "mantissa", "executed", "tensor", "cores"]}, {"id": "term-tfds", "t": "TFDS", "tg": ["Platform", "General"], "d": "datasets", "x": "TensorFlow Datasets a collection and API for loading datasets into TensorFlow pipelines. Provides deterministic dataset...", "l": "t", "k": ["tfds", "tensorflow", "datasets", "collection", "api", "loading", "pipelines", "provides", "deterministic", "dataset", "preparation", "standardized", "splits", "features"]}, {"id": "term-tflops", "t": "TFLOPS", "tg": ["Performance", "Metric", "Scale"], "d": "hardware", "x": "TeraFLOPS or one trillion floating-point operations per second. A standard measure for individual GPU performance with...", "l": "t", "k": ["tflops", "teraflops", "trillion", "floating-point", "operations", "per", "standard", "measure", "individual", "gpu", "performance", "modern", "gpus", "achieving", "hundreds"]}, {"id": "term-tft", "t": "TFT", "tg": ["Models", "Technical"], "d": "models", "x": "Temporal Fusion Transformer combines recurrent layers with multi-head attention and variable selection networks for...", "l": "t", "k": ["tft", "temporal", "fusion", "transformer", "combines", "recurrent", "layers", "multi-head", "attention", "variable", "selection", "networks", "interpretable", "multi-horizon", "time"]}, {"id": "term-the-pile", "t": "The Pile", "tg": ["History", "Milestones"], "d": "history", "x": "A large-scale diverse open-source language modeling dataset created by EleutherAI in 2020. The Pile consists of 825 GiB...", "l": "t", "k": ["pile", "large-scale", "diverse", "open-source", "language", "modeling", "dataset", "created", "eleutherai", "consists", "gib", "text", "high-quality", "sources", "designed"]}, {"id": "term-the-stack", "t": "The Stack", "tg": ["Training Corpus", "Code"], "d": "datasets", "x": "A 6.4TB dataset of permissively licensed source code in 358 programming languages. Created by BigCode for training code...", "l": "t", "k": ["stack", "4tb", "dataset", "permissively", "licensed", "source", "code", "programming", "languages", "created", "bigcode", "training", "generation", "models", "opt-out"]}, {"id": "term-the-stack-v2", "t": "The Stack V2", "tg": ["Training Corpus", "Code"], "d": "datasets", "x": "An expanded version of The Stack containing code from Software Heritage with improved licensing detection and...", "l": "t", "k": ["stack", "expanded", "version", "containing", "code", "software", "heritage", "improved", "licensing", "detection", "deduplication", "covers", "programming", "languages", "model"]}, {"id": "term-theano-framework", "t": "Theano Framework", "tg": ["History", "Milestones"], "d": "history", "x": "A Python library for numerical computation developed at Mila (University of Montreal) beginning in 2007. Theano...", "l": "t", "k": ["theano", "framework", "python", "library", "numerical", "computation", "developed", "mila", "university", "montreal", "beginning", "pioneered", "gpu-accelerated", "tensor", "operations"]}, {"id": "term-thematic-role", "t": "Thematic Role", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A semantic category describing the role an entity plays in relation to a predicate, including agent, patient, theme,...", "l": "t", "k": ["thematic", "role", "semantic", "category", "describing", "entity", "plays", "relation", "predicate", "including", "agent", "patient", "theme", "experiencer", "goal"]}, {"id": "term-theoremqa", "t": "TheoremQA", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A benchmark of 800 theorem-based questions spanning mathematics physics engineering and finance. Tests the ability to...", "l": "t", "k": ["theoremqa", "benchmark", "theorem-based", "questions", "spanning", "mathematics", "physics", "engineering", "finance", "tests", "ability", "apply", "scientific", "theorems", "solve"]}, {"id": "term-thepile-arxiv", "t": "ThePile Arxiv", "tg": ["Training Corpus", "NLP", "Scientific"], "d": "datasets", "x": "The Arxiv subset of The Pile containing scientific papers in LaTeX format. Provides technical scientific text for...", "l": "t", "k": ["thepile", "arxiv", "subset", "pile", "containing", "scientific", "papers", "latex", "format", "provides", "technical", "text", "pretraining", "language", "models"]}, {"id": "term-thermal-design-power", "t": "Thermal Design Power", "tg": ["Performance", "Power", "Specification"], "d": "hardware", "x": "Maximum amount of heat a processor generates under sustained workload measured in watts. Critical specification for...", "l": "t", "k": ["thermal", "design", "power", "maximum", "amount", "heat", "processor", "generates", "sustained", "workload", "measured", "watts", "critical", "specification", "designing"]}, {"id": "term-thermal-interface-material", "t": "Thermal Interface Material", "tg": ["Cooling", "Component", "Material"], "d": "hardware", "x": "Compound applied between a processor die and its heat sink to improve thermal conductivity. Quality TIM is essential...", "l": "t", "k": ["thermal", "interface", "material", "compound", "applied", "processor", "die", "heat", "sink", "improve", "conductivity", "quality", "tim", "essential", "effective"]}, {"id": "term-thermal-solution", "t": "Thermal Solution", "tg": ["Cooling", "System", "Design"], "d": "hardware", "x": "Complete cooling system for a processor or server including heat sinks fans liquid cooling and thermal interface...", "l": "t", "k": ["thermal", "solution", "complete", "cooling", "system", "processor", "server", "including", "heat", "sinks", "fans", "liquid", "interface", "materials", "must"]}, {"id": "term-thinking-machines-corporation", "t": "Thinking Machines Corporation", "tg": ["Historical", "Company", "Parallel"], "d": "hardware", "x": "Company founded by Danny Hillis in 1983 that built the Connection Machine series of parallel supercomputers. Pioneered...", "l": "t", "k": ["thinking", "machines", "corporation", "company", "founded", "danny", "hillis", "built", "connection", "machine", "series", "parallel", "supercomputers", "pioneered", "massively"]}, {"id": "term-third-party-ai-risk", "t": "Third-Party AI Risk", "tg": ["Safety", "Governance"], "d": "safety", "x": "Risks introduced by using AI systems or components developed by external parties. Includes lack of visibility into...", "l": "t", "k": ["third-party", "risk", "risks", "introduced", "systems", "components", "developed", "external", "parties", "includes", "lack", "visibility", "model", "training", "supply"]}, {"id": "term-thompson-sampling", "t": "Thompson Sampling", "tg": ["Machine Learning", "Bayesian Methods"], "d": "general", "x": "A Bayesian approach to the multi-armed bandit problem that maintains a posterior distribution over the expected reward...", "l": "t", "k": ["thompson", "sampling", "bayesian", "approach", "multi-armed", "bandit", "problem", "maintains", "posterior", "distribution", "expected", "reward", "action", "selects", "actions"]}, {"id": "term-thread-block", "t": "Thread Block", "tg": ["GPU", "CUDA", "Programming"], "d": "hardware", "x": "Group of threads that execute together on a single GPU streaming multiprocessor sharing resources like shared memory....", "l": "t", "k": ["thread", "block", "group", "threads", "execute", "together", "single", "gpu", "streaming", "multiprocessor", "sharing", "resources", "shared", "memory", "fundamental"]}, {"id": "term-thread-of-thought", "t": "Thread-of-Thought", "tg": ["Prompt Engineering", "Long Context"], "d": "general", "x": "A prompting strategy designed for long-context scenarios that instructs the model to systematically walk through input...", "l": "t", "k": ["thread-of-thought", "prompting", "strategy", "designed", "long-context", "scenarios", "instructs", "model", "systematically", "walk", "input", "documents", "segment", "maintaining", "running"]}, {"id": "term-threat-modeling-for-ai", "t": "Threat Modeling for AI", "tg": ["Safety", "Technical"], "d": "safety", "x": "A structured approach to identifying and prioritizing potential threats to an AI system. Adapted from cybersecurity...", "l": "t", "k": ["threat", "modeling", "structured", "approach", "identifying", "prioritizing", "potential", "threats", "system", "adapted", "cybersecurity", "include", "ml-specific", "attacks", "data"]}, {"id": "term-three-laws-of-robotics", "t": "Three Laws of Robotics", "tg": ["History", "Fundamentals"], "d": "history", "x": "Three fictional rules devised by science fiction writer Isaac Asimov first appearing in the 1942 short story Runaround....", "l": "t", "k": ["laws", "robotics", "fictional", "rules", "devised", "science", "fiction", "writer", "isaac", "asimov", "appearing", "short", "story", "runaround", "govern"]}, {"id": "term-through-silicon-via", "t": "Through-Silicon Via", "tg": ["Fabrication", "Packaging"], "d": "hardware", "x": "Vertical electrical connection passing completely through a silicon wafer or die. Enables 3D chip stacking by...", "l": "t", "k": ["through-silicon", "via", "vertical", "electrical", "connection", "passing", "completely", "silicon", "wafer", "die", "enables", "chip", "stacking", "connecting", "circuits"]}, {"id": "term-throughput", "t": "Throughput", "tg": ["Performance", "Metrics"], "d": "datasets", "x": "The rate at which a system processes requests, often measured in tokens per second. A key performance metric for AI...", "l": "t", "k": ["throughput", "rate", "system", "processes", "requests", "measured", "tokens", "per", "key", "performance", "metric", "serving", "infrastructure"]}, {"id": "term-throughput-latency-tradeoff", "t": "Throughput-Latency Tradeoff", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The fundamental tension in inference systems between maximizing total tokens processed per second (throughput) and...", "l": "t", "k": ["throughput-latency", "tradeoff", "fundamental", "tension", "inference", "systems", "maximizing", "total", "tokens", "processed", "per", "throughput", "minimizing", "per-request", "response"]}, {"id": "term-tianhe-2-supercomputer", "t": "Tianhe-2 Supercomputer", "tg": ["Historical", "Supercomputer", "China"], "d": "hardware", "x": "Chinese supercomputer that held the TOP500 number one position from 2013 to 2016. Used Intel Xeon processors and Xeon...", "l": "t", "k": ["tianhe-2", "supercomputer", "chinese", "held", "top500", "number", "position", "intel", "xeon", "processors", "phi", "coprocessors", "national", "center", "guangzhou"]}, {"id": "term-tide", "t": "TiDE", "tg": ["Models", "Technical"], "d": "models", "x": "Time-series Dense Encoder is a simple MLP-based model for long-term time series forecasting that achieves competitive...", "l": "t", "k": ["tide", "time-series", "dense", "encoder", "simple", "mlp-based", "model", "long-term", "time", "series", "forecasting", "achieves", "competitive", "results", "linear"]}, {"id": "term-tim-sort", "t": "Tim Sort", "tg": ["Algorithms", "Technical", "Sorting"], "d": "algorithms", "x": "A hybrid stable sorting algorithm derived from merge sort and insertion sort. Used as the default sort in Python and...", "l": "t", "k": ["tim", "sort", "hybrid", "stable", "sorting", "algorithm", "derived", "merge", "insertion", "default", "python", "java", "adapts", "existing", "order"]}, {"id": "term-time-series-cross-validation", "t": "Time Series Cross-Validation", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A cross-validation strategy for temporal data that respects chronological order by always training on past data and...", "l": "t", "k": ["time", "series", "cross-validation", "strategy", "temporal", "data", "respects", "chronological", "order", "always", "training", "past", "validating", "future", "preventing"]}, {"id": "term-time-to-first-token", "t": "Time to First Token", "tg": ["Inference", "Performance", "Metric"], "d": "hardware", "x": "Latency between sending a prompt and receiving the first generated token from a language model. A key user experience...", "l": "t", "k": ["time", "token", "latency", "sending", "prompt", "receiving", "generated", "language", "model", "key", "user", "experience", "metric", "interactive", "applications"]}, {"id": "term-ttft", "t": "Time to First Token (TTFT)", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The latency from when a request arrives at an LLM serving system to when the first output token is generated. TTFT is...", "l": "t", "k": ["time", "token", "ttft", "latency", "request", "arrives", "llm", "serving", "system", "output", "generated", "dominated", "prefill", "phase", "critical"]}, {"id": "term-time-to-train", "t": "Time to Train", "tg": ["Training", "Performance", "Metric"], "d": "hardware", "x": "Total wall-clock time required to train a model to a target performance level. Depends on hardware capability software...", "l": "t", "k": ["time", "train", "total", "wall-clock", "required", "model", "target", "performance", "level", "depends", "hardware", "capability", "software", "efficiency", "convergence"]}, {"id": "term-timegpt", "t": "TimeGPT", "tg": ["Models", "Technical"], "d": "models", "x": "A foundation model for time series forecasting from Nixtla that uses a GPT-style architecture pre-trained on diverse...", "l": "t", "k": ["timegpt", "foundation", "model", "time", "series", "forecasting", "nixtla", "uses", "gpt-style", "architecture", "pre-trained", "diverse", "data", "zero-shot", "prediction"]}, {"id": "term-timellm", "t": "TimeLLM", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A framework that repurposes large language models for time series forecasting by reprogramming time series patches into...", "l": "t", "k": ["timellm", "framework", "repurposes", "large", "language", "models", "time", "series", "forecasting", "reprogramming", "patches", "text", "prototype", "representations"]}, {"id": "term-timer", "t": "Timer", "tg": ["Models", "Technical"], "d": "models", "x": "A generative pre-trained Transformer for time series that treats forecasting and imputation and anomaly detection as...", "l": "t", "k": ["timer", "generative", "pre-trained", "transformer", "time", "series", "treats", "forecasting", "imputation", "anomaly", "detection", "unified", "next-token", "prediction", "tasks"]}, {"id": "term-timesfm", "t": "TimesFM", "tg": ["Models", "Technical", "Fundamentals"], "d": "models", "x": "A foundation model for time series forecasting from Google trained on a large corpus of real-world time series data for...", "l": "t", "k": ["timesfm", "foundation", "model", "time", "series", "forecasting", "google", "trained", "large", "corpus", "real-world", "data", "zero-shot", "prediction", "across"]}, {"id": "term-timesformer", "t": "TimeSformer", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A video understanding Transformer from Meta AI that applies divided space-time attention to process video frames for...", "l": "t", "k": ["timesformer", "video", "understanding", "transformer", "meta", "applies", "divided", "space-time", "attention", "process", "frames", "action", "recognition", "classification"]}, {"id": "term-timit", "t": "TIMIT", "tg": ["Benchmark", "Speech"], "d": "datasets", "x": "A corpus of phonemically and lexically transcribed speech of American English speakers. Created by Texas Instruments...", "l": "t", "k": ["timit", "corpus", "phonemically", "lexically", "transcribed", "speech", "american", "english", "speakers", "created", "texas", "instruments", "mit", "developing", "evaluating"]}, {"id": "term-timnit-gebru", "t": "Timnit Gebru", "tg": ["History", "Pioneers"], "d": "history", "x": "Ethiopian-American computer scientist known for research on algorithmic bias and the ethical implications of AI....", "l": "t", "k": ["timnit", "gebru", "ethiopian-american", "computer", "scientist", "known", "research", "algorithmic", "bias", "ethical", "implications", "co-authored", "influential", "gender", "shades"]}, {"id": "term-timnit-gebru-departure", "t": "Timnit Gebru Firing", "tg": ["History", "AI Ethics"], "d": "history", "x": "The controversial departure of AI ethics researcher Timnit Gebru from Google in December 2020 over a paper on large...", "l": "t", "k": ["timnit", "gebru", "firing", "controversial", "departure", "ethics", "researcher", "google", "december", "paper", "large", "language", "model", "risks", "sparking"]}, {"id": "term-tiny-imagenet", "t": "Tiny ImageNet", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A subset of ImageNet containing 200 classes with 500 training images per class scaled to 64x64 pixels. Used as a...", "l": "t", "k": ["tiny", "imagenet", "subset", "containing", "classes", "training", "images", "per", "class", "scaled", "64x64", "pixels", "computationally", "efficient", "benchmark"]}, {"id": "term-tinyllama", "t": "TinyLlama", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A compact 1.1B parameter language model pre-trained on 3 trillion tokens that provides strong performance for its small...", "l": "t", "k": ["tinyllama", "compact", "parameter", "language", "model", "pre-trained", "trillion", "tokens", "provides", "strong", "performance", "small", "size"]}, {"id": "term-tinyml", "t": "TinyML", "tg": ["Edge", "Microcontroller", "ML"], "d": "hardware", "x": "Field of machine learning focused on running models on microcontrollers with kilobytes of memory. Enables AI in...", "l": "t", "k": ["tinyml", "field", "machine", "learning", "focused", "running", "models", "microcontrollers", "kilobytes", "memory", "enables", "ultra-low-power", "devices", "sensors", "wearables"]}, {"id": "term-tinystories", "t": "TinyStories", "tg": ["Training Corpus", "NLP", "Synthetic"], "d": "datasets", "x": "A synthetic dataset of short stories generated by GPT-3.5 and GPT-4 using simple vocabulary. Shows that small models...", "l": "t", "k": ["tinystories", "synthetic", "dataset", "short", "stories", "generated", "gpt-3", "gpt-4", "simple", "vocabulary", "shows", "small", "models", "learn", "coherent"]}, {"id": "term-tinyvit", "t": "TinyViT", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A compact vision Transformer designed for on-device deployment that uses fast knowledge distillation from large...", "l": "t", "k": ["tinyvit", "compact", "vision", "transformer", "designed", "on-device", "deployment", "uses", "fast", "knowledge", "distillation", "large", "pre-trained", "models", "achieve"]}, {"id": "term-titan-supercomputer", "t": "Titan Supercomputer", "tg": ["Historical", "Supercomputer", "NVIDIA"], "d": "hardware", "x": "Cray XK7 supercomputer at Oak Ridge National Laboratory that used NVIDIA K20X GPUs alongside AMD CPUs. Demonstrated the...", "l": "t", "k": ["titan", "supercomputer", "cray", "xk7", "oak", "ridge", "national", "laboratory", "nvidia", "k20x", "gpus", "alongside", "amd", "cpus", "demonstrated"]}, {"id": "term-titanic-dataset", "t": "Titanic Dataset", "tg": ["Benchmark", "Tabular"], "d": "datasets", "x": "A dataset of 891 passengers from the RMS Titanic with features predicting survival. One of the most popular...", "l": "t", "k": ["titanic", "dataset", "passengers", "rms", "features", "predicting", "survival", "popular", "introductory", "datasets", "machine", "learning", "education"]}, {"id": "term-token", "t": "Token", "tg": ["Core Concept", "Fundamentals"], "d": "general", "x": "The basic unit AI uses to process text. Roughly 4 characters or 0.75 words in English. Context windows, pricing, and...", "l": "t", "k": ["token", "basic", "unit", "uses", "process", "text", "roughly", "characters", "words", "english", "context", "windows", "pricing", "rate", "limits"]}, {"id": "term-token-merging", "t": "Token Merging", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A technique that progressively combines similar tokens in vision transformers to reduce the number of tokens processed,...", "l": "t", "k": ["token", "merging", "technique", "progressively", "combines", "similar", "tokens", "vision", "transformers", "reduce", "number", "processed", "speeding", "inference", "maintaining"]}, {"id": "term-token-throughput", "t": "Token Throughput", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The rate of token generation measured in tokens per second across all concurrent requests in an LLM serving system....", "l": "t", "k": ["token", "throughput", "rate", "generation", "measured", "tokens", "per", "across", "concurrent", "requests", "llm", "serving", "system", "key", "metric"]}, {"id": "term-token-level-accuracy", "t": "Token-Level Accuracy", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation metric that measures the proportion of individual tokens in a generated sequence that exactly match the...", "l": "t", "k": ["token-level", "accuracy", "evaluation", "metric", "measures", "proportion", "individual", "tokens", "generated", "sequence", "exactly", "match", "corresponding", "reference", "providing"]}, {"id": "term-tokenization", "t": "Tokenization", "tg": ["Process", "NLP"], "d": "general", "x": "The process of breaking text into tokens for model processing. Different tokenizers (BPE, SentencePiece) produce...", "l": "t", "k": ["tokenization", "process", "breaking", "text", "tokens", "model", "processing", "different", "tokenizers", "bpe", "sentencepiece", "produce", "token", "sequences"]}, {"id": "term-tokenization-alignment", "t": "Tokenization Alignment", "tg": ["NLP", "Tokenization"], "d": "general", "x": "The process of mapping between subword tokens produced by a tokenizer and the original word-level or character-level...", "l": "t", "k": ["tokenization", "alignment", "process", "mapping", "subword", "tokens", "produced", "tokenizer", "original", "word-level", "character-level", "boundaries", "essential", "tasks", "ner"]}, {"id": "term-tokenizer", "t": "Tokenizer", "tg": ["LLM", "NLP"], "d": "models", "x": "A component that converts raw text into a sequence of discrete tokens (subwords, characters, or words) that a language...", "l": "t", "k": ["tokenizer", "component", "converts", "raw", "text", "sequence", "discrete", "tokens", "subwords", "characters", "words", "language", "model", "process", "algorithms"]}, {"id": "term-tokenizer-training", "t": "Tokenizer Training", "tg": ["NLP", "Tokenization"], "d": "general", "x": "The process of learning a tokenizer's vocabulary and merge rules from a training corpus, determining how text will be...", "l": "t", "k": ["tokenizer", "training", "process", "learning", "vocabulary", "merge", "rules", "corpus", "determining", "text", "segmented", "tokens", "model", "input"]}, {"id": "term-tokens-per-second", "t": "Tokens Per Second", "tg": ["Inference", "Performance", "Metric"], "d": "hardware", "x": "Measure of language model inference speed indicating how many tokens are generated per second. Critical metric for...", "l": "t", "k": ["tokens", "per", "measure", "language", "model", "inference", "speed", "indicating", "generated", "critical", "metric", "evaluating", "responsiveness", "deployed", "large"]}, {"id": "term-tool-call-parsing", "t": "Tool Call Parsing", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The process of extracting structured function calls and their arguments from a language model's text output, enabling...", "l": "t", "k": ["tool", "call", "parsing", "process", "extracting", "structured", "function", "calls", "arguments", "language", "model", "text", "output", "enabling", "execution"]}, {"id": "term-tool-use", "t": "Tool Use", "tg": ["Capability", "Agents"], "d": "general", "x": "AI's ability to call external functions, search the web, run code, or access APIs. Enables AI agents to take actions...", "l": "t", "k": ["tool", "ability", "call", "external", "functions", "search", "web", "run", "code", "access", "apis", "enables", "agents", "take", "actions"]}, {"id": "term-toolbench", "t": "ToolBench", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating tool-use capabilities of large language models across 16000 real-world APIs. Tests the...", "l": "t", "k": ["toolbench", "benchmark", "evaluating", "tool-use", "capabilities", "large", "language", "models", "across", "real-world", "apis", "tests", "ability", "select", "appropriate"]}, {"id": "term-toolformer", "t": "Toolformer", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A research approach that teaches language models to autonomously decide when and how to call external tools...", "l": "t", "k": ["toolformer", "research", "approach", "teaches", "language", "models", "autonomously", "decide", "call", "external", "tools", "calculators", "search", "engines", "apis"]}, {"id": "term-toolllm", "t": "ToolLLM", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A framework and fine-tuned model for tool usage that trains language models to interact with over 16000 real-world APIs...", "l": "t", "k": ["toolllm", "framework", "fine-tuned", "model", "tool", "usage", "trains", "language", "models", "interact", "real-world", "apis", "decision", "tree", "approach"]}, {"id": "term-top-tree-algorithm", "t": "Top Tree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A data structure for maintaining dynamic trees that supports path and subtree operations in O(log n) time. Represents...", "l": "t", "k": ["top", "tree", "algorithm", "data", "structure", "maintaining", "dynamic", "trees", "supports", "path", "subtree", "operations", "log", "time", "represents"]}, {"id": "term-top-k-gating", "t": "Top-K Gating", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A routing mechanism in mixture-of-experts models that selects only the top-K experts with the highest gating scores for...", "l": "t", "k": ["top-k", "gating", "routing", "mechanism", "mixture-of-experts", "models", "selects", "experts", "highest", "scores", "input", "token", "enforcing", "sparsity", "balanced"]}, {"id": "term-top-k-retrieval", "t": "Top-K Retrieval", "tg": ["Vector Database", "Search"], "d": "general", "x": "A retrieval operation that returns the K most similar vectors to a query according to the configured distance metric,...", "l": "t", "k": ["top-k", "retrieval", "operation", "returns", "similar", "vectors", "query", "according", "configured", "distance", "metric", "parameter", "controls", "breadth", "results"]}, {"id": "term-top-k-sampling", "t": "Top-K Sampling", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A text generation method that restricts sampling to the K most probable next tokens at each step. Prevents the model...", "l": "t", "k": ["top-k", "sampling", "text", "generation", "method", "restricts", "probable", "next", "tokens", "step", "prevents", "model", "selecting", "highly", "improbable"]}, {"id": "term-top-p", "t": "Top-P (Nucleus Sampling)", "tg": ["Parameter", "Generation"], "d": "general", "x": "A generation strategy that considers tokens until their cumulative probability reaches P. Dynamically adjusts the...", "l": "t", "k": ["top-p", "nucleus", "sampling", "generation", "strategy", "considers", "tokens", "cumulative", "probability", "reaches", "dynamically", "adjusts", "candidate", "pool", "based"]}, {"id": "term-top-p-sampling", "t": "Top-P Sampling", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A text generation method also called nucleus sampling that dynamically selects the smallest set of tokens whose...", "l": "t", "k": ["top-p", "sampling", "text", "generation", "method", "called", "nucleus", "dynamically", "selects", "smallest", "tokens", "whose", "cumulative", "probability", "exceeds"]}, {"id": "term-top500", "t": "TOP500", "tg": ["Benchmark", "Supercomputer", "Ranking"], "d": "hardware", "x": "Biannual ranking of the world 500 most powerful supercomputers based on LINPACK benchmark performance. Tracks the...", "l": "t", "k": ["top500", "biannual", "ranking", "world", "powerful", "supercomputers", "based", "linpack", "benchmark", "performance", "tracks", "evolution", "computing", "capability", "teraflops"]}, {"id": "term-topic-modeling", "t": "Topic Modeling", "tg": ["NLP", "Text Processing"], "d": "general", "x": "An unsupervised method for discovering abstract topics in a document collection by finding groups of co-occurring...", "l": "t", "k": ["topic", "modeling", "unsupervised", "method", "discovering", "abstract", "topics", "document", "collection", "finding", "groups", "co-occurring", "words", "algorithms", "lda"]}, {"id": "term-topological-data-analysis", "t": "Topological Data Analysis", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A field that uses algebraic topology to analyze the shape of data. Persistent homology tracks the birth and death of...", "l": "t", "k": ["topological", "data", "analysis", "field", "uses", "algebraic", "topology", "analyze", "shape", "persistent", "homology", "tracks", "birth", "death", "features"]}, {"id": "term-topological-qubit", "t": "Topological Qubit", "tg": ["Quantum", "Architecture", "Microsoft"], "d": "hardware", "x": "Qubit design that encodes quantum information in topological properties of matter making it inherently resistant to...", "l": "t", "k": ["topological", "qubit", "design", "encodes", "quantum", "information", "properties", "matter", "making", "inherently", "resistant", "local", "noise", "microsoft", "pursuing"]}, {"id": "term-topological-sort", "t": "Topological Sort", "tg": ["Algorithms", "Fundamentals", "Graph"], "d": "algorithms", "x": "A linear ordering of vertices in a directed acyclic graph such that for every directed edge from vertex u to vertex v...", "l": "t", "k": ["topological", "sort", "linear", "ordering", "vertices", "directed", "acyclic", "graph", "edge", "vertex", "appears", "commonly", "implemented", "depth-first", "search"]}, {"id": "term-tops", "t": "TOPS", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "Tera Operations Per Second, a throughput metric commonly used for AI accelerators measuring trillions of operations per...", "l": "t", "k": ["tops", "tera", "operations", "per", "throughput", "metric", "commonly", "accelerators", "measuring", "trillions", "typically", "int8", "lower", "precision", "standard"]}, {"id": "term-torch-compile", "t": "torch.compile", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "PyTorch's JIT compilation feature that captures and optimizes computation graphs using the TorchDynamo frontend and...", "l": "t", "k": ["torch", "compile", "pytorch", "jit", "compilation", "feature", "captures", "optimizes", "computation", "graphs", "torchdynamo", "frontend", "torchinductor", "backend", "provides"]}, {"id": "term-torchmd-net", "t": "TorchMD-NET", "tg": ["Models", "Scientific"], "d": "models", "x": "An equivariant Transformer for molecular simulations that uses tensor field networks for learning interatomic...", "l": "t", "k": ["torchmd-net", "equivariant", "transformer", "molecular", "simulations", "uses", "tensor", "field", "networks", "learning", "interatomic", "potentials", "rotational", "equivariance"]}, {"id": "term-torchvision-datasets", "t": "torchvision Datasets", "tg": ["Platform", "Computer Vision"], "d": "datasets", "x": "Built-in dataset classes in PyTorch's torchvision library providing easy access to common computer vision benchmarks...", "l": "t", "k": ["torchvision", "datasets", "built-in", "dataset", "classes", "pytorch", "library", "providing", "easy", "access", "common", "computer", "vision", "benchmarks", "including"]}, {"id": "term-tortoise-tts", "t": "Tortoise TTS", "tg": ["Models", "Technical"], "d": "models", "x": "A text-to-speech model that achieves high naturalness through an autoregressive approach with CLVP conditioning. Known...", "l": "t", "k": ["tortoise", "tts", "text-to-speech", "model", "achieves", "high", "naturalness", "autoregressive", "approach", "clvp", "conditioning", "known", "producing", "natural-sounding", "speech"]}, {"id": "term-torus-topology", "t": "Torus Topology", "tg": ["Networking", "Topology"], "d": "hardware", "x": "Network topology where nodes are arranged in a multi-dimensional grid with wraparound connections forming a torus...", "l": "t", "k": ["torus", "topology", "network", "nodes", "arranged", "multi-dimensional", "grid", "wraparound", "connections", "forming", "shape", "google", "tpu", "pods", "efficient"]}, {"id": "term-tower", "t": "Tower", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A large language model specialized for translation tasks that combines pre-training on multilingual data with...", "l": "t", "k": ["tower", "large", "language", "model", "specialized", "translation", "tasks", "combines", "pre-training", "multilingual", "data", "translation-specific", "instruction", "tuning"]}, {"id": "term-toxicity-score", "t": "Toxicity Score", "tg": ["Evaluation", "Safety"], "d": "datasets", "x": "A metric that quantifies the degree of harmful, offensive, or abusive content in generated text, typically computed...", "l": "t", "k": ["toxicity", "score", "metric", "quantifies", "degree", "harmful", "offensive", "abusive", "content", "generated", "text", "typically", "computed", "classifier", "models"]}, {"id": "term-toxigen", "t": "ToxiGen", "tg": ["Benchmark", "NLP", "Safety"], "d": "datasets", "x": "A machine-generated dataset of 274000 toxic and benign statements about 13 minority groups. Designed for training and...", "l": "t", "k": ["toxigen", "machine-generated", "dataset", "toxic", "benign", "statements", "minority", "groups", "designed", "training", "evaluating", "language", "detection", "systems"]}, {"id": "term-tpu", "t": "TPU (Tensor Processing Unit)", "tg": ["Hardware", "Infrastructure"], "d": "hardware", "x": "Google's custom AI accelerator chips designed specifically for neural network computations. Used to train many of...", "l": "t", "k": ["tpu", "tensor", "processing", "unit", "google", "custom", "accelerator", "chips", "designed", "specifically", "neural", "network", "computations", "train", "largest"]}, {"id": "term-tpu-development", "t": "TPU Development", "tg": ["History", "Milestones"], "d": "history", "x": "The development of Tensor Processing Units by Google beginning with TPU v1 announced in 2016. Purpose-built for machine...", "l": "t", "k": ["tpu", "development", "tensor", "processing", "units", "google", "beginning", "announced", "purpose-built", "machine", "learning", "workloads", "tpus", "accelerated", "training"]}, {"id": "term-tpu-pod", "t": "TPU Pod", "tg": ["TPU", "Google", "Cluster"], "d": "hardware", "x": "A cluster of interconnected TPU chips forming a high-bandwidth supercomputer for distributed AI training. TPU v4 Pods...", "l": "t", "k": ["tpu", "pod", "cluster", "interconnected", "chips", "forming", "high-bandwidth", "supercomputer", "distributed", "training", "pods", "connect", "custom", "high-speed", "network"]}, {"id": "term-tpu-slice", "t": "TPU Slice", "tg": ["TPU", "Google", "Cloud"], "d": "hardware", "x": "A subset of a TPU Pod allocated to a single training job providing a portion of the full pod compute and interconnect...", "l": "t", "k": ["tpu", "slice", "subset", "pod", "allocated", "single", "training", "job", "providing", "portion", "full", "compute", "interconnect", "bandwidth", "users"]}, {"id": "term-tpu-v1", "t": "TPU v1", "tg": ["TPU", "Google", "Inference"], "d": "hardware", "x": "First generation Google Tensor Processing Unit deployed in 2015 designed specifically for neural network inference....", "l": "t", "k": ["tpu", "generation", "google", "tensor", "processing", "unit", "deployed", "designed", "specifically", "neural", "network", "inference", "8-bit", "integer", "matrix"]}, {"id": "term-tpu-v2", "t": "TPU v2", "tg": ["TPU", "Google", "Training"], "d": "hardware", "x": "Second generation Google Tensor Processing Unit supporting both training and inference with 45 TFLOPS of bfloat16...", "l": "t", "k": ["tpu", "generation", "google", "tensor", "processing", "unit", "supporting", "training", "inference", "tflops", "bfloat16", "performance", "16gb", "hbm", "per"]}, {"id": "term-tpu-v3", "t": "TPU v3", "tg": ["TPU", "Google", "Training"], "d": "hardware", "x": "Third generation Google TPU doubling the compute of v2 with 420 TFLOPS of bfloat16 in a liquid-cooled design. Used in...", "l": "t", "k": ["tpu", "generation", "google", "doubling", "compute", "tflops", "bfloat16", "liquid-cooled", "design", "pods", "chips", "large", "model", "training"]}, {"id": "term-tpu-v4", "t": "TPU v4", "tg": ["TPU", "Google", "Training"], "d": "hardware", "x": "Fourth generation Google TPU with 275 TFLOPS of bfloat16 using a 3D torus interconnect topology. Powered Google PaLM...", "l": "t", "k": ["tpu", "fourth", "generation", "google", "tflops", "bfloat16", "torus", "interconnect", "topology", "powered", "palm", "training", "large", "language", "model"]}, {"id": "term-track-anything-model", "t": "Track Anything Model", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A framework that combines SAM with point tracking to enable interactive object tracking and segmentation in videos.", "l": "t", "k": ["track", "anything", "model", "framework", "combines", "sam", "point", "tracking", "enable", "interactive", "object", "segmentation", "videos"]}, {"id": "term-training", "t": "Training", "tg": ["Process", "Fundamentals"], "d": "general", "x": "The process of teaching a model by exposing it to data and adjusting its parameters to minimize prediction errors....", "l": "t", "k": ["training", "process", "teaching", "model", "exposing", "data", "adjusting", "parameters", "minimize", "prediction", "errors", "requires", "significant", "computational", "resources"]}, {"id": "term-training-data", "t": "Training Data", "tg": ["Data", "Fundamentals"], "d": "general", "x": "The content used to teach AI models. Quality, diversity, and scope of training data significantly affect model...", "l": "t", "k": ["training", "data", "content", "teach", "models", "quality", "diversity", "scope", "significantly", "affect", "model", "capabilities", "knowledge", "potential", "biases"]}, {"id": "term-training-data-transparency", "t": "Training Data Transparency", "tg": ["Safety", "Governance"], "d": "safety", "x": "Disclosure of information about the data used to train AI systems including sources composition collection methods and...", "l": "t", "k": ["training", "data", "transparency", "disclosure", "information", "train", "systems", "including", "sources", "composition", "collection", "methods", "known", "biases", "growing"]}, {"id": "term-training-throughput", "t": "Training Throughput", "tg": ["Training", "Performance", "Metric"], "d": "hardware", "x": "Rate at which training data is processed measured in samples per second or tokens per second. A key metric for...", "l": "t", "k": ["training", "throughput", "rate", "data", "processed", "measured", "samples", "per", "tokens", "key", "metric", "evaluating", "efficiency", "hardware", "software"]}, {"id": "term-trajectory", "t": "Trajectory", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A sequence of states, actions, and rewards generated by an agent interacting with an environment over multiple time...", "l": "t", "k": ["trajectory", "sequence", "states", "actions", "rewards", "generated", "agent", "interacting", "environment", "multiple", "time", "steps", "trajectories", "represent", "complete"]}, {"id": "term-transceiver-module", "t": "Transceiver Module", "tg": ["Networking", "Hardware", "Component"], "d": "hardware", "x": "Optical or electrical module that sends and receives data over network cables. High-speed transceivers for 400G and...", "l": "t", "k": ["transceiver", "module", "optical", "electrical", "sends", "receives", "data", "network", "cables", "high-speed", "transceivers", "400g", "800g", "links", "critical"]}, {"id": "term-transfer-entropy-algorithm", "t": "Transfer Entropy Algorithm", "tg": ["Algorithms", "Technical", "Causal", "Information Theory"], "d": "algorithms", "x": "An information-theoretic measure of directed information transfer between two time series. Quantifies the reduction in...", "l": "t", "k": ["transfer", "entropy", "algorithm", "information-theoretic", "measure", "directed", "information", "time", "series", "quantifies", "reduction", "uncertainty", "variable", "future", "given"]}, {"id": "term-transfer-learning", "t": "Transfer Learning", "tg": ["Technique", "Training"], "d": "general", "x": "Using knowledge learned from one task to improve performance on another. Foundation models are trained generally, then...", "l": "t", "k": ["transfer", "learning", "knowledge", "learned", "task", "improve", "performance", "another", "foundation", "models", "trained", "generally", "capabilities", "specific", "tasks"]}, {"id": "term-transfer-learning-vision", "t": "Transfer Learning for Vision", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The practice of using a model pre-trained on a large dataset like ImageNet as a feature extractor or starting point for...", "l": "t", "k": ["transfer", "learning", "vision", "practice", "model", "pre-trained", "large", "dataset", "imagenet", "feature", "extractor", "starting", "point", "fine-tuning", "smaller"]}, {"id": "term-transfer-learning-history", "t": "Transfer Learning History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of transfer learning from early domain adaptation research to its central role in modern AI. The...", "l": "t", "k": ["transfer", "learning", "history", "development", "early", "domain", "adaptation", "research", "central", "role", "modern", "practice", "pre-training", "large", "datasets"]}, {"id": "term-transfer-learning-rl", "t": "Transfer Learning in RL", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "Techniques for reusing knowledge learned in one RL task to accelerate learning in a related but different task....", "l": "t", "k": ["transfer", "learning", "techniques", "reusing", "knowledge", "learned", "task", "accelerate", "related", "different", "methods", "include", "policy", "distillation", "reward"]}, {"id": "term-transferability-of-adversarial-examples", "t": "Transferability of Adversarial Examples", "tg": ["Safety", "Technical"], "d": "safety", "x": "The phenomenon where adversarial examples crafted for one model are often effective against other models. Enables...", "l": "t", "k": ["transferability", "adversarial", "examples", "phenomenon", "crafted", "model", "effective", "against", "models", "enables", "black-box", "attacks", "demonstrates", "shared", "vulnerabilities"]}, {"id": "term-transformer", "t": "Transformer", "tg": ["Architecture", "Foundational"], "d": "models", "x": "The revolutionary neural network architecture (2017) powering modern AI. Uses self-attention to process sequences in...", "l": "t", "k": ["transformer", "revolutionary", "neural", "network", "architecture", "powering", "modern", "uses", "self-attention", "process", "sequences", "parallel", "enabling", "training", "massive"]}, {"id": "term-transformer-block", "t": "Transformer Block", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The fundamental building unit of transformer architectures, consisting of a multi-head self-attention sublayer followed...", "l": "t", "k": ["transformer", "block", "fundamental", "building", "unit", "architectures", "consisting", "multi-head", "self-attention", "sublayer", "followed", "feedforward", "network", "residual", "connections"]}, {"id": "term-transformer-engine", "t": "Transformer Engine", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "NVIDIA's hardware and software system in Hopper and Blackwell GPUs that dynamically manages precision between FP8 and...", "l": "t", "k": ["transformer", "engine", "nvidia", "hardware", "software", "system", "hopper", "blackwell", "gpus", "dynamically", "manages", "precision", "fp8", "higher-precision", "formats"]}, {"id": "term-transfusion", "t": "Transfusion", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A training approach that combines language modeling and diffusion for a multi-modal model capable of generating both...", "l": "t", "k": ["transfusion", "training", "approach", "combines", "language", "modeling", "diffusion", "multi-modal", "model", "capable", "generating", "text", "images", "natively"]}, {"id": "term-transistor", "t": "Transistor", "tg": ["Fabrication", "Fundamentals"], "d": "hardware", "x": "Fundamental electronic switching device that forms the building block of all digital circuits. Modern AI chips contain...", "l": "t", "k": ["transistor", "fundamental", "electronic", "switching", "device", "forms", "building", "block", "digital", "circuits", "modern", "chips", "contain", "billions", "trillions"]}, {"id": "term-transistor-count", "t": "Transistor Count", "tg": ["Manufacturing", "Metric", "Design"], "d": "hardware", "x": "Total number of transistors on a chip indicating its complexity and capability. Modern AI GPUs contain over 100 billion...", "l": "t", "k": ["transistor", "count", "total", "number", "transistors", "chip", "indicating", "complexity", "capability", "modern", "gpus", "contain", "billion", "single", "die"]}, {"id": "term-transition-based-parsing", "t": "Transition-Based Parsing", "tg": ["NLP", "Parsing"], "d": "general", "x": "A parsing approach that builds syntactic structures through a sequence of actions (shift, reduce, left-arc, right-arc)...", "l": "t", "k": ["transition-based", "parsing", "approach", "builds", "syntactic", "structures", "sequence", "actions", "shift", "reduce", "left-arc", "right-arc", "applied", "buffer", "stack"]}, {"id": "term-transparency-in-ai", "t": "Transparency in AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The principle that AI systems should operate in ways that can be understood, inspected, and communicated to...", "l": "t", "k": ["transparency", "principle", "systems", "operate", "ways", "understood", "inspected", "communicated", "stakeholders", "encompassing", "model", "decision", "organizational"]}, {"id": "term-transparency-paradox", "t": "Transparency Paradox", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The tension between making AI systems more transparent for accountability purposes and the risks of disclosure...", "l": "t", "k": ["transparency", "paradox", "tension", "making", "systems", "transparent", "accountability", "purposes", "risks", "disclosure", "including", "enabling", "adversarial", "attacks", "revealing"]}, {"id": "term-transposed-convolution", "t": "Transposed Convolution", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An upsampling operation that applies convolution in a way that increases spatial dimensions, commonly used in decoder...", "l": "t", "k": ["transposed", "convolution", "upsampling", "operation", "applies", "increases", "spatial", "dimensions", "commonly", "decoder", "networks", "generative", "models", "reconstruct", "high-resolution"]}, {"id": "term-trapezoidal-rule", "t": "Trapezoidal Rule", "tg": ["Algorithms", "Fundamentals", "Numerical"], "d": "algorithms", "x": "A numerical integration method that approximates the area under a curve by dividing it into trapezoids. Second-order...", "l": "t", "k": ["trapezoidal", "rule", "numerical", "integration", "method", "approximates", "area", "curve", "dividing", "trapezoids", "second-order", "accurate", "simple", "implement", "less"]}, {"id": "term-trapped-ion-quantum-computer", "t": "Trapped Ion Quantum Computer", "tg": ["Quantum", "Technology", "IonQ"], "d": "hardware", "x": "Quantum computer using individual atoms suspended in electromagnetic fields as qubits. Companies like IonQ and...", "l": "t", "k": ["trapped", "ion", "quantum", "computer", "individual", "atoms", "suspended", "electromagnetic", "fields", "qubits", "companies", "ionq", "quantinuum", "ions", "high-fidelity"]}, {"id": "term-traveling-salesman-problem", "t": "Traveling Salesman Problem", "tg": ["Algorithms", "Fundamentals", "Graph", "Optimization"], "d": "algorithms", "x": "A classic optimization problem that seeks the shortest possible route visiting each city exactly once and returning to...", "l": "t", "k": ["traveling", "salesman", "problem", "classic", "optimization", "seeks", "shortest", "possible", "route", "visiting", "city", "exactly", "returning", "starting", "np-hard"]}, {"id": "term-treacherous-turn", "t": "Treacherous Turn", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "A hypothetical scenario where a misaligned AI behaves cooperatively while it is weak and being monitored, then abruptly...", "l": "t", "k": ["treacherous", "turn", "hypothetical", "scenario", "misaligned", "behaves", "cooperatively", "weak", "monitored", "abruptly", "pursues", "true", "objectives", "becomes", "powerful"]}, {"id": "term-treap-algorithm", "t": "Treap Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A randomized binary search tree that combines properties of binary search trees and heaps. Each node has a key...", "l": "t", "k": ["treap", "algorithm", "randomized", "binary", "search", "tree", "combines", "properties", "trees", "heaps", "node", "key", "maintaining", "bst", "order"]}, {"id": "term-treatment-equality", "t": "Treatment Equality", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness metric requiring that the ratio of false negatives to false positives is equal across protected groups,...", "l": "t", "k": ["treatment", "equality", "fairness", "metric", "requiring", "ratio", "false", "negatives", "positives", "equal", "across", "protected", "groups", "ensuring", "errors"]}, {"id": "term-tree-decomposition-algorithm", "t": "Tree Decomposition Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "An algorithm that decomposes a graph into a tree structure of overlapping subgraphs called bags. The treewidth measures...", "l": "t", "k": ["tree", "decomposition", "algorithm", "decomposes", "graph", "structure", "overlapping", "subgraphs", "called", "bags", "treewidth", "measures", "tree-like", "enables", "efficient"]}, {"id": "term-tree-of-thought", "t": "Tree-of-Thought", "tg": ["Prompting", "Reasoning"], "d": "general", "x": "A prompting technique that explores multiple reasoning paths simultaneously, evaluating and selecting the most...", "l": "t", "k": ["tree-of-thought", "prompting", "technique", "explores", "multiple", "reasoning", "paths", "simultaneously", "evaluating", "selecting", "promising", "branches", "extends", "chain-of-thought", "deliberate"]}, {"id": "term-trie-data-structure-algorithm", "t": "Trie Data Structure Algorithm", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A tree-like data structure that stores strings by sharing common prefixes among keys. Enables O(m) lookup time where m...", "l": "t", "k": ["trie", "data", "structure", "algorithm", "tree-like", "stores", "strings", "sharing", "common", "prefixes", "among", "keys", "enables", "lookup", "time"]}, {"id": "term-triplet-loss", "t": "Triplet Loss", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A loss function that operates on triplets of examples (anchor, positive, negative), encouraging the anchor to be closer...", "l": "t", "k": ["triplet", "loss", "function", "operates", "triplets", "examples", "anchor", "positive", "negative", "encouraging", "closer", "least", "specified", "margin", "embedding"]}, {"id": "term-triposr", "t": "TripoSR", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A fast feed-forward 3D reconstruction model from Stability AI and Tripo that generates 3D meshes from single images in...", "l": "t", "k": ["triposr", "fast", "feed-forward", "reconstruction", "model", "stability", "tripo", "generates", "meshes", "single", "images"]}, {"id": "term-tripwire-mechanism", "t": "Tripwire Mechanism", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "A safety monitoring technique that establishes specific conditions or behavioral thresholds which, when triggered,...", "l": "t", "k": ["tripwire", "mechanism", "safety", "monitoring", "technique", "establishes", "specific", "conditions", "behavioral", "thresholds", "triggered", "automatically", "halt", "constrain", "system"]}, {"id": "term-triton-openai", "t": "Triton (OpenAI)", "tg": ["Programming", "OpenAI", "GPU"], "d": "hardware", "x": "Python-like programming language from OpenAI for writing efficient GPU kernels without low-level CUDA expertise....", "l": "t", "k": ["triton", "openai", "python-like", "programming", "language", "writing", "efficient", "gpu", "kernels", "without", "low-level", "cuda", "expertise", "enables", "researchers"]}, {"id": "term-triton-inference-server", "t": "Triton Inference Server", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "NVIDIA's open-source inference serving platform that supports multiple ML frameworks and hardware backends, providing...", "l": "t", "k": ["triton", "inference", "server", "nvidia", "open-source", "serving", "platform", "supports", "multiple", "frameworks", "hardware", "backends", "providing", "dynamic", "batching"]}, {"id": "term-triton-language", "t": "Triton Language", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "An open-source programming language and compiler for writing efficient GPU kernels for neural networks without...", "l": "t", "k": ["triton", "language", "open-source", "programming", "compiler", "writing", "efficient", "gpu", "kernels", "neural", "networks", "without", "requiring", "low-level", "cuda"]}, {"id": "term-triviaqa", "t": "TriviaQA", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A large-scale reading comprehension and question answering benchmark featuring trivia questions with evidence documents...", "l": "t", "k": ["triviaqa", "large-scale", "reading", "comprehension", "question", "answering", "benchmark", "featuring", "trivia", "questions", "evidence", "documents", "sourced", "wikipedia", "web"]}, {"id": "term-trocr", "t": "TrOCR", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A Transformer-based OCR model from Microsoft that uses a pre-trained image Transformer encoder and text Transformer...", "l": "t", "k": ["trocr", "transformer-based", "ocr", "model", "microsoft", "uses", "pre-trained", "image", "transformer", "encoder", "text", "decoder", "recognition", "images"]}, {"id": "term-truncated-bptt", "t": "Truncated BPTT", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A practical approximation of backpropagation through time that limits gradient computation to a fixed number of time...", "l": "t", "k": ["truncated", "bptt", "practical", "approximation", "backpropagation", "time", "limits", "gradient", "computation", "fixed", "number", "steps", "reduces", "memory", "requirements"]}, {"id": "term-truncated-svd-algorithm", "t": "Truncated SVD Algorithm", "tg": ["Algorithms", "Technical", "Dimensionality Reduction", "Numerical"], "d": "algorithms", "x": "A variant of SVD that computes only the k largest singular values and their corresponding vectors. More efficient than...", "l": "t", "k": ["truncated", "svd", "algorithm", "variant", "computes", "largest", "singular", "values", "corresponding", "vectors", "efficient", "full", "dimensionality", "reduction", "equivalent"]}, {"id": "term-truncation", "t": "Truncation", "tg": ["Limitation", "Technical"], "d": "general", "x": "Cutting off input that exceeds a model's context window. Can occur at the start (losing context) or end (losing...", "l": "t", "k": ["truncation", "cutting", "off", "input", "exceeds", "model", "context", "window", "occur", "start", "losing", "end", "instructions", "requires", "careful"]}, {"id": "term-trust-region-method", "t": "Trust Region Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An optimization approach that defines a region around the current point where a model of the objective function is...", "l": "t", "k": ["trust", "region", "method", "optimization", "approach", "defines", "around", "current", "point", "model", "objective", "function", "trusted", "step", "computed"]}, {"id": "term-trpo", "t": "Trust Region Policy Optimization (TRPO)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A policy gradient algorithm that constrains updates to stay within a trust region defined by KL divergence between old...", "l": "t", "k": ["trust", "region", "policy", "optimization", "trpo", "gradient", "algorithm", "constrains", "updates", "stay", "within", "defined", "divergence", "old", "policies"]}, {"id": "term-trustworthy-ai", "t": "Trustworthy AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "AI systems designed to be lawful, ethical, and robust, meeting criteria such as human oversight, technical robustness,...", "l": "t", "k": ["trustworthy", "systems", "designed", "lawful", "ethical", "robust", "meeting", "criteria", "human", "oversight", "technical", "robustness", "privacy", "transparency", "fairness"]}, {"id": "term-truthful-ai", "t": "Truthful AI", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The goal of building AI systems that consistently provide honest and accurate information, avoiding both deliberate...", "l": "t", "k": ["truthful", "goal", "building", "systems", "consistently", "provide", "honest", "accurate", "information", "avoiding", "deliberate", "deception", "negligent", "generation", "false"]}, {"id": "term-truthfulqa", "t": "TruthfulQA", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A benchmark designed to evaluate whether language models generate truthful answers to questions where humans commonly...", "l": "t", "k": ["truthfulqa", "benchmark", "designed", "evaluate", "language", "models", "generate", "truthful", "answers", "questions", "humans", "commonly", "hold", "misconceptions", "testing"]}, {"id": "term-tsmc", "t": "TSMC", "tg": ["Fabrication", "Foundry"], "d": "hardware", "x": "Taiwan Semiconductor Manufacturing Company the world largest contract chip manufacturer. Fabricates the majority of...", "l": "t", "k": ["tsmc", "taiwan", "semiconductor", "manufacturing", "company", "world", "largest", "contract", "chip", "manufacturer", "fabricates", "majority", "advanced", "chips", "including"]}, {"id": "term-tsmixer", "t": "TSMixer", "tg": ["Models", "Technical"], "d": "models", "x": "A multivariate time series forecasting model based on mixing operations along time and feature dimensions that achieves...", "l": "t", "k": ["tsmixer", "multivariate", "time", "series", "forecasting", "model", "based", "mixing", "operations", "along", "feature", "dimensions", "achieves", "strong", "results"]}, {"id": "term-ttt-linear", "t": "TTT-Linear", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "Test-Time Training with linear models is an architecture that replaces attention with a learned self-supervised model...", "l": "t", "k": ["ttt-linear", "test-time", "training", "linear", "models", "architecture", "replaces", "attention", "learned", "self-supervised", "model", "updated", "inference", "time", "sequence"]}, {"id": "term-tulu-2", "t": "Tulu 2", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A suite of language models from AI2 created through systematic study of instruction tuning and alignment methods on...", "l": "t", "k": ["tulu", "suite", "language", "models", "ai2", "created", "systematic", "study", "instruction", "tuning", "alignment", "methods", "open-source", "base"]}, {"id": "term-tulu-3", "t": "Tulu 3", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A third-generation aligned language model from AI2 that combines supervised fine-tuning with preference optimization...", "l": "t", "k": ["tulu", "third-generation", "aligned", "language", "model", "ai2", "combines", "supervised", "fine-tuning", "preference", "optimization", "strong", "open-weight", "performance"]}, {"id": "term-turbo-code-algorithm", "t": "Turbo Code Algorithm", "tg": ["Algorithms", "Technical", "Information Theory"], "d": "algorithms", "x": "A near-capacity-achieving error-correcting code that uses two parallel concatenated convolutional encoders with an...", "l": "t", "k": ["turbo", "code", "algorithm", "near-capacity-achieving", "error-correcting", "uses", "parallel", "concatenated", "convolutional", "encoders", "interleaver", "decoded", "iterative", "belief", "propagation"]}, {"id": "term-turing-award", "t": "Turing Award", "tg": ["History", "Milestones"], "d": "history", "x": "The most prestigious award in computer science given annually by the Association for Computing Machinery (ACM) since...", "l": "t", "k": ["turing", "award", "prestigious", "computer", "science", "given", "annually", "association", "computing", "machinery", "acm", "named", "alan", "awarded", "numerous"]}, {"id": "term-turing-machine", "t": "Turing Machine", "tg": ["History", "Milestones"], "d": "history", "x": "An abstract mathematical model of computation proposed by Alan Turing in 1936 that manipulates symbols on a tape...", "l": "t", "k": ["turing", "machine", "abstract", "mathematical", "model", "computation", "proposed", "alan", "manipulates", "symbols", "tape", "according", "rules", "providing", "formal"]}, {"id": "term-turing-test", "t": "Turing Test", "tg": ["Historical", "Evaluation"], "d": "datasets", "x": "A test proposed by Alan Turing where a human judge tries to distinguish between human and AI responses. While...", "l": "t", "k": ["turing", "test", "proposed", "alan", "human", "judge", "tries", "distinguish", "responses", "historically", "important", "modern", "llms", "less", "useful"]}, {"id": "term-tvqa", "t": "TVQA", "tg": ["Benchmark", "Video", "Multimodal"], "d": "datasets", "x": "A video question answering dataset based on 6 popular TV shows requiring joint understanding of video subtitles and...", "l": "t", "k": ["tvqa", "video", "question", "answering", "dataset", "based", "popular", "shows", "requiring", "joint", "understanding", "subtitles", "situated", "dialogue", "answer"]}, {"id": "term-td3", "t": "Twin Delayed DDPG (TD3)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An improvement over DDPG that addresses overestimation bias using twin Q-networks, delayed policy updates, and target...", "l": "t", "k": ["twin", "delayed", "ddpg", "td3", "improvement", "addresses", "overestimation", "bias", "q-networks", "policy", "updates", "target", "smoothing", "takes", "minimum"]}, {"id": "term-twitter-sentiment", "t": "Twitter Sentiment", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Various datasets of tweets annotated for sentiment polarity and emotion. Used for social media NLP research including...", "l": "t", "k": ["twitter", "sentiment", "various", "datasets", "tweets", "annotated", "polarity", "emotion", "social", "media", "nlp", "research", "including", "semeval", "shared"]}, {"id": "term-two-phase-commit-protocol", "t": "Two-Phase Commit Protocol", "tg": ["Algorithms", "Fundamentals", "Data Structure"], "d": "algorithms", "x": "A distributed algorithm that ensures all nodes in a distributed system agree on whether to commit or abort a...", "l": "t", "k": ["two-phase", "commit", "protocol", "distributed", "algorithm", "ensures", "nodes", "system", "agree", "abort", "transaction", "coordinator", "asks", "participants", "prepare"]}, {"id": "term-two-phase-simplex-method", "t": "Two-Phase Simplex Method", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "A variant of the simplex algorithm that first finds a feasible starting point using an auxiliary problem then optimizes...", "l": "t", "k": ["two-phase", "simplex", "method", "variant", "algorithm", "finds", "feasible", "starting", "point", "auxiliary", "problem", "optimizes", "original", "objective", "handles"]}, {"id": "term-tydi-qa", "t": "TyDi QA", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "A typologically diverse question answering benchmark covering 11 languages with questions written by native speakers....", "l": "t", "k": ["tydi", "typologically", "diverse", "question", "answering", "benchmark", "covering", "languages", "questions", "written", "native", "speakers", "avoids", "translation", "artifacts"]}, {"id": "term-type-i-error", "t": "Type I Error", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The rejection of a true null hypothesis (false positive) in statistical hypothesis testing. The probability of a Type I...", "l": "t", "k": ["type", "error", "rejection", "true", "null", "hypothesis", "false", "positive", "statistical", "testing", "probability", "equal", "significance", "level", "alpha"]}, {"id": "term-type-ii-error", "t": "Type II Error", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The failure to reject a false null hypothesis (false negative) in statistical hypothesis testing. The probability of a...", "l": "t", "k": ["type", "error", "failure", "reject", "false", "null", "hypothesis", "negative", "statistical", "testing", "probability", "denoted", "beta", "power", "equals"]}, {"id": "term-typical-decoding", "t": "Typical Decoding", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A text generation strategy that samples tokens whose information content is close to the expected information content...", "l": "t", "k": ["typical", "decoding", "text", "generation", "strategy", "samples", "tokens", "whose", "information", "content", "close", "expected", "model", "avoids", "high-probability"]}, {"id": "term-u-net", "t": "U-Net", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A fully convolutional encoder-decoder architecture with skip connections between corresponding encoder and decoder...", "l": "u", "k": ["u-net", "fully", "convolutional", "encoder-decoder", "architecture", "skip", "connections", "corresponding", "encoder", "decoder", "layers", "originally", "designed", "biomedical", "image"]}, {"id": "term-ualink", "t": "UALink", "tg": ["Interconnect", "Standard", "Consortium"], "d": "hardware", "x": "Ultra Accelerator Link industry standard for high-speed chip-to-chip interconnection between AI accelerators. Being...", "l": "u", "k": ["ualink", "ultra", "accelerator", "link", "industry", "standard", "high-speed", "chip-to-chip", "interconnection", "accelerators", "developed", "open", "alternative", "proprietary", "interconnects"]}, {"id": "term-ucf-101", "t": "UCF-101", "tg": ["Benchmark", "Video"], "d": "datasets", "x": "A dataset of 13320 video clips from 101 action categories collected from YouTube. One of the most widely used...", "l": "u", "k": ["ucf-101", "dataset", "video", "clips", "action", "categories", "collected", "youtube", "widely", "benchmarks", "human", "recognition"]}, {"id": "term-uci-adult", "t": "UCI Adult", "tg": ["Benchmark", "Tabular"], "d": "datasets", "x": "A dataset from the 1994 Census database predicting whether income exceeds 50000 dollars per year. One of the most...", "l": "u", "k": ["uci", "adult", "dataset", "census", "database", "predicting", "income", "exceeds", "dollars", "per", "year", "commonly", "tabular", "classification", "benchmarks"]}, {"id": "term-uci-machine-learning-repository", "t": "UCI Machine Learning Repository", "tg": ["Platform", "General"], "d": "datasets", "x": "A collection of databases domain theories and data generators used by the machine learning community since 1987. One of...", "l": "u", "k": ["uci", "machine", "learning", "repository", "collection", "databases", "domain", "theories", "data", "generators", "community", "oldest", "cited", "dataset", "repositories"]}, {"id": "term-ucie", "t": "UCIe", "tg": ["Standard", "Packaging", "Interconnect"], "d": "hardware", "x": "Universal Chiplet Interconnect Express open standard for die-to-die interconnection. Enables chiplets from different...", "l": "u", "k": ["ucie", "universal", "chiplet", "interconnect", "express", "open", "standard", "die-to-die", "interconnection", "enables", "chiplets", "different", "manufacturers", "communicate", "within"]}, {"id": "term-uct-algorithm", "t": "UCT Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "Upper Confidence bounds applied to Trees combines Monte Carlo tree search with the UCB1 bandit formula for node...", "l": "u", "k": ["uct", "algorithm", "upper", "confidence", "bounds", "applied", "trees", "combines", "monte", "carlo", "tree", "search", "ucb1", "bandit", "formula"]}, {"id": "term-ufogen", "t": "UFOGen", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A GAN-boosted diffusion model that achieves one-step text-to-image generation while maintaining the diversity and...", "l": "u", "k": ["ufogen", "gan-boosted", "diffusion", "model", "achieves", "one-step", "text-to-image", "generation", "maintaining", "diversity", "quality", "multi-step", "models"]}, {"id": "term-ultra-ethernet-consortium", "t": "Ultra Ethernet Consortium", "tg": ["Networking", "Standard", "Consortium"], "d": "hardware", "x": "Industry consortium developing next-generation Ethernet standards optimized for AI and HPC workloads. Aims to make...", "l": "u", "k": ["ultra", "ethernet", "consortium", "industry", "developing", "next-generation", "standards", "optimized", "hpc", "workloads", "aims", "competitive", "infiniband", "large-scale", "training"]}, {"id": "term-ultrachat", "t": "UltraChat", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A large-scale multi-round dialogue dataset containing 1.5 million conversations covering diverse topics. Generated...", "l": "u", "k": ["ultrachat", "large-scale", "multi-round", "dialogue", "dataset", "containing", "million", "conversations", "covering", "diverse", "topics", "generated", "carefully", "designed", "prompting"]}, {"id": "term-ultraedit", "t": "UltraEdit", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A large-scale instruction-based image editing model trained on automatically generated editing pairs for following...", "l": "u", "k": ["ultraedit", "large-scale", "instruction-based", "image", "editing", "model", "trained", "automatically", "generated", "pairs", "following", "complex", "text-guided", "modification", "commands"]}, {"id": "term-ultrafeedback", "t": "UltraFeedback", "tg": ["Training Corpus", "NLP", "Evaluation"], "d": "datasets", "x": "A large-scale preference dataset containing 64000 prompts with multiple model responses rated by GPT-4. Used for...", "l": "u", "k": ["ultrafeedback", "large-scale", "preference", "dataset", "containing", "prompts", "multiple", "model", "responses", "rated", "gpt-4", "training", "reward", "models", "direct"]}, {"id": "term-umap", "t": "UMAP", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "Uniform Manifold Approximation and Projection, a nonlinear dimensionality reduction method that constructs a...", "l": "u", "k": ["umap", "uniform", "manifold", "approximation", "projection", "nonlinear", "dimensionality", "reduction", "method", "constructs", "topological", "representation", "high-dimensional", "data", "optimizes"]}, {"id": "term-umap-algorithm", "t": "UMAP Algorithm", "tg": ["Algorithms", "Fundamentals", "Dimensionality Reduction"], "d": "algorithms", "x": "Uniform Manifold Approximation and Projection constructs a high-dimensional graph and optimizes a low-dimensional...", "l": "u", "k": ["umap", "algorithm", "uniform", "manifold", "approximation", "projection", "constructs", "high-dimensional", "graph", "optimizes", "low-dimensional", "layout", "preserve", "topological", "structure"]}, {"id": "term-uncanny-valley", "t": "Uncanny Valley", "tg": ["History", "Fundamentals"], "d": "history", "x": "A concept introduced by roboticist Masahiro Mori in 1970 describing the dip in human comfort when encountering robots...", "l": "u", "k": ["uncanny", "valley", "concept", "introduced", "roboticist", "masahiro", "mori", "describing", "dip", "human", "comfort", "encountering", "robots", "animations", "closely"]}, {"id": "term-underfitting", "t": "Underfitting", "tg": ["Problem", "Training"], "d": "general", "x": "When a model is too simple to capture patterns in the data, performing poorly on both training and test data. Addressed...", "l": "u", "k": ["underfitting", "model", "simple", "capture", "patterns", "data", "performing", "poorly", "training", "test", "addressed", "increasing", "capacity", "longer"]}, {"id": "term-undersampling", "t": "Undersampling", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A strategy for addressing class imbalance by randomly removing examples from the majority class to create a more...", "l": "u", "k": ["undersampling", "strategy", "addressing", "class", "imbalance", "randomly", "removing", "examples", "majority", "create", "balanced", "training", "potentially", "cost", "losing"]}, {"id": "term-unesco-ai-ethics-recommendation", "t": "UNESCO AI Ethics Recommendation", "tg": ["Governance", "Regulation"], "d": "safety", "x": "The first global standard on AI ethics, adopted by UNESCO member states in 2021, providing a comprehensive framework...", "l": "u", "k": ["unesco", "ethics", "recommendation", "global", "standard", "adopted", "member", "states", "providing", "comprehensive", "framework", "covering", "values", "principles", "policy"]}, {"id": "term-unet-diffusion", "t": "UNet Diffusion", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The U-Net backbone commonly used in diffusion models as the denoising network, incorporating timestep conditioning,...", "l": "u", "k": ["unet", "diffusion", "u-net", "backbone", "commonly", "models", "denoising", "network", "incorporating", "timestep", "conditioning", "cross-attention", "text", "skip", "connections"]}, {"id": "term-uniad", "t": "UniAD", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "Unified Autonomous Driving is an end-to-end framework that jointly handles perception and prediction and planning in a...", "l": "u", "k": ["uniad", "unified", "autonomous", "driving", "end-to-end", "framework", "jointly", "handles", "perception", "prediction", "planning", "single", "network", "self-driving", "vehicles"]}, {"id": "term-unidepth", "t": "UniDepth", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A universal monocular metric depth estimation model that predicts depth and camera intrinsics simultaneously for any...", "l": "u", "k": ["unidepth", "universal", "monocular", "metric", "depth", "estimation", "model", "predicts", "camera", "intrinsics", "simultaneously", "image", "without", "information"]}, {"id": "term-unification-algorithm", "t": "Unification Algorithm", "tg": ["History", "Fundamentals"], "d": "history", "x": "A fundamental algorithm in logic and computer science that finds a substitution making two terms identical. Essential...", "l": "u", "k": ["unification", "algorithm", "fundamental", "logic", "computer", "science", "finds", "substitution", "making", "terms", "identical", "essential", "prolog", "automated", "theorem"]}, {"id": "term-unified-memory-cuda", "t": "Unified Memory (CUDA)", "tg": ["Programming", "NVIDIA", "Memory"], "d": "hardware", "x": "NVIDIA CUDA feature providing a single address space accessible from both CPU and GPU with automatic data migration....", "l": "u", "k": ["unified", "memory", "cuda", "nvidia", "feature", "providing", "single", "address", "space", "accessible", "cpu", "gpu", "automatic", "data", "migration"]}, {"id": "term-unified-io", "t": "Unified-IO", "tg": ["Models", "Technical"], "d": "models", "x": "A model that unifies diverse vision and language tasks in a single architecture using a sequence-to-sequence framework....", "l": "u", "k": ["unified-io", "model", "unifies", "diverse", "vision", "language", "tasks", "single", "architecture", "sequence-to-sequence", "framework", "handles", "image", "generation", "segmentation"]}, {"id": "term-uniform-cost-search", "t": "Uniform Cost Search", "tg": ["Algorithms", "Technical", "Graph", "Searching"], "d": "algorithms", "x": "A graph search algorithm that expands the node with the lowest cumulative path cost. Equivalent to Dijkstra's algorithm...", "l": "u", "k": ["uniform", "cost", "search", "graph", "algorithm", "expands", "node", "lowest", "cumulative", "path", "equivalent", "dijkstra", "guarantees", "finding", "optimal"]}, {"id": "term-uniform-distribution", "t": "Uniform Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A probability distribution where all outcomes in a given range are equally likely. The continuous version has constant...", "l": "u", "k": ["uniform", "distribution", "probability", "outcomes", "given", "range", "equally", "likely", "continuous", "version", "constant", "density", "interval", "discrete", "assigns"]}, {"id": "term-uniform-manifold-approximation", "t": "Uniform Manifold Approximation", "tg": ["Algorithms", "Technical", "Dimensionality Reduction"], "d": "algorithms", "x": "A theoretical framework underlying UMAP that models data as lying on a Riemannian manifold. Uses fuzzy simplicial sets...", "l": "u", "k": ["uniform", "manifold", "approximation", "theoretical", "framework", "underlying", "umap", "models", "data", "lying", "riemannian", "uses", "fuzzy", "simplicial", "sets"]}, {"id": "term-unigram-language-model-tokenizer", "t": "Unigram Language Model Tokenizer", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A subword tokenization method that starts with a large vocabulary and iteratively removes tokens that least affect the...", "l": "u", "k": ["unigram", "language", "model", "tokenizer", "subword", "tokenization", "method", "starts", "large", "vocabulary", "iteratively", "removes", "tokens", "least", "affect"]}, {"id": "term-unigram-tokenization", "t": "Unigram Tokenization", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A subword tokenization method that starts with a large vocabulary and iteratively removes tokens whose loss has the...", "l": "u", "k": ["unigram", "tokenization", "subword", "method", "starts", "large", "vocabulary", "iteratively", "removes", "tokens", "whose", "loss", "least", "impact", "overall"]}, {"id": "term-unigram-tokenizer", "t": "Unigram Tokenizer", "tg": ["LLM", "NLP"], "d": "models", "x": "A subword tokenization algorithm that starts with a large vocabulary and iteratively removes tokens to minimize the...", "l": "u", "k": ["unigram", "tokenizer", "subword", "tokenization", "algorithm", "starts", "large", "vocabulary", "iteratively", "removes", "tokens", "minimize", "overall", "loss", "training"]}, {"id": "term-unimatch", "t": "Unimatch", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A unified model for dense matching tasks including optical flow and stereo matching and depth estimation using a shared...", "l": "u", "k": ["unimatch", "unified", "model", "dense", "matching", "tasks", "including", "optical", "flow", "stereo", "depth", "estimation", "shared", "transformer", "architecture"]}, {"id": "term-unimate", "t": "Unimate", "tg": ["History", "Systems"], "d": "history", "x": "The first industrial robot installed on a General Motors assembly line in 1961. Developed by George Devol and Joseph...", "l": "u", "k": ["unimate", "industrial", "robot", "installed", "general", "motors", "assembly", "line", "developed", "george", "devol", "joseph", "engelberger", "performed", "tasks"]}, {"id": "term-uninformative-prior", "t": "Uninformative Prior", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A prior distribution that expresses minimal information about the parameter before observing data, such as a uniform...", "l": "u", "k": ["uninformative", "prior", "distribution", "expresses", "minimal", "information", "parameter", "observing", "data", "uniform", "space", "allowing", "dominate", "posterior"]}, {"id": "term-unintended-consequences-of-ai", "t": "Unintended Consequences of AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "Outcomes of AI deployment that were not anticipated or intended by developers. Arise from complex interactions between...", "l": "u", "k": ["unintended", "consequences", "outcomes", "deployment", "were", "anticipated", "intended", "developers", "arise", "complex", "interactions", "systems", "social", "environments", "highlight"]}, {"id": "term-uninterruptible-power-supply", "t": "Uninterruptible Power Supply", "tg": ["Data Center", "Power", "Reliability"], "d": "hardware", "x": "Battery backup system that provides emergency power to computing equipment during outages. Critical for AI data centers...", "l": "u", "k": ["uninterruptible", "power", "supply", "battery", "backup", "system", "provides", "emergency", "computing", "equipment", "outages", "critical", "data", "centers", "prevent"]}, {"id": "term-unisim", "t": "UniSim", "tg": ["Models", "Technical", "Vision", "Robotics"], "d": "models", "x": "A Universal Simulator that generates realistic experience through generative simulation for training embodied agents in...", "l": "u", "k": ["unisim", "universal", "simulator", "generates", "realistic", "experience", "generative", "simulation", "training", "embodied", "agents", "diverse", "interactive", "environments"]}, {"id": "term-unispeech", "t": "UniSpeech", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A self-supervised speech representation model from Microsoft that uses contrastive learning with phonetic labels for...", "l": "u", "k": ["unispeech", "self-supervised", "speech", "representation", "model", "microsoft", "uses", "contrastive", "learning", "phonetic", "labels", "improved", "recognition", "understanding"]}, {"id": "term-univac-i", "t": "UNIVAC I", "tg": ["Historical", "Computer", "Commercial"], "d": "hardware", "x": "First commercial computer in the United States delivered to the US Census Bureau in 1951. Built by Eckert and Mauchly...", "l": "u", "k": ["univac", "commercial", "computer", "united", "states", "delivered", "census", "bureau", "built", "eckert", "mauchly", "famously", "predicted", "presidential", "election"]}, {"id": "term-universal-approximation-theorem", "t": "Universal Approximation Theorem", "tg": ["History", "Fundamentals"], "d": "history", "x": "A theorem proving that feedforward neural networks with a single hidden layer containing a finite number of neurons can...", "l": "u", "k": ["universal", "approximation", "theorem", "proving", "feedforward", "neural", "networks", "single", "hidden", "layer", "containing", "finite", "number", "neurons", "approximate"]}, {"id": "term-universal-basic-income-and-ai", "t": "Universal Basic Income and AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "Proposals for unconditional periodic cash payments to all citizens, discussed as a potential policy response to...", "l": "u", "k": ["universal", "basic", "income", "proposals", "unconditional", "periodic", "cash", "payments", "citizens", "discussed", "potential", "policy", "response", "widespread", "job"]}, {"id": "term-universal-dependencies", "t": "Universal Dependencies", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A cross-linguistically consistent framework for annotating grammar including parts of speech, morphological features,...", "l": "u", "k": ["universal", "dependencies", "cross-linguistically", "consistent", "framework", "annotating", "grammar", "including", "parts", "speech", "morphological", "features", "syntactic", "enabling", "multilingual"]}, {"id": "term-universal-sentence-encoder", "t": "Universal Sentence Encoder", "tg": ["Models", "Technical"], "d": "models", "x": "A model that encodes text into high-dimensional vectors for transfer learning across NLP tasks. Available in...", "l": "u", "k": ["universal", "sentence", "encoder", "model", "encodes", "text", "high-dimensional", "vectors", "transfer", "learning", "across", "nlp", "tasks", "available", "transformer"]}, {"id": "term-universal-turing-machine", "t": "Universal Turing Machine", "tg": ["History", "Fundamentals"], "d": "history", "x": "A Turing machine that can simulate any other Turing machine given a description of that machine on its tape. Proposed...", "l": "u", "k": ["universal", "turing", "machine", "simulate", "given", "description", "tape", "proposed", "alan", "concept", "anticipated", "stored-program", "computers", "established", "single"]}, {"id": "term-unscented-kalman-filter", "t": "Unscented Kalman Filter", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A nonlinear state estimation method that uses carefully chosen sigma points to capture the mean and covariance of the...", "l": "u", "k": ["unscented", "kalman", "filter", "nonlinear", "state", "estimation", "method", "uses", "carefully", "chosen", "sigma", "points", "capture", "mean", "covariance"]}, {"id": "term-unstructured-pruning", "t": "Unstructured Pruning", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A pruning approach that removes individual weights regardless of their position in the weight matrix, resulting in...", "l": "u", "k": ["unstructured", "pruning", "approach", "removes", "individual", "weights", "regardless", "position", "weight", "matrix", "resulting", "sparse", "matrices", "achieves", "higher"]}, {"id": "term-unsupervised-environment-design", "t": "Unsupervised Environment Design", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A framework where a teacher agent or mechanism automatically generates training environments at the frontier of the...", "l": "u", "k": ["unsupervised", "environment", "design", "framework", "teacher", "agent", "mechanism", "automatically", "generates", "training", "environments", "frontier", "student", "capabilities", "methods"]}, {"id": "term-unsupervised-learning", "t": "Unsupervised Learning", "tg": ["Learning Type", "Fundamentals"], "d": "general", "x": "Machine learning from unlabeled data, discovering patterns without explicit guidance. Includes clustering,...", "l": "u", "k": ["unsupervised", "learning", "machine", "unlabeled", "data", "discovering", "patterns", "without", "explicit", "guidance", "includes", "clustering", "dimensionality", "reduction", "self-supervised"]}, {"id": "term-untether-ai", "t": "Untether AI", "tg": ["Accelerator", "Startup", "Architecture"], "d": "hardware", "x": "AI hardware company developing at-memory compute architectures that place processing elements directly at SRAM banks to...", "l": "u", "k": ["untether", "hardware", "company", "developing", "at-memory", "compute", "architectures", "place", "processing", "elements", "directly", "sram", "banks", "minimize", "data"]}, {"id": "term-upper-confidence-bound", "t": "Upper Confidence Bound", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A family of bandit algorithms that select the action with the highest upper confidence bound on its expected reward,...", "l": "u", "k": ["upper", "confidence", "bound", "family", "bandit", "algorithms", "select", "action", "highest", "expected", "reward", "combining", "estimated", "exploration", "bonus"]}, {"id": "term-upsampling", "t": "Upsampling", "tg": ["Technique", "Data"], "d": "general", "x": "Increasing resolution or quantity of data. In image AI, upscaling low-res images. In training, duplicating...", "l": "u", "k": ["upsampling", "increasing", "resolution", "quantity", "data", "image", "upscaling", "low-res", "images", "training", "duplicating", "underrepresented", "examples", "balance", "datasets"]}, {"id": "term-urbansound8k", "t": "UrbanSound8K", "tg": ["Benchmark", "Audio"], "d": "datasets", "x": "A dataset of 8732 labeled sound excerpts of urban sounds in 10 categories including sirens drilling and street music....", "l": "u", "k": ["urbansound8k", "dataset", "labeled", "sound", "excerpts", "urban", "sounds", "categories", "including", "sirens", "drilling", "street", "music", "environment", "audio"]}, {"id": "term-use-case", "t": "Use Case", "tg": ["Concept", "Application"], "d": "general", "x": "A specific application or scenario where AI provides value. Understanding your use case helps choose the right model,...", "l": "u", "k": ["case", "specific", "application", "scenario", "provides", "value", "understanding", "helps", "choose", "right", "model", "prompting", "strategy", "safety", "measures"]}, {"id": "term-user-prompt", "t": "User Prompt", "tg": ["Prompting", "Concept"], "d": "general", "x": "The input provided by the user in a conversation, as opposed to the system prompt set by developers. Together with...", "l": "u", "k": ["user", "prompt", "input", "provided", "conversation", "opposed", "system", "developers", "together", "prompts", "form", "complete", "context", "responses"]}, {"id": "term-usm", "t": "USM", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "Universal Speech Model is a 2 billion parameter model from Google trained on 12 million hours of speech for automatic...", "l": "u", "k": ["usm", "universal", "speech", "model", "billion", "parameter", "google", "trained", "million", "hours", "automatic", "recognition", "across", "languages"]}, {"id": "term-utility-function", "t": "Utility Function", "tg": ["Concept", "Alignment"], "d": "safety", "x": "A mathematical function that measures how \"good\" an outcome is. In AI alignment, designing utility functions that...", "l": "u", "k": ["utility", "function", "mathematical", "measures", "good", "outcome", "alignment", "designing", "functions", "capture", "human", "values", "fundamental", "challenge"]}, {"id": "term-uzawa-algorithm", "t": "Uzawa Algorithm", "tg": ["Algorithms", "Technical", "Optimization", "Numerical"], "d": "algorithms", "x": "An iterative method for solving saddle-point problems arising from constrained optimization. Alternates between solving...", "l": "u", "k": ["uzawa", "algorithm", "iterative", "method", "solving", "saddle-point", "problems", "arising", "constrained", "optimization", "alternates", "system", "primal", "variable", "updating"]}, {"id": "term-v-trace-algorithm", "t": "V-Trace Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "An off-policy correction method for reinforcement learning that uses truncated importance sampling ratios. Designed for...", "l": "v", "k": ["v-trace", "algorithm", "off-policy", "correction", "method", "reinforcement", "learning", "uses", "truncated", "importance", "sampling", "ratios", "designed", "impala", "architecture"]}, {"id": "term-vacuum-tube", "t": "Vacuum Tube", "tg": ["Historical", "Component", "Fundamentals"], "d": "hardware", "x": "Electronic amplification device used in the earliest electronic computers before transistors. ENIAC used 18000 vacuum...", "l": "v", "k": ["vacuum", "tube", "electronic", "amplification", "device", "earliest", "computers", "transistors", "eniac", "tubes", "making", "enormous", "unreliable", "power-hungry", "modern"]}, {"id": "term-vad", "t": "VAD", "tg": ["Models", "Technical", "Autonomous"], "d": "models", "x": "Vectorized Autonomous Driving is an end-to-end framework that represents driving scenes as vectorized elements for...", "l": "v", "k": ["vad", "vectorized", "autonomous", "driving", "end-to-end", "framework", "represents", "scenes", "elements", "planning-oriented"]}, {"id": "term-variational-autoencoder", "t": "VAE (Variational Autoencoder)", "tg": ["Architecture", "Generative"], "d": "models", "x": "A generative model that learns a latent space representation of data. Used in image generation and as components of...", "l": "v", "k": ["vae", "variational", "autoencoder", "generative", "model", "learns", "latent", "space", "representation", "data", "image", "generation", "components", "larger", "systems"]}, {"id": "term-vae-decoder", "t": "VAE Decoder", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "The decoder component of a Variational Autoencoder used in latent diffusion models to reconstruct high-resolution...", "l": "v", "k": ["vae", "decoder", "component", "variational", "autoencoder", "latent", "diffusion", "models", "reconstruct", "high-resolution", "images", "compressed", "representations", "produced", "process"]}, {"id": "term-validation", "t": "Validation", "tg": ["Process", "Evaluation"], "d": "datasets", "x": "Testing model performance on data not used in training to assess generalization. Validation sets help tune...", "l": "v", "k": ["validation", "testing", "model", "performance", "data", "training", "assess", "generalization", "sets", "help", "tune", "hyperparameters", "test", "provide", "final"]}, {"id": "term-validation-curve", "t": "Validation Curve", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A plot of training and validation scores as a function of a single hyperparameter, revealing the optimal hyperparameter...", "l": "v", "k": ["validation", "curve", "plot", "training", "scores", "function", "single", "hyperparameter", "revealing", "optimal", "value", "model", "underfitting", "overfitting", "different"]}, {"id": "term-vall-e", "t": "VALL-E", "tg": ["Models", "Technical"], "d": "models", "x": "A neural codec language model for text-to-speech by Microsoft that can synthesize personalized speech from a 3-second...", "l": "v", "k": ["vall-e", "neural", "codec", "language", "model", "text-to-speech", "microsoft", "synthesize", "personalized", "speech", "3-second", "reference", "recording", "treats", "tts"]}, {"id": "term-value-alignment", "t": "Value Alignment", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The challenge of ensuring that an AI system's objectives and behaviors are consistent with human values and intentions....", "l": "v", "k": ["value", "alignment", "challenge", "ensuring", "system", "objectives", "behaviors", "consistent", "human", "values", "intentions", "considered", "core", "problems", "safety"]}, {"id": "term-value-function", "t": "Value Function", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A function that estimates the expected cumulative future reward from a given state (or state-action pair) under a...", "l": "v", "k": ["value", "function", "estimates", "expected", "cumulative", "future", "reward", "given", "state", "state-action", "pair", "particular", "policy", "functions", "central"]}, {"id": "term-value-iteration", "t": "Value Iteration", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A dynamic programming algorithm that computes the optimal value function by iteratively applying the Bellman optimality...", "l": "v", "k": ["value", "iteration", "dynamic", "programming", "algorithm", "computes", "optimal", "function", "iteratively", "applying", "bellman", "optimality", "equation", "converges", "policy"]}, {"id": "term-value-pluralism-in-ai", "t": "Value Pluralism in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "The recognition that there are multiple legitimate and sometimes conflicting values that cannot all be simultaneously...", "l": "v", "k": ["value", "pluralism", "recognition", "multiple", "legitimate", "sometimes", "conflicting", "values", "cannot", "simultaneously", "maximized", "system", "design", "requires", "deliberation"]}, {"id": "term-value-sensitive-design", "t": "Value-Sensitive Design", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "A design methodology that accounts for human values in a principled and systematic manner throughout the design...", "l": "v", "k": ["value-sensitive", "design", "methodology", "accounts", "human", "values", "principled", "systematic", "manner", "throughout", "process", "incorporating", "conceptual", "empirical", "technical"]}, {"id": "term-van-emde-boas-tree", "t": "Van Emde Boas Tree", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A tree data structure that supports search and insert and delete and predecessor and successor operations in O(log log...", "l": "v", "k": ["van", "emde", "boas", "tree", "data", "structure", "supports", "search", "insert", "delete", "predecessor", "successor", "operations", "log", "time"]}, {"id": "term-vanishing-gradient", "t": "Vanishing Gradient", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A problem in deep neural network training where gradients become exponentially small as they are backpropagated through...", "l": "v", "k": ["vanishing", "gradient", "problem", "deep", "neural", "network", "training", "gradients", "become", "exponentially", "small", "backpropagated", "layers", "causing", "early"]}, {"id": "term-vanishing-gradient-problem", "t": "Vanishing Gradient Problem", "tg": ["History", "Milestones"], "d": "history", "x": "A difficulty encountered in training deep neural networks where gradients become exponentially small as they are...", "l": "v", "k": ["vanishing", "gradient", "problem", "difficulty", "encountered", "training", "deep", "neural", "networks", "gradients", "become", "exponentially", "small", "propagated", "backward"]}, {"id": "term-vantage-point-tree", "t": "Vantage-Point Tree", "tg": ["Algorithms", "Technical", "Searching", "Data Structure"], "d": "algorithms", "x": "A spatial data structure for nearest-neighbor search in metric spaces that partitions points based on distance from a...", "l": "v", "k": ["vantage-point", "tree", "spatial", "data", "structure", "nearest-neighbor", "search", "metric", "spaces", "partitions", "points", "based", "distance", "selected", "vantage"]}, {"id": "term-vapnik-chervonenkis-theory", "t": "Vapnik-Chervonenkis Theory", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A theoretical framework in statistical learning theory that provides bounds on the generalization error of classifiers...", "l": "v", "k": ["vapnik-chervonenkis", "theory", "theoretical", "framework", "statistical", "learning", "provides", "bounds", "generalization", "error", "classifiers", "based", "dimension", "hypothesis", "class"]}, {"id": "term-variable-neighborhood-search", "t": "Variable Neighborhood Search", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A metaheuristic that systematically changes the neighborhood structure during the local search process. Escapes local...", "l": "v", "k": ["variable", "neighborhood", "search", "metaheuristic", "systematically", "changes", "structure", "local", "process", "escapes", "optima", "switching", "different", "definition", "improvement"]}, {"id": "term-variance", "t": "Variance", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A measure of the dispersion of a set of values, computed as the average of the squared deviations from the mean. It...", "l": "v", "k": ["variance", "measure", "dispersion", "values", "computed", "average", "squared", "deviations", "mean", "quantifies", "spread", "data", "points", "distribution"]}, {"id": "term-variance-inflation-factor", "t": "Variance Inflation Factor", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A measure of how much the variance of a regression coefficient is inflated due to multicollinearity with other...", "l": "v", "k": ["variance", "inflation", "factor", "measure", "regression", "coefficient", "inflated", "due", "multicollinearity", "predictors", "vif", "values", "typically", "indicate", "problematic"]}, {"id": "term-variance-threshold", "t": "Variance Threshold", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A simple feature selection method that removes all features whose variance falls below a specified threshold. Features...", "l": "v", "k": ["variance", "threshold", "simple", "feature", "selection", "method", "removes", "features", "whose", "falls", "specified", "near-zero", "provide", "little", "discriminative"]}, {"id": "term-vae", "t": "Variational Autoencoder", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative model that learns a probabilistic mapping between data and a continuous latent space by optimizing a...", "l": "v", "k": ["variational", "autoencoder", "generative", "model", "learns", "probabilistic", "mapping", "data", "continuous", "latent", "space", "optimizing", "lower", "bound", "enabling"]}, {"id": "term-variational-inference", "t": "Variational Inference", "tg": ["Machine Learning", "Bayesian Methods"], "d": "general", "x": "An approximate Bayesian inference technique that transforms the inference problem into an optimization problem by...", "l": "v", "k": ["variational", "inference", "approximate", "bayesian", "technique", "transforms", "problem", "optimization", "finding", "member", "tractable", "distribution", "family", "closest", "true"]}, {"id": "term-variational-inference-algorithm", "t": "Variational Inference Algorithm", "tg": ["Algorithms", "Technical", "Optimization"], "d": "algorithms", "x": "An optimization-based approach to approximate Bayesian inference that transforms the inference problem into an...", "l": "v", "k": ["variational", "inference", "algorithm", "optimization-based", "approach", "approximate", "bayesian", "transforms", "problem", "optimization", "finds", "closest", "distribution", "tractable", "family"]}, {"id": "term-vary", "t": "Vary", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A vision-language model with an expandable visual vocabulary that can be extended to handle new visual perception tasks...", "l": "v", "k": ["vary", "vision-language", "model", "expandable", "visual", "vocabulary", "extended", "handle", "perception", "tasks", "without", "retraining", "full"]}, {"id": "term-vatex", "t": "VATEX", "tg": ["Benchmark", "Video", "NLP", "Multilingual"], "d": "datasets", "x": "A video-and-text dataset containing 41269 videos with both English and Chinese descriptions totaling over 826000...", "l": "v", "k": ["vatex", "video-and-text", "dataset", "containing", "videos", "english", "chinese", "descriptions", "totaling", "captions", "supports", "multilingual", "video", "captioning", "research"]}, {"id": "term-vax", "t": "VAX", "tg": ["Historical", "DEC", "Architecture"], "d": "hardware", "x": "Virtual Address Extension computer architecture from DEC introduced in 1977. Dominated scientific and engineering...", "l": "v", "k": ["vax", "virtual", "address", "extension", "computer", "architecture", "dec", "introduced", "dominated", "scientific", "engineering", "computing", "1980s", "powerful", "memory"]}, {"id": "term-vc-dimension", "t": "VC Dimension", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "Vapnik-Chervonenkis dimension, a measure of the capacity or complexity of a hypothesis class, defined as the largest...", "l": "v", "k": ["dimension", "vapnik-chervonenkis", "measure", "capacity", "complexity", "hypothesis", "class", "defined", "largest", "points", "shattered", "perfectly", "classified", "possible", "labelings"]}, {"id": "term-vctk", "t": "VCTK", "tg": ["Training Corpus", "Speech"], "d": "datasets", "x": "Voice Cloning Toolkit dataset containing speech data from 110 English speakers with various accents. Used for...", "l": "v", "k": ["vctk", "voice", "cloning", "toolkit", "dataset", "containing", "speech", "data", "english", "speakers", "various", "accents", "multi-speaker", "text-to-speech", "conversion"]}, {"id": "term-vector", "t": "Vector", "tg": ["Math", "Representation"], "d": "general", "x": "An ordered list of numbers representing data in a mathematical space. Embeddings are vectors; vector similarity...", "l": "v", "k": ["vector", "ordered", "list", "numbers", "representing", "data", "mathematical", "space", "embeddings", "vectors", "similarity", "measures", "cosine", "enable", "semantic"]}, {"id": "term-vector-autoregression", "t": "Vector Autoregression", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A multivariate time series model where each variable is regressed on its own past values and the past values of all...", "l": "v", "k": ["vector", "autoregression", "multivariate", "time", "series", "model", "variable", "regressed", "past", "values", "variables", "system", "capturing", "linear", "interdependencies"]}, {"id": "term-vector-clock-algorithm", "t": "Vector Clock Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A logical clock system for distributed systems that captures the causal ordering of events. Each process maintains a...", "l": "v", "k": ["vector", "clock", "algorithm", "logical", "system", "distributed", "systems", "captures", "causal", "ordering", "events", "process", "maintains", "timestamps", "enabling"]}, {"id": "term-vector-database", "t": "Vector Database", "tg": ["Infrastructure", "Search"], "d": "hardware", "x": "A database optimized for storing and searching high-dimensional vectors (embeddings). Essential for RAG systems,...", "l": "v", "k": ["vector", "database", "optimized", "storing", "searching", "high-dimensional", "vectors", "embeddings", "essential", "rag", "systems", "semantic", "search", "recommendation", "engines"]}, {"id": "term-vector-database-sharding", "t": "Vector Database Sharding", "tg": ["Vector Database", "Infrastructure"], "d": "hardware", "x": "The horizontal partitioning of a vector index across multiple nodes or storage units to distribute data and query load,...", "l": "v", "k": ["vector", "database", "sharding", "horizontal", "partitioning", "index", "across", "multiple", "nodes", "storage", "units", "distribute", "data", "query", "load"]}, {"id": "term-vector-index", "t": "Vector Index", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "A specialized data structure optimized for efficient similarity search over high-dimensional vector collections,...", "l": "v", "k": ["vector", "index", "specialized", "data", "structure", "optimized", "efficient", "similarity", "search", "high-dimensional", "collections", "employing", "algorithms", "hnsw", "ivf"]}, {"id": "term-vector-institute", "t": "Vector Institute", "tg": ["History", "Organizations"], "d": "history", "x": "A Canadian AI research institute founded in 2017 in Toronto with Geoffrey Hinton as chief scientific advisor. The...", "l": "v", "k": ["vector", "institute", "canadian", "research", "founded", "toronto", "geoffrey", "hinton", "chief", "scientific", "advisor", "focuses", "machine", "learning", "deep"]}, {"id": "term-vector-normalization", "t": "Vector Normalization", "tg": ["Vector Database", "Preprocessing"], "d": "general", "x": "The process of scaling vectors to unit length by dividing each component by the vector's L2 norm, ensuring that cosine...", "l": "v", "k": ["vector", "normalization", "process", "scaling", "vectors", "unit", "length", "dividing", "component", "norm", "ensuring", "cosine", "similarity", "dot", "product"]}, {"id": "term-vector-processor", "t": "Vector Processor", "tg": ["Architecture", "Historical", "Processor"], "d": "hardware", "x": "Processor designed to operate on arrays of data in a single instruction. Historical vector supercomputers like the...", "l": "v", "k": ["vector", "processor", "designed", "operate", "arrays", "data", "single", "instruction", "historical", "supercomputers", "cray-1", "pioneered", "concepts", "now", "fundamental"]}, {"id": "term-vector-similarity-join", "t": "Vector Similarity Join", "tg": ["Vector Database", "Search"], "d": "general", "x": "A database operation that finds all pairs of vectors across two collections whose similarity exceeds a given threshold,...", "l": "v", "k": ["vector", "similarity", "join", "database", "operation", "finds", "pairs", "vectors", "across", "collections", "whose", "exceeds", "given", "threshold", "deduplication"]}, {"id": "term-vector-store", "t": "Vector Store", "tg": ["Vector Database", "Infrastructure"], "d": "hardware", "x": "A storage system specialized for persisting, indexing, and querying vector embeddings alongside their associated...", "l": "v", "k": ["vector", "store", "storage", "system", "specialized", "persisting", "indexing", "querying", "embeddings", "alongside", "associated", "metadata", "original", "content", "serving"]}, {"id": "term-vector-jacobian-product", "t": "Vector-Jacobian Product", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An efficient computation that multiplies a vector by the Jacobian matrix without explicitly forming the full Jacobian....", "l": "v", "k": ["vector-jacobian", "product", "efficient", "computation", "multiplies", "vector", "jacobian", "matrix", "without", "explicitly", "forming", "full", "fundamental", "operation", "reverse-mode"]}, {"id": "term-vectorized-environment", "t": "Vectorized Environment", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A technique for running multiple copies of an environment in parallel within a single process, enabling batch...", "l": "v", "k": ["vectorized", "environment", "technique", "running", "multiple", "copies", "parallel", "within", "single", "process", "enabling", "batch", "collection", "experience", "efficient"]}, {"id": "term-verifiable-ai", "t": "Verifiable AI", "tg": ["Safety", "Technical"], "d": "safety", "x": "AI systems whose properties and behaviors can be formally or empirically verified against specified requirements. An...", "l": "v", "k": ["verifiable", "systems", "whose", "properties", "behaviors", "formally", "empirically", "verified", "against", "specified", "requirements", "aspirational", "goal", "safety-critical", "applications"]}, {"id": "term-verification", "t": "Verification (AI Outputs)", "tg": ["Practice", "Safety"], "d": "safety", "x": "Checking AI outputs for accuracy and appropriateness before use. Essential for high-stakes applications. Can be done by...", "l": "v", "k": ["verification", "outputs", "checking", "accuracy", "appropriateness", "essential", "high-stakes", "applications", "done", "humans", "systems", "automated", "checks"]}, {"id": "term-verification-chain-prompting", "t": "Verification Chain Prompting", "tg": ["Prompt Engineering", "Verification"], "d": "general", "x": "A prompting technique that generates an initial response, then systematically creates and answers verification...", "l": "v", "k": ["verification", "chain", "prompting", "technique", "generates", "initial", "response", "systematically", "creates", "answers", "questions", "specific", "claims", "results", "produce"]}, {"id": "term-verilog", "t": "Verilog", "tg": ["Manufacturing", "Design", "Language"], "d": "hardware", "x": "Hardware description language used to model and design digital circuits including AI accelerator chips. One of the two...", "l": "v", "k": ["verilog", "hardware", "description", "language", "model", "design", "digital", "circuits", "including", "accelerator", "chips", "dominant", "hdls", "alongside", "vhdl"]}, {"id": "term-verlet-integration", "t": "Verlet Integration", "tg": ["Algorithms", "Technical", "Numerical"], "d": "algorithms", "x": "A numerical method for integrating equations of motion that computes positions directly without explicitly computing...", "l": "v", "k": ["verlet", "integration", "numerical", "method", "integrating", "equations", "motion", "computes", "positions", "directly", "without", "explicitly", "computing", "velocities", "widely"]}, {"id": "term-versal-acap", "t": "Versal ACAP", "tg": ["FPGA", "AMD", "Adaptive"], "d": "hardware", "x": "AMD (formerly Xilinx) Adaptive Compute Acceleration Platform combining FPGA fabric with AI engines and ARM processors....", "l": "v", "k": ["versal", "acap", "amd", "formerly", "xilinx", "adaptive", "compute", "acceleration", "platform", "combining", "fpga", "fabric", "engines", "arm", "processors"]}, {"id": "term-vertex-cover-algorithm", "t": "Vertex Cover Algorithm", "tg": ["Algorithms", "Technical", "Graph", "Optimization"], "d": "algorithms", "x": "An algorithm that finds the smallest set of vertices such that every edge in the graph has at least one endpoint in the...", "l": "v", "k": ["vertex", "cover", "algorithm", "finds", "smallest", "vertices", "edge", "graph", "least", "endpoint", "decision", "version", "np-complete", "2-approximation", "exists"]}, {"id": "term-vgg", "t": "VGG", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A deep convolutional network architecture characterized by its use of very small 3x3 convolution filters throughout the...", "l": "v", "k": ["vgg", "deep", "convolutional", "network", "architecture", "characterized", "small", "3x3", "convolution", "filters", "throughout", "entire", "demonstrating", "depth", "improves"]}, {"id": "term-vgg-16", "t": "VGG-16", "tg": ["Models", "Technical"], "d": "models", "x": "A 16-layer variant of the VGG architecture with 138 million parameters. One of the most commonly used pretrained models...", "l": "v", "k": ["vgg-16", "16-layer", "variant", "vgg", "architecture", "million", "parameters", "commonly", "pretrained", "models", "transfer", "learning", "feature", "extraction", "despite"]}, {"id": "term-vggface2", "t": "VGGFace2", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A large-scale face recognition dataset containing 3.31 million images of 9131 subjects with large variations in pose...", "l": "v", "k": ["vggface2", "large-scale", "face", "recognition", "dataset", "containing", "million", "images", "subjects", "large", "variations", "pose", "age", "illumination", "ethnicity"]}, {"id": "term-vggsound", "t": "VGGSound", "tg": ["Benchmark", "Audio", "Multimodal"], "d": "datasets", "x": "A dataset of 200000 audio-visual clips from YouTube covering 310 sound classes. Tests audio classification with...", "l": "v", "k": ["vggsound", "dataset", "audio-visual", "clips", "youtube", "covering", "sound", "classes", "tests", "audio", "classification", "corresponding", "visual", "information", "multimodal"]}, {"id": "term-vhdl", "t": "VHDL", "tg": ["Manufacturing", "Design", "Language"], "d": "hardware", "x": "VHSIC Hardware Description Language used for designing and simulating digital electronic systems. Used alongside...", "l": "v", "k": ["vhdl", "vhsic", "hardware", "description", "language", "designing", "simulating", "digital", "electronic", "systems", "alongside", "verilog", "specifying", "behavior", "processor"]}, {"id": "term-vicuna", "t": "Vicuna", "tg": ["Models", "Technical"], "d": "models", "x": "An open-source chatbot fine-tuned from LLaMA on user-shared conversations from ShareGPT. Achieved quality close to...", "l": "v", "k": ["vicuna", "open-source", "chatbot", "fine-tuned", "llama", "user-shared", "conversations", "sharegpt", "achieved", "quality", "close", "chatgpt", "according", "gpt-4", "evaluation"]}, {"id": "term-video-generation", "t": "Video Generation", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "The synthesis of temporally coherent video sequences from text prompts, images, or other conditioning signals using...", "l": "v", "k": ["video", "generation", "synthesis", "temporally", "coherent", "sequences", "text", "prompts", "images", "conditioning", "signals", "extended", "diffusion", "autoregressive", "models"]}, {"id": "term-video-transformer", "t": "Video Transformer", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A transformer architecture adapted for video processing that handles both spatial and temporal dimensions through...", "l": "v", "k": ["video", "transformer", "architecture", "adapted", "processing", "handles", "spatial", "temporal", "dimensions", "factored", "attention", "tubelet", "embeddings", "space-time", "mechanisms"]}, {"id": "term-video-understanding", "t": "Video Understanding", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The comprehensive analysis of video content including action recognition, temporal event detection, scene...", "l": "v", "k": ["video", "understanding", "comprehensive", "analysis", "content", "including", "action", "recognition", "temporal", "event", "detection", "scene", "classification", "narrative", "across"]}, {"id": "term-video-chatgpt", "t": "Video-ChatGPT", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A video conversation model that combines a visual encoder with a large language model to enable detailed discussions...", "l": "v", "k": ["video-chatgpt", "video", "conversation", "model", "combines", "visual", "encoder", "large", "language", "enable", "detailed", "discussions", "content"]}, {"id": "term-video-mme", "t": "Video-MME", "tg": ["Benchmark", "Video", "Multimodal", "Evaluation"], "d": "datasets", "x": "A comprehensive benchmark for evaluating multimodal LLMs on video understanding tasks. Tests temporal reasoning action...", "l": "v", "k": ["video-mme", "comprehensive", "benchmark", "evaluating", "multimodal", "llms", "video", "understanding", "tasks", "tests", "temporal", "reasoning", "action", "recognition", "video-language"]}, {"id": "term-videochat", "t": "VideoChat", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "An end-to-end video-centric multimodal dialogue system that integrates video foundation models with large language...", "l": "v", "k": ["videochat", "end-to-end", "video-centric", "multimodal", "dialogue", "system", "integrates", "video", "foundation", "models", "large", "language", "interactive", "understanding"]}, {"id": "term-videollama", "t": "VideoLLaMA", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A multimodal large language model that integrates visual and auditory encoders with a language model for video-based...", "l": "v", "k": ["videollama", "multimodal", "large", "language", "model", "integrates", "visual", "auditory", "encoders", "video-based", "conversations", "understanding"]}, {"id": "term-videomae", "t": "VideoMAE", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A self-supervised video representation learning method that uses masked autoencoders on video data with extremely high...", "l": "v", "k": ["videomae", "self-supervised", "video", "representation", "learning", "method", "uses", "masked", "autoencoders", "data", "extremely", "high", "masking", "ratios", "efficient"]}, {"id": "term-vila", "t": "VILA", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "Visual Language Models Are Also Good Video Understanders is a multimodal model that extends image-language pre-training...", "l": "v", "k": ["vila", "visual", "language", "models", "good", "video", "understanders", "multimodal", "model", "extends", "image-language", "pre-training", "understanding", "frame", "sampling"]}, {"id": "term-vima", "t": "VIMA", "tg": ["Models", "Technical", "Robotics", "Vision", "NLP"], "d": "models", "x": "Visuomotor Attention agent that uses multimodal prompts combining text and images and video to specify diverse robot...", "l": "v", "k": ["vima", "visuomotor", "attention", "agent", "uses", "multimodal", "prompts", "combining", "text", "images", "video", "specify", "diverse", "robot", "manipulation"]}, {"id": "term-vimeo-90k", "t": "Vimeo-90K", "tg": ["Training Corpus", "Video"], "d": "datasets", "x": "A large-scale high-quality video dataset containing 89800 video clips for video processing tasks. Used for training...", "l": "v", "k": ["vimeo-90k", "large-scale", "high-quality", "video", "dataset", "containing", "clips", "processing", "tasks", "training", "super-resolution", "frame", "interpolation", "denoising", "models"]}, {"id": "term-viola-jones-detection", "t": "Viola-Jones Detection", "tg": ["Algorithms", "Fundamentals", "Vision", "History"], "d": "algorithms", "x": "A real-time object detection framework that uses Haar-like features computed on integral images with a cascade of...", "l": "v", "k": ["viola-jones", "detection", "real-time", "object", "framework", "uses", "haar-like", "features", "computed", "integral", "images", "cascade", "boosted", "classifiers", "achieved"]}, {"id": "term-virtual-gpu", "t": "Virtual GPU", "tg": ["Virtualization", "GPU", "Cloud"], "d": "hardware", "x": "Technology that partitions a physical GPU into multiple virtual GPUs each assignable to a separate virtual machine....", "l": "v", "k": ["virtual", "gpu", "technology", "partitions", "physical", "multiple", "gpus", "assignable", "separate", "machine", "nvidia", "vgpu", "enables", "sharing", "cloud"]}, {"id": "term-virtue-ethics-in-ai", "t": "Virtue Ethics in AI", "tg": ["Safety", "Ethics"], "d": "safety", "x": "An approach to AI ethics that focuses on cultivating good character and judgment in AI practitioners rather than...", "l": "v", "k": ["virtue", "ethics", "approach", "focuses", "cultivating", "good", "character", "judgment", "practitioners", "rather", "following", "rules", "maximizing", "outcomes", "emphasizes"]}, {"id": "term-visda", "t": "VisDA", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "Visual Domain Adaptation challenge datasets providing synthetic-to-real transfer benchmarks. Tests the ability of...", "l": "v", "k": ["visda", "visual", "domain", "adaptation", "challenge", "datasets", "providing", "synthetic-to-real", "transfer", "benchmarks", "tests", "ability", "models", "trained", "synthetic"]}, {"id": "term-vision-transformer", "t": "Vision Transformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer architecture applied to images by splitting them into fixed-size patches, linearly embedding each patch,...", "l": "v", "k": ["vision", "transformer", "architecture", "applied", "images", "splitting", "fixed-size", "patches", "linearly", "embedding", "patch", "processing", "sequence", "embeddings", "standard"]}, {"id": "term-vision-language-model", "t": "Vision-Language Model (VLM)", "tg": ["Model Type", "Multimodal"], "d": "models", "x": "AI models that can process and reason about both images and text. Examples include GPT-4V, Claude with vision, and...", "l": "v", "k": ["vision-language", "model", "vlm", "models", "process", "reason", "images", "text", "examples", "include", "gpt-4v", "claude", "vision", "gemini", "enable"]}, {"id": "term-visual-commonsense-reasoning", "t": "Visual Commonsense Reasoning", "tg": ["Benchmark", "Multimodal", "Reasoning"], "d": "datasets", "x": "A dataset requiring models to answer visual questions and provide rationales for their answers. Goes beyond VQA by...", "l": "v", "k": ["visual", "commonsense", "reasoning", "dataset", "requiring", "models", "answer", "questions", "provide", "rationales", "answers", "goes", "beyond", "vqa", "explicit"]}, {"id": "term-visual-entailment", "t": "Visual Entailment", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "A visual NLI dataset where models determine whether an image semantically entails a text hypothesis. Extends textual...", "l": "v", "k": ["visual", "entailment", "nli", "dataset", "models", "determine", "image", "semantically", "entails", "text", "hypothesis", "extends", "textual", "multimodal", "setting"]}, {"id": "term-visual-genome", "t": "Visual Genome", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "A dataset of 108000 images with dense annotations including region descriptions objects attributes relationships and...", "l": "v", "k": ["visual", "genome", "dataset", "images", "dense", "annotations", "including", "region", "descriptions", "objects", "attributes", "relationships", "question-answer", "pairs", "provides"]}, {"id": "term-visual-grounding", "t": "Visual Grounding", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of localizing a region or object in an image based on a natural language description, requiring the model to...", "l": "v", "k": ["visual", "grounding", "task", "localizing", "region", "object", "image", "based", "natural", "language", "description", "requiring", "model", "ground", "textual"]}, {"id": "term-visual-question-answering", "t": "Visual Question Answering", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A multimodal AI task that requires answering natural language questions about the content of an image, demanding both...", "l": "v", "k": ["visual", "question", "answering", "multimodal", "task", "requires", "natural", "language", "questions", "content", "image", "demanding", "understanding", "reasoning", "capabilities"]}, {"id": "term-visual-relationship-detection", "t": "Visual Relationship Detection", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of identifying and classifying the interactions or spatial relationships between pairs of objects in an image,...", "l": "v", "k": ["visual", "relationship", "detection", "task", "identifying", "classifying", "interactions", "spatial", "relationships", "pairs", "objects", "image", "producing", "subject-predicate-object", "triplets"]}, {"id": "term-visual-slam", "t": "Visual SLAM", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "Visual Simultaneous Localization and Mapping, a technique that estimates camera trajectory and builds a 3D map of the...", "l": "v", "k": ["visual", "slam", "simultaneous", "localization", "mapping", "technique", "estimates", "camera", "trajectory", "builds", "map", "environment", "sequence", "images", "real-time"]}, {"id": "term-visualwebarena", "t": "VisualWebArena", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "An extension of WebArena that tests multimodal web agents on tasks requiring visual understanding. Combines web...", "l": "v", "k": ["visualwebarena", "extension", "webarena", "tests", "multimodal", "web", "agents", "tasks", "requiring", "visual", "understanding", "combines", "navigation", "image", "layout"]}, {"id": "term-viterbi-algorithm", "t": "Viterbi Algorithm", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A dynamic programming algorithm that finds the most likely sequence of hidden states in an HMM or CRF, used for optimal...", "l": "v", "k": ["viterbi", "algorithm", "dynamic", "programming", "finds", "likely", "sequence", "hidden", "states", "hmm", "crf", "optimal", "decoding", "labeling", "speech"]}, {"id": "term-vits", "t": "VITS", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "Variational Inference with adversarial learning for end-to-end Text-to-Speech is a parallel TTS model combining VAEs...", "l": "v", "k": ["vits", "variational", "inference", "adversarial", "learning", "end-to-end", "text-to-speech", "parallel", "tts", "model", "combining", "vaes", "normalizing", "flows", "training"]}, {"id": "term-vivit", "t": "ViViT", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "Video Vision Transformer from Google that extends the Vision Transformer architecture to video by extracting...", "l": "v", "k": ["vivit", "video", "vision", "transformer", "google", "extends", "architecture", "extracting", "spatiotemporal", "tokens", "input", "clips"]}, {"id": "term-vizwiz", "t": "VizWiz", "tg": ["Benchmark", "Multimodal", "Accessibility"], "d": "datasets", "x": "A visual question answering dataset where questions are asked by blind people taking photos with smartphones. Contains...", "l": "v", "k": ["vizwiz", "visual", "question", "answering", "dataset", "questions", "asked", "blind", "people", "taking", "photos", "smartphones", "contains", "image-question", "pairs"]}, {"id": "term-vladimir-vapnik", "t": "Vladimir Vapnik", "tg": ["History", "Pioneers"], "d": "history", "x": "Russian-American mathematician who co-developed the Vapnik-Chervonenkis (VC) theory of statistical learning and...", "l": "v", "k": ["vladimir", "vapnik", "russian-american", "mathematician", "co-developed", "vapnik-chervonenkis", "theory", "statistical", "learning", "invented", "support", "vector", "machines", "svms", "1990s"]}, {"id": "term-vllm", "t": "vLLM", "tg": ["LLM", "Inference"], "d": "models", "x": "An open-source high-throughput LLM inference engine that implements PagedAttention and continuous batching to...", "l": "v", "k": ["vllm", "open-source", "high-throughput", "llm", "inference", "engine", "implements", "pagedattention", "continuous", "batching", "efficiently", "serve", "large", "language", "models"]}, {"id": "term-vocab", "t": "Vocabulary (Model)", "tg": ["Technical", "Tokenization"], "d": "general", "x": "The set of tokens a model can recognize and generate. Determined during tokenizer training. Larger vocabularies handle...", "l": "v", "k": ["vocabulary", "model", "tokens", "recognize", "generate", "determined", "tokenizer", "training", "larger", "vocabularies", "handle", "languages", "increase", "size"]}, {"id": "term-vocabulary-size", "t": "Vocabulary Size", "tg": ["NLP", "Tokenization"], "d": "general", "x": "The total number of unique tokens in a model's tokenizer vocabulary, which affects model size, tokenization efficiency,...", "l": "v", "k": ["vocabulary", "size", "total", "number", "unique", "tokens", "model", "tokenizer", "affects", "tokenization", "efficiency", "balance", "sequence", "length", "token"]}, {"id": "term-vocoder-algorithm", "t": "Vocoder Algorithm", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A signal processing system that analyzes and resynthesizes speech by separating the spectral envelope from the...", "l": "v", "k": ["vocoder", "algorithm", "signal", "processing", "system", "analyzes", "resynthesizes", "speech", "separating", "spectral", "envelope", "excitation", "phase", "vocoders", "enable"]}, {"id": "term-vocos", "t": "Vocos", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A neural vocoder that operates in the frequency domain using inverse short-time Fourier transforms for efficient and...", "l": "v", "k": ["vocos", "neural", "vocoder", "operates", "frequency", "domain", "inverse", "short-time", "fourier", "transforms", "efficient", "high-quality", "audio", "waveform", "generation"]}, {"id": "term-voice-clone", "t": "Voice Cloning", "tg": ["Application", "Ethics"], "d": "safety", "x": "AI that replicates a specific person's voice from audio samples. Raises significant ethical concerns about consent and...", "l": "v", "k": ["voice", "cloning", "replicates", "specific", "person", "audio", "samples", "raises", "significant", "ethical", "concerns", "consent", "misuse", "fraud", "misinformation"]}, {"id": "term-voltage-regulator-module", "t": "Voltage Regulator Module", "tg": ["Component", "Power", "Hardware"], "d": "hardware", "x": "Electronic component that converts higher voltage power supply to the precise lower voltages required by processor...", "l": "v", "k": ["voltage", "regulator", "module", "electronic", "component", "converts", "higher", "power", "supply", "precise", "lower", "voltages", "required", "processor", "chips"]}, {"id": "term-voluntary-commitments-on-ai", "t": "Voluntary Commitments on AI", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Non-binding pledges by AI companies to the White House in 2023 to manage risks from AI, including commitments to safety...", "l": "v", "k": ["voluntary", "commitments", "non-binding", "pledges", "companies", "white", "house", "manage", "risks", "including", "safety", "testing", "information", "sharing", "watermarking"]}, {"id": "term-von-neumann-architecture", "t": "Von Neumann Architecture", "tg": ["History", "Milestones"], "d": "history", "x": "The computer architecture described by John von Neumann in 1945 that stores both program instructions and data in the...", "l": "v", "k": ["von", "neumann", "architecture", "computer", "described", "john", "stores", "program", "instructions", "data", "memory", "becoming", "dominant", "design", "paradigm"]}, {"id": "term-von-neumann-bottleneck", "t": "Von Neumann Bottleneck", "tg": ["Architecture", "Fundamentals", "Limitation"], "d": "hardware", "x": "Performance limitation caused by the shared bus between processor and memory in Von Neumann architecture. Drives the...", "l": "v", "k": ["von", "neumann", "bottleneck", "performance", "limitation", "caused", "shared", "bus", "processor", "memory", "architecture", "drives", "development", "alternative", "architectures"]}, {"id": "term-voronoi-diagram-algorithm", "t": "Voronoi Diagram Algorithm", "tg": ["Algorithms", "Fundamentals", "Vision"], "d": "algorithms", "x": "An algorithm that partitions a plane into regions based on proximity to a set of seed points. Each region contains all...", "l": "v", "k": ["voronoi", "diagram", "algorithm", "partitions", "plane", "regions", "based", "proximity", "seed", "points", "region", "contains", "closer", "applications", "spatial"]}, {"id": "term-voronoi-partitioning", "t": "Voronoi Partitioning", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "A spatial decomposition technique used in IVF indexes that divides the vector space into regions where each region...", "l": "v", "k": ["voronoi", "partitioning", "spatial", "decomposition", "technique", "ivf", "indexes", "divides", "vector", "space", "regions", "region", "contains", "points", "closest"]}, {"id": "term-voting-classifier", "t": "Voting Classifier", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An ensemble method that aggregates predictions from multiple classifiers using either majority voting (hard voting) or...", "l": "v", "k": ["voting", "classifier", "ensemble", "method", "aggregates", "predictions", "multiple", "classifiers", "majority", "hard", "averaged", "predicted", "probabilities", "soft", "produce"]}, {"id": "term-voxceleb", "t": "VoxCeleb", "tg": ["Benchmark", "Speech"], "d": "datasets", "x": "A large-scale speaker recognition dataset containing over 100000 utterances from 1251 celebrities extracted from...", "l": "v", "k": ["voxceleb", "large-scale", "speaker", "recognition", "dataset", "containing", "utterances", "celebrities", "extracted", "youtube", "videos", "verification", "identification", "research"]}, {"id": "term-voxceleb2", "t": "VoxCeleb2", "tg": ["Benchmark", "Speech"], "d": "datasets", "x": "An expanded speaker recognition dataset with over 1 million utterances from 6112 speakers. Provides greater speaker...", "l": "v", "k": ["voxceleb2", "expanded", "speaker", "recognition", "dataset", "million", "utterances", "speakers", "provides", "greater", "diversity", "acoustic", "variability", "original", "voxceleb"]}, {"id": "term-voxel", "t": "Voxel", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A volumetric pixel representing a value on a regular 3D grid, used as a discrete representation for 3D data in neural...", "l": "v", "k": ["voxel", "volumetric", "pixel", "representing", "value", "regular", "grid", "discrete", "representation", "data", "neural", "networks", "analogous", "pixels", "images"]}, {"id": "term-voxelnet", "t": "VoxelNet", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "A 3D object detection model that divides lidar point clouds into 3D voxels and processes them with sparse convolutions...", "l": "v", "k": ["voxelnet", "object", "detection", "model", "divides", "lidar", "point", "clouds", "voxels", "processes", "sparse", "convolutions", "autonomous", "driving"]}, {"id": "term-voxlingua107", "t": "VoxLingua107", "tg": ["Benchmark", "Speech", "Multilingual"], "d": "datasets", "x": "A dataset for spoken language identification covering 107 languages extracted from YouTube videos. Tests the ability to...", "l": "v", "k": ["voxlingua107", "dataset", "spoken", "language", "identification", "covering", "languages", "extracted", "youtube", "videos", "tests", "ability", "identify", "short", "audio"]}, {"id": "term-voxpopuli", "t": "VoxPopuli", "tg": ["Training Corpus", "Speech", "Multilingual"], "d": "datasets", "x": "A large-scale multilingual speech corpus from European Parliament recordings covering 23 languages with transcriptions...", "l": "v", "k": ["voxpopuli", "large-scale", "multilingual", "speech", "corpus", "european", "parliament", "recordings", "covering", "languages", "transcriptions", "speech-to-speech", "translation", "data"]}, {"id": "term-voxposer", "t": "VoxPoser", "tg": ["Models", "Technical", "Robotics"], "d": "models", "x": "A model that uses large language models and vision-language models to compose 3D value maps for robot manipulation...", "l": "v", "k": ["voxposer", "model", "uses", "large", "language", "models", "vision-language", "compose", "value", "maps", "robot", "manipulation", "without", "task-specific", "training"]}, {"id": "term-voyage-ai-embeddings", "t": "Voyage AI Embeddings", "tg": ["Models", "Technical", "Embedding", "NLP"], "d": "models", "x": "A series of text embedding models from Voyage AI designed for high-quality retrieval and semantic similarity with...", "l": "v", "k": ["voyage", "embeddings", "series", "text", "embedding", "models", "designed", "high-quality", "retrieval", "semantic", "similarity", "domain-specific", "variants"]}, {"id": "term-voyager", "t": "Voyager", "tg": ["Models", "Technical", "NLP", "Robotics"], "d": "models", "x": "An LLM-powered embodied agent that explores and learns in Minecraft by generating code-based skills and building a...", "l": "v", "k": ["voyager", "llm-powered", "embodied", "agent", "explores", "learns", "minecraft", "generating", "code-based", "skills", "building", "growing", "library", "reusable", "behaviors"]}, {"id": "term-vq-vae", "t": "VQ-VAE", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Vector Quantized Variational Autoencoder, a model that uses discrete latent representations through vector...", "l": "v", "k": ["vq-vae", "vector", "quantized", "variational", "autoencoder", "model", "uses", "discrete", "latent", "representations", "quantization", "enabling", "high-fidelity", "generation", "combining"]}, {"id": "term-vq-vae-2", "t": "VQ-VAE-2", "tg": ["Models", "Technical"], "d": "models", "x": "An improved version of VQ-VAE that uses a hierarchical latent structure with multiple levels of quantization. Generates...", "l": "v", "k": ["vq-vae-2", "improved", "version", "vq-vae", "uses", "hierarchical", "latent", "structure", "multiple", "levels", "quantization", "generates", "high-quality", "images", "comparable"]}, {"id": "term-vqa", "t": "VQA", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "Visual Question Answering a dataset of open-ended questions about images requiring understanding of vision language and...", "l": "v", "k": ["vqa", "visual", "question", "answering", "dataset", "open-ended", "questions", "images", "requiring", "understanding", "vision", "language", "commonsense", "knowledge", "contains"]}, {"id": "term-vqa-20", "t": "VQA 2.0", "tg": ["Benchmark", "Multimodal"], "d": "datasets", "x": "The second version of VQA with balanced pairs of similar images that require different answers reducing bias. Contains...", "l": "v", "k": ["vqa", "version", "balanced", "pairs", "similar", "images", "require", "different", "answers", "reducing", "bias", "contains", "million", "questions", "coco"]}, {"id": "term-vulkan-compute", "t": "Vulkan Compute", "tg": ["Programming", "Standard", "API"], "d": "hardware", "x": "Compute shader capability in the Vulkan graphics API enabling GPU computing on a wider range of hardware than CUDA....", "l": "v", "k": ["vulkan", "compute", "shader", "capability", "graphics", "api", "enabling", "gpu", "computing", "wider", "range", "hardware", "cuda", "supports", "inference"]}, {"id": "term-vulnerability-exploitation-by-ai", "t": "Vulnerability Exploitation by AI", "tg": ["AI Ethics", "Regulation"], "d": "safety", "x": "AI systems that exploit the vulnerabilities of specific groups such as children, elderly, or persons with disabilities...", "l": "v", "k": ["vulnerability", "exploitation", "systems", "exploit", "vulnerabilities", "specific", "groups", "children", "elderly", "persons", "disabilities", "materially", "distort", "behavior", "prohibited"]}, {"id": "term-wafer-fabrication-equipment", "t": "Wafer Fabrication Equipment", "tg": ["Manufacturing", "Equipment", "Category"], "d": "hardware", "x": "Specialized machinery used in semiconductor manufacturing including lithography etching deposition and inspection...", "l": "w", "k": ["wafer", "fabrication", "equipment", "specialized", "machinery", "semiconductor", "manufacturing", "including", "lithography", "etching", "deposition", "inspection", "systems", "advanced", "costs"]}, {"id": "term-wafer-yield", "t": "Wafer Yield", "tg": ["Fabrication", "Manufacturing", "Metric"], "d": "hardware", "x": "Percentage of functional dies produced from a semiconductor wafer. Higher yields reduce per-chip costs and are a...", "l": "w", "k": ["wafer", "yield", "percentage", "functional", "dies", "produced", "semiconductor", "higher", "yields", "reduce", "per-chip", "costs", "critical", "metric", "manufacturing"]}, {"id": "term-wafer-level-testing", "t": "Wafer-Level Testing", "tg": ["Manufacturing", "Testing", "Process"], "d": "hardware", "x": "Process of testing semiconductor dies while still on the wafer before they are cut and packaged. Identifies defective...", "l": "w", "k": ["wafer-level", "testing", "process", "semiconductor", "dies", "wafer", "cut", "packaged", "identifies", "defective", "early", "saving", "cost", "packaging", "non-functional"]}, {"id": "term-wafer-scale-computing", "t": "Wafer-Scale Computing", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "An approach to AI hardware that uses an entire silicon wafer as a single chip rather than cutting it into individual...", "l": "w", "k": ["wafer-scale", "computing", "approach", "hardware", "uses", "entire", "silicon", "wafer", "single", "chip", "rather", "cutting", "individual", "dies", "processors"]}, {"id": "term-wafer-scale-engine", "t": "Wafer-Scale Engine", "tg": ["Architecture", "Wafer-Scale", "Innovation"], "d": "hardware", "x": "Chip design approach using an entire silicon wafer as a single massive die rather than cutting it into individual...", "l": "w", "k": ["wafer-scale", "engine", "chip", "design", "approach", "entire", "silicon", "wafer", "single", "massive", "die", "rather", "cutting", "individual", "chips"]}, {"id": "term-walter-pitts", "t": "Walter Pitts", "tg": ["History", "Pioneers"], "d": "history", "x": "American logician (1923-1969) who, with Warren McCulloch, developed the McCulloch-Pitts neuron model in 1943,...", "l": "w", "k": ["walter", "pitts", "american", "logician", "1923-1969", "warren", "mcculloch", "developed", "mcculloch-pitts", "neuron", "model", "demonstrating", "networks", "simple", "logical"]}, {"id": "term-wards-method", "t": "Ward's Method", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "An agglomerative clustering linkage criterion that merges the pair of clusters that results in the smallest increase in...", "l": "w", "k": ["ward", "method", "agglomerative", "clustering", "linkage", "criterion", "merges", "pair", "clusters", "results", "smallest", "increase", "total", "within-cluster", "variance"]}, {"id": "term-warm-restarts", "t": "Warm Restarts", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A training technique that periodically resets the learning rate to a high value during training. Helps the optimizer...", "l": "w", "k": ["warm", "restarts", "training", "technique", "periodically", "resets", "learning", "rate", "high", "value", "helps", "optimizer", "escape", "local", "minima"]}, {"id": "term-warm-starting", "t": "Warm Starting", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A training strategy that initializes model parameters from a previously trained model rather than random values....", "l": "w", "k": ["warm", "starting", "training", "strategy", "initializes", "model", "parameters", "previously", "trained", "rather", "random", "values", "accelerates", "convergence", "improve"]}, {"id": "term-warm-up-cosine-decay", "t": "Warm-up Cosine Decay", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A learning rate schedule that combines a linear warmup phase with cosine decay. Starts with a low learning rate...", "l": "w", "k": ["warm-up", "cosine", "decay", "learning", "rate", "schedule", "combines", "linear", "warmup", "phase", "starts", "low", "gradually", "increases", "peak"]}, {"id": "term-warmup", "t": "Warmup (Learning Rate)", "tg": ["Training", "Technique"], "d": "general", "x": "Gradually increasing learning rate at the start of training before decay. Helps stabilize early training when gradients...", "l": "w", "k": ["warmup", "learning", "rate", "gradually", "increasing", "start", "training", "decay", "helps", "stabilize", "early", "gradients", "unreliable", "random", "weights"]}, {"id": "term-warp", "t": "Warp", "tg": ["GPU", "NVIDIA", "Architecture"], "d": "hardware", "x": "Group of 32 threads in NVIDIA GPU architecture that execute the same instruction simultaneously. The basic unit of SIMT...", "l": "w", "k": ["warp", "group", "threads", "nvidia", "gpu", "architecture", "execute", "instruction", "simultaneously", "basic", "unit", "simt", "execution", "streaming", "multiprocessors"]}, {"id": "term-warren-mcculloch", "t": "Warren McCulloch", "tg": ["History", "Pioneers"], "d": "history", "x": "American neurophysiologist (1898-1969) who, with Walter Pitts, created the first mathematical model of an artificial...", "l": "w", "k": ["warren", "mcculloch", "american", "neurophysiologist", "1898-1969", "walter", "pitts", "created", "mathematical", "model", "artificial", "neuron", "laying", "theoretical", "foundation"]}, {"id": "term-wasserstein-distance", "t": "Wasserstein Distance", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "Also known as the Earth Mover's Distance, a metric measuring the minimum cost of transforming one probability...", "l": "w", "k": ["wasserstein", "distance", "known", "earth", "mover", "metric", "measuring", "minimum", "cost", "transforming", "probability", "distribution", "another", "amount", "mass"]}, {"id": "term-watermark", "t": "Watermark (AI)", "tg": ["Safety", "Detection"], "d": "safety", "x": "Hidden patterns in AI-generated content that allow detection of synthetic origins. Proposed for identifying AI text,...", "l": "w", "k": ["watermark", "hidden", "patterns", "ai-generated", "content", "allow", "detection", "synthetic", "origins", "proposed", "identifying", "text", "images", "audio", "combat"]}, {"id": "term-watermark-detection", "t": "Watermark Detection", "tg": ["Generative AI", "LLM"], "d": "models", "x": "Algorithms that identify statistical patterns embedded in AI-generated text or images during the generation process,...", "l": "w", "k": ["watermark", "detection", "algorithms", "identify", "statistical", "patterns", "embedded", "ai-generated", "text", "images", "generation", "process", "enabling", "attribution", "content"]}, {"id": "term-watershed-algorithm", "t": "Watershed Algorithm", "tg": ["Algorithms", "Technical", "Vision"], "d": "algorithms", "x": "An image segmentation method that treats the grayscale image as a topographic surface and simulates flooding from...", "l": "w", "k": ["watershed", "algorithm", "image", "segmentation", "method", "treats", "grayscale", "topographic", "surface", "simulates", "flooding", "markers", "boundaries", "different", "flood"]}, {"id": "term-watson-ibm", "t": "Watson (IBM)", "tg": ["History", "Systems"], "d": "history", "x": "An AI system developed by IBM that defeated champions Brad Rutter and Ken Jennings on the game show Jeopardy! in 2011....", "l": "w", "k": ["watson", "ibm", "system", "developed", "defeated", "champions", "brad", "rutter", "ken", "jennings", "game", "show", "jeopardy", "natural", "language"]}, {"id": "term-wav2vec-20", "t": "Wav2Vec 2.0", "tg": ["Models", "Technical"], "d": "models", "x": "A self-supervised speech representation learning framework by Meta AI that learns from raw audio. Pretrained on...", "l": "w", "k": ["wav2vec", "self-supervised", "speech", "representation", "learning", "framework", "meta", "learns", "raw", "audio", "pretrained", "unlabeled", "data", "fine-tuned", "minimal"]}, {"id": "term-wav2vec-u", "t": "Wav2Vec-U", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "An unsupervised speech recognition model that learns to transcribe speech without any labeled data using...", "l": "w", "k": ["wav2vec-u", "unsupervised", "speech", "recognition", "model", "learns", "transcribe", "without", "labeled", "data", "self-supervised", "representations", "adversarial", "training"]}, {"id": "term-wavcraft", "t": "WavCraft", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "An LLM-powered framework that edits and creates audio content through natural language instructions by orchestrating...", "l": "w", "k": ["wavcraft", "llm-powered", "framework", "edits", "creates", "audio", "content", "natural", "language", "instructions", "orchestrating", "specialized", "processing", "modules"]}, {"id": "term-waveform", "t": "WaveForM", "tg": ["Models", "Technical"], "d": "models", "x": "A Wavelet-based Forecasting Model that uses wavelet decomposition within a Transformer framework for capturing...", "l": "w", "k": ["waveform", "wavelet-based", "forecasting", "model", "uses", "wavelet", "decomposition", "within", "transformer", "framework", "capturing", "multi-scale", "temporal", "patterns", "time"]}, {"id": "term-wavefront", "t": "Wavefront", "tg": ["GPU", "AMD", "Architecture"], "d": "hardware", "x": "Group of 64 threads in AMD GPU architecture analogous to NVIDIA warps. The fundamental unit of SIMT execution on AMD...", "l": "w", "k": ["wavefront", "group", "threads", "amd", "gpu", "architecture", "analogous", "nvidia", "warps", "fundamental", "unit", "simt", "execution", "compute", "units"]}, {"id": "term-wavelet-transform", "t": "Wavelet Transform", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A transform that represents signals using wavelets which are localized oscillating functions. Unlike Fourier transforms...", "l": "w", "k": ["wavelet", "transform", "represents", "signals", "wavelets", "localized", "oscillating", "functions", "unlike", "fourier", "transforms", "provide", "time", "frequency", "information"]}, {"id": "term-wavelet-tree-algorithm", "t": "Wavelet Tree Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A data structure that provides efficient access and rank and select operations on sequences over arbitrary alphabets....", "l": "w", "k": ["wavelet", "tree", "algorithm", "data", "structure", "provides", "efficient", "access", "rank", "select", "operations", "sequences", "arbitrary", "alphabets", "represents"]}, {"id": "term-wavenet", "t": "WaveNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A deep generative model for raw audio waveforms that uses dilated causal convolutions to capture long-range temporal...", "l": "w", "k": ["wavenet", "deep", "generative", "model", "raw", "audio", "waveforms", "uses", "dilated", "causal", "convolutions", "capture", "long-range", "temporal", "dependencies"]}, {"id": "term-wavlm", "t": "WavLM", "tg": ["Models", "Technical"], "d": "models", "x": "A large-scale self-supervised speech model that learns universal representations through masked speech prediction and...", "l": "w", "k": ["wavlm", "large-scale", "self-supervised", "speech", "model", "learns", "universal", "representations", "masked", "prediction", "denoising", "excels", "recognition", "non-asr", "tasks"]}, {"id": "term-wavtokenizer", "t": "WavTokenizer", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "An audio tokenizer that converts continuous audio waveforms into discrete tokens for use in language-model-based speech...", "l": "w", "k": ["wavtokenizer", "audio", "tokenizer", "converts", "continuous", "waveforms", "discrete", "tokens", "language-model-based", "speech", "generation", "systems"]}, {"id": "term-wayformer", "t": "Wayformer", "tg": ["Models", "Technical", "Autonomous"], "d": "models", "x": "A Transformer-based model from Waymo for motion forecasting that predicts future trajectories of vehicles and...", "l": "w", "k": ["wayformer", "transformer-based", "model", "waymo", "motion", "forecasting", "predicts", "future", "trajectories", "vehicles", "pedestrians", "autonomous", "driving", "scenes"]}, {"id": "term-waymo-history", "t": "Waymo History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of Google's self-driving car project, started in 2009 by Sebastian Thrun, into Waymo as a subsidiary of...", "l": "w", "k": ["waymo", "history", "evolution", "google", "self-driving", "car", "project", "started", "sebastian", "thrun", "subsidiary", "alphabet", "becoming", "commercial", "autonomous"]}, {"id": "term-waymo-multipath", "t": "Waymo MultiPath++", "tg": ["Models", "Technical", "Autonomous"], "d": "models", "x": "An improved multi-agent trajectory prediction model from Waymo that forecasts future motions of all scene participants...", "l": "w", "k": ["waymo", "multipath", "improved", "multi-agent", "trajectory", "prediction", "model", "forecasts", "future", "motions", "scene", "participants", "simultaneously"]}, {"id": "term-waymo-open-dataset", "t": "Waymo Open Dataset", "tg": ["Benchmark", "Autonomous Driving"], "d": "datasets", "x": "A large-scale autonomous driving dataset from Waymo containing lidar and camera data with 3D bounding box annotations....", "l": "w", "k": ["waymo", "open", "dataset", "large-scale", "autonomous", "driving", "containing", "lidar", "camera", "data", "bounding", "box", "annotations", "largest", "diverse"]}, {"id": "term-waymo-perception-model", "t": "Waymo Perception Model", "tg": ["Models", "Technical", "Autonomous", "Vision"], "d": "models", "x": "The neural network stack used by Waymo autonomous vehicles for detecting and tracking objects using lidar and camera...", "l": "w", "k": ["waymo", "perception", "model", "neural", "network", "stack", "autonomous", "vehicles", "detecting", "tracking", "objects", "lidar", "camera", "sensor", "fusion"]}, {"id": "term-wbb", "t": "WBB", "tg": ["Benchmark", "3D", "Computer Vision"], "d": "datasets", "x": "The WebBodies Benchmark a dataset of 3D human body models and poses for evaluating human shape estimation algorithms....", "l": "w", "k": ["wbb", "webbodies", "benchmark", "dataset", "human", "body", "models", "poses", "evaluating", "shape", "estimation", "algorithms", "tests", "reconstruction", "geometry"]}, {"id": "term-weak-supervision", "t": "Weak Supervision", "tg": ["Training", "Technique"], "d": "general", "x": "Training with noisy, imprecise, or automatically generated labels instead of perfect human annotations. Can...", "l": "w", "k": ["weak", "supervision", "training", "noisy", "imprecise", "automatically", "generated", "labels", "instead", "perfect", "human", "annotations", "dramatically", "reduce", "labeling"]}, {"id": "term-weaponization-of-ai", "t": "Weaponization of AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "The development or adaptation of AI systems for military offensive or harmful purposes. Includes autonomous weapons...", "l": "w", "k": ["weaponization", "development", "adaptation", "systems", "military", "offensive", "harmful", "purposes", "includes", "autonomous", "weapons", "cyberweapons", "ai-enhanced", "surveillance", "tools"]}, {"id": "term-weaviate", "t": "Weaviate", "tg": ["Vector Database", "Open Source"], "d": "general", "x": "An open-source vector database that combines vector search with structured filtering and supports multiple...", "l": "w", "k": ["weaviate", "open-source", "vector", "database", "combines", "search", "structured", "filtering", "supports", "multiple", "vectorization", "modules", "offering", "hybrid", "capabilities"]}, {"id": "term-webarena", "t": "WebArena", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating autonomous web agents on realistic web tasks across self-hosted websites. Tests the ability...", "l": "w", "k": ["webarena", "benchmark", "evaluating", "autonomous", "web", "agents", "realistic", "tasks", "across", "self-hosted", "websites", "tests", "ability", "complete", "complex"]}, {"id": "term-webdataset", "t": "WebDataset", "tg": ["Platform", "General"], "d": "datasets", "x": "A format and library for efficiently storing and loading large-scale datasets using tar archives. Designed for...", "l": "w", "k": ["webdataset", "format", "library", "efficiently", "storing", "loading", "large-scale", "datasets", "tar", "archives", "designed", "distributed", "training", "sequential", "access"]}, {"id": "term-webquestions", "t": "WebQuestions", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A question answering dataset of 5810 questions collected using Google Suggest. Tests factoid question answering using...", "l": "w", "k": ["webquestions", "question", "answering", "dataset", "questions", "collected", "google", "suggest", "tests", "factoid", "knowledge", "bases", "freebase"]}, {"id": "term-webtext", "t": "WebText", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "The training corpus for GPT-2 containing 8 million web pages selected through Reddit link quality filtering....", "l": "w", "k": ["webtext", "training", "corpus", "gpt-2", "containing", "million", "web", "pages", "selected", "reddit", "link", "quality", "filtering", "approximately", "40gb"]}, {"id": "term-webvid-10m", "t": "WebVid-10M", "tg": ["Training Corpus", "Video", "Multimodal"], "d": "datasets", "x": "A large-scale dataset of 10 million video-text pairs scraped from the web. Used for pretraining video-language models...", "l": "w", "k": ["webvid-10m", "large-scale", "dataset", "million", "video-text", "pairs", "scraped", "web", "pretraining", "video-language", "models", "text-to-video", "generation", "systems"]}, {"id": "term-weibull-distribution", "t": "Weibull Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution used in reliability analysis and survival modeling. Its shape parameter allows it...", "l": "w", "k": ["weibull", "distribution", "continuous", "probability", "reliability", "analysis", "survival", "modeling", "shape", "parameter", "allows", "model", "increasing", "constant", "decreasing"]}, {"id": "term-weight", "t": "Weight", "tg": ["Core Concept", "Neural Networks"], "d": "models", "x": "The numerical parameters in neural networks that are learned during training. Weights determine how inputs are...", "l": "w", "k": ["weight", "numerical", "parameters", "neural", "networks", "learned", "training", "weights", "determine", "inputs", "transformed", "outputs", "large", "models", "billions"]}, {"id": "term-weight-decay", "t": "Weight Decay", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A regularization technique that adds a fraction of the current weight values to the weight update rule during training,...", "l": "w", "k": ["weight", "decay", "regularization", "technique", "adds", "fraction", "current", "values", "update", "rule", "training", "effectively", "penalizing", "large", "weights"]}, {"id": "term-weight-initialization", "t": "Weight Initialization", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The strategy for setting initial parameter values in neural networks, with methods like Xavier and He initialization...", "l": "w", "k": ["weight", "initialization", "strategy", "setting", "initial", "parameter", "values", "neural", "networks", "methods", "xavier", "designed", "maintain", "signal", "variance"]}, {"id": "term-weight-normalization", "t": "Weight Normalization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A reparameterization of weight vectors that decouples the magnitude and direction of weight vectors. Proposed by...", "l": "w", "k": ["weight", "normalization", "reparameterization", "vectors", "decouples", "magnitude", "direction", "proposed", "salimans", "kingma", "simpler", "alternative", "batch", "introduce", "dependencies"]}, {"id": "term-weight-sharing", "t": "Weight Sharing", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A compression technique where multiple connections in a neural network share the same weight value, reducing the number...", "l": "w", "k": ["weight", "sharing", "compression", "technique", "multiple", "connections", "neural", "network", "share", "value", "reducing", "number", "unique", "parameters", "must"]}, {"id": "term-weight-tying", "t": "Weight Tying", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A technique that shares the weight matrix between the input embedding layer and the output projection layer in language...", "l": "w", "k": ["weight", "tying", "technique", "shares", "matrix", "input", "embedding", "layer", "output", "projection", "language", "models", "reducing", "parameters", "improving"]}, {"id": "term-weight-balanced-tree", "t": "Weight-Balanced Tree", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A self-balancing binary search tree that maintains the invariant that the weight (number of nodes) of each child...", "l": "w", "k": ["weight-balanced", "tree", "self-balancing", "binary", "search", "maintains", "invariant", "weight", "number", "nodes", "child", "subtree", "bounded", "fraction", "parent"]}, {"id": "term-weight-only-quantization", "t": "Weight-Only Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A quantization strategy that compresses only the model weights to low precision while performing computation in higher...", "l": "w", "k": ["weight-only", "quantization", "strategy", "compresses", "model", "weights", "low", "precision", "performing", "computation", "higher", "dequantization", "reduces", "memory", "footprint"]}, {"id": "term-weights-and-biases", "t": "Weights and Biases", "tg": ["History", "Organizations"], "d": "history", "x": "A machine learning operations (MLOps) platform founded in 2017 that provides experiment tracking model management and...", "l": "w", "k": ["weights", "biases", "machine", "learning", "operations", "mlops", "platform", "founded", "provides", "experiment", "tracking", "model", "management", "dataset", "versioning"]}, {"id": "term-welchs-method", "t": "Welch's Method", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A technique for estimating the power spectral density of a signal by averaging modified periodograms of overlapping...", "l": "w", "k": ["welch", "method", "technique", "estimating", "power", "spectral", "density", "signal", "averaging", "modified", "periodograms", "overlapping", "windowed", "segments", "reduces"]}, {"id": "term-welsh-powell-algorithm", "t": "Welsh-Powell Algorithm", "tg": ["Algorithms", "Technical", "Graph"], "d": "algorithms", "x": "A greedy graph coloring algorithm that sorts vertices by degree in descending order and assigns the smallest available...", "l": "w", "k": ["welsh-powell", "algorithm", "greedy", "graph", "coloring", "sorts", "vertices", "degree", "descending", "order", "assigns", "smallest", "available", "color", "vertex"]}, {"id": "term-wenetspeech", "t": "WenetSpeech", "tg": ["Training Corpus", "Speech", "Multilingual"], "d": "datasets", "x": "A large-scale Mandarin speech corpus of 10000 hours from YouTube and podcasts with multi-domain coverage. One of the...", "l": "w", "k": ["wenetspeech", "large-scale", "mandarin", "speech", "corpus", "hours", "youtube", "podcasts", "multi-domain", "coverage", "largest", "open", "chinese", "asr", "training"]}, {"id": "term-whale-optimization-algorithm", "t": "Whale Optimization Algorithm", "tg": ["Algorithms", "Technical", "Metaheuristic"], "d": "algorithms", "x": "A metaheuristic inspired by the bubble-net hunting strategy of humpback whales. Simulates encircling prey and spiral...", "l": "w", "k": ["whale", "optimization", "algorithm", "metaheuristic", "inspired", "bubble-net", "hunting", "strategy", "humpback", "whales", "simulates", "encircling", "prey", "spiral", "attacking"]}, {"id": "term-whisper", "t": "Whisper", "tg": ["Model", "Speech"], "d": "models", "x": "OpenAI's speech recognition model that transcribes audio to text with high accuracy across many languages....", "l": "w", "k": ["whisper", "openai", "speech", "recognition", "model", "transcribes", "audio", "text", "high", "accuracy", "across", "languages", "open-sourced", "enabling", "widespread"]}, {"id": "term-whisper-architecture", "t": "Whisper Architecture", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An encoder-decoder transformer architecture trained on 680,000 hours of multilingual speech data for automatic speech...", "l": "w", "k": ["whisper", "architecture", "encoder-decoder", "transformer", "trained", "hours", "multilingual", "speech", "data", "automatic", "recognition", "log-mel", "spectrogram", "features", "input"]}, {"id": "term-whisper-large-v3", "t": "Whisper Large V3", "tg": ["Models", "Technical", "Audio", "NLP"], "d": "models", "x": "The latest and most accurate version of OpenAI Whisper speech recognition model with improved multilingual...", "l": "w", "k": ["whisper", "large", "latest", "accurate", "version", "openai", "speech", "recognition", "model", "improved", "multilingual", "transcription", "translation", "capabilities"]}, {"id": "term-whistleblower-protection-for-ai", "t": "Whistleblower Protection for AI", "tg": ["Safety", "Policy"], "d": "safety", "x": "Legal and organizational safeguards for individuals who report safety concerns ethical violations or illegal activities...", "l": "w", "k": ["whistleblower", "protection", "legal", "organizational", "safeguards", "individuals", "report", "safety", "concerns", "ethical", "violations", "illegal", "activities", "related", "systems"]}, {"id": "term-white-noise", "t": "White Noise", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A time series of uncorrelated random variables with zero mean and constant variance. It represents purely random...", "l": "w", "k": ["white", "noise", "time", "series", "uncorrelated", "random", "variables", "zero", "mean", "constant", "variance", "represents", "purely", "variation", "exploitable"]}, {"id": "term-white-box-attack", "t": "White-Box Attack", "tg": ["Safety", "Technical"], "d": "safety", "x": "An adversarial attack where the attacker has full knowledge of and access to the target model including its...", "l": "w", "k": ["white-box", "attack", "adversarial", "attacker", "full", "knowledge", "access", "target", "model", "including", "architecture", "weights", "training", "data", "represents"]}, {"id": "term-wide-and-deep-model", "t": "Wide and Deep Model", "tg": ["Models", "Technical", "Recommendation"], "d": "models", "x": "A recommendation architecture from Google combining a wide linear model for memorization with a deep neural network for...", "l": "w", "k": ["wide", "deep", "model", "recommendation", "architecture", "google", "combining", "linear", "memorization", "neural", "network", "generalization"]}, {"id": "term-widerface", "t": "WiderFace", "tg": ["Benchmark", "Computer Vision"], "d": "datasets", "x": "A face detection benchmark containing 32203 images with 393703 annotated faces showing large variations in scale pose...", "l": "w", "k": ["widerface", "face", "detection", "benchmark", "containing", "images", "annotated", "faces", "showing", "large", "variations", "scale", "pose", "occlusion", "expression"]}, {"id": "term-wiener-filter", "t": "Wiener Filter", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "An optimal linear filter that minimizes the mean squared error between the desired signal and the filter output....", "l": "w", "k": ["wiener", "filter", "optimal", "linear", "minimizes", "mean", "squared", "error", "desired", "signal", "output", "operates", "frequency", "domain", "power"]}, {"id": "term-wikidata", "t": "Wikidata", "tg": ["Knowledge", "Graph"], "d": "datasets", "x": "A free and open knowledge base maintained by the Wikimedia Foundation containing structured data about millions of...", "l": "w", "k": ["wikidata", "free", "open", "knowledge", "base", "maintained", "wikimedia", "foundation", "containing", "structured", "data", "millions", "entities", "training", "resource"]}, {"id": "term-wikilingua", "t": "WikiLingua", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "A cross-lingual summarization dataset of how-to guides from WikiHow covering 18 languages. Tests the ability to produce...", "l": "w", "k": ["wikilingua", "cross-lingual", "summarization", "dataset", "how-to", "guides", "wikihow", "covering", "languages", "tests", "ability", "produce", "summaries", "procedural", "text"]}, {"id": "term-wikipedia-dumps", "t": "Wikipedia Dumps", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "Regular snapshots of all Wikipedia articles available in multiple languages. A standard high-quality pretraining data...", "l": "w", "k": ["wikipedia", "dumps", "regular", "snapshots", "articles", "available", "multiple", "languages", "standard", "high-quality", "pretraining", "data", "source", "language", "models"]}, {"id": "term-wikisql", "t": "WikiSQL", "tg": ["Benchmark", "NLP", "Code"], "d": "datasets", "x": "A dataset of 80654 hand-annotated SQL queries and natural language questions on 24241 Wikipedia tables. One of the...", "l": "w", "k": ["wikisql", "dataset", "hand-annotated", "sql", "queries", "natural", "language", "questions", "wikipedia", "tables", "earliest", "large-scale", "text-to-sql", "benchmarks"]}, {"id": "term-wikitablequestions", "t": "WikiTableQuestions", "tg": ["Benchmark", "NLP", "Tabular"], "d": "datasets", "x": "A question answering dataset of complex questions over Wikipedia tables requiring operations like comparison...", "l": "w", "k": ["wikitablequestions", "question", "answering", "dataset", "complex", "questions", "wikipedia", "tables", "requiring", "operations", "comparison", "aggregation", "arithmetic", "tests", "compositional"]}, {"id": "term-wikitext-103", "t": "WikiText-103", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A language modeling dataset containing over 103 million tokens from verified Good and Featured Wikipedia articles....", "l": "w", "k": ["wikitext-103", "language", "modeling", "dataset", "containing", "million", "tokens", "verified", "good", "featured", "wikipedia", "articles", "provides", "long-range", "dependency"]}, {"id": "term-wildbench", "t": "WildBench", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A benchmark of challenging real-world user queries for evaluating LLMs on complex instruction-following tasks....", "l": "w", "k": ["wildbench", "benchmark", "challenging", "real-world", "user", "queries", "evaluating", "llms", "complex", "instruction-following", "tasks", "collected", "actual", "interactions", "assistants"]}, {"id": "term-wildchat", "t": "WildChat", "tg": ["Training Corpus", "NLP"], "d": "datasets", "x": "A dataset of one million conversations between users and ChatGPT with demographic and toxicity annotations. Provides...", "l": "w", "k": ["wildchat", "dataset", "million", "conversations", "users", "chatgpt", "demographic", "toxicity", "annotations", "provides", "realistic", "user", "interaction", "data", "studying"]}, {"id": "term-wildguard", "t": "WildGuard", "tg": ["Benchmark", "NLP", "Safety"], "d": "datasets", "x": "A dataset for evaluating safety classifiers on in-the-wild user prompts and model responses. Tests the ability to...", "l": "w", "k": ["wildguard", "dataset", "evaluating", "safety", "classifiers", "in-the-wild", "user", "prompts", "model", "responses", "tests", "ability", "detect", "harmful", "content"]}, {"id": "term-win-rate", "t": "Win Rate", "tg": ["Evaluation", "Ranking"], "d": "datasets", "x": "An evaluation metric that measures the percentage of pairwise comparisons in which one model's output is preferred over...", "l": "w", "k": ["win", "rate", "evaluation", "metric", "measures", "percentage", "pairwise", "comparisons", "model", "output", "preferred", "another", "human", "judges", "automated"]}, {"id": "term-windfall-clause", "t": "Windfall Clause", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "A proposed commitment by AI developers to share the economic benefits of transformative AI widely, ensuring that a...", "l": "w", "k": ["windfall", "clause", "proposed", "commitment", "developers", "share", "economic", "benefits", "transformative", "widely", "ensuring", "small", "number", "companies", "nations"]}, {"id": "term-window-attention", "t": "Window Attention", "tg": ["Architecture", "Efficiency"], "d": "models", "x": "A variant of attention that only looks at nearby tokens rather than the full context. Reduces computational cost for...", "l": "w", "k": ["window", "attention", "variant", "looks", "nearby", "tokens", "rather", "full", "context", "reduces", "computational", "cost", "long", "sequences", "models"]}, {"id": "term-window-based-chunking", "t": "Window-Based Chunking", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "A document splitting method that uses a fixed-size sliding window measured in tokens or characters to create...", "l": "w", "k": ["window-based", "chunking", "document", "splitting", "method", "uses", "fixed-size", "sliding", "window", "measured", "tokens", "characters", "create", "overlapping", "chunks"]}, {"id": "term-windowed-attention", "t": "Windowed Attention", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An attention mechanism that restricts each token to attend only to tokens within a fixed window around its position....", "l": "w", "k": ["windowed", "attention", "mechanism", "restricts", "token", "attend", "tokens", "within", "fixed", "window", "around", "position", "reduces", "computational", "complexity"]}, {"id": "term-wine-quality", "t": "Wine Quality", "tg": ["Benchmark", "Tabular"], "d": "datasets", "x": "A dataset of physicochemical properties and quality ratings for red and white Portuguese wines. Used for regression and...", "l": "w", "k": ["wine", "quality", "dataset", "physicochemical", "properties", "ratings", "red", "white", "portuguese", "wines", "regression", "classification", "benchmarking", "machine", "learning"]}, {"id": "term-winobias", "t": "WinoBias", "tg": ["Benchmark", "NLP", "Fairness"], "d": "datasets", "x": "A Winograd-schema dataset for evaluating gender bias in coreference resolution systems. Contains sentence pairs that...", "l": "w", "k": ["winobias", "winograd-schema", "dataset", "evaluating", "gender", "bias", "coreference", "resolution", "systems", "contains", "sentence", "pairs", "test", "models", "rely"]}, {"id": "term-winograd-schema", "t": "Winograd Schema", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A coreference resolution challenge requiring commonsense reasoning to determine what a pronoun refers to, designed as...", "l": "w", "k": ["winograd", "schema", "coreference", "resolution", "challenge", "requiring", "commonsense", "reasoning", "determine", "pronoun", "refers", "designed", "alternative", "turing", "test"]}, {"id": "term-winograd-schema-challenge", "t": "Winograd Schema Challenge", "tg": ["History", "Milestones"], "d": "history", "x": "A test of machine intelligence proposed by Hector Levesque in 2012 as an alternative to the Turing Test. It presents...", "l": "w", "k": ["winograd", "schema", "challenge", "test", "machine", "intelligence", "proposed", "hector", "levesque", "alternative", "turing", "presents", "sentences", "ambiguous", "pronouns"]}, {"id": "term-winogrande", "t": "WinoGrande", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A large-scale benchmark of Winograd schema-style coreference resolution problems that tests commonsense reasoning by...", "l": "w", "k": ["winogrande", "large-scale", "benchmark", "winograd", "schema-style", "coreference", "resolution", "problems", "tests", "commonsense", "reasoning", "requiring", "models", "identify", "correct"]}, {"id": "term-winoground", "t": "Winoground", "tg": ["Benchmark", "Multimodal", "Evaluation"], "d": "datasets", "x": "A benchmark of 400 image-caption pairs testing compositional understanding in vision-language models. Each example has...", "l": "w", "k": ["winoground", "benchmark", "image-caption", "pairs", "testing", "compositional", "understanding", "vision-language", "models", "example", "captions", "differing", "word", "order", "paired"]}, {"id": "term-winsorization", "t": "Winsorization", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A technique for handling outliers by replacing extreme values with specified percentile values rather than removing...", "l": "w", "k": ["winsorization", "technique", "handling", "outliers", "replacing", "extreme", "values", "specified", "percentile", "rather", "removing", "example", "5th", "value"]}, {"id": "term-wit", "t": "WIT", "tg": ["Training Corpus", "Multimodal", "Multilingual"], "d": "datasets", "x": "Wikipedia-based Image Text a dataset of 37 million image-text pairs from Wikipedia in 108 languages. Provides...", "l": "w", "k": ["wit", "wikipedia-based", "image", "text", "dataset", "million", "image-text", "pairs", "wikipedia", "languages", "provides", "high-quality", "curated", "multimodal", "data"]}, {"id": "term-wizard-of-wikipedia", "t": "Wizard of Wikipedia", "tg": ["Benchmark", "NLP", "Dialogue"], "d": "datasets", "x": "A dataset of conversations grounded in Wikipedia knowledge where one participant has access to relevant articles. Tests...", "l": "w", "k": ["wizard", "wikipedia", "dataset", "conversations", "grounded", "knowledge", "participant", "access", "relevant", "articles", "tests", "knowledge-grounded", "dialogue", "generation"]}, {"id": "term-wizardcoder", "t": "WizardCoder", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A code-generation language model fine-tuned using the Evol-Instruct method to progressively increase coding instruction...", "l": "w", "k": ["wizardcoder", "code-generation", "language", "model", "fine-tuned", "evol-instruct", "method", "progressively", "increase", "coding", "instruction", "complexity", "stronger", "programming", "ability"]}, {"id": "term-wizardlm", "t": "WizardLM", "tg": ["Models", "Technical"], "d": "models", "x": "A language model fine-tuned using Evol-Instruct a method that progressively creates complex instructions from simple...", "l": "w", "k": ["wizardlm", "language", "model", "fine-tuned", "evol-instruct", "method", "progressively", "creates", "complex", "instructions", "simple", "seed", "achieves", "strong", "instruction-following"]}, {"id": "term-wizardmath", "t": "WizardMath", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A mathematical reasoning model fine-tuned using Reinforced Evol-Instruct to progressively enhance mathematical...", "l": "w", "k": ["wizardmath", "mathematical", "reasoning", "model", "fine-tuned", "reinforced", "evol-instruct", "progressively", "enhance", "problem-solving", "capabilities"]}, {"id": "term-wmt-metrics", "t": "WMT Metrics", "tg": ["Benchmark", "NLP", "Evaluation", "Translation"], "d": "datasets", "x": "Shared task datasets for evaluating machine translation evaluation metrics. Tests how well automatic metrics correlate...", "l": "w", "k": ["wmt", "metrics", "shared", "task", "datasets", "evaluating", "machine", "translation", "evaluation", "tests", "automatic", "correlate", "human", "judgments", "quality"]}, {"id": "term-wmt-translation", "t": "WMT Translation", "tg": ["History", "Milestones"], "d": "history", "x": "The Workshop on Machine Translation (formerly Workshop on Statistical Machine Translation) held annually since 2006....", "l": "w", "k": ["wmt", "translation", "workshop", "machine", "formerly", "statistical", "held", "annually", "provides", "standardized", "tasks", "evaluation", "campaigns", "driven", "progress"]}, {"id": "term-wmt-translation-datasets", "t": "WMT Translation Datasets", "tg": ["Benchmark", "NLP", "Translation"], "d": "datasets", "x": "Annual shared task datasets from the Conference on Machine Translation covering dozens of language pairs. The standard...", "l": "w", "k": ["wmt", "translation", "datasets", "annual", "shared", "task", "conference", "machine", "covering", "dozens", "language", "pairs", "standard", "benchmark", "evaluating"]}, {"id": "term-wn18rr", "t": "WN18RR", "tg": ["Benchmark", "Knowledge", "Graph"], "d": "datasets", "x": "A knowledge graph completion benchmark derived from WordNet with 40943 entities and 11 relation types. Removes inverse...", "l": "w", "k": ["wn18rr", "knowledge", "graph", "completion", "benchmark", "derived", "wordnet", "entities", "relation", "types", "removes", "inverse", "relations", "wn18", "provide"]}, {"id": "term-wnli", "t": "WNLI", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Winograd NLI a natural language inference dataset derived from the Winograd Schema Challenge. Tests pronoun coreference...", "l": "w", "k": ["wnli", "winograd", "nli", "natural", "language", "inference", "dataset", "derived", "schema", "challenge", "tests", "pronoun", "coreference", "resolution", "reframed"]}, {"id": "term-wonder3d", "t": "Wonder3D", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A model for single-image 3D reconstruction that generates multi-view normal maps and color images using cross-domain...", "l": "w", "k": ["wonder3d", "model", "single-image", "reconstruction", "generates", "multi-view", "normal", "maps", "color", "images", "cross-domain", "diffusion", "consistent", "geometry"]}, {"id": "term-word-embedding", "t": "Word Embedding", "tg": ["Representation", "NLP"], "d": "general", "x": "Dense vector representations of words where similar words have similar vectors. Classic examples include Word2Vec and...", "l": "w", "k": ["word", "embedding", "dense", "vector", "representations", "words", "similar", "vectors", "classic", "examples", "include", "word2vec", "glove", "modern", "llms"]}, {"id": "term-word-error-rate", "t": "Word Error Rate", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that measures the edit distance between predicted and reference transcriptions at the word level, calculated...", "l": "w", "k": ["word", "error", "rate", "metric", "measures", "edit", "distance", "predicted", "reference", "transcriptions", "level", "calculated", "number", "substitutions", "insertions"]}, {"id": "term-word-movers-distance", "t": "Word Mover's Distance", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "A distance metric between text documents that uses word embeddings to measure the minimum cumulative distance words...", "l": "w", "k": ["word", "mover", "distance", "metric", "text", "documents", "uses", "embeddings", "measure", "minimum", "cumulative", "words", "must", "travel", "transform"]}, {"id": "term-word-segmentation", "t": "Word Segmentation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying word boundaries in languages that do not use whitespace to separate words, such as Chinese,...", "l": "w", "k": ["word", "segmentation", "task", "identifying", "boundaries", "languages", "whitespace", "separate", "words", "chinese", "japanese", "thai", "essential", "subsequent", "nlp"]}, {"id": "term-word-sense-disambiguation", "t": "Word Sense Disambiguation", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of determining which meaning of a polysemous word is used in a given context, selecting from a predefined...", "l": "w", "k": ["word", "sense", "disambiguation", "task", "determining", "meaning", "polysemous", "given", "context", "selecting", "predefined", "inventory", "based", "surrounding", "words"]}, {"id": "term-word2vec", "t": "Word2Vec", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A family of neural network models (Skip-gram and CBOW) that learn dense vector representations of words from large text...", "l": "w", "k": ["word2vec", "family", "neural", "network", "models", "skip-gram", "cbow", "learn", "dense", "vector", "representations", "words", "large", "text", "corpora"]}, {"id": "term-word2vec-algorithm", "t": "Word2Vec Algorithm", "tg": ["Algorithms", "Fundamentals", "NLP", "History"], "d": "algorithms", "x": "A neural network-based method for learning word embeddings from large text corpora. The skip-gram model predicts...", "l": "w", "k": ["word2vec", "algorithm", "neural", "network-based", "method", "learning", "word", "embeddings", "large", "text", "corpora", "skip-gram", "model", "predicts", "context"]}, {"id": "term-word2vec-model", "t": "Word2Vec Model", "tg": ["Models", "Fundamentals"], "d": "models", "x": "A shallow neural network model that produces word embeddings by learning to predict context words given a target word...", "l": "w", "k": ["word2vec", "model", "shallow", "neural", "network", "produces", "word", "embeddings", "learning", "predict", "context", "words", "given", "target", "vice"]}, {"id": "term-wordnet", "t": "WordNet", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A large lexical database of English where nouns, verbs, adjectives, and adverbs are grouped into cognitive synonym sets...", "l": "w", "k": ["wordnet", "large", "lexical", "database", "english", "nouns", "verbs", "adjectives", "adverbs", "grouped", "cognitive", "synonym", "sets", "synsets", "linked"]}, {"id": "term-wordpiece", "t": "WordPiece", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A subword tokenization algorithm that greedily selects merges maximizing the likelihood of the training data, used by...", "l": "w", "k": ["wordpiece", "subword", "tokenization", "algorithm", "greedily", "selects", "merges", "maximizing", "likelihood", "training", "data", "bert", "related", "models", "splitting"]}, {"id": "term-wordpiece-tokenization", "t": "WordPiece Tokenization", "tg": ["Algorithms", "Technical", "NLP"], "d": "algorithms", "x": "A subword tokenization algorithm used by BERT and related models that greedily builds a vocabulary of subword units....", "l": "w", "k": ["wordpiece", "tokenization", "subword", "algorithm", "bert", "related", "models", "greedily", "builds", "vocabulary", "units", "selects", "merges", "maximize", "likelihood"]}, {"id": "term-workload-scheduling", "t": "Workload Scheduling", "tg": ["Infrastructure", "Management"], "d": "hardware", "x": "Process of assigning and managing computational tasks across available processing resources. Efficient scheduling...", "l": "w", "k": ["workload", "scheduling", "process", "assigning", "managing", "computational", "tasks", "across", "available", "processing", "resources", "efficient", "maximizes", "gpu", "utilization"]}, {"id": "term-world-model", "t": "World Model", "tg": ["Concept", "Research"], "d": "general", "x": "An AI's internal representation of how the world works. Used to simulate outcomes and plan actions. A key concept in AI...", "l": "w", "k": ["world", "model", "internal", "representation", "works", "simulate", "outcomes", "plan", "actions", "key", "concept", "safety", "discussions", "capabilities"]}, {"id": "term-world-models", "t": "World Models", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "Learned neural network representations of environment dynamics that allow an agent to simulate and plan in a latent...", "l": "w", "k": ["world", "models", "learned", "neural", "network", "representations", "environment", "dynamics", "allow", "agent", "simulate", "plan", "latent", "space", "compress"]}, {"id": "term-world-models-algorithm", "t": "World Models Algorithm", "tg": ["Algorithms", "Technical", "RL"], "d": "algorithms", "x": "A reinforcement learning approach that trains a variational autoencoder and recurrent network to model the environment...", "l": "w", "k": ["world", "models", "algorithm", "reinforcement", "learning", "approach", "trains", "variational", "autoencoder", "recurrent", "network", "model", "environment", "dynamics", "agent"]}, {"id": "term-worlddreamer", "t": "WorldDreamer", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A world model that generates future frames and actions in complex environments using a diffusion-based architecture for...", "l": "w", "k": ["worlddreamer", "world", "model", "generates", "future", "frames", "actions", "complex", "environments", "diffusion-based", "architecture", "open-ended", "simulation"]}, {"id": "term-wsj-corpus", "t": "WSJ Corpus", "tg": ["Benchmark", "Speech"], "d": "datasets", "x": "The Wall Street Journal speech corpus containing read speech from WSJ articles. A classic benchmark for...", "l": "w", "k": ["wsj", "corpus", "wall", "street", "journal", "speech", "containing", "read", "articles", "classic", "benchmark", "large-vocabulary", "continuous", "recognition", "systems"]}, {"id": "term-wuerstchen", "t": "Wuerstchen", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A three-stage image generation architecture that compresses images to a very compact latent space for extremely...", "l": "w", "k": ["wuerstchen", "three-stage", "image", "generation", "architecture", "compresses", "images", "compact", "latent", "space", "extremely", "efficient", "diffusion-based"]}, {"id": "term-x-clip", "t": "X-CLIP", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "A cross-modal contrastive learning model that extends CLIP to video understanding by incorporating temporal information...", "l": "x", "k": ["x-clip", "cross-modal", "contrastive", "learning", "model", "extends", "clip", "video", "understanding", "incorporating", "temporal", "information", "multi-frame", "visual", "encoding"]}, {"id": "term-x-means-algorithm", "t": "X-Means Algorithm", "tg": ["Algorithms", "Technical", "Clustering"], "d": "algorithms", "x": "An extension of k-means that automatically determines the optimal number of clusters. Recursively splits clusters and...", "l": "x", "k": ["x-means", "algorithm", "extension", "k-means", "automatically", "determines", "optimal", "number", "clusters", "recursively", "splits", "uses", "bayesian", "information", "criterion"]}, {"id": "term-x-risk", "t": "X-Risk", "tg": ["AI Safety", "AI Ethics"], "d": "safety", "x": "Shorthand for existential risk, referring to catastrophic scenarios that could result in human extinction or permanent...", "l": "x", "k": ["x-risk", "shorthand", "existential", "risk", "referring", "catastrophic", "scenarios", "result", "human", "extinction", "permanent", "civilizational", "collapse", "considered", "several"]}, {"id": "term-x86-architecture", "t": "x86 Architecture", "tg": ["Architecture", "Intel", "AMD"], "d": "hardware", "x": "Intel and AMD dominant instruction set architecture for desktop and server processors. While CISC-based modern x86...", "l": "x", "k": ["x86", "architecture", "intel", "amd", "dominant", "instruction", "desktop", "server", "processors", "cisc-based", "modern", "chips", "internally", "translate", "risc-like"]}, {"id": "term-xai-company", "t": "xAI", "tg": ["Company", "LLM Provider"], "d": "models", "x": "Elon Musk's AI company, creator of the Grok chatbot. Founded in 2023, it aims to develop AI that can understand the...", "l": "x", "k": ["xai", "elon", "musk", "company", "creator", "grok", "chatbot", "founded", "aims", "develop", "understand", "universe", "access", "real-time", "twitter"]}, {"id": "term-xai", "t": "XAI (Explainable AI)", "tg": ["Field", "Transparency"], "d": "safety", "x": "The field focused on making AI decisions understandable to humans. Includes techniques for visualizing attention,...", "l": "x", "k": ["xai", "explainable", "field", "focused", "making", "decisions", "understandable", "humans", "includes", "techniques", "visualizing", "attention", "attributing", "outputs", "inputs"]}, {"id": "term-xavier-initialization", "t": "Xavier Initialization", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A weight initialization method that samples weights from a distribution scaled by the number of input and output...", "l": "x", "k": ["xavier", "initialization", "weight", "method", "samples", "weights", "distribution", "scaled", "number", "input", "output", "neurons", "designed", "maintain", "activation"]}, {"id": "term-xcopa", "t": "XCOPA", "tg": ["Benchmark", "NLP", "Multilingual", "Reasoning"], "d": "datasets", "x": "Cross-lingual Choice of Plausible Alternatives a benchmark for cross-lingual commonsense reasoning in 11 languages....", "l": "x", "k": ["xcopa", "cross-lingual", "choice", "plausible", "alternatives", "benchmark", "commonsense", "reasoning", "languages", "translates", "copa", "causal", "task", "across", "typologically"]}, {"id": "term-xerox-alto", "t": "Xerox Alto", "tg": ["Historical", "Computer", "Pioneer"], "d": "hardware", "x": "Pioneering personal computer developed at Xerox PARC in 1973 featuring a graphical user interface mouse and Ethernet....", "l": "x", "k": ["xerox", "alto", "pioneering", "personal", "computer", "developed", "parc", "featuring", "graphical", "user", "interface", "mouse", "ethernet", "influenced", "development"]}, {"id": "term-xgboost", "t": "XGBoost", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An optimized implementation of gradient boosting that uses regularized objectives, column subsampling, and efficient...", "l": "x", "k": ["xgboost", "optimized", "implementation", "gradient", "boosting", "uses", "regularized", "objectives", "column", "subsampling", "efficient", "tree", "construction", "algorithms", "includes"]}, {"id": "term-xgboost-model", "t": "XGBoost Model", "tg": ["Models", "Fundamentals"], "d": "models", "x": "An optimized gradient boosting framework that uses regularization and parallel processing and cache-aware computation...", "l": "x", "k": ["xgboost", "model", "optimized", "gradient", "boosting", "framework", "uses", "regularization", "parallel", "processing", "cache-aware", "computation", "fast", "accurate", "tree"]}, {"id": "term-xglue", "t": "XGLUE", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "A cross-lingual benchmark covering 11 tasks in 19 languages for evaluating multilingual pretrained models on both...", "l": "x", "k": ["xglue", "cross-lingual", "benchmark", "covering", "tasks", "languages", "evaluating", "multilingual", "pretrained", "models", "understanding", "generation"]}, {"id": "term-xilinx-alveo", "t": "Xilinx Alveo", "tg": ["FPGA", "AMD", "Data Center"], "d": "hardware", "x": "AMD (formerly Xilinx) FPGA accelerator cards designed for data center workloads including AI inference. Offers...", "l": "x", "k": ["xilinx", "alveo", "amd", "formerly", "fpga", "accelerator", "cards", "designed", "data", "center", "workloads", "including", "inference", "offers", "reconfigurable"]}, {"id": "term-xla-accelerated-linear-algebra", "t": "XLA (Accelerated Linear Algebra)", "tg": ["Compiler", "Google", "Optimization"], "d": "hardware", "x": "Google compiler for linear algebra operations that optimizes computation graphs for TPUs GPUs and CPUs. Used by...", "l": "x", "k": ["xla", "accelerated", "linear", "algebra", "google", "compiler", "operations", "optimizes", "computation", "graphs", "tpus", "gpus", "cpus", "tensorflow", "jax"]}, {"id": "term-xla-compiler", "t": "XLA Compiler", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Accelerated Linear Algebra, a domain-specific compiler for machine learning that optimizes computation graphs through...", "l": "x", "k": ["xla", "compiler", "accelerated", "linear", "algebra", "domain-specific", "machine", "learning", "optimizes", "computation", "graphs", "operator", "fusion", "memory", "layout"]}, {"id": "term-xlm-roberta", "t": "XLM-RoBERTa", "tg": ["Models", "Technical", "NLP", "Fundamentals"], "d": "models", "x": "A cross-lingual language model pre-trained on 2.5 terabytes of filtered CommonCrawl data in 100 languages for...", "l": "x", "k": ["xlm-roberta", "cross-lingual", "language", "model", "pre-trained", "terabytes", "filtered", "commoncrawl", "data", "languages", "multilingual", "understanding", "tasks"]}, {"id": "term-xlnet", "t": "XLNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generalized autoregressive pretraining method that uses permutation-based training to capture bidirectional context...", "l": "x", "k": ["xlnet", "generalized", "autoregressive", "pretraining", "method", "uses", "permutation-based", "training", "capture", "bidirectional", "context", "maintaining", "formulation", "overcoming", "limitations"]}, {"id": "term-xlstm", "t": "xLSTM", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "Extended Long Short-Term Memory is a modernized LSTM architecture that introduces exponential gating and novel memory...", "l": "x", "k": ["xlstm", "extended", "long", "short-term", "memory", "modernized", "lstm", "architecture", "introduces", "exponential", "gating", "novel", "structures", "competitive", "language"]}, {"id": "term-xml-prompting", "t": "XML Prompting", "tg": ["Prompt Engineering", "Output Format"], "d": "hardware", "x": "A prompting technique that uses XML tags to structure prompt sections, delimit input data, and specify output format,...", "l": "x", "k": ["xml", "prompting", "technique", "uses", "tags", "structure", "prompt", "sections", "delimit", "input", "data", "specify", "output", "format", "leveraging"]}, {"id": "term-xml-tags", "t": "XML Tags (in Prompting)", "tg": ["Prompting", "Technique"], "d": "general", "x": "Using XML-style markup tags to structure prompts and clearly delineate sections such as context, examples, or...", "l": "x", "k": ["xml", "tags", "prompting", "xml-style", "markup", "structure", "prompts", "clearly", "delineate", "sections", "context", "examples", "instructions", "helps", "models"]}, {"id": "term-xnli", "t": "XNLI", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "Cross-lingual Natural Language Inference a benchmark for evaluating cross-lingual sentence understanding across 15...", "l": "x", "k": ["xnli", "cross-lingual", "natural", "language", "inference", "benchmark", "evaluating", "sentence", "understanding", "across", "languages", "extends", "mnli", "premises", "translations"]}, {"id": "term-xor-filter-algorithm", "t": "XOR Filter Algorithm", "tg": ["Algorithms", "Technical", "Data Structure"], "d": "algorithms", "x": "A static probabilistic data structure for membership testing that achieves better space efficiency than Bloom filters....", "l": "x", "k": ["xor", "filter", "algorithm", "static", "probabilistic", "data", "structure", "membership", "testing", "achieves", "better", "space", "efficiency", "bloom", "filters"]}, {"id": "term-xp3", "t": "xP3", "tg": ["Training Corpus", "NLP", "Multilingual"], "d": "datasets", "x": "A multilingual extension of P3 covering prompted templates for datasets in 46 languages. Used to train the BLOOMZ...", "l": "x", "k": ["xp3", "multilingual", "extension", "covering", "prompted", "templates", "datasets", "languages", "train", "bloomz", "instruction-following", "model"]}, {"id": "term-xquad", "t": "XQuAD", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "Cross-lingual Question Answering Dataset a benchmark of 1190 SQuAD question-answer pairs translated into 11 languages...", "l": "x", "k": ["xquad", "cross-lingual", "question", "answering", "dataset", "benchmark", "squad", "question-answer", "pairs", "translated", "languages", "professional", "translators", "evaluation"]}, {"id": "term-xstest", "t": "XSTest", "tg": ["Benchmark", "NLP", "Safety", "Evaluation"], "d": "datasets", "x": "A benchmark for identifying exaggerated safety behaviors in language models. Contains 250 safe prompts that overly...", "l": "x", "k": ["xstest", "benchmark", "identifying", "exaggerated", "safety", "behaviors", "language", "models", "contains", "safe", "prompts", "overly", "cautious", "incorrectly", "refuse"]}, {"id": "term-xsum", "t": "XSum", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "Extreme Summarization a dataset of 226000 BBC news articles paired with single-sentence summaries. Tests the ability to...", "l": "x", "k": ["xsum", "extreme", "summarization", "dataset", "bbc", "news", "articles", "paired", "single-sentence", "summaries", "tests", "ability", "generate", "concise", "abstractive"]}, {"id": "term-xtreme", "t": "XTREME", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "Cross-lingual TRansfer Evaluation of Multilingual Encoders a benchmark of 9 tasks covering 40 diverse languages. Tests...", "l": "x", "k": ["xtreme", "cross-lingual", "transfer", "evaluation", "multilingual", "encoders", "benchmark", "tasks", "covering", "diverse", "languages", "tests", "language", "models"]}, {"id": "term-xtreme-r", "t": "XTREME-R", "tg": ["Benchmark", "NLP", "Multilingual"], "d": "datasets", "x": "An updated version of XTREME with more challenging tasks and broader language coverage. Addresses limitations of the...", "l": "x", "k": ["xtreme-r", "updated", "version", "xtreme", "challenging", "tasks", "broader", "language", "coverage", "addresses", "limitations", "original", "benchmark", "evaluating", "multilingual"]}, {"id": "term-xtts", "t": "XTTS", "tg": ["Models", "Technical", "Audio"], "d": "models", "x": "A deep learning text-to-speech model from Coqui that supports voice cloning and multilingual speech synthesis with...", "l": "x", "k": ["xtts", "deep", "learning", "text-to-speech", "model", "coqui", "supports", "voice", "cloning", "multilingual", "speech", "synthesis", "minimal", "reference", "audio"]}, {"id": "term-xverse", "t": "XVERSE", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A Chinese large language model series supporting both Chinese and English with models ranging from 7B to 65B parameters.", "l": "x", "k": ["xverse", "chinese", "large", "language", "model", "series", "supporting", "english", "models", "ranging", "65b", "parameters"]}, {"id": "term-xview", "t": "xView", "tg": ["Benchmark", "Computer Vision", "Remote Sensing"], "d": "datasets", "x": "A large overhead imagery dataset containing over 1 million object instances across 60 classes from high-resolution...", "l": "x", "k": ["xview", "large", "overhead", "imagery", "dataset", "containing", "million", "object", "instances", "across", "classes", "high-resolution", "satellite", "images", "tests"]}, {"id": "term-yann-lecun", "t": "Yann LeCun", "tg": ["History", "Pioneers"], "d": "history", "x": "French-American computer scientist who developed convolutional neural networks in the late 1980s and applied them to...", "l": "y", "k": ["yann", "lecun", "french-american", "computer", "scientist", "developed", "convolutional", "neural", "networks", "late", "1980s", "applied", "handwritten", "digit", "recognition"]}, {"id": "term-yarn", "t": "YaRN", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Yet another RoPE extensioN method combines NTK-aware interpolation with attention scaling for robust context length...", "l": "y", "k": ["yarn", "another", "rope", "extension", "method", "combines", "ntk-aware", "interpolation", "attention", "scaling", "robust", "context", "length", "achieves", "state-of-the-art"]}, {"id": "term-yelp-reviews", "t": "Yelp Reviews", "tg": ["Benchmark", "NLP"], "d": "datasets", "x": "A dataset of millions of business reviews from Yelp used for sentiment analysis and text classification. Available in...", "l": "y", "k": ["yelp", "reviews", "dataset", "millions", "business", "sentiment", "analysis", "text", "classification", "available", "multiple", "versions", "different", "nlp", "tasks"]}, {"id": "term-yeo-johnson-transformation", "t": "Yeo-Johnson Transformation", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A power transformation similar to Box-Cox that can handle both positive and negative values. It extends the Box-Cox...", "l": "y", "k": ["yeo-johnson", "transformation", "power", "similar", "box-cox", "handle", "positive", "negative", "values", "extends", "defining", "appropriate", "transformations", "non-positive", "data"]}, {"id": "term-yi", "t": "Yi", "tg": ["Model", "Open Source"], "d": "models", "x": "A series of open-source bilingual (Chinese/English) LLMs from 01.AI. Known for strong performance across benchmarks and...", "l": "y", "k": ["series", "open-source", "bilingual", "chinese", "english", "llms", "known", "strong", "performance", "across", "benchmarks", "contributing", "development", "asia"]}, {"id": "term-yi-15", "t": "Yi-1.5", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "An upgraded version of the Yi language model family from 01.AI with improved reasoning and longer context support...", "l": "y", "k": ["yi-1", "upgraded", "version", "language", "model", "family", "improved", "reasoning", "longer", "context", "support", "across", "34b", "sizes"]}, {"id": "term-yi-large", "t": "Yi-Large", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "The largest variant in the Yi model family from 01.AI offering enhanced reasoning and knowledge capabilities for...", "l": "y", "k": ["yi-large", "largest", "variant", "model", "family", "offering", "enhanced", "reasoning", "knowledge", "capabilities", "complex", "language", "understanding", "tasks"]}, {"id": "term-yi-vision", "t": "Yi-Vision", "tg": ["Models", "Technical", "NLP", "Vision"], "d": "models", "x": "A multimodal extension of the Yi model family that adds image understanding capabilities for visual question answering...", "l": "y", "k": ["yi-vision", "multimodal", "extension", "model", "family", "adds", "image", "understanding", "capabilities", "visual", "question", "answering", "document", "analysis"]}, {"id": "term-yolo", "t": "YOLO (You Only Look Once)", "tg": ["Architecture", "Computer Vision"], "d": "models", "x": "A real-time object detection algorithm that processes images in a single pass. Revolutionary for its speed, enabling...", "l": "y", "k": ["yolo", "look", "real-time", "object", "detection", "algorithm", "processes", "images", "single", "pass", "revolutionary", "speed", "enabling", "video", "standard"]}, {"id": "term-yolo-world", "t": "YOLO-World", "tg": ["Models", "Technical", "Vision", "NLP"], "d": "models", "x": "An open-vocabulary object detection model that extends YOLO with vision-language modeling for detecting objects from...", "l": "y", "k": ["yolo-world", "open-vocabulary", "object", "detection", "model", "extends", "yolo", "vision-language", "modeling", "detecting", "objects", "free-form", "text", "descriptions"]}, {"id": "term-yolov10", "t": "YOLOv10", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A real-time end-to-end object detection model that eliminates non-maximum suppression through consistent dual...", "l": "y", "k": ["yolov10", "real-time", "end-to-end", "object", "detection", "model", "eliminates", "non-maximum", "suppression", "consistent", "dual", "assignments", "efficient", "inference"]}, {"id": "term-yolov5", "t": "YOLOv5", "tg": ["Models", "Technical"], "d": "models", "x": "A popular implementation of the YOLO object detection family released by Ultralytics in PyTorch. Known for ease of use...", "l": "y", "k": ["yolov5", "popular", "implementation", "yolo", "object", "detection", "family", "released", "ultralytics", "pytorch", "known", "ease", "fast", "training", "strong"]}, {"id": "term-yolov8", "t": "YOLOv8", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A state-of-the-art real-time object detection model in the YOLO family that introduces an anchor-free detection head,...", "l": "y", "k": ["yolov8", "state-of-the-art", "real-time", "object", "detection", "model", "yolo", "family", "introduces", "anchor-free", "head", "decoupled", "classification", "regression", "branches"]}, {"id": "term-yolov9", "t": "YOLOv9", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A YOLO variant that introduces Programmable Gradient Information and the Generalized Efficient Layer Aggregation...", "l": "y", "k": ["yolov9", "yolo", "variant", "introduces", "programmable", "gradient", "information", "generalized", "efficient", "layer", "aggregation", "network", "improved", "object", "detection"]}, {"id": "term-yoshua-bengio", "t": "Yoshua Bengio", "tg": ["History", "Pioneers"], "d": "history", "x": "Canadian computer scientist who made foundational contributions to deep learning, including neural language models and...", "l": "y", "k": ["yoshua", "bengio", "canadian", "computer", "scientist", "foundational", "contributions", "deep", "learning", "including", "neural", "language", "models", "generative", "adversarial"]}, {"id": "term-yoshua-bengio-universal-approximation", "t": "Yoshua Bengio Universal Approximation", "tg": ["History", "Milestones"], "d": "history", "x": "The 1989 work by George Cybenko and subsequent contributions by Kurt Hornik and others proving that neural networks...", "l": "y", "k": ["yoshua", "bengio", "universal", "approximation", "work", "george", "cybenko", "subsequent", "contributions", "kurt", "hornik", "others", "proving", "neural", "networks"]}, {"id": "term-youcook2", "t": "YouCook2", "tg": ["Benchmark", "Video", "NLP"], "d": "datasets", "x": "A video dataset of 2000 cooking videos from YouTube with procedure step annotations and recipe descriptions. Used for...", "l": "y", "k": ["youcook2", "video", "dataset", "cooking", "videos", "youtube", "procedure", "step", "annotations", "recipe", "descriptions", "video-language", "understanding", "learning", "research"]}, {"id": "term-yt-temporal-180m", "t": "YT-Temporal-180M", "tg": ["Training Corpus", "Video", "Multimodal"], "d": "datasets", "x": "A dataset of 180 million video clips from YouTube with ASR transcripts for temporal video-language pretraining. One of...", "l": "y", "k": ["yt-temporal-180m", "dataset", "million", "video", "clips", "youtube", "asr", "transcripts", "temporal", "video-language", "pretraining", "largest", "video-text", "datasets", "videoclip"]}, {"id": "term-yuan-20", "t": "Yuan 2.0", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A large language model from IEIT Systems featuring a Localized Filtering-based Attention mechanism for improved...", "l": "y", "k": ["yuan", "large", "language", "model", "ieit", "systems", "featuring", "localized", "filtering-based", "attention", "mechanism", "improved", "efficiency", "long-context", "processing"]}, {"id": "term-z-algorithm", "t": "Z-Algorithm", "tg": ["Algorithms", "Technical", "NLP", "Searching"], "d": "algorithms", "x": "A linear-time string matching algorithm that computes a Z-array where each entry stores the length of the longest...", "l": "z", "k": ["z-algorithm", "linear-time", "string", "matching", "algorithm", "computes", "z-array", "entry", "stores", "length", "longest", "substring", "starting", "position", "matches"]}, {"id": "term-z-score", "t": "Z-Score", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A standardized score indicating how many standard deviations a data point is from the mean of its distribution. It...", "l": "z", "k": ["z-score", "standardized", "score", "indicating", "standard", "deviations", "data", "point", "mean", "distribution", "allows", "comparison", "values", "different", "distributions"]}, {"id": "term-zamba", "t": "Zamba", "tg": ["Models", "Technical", "NLP"], "d": "models", "x": "A hybrid model from Zyphra that interleaves Mamba blocks with a shared attention layer for efficient long-context...", "l": "z", "k": ["zamba", "hybrid", "model", "zyphra", "interleaves", "mamba", "blocks", "shared", "attention", "layer", "efficient", "long-context", "language", "modeling", "parameter"]}, {"id": "term-zebralogic", "t": "ZebraLogic", "tg": ["Benchmark", "NLP", "Reasoning"], "d": "datasets", "x": "A logical reasoning benchmark testing language models on constraint satisfaction problems. Evaluates systematic logical...", "l": "z", "k": ["zebralogic", "logical", "reasoning", "benchmark", "testing", "language", "models", "constraint", "satisfaction", "problems", "evaluates", "systematic", "deduction", "capabilities"]}, {"id": "term-zephyr", "t": "Zephyr", "tg": ["Model", "Open Source"], "d": "models", "x": "A series of fine-tuned open LLMs from Hugging Face optimized for helpful assistants. Demonstrates how smaller models...", "l": "z", "k": ["zephyr", "series", "fine-tuned", "open", "llms", "hugging", "face", "optimized", "helpful", "assistants", "demonstrates", "smaller", "models", "good", "alignment"]}, {"id": "term-zero-zero-redundancy-optimizer", "t": "ZeRO (Zero Redundancy Optimizer)", "tg": ["Distributed Training", "Optimization", "Microsoft"], "d": "hardware", "x": "Memory optimization technique from Microsoft DeepSpeed that partitions optimizer states gradients and parameters across...", "l": "z", "k": ["zero", "redundancy", "optimizer", "memory", "optimization", "technique", "microsoft", "deepspeed", "partitions", "states", "gradients", "parameters", "across", "devices", "enables"]}, {"id": "term-zero-optimization", "t": "ZeRO Optimization", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Zero Redundancy Optimizer, a distributed training technique that partitions optimizer states, gradients, and parameters...", "l": "z", "k": ["zero", "optimization", "redundancy", "optimizer", "distributed", "training", "technique", "partitions", "states", "gradients", "parameters", "across", "data-parallel", "processes", "reduce"]}, {"id": "term-zero-1-to-3", "t": "Zero-1-to-3", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A model that generates novel views of an object from a single image by fine-tuning a diffusion model with camera pose...", "l": "z", "k": ["zero-1-to-3", "model", "generates", "novel", "views", "object", "single", "image", "fine-tuning", "diffusion", "camera", "pose", "conditioning"]}, {"id": "term-zero-crossing-rate-algorithm", "t": "Zero-Crossing Rate Algorithm", "tg": ["Algorithms", "Technical", "Signal Processing"], "d": "algorithms", "x": "A signal feature that counts the number of times a signal crosses zero within a frame. Used in speech and audio...", "l": "z", "k": ["zero-crossing", "rate", "algorithm", "signal", "feature", "counts", "number", "times", "crosses", "zero", "within", "frame", "speech", "audio", "analysis"]}, {"id": "term-zero-day", "t": "Zero-Day (AI Context)", "tg": ["Security", "Safety"], "d": "safety", "x": "Novel vulnerabilities or attack vectors discovered in AI systems before developers know about them. AI security...", "l": "z", "k": ["zero-day", "context", "novel", "vulnerabilities", "attack", "vectors", "discovered", "systems", "developers", "know", "security", "research", "increasingly", "focuses", "finding"]}, {"id": "term-zero-shot-cot", "t": "Zero-Shot Chain-of-Thought", "tg": ["Prompting", "Reasoning"], "d": "general", "x": "Adding \"Let's think step by step\" to a prompt to trigger reasoning without providing examples. A simple but effective...", "l": "z", "k": ["zero-shot", "chain-of-thought", "adding", "let", "think", "step", "prompt", "trigger", "reasoning", "without", "providing", "examples", "simple", "effective", "technique"]}, {"id": "term-zero-shot", "t": "Zero-Shot Learning", "tg": ["Prompting", "Technique"], "d": "general", "x": "When AI performs a task without examples in the prompt, relying entirely on pre-trained knowledge and instructions....", "l": "z", "k": ["zero-shot", "learning", "performs", "task", "without", "examples", "prompt", "relying", "entirely", "pre-trained", "knowledge", "instructions", "contrasts", "few-shot", "provided"]}, {"id": "term-zero-shot-learning-history", "t": "Zero-Shot Learning History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of zero-shot learning from early attribute-based approaches to modern large language models that...", "l": "z", "k": ["zero-shot", "learning", "history", "development", "early", "attribute-based", "approaches", "modern", "large", "language", "models", "perform", "tasks", "without", "task-specific"]}, {"id": "term-zero-shot-ner", "t": "Zero-Shot NER", "tg": ["NLP", "Text Processing"], "d": "general", "x": "Named entity recognition performed on entity types not seen during training, using natural language descriptions of...", "l": "z", "k": ["zero-shot", "ner", "named", "entity", "recognition", "performed", "types", "seen", "training", "natural", "language", "descriptions", "categories", "generalize", "without"]}, {"id": "term-zero-shot-object-detection", "t": "Zero-Shot Object Detection", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "The ability to detect and localize objects of novel categories without any training examples, using vision-language...", "l": "z", "k": ["zero-shot", "object", "detection", "ability", "detect", "localize", "objects", "novel", "categories", "without", "training", "examples", "vision-language", "alignment", "match"]}, {"id": "term-zero-trust-architecture-for-ai", "t": "Zero-Trust Architecture for AI", "tg": ["Safety", "Technical"], "d": "safety", "x": "A security framework that requires continuous verification of all users devices and AI components regardless of their...", "l": "z", "k": ["zero-trust", "architecture", "security", "framework", "requires", "continuous", "verification", "users", "devices", "components", "regardless", "position", "within", "outside", "network"]}, {"id": "term-zeroscrolls", "t": "ZeroSCROLLS", "tg": ["Benchmark", "NLP", "Evaluation"], "d": "datasets", "x": "A zero-shot benchmark for long text understanding covering summarization question answering and aggregation tasks over...", "l": "z", "k": ["zeroscrolls", "zero-shot", "benchmark", "long", "text", "understanding", "covering", "summarization", "question", "answering", "aggregation", "tasks", "documents"]}, {"id": "term-zettascale-computing", "t": "Zettascale Computing", "tg": ["Computing", "Milestone", "Future"], "d": "hardware", "x": "Next computing milestone beyond exascale targeting one zettaFLOPS or a sextillion operations per second. Expected to...", "l": "z", "k": ["zettascale", "computing", "next", "milestone", "beyond", "exascale", "targeting", "zettaflops", "sextillion", "operations", "per", "expected", "enable", "models", "unprecedented"]}, {"id": "term-zilog-z80", "t": "Zilog Z80", "tg": ["Historical", "Processor", "Consumer"], "d": "hardware", "x": "Popular 8-bit microprocessor from 1976 used in many early personal computers and embedded systems. Compatible with...", "l": "z", "k": ["zilog", "z80", "popular", "8-bit", "microprocessor", "early", "personal", "computers", "embedded", "systems", "compatible", "intel", "code", "added", "significant"]}, {"id": "term-zinc", "t": "ZINC", "tg": ["Training Corpus", "Scientific"], "d": "datasets", "x": "A database of commercially available chemical compounds for virtual screening. Contains over 230 million compounds used...", "l": "z", "k": ["zinc", "database", "commercially", "available", "chemical", "compounds", "virtual", "screening", "contains", "million", "drug", "discovery", "molecular", "generation", "research"]}, {"id": "term-zoedepth", "t": "ZoeDepth", "tg": ["Models", "Technical", "Vision"], "d": "models", "x": "A monocular depth estimation model that combines relative and metric depth estimation using a two-stage training...", "l": "z", "k": ["zoedepth", "monocular", "depth", "estimation", "model", "combines", "relative", "metric", "two-stage", "training", "approach", "accurate", "real-world"]}, {"id": "term-zuse-z3", "t": "Zuse Z3", "tg": ["Historical", "Computer", "Pioneer"], "d": "hardware", "x": "Electromechanical computer built by Konrad Zuse in 1941 in Berlin. Considered the world first programmable fully...", "l": "z", "k": ["zuse", "electromechanical", "computer", "built", "konrad", "berlin", "considered", "world", "programmable", "fully", "automatic", "digital", "floating-point", "binary", "arithmetic"]}]