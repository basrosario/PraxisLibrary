[{"id": "term-a-search", "t": "A* Search", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A graph search algorithm that finds the shortest path by combining the actual cost from the start node with a heuristic...", "l": "a", "k": ["search", "graph", "algorithm", "finds", "shortest", "path", "combining", "actual", "cost", "start", "node", "heuristic", "estimate", "remaining", "guarantees"]}, {"id": "term-a-star-search-algorithm", "t": "A* Search Algorithm", "tg": ["History", "Milestones"], "d": "history", "x": "A graph search algorithm developed by Peter Hart, Nils Nilsson, and Bertram Raphael at SRI International in 1968 that...", "l": "a", "k": ["search", "algorithm", "graph", "developed", "peter", "hart", "nils", "nilsson", "bertram", "raphael", "sri", "international", "finds", "shortest", "path"]}, {"id": "term-ab-testing", "t": "A/B Testing", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A randomized controlled experiment that compares two variants (A and B) to determine which performs better on a...", "l": "a", "k": ["testing", "randomized", "controlled", "experiment", "compares", "variants", "determine", "performs", "better", "specified", "metric", "widely", "product", "development", "data-driven"]}, {"id": "term-ab-testing-for-llms", "t": "A/B Testing for LLMs", "tg": ["Evaluation", "Methodology"], "d": "datasets", "x": "A comparative evaluation methodology where two language model variants or prompt configurations are deployed to...", "l": "a", "k": ["testing", "llms", "comparative", "evaluation", "methodology", "language", "model", "variants", "prompt", "configurations", "deployed", "different", "user", "segments", "statistical"]}, {"id": "term-aaai-organization", "t": "AAAI (Organization)", "tg": ["History", "Organizations"], "d": "history", "x": "The Association for the Advancement of Artificial Intelligence founded in 1979 (originally the American Association for...", "l": "a", "k": ["aaai", "organization", "association", "advancement", "artificial", "intelligence", "founded", "originally", "american", "nonprofit", "scientific", "society", "devoted", "advancing", "understanding"]}, {"id": "term-aaai-conference", "t": "AAAI Conference", "tg": ["History", "Conferences"], "d": "history", "x": "The Association for the Advancement of Artificial Intelligence conference held annually since 1980. One of the premier...", "l": "a", "k": ["aaai", "conference", "association", "advancement", "artificial", "intelligence", "held", "annually", "premier", "conferences", "covering", "areas", "research", "robotics", "natural"]}, {"id": "term-aaron", "t": "AARON", "tg": ["History", "Systems"], "d": "history", "x": "An art-generating program created by Harold Cohen beginning in 1973. One of the longest-running AI art projects AARON...", "l": "a", "k": ["aaron", "art-generating", "program", "created", "harold", "cohen", "beginning", "longest-running", "art", "projects", "evolved", "producing", "abstract", "drawings", "creating"]}, {"id": "term-abductive-reasoning", "t": "Abductive Reasoning", "tg": ["History", "Fundamentals"], "d": "history", "x": "A form of logical inference that starts with an observation and seeks the simplest or most likely explanation. Used in...", "l": "a", "k": ["abductive", "reasoning", "form", "logical", "inference", "starts", "observation", "seeks", "simplest", "likely", "explanation", "diagnosis", "hypothesis", "generation", "plan"]}, {"id": "term-ablation-study", "t": "Ablation Study", "tg": ["Research", "Methodology"], "d": "general", "x": "A research technique that removes components of a model to understand their contribution. Helps researchers understand...", "l": "a", "k": ["ablation", "study", "research", "technique", "removes", "components", "model", "understand", "contribution", "helps", "researchers", "parts", "system", "responsible", "capabilities"]}, {"id": "term-absolute-position-encoding", "t": "Absolute Position Encoding", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A method of injecting position information into transformer inputs by adding a fixed or learned vector to each...", "l": "a", "k": ["absolute", "position", "encoding", "method", "injecting", "information", "transformer", "inputs", "adding", "fixed", "learned", "vector", "original", "sinusoidal", "functions"]}, {"id": "term-amr", "t": "Abstract Meaning Representation", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A semantic representation that encodes the meaning of a sentence as a rooted directed acyclic graph, abstracting away...", "l": "a", "k": ["abstract", "meaning", "representation", "semantic", "encodes", "sentence", "rooted", "directed", "acyclic", "graph", "abstracting", "away", "syntactic", "details", "capture"]}, {"id": "term-abstractive-summarization", "t": "Abstractive Summarization", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A summarization approach that generates novel sentences capturing the key information from the source text, potentially...", "l": "a", "k": ["abstractive", "summarization", "approach", "generates", "novel", "sentences", "capturing", "key", "information", "source", "text", "potentially", "words", "phrasings", "present"]}, {"id": "term-academic-prompting", "t": "Academic Prompting", "tg": ["Prompt Engineering", "Academic"], "d": "general", "x": "A prompting approach tailored for scholarly tasks that instructs the model to follow academic conventions including...", "l": "a", "k": ["academic", "prompting", "approach", "tailored", "scholarly", "tasks", "instructs", "model", "follow", "conventions", "including", "formal", "tone", "citation", "awareness"]}, {"id": "term-accountability-in-ai", "t": "Accountability in AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The principle that identifiable individuals or organizations should be answerable for the outcomes and impacts of AI...", "l": "a", "k": ["accountability", "principle", "identifiable", "individuals", "organizations", "answerable", "outcomes", "impacts", "systems", "including", "mechanisms", "redress", "causes", "harm"]}, {"id": "term-accumulated-local-effects", "t": "Accumulated Local Effects", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A model-agnostic method for visualizing feature effects that is unbiased in the presence of correlated features, unlike...", "l": "a", "k": ["accumulated", "local", "effects", "model-agnostic", "method", "visualizing", "feature", "unbiased", "presence", "correlated", "features", "unlike", "partial", "dependence", "plots"]}, {"id": "term-accuracy", "t": "Accuracy", "tg": ["Metrics", "Evaluation"], "d": "datasets", "x": "A metric measuring how often a model's predictions are correct. Calculated as the ratio of correct predictions to total...", "l": "a", "k": ["accuracy", "metric", "measuring", "model", "predictions", "correct", "calculated", "ratio", "total", "intuitive", "misleading", "imbalanced", "datasets"]}, {"id": "term-acl-conference", "t": "ACL Conference", "tg": ["History", "Conferences"], "d": "history", "x": "The Annual Meeting of the Association for Computational Linguistics first held in 1963. The premier conference for...", "l": "a", "k": ["acl", "conference", "annual", "meeting", "association", "computational", "linguistics", "held", "premier", "natural", "language", "processing", "research", "foundational", "papers"]}, {"id": "term-acm-organization", "t": "ACM (Organization)", "tg": ["History", "Organizations"], "d": "history", "x": "The Association for Computing Machinery founded in 1947 as the world's largest educational and scientific computing...", "l": "a", "k": ["acm", "organization", "association", "computing", "machinery", "founded", "world", "largest", "educational", "scientific", "society", "administers", "turing", "award", "called"]}, {"id": "term-act-r", "t": "ACT-R", "tg": ["History", "Systems"], "d": "history", "x": "A cognitive architecture developed by John Robert Anderson at Carnegie Mellon University since 1993. ACT-R models human...", "l": "a", "k": ["act-r", "cognitive", "architecture", "developed", "john", "robert", "anderson", "carnegie", "mellon", "university", "models", "human", "cognition", "interaction", "declarative"]}, {"id": "term-action", "t": "Action", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A decision or move taken by an RL agent that affects the environment and transitions the system to a new state. Actions...", "l": "a", "k": ["action", "decision", "move", "taken", "agent", "affects", "environment", "transitions", "system", "state", "actions", "discrete", "finite", "choices", "continuous"]}, {"id": "term-action-masking", "t": "Action Masking", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A technique that restricts the set of available actions at each state by zeroing out invalid action probabilities...", "l": "a", "k": ["action", "masking", "technique", "restricts", "available", "actions", "state", "zeroing", "invalid", "probabilities", "policy", "sampling", "enforces", "domain", "constraints"]}, {"id": "term-action-recognition", "t": "Action Recognition", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A video understanding task that identifies and classifies human actions or activities in video sequences, using...", "l": "a", "k": ["action", "recognition", "video", "understanding", "task", "identifies", "classifies", "human", "actions", "activities", "sequences", "temporal", "modeling", "motion", "patterns"]}, {"id": "term-action-repeat", "t": "Action Repeat", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A technique where each selected action is executed for multiple consecutive environment steps, reducing the effective...", "l": "a", "k": ["action", "repeat", "technique", "selected", "executed", "multiple", "consecutive", "environment", "steps", "reducing", "effective", "decision", "frequency", "simplifies", "control"]}, {"id": "term-action-space", "t": "Action Space", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The set of all possible actions available to an RL agent, defined as discrete (finite choices), continuous (real-valued...", "l": "a", "k": ["action", "space", "possible", "actions", "available", "agent", "defined", "discrete", "finite", "choices", "continuous", "real-valued", "vectors", "multi-discrete", "structure"]}, {"id": "term-activation-checkpointing", "t": "Activation Checkpointing", "tg": ["LLM", "Inference"], "d": "models", "x": "A memory optimization technique that trades compute for memory by discarding intermediate activations during the...", "l": "a", "k": ["activation", "checkpointing", "memory", "optimization", "technique", "trades", "compute", "discarding", "intermediate", "activations", "forward", "pass", "recomputing", "backpropagation"]}, {"id": "term-activation-function", "t": "Activation Function", "tg": ["Neural Networks", "Technical"], "d": "models", "x": "A mathematical function applied to neurons in neural networks that introduces non-linearity, enabling the network to...", "l": "a", "k": ["activation", "function", "mathematical", "applied", "neurons", "neural", "networks", "introduces", "non-linearity", "enabling", "network", "learn", "complex", "patterns", "common"]}, {"id": "term-activation-patching", "t": "Activation Patching", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An interpretability technique that replaces activations at specific positions and layers with activations from a...", "l": "a", "k": ["activation", "patching", "interpretability", "technique", "replaces", "activations", "specific", "positions", "layers", "different", "input", "determine", "causal", "effects", "identifies"]}, {"id": "term-activation-quantization", "t": "Activation Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "The process of quantizing intermediate activations (not just weights) during inference to reduce memory bandwidth...", "l": "a", "k": ["activation", "quantization", "process", "quantizing", "intermediate", "activations", "weights", "inference", "reduce", "memory", "bandwidth", "requirements", "enable", "int8", "lower-precision"]}, {"id": "term-active-learning", "t": "Active Learning", "tg": ["Training", "Technique"], "d": "general", "x": "A training approach where the model identifies which unlabeled examples would be most valuable to learn from. Reduces...", "l": "a", "k": ["active", "learning", "training", "approach", "model", "identifies", "unlabeled", "examples", "valuable", "learn", "reduces", "labeling", "costs", "focusing", "human"]}, {"id": "term-active-learning-algorithm", "t": "Active Learning Algorithm", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A machine learning approach where the model selectively queries an oracle for labels on the most informative unlabeled...", "l": "a", "k": ["active", "learning", "algorithm", "machine", "approach", "model", "selectively", "queries", "oracle", "labels", "informative", "unlabeled", "examples", "reduces", "labeling"]}, {"id": "term-active-prompting", "t": "Active Prompting", "tg": ["Prompt Engineering", "Active Learning"], "d": "general", "x": "A method that identifies the most uncertain or informative questions for chain-of-thought annotation by measuring model...", "l": "a", "k": ["active", "prompting", "method", "identifies", "uncertain", "informative", "questions", "chain-of-thought", "annotation", "measuring", "model", "disagreement", "across", "sampled", "outputs"]}, {"id": "term-actor-critic", "t": "Actor-Critic", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An RL architecture combining a policy network (actor) that selects actions with a value network (critic) that evaluates...", "l": "a", "k": ["actor-critic", "architecture", "combining", "policy", "network", "actor", "selects", "actions", "value", "critic", "evaluates", "estimates", "reduce", "variance", "gradient"]}, {"id": "term-ada-lovelace", "t": "Ada Lovelace", "tg": ["History", "Pioneers"], "d": "history", "x": "British mathematician (1815-1852) who wrote the first published algorithm intended for a machine, working with Charles...", "l": "a", "k": ["ada", "lovelace", "british", "mathematician", "1815-1852", "wrote", "published", "algorithm", "intended", "machine", "working", "charles", "babbage", "analytical", "engine"]}, {"id": "term-adaboost", "t": "AdaBoost", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An ensemble method that trains weak learners sequentially, assigning higher weights to misclassified samples so that...", "l": "a", "k": ["adaboost", "ensemble", "method", "trains", "weak", "learners", "sequentially", "assigning", "higher", "weights", "misclassified", "samples", "subsequent", "focus", "hardest"]}, {"id": "term-adadelta", "t": "AdaDelta", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An adaptive learning rate optimizer that eliminates the need for a manually set global learning rate. Adapts based on a...", "l": "a", "k": ["adadelta", "adaptive", "learning", "rate", "optimizer", "eliminates", "need", "manually", "global", "adapts", "based", "moving", "window", "gradient", "updates"]}, {"id": "term-adafactor", "t": "Adafactor", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A memory-efficient adaptive optimizer that factorizes the second moment accumulator into row and column factors....", "l": "a", "k": ["adafactor", "memory-efficient", "adaptive", "optimizer", "factorizes", "moment", "accumulator", "row", "column", "factors", "reduces", "memory", "usage", "weight", "matrices"]}, {"id": "term-adagrad", "t": "AdaGrad", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "An optimization algorithm that adapts the learning rate for each parameter individually by dividing by the square root...", "l": "a", "k": ["adagrad", "optimization", "algorithm", "adapts", "learning", "rate", "parameter", "individually", "dividing", "square", "root", "sum", "historical", "squared", "gradients"]}, {"id": "term-adam", "t": "Adam Optimizer", "tg": ["Training", "Algorithm"], "d": "algorithms", "x": "A popular optimization algorithm combining momentum with adaptive learning rates. The default choice for training many...", "l": "a", "k": ["adam", "optimizer", "popular", "optimization", "algorithm", "combining", "momentum", "adaptive", "learning", "rates", "default", "choice", "training", "neural", "networks"]}, {"id": "term-adamw", "t": "AdamW", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A variant of the Adam optimizer that decouples weight decay from the gradient update. Proposed by Loshchilov and Hutter...", "l": "a", "k": ["adamw", "variant", "adam", "optimizer", "decouples", "weight", "decay", "gradient", "update", "proposed", "loshchilov", "hutter", "shown", "provide", "better"]}, {"id": "term-adapter-layer", "t": "Adapter Layer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A small trainable module inserted between frozen pretrained layers that learns task-specific transformations with...", "l": "a", "k": ["adapter", "layer", "small", "trainable", "module", "inserted", "frozen", "pretrained", "layers", "learns", "task-specific", "transformations", "minimal", "additional", "parameters"]}, {"id": "term-adapter-layers", "t": "Adapter Layers", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Small trainable modules inserted between the layers of a pretrained model for parameter-efficient fine-tuning....", "l": "a", "k": ["adapter", "layers", "small", "trainable", "modules", "inserted", "pretrained", "model", "parameter-efficient", "fine-tuning", "typically", "consist", "down-projection", "nonlinearity", "up-projection"]}, {"id": "term-adaptive-pooling", "t": "Adaptive Pooling", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A pooling operation that automatically adjusts its kernel size and stride to produce a specified output size regardless...", "l": "a", "k": ["adaptive", "pooling", "operation", "automatically", "adjusts", "kernel", "size", "stride", "produce", "specified", "output", "regardless", "input", "dimensions", "commonly"]}, {"id": "term-additive-attention", "t": "Additive Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that computes compatibility scores by passing the concatenation of query and key through a...", "l": "a", "k": ["additive", "attention", "mechanism", "computes", "compatibility", "scores", "passing", "concatenation", "query", "key", "feedforward", "layer", "known", "bahdanau", "early"]}, {"id": "term-adjusted-rand-index", "t": "Adjusted Rand Index", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A clustering evaluation metric that measures agreement between two clusterings adjusted for chance. Ranges from -1 to 1...", "l": "a", "k": ["adjusted", "rand", "index", "clustering", "evaluation", "metric", "measures", "agreement", "clusterings", "chance", "ranges", "indicating", "perfect", "random", "corrects"]}, {"id": "term-a2c", "t": "Advantage Actor-Critic (A2C)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A synchronous variant of the actor-critic method that uses the advantage function (difference between action value and...", "l": "a", "k": ["advantage", "actor-critic", "a2c", "synchronous", "variant", "method", "uses", "function", "difference", "action", "value", "state", "reduce", "variance", "policy"]}, {"id": "term-advantage-function", "t": "Advantage Function", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "The difference A(s,a) = Q(s,a) - V(s) between the action-value and state-value functions, measuring how much better an...", "l": "a", "k": ["advantage", "function", "difference", "action-value", "state-value", "functions", "measuring", "better", "action", "compared", "average", "current", "policy", "reduces", "variance"]}, {"id": "term-advantage-weighted-regression", "t": "Advantage-Weighted Regression (AWR)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An offline RL algorithm that learns a policy by performing weighted maximum likelihood on a dataset, where the weights...", "l": "a", "k": ["advantage-weighted", "regression", "awr", "offline", "algorithm", "learns", "policy", "performing", "weighted", "maximum", "likelihood", "dataset", "weights", "exponentiated", "advantages"]}, {"id": "term-adversarial-attack", "t": "Adversarial Attack", "tg": ["Security", "Safety"], "d": "safety", "x": "Deliberate attempts to deceive AI systems by providing specially crafted inputs. These can cause models to make...", "l": "a", "k": ["adversarial", "attack", "deliberate", "attempts", "deceive", "systems", "providing", "specially", "crafted", "inputs", "cause", "models", "incorrect", "predictions", "generate"]}, {"id": "term-adversarial-example-cv", "t": "Adversarial Example", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "An input image with carefully crafted, often imperceptible perturbations that cause a vision model to make incorrect...", "l": "a", "k": ["adversarial", "example", "input", "image", "carefully", "crafted", "imperceptible", "perturbations", "cause", "vision", "model", "incorrect", "predictions", "high", "confidence"]}, {"id": "term-adversarial-prompting", "t": "Adversarial Prompting", "tg": ["Prompt Engineering", "Safety"], "d": "safety", "x": "The deliberate crafting of inputs designed to exploit vulnerabilities in language models, causing them to produce...", "l": "a", "k": ["adversarial", "prompting", "deliberate", "crafting", "inputs", "designed", "exploit", "vulnerabilities", "language", "models", "causing", "produce", "harmful", "outputs", "bypass"]}, {"id": "term-adversarial-training", "t": "Adversarial Training", "tg": ["Algorithms", "Safety"], "d": "algorithms", "x": "A training procedure that augments the training set with adversarial examples generated during training. The model...", "l": "a", "k": ["adversarial", "training", "procedure", "augments", "examples", "generated", "model", "learns", "correctly", "classify", "clean", "inputs", "improving", "robustness", "currently"]}, {"id": "term-affinity-propagation", "t": "Affinity Propagation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A clustering algorithm that identifies exemplars among data points by passing messages between pairs. Does not require...", "l": "a", "k": ["affinity", "propagation", "clustering", "algorithm", "identifies", "exemplars", "among", "data", "points", "passing", "messages", "pairs", "require", "specifying", "number"]}, {"id": "term-agent", "t": "Agent (AI Agent)", "tg": ["Architecture", "Advanced"], "d": "models", "x": "An AI system that can perceive its environment, make decisions, and take actions to achieve goals. Modern AI agents can...", "l": "a", "k": ["agent", "system", "perceive", "environment", "decisions", "take", "actions", "achieve", "goals", "modern", "agents", "tools", "browse", "web", "execute"]}, {"id": "term-agent-framework", "t": "Agent Framework", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A software architecture that enables LLMs to autonomously plan, reason, and execute multi-step tasks by combining...", "l": "a", "k": ["agent", "framework", "software", "architecture", "enables", "llms", "autonomously", "plan", "reason", "execute", "multi-step", "tasks", "combining", "language", "understanding"]}, {"id": "term-agentic-ai", "t": "Agentic AI", "tg": ["Architecture", "Advanced"], "d": "models", "x": "AI systems that can autonomously plan, reason, and take actions to accomplish goals. Includes tool use, multi-step...", "l": "a", "k": ["agentic", "systems", "autonomously", "plan", "reason", "take", "actions", "accomplish", "goals", "includes", "tool", "multi-step", "planning", "self-correction", "capabilities"]}, {"id": "term-agentic-chunking", "t": "Agentic Chunking", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "A document splitting strategy that uses a language model agent to make intelligent decisions about chunk boundaries,...", "l": "a", "k": ["agentic", "chunking", "document", "splitting", "strategy", "uses", "language", "model", "agent", "intelligent", "decisions", "chunk", "boundaries", "content", "grouping"]}, {"id": "term-agentic-rag", "t": "Agentic RAG", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A retrieval-augmented generation approach where an LLM agent dynamically decides what to retrieve, refines queries...", "l": "a", "k": ["agentic", "rag", "retrieval-augmented", "generation", "approach", "llm", "agent", "dynamically", "decides", "retrieve", "refines", "queries", "based", "initial", "results"]}, {"id": "term-aggregation-bias", "t": "Aggregation Bias", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Bias arising when a single model is used for groups with different conditional distributions, leading to poor...", "l": "a", "k": ["aggregation", "bias", "arising", "single", "model", "groups", "different", "conditional", "distributions", "leading", "poor", "performance", "subgroups", "whose", "patterns"]}, {"id": "term-agi", "t": "AGI (Artificial General Intelligence)", "tg": ["Concept", "Future"], "d": "general", "x": "Hypothetical AI that can perform any intellectual task a human can. Unlike today's narrow AI, AGI would generalize...", "l": "a", "k": ["agi", "artificial", "general", "intelligence", "hypothetical", "perform", "intellectual", "task", "human", "unlike", "today", "narrow", "generalize", "across", "domains"]}, {"id": "term-agi-safety", "t": "AGI Safety", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The subfield of AI safety specifically focused on ensuring that artificial general intelligence, systems matching or...", "l": "a", "k": ["agi", "safety", "subfield", "specifically", "focused", "ensuring", "artificial", "general", "intelligence", "systems", "matching", "exceeding", "human", "cognitive", "abilities"]}, {"id": "term-ai", "t": "AI (Artificial Intelligence)", "tg": ["Fundamentals", "Core Concept"], "d": "general", "x": "Computer systems designed to perform tasks that typically require human intelligence, such as understanding language,...", "l": "a", "k": ["artificial", "intelligence", "computer", "systems", "designed", "perform", "tasks", "typically", "require", "human", "understanding", "language", "recognizing", "patterns", "making"]}, {"id": "term-ai-alignment-problem", "t": "AI Alignment Problem", "tg": ["History", "Fundamentals"], "d": "history", "x": "The challenge of ensuring that AI systems pursue goals and behaviors aligned with human values and intentions. As AI...", "l": "a", "k": ["alignment", "problem", "challenge", "ensuring", "systems", "pursue", "goals", "behaviors", "aligned", "human", "values", "intentions", "become", "capable", "critical"]}, {"id": "term-ai-alignment-tax", "t": "AI Alignment Tax", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The additional cost in performance, compute, or development time required to make an AI system aligned with human...", "l": "a", "k": ["alignment", "tax", "additional", "cost", "performance", "compute", "development", "time", "required", "system", "aligned", "human", "values", "representing", "trade-off"]}, {"id": "term-ai-and-employment", "t": "AI and Employment", "tg": ["History", "Fundamentals"], "d": "history", "x": "The ongoing debate about how AI and automation will affect jobs and the labor market. Predictions range from mass...", "l": "a", "k": ["employment", "ongoing", "debate", "automation", "affect", "jobs", "labor", "market", "predictions", "range", "mass", "unemployment", "job", "transformation", "augmentation"]}, {"id": "term-ai-arms-race", "t": "AI Arms Race", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "The competitive dynamic between nations or companies racing to develop the most advanced AI capabilities, potentially...", "l": "a", "k": ["arms", "race", "competitive", "dynamic", "nations", "companies", "racing", "develop", "advanced", "capabilities", "potentially", "expense", "safety", "research", "ethical"]}, {"id": "term-ai-bill-of-rights", "t": "AI Bill of Rights", "tg": ["Governance", "Regulation"], "d": "safety", "x": "The Blueprint for an AI Bill of Rights released by the White House OSTP in 2022, outlining five principles for...", "l": "a", "k": ["bill", "rights", "blueprint", "released", "white", "house", "ostp", "outlining", "five", "principles", "responsible", "safe", "systems", "algorithmic", "discrimination"]}, {"id": "term-ai-boom-2023", "t": "AI Boom 2023", "tg": ["History", "Milestones"], "d": "history", "x": "The period of intense investment, development, and public attention in AI following the launch of ChatGPT,...", "l": "a", "k": ["boom", "period", "intense", "investment", "development", "public", "attention", "following", "launch", "chatgpt", "characterized", "rapid", "advances", "large", "language"]}, {"id": "term-ai-chip-race", "t": "AI Chip Race", "tg": ["History", "Milestones"], "d": "history", "x": "The competition among semiconductor companies to develop specialized chips optimized for AI workloads. Key players...", "l": "a", "k": ["chip", "race", "competition", "among", "semiconductor", "companies", "develop", "specialized", "chips", "optimized", "workloads", "key", "players", "include", "nvidia"]}, {"id": "term-ai-consciousness", "t": "AI Consciousness", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The philosophical and scientific question of whether AI systems can have subjective experiences or phenomenal...", "l": "a", "k": ["consciousness", "philosophical", "scientific", "question", "systems", "subjective", "experiences", "phenomenal", "awareness", "implications", "moral", "consideration", "ethical", "treatment", "entities"]}, {"id": "term-ai-containment", "t": "AI Containment", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "Strategies and technical measures designed to prevent an advanced AI system from exerting unintended influence on the...", "l": "a", "k": ["containment", "strategies", "technical", "measures", "designed", "prevent", "advanced", "system", "exerting", "unintended", "influence", "external", "world", "including", "air-gapping"]}, {"id": "term-ai-detection", "t": "AI Detection", "tg": ["Generative AI", "LLM"], "d": "models", "x": "Methods and tools designed to distinguish AI-generated text, images, or media from human-created content, using...", "l": "a", "k": ["detection", "methods", "tools", "designed", "distinguish", "ai-generated", "text", "images", "media", "human-created", "content", "statistical", "analysis", "token", "distributions"]}, {"id": "term-ai-digital-divide", "t": "AI Digital Divide", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "The gap between those who have access to AI technologies and the skills to use them and those who do not, potentially...", "l": "a", "k": ["digital", "divide", "gap", "access", "technologies", "skills", "potentially", "exacerbating", "existing", "social", "economic", "inequalities", "across", "within", "nations"]}, {"id": "term-ai-effect", "t": "AI Effect", "tg": ["History", "Fundamentals"], "d": "history", "x": "The phenomenon where once a machine can perform a task that was previously considered to require intelligence that task...", "l": "a", "k": ["effect", "phenomenon", "machine", "perform", "task", "previously", "considered", "require", "intelligence", "longer", "regarded", "requiring", "true", "moving", "goalpost"]}, {"id": "term-ai-environmental-impact", "t": "AI Environmental Impact", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The environmental costs of AI development and deployment, including the substantial energy consumption and carbon...", "l": "a", "k": ["environmental", "impact", "costs", "development", "deployment", "including", "substantial", "energy", "consumption", "carbon", "emissions", "training", "large", "models", "water"]}, {"id": "term-ai-ethics", "t": "AI Ethics", "tg": ["Ethics", "Society"], "d": "safety", "x": "The study of moral principles and values that should guide the development and use of AI systems. Covers fairness,...", "l": "a", "k": ["ethics", "study", "moral", "principles", "values", "guide", "development", "systems", "covers", "fairness", "transparency", "privacy", "accountability", "societal", "impact"]}, {"id": "term-ai-ethics-history", "t": "AI Ethics History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of AI ethics from early philosophical questions (Turing 1950 Weizenbaum 1976) through concerns about bias...", "l": "a", "k": ["ethics", "history", "evolution", "early", "philosophical", "questions", "turing", "weizenbaum", "concerns", "bias", "fairness", "2010s", "modern", "debates", "existential"]}, {"id": "term-ai-governance", "t": "AI Governance", "tg": ["Governance", "Regulation"], "d": "safety", "x": "The set of policies, regulations, standards, and institutional frameworks that guide the development, deployment, and...", "l": "a", "k": ["governance", "policies", "regulations", "standards", "institutional", "frameworks", "guide", "development", "deployment", "oversight", "artificial", "intelligence", "systems", "organizational", "national"]}, {"id": "term-ai-hallucination-problem", "t": "AI Hallucination Problem", "tg": ["History", "Fundamentals"], "d": "history", "x": "The phenomenon where AI language models generate plausible-sounding but factually incorrect or fabricated information....", "l": "a", "k": ["hallucination", "problem", "phenomenon", "language", "models", "generate", "plausible-sounding", "factually", "incorrect", "fabricated", "information", "recognized", "major", "challenge", "deploying"]}, {"id": "term-ai-impact-assessment", "t": "AI Impact Assessment", "tg": ["Governance", "AI Ethics"], "d": "safety", "x": "A systematic process for evaluating the potential social, ethical, economic, and environmental effects of an AI system...", "l": "a", "k": ["impact", "assessment", "systematic", "process", "evaluating", "potential", "social", "ethical", "economic", "environmental", "effects", "system", "deployment", "analogous", "assessments"]}, {"id": "term-ai-incident-database", "t": "AI Incident Database", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "A repository cataloging real-world instances where AI systems caused harm or exhibited problematic behavior, maintained...", "l": "a", "k": ["incident", "database", "repository", "cataloging", "real-world", "instances", "systems", "caused", "harm", "exhibited", "problematic", "behavior", "maintained", "organizations", "partnership"]}, {"id": "term-ai-index-report", "t": "AI Index Report", "tg": ["History", "Organizations"], "d": "history", "x": "An annual report from Stanford University's Human-Centered AI Institute that tracks measures and visualizes data...", "l": "a", "k": ["index", "report", "annual", "stanford", "university", "human-centered", "institute", "tracks", "measures", "visualizes", "data", "related", "progress", "provides", "comprehensive"]}, {"id": "term-ai-labor-displacement", "t": "AI Labor Displacement", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The phenomenon of AI and automation systems replacing human workers in various occupations, raising concerns about...", "l": "a", "k": ["labor", "displacement", "phenomenon", "automation", "systems", "replacing", "human", "workers", "various", "occupations", "raising", "concerns", "unemployment", "wage", "depression"]}, {"id": "term-ai-liability-framework", "t": "AI Liability Framework", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Legal frameworks determining who bears responsibility when AI systems cause harm, including debates over strict...", "l": "a", "k": ["liability", "framework", "legal", "frameworks", "determining", "bears", "responsibility", "systems", "cause", "harm", "including", "debates", "strict", "negligence", "standards"]}, {"id": "term-ai-moratorium", "t": "AI Moratorium", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "A proposed temporary pause on the development of AI systems above a certain capability threshold, notably advocated in...", "l": "a", "k": ["moratorium", "proposed", "temporary", "pause", "development", "systems", "certain", "capability", "threshold", "notably", "advocated", "march", "open", "letter", "signed"]}, {"id": "term-ai-personhood", "t": "AI Personhood", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The legal and philosophical concept of granting AI systems some form of legal personality, enabling them to hold...", "l": "a", "k": ["personhood", "legal", "philosophical", "concept", "granting", "systems", "form", "personality", "enabling", "hold", "rights", "enter", "contracts", "bear", "liability"]}, {"id": "term-ai-readiness", "t": "AI Readiness", "tg": ["Fundamentals", "Skill"], "d": "general", "x": "The skills, knowledge, and mindset needed to use AI tools effectively and responsibly. Includes understanding both...", "l": "a", "k": ["readiness", "skills", "knowledge", "mindset", "needed", "tools", "effectively", "responsibly", "includes", "understanding", "capabilities", "limitations"]}, {"id": "term-ai-red-lines", "t": "AI Red Lines", "tg": ["AI Safety", "AI Ethics"], "d": "safety", "x": "Clearly defined boundaries that AI systems should never cross, such as refusing to assist with creating weapons of mass...", "l": "a", "k": ["red", "lines", "clearly", "defined", "boundaries", "systems", "never", "cross", "refusing", "assist", "creating", "weapons", "mass", "destruction", "generating"]}, {"id": "term-ai-regulation-timeline", "t": "AI Regulation Timeline", "tg": ["History", "Regulation"], "d": "history", "x": "The chronological progression of AI governance efforts from early ethical guidelines in the 2010s through the EU AI...", "l": "a", "k": ["regulation", "timeline", "chronological", "progression", "governance", "efforts", "early", "ethical", "guidelines", "2010s", "act", "executive", "orders", "international", "summits"]}, {"id": "term-ai-regulatory-sandbox", "t": "AI Regulatory Sandbox", "tg": ["Governance", "Regulation"], "d": "safety", "x": "A controlled environment established by regulators where AI companies can test innovative products under relaxed...", "l": "a", "k": ["regulatory", "sandbox", "controlled", "environment", "established", "regulators", "companies", "test", "innovative", "products", "relaxed", "requirements", "maintaining", "safeguards", "provided"]}, {"id": "term-ai-risk-levels", "t": "AI Risk Levels", "tg": ["Governance", "Regulation"], "d": "safety", "x": "A classification scheme, notably used in the EU AI Act, that categorizes AI applications into tiers such as...", "l": "a", "k": ["risk", "levels", "classification", "scheme", "notably", "act", "categorizes", "applications", "tiers", "unacceptable", "high", "limited", "minimal", "corresponding", "regulatory"]}, {"id": "term-ai-safety", "t": "AI Safety", "tg": ["Field", "Safety"], "d": "safety", "x": "The field focused on ensuring AI systems behave safely and beneficially. Includes technical research on alignment,...", "l": "a", "k": ["safety", "field", "focused", "ensuring", "systems", "behave", "safely", "beneficially", "includes", "technical", "research", "alignment", "governance", "preventing", "misuse"]}, {"id": "term-ai-safety-institute", "t": "AI Safety Institute", "tg": ["Governance", "AI Safety"], "d": "safety", "x": "A government-backed organization, first established by the UK in 2023, dedicated to evaluating and testing frontier AI...", "l": "a", "k": ["safety", "institute", "government-backed", "organization", "established", "dedicated", "evaluating", "testing", "frontier", "models", "risks", "similar", "institutes", "subsequently", "created"]}, {"id": "term-ai-safety-research", "t": "AI Safety Research", "tg": ["History", "Fundamentals"], "d": "history", "x": "The field of research dedicated to ensuring that AI systems are safe beneficial and aligned with human values. AI...", "l": "a", "k": ["safety", "research", "field", "dedicated", "ensuring", "systems", "safe", "beneficial", "aligned", "human", "values", "encompasses", "technical", "alignment", "governance"]}, {"id": "term-ai-sandboxing", "t": "AI Sandboxing", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "The practice of running AI systems in isolated environments with restricted access to networks, resources, and...", "l": "a", "k": ["sandboxing", "practice", "running", "systems", "isolated", "environments", "restricted", "access", "networks", "resources", "actuators", "limit", "potential", "harm", "testing"]}, {"id": "term-ai-spring", "t": "AI Spring", "tg": ["History", "Milestones"], "d": "history", "x": "A period of renewed optimism investment and progress in AI research typically referring to the resurgence beginning...", "l": "a", "k": ["spring", "period", "renewed", "optimism", "investment", "progress", "research", "typically", "referring", "resurgence", "beginning", "around", "driven", "deep", "learning"]}, {"id": "term-ai-washing", "t": "AI Washing", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The practice of companies exaggerating or fabricating the role of AI in their products or services for marketing...", "l": "a", "k": ["washing", "practice", "companies", "exaggerating", "fabricating", "role", "products", "services", "marketing", "purposes", "misleading", "consumers", "investors", "actual", "capabilities"]}, {"id": "term-ai-whistleblowing", "t": "AI Whistleblowing", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The act of insiders at AI companies publicly disclosing information about safety concerns, unethical practices, or...", "l": "a", "k": ["whistleblowing", "act", "insiders", "companies", "publicly", "disclosing", "information", "safety", "concerns", "unethical", "practices", "dangerous", "capabilities", "seen", "open"]}, {"id": "term-ai-winter", "t": "AI Winter", "tg": ["Historical", "Industry"], "d": "history", "x": "Periods of reduced funding and interest in AI research following failed expectations. Notable winters occurred in the...", "l": "a", "k": ["winter", "periods", "reduced", "funding", "interest", "research", "following", "failed", "expectations", "notable", "winters", "occurred", "1970s", "late", "1980s"]}, {"id": "term-ai2-allen-institute-for-ai", "t": "AI2 (Allen Institute for AI)", "tg": ["History", "Organizations"], "d": "history", "x": "A research institute founded by Paul Allen in 2014 dedicated to AI research for the common good. AI2 has produced...", "l": "a", "k": ["ai2", "allen", "institute", "research", "founded", "paul", "dedicated", "common", "good", "produced", "influential", "work", "including", "semantic", "scholar"]}, {"id": "term-akaike-information-criterion", "t": "Akaike Information Criterion", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A model selection metric that balances goodness of fit with model complexity by adding a penalty proportional to the...", "l": "a", "k": ["akaike", "information", "criterion", "model", "selection", "metric", "balances", "goodness", "fit", "complexity", "adding", "penalty", "proportional", "number", "parameters"]}, {"id": "term-alain-colmerauer", "t": "Alain Colmerauer", "tg": ["History", "Pioneers"], "d": "history", "x": "French computer scientist who co-created the Prolog programming language in 1972 with Philippe Roussel. Prolog became...", "l": "a", "k": ["alain", "colmerauer", "french", "computer", "scientist", "co-created", "prolog", "programming", "language", "philippe", "roussel", "became", "primary", "logic", "adopted"]}, {"id": "term-alan-turing", "t": "Alan Turing", "tg": ["History", "Pioneers"], "d": "history", "x": "British mathematician and logician (1912-1954) who formalized computation with the Turing machine, broke the Enigma...", "l": "a", "k": ["alan", "turing", "british", "mathematician", "logician", "1912-1954", "formalized", "computation", "machine", "broke", "enigma", "code", "bletchley", "park", "proposed"]}, {"id": "term-alan-turing-institute", "t": "Alan Turing Institute", "tg": ["History", "Organizations"], "d": "history", "x": "The United Kingdom's national institute for data science and artificial intelligence founded in 2015. Named after Alan...", "l": "a", "k": ["alan", "turing", "institute", "united", "kingdom", "national", "data", "science", "artificial", "intelligence", "founded", "named", "brings", "together", "researchers"]}, {"id": "term-albert", "t": "ALBERT", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A Lite BERT that reduces model size through factorized embedding parameterization and cross-layer parameter sharing...", "l": "a", "k": ["albert", "lite", "bert", "reduces", "model", "size", "factorized", "embedding", "parameterization", "cross-layer", "parameter", "sharing", "maintaining", "competitive", "performance"]}, {"id": "term-alec-radford", "t": "Alec Radford", "tg": ["History", "Pioneers"], "d": "history", "x": "American AI researcher at OpenAI who led the development of GPT (2018) and GPT-2 (2019) demonstrating that unsupervised...", "l": "a", "k": ["alec", "radford", "american", "researcher", "openai", "led", "development", "gpt", "gpt-2", "demonstrating", "unsupervised", "pre-training", "language", "models", "large"]}, {"id": "term-alex-krizhevsky", "t": "Alex Krizhevsky", "tg": ["History", "Pioneers"], "d": "history", "x": "Ukrainian-Canadian computer scientist who designed AlexNet the deep convolutional neural network that won the 2012...", "l": "a", "k": ["alex", "krizhevsky", "ukrainian-canadian", "computer", "scientist", "designed", "alexnet", "deep", "convolutional", "neural", "network", "won", "imagenet", "large", "scale"]}, {"id": "term-alexa-launch", "t": "Alexa Launch", "tg": ["History", "Milestones"], "d": "history", "x": "Amazon's launch of Alexa and the Echo smart speaker in November 2014, popularizing voice-activated AI assistants in the...", "l": "a", "k": ["alexa", "launch", "amazon", "echo", "smart", "speaker", "november", "popularizing", "voice-activated", "assistants", "home", "establishing", "major", "platform", "ambient"]}, {"id": "term-alexnet", "t": "AlexNet", "tg": ["History", "Milestones"], "d": "history", "x": "A deep convolutional neural network designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton that won the 2012...", "l": "a", "k": ["alexnet", "deep", "convolutional", "neural", "network", "designed", "alex", "krizhevsky", "ilya", "sutskever", "geoffrey", "hinton", "won", "imagenet", "competition"]}, {"id": "term-algorithm", "t": "Algorithm", "tg": ["Fundamentals", "Computer Science"], "d": "hardware", "x": "A step-by-step procedure or set of rules for solving a problem or accomplishing a task. In AI, algorithms define how...", "l": "a", "k": ["algorithm", "step-by-step", "procedure", "rules", "solving", "problem", "accomplishing", "task", "algorithms", "define", "models", "learn", "data", "predictions"]}, {"id": "term-algorithmic-bias", "t": "Algorithmic Bias", "tg": ["History", "Fundamentals"], "d": "history", "x": "Systematic and repeatable errors in computer systems that create unfair outcomes. Algorithmic bias in AI can arise from...", "l": "a", "k": ["algorithmic", "bias", "systematic", "repeatable", "errors", "computer", "systems", "create", "unfair", "outcomes", "arise", "biased", "training", "data", "algorithm"]}, {"id": "term-algorithmic-discrimination", "t": "Algorithmic Discrimination", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Systematic and unfair differential treatment of individuals or groups by automated decision-making systems, often...", "l": "a", "k": ["algorithmic", "discrimination", "systematic", "unfair", "differential", "treatment", "individuals", "groups", "automated", "decision-making", "systems", "arising", "biased", "training", "data"]}, {"id": "term-algorithmic-impact-assessment", "t": "Algorithmic Impact Assessment", "tg": ["Governance", "AI Ethics"], "d": "safety", "x": "A formal evaluation process required in some jurisdictions to assess the potential effects of automated decision-making...", "l": "a", "k": ["algorithmic", "impact", "assessment", "formal", "evaluation", "process", "required", "jurisdictions", "assess", "potential", "effects", "automated", "decision-making", "systems", "individuals"]}, {"id": "term-algorithmic-recourse", "t": "Algorithmic Recourse", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "The ability of individuals affected by automated decisions to take meaningful actions to change the outcome, such as...", "l": "a", "k": ["algorithmic", "recourse", "ability", "individuals", "affected", "automated", "decisions", "take", "meaningful", "actions", "change", "outcome", "understanding", "inputs", "modify"]}, {"id": "term-algorithmic-transparency", "t": "Algorithmic Transparency", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The degree to which the logic, rules, and data dependencies of an algorithm are made visible and understandable to...", "l": "a", "k": ["algorithmic", "transparency", "degree", "logic", "rules", "data", "dependencies", "algorithm", "visible", "understandable", "affected", "individuals", "regulators", "public"]}, {"id": "term-alibi", "t": "ALiBi", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Attention with Linear Biases, a positional encoding method that adds a linear bias proportional to the distance between...", "l": "a", "k": ["alibi", "attention", "linear", "biases", "positional", "encoding", "method", "adds", "bias", "proportional", "distance", "key", "query", "positions", "directly"]}, {"id": "term-align", "t": "ALIGN", "tg": ["Models", "Technical"], "d": "models", "x": "A Large-scale ImaGe and Noisy-text embedding model trained on over one billion noisy image-text pairs with minimal...", "l": "a", "k": ["align", "large-scale", "image", "noisy-text", "embedding", "model", "trained", "billion", "noisy", "image-text", "pairs", "minimal", "filtering", "demonstrates", "scaling"]}, {"id": "term-alignment", "t": "Alignment", "tg": ["Safety", "Research"], "d": "safety", "x": "The challenge of ensuring AI systems behave in ways that match human values and intentions. A key concern in AI safety...", "l": "a", "k": ["alignment", "challenge", "ensuring", "systems", "behave", "ways", "match", "human", "values", "intentions", "key", "concern", "safety", "research", "involving"]}, {"id": "term-all-gather", "t": "All-Gather Operation", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A collective communication pattern where each participant broadcasts its data to all others, so every participant ends...", "l": "a", "k": ["all-gather", "operation", "collective", "communication", "pattern", "participant", "broadcasts", "data", "others", "ends", "complete", "concatenated", "dataset", "fsdp", "reconstruct"]}, {"id": "term-all-reduce", "t": "All-Reduce Operation", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A collective communication pattern where all participating GPUs contribute data, perform a reduction operation...", "l": "a", "k": ["all-reduce", "operation", "collective", "communication", "pattern", "participating", "gpus", "contribute", "data", "perform", "reduction", "typically", "summation", "receive", "result"]}, {"id": "term-allen-institute-for-ai", "t": "Allen Institute for AI", "tg": ["History", "Organizations"], "d": "history", "x": "A research institute founded by Paul Allen in 2014 dedicated to conducting high-impact AI research and engineering....", "l": "a", "k": ["allen", "institute", "research", "founded", "paul", "dedicated", "conducting", "high-impact", "engineering", "known", "projects", "including", "semantic", "scholar", "ai2"]}, {"id": "term-allen-newell", "t": "Allen Newell", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1927-1992) who, together with Herbert Simon, developed the Logic Theorist and General...", "l": "a", "k": ["allen", "newell", "american", "computer", "scientist", "1927-1992", "together", "herbert", "simon", "developed", "logic", "theorist", "general", "problem", "solver"]}, {"id": "term-allocative-harm", "t": "Allocative Harm", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Harm that occurs when an AI system unfairly distributes resources, opportunities, or outcomes across different groups,...", "l": "a", "k": ["allocative", "harm", "occurs", "system", "unfairly", "distributes", "resources", "opportunities", "outcomes", "across", "different", "groups", "denying", "loans", "jobs"]}, {"id": "term-alpac-report", "t": "ALPAC Report", "tg": ["History", "Milestones"], "d": "history", "x": "A 1966 report by the Automatic Language Processing Advisory Committee that concluded machine translation was not likely...", "l": "a", "k": ["alpac", "report", "automatic", "language", "processing", "advisory", "committee", "concluded", "machine", "translation", "likely", "reach", "quality", "human", "near"]}, {"id": "term-alpaca", "t": "Alpaca", "tg": ["Model", "Historical"], "d": "models", "x": "An early instruction-tuned version of Llama created by Stanford researchers. Demonstrated that instruction-following...", "l": "a", "k": ["alpaca", "early", "instruction-tuned", "version", "llama", "created", "stanford", "researchers", "demonstrated", "instruction-following", "achieved", "synthetic", "data", "low", "cost"]}, {"id": "term-alpacaeval", "t": "AlpacaEval", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "An automatic evaluation framework that compares model outputs against a reference model using LLM-based pairwise...", "l": "a", "k": ["alpacaeval", "automatic", "evaluation", "framework", "compares", "model", "outputs", "against", "reference", "llm-based", "pairwise", "judgments", "providing", "fast", "cost-effective"]}, {"id": "term-alpha-beta-pruning", "t": "Alpha-Beta Pruning", "tg": ["History", "Fundamentals"], "d": "history", "x": "An optimization of the minimax algorithm that eliminates branches of the game tree that cannot possibly influence the...", "l": "a", "k": ["alpha-beta", "pruning", "optimization", "minimax", "algorithm", "eliminates", "branches", "game", "tree", "cannot", "possibly", "influence", "final", "decision", "developed"]}, {"id": "term-alphacode", "t": "AlphaCode", "tg": ["Models", "Technical"], "d": "models", "x": "A code generation system by DeepMind that solves competitive programming problems at a human-competitive level....", "l": "a", "k": ["alphacode", "code", "generation", "system", "deepmind", "solves", "competitive", "programming", "problems", "human-competitive", "level", "generates", "millions", "candidates", "filters"]}, {"id": "term-alphafold", "t": "AlphaFold", "tg": ["History", "Milestones"], "d": "history", "x": "DeepMind's AI system that solved the protein structure prediction problem, winning CASP14 in 2020 and subsequently...", "l": "a", "k": ["alphafold", "deepmind", "system", "solved", "protein", "structure", "prediction", "problem", "winning", "casp14", "subsequently", "predicting", "structures", "nearly", "known"]}, {"id": "term-alphago", "t": "AlphaGo", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A DeepMind system that combined deep neural networks with Monte Carlo tree search to defeat world champion Go players....", "l": "a", "k": ["alphago", "deepmind", "system", "combined", "deep", "neural", "networks", "monte", "carlo", "tree", "search", "defeat", "world", "champion", "players"]}, {"id": "term-alphago-vs-lee-sedol", "t": "AlphaGo vs Lee Sedol", "tg": ["History", "Milestones"], "d": "history", "x": "The March 2016 match in which DeepMind's AlphaGo defeated world champion Go player Lee Sedol 4-1, a landmark...", "l": "a", "k": ["alphago", "lee", "sedol", "march", "match", "deepmind", "defeated", "world", "champion", "player", "4-1", "landmark", "achievement", "long", "considered"]}, {"id": "term-alphazero", "t": "AlphaZero", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A generalized version of AlphaGo that learns to play Go, chess, and shogi entirely through self-play without human game...", "l": "a", "k": ["alphazero", "generalized", "version", "alphago", "learns", "play", "chess", "shogi", "entirely", "self-play", "without", "human", "game", "data", "uses"]}, {"id": "term-alternative-hypothesis", "t": "Alternative Hypothesis", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The hypothesis that contradicts the null hypothesis in statistical testing, typically representing the effect or...", "l": "a", "k": ["alternative", "hypothesis", "contradicts", "null", "statistical", "testing", "typically", "representing", "effect", "difference", "researcher", "aims", "detect", "one-sided", "two-sided"]}, {"id": "term-alvinn", "t": "ALVINN", "tg": ["History", "Systems"], "d": "history", "x": "Autonomous Land Vehicle In a Neural Network developed by Dean Pomerleau at Carnegie Mellon in 1989. One of the earliest...", "l": "a", "k": ["alvinn", "autonomous", "land", "vehicle", "neural", "network", "developed", "dean", "pomerleau", "carnegie", "mellon", "earliest", "demonstrations", "networks", "driving"]}, {"id": "term-am-automated-mathematician", "t": "AM (Automated Mathematician)", "tg": ["History", "Systems"], "d": "history", "x": "An AI program written by Douglas Lenat in 1976 that discovered mathematical concepts by exploring modifications to...", "l": "a", "k": ["automated", "mathematician", "program", "written", "douglas", "lenat", "discovered", "mathematical", "concepts", "exploring", "modifications", "existing", "demonstrated", "discovery", "mathematics"]}, {"id": "term-amazon-bedrock", "t": "Amazon Bedrock", "tg": ["Platform", "Cloud"], "d": "general", "x": "AWS's managed service for accessing foundation models from multiple providers. Offers Claude, Llama, and other models...", "l": "a", "k": ["amazon", "bedrock", "aws", "managed", "service", "accessing", "foundation", "models", "multiple", "providers", "offers", "claude", "llama", "unified", "api"]}, {"id": "term-amd-mi300x", "t": "AMD MI300X", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "AMD's data center GPU accelerator featuring 192GB HBM3 memory and CDNA 3 architecture, competing with NVIDIA's H100 for...", "l": "a", "k": ["amd", "mi300x", "data", "center", "gpu", "accelerator", "featuring", "192gb", "hbm3", "memory", "cdna", "architecture", "competing", "nvidia", "h100"]}, {"id": "term-analogical-prompting", "t": "Analogical Prompting", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A technique that asks the model to generate relevant analogous problems and their solutions before tackling the target...", "l": "a", "k": ["analogical", "prompting", "technique", "asks", "model", "generate", "relevant", "analogous", "problems", "solutions", "tackling", "target", "problem", "leveraging", "self-generated"]}, {"id": "term-anaphora-resolution", "t": "Anaphora Resolution", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of determining which previously mentioned entity a pronoun or other referring expression points back to in a...", "l": "a", "k": ["anaphora", "resolution", "task", "determining", "previously", "mentioned", "entity", "pronoun", "referring", "expression", "points", "text", "subproblem", "coreference"]}, {"id": "term-anchor", "t": "Anchor (Prompting)", "tg": ["Prompting", "Technique"], "d": "general", "x": "A reference point or example in a prompt that guides the AI's response style or format. Anchors help establish...", "l": "a", "k": ["anchor", "prompting", "reference", "point", "example", "prompt", "guides", "response", "style", "format", "anchors", "help", "establish", "expectations", "output"]}, {"id": "term-anchor-box", "t": "Anchor Box", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A predefined set of bounding boxes with various aspect ratios and scales placed at each spatial location in a feature...", "l": "a", "k": ["anchor", "box", "predefined", "bounding", "boxes", "various", "aspect", "ratios", "scales", "placed", "spatial", "location", "feature", "map", "serving"]}, {"id": "term-anchoring-bias-in-ai", "t": "Anchoring Bias in AI", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "A cognitive bias where initial AI-generated suggestions disproportionately influence subsequent human decisions,...", "l": "a", "k": ["anchoring", "bias", "cognitive", "initial", "ai-generated", "suggestions", "disproportionately", "influence", "subsequent", "human", "decisions", "causing", "users", "adjust", "insufficiently"]}, {"id": "term-andrej-karpathy", "t": "Andrej Karpathy", "tg": ["History", "Pioneers"], "d": "history", "x": "Slovak-Canadian AI researcher who led computer vision at Tesla Autopilot and later worked at OpenAI. Known for...", "l": "a", "k": ["andrej", "karpathy", "slovak-canadian", "researcher", "led", "computer", "vision", "tesla", "autopilot", "later", "worked", "openai", "known", "educational", "contributions"]}, {"id": "term-andrew-barto", "t": "Andrew Barto", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who co-authored the influential textbook Reinforcement Learning: An Introduction with...", "l": "a", "k": ["andrew", "barto", "american", "computer", "scientist", "co-authored", "influential", "textbook", "reinforcement", "learning", "introduction", "richard", "sutton", "pioneer", "university"]}, {"id": "term-andrew-ng", "t": "Andrew Ng", "tg": ["History", "Pioneers"], "d": "history", "x": "British-American computer scientist who co-founded Google Brain, led AI at Baidu, founded Coursera and deeplearning.ai,...", "l": "a", "k": ["andrew", "british-american", "computer", "scientist", "co-founded", "google", "brain", "led", "baidu", "founded", "coursera", "deeplearning", "popularized", "deep", "learning"]}, {"id": "term-animatediff", "t": "AnimateDiff", "tg": ["Models", "Technical"], "d": "models", "x": "A framework for animating personalized text-to-image models by inserting motion modules into the existing architecture....", "l": "a", "k": ["animatediff", "framework", "animating", "personalized", "text-to-image", "models", "inserting", "motion", "modules", "existing", "architecture", "enables", "video", "generation", "fine-tuned"]}, {"id": "term-ann-benchmark", "t": "ANN Benchmark", "tg": ["Vector Database", "Evaluation"], "d": "datasets", "x": "Standardized evaluation suites for comparing approximate nearest neighbor algorithms across metrics like recall,...", "l": "a", "k": ["ann", "benchmark", "standardized", "evaluation", "suites", "comparing", "approximate", "nearest", "neighbor", "algorithms", "across", "metrics", "recall", "queries", "per"]}, {"id": "term-annotation", "t": "Annotation", "tg": ["Data", "Training"], "d": "general", "x": "The process of labeling data to create training datasets for supervised learning. Human annotators add labels,...", "l": "a", "k": ["annotation", "process", "labeling", "data", "create", "training", "datasets", "supervised", "learning", "human", "annotators", "add", "labels", "categories", "descriptions"]}, {"id": "term-annotation-labor-ethics", "t": "Annotation Labor Ethics", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "Ethical concerns about the working conditions, compensation, and psychological impacts experienced by data annotation...", "l": "a", "k": ["annotation", "labor", "ethics", "ethical", "concerns", "working", "conditions", "compensation", "psychological", "impacts", "experienced", "data", "workers", "label", "training"]}, {"id": "term-annoy", "t": "Annoy", "tg": ["Vector Database", "Libraries"], "d": "general", "x": "Approximate Nearest Neighbors Oh Yeah, an open-source library by Spotify that builds forest-of-trees indexes using...", "l": "a", "k": ["annoy", "approximate", "nearest", "neighbors", "yeah", "open-source", "library", "spotify", "builds", "forest-of-trees", "indexes", "random", "hyperplane", "splits", "fast"]}, {"id": "term-anomaly-detection", "t": "Anomaly Detection", "tg": ["ML Task", "Application"], "d": "general", "x": "Identifying unusual patterns or outliers in data that don't conform to expected behavior. Used in fraud detection,...", "l": "a", "k": ["anomaly", "detection", "identifying", "unusual", "patterns", "outliers", "data", "don", "conform", "expected", "behavior", "fraud", "system", "monitoring", "quality"]}, {"id": "term-anomaly-detection-images", "t": "Anomaly Detection in Images", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of identifying unusual patterns, defects, or out-of-distribution samples in images, widely used in industrial...", "l": "a", "k": ["anomaly", "detection", "images", "task", "identifying", "unusual", "patterns", "defects", "out-of-distribution", "samples", "widely", "industrial", "quality", "inspection", "medical"]}, {"id": "term-anova", "t": "ANOVA", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "Analysis of Variance, a statistical method that tests whether the means of three or more groups are significantly...", "l": "a", "k": ["anova", "analysis", "variance", "statistical", "method", "tests", "means", "groups", "significantly", "different", "comparing", "within-group", "between-group", "f-statistic"]}, {"id": "term-answer-engineering", "t": "Answer Engineering", "tg": ["Prompting", "Technique"], "d": "general", "x": "Designing prompts to elicit specific response formats or structured outputs. Complements prompt engineering by focusing...", "l": "a", "k": ["answer", "engineering", "designing", "prompts", "elicit", "specific", "response", "formats", "structured", "outputs", "complements", "prompt", "focusing", "answers"]}, {"id": "term-ant-colony-optimization", "t": "Ant Colony Optimization", "tg": ["History", "Milestones"], "d": "history", "x": "A metaheuristic optimization algorithm proposed by Marco Dorigo in 1992, inspired by the foraging behavior of ants...", "l": "a", "k": ["ant", "colony", "optimization", "metaheuristic", "algorithm", "proposed", "marco", "dorigo", "inspired", "foraging", "behavior", "ants", "pheromone", "trails", "applied"]}, {"id": "term-anthropic", "t": "Anthropic", "tg": ["Company", "LLM Provider"], "d": "models", "x": "An AI safety company founded in 2021 by former OpenAI researchers. Creator of the Claude family of AI assistants,...", "l": "a", "k": ["anthropic", "safety", "company", "founded", "former", "openai", "researchers", "creator", "claude", "family", "assistants", "focused", "developing", "safe", "beneficial"]}, {"id": "term-anthropic-founding", "t": "Anthropic Founding", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of Anthropic in 2021 by former OpenAI researchers Dario and Daniela Amodei, establishing a safety-focused...", "l": "a", "k": ["anthropic", "founding", "former", "openai", "researchers", "dario", "daniela", "amodei", "establishing", "safety-focused", "company", "developed", "constitutional", "claude", "family"]}, {"id": "term-api", "t": "API (Application Programming Interface)", "tg": ["Technical", "Integration"], "d": "general", "x": "A set of protocols that allows different software applications to communicate. AI APIs enable developers to integrate...", "l": "a", "k": ["api", "application", "programming", "interface", "protocols", "allows", "different", "software", "applications", "communicate", "apis", "enable", "developers", "integrate", "capabilities"]}, {"id": "term-apple-neural-engine", "t": "Apple Neural Engine", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "Apple's dedicated neural network accelerator integrated into Apple Silicon chips (M-series and A-series), delivering up...", "l": "a", "k": ["apple", "neural", "engine", "dedicated", "network", "accelerator", "integrated", "silicon", "chips", "m-series", "a-series", "delivering", "tops", "on-device", "inference"]}, {"id": "term-appropriate-reliance", "t": "Appropriate Reliance", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The calibrated level of trust humans should place in AI systems, avoiding both over-reliance that leads to automation...", "l": "a", "k": ["appropriate", "reliance", "calibrated", "level", "trust", "humans", "place", "systems", "avoiding", "over-reliance", "leads", "automation", "complacency", "under-reliance", "fails"]}, {"id": "term-approximate-nearest-neighbor", "t": "Approximate Nearest Neighbor", "tg": ["Vector Database", "Search"], "d": "general", "x": "A class of search algorithms that find vectors approximately closest to a query vector with high probability rather...", "l": "a", "k": ["approximate", "nearest", "neighbor", "class", "search", "algorithms", "find", "vectors", "approximately", "closest", "query", "vector", "high", "probability", "rather"]}, {"id": "term-arc-benchmark", "t": "ARC Benchmark", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "The AI2 Reasoning Challenge, a question-answering benchmark consisting of elementary and middle school science exam...", "l": "a", "k": ["arc", "benchmark", "ai2", "reasoning", "challenge", "question-answering", "consisting", "elementary", "middle", "school", "science", "exam", "questions", "easy", "sets"]}, {"id": "term-arcface", "t": "ArcFace", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A face recognition loss function that adds an angular margin penalty in the normalized feature space, improving the...", "l": "a", "k": ["arcface", "face", "recognition", "loss", "function", "adds", "angular", "margin", "penalty", "normalized", "feature", "space", "improving", "discriminative", "power"]}, {"id": "term-architecture-search", "t": "Architecture Search (NAS)", "tg": ["Research", "Optimization"], "d": "algorithms", "x": "Automated methods for discovering optimal neural network architectures. Can find better designs than human-created...", "l": "a", "k": ["architecture", "search", "nas", "automated", "methods", "discovering", "optimal", "neural", "network", "architectures", "find", "better", "designs", "human-created", "networks"]}, {"id": "term-arena-score", "t": "Arena Score", "tg": ["Evaluation", "Ranking"], "d": "datasets", "x": "A model ranking metric derived from competitive evaluation platforms where models are compared in blind pairwise...", "l": "a", "k": ["arena", "score", "model", "ranking", "metric", "derived", "competitive", "evaluation", "platforms", "models", "compared", "blind", "pairwise", "matchups", "human"]}, {"id": "term-arima", "t": "ARIMA", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "AutoRegressive Integrated Moving Average, a class of time series models combining autoregression (AR), differencing for...", "l": "a", "k": ["arima", "autoregressive", "integrated", "moving", "average", "class", "time", "series", "models", "combining", "autoregression", "differencing", "stationarity", "components", "widely"]}, {"id": "term-arithmetic-intensity", "t": "Arithmetic Intensity", "tg": ["Hardware", "Model Optimization"], "d": "models", "x": "The ratio of floating-point operations to bytes of memory accessed in a computation, determining whether performance is...", "l": "a", "k": ["arithmetic", "intensity", "ratio", "floating-point", "operations", "bytes", "memory", "accessed", "computation", "determining", "performance", "limited", "compute", "bandwidth", "llm"]}, {"id": "term-arthur-samuel", "t": "Arthur Samuel", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1901-1990) who created a checkers-playing program at IBM in 1959 that learned through...", "l": "a", "k": ["arthur", "samuel", "american", "computer", "scientist", "1901-1990", "created", "checkers-playing", "program", "ibm", "learned", "self-play", "coining", "term", "machine"]}, {"id": "term-artificial-general-intelligence-history", "t": "Artificial General Intelligence History", "tg": ["History", "Milestones"], "d": "history", "x": "The pursuit of machines that can understand learn and apply knowledge across any intellectual task that humans can....", "l": "a", "k": ["artificial", "general", "intelligence", "history", "pursuit", "machines", "understand", "learn", "apply", "knowledge", "across", "intellectual", "task", "humans", "original"]}, {"id": "term-artificial-intelligence-a-modern-approach", "t": "Artificial Intelligence: A Modern Approach", "tg": ["History", "Milestones"], "d": "history", "x": "The standard AI textbook by Stuart Russell and Peter Norvig first published in 1995. Used in over 1500 universities in...", "l": "a", "k": ["artificial", "intelligence", "modern", "approach", "standard", "textbook", "stuart", "russell", "peter", "norvig", "published", "universities", "countries", "book", "covers"]}, {"id": "term-artificial-life", "t": "Artificial Life", "tg": ["History", "Fundamentals"], "d": "history", "x": "A field of study that examines systems related to natural life their processes and their evolution through the use of...", "l": "a", "k": ["artificial", "life", "field", "study", "examines", "systems", "related", "natural", "processes", "evolution", "simulations", "models", "pioneered", "christopher", "langton"]}, {"id": "term-artificial-neuron", "t": "Artificial Neuron", "tg": ["Architecture", "Fundamentals"], "d": "models", "x": "The basic computational unit in neural networks, loosely inspired by biological neurons. Computes a weighted sum of...", "l": "a", "k": ["artificial", "neuron", "basic", "computational", "unit", "neural", "networks", "loosely", "inspired", "biological", "neurons", "computes", "weighted", "sum", "inputs"]}, {"id": "term-ashish-vaswani", "t": "Ashish Vaswani", "tg": ["History", "Pioneers"], "d": "history", "x": "Lead author of the 2017 Attention Is All You Need paper that introduced the transformer architecture at Google Brain,...", "l": "a", "k": ["ashish", "vaswani", "lead", "author", "attention", "need", "paper", "introduced", "transformer", "architecture", "google", "brain", "fundamentally", "changing", "trajectory"]}, {"id": "term-asic-ai", "t": "ASIC for AI", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "Application-Specific Integrated Circuits designed exclusively for AI computation, offering maximum performance and...", "l": "a", "k": ["asic", "application-specific", "integrated", "circuits", "designed", "exclusively", "computation", "offering", "maximum", "performance", "energy", "efficiency", "fixed", "workloads", "asics"]}, {"id": "term-asilomar-ai-principles", "t": "Asilomar AI Principles", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "A set of 23 principles for beneficial AI research developed at the 2017 Asilomar conference, covering research issues,...", "l": "a", "k": ["asilomar", "principles", "beneficial", "research", "developed", "conference", "covering", "issues", "ethics", "values", "longer-term", "concerns", "advanced", "safety"]}, {"id": "term-aspect-based-sentiment", "t": "Aspect-Based Sentiment Analysis", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A fine-grained sentiment analysis task that identifies sentiment toward specific aspects or features of an entity,...", "l": "a", "k": ["aspect-based", "sentiment", "analysis", "fine-grained", "task", "identifies", "toward", "specific", "aspects", "features", "entity", "distinguishing", "different", "opinions", "attributes"]}, {"id": "term-assistant-message", "t": "Assistant Message", "tg": ["API", "Technical"], "d": "general", "x": "In chat APIs, the AI's response in a conversation. Combined with system and user messages to form the complete...", "l": "a", "k": ["assistant", "message", "chat", "apis", "response", "conversation", "combined", "system", "user", "messages", "form", "complete", "context", "generating", "next"]}, {"id": "term-a3c", "t": "Asynchronous Advantage Actor-Critic (A3C)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An actor-critic algorithm that runs multiple agent instances in parallel on separate environment copies, each...", "l": "a", "k": ["asynchronous", "advantage", "actor-critic", "a3c", "algorithm", "runs", "multiple", "agent", "instances", "parallel", "separate", "environment", "copies", "asynchronously", "updating"]}, {"id": "term-async-generation", "t": "Asynchronous Generation", "tg": ["Technical", "Production"], "d": "general", "x": "Running multiple AI inference requests in parallel rather than waiting for each to complete. Improves throughput for...", "l": "a", "k": ["asynchronous", "generation", "running", "multiple", "inference", "requests", "parallel", "rather", "waiting", "complete", "improves", "throughput", "applications", "handling", "concurrent"]}, {"id": "term-asynchronous-sgd", "t": "Asynchronous SGD", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A distributed training approach where workers compute and apply gradients independently without waiting for...", "l": "a", "k": ["asynchronous", "sgd", "distributed", "training", "approach", "workers", "compute", "apply", "gradients", "independently", "without", "waiting", "synchronization", "trading", "gradient"]}, {"id": "term-atrous-convolution", "t": "Atrous Convolution", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Also known as dilated convolution, a convolution operation that inserts gaps between filter elements to increase the...", "l": "a", "k": ["atrous", "convolution", "known", "dilated", "operation", "inserts", "gaps", "filter", "elements", "increase", "receptive", "field", "without", "adding", "parameters"]}, {"id": "term-attention-head-pruning", "t": "Attention Head Pruning", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A model compression technique that removes redundant or less important attention heads from a multi-head attention...", "l": "a", "k": ["attention", "head", "pruning", "model", "compression", "technique", "removes", "redundant", "less", "important", "heads", "multi-head", "mechanism", "reducing", "computation"]}, {"id": "term-attention-is-all-you-need", "t": "Attention Is All You Need", "tg": ["History", "Milestones"], "d": "history", "x": "The landmark 2017 paper by Vaswani et al. that introduced the transformer architecture, replacing recurrence with...", "l": "a", "k": ["attention", "need", "landmark", "paper", "vaswani", "introduced", "transformer", "architecture", "replacing", "recurrence", "self-attention", "mechanisms", "enabling", "massive", "scaling"]}, {"id": "term-attention-mask", "t": "Attention Mask", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A binary or float tensor applied to attention scores before softmax to prevent the model from attending to certain...", "l": "a", "k": ["attention", "mask", "binary", "float", "tensor", "applied", "scores", "softmax", "prevent", "model", "attending", "certain", "positions", "padding", "tokens"]}, {"id": "term-attention-masking", "t": "Attention Masking", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A technique that prevents attention from attending to certain positions by setting their attention scores to negative...", "l": "a", "k": ["attention", "masking", "technique", "prevents", "attending", "certain", "positions", "setting", "scores", "negative", "infinity", "softmax", "causal", "autoregressive", "models"]}, {"id": "term-attention-mechanism", "t": "Attention Mechanism", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A neural network component that allows the model to focus on relevant parts of the input when producing output....", "l": "a", "k": ["attention", "mechanism", "neural", "network", "component", "allows", "model", "focus", "relevant", "parts", "input", "producing", "output", "computes", "weighted"]}, {"id": "term-attention-mechanism-history", "t": "Attention Mechanism History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of attention mechanisms from Bahdanau et al.'s 2014 neural machine translation work through the...", "l": "a", "k": ["attention", "mechanism", "history", "development", "mechanisms", "bahdanau", "neural", "machine", "translation", "work", "self-attention", "innovation", "transformer", "paper", "became"]}, {"id": "term-attention-pooling", "t": "Attention Pooling", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A pooling mechanism that uses learned attention weights to aggregate features, allowing the model to focus on the most...", "l": "a", "k": ["attention", "pooling", "mechanism", "uses", "learned", "weights", "aggregate", "features", "allowing", "model", "focus", "informative", "elements", "rather", "fixed"]}, {"id": "term-attention-score", "t": "Attention Score", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The raw compatibility value computed between a query and key vector, typically via scaled dot product, before softmax...", "l": "a", "k": ["attention", "score", "raw", "compatibility", "value", "computed", "query", "key", "vector", "typically", "via", "scaled", "dot", "product", "softmax"]}, {"id": "term-attention-sink", "t": "Attention Sink", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A phenomenon where initial tokens in a sequence receive disproportionately high attention scores regardless of content,...", "l": "a", "k": ["attention", "sink", "phenomenon", "initial", "tokens", "sequence", "receive", "disproportionately", "high", "scores", "regardless", "content", "discovered", "important", "maintaining"]}, {"id": "term-attention-visualization", "t": "Attention Visualization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A model interpretability technique that displays attention weight patterns as heatmaps showing which input tokens the...", "l": "a", "k": ["attention", "visualization", "model", "interpretability", "technique", "displays", "weight", "patterns", "heatmaps", "showing", "input", "tokens", "attends", "making", "predictions"]}, {"id": "term-attention-based-parsing", "t": "Attention-Based Parsing", "tg": ["NLP", "Parsing"], "d": "general", "x": "A parsing approach that uses attention mechanisms from neural networks to determine syntactic structure, often...", "l": "a", "k": ["attention-based", "parsing", "approach", "uses", "attention", "mechanisms", "neural", "networks", "determine", "syntactic", "structure", "achieving", "state-of-the-art", "results", "attending"]}, {"id": "term-attention-based-policy", "t": "Attention-Based Policy", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "An RL policy architecture that uses attention mechanisms to selectively focus on relevant parts of the observation or...", "l": "a", "k": ["attention-based", "policy", "architecture", "uses", "attention", "mechanisms", "selectively", "focus", "relevant", "parts", "observation", "memory", "policies", "excel", "environments"]}, {"id": "term-attention-based-translation", "t": "Attention-Based Translation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A neural machine translation approach where the decoder attends to different parts of the source sentence at each...", "l": "a", "k": ["attention-based", "translation", "neural", "machine", "approach", "decoder", "attends", "different", "parts", "source", "sentence", "generation", "step", "eliminating", "information"]}, {"id": "term-auc", "t": "AUC", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "Area Under the ROC Curve, a scalar metric summarizing classifier performance across all thresholds. An AUC of 1.0...", "l": "a", "k": ["auc", "area", "roc", "curve", "scalar", "metric", "summarizing", "classifier", "performance", "across", "thresholds", "indicates", "perfect", "classification", "equivalent"]}, {"id": "term-auc-pr", "t": "AUC-PR", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Area Under the Precision-Recall Curve measures classifier performance focusing on the positive class. More informative...", "l": "a", "k": ["auc-pr", "area", "precision-recall", "curve", "measures", "classifier", "performance", "focusing", "positive", "class", "informative", "auc-roc", "imbalanced", "datasets", "baseline"]}, {"id": "term-auc-roc", "t": "AUC-ROC", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Area Under the Receiver Operating Characteristic Curve measures a classifier's ability to distinguish between classes...", "l": "a", "k": ["auc-roc", "area", "receiver", "operating", "characteristic", "curve", "measures", "classifier", "ability", "distinguish", "classes", "across", "threshold", "values", "ranges"]}, {"id": "term-audio-generation", "t": "Audio Generation", "tg": ["Application", "Generative"], "d": "general", "x": "AI that creates speech, music, or sound effects from text or other inputs. Includes text-to-speech (TTS), music...", "l": "a", "k": ["audio", "generation", "creates", "speech", "music", "sound", "effects", "text", "inputs", "includes", "text-to-speech", "tts", "design", "applications"]}, {"id": "term-audiolm", "t": "AudioLM", "tg": ["Models", "Technical"], "d": "models", "x": "A language model for audio generation by Google that treats audio as a sequence of discrete tokens. Generates...", "l": "a", "k": ["audiolm", "language", "model", "audio", "generation", "google", "treats", "sequence", "discrete", "tokens", "generates", "natural-sounding", "speech", "music", "long-term"]}, {"id": "term-auditability", "t": "Auditability", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The property of an AI system that allows independent third parties to examine its data, algorithms, models, and...", "l": "a", "k": ["auditability", "property", "system", "allows", "independent", "parties", "examine", "data", "algorithms", "models", "decision-making", "processes", "assess", "compliance", "standards"]}, {"id": "term-augmentation", "t": "Augmentation", "tg": ["Training", "Data"], "d": "general", "x": "Expanding training data by creating modified versions of existing examples. In text: paraphrasing, back-translation. In...", "l": "a", "k": ["augmentation", "expanding", "training", "data", "creating", "modified", "versions", "existing", "examples", "text", "paraphrasing", "back-translation", "images", "rotation", "cropping"]}, {"id": "term-augmented-dickey-fuller-test", "t": "Augmented Dickey-Fuller Test", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A statistical test for determining whether a unit root is present in a time series, which would indicate...", "l": "a", "k": ["augmented", "dickey-fuller", "test", "statistical", "determining", "unit", "root", "present", "time", "series", "indicate", "non-stationarity", "significant", "statistic", "leads"]}, {"id": "term-auto-complete", "t": "Auto-Complete", "tg": ["Application", "Feature"], "d": "general", "x": "AI feature that predicts and suggests text as you type. Powers writing assistants, code completion, and search...", "l": "a", "k": ["auto-complete", "feature", "predicts", "suggests", "text", "type", "powers", "writing", "assistants", "code", "completion", "search", "suggestions", "based", "language"]}, {"id": "term-autoaugment", "t": "AutoAugment", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "An automated augmentation policy search method that uses reinforcement learning to find optimal combinations and...", "l": "a", "k": ["autoaugment", "automated", "augmentation", "policy", "search", "method", "uses", "reinforcement", "learning", "find", "optimal", "combinations", "magnitudes", "operations", "given"]}, {"id": "term-autocorrelation", "t": "Autocorrelation", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "The correlation of a time series with a lagged version of itself. In regression, autocorrelated residuals violate the...", "l": "a", "k": ["autocorrelation", "correlation", "time", "series", "lagged", "version", "itself", "regression", "autocorrelated", "residuals", "violate", "independence", "assumption", "lead", "inefficient"]}, {"id": "term-autocorrelation-function", "t": "Autocorrelation Function", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A function that measures the correlation between a time series and a lagged version of itself at various time delays....", "l": "a", "k": ["autocorrelation", "function", "measures", "correlation", "time", "series", "lagged", "version", "itself", "various", "delays", "identify", "repeating", "patterns", "periodicity"]}, {"id": "term-autoencoder", "t": "Autoencoder", "tg": ["Architecture", "Unsupervised"], "d": "models", "x": "A neural network that learns to compress data into a smaller representation and then reconstruct it. Used for...", "l": "a", "k": ["autoencoder", "neural", "network", "learns", "compress", "data", "smaller", "representation", "reconstruct", "dimensionality", "reduction", "denoising", "learning", "efficient", "representations"]}, {"id": "term-autoencoder-history", "t": "Autoencoder History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of autoencoders from early work on data compression (Hinton and Salakhutdinov 2006) through denoising...", "l": "a", "k": ["autoencoder", "history", "development", "autoencoders", "early", "work", "data", "compression", "hinton", "salakhutdinov", "denoising", "vincent", "variational", "kingma", "welling"]}, {"id": "term-autogen", "t": "AutoGen", "tg": ["Framework", "Application"], "d": "general", "x": "Microsoft's framework for building multi-agent AI applications. Enables conversations between multiple AI agents that...", "l": "a", "k": ["autogen", "microsoft", "framework", "building", "multi-agent", "applications", "enables", "conversations", "multiple", "agents", "collaborate", "debate", "solve", "complex", "problems"]}, {"id": "term-automated-theorem-proving", "t": "Automated Theorem Proving", "tg": ["History", "Fundamentals"], "d": "history", "x": "The use of computers to prove mathematical theorems automatically. Beginning with the Logic Theorist in 1956 automated...", "l": "a", "k": ["automated", "theorem", "proving", "computers", "prove", "mathematical", "theorems", "automatically", "beginning", "logic", "theorist", "advanced", "resolution-based", "methods", "1960s"]}, {"id": "term-automatic-chain-of-thought", "t": "Automatic Chain-of-Thought", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A method that automatically constructs chain-of-thought demonstrations by clustering questions and selecting...", "l": "a", "k": ["automatic", "chain-of-thought", "method", "automatically", "constructs", "demonstrations", "clustering", "questions", "selecting", "representative", "examples", "model", "generate", "reasoning", "chains"]}, {"id": "term-automatic-differentiation", "t": "Automatic Differentiation", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A family of techniques for efficiently computing derivatives of numerical functions by decomposing them into elementary...", "l": "a", "k": ["automatic", "differentiation", "family", "techniques", "efficiently", "computing", "derivatives", "numerical", "functions", "decomposing", "elementary", "operations", "includes", "forward", "mode"]}, {"id": "term-automatic-prompt-engineer", "t": "Automatic Prompt Engineer", "tg": ["Prompt Engineering", "Optimization"], "d": "algorithms", "x": "An automated method (APE) that uses language models to generate, score, and select optimal prompt instructions for a...", "l": "a", "k": ["automatic", "prompt", "engineer", "automated", "method", "ape", "uses", "language", "models", "generate", "score", "select", "optimal", "instructions", "given"]}, {"id": "term-asr", "t": "Automatic Speech Recognition", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The technology that converts spoken language audio into text, using acoustic models, language models, and decoding...", "l": "a", "k": ["automatic", "speech", "recognition", "technology", "converts", "spoken", "language", "audio", "text", "acoustic", "models", "decoding", "algorithms", "transcribe", "signals"]}, {"id": "term-automation-bias", "t": "Automation Bias", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The human tendency to over-rely on automated systems and accept their outputs without sufficient critical evaluation,...", "l": "a", "k": ["automation", "bias", "human", "tendency", "over-rely", "automated", "systems", "accept", "outputs", "without", "sufficient", "critical", "evaluation", "contradicted", "evidence"]}, {"id": "term-auto-ml", "t": "AutoML", "tg": ["Tools", "Automation"], "d": "general", "x": "Automated machine learning tools that handle model selection, hyperparameter tuning, and feature engineering. Makes ML...", "l": "a", "k": ["automl", "automated", "machine", "learning", "tools", "handle", "model", "selection", "hyperparameter", "tuning", "feature", "engineering", "makes", "accessible", "non-experts"]}, {"id": "term-autonomous-weapons-debate", "t": "Autonomous Weapons Debate", "tg": ["History", "Fundamentals"], "d": "history", "x": "The ongoing international debate about the development and use of lethal autonomous weapons systems (LAWS) that can...", "l": "a", "k": ["autonomous", "weapons", "debate", "ongoing", "international", "development", "lethal", "systems", "laws", "select", "engage", "targets", "without", "human", "intervention"]}, {"id": "term-autonomous-weapons-systems", "t": "Autonomous Weapons Systems", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "Weapons systems that can select and engage targets without direct human intervention, raising profound ethical and...", "l": "a", "k": ["autonomous", "weapons", "systems", "select", "engage", "targets", "without", "direct", "human", "intervention", "raising", "profound", "ethical", "legal", "questions"]}, {"id": "term-autoregressive-model", "t": "Autoregressive Model", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A time series model that predicts the current value as a linear combination of its own past values plus a noise term....", "l": "a", "k": ["autoregressive", "model", "time", "series", "predicts", "current", "value", "linear", "combination", "past", "values", "plus", "noise", "term", "order"]}, {"id": "term-auxiliary-loss", "t": "Auxiliary Loss", "tg": ["Training", "Advanced"], "d": "general", "x": "Additional loss terms added during training to help learning. Can improve training stability, add regularization, or...", "l": "a", "k": ["auxiliary", "loss", "additional", "terms", "added", "training", "help", "learning", "improve", "stability", "add", "regularization", "encourage", "specific", "behaviors"]}, {"id": "term-auxiliary-task-rl", "t": "Auxiliary Task in RL", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "An additional prediction or control objective trained alongside the main RL objective to improve representation...", "l": "a", "k": ["auxiliary", "task", "additional", "prediction", "control", "objective", "trained", "alongside", "main", "improve", "representation", "learning", "tasks", "pixel", "reward"]}, {"id": "term-average-pooling", "t": "Average Pooling", "tg": ["Architecture", "Technique"], "d": "models", "x": "A technique that reduces data dimensionality by computing the average of regions. Used in CNNs and for creating...", "l": "a", "k": ["average", "pooling", "technique", "reduces", "data", "dimensionality", "computing", "regions", "cnns", "creating", "fixed-size", "representations", "variable-length", "sequences"]}, {"id": "term-average-precision", "t": "Average Precision", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A single-number summary of the precision-recall curve, computed as the weighted mean of precisions at each threshold...", "l": "a", "k": ["average", "precision", "single-number", "summary", "precision-recall", "curve", "computed", "weighted", "mean", "precisions", "threshold", "increase", "recall", "weight", "equivalent"]}, {"id": "term-awq", "t": "AWQ", "tg": ["LLM", "Inference"], "d": "models", "x": "Activation-aware Weight Quantization, a method that identifies and preserves salient weight channels based on...", "l": "a", "k": ["awq", "activation-aware", "weight", "quantization", "method", "identifies", "preserves", "salient", "channels", "based", "activation", "magnitudes", "enabling", "efficient", "low-bit"]}, {"id": "term-aws-inferentia", "t": "AWS Inferentia", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "Amazon's purpose-built inference accelerator chip designed for high-throughput, low-cost ML inference in the cloud....", "l": "a", "k": ["aws", "inferentia", "amazon", "purpose-built", "inference", "accelerator", "chip", "designed", "high-throughput", "low-cost", "cloud", "provides", "better", "throughput", "per"]}, {"id": "term-aws-sagemaker", "t": "AWS SageMaker", "tg": ["Platform", "Cloud"], "d": "general", "x": "Amazon's ML platform for building, training, and deploying models. Provides infrastructure, tools, and pre-built...", "l": "a", "k": ["aws", "sagemaker", "amazon", "platform", "building", "training", "deploying", "models", "provides", "infrastructure", "tools", "pre-built", "algorithms", "complete", "lifecycle"]}, {"id": "term-aws-trainium", "t": "AWS Trainium", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "Amazon's custom AI training accelerator chip offering high-performance, cost-effective alternatives to NVIDIA GPUs for...", "l": "a", "k": ["aws", "trainium", "amazon", "custom", "training", "accelerator", "chip", "offering", "high-performance", "cost-effective", "alternatives", "nvidia", "gpus", "deep", "learning"]}, {"id": "term-azure-openai", "t": "Azure OpenAI Service", "tg": ["Platform", "Cloud"], "d": "general", "x": "Microsoft's enterprise offering of OpenAI models through Azure cloud. Provides GPT-4, ChatGPT, and DALL-E with...", "l": "a", "k": ["azure", "openai", "service", "microsoft", "enterprise", "offering", "models", "cloud", "provides", "gpt-4", "chatgpt", "dall-e", "security", "compliance", "regional"]}, {"id": "term-back-translation", "t": "Back-Translation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A data augmentation technique for machine translation that translates monolingual target-language text back to the...", "l": "b", "k": ["back-translation", "data", "augmentation", "technique", "machine", "translation", "translates", "monolingual", "target-language", "text", "source", "language", "reverse", "model", "creating"]}, {"id": "term-background-removal", "t": "Background Removal", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of automatically separating foreground subjects from their background in images using deep learning...", "l": "b", "k": ["background", "removal", "process", "automatically", "separating", "foreground", "subjects", "images", "deep", "learning", "segmentation", "matting", "models", "widely", "photography"]}, {"id": "term-backpropagation", "t": "Backpropagation", "tg": ["Training", "Neural Networks"], "d": "models", "x": "The fundamental algorithm for training neural networks. It calculates how much each weight contributed to the error and...", "l": "b", "k": ["backpropagation", "fundamental", "algorithm", "training", "neural", "networks", "calculates", "weight", "contributed", "error", "adjusts", "weights", "accordingly", "propagating", "signal"]}, {"id": "term-backpropagation-discovery", "t": "Backpropagation Discovery", "tg": ["History", "Milestones"], "d": "history", "x": "The development of the backpropagation algorithm for training multi-layer neural networks. While the mathematical...", "l": "b", "k": ["backpropagation", "discovery", "development", "algorithm", "training", "multi-layer", "neural", "networks", "mathematical", "foundations", "existed", "earlier", "paper", "rumelhart", "hinton"]}, {"id": "term-backpropagation-history", "t": "Backpropagation History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of the backpropagation algorithm for training neural networks, independently discovered multiple times...", "l": "b", "k": ["backpropagation", "history", "development", "algorithm", "training", "neural", "networks", "independently", "discovered", "multiple", "times", "popularized", "rumelhart", "hinton", "williams"]}, {"id": "term-backpropagation-through-time", "t": "Backpropagation Through Time", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "An extension of backpropagation for training recurrent neural networks that unrolls the network through time steps and...", "l": "b", "k": ["backpropagation", "time", "extension", "training", "recurrent", "neural", "networks", "unrolls", "network", "steps", "applies", "chain", "rule", "computational", "cost"]}, {"id": "term-bag-of-words", "t": "Bag of Words", "tg": ["NLP", "Representation"], "d": "general", "x": "A simple text representation that counts word occurrences, ignoring order and grammar. Despite its simplicity, still...", "l": "b", "k": ["bag", "words", "simple", "text", "representation", "counts", "word", "occurrences", "ignoring", "order", "grammar", "despite", "simplicity", "useful", "classification"]}, {"id": "term-bagging", "t": "Bagging (Bootstrap Aggregating)", "tg": ["Technique", "Ensemble"], "d": "general", "x": "Training multiple models on random subsets of data and averaging their predictions. Reduces variance and overfitting....", "l": "b", "k": ["bagging", "bootstrap", "aggregating", "training", "multiple", "models", "random", "subsets", "data", "averaging", "predictions", "reduces", "variance", "overfitting", "basis"]}, {"id": "term-baichuan", "t": "Baichuan", "tg": ["Model", "Chinese AI"], "d": "models", "x": "A series of Chinese bilingual LLMs known for strong performance in Chinese language tasks. Part of the growing...", "l": "b", "k": ["baichuan", "series", "chinese", "bilingual", "llms", "known", "strong", "performance", "language", "tasks", "part", "growing", "ecosystem", "non-western", "foundation"]}, {"id": "term-bandit-algorithm", "t": "Bandit Algorithm", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "An algorithm for the multi-armed bandit problem that balances exploration (trying new actions) with exploitation...", "l": "b", "k": ["bandit", "algorithm", "multi-armed", "problem", "balances", "exploration", "trying", "actions", "exploitation", "choosing", "best-known", "action", "maximize", "cumulative", "reward"]}, {"id": "term-bandwidth", "t": "Bandwidth (AI Context)", "tg": ["Infrastructure", "Performance"], "d": "hardware", "x": "The rate at which data can be transferred, crucial for AI infrastructure. Memory bandwidth often limits GPU...", "l": "b", "k": ["bandwidth", "context", "rate", "data", "transferred", "crucial", "infrastructure", "memory", "limits", "gpu", "performance", "network", "affects", "distributed", "training"]}, {"id": "term-bandwidth-selection", "t": "Bandwidth Selection", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "The process of choosing the bandwidth parameter in kernel density estimation, which controls the smoothness of the...", "l": "b", "k": ["bandwidth", "selection", "process", "choosing", "parameter", "kernel", "density", "estimation", "controls", "smoothness", "estimated", "methods", "include", "cross-validation", "silverman"]}, {"id": "term-barbara-liskov", "t": "Barbara Liskov", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who received the 2008 Turing Award for contributions to programming language design...", "l": "b", "k": ["barbara", "liskov", "american", "computer", "scientist", "received", "turing", "award", "contributions", "programming", "language", "design", "including", "data", "abstraction"]}, {"id": "term-bard", "t": "Bard", "tg": ["Product", "Historical"], "d": "history", "x": "Google's conversational AI product, later renamed to Gemini. Competed with ChatGPT using Google's LLM technology and...", "l": "b", "k": ["bard", "google", "conversational", "product", "later", "renamed", "gemini", "competed", "chatgpt", "llm", "technology", "integration", "services"]}, {"id": "term-bark", "t": "Bark", "tg": ["Models", "Technical"], "d": "models", "x": "An open-source text-to-audio model by Suno AI that generates realistic speech in multiple languages including nonverbal...", "l": "b", "k": ["bark", "open-source", "text-to-audio", "model", "suno", "generates", "realistic", "speech", "multiple", "languages", "including", "nonverbal", "sounds", "laughter", "music"]}, {"id": "term-bart", "t": "BART", "tg": ["Models", "Technical"], "d": "models", "x": "Bidirectional and Auto-Regressive Transformers combines a bidirectional encoder with an autoregressive decoder....", "l": "b", "k": ["bart", "bidirectional", "auto-regressive", "transformers", "combines", "encoder", "autoregressive", "decoder", "pretrained", "corrupting", "text", "arbitrary", "noise", "learning", "reconstruct"]}, {"id": "term-base-model", "t": "Base Model", "tg": ["Model Type", "Training"], "d": "models", "x": "A pre-trained model before fine-tuning for specific tasks. Base models are good at text completion but need instruction...", "l": "b", "k": ["base", "model", "pre-trained", "fine-tuning", "specific", "tasks", "models", "good", "text", "completion", "need", "instruction", "tuning", "become", "helpful"]}, {"id": "term-baseline", "t": "Baseline", "tg": ["Evaluation", "Research"], "d": "datasets", "x": "A simple model or approach used as a reference point for comparison. New methods should outperform baselines to...", "l": "b", "k": ["baseline", "simple", "model", "approach", "reference", "point", "comparison", "methods", "outperform", "baselines", "demonstrate", "value", "common", "include", "random"]}, {"id": "term-batch", "t": "Batch", "tg": ["Training", "Technical"], "d": "general", "x": "A subset of training data processed together in one iteration. Batch processing improves training efficiency and...", "l": "b", "k": ["batch", "subset", "training", "data", "processed", "together", "iteration", "processing", "improves", "efficiency", "stability", "compared", "example", "time"]}, {"id": "term-batch-indexing", "t": "Batch Indexing", "tg": ["Vector Database", "Maintenance"], "d": "general", "x": "The process of building or rebuilding a vector index from a complete dataset in a single operation, producing an...", "l": "b", "k": ["batch", "indexing", "process", "building", "rebuilding", "vector", "index", "complete", "dataset", "single", "operation", "producing", "optimally", "structured", "typically"]}, {"id": "term-batch-normalization", "t": "Batch Normalization", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A technique that normalizes the inputs to each layer by subtracting the batch mean and dividing by the batch standard...", "l": "b", "k": ["batch", "normalization", "technique", "normalizes", "inputs", "layer", "subtracting", "mean", "dividing", "standard", "deviation", "applying", "learned", "scale", "shift"]}, {"id": "term-batch-processing-in-ai", "t": "Batch Processing in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "The practice of processing data in groups rather than individually during neural network training. Stochastic gradient...", "l": "b", "k": ["batch", "processing", "practice", "data", "groups", "rather", "individually", "neural", "network", "training", "stochastic", "gradient", "descent", "mini-batches", "became"]}, {"id": "term-batch-rl", "t": "Batch Reinforcement Learning", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "An approach to RL where the agent learns from a fixed batch of pre-collected transitions without online interaction....", "l": "b", "k": ["batch", "reinforcement", "learning", "approach", "agent", "learns", "fixed", "pre-collected", "transitions", "without", "online", "interaction", "methods", "fitted", "q-iteration"]}, {"id": "term-batch-renormalization", "t": "Batch Renormalization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An extension of batch normalization that introduces correction terms to reduce dependence on mini-batch statistics....", "l": "b", "k": ["batch", "renormalization", "extension", "normalization", "introduces", "correction", "terms", "reduce", "dependence", "mini-batch", "statistics", "proposed", "ioffe", "address", "failures"]}, {"id": "term-batch-scheduling", "t": "Batch Scheduling for Inference", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The strategy of grouping multiple inference requests together for simultaneous processing on a GPU, improving hardware...", "l": "b", "k": ["batch", "scheduling", "inference", "strategy", "grouping", "multiple", "requests", "together", "simultaneous", "processing", "gpu", "improving", "hardware", "utilization", "throughput"]}, {"id": "term-batch-size", "t": "Batch Size", "tg": ["Hyperparameter", "Training"], "d": "general", "x": "The number of training examples processed together before updating model weights. Larger batches provide more stable...", "l": "b", "k": ["batch", "size", "number", "training", "examples", "processed", "together", "updating", "model", "weights", "larger", "batches", "provide", "stable", "gradients"]}, {"id": "term-batched-inference", "t": "Batched Inference", "tg": ["LLM", "Inference"], "d": "models", "x": "The practice of processing multiple inference requests simultaneously through a model to maximize GPU utilization and...", "l": "b", "k": ["batched", "inference", "practice", "processing", "multiple", "requests", "simultaneously", "model", "maximize", "gpu", "utilization", "throughput", "amortizing", "cost", "loading"]}, {"id": "term-bayes-error-rate", "t": "Bayes Error Rate", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "The lowest achievable error rate for any classifier on a given classification problem, determined by the irreducible...", "l": "b", "k": ["bayes", "error", "rate", "lowest", "achievable", "classifier", "given", "classification", "problem", "determined", "irreducible", "noise", "data", "represents", "theoretical"]}, {"id": "term-bayes-theorem", "t": "Bayes' Theorem", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A fundamental rule of probability that relates the conditional probability of a hypothesis given evidence to the prior...", "l": "b", "k": ["bayes", "theorem", "fundamental", "rule", "probability", "relates", "conditional", "hypothesis", "given", "evidence", "prior", "likelihood", "marginal"]}, {"id": "term-bayesian-inference", "t": "Bayesian Inference", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A statistical framework that updates probability estimates for hypotheses as additional evidence is acquired, using...", "l": "b", "k": ["bayesian", "inference", "statistical", "framework", "updates", "probability", "estimates", "hypotheses", "additional", "evidence", "acquired", "bayes", "theorem", "compute", "posterior"]}, {"id": "term-bayesian-information-criterion", "t": "Bayesian Information Criterion", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A model selection criterion similar to AIC but with a larger penalty for the number of parameters that depends on...", "l": "b", "k": ["bayesian", "information", "criterion", "model", "selection", "similar", "aic", "larger", "penalty", "number", "parameters", "depends", "sample", "size", "tends"]}, {"id": "term-bayesian", "t": "Bayesian Methods", "tg": ["Statistics", "Theory"], "d": "algorithms", "x": "Statistical approaches that incorporate prior knowledge and update beliefs based on evidence. Used for uncertainty...", "l": "b", "k": ["bayesian", "methods", "statistical", "approaches", "incorporate", "prior", "knowledge", "update", "beliefs", "based", "evidence", "uncertainty", "quantification", "hyperparameter", "optimization"]}, {"id": "term-bayesian-model-averaging", "t": "Bayesian Model Averaging", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A technique that accounts for model uncertainty by averaging predictions across multiple models weighted by their...", "l": "b", "k": ["bayesian", "model", "averaging", "technique", "accounts", "uncertainty", "predictions", "across", "multiple", "models", "weighted", "posterior", "probabilities", "rather", "selecting"]}, {"id": "term-bayesian-network", "t": "Bayesian Network", "tg": ["Machine Learning", "Bayesian Methods"], "d": "general", "x": "A directed acyclic graph that represents a set of random variables and their conditional dependencies. Each node has a...", "l": "b", "k": ["bayesian", "network", "directed", "acyclic", "graph", "represents", "random", "variables", "conditional", "dependencies", "node", "probability", "table", "specifying", "given"]}, {"id": "term-bayesian-network-history", "t": "Bayesian Network History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of Bayesian networks by Judea Pearl and others in the 1980s, providing a graphical framework for...", "l": "b", "k": ["bayesian", "network", "history", "development", "networks", "judea", "pearl", "others", "1980s", "providing", "graphical", "framework", "representing", "reasoning", "uncertainty"]}, {"id": "term-bayesian-networks", "t": "Bayesian Networks", "tg": ["History", "Fundamentals"], "d": "history", "x": "Probabilistic graphical models that represent a set of variables and their conditional dependencies via directed...", "l": "b", "k": ["bayesian", "networks", "probabilistic", "graphical", "models", "represent", "variables", "conditional", "dependencies", "via", "directed", "acyclic", "graphs", "pioneered", "judea"]}, {"id": "term-bayesian-optimization", "t": "Bayesian Optimization", "tg": ["Machine Learning", "Bayesian Methods"], "d": "general", "x": "A sequential strategy for optimizing expensive black-box functions that builds a probabilistic surrogate model...", "l": "b", "k": ["bayesian", "optimization", "sequential", "strategy", "optimizing", "expensive", "black-box", "functions", "builds", "probabilistic", "surrogate", "model", "typically", "gaussian", "process"]}, {"id": "term-beam-search", "t": "Beam Search", "tg": ["Generation", "Algorithm"], "d": "algorithms", "x": "A search algorithm used in text generation that maintains multiple candidate sequences at each step, selecting the most...", "l": "b", "k": ["beam", "search", "algorithm", "text", "generation", "maintains", "multiple", "candidate", "sequences", "step", "selecting", "promising", "ones", "balances", "quality"]}, {"id": "term-beam-search-decoding", "t": "Beam Search Decoding", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A search algorithm for sequence generation that maintains the top-K partial sequences at each step. Explores multiple...", "l": "b", "k": ["beam", "search", "decoding", "algorithm", "sequence", "generation", "maintains", "top-k", "partial", "sequences", "step", "explores", "multiple", "hypotheses", "simultaneously"]}, {"id": "term-behavior-cloning", "t": "Behavior Cloning", "tg": ["Training", "Imitation"], "d": "general", "x": "Learning to imitate expert behavior from demonstrations. The model learns to map observations to actions by copying...", "l": "b", "k": ["behavior", "cloning", "learning", "imitate", "expert", "demonstrations", "model", "learns", "map", "observations", "actions", "copying", "experts", "similar", "situations"]}, {"id": "term-behavior-based-robotics", "t": "Behavior-Based Robotics", "tg": ["History", "Fundamentals"], "d": "history", "x": "An approach to robotics that generates complex behavior from the interaction of simple reactive behaviors rather than...", "l": "b", "k": ["behavior-based", "robotics", "approach", "generates", "complex", "behavior", "interaction", "simple", "reactive", "behaviors", "rather", "centralized", "planning", "pioneered", "rodney"]}, {"id": "term-beijing-ai-principles", "t": "Beijing AI Principles", "tg": ["Governance", "Regulation"], "d": "safety", "x": "A set of AI governance principles released in 2019 by the Beijing Academy of AI, emphasizing harmony, fairness, safety,...", "l": "b", "k": ["beijing", "principles", "governance", "released", "academy", "emphasizing", "harmony", "fairness", "safety", "shared", "benefits", "responsible", "development", "chinese", "context"]}, {"id": "term-beit", "t": "BEiT", "tg": ["Models", "Technical"], "d": "models", "x": "Bidirectional Encoder representation from Image Transformers applies masked image modeling as a pretraining task for...", "l": "b", "k": ["beit", "bidirectional", "encoder", "representation", "image", "transformers", "applies", "masked", "modeling", "pretraining", "task", "vision", "tokenizes", "patches", "visual"]}, {"id": "term-bell-labs-ai-research", "t": "Bell Labs AI Research", "tg": ["History", "Organizations"], "d": "history", "x": "AI and machine learning research conducted at Bell Laboratories (AT&T) where foundational work was done on information...", "l": "b", "k": ["bell", "labs", "research", "machine", "learning", "conducted", "laboratories", "foundational", "work", "done", "information", "theory", "speech", "recognition", "neural"]}, {"id": "term-bellman-equation", "t": "Bellman Equation", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A recursive equation relating the value of a state to the immediate reward plus the discounted value of successor...", "l": "b", "k": ["bellman", "equation", "recursive", "relating", "value", "state", "immediate", "reward", "plus", "discounted", "successor", "states", "provides", "foundation", "dynamic"]}, {"id": "term-benchmark", "t": "Benchmark", "tg": ["Evaluation", "Research"], "d": "datasets", "x": "A standardized test or dataset used to evaluate and compare AI model performance. Common LLM benchmarks include MMLU,...", "l": "b", "k": ["benchmark", "standardized", "test", "dataset", "evaluate", "compare", "model", "performance", "common", "llm", "benchmarks", "include", "mmlu", "hellaswag", "humaneval"]}, {"id": "term-benchmark-gaming", "t": "Benchmark Gaming", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The practice of optimizing a model specifically to achieve high scores on popular benchmarks without corresponding...", "l": "b", "k": ["benchmark", "gaming", "practice", "optimizing", "model", "specifically", "achieve", "high", "scores", "popular", "benchmarks", "without", "corresponding", "improvements", "real-world"]}, {"id": "term-benefit-sharing-in-ai", "t": "Benefit Sharing in AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The principle that the economic and social benefits generated by AI should be distributed broadly across society rather...", "l": "b", "k": ["benefit", "sharing", "principle", "economic", "social", "benefits", "generated", "distributed", "broadly", "across", "society", "rather", "concentrated", "among", "small"]}, {"id": "term-berkeley-ai-research-lab", "t": "Berkeley AI Research Lab", "tg": ["History", "Organizations"], "d": "history", "x": "The Berkeley Artificial Intelligence Research Laboratory (BAIR) at UC Berkeley conducting research across computer...", "l": "b", "k": ["berkeley", "research", "lab", "artificial", "intelligence", "laboratory", "bair", "conducting", "across", "computer", "vision", "nlp", "robotics", "machine", "learning"]}, {"id": "term-bernoulli-distribution", "t": "Bernoulli Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "The simplest discrete probability distribution, modeling a single trial with two outcomes (success with probability p,...", "l": "b", "k": ["bernoulli", "distribution", "simplest", "discrete", "probability", "modeling", "single", "trial", "outcomes", "success", "failure", "1-p", "building", "block", "binomial"]}, {"id": "term-bert", "t": "BERT (Bidirectional Encoder Representations from Transformers)", "tg": ["Model", "Architecture"], "d": "models", "x": "A influential language model from Google (2018) that processes text bidirectionally, understanding context from both...", "l": "b", "k": ["bert", "bidirectional", "encoder", "representations", "transformers", "influential", "language", "model", "google", "processes", "text", "bidirectionally", "understanding", "context", "left"]}, {"id": "term-bert-release", "t": "BERT Release", "tg": ["History", "Milestones"], "d": "history", "x": "Google's Bidirectional Encoder Representations from Transformers model, released in October 2018, which achieved...", "l": "b", "k": ["bert", "release", "google", "bidirectional", "encoder", "representations", "transformers", "model", "released", "october", "achieved", "state-of-the-art", "results", "across", "numerous"]}, {"id": "term-bertscore", "t": "BERTScore", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation metric that computes the similarity between generated and reference texts using contextual BERT...", "l": "b", "k": ["bertscore", "evaluation", "metric", "computes", "similarity", "generated", "reference", "texts", "contextual", "bert", "embeddings", "greedy", "token", "matching", "capturing"]}, {"id": "term-best-of-n-sampling", "t": "Best-of-N Sampling", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "An inference strategy that generates N candidate completions and returns the one scoring highest on a reward model,...", "l": "b", "k": ["best-of-n", "sampling", "inference", "strategy", "generates", "candidate", "completions", "returns", "scoring", "highest", "reward", "model", "trading", "increased", "compute"]}, {"id": "term-beta-distribution", "t": "Beta Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution defined on the interval [0, 1], parametrized by two shape parameters. It is...", "l": "b", "k": ["beta", "distribution", "continuous", "probability", "defined", "interval", "parametrized", "shape", "parameters", "commonly", "prior", "probabilities", "bayesian", "inference"]}, {"id": "term-beta-vae", "t": "Beta-VAE", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A modification of the variational autoencoder that introduces a hyperparameter beta to weight the KL divergence term,...", "l": "b", "k": ["beta-vae", "modification", "variational", "autoencoder", "introduces", "hyperparameter", "beta", "weight", "divergence", "term", "promoting", "disentangled", "latent", "representations", "greater"]}, {"id": "term-bev-perception", "t": "BEV Perception", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "Bird's Eye View perception, a paradigm in autonomous driving that transforms multi-camera or LiDAR data into a unified...", "l": "b", "k": ["bev", "perception", "bird", "eye", "view", "paradigm", "autonomous", "driving", "transforms", "multi-camera", "lidar", "data", "unified", "top-down", "representation"]}, {"id": "term-bf16", "t": "BF16 (Brain Floating Point)", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "A 16-bit floating-point format with 8 exponent bits (same as FP32) and 7 mantissa bits, developed by Google Brain. BF16...", "l": "b", "k": ["bf16", "brain", "floating", "point", "16-bit", "floating-point", "format", "exponent", "bits", "fp32", "mantissa", "developed", "google", "maintains", "dynamic"]}, {"id": "term-bfloat16", "t": "bfloat16", "tg": ["Technical", "Precision"], "d": "general", "x": "A 16-bit floating-point format optimized for neural network training. Sacrifices precision for range compared to...", "l": "b", "k": ["bfloat16", "16-bit", "floating-point", "format", "optimized", "neural", "network", "training", "sacrifices", "precision", "range", "compared", "float16", "offering", "better"]}, {"id": "term-bge", "t": "BGE", "tg": ["Models", "Technical"], "d": "models", "x": "BAAI General Embedding is a family of embedding models that achieve strong performance on text retrieval tasks. Trained...", "l": "b", "k": ["bge", "baai", "general", "embedding", "family", "models", "achieve", "strong", "performance", "text", "retrieval", "tasks", "trained", "beijing", "academy"]}, {"id": "term-bi-encoder", "t": "Bi-Encoder", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "A neural retrieval architecture that independently encodes queries and documents into fixed-size vectors using separate...", "l": "b", "k": ["bi-encoder", "neural", "retrieval", "architecture", "independently", "encodes", "queries", "documents", "fixed-size", "vectors", "separate", "shared", "encoders", "enabling", "pre-computation"]}, {"id": "term-estimation-bias", "t": "Bias", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "In statistical estimation, the difference between the expected value of an estimator and the true value of the...", "l": "b", "k": ["bias", "statistical", "estimation", "difference", "expected", "value", "estimator", "true", "parameter", "estimated", "unbiased", "zero"]}, {"id": "term-bias-score", "t": "Bias Score", "tg": ["Evaluation", "Safety"], "d": "datasets", "x": "A metric that measures the degree of systematic prejudice or unfair treatment in model outputs across demographic...", "l": "b", "k": ["bias", "score", "metric", "measures", "degree", "systematic", "prejudice", "unfair", "treatment", "model", "outputs", "across", "demographic", "groups", "assessed"]}, {"id": "term-bias-variance-decomposition", "t": "Bias-Variance Decomposition", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A mathematical decomposition of expected prediction error into three components: irreducible noise, squared bias...", "l": "b", "k": ["bias-variance", "decomposition", "mathematical", "expected", "prediction", "error", "components", "irreducible", "noise", "squared", "bias", "systematic", "variance", "sensitivity", "training"]}, {"id": "term-bias-variance-tradeoff", "t": "Bias-Variance Tradeoff", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "The fundamental tension in supervised learning between a model's ability to minimize bias (error from overly simplistic...", "l": "b", "k": ["bias-variance", "tradeoff", "fundamental", "tension", "supervised", "learning", "model", "ability", "minimize", "bias", "error", "overly", "simplistic", "assumptions", "variance"]}, {"id": "term-biden-executive-order-on-ai", "t": "Biden Executive Order on AI", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Executive Order 14110, issued by President Biden in October 2023, establishing requirements for AI safety and security...", "l": "b", "k": ["biden", "executive", "order", "issued", "president", "october", "establishing", "requirements", "safety", "security", "including", "red-teaming", "standards", "reporting", "large"]}, {"id": "term-bidirectional", "t": "Bidirectional", "tg": ["Architecture", "Processing"], "d": "models", "x": "Processing sequences in both directions (left-to-right and right-to-left). BERT processes bidirectionally for...", "l": "b", "k": ["bidirectional", "processing", "sequences", "directions", "left-to-right", "right-to-left", "bert", "processes", "bidirectionally", "understanding", "gpt", "unidirectionally", "generation"]}, {"id": "term-bidirectional-attention", "t": "Bidirectional Attention", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An attention pattern that allows each token to attend to all other tokens in both directions. Used in encoder models...", "l": "b", "k": ["bidirectional", "attention", "pattern", "allows", "token", "attend", "tokens", "directions", "encoder", "models", "bert", "building", "contextual", "representations", "contrasts"]}, {"id": "term-bidirectional-rnn", "t": "Bidirectional RNN", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A recurrent architecture that processes input sequences in both forward and backward directions simultaneously,...", "l": "b", "k": ["bidirectional", "rnn", "recurrent", "architecture", "processes", "input", "sequences", "forward", "backward", "directions", "simultaneously", "combining", "context", "produce", "richer"]}, {"id": "term-bigbench", "t": "BigBench", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Beyond the Imitation Game Benchmark, a large collaborative benchmark containing over 200 diverse tasks contributed by...", "l": "b", "k": ["bigbench", "beyond", "imitation", "game", "benchmark", "large", "collaborative", "containing", "diverse", "tasks", "contributed", "researchers", "designed", "probe", "language"]}, {"id": "term-bigbird", "t": "BigBird", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A sparse attention transformer that combines random attention, window attention, and global attention patterns to...", "l": "b", "k": ["bigbird", "sparse", "attention", "transformer", "combines", "random", "window", "global", "patterns", "achieve", "linear", "complexity", "provably", "maintaining", "expressive"]}, {"id": "term-biggan", "t": "BigGAN", "tg": ["Models", "Technical"], "d": "models", "x": "A large-scale GAN that generates high-fidelity images by scaling up batch size model size and applying...", "l": "b", "k": ["biggan", "large-scale", "gan", "generates", "high-fidelity", "images", "scaling", "batch", "size", "model", "applying", "class-conditional", "generation", "truncation", "demonstrated"]}, {"id": "term-bigram", "t": "Bigram / N-gram", "tg": ["NLP", "Historical"], "d": "history", "x": "Sequences of N consecutive tokens used in language modeling. Bigrams are pairs; trigrams are triples. N-gram models...", "l": "b", "k": ["bigram", "n-gram", "sequences", "consecutive", "tokens", "language", "modeling", "bigrams", "pairs", "trigrams", "triples", "models", "were", "dominant", "neural"]}, {"id": "term-binary-classification", "t": "Binary Classification", "tg": ["ML Task", "Classification"], "d": "general", "x": "A classification task with exactly two possible outcomes (yes/no, spam/not spam). The simplest classification problem,...", "l": "b", "k": ["binary", "classification", "task", "exactly", "possible", "outcomes", "yes", "spam", "simplest", "problem", "building", "block", "complex", "tasks"]}, {"id": "term-binary-cross-entropy", "t": "Binary Cross-Entropy", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A loss function for binary classification that measures the divergence between predicted probabilities and binary...", "l": "b", "k": ["binary", "cross-entropy", "loss", "function", "classification", "measures", "divergence", "predicted", "probabilities", "labels", "defined", "log", "1-y", "1-p", "equivalent"]}, {"id": "term-binary-quantization", "t": "Binary Quantization", "tg": ["Vector Database", "Quantization"], "d": "general", "x": "An aggressive vector compression technique that reduces each vector dimension to a single bit based on its sign,...", "l": "b", "k": ["binary", "quantization", "aggressive", "vector", "compression", "technique", "reduces", "dimension", "single", "bit", "based", "sign", "enabling", "32x", "float32"]}, {"id": "term-bing-chat", "t": "Bing Chat / Copilot", "tg": ["Product", "Microsoft"], "d": "general", "x": "Microsoft's AI-powered search assistant, integrating GPT-4 with web search. Can answer questions with citations, create...", "l": "b", "k": ["bing", "chat", "copilot", "microsoft", "ai-powered", "search", "assistant", "integrating", "gpt-4", "web", "answer", "questions", "citations", "create", "content"]}, {"id": "term-binomial-distribution", "t": "Binomial Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A discrete probability distribution modeling the number of successes in a fixed number of independent Bernoulli trials,...", "l": "b", "k": ["binomial", "distribution", "discrete", "probability", "modeling", "number", "successes", "fixed", "independent", "bernoulli", "trials", "success", "parametrized"]}, {"id": "term-bio-tagging", "t": "BIO Tagging", "tg": ["NLP", "Text Processing"], "d": "general", "x": "An alternative name for IOB tagging format using Begin, Inside, Outside labels for sequence labeling tasks, where B...", "l": "b", "k": ["bio", "tagging", "alternative", "name", "iob", "format", "begin", "inside", "outside", "labels", "sequence", "labeling", "tasks", "marks", "start"]}, {"id": "term-bioes-tagging", "t": "BIOES Tagging", "tg": ["NLP", "Text Processing"], "d": "general", "x": "An extended sequence labeling scheme that adds End and Single tags to the BIO format, providing more precise boundary...", "l": "b", "k": ["bioes", "tagging", "extended", "sequence", "labeling", "scheme", "adds", "end", "single", "tags", "bio", "format", "providing", "precise", "boundary"]}, {"id": "term-biogpt", "t": "BioGPT", "tg": ["Models", "Technical"], "d": "models", "x": "A domain-specific generative language model pretrained on large-scale biomedical literature. Achieves strong...", "l": "b", "k": ["biogpt", "domain-specific", "generative", "language", "model", "pretrained", "large-scale", "biomedical", "literature", "achieves", "strong", "performance", "text", "generation", "question"]}, {"id": "term-biometric-ai-regulation", "t": "Biometric AI Regulation", "tg": ["Privacy", "Regulation"], "d": "safety", "x": "Legal restrictions on AI systems that process biometric data such as facial features, fingerprints, or gait patterns,...", "l": "b", "k": ["biometric", "regulation", "legal", "restrictions", "systems", "process", "data", "facial", "features", "fingerprints", "gait", "patterns", "including", "bans", "real-time"]}, {"id": "term-birch", "t": "BIRCH", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Balanced Iterative Reducing and Clustering using Hierarchies is a clustering algorithm designed for large datasets....", "l": "b", "k": ["birch", "balanced", "iterative", "reducing", "clustering", "hierarchies", "algorithm", "designed", "large", "datasets", "builds", "compact", "summary", "called", "cf-tree"]}, {"id": "term-bisimulation-metric", "t": "Bisimulation Metric", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A distance metric on states that groups together states with similar reward and transition dynamics, providing a...", "l": "b", "k": ["bisimulation", "metric", "distance", "states", "groups", "together", "similar", "reward", "transition", "dynamics", "providing", "principled", "basis", "state", "abstraction"]}, {"id": "term-bit-precision", "t": "Bit Precision", "tg": ["Technical", "Optimization"], "d": "algorithms", "x": "The number of bits used to represent model weights and activations. Lower precision (8-bit, 4-bit) reduces memory and...", "l": "b", "k": ["bit", "precision", "number", "bits", "represent", "model", "weights", "activations", "lower", "8-bit", "4-bit", "reduces", "memory", "increases", "speed"]}, {"id": "term-bitter-lesson", "t": "Bitter Lesson", "tg": ["History", "Milestones"], "d": "history", "x": "An influential 2019 essay by Rich Sutton arguing that the history of AI shows general methods leveraging computation...", "l": "b", "k": ["bitter", "lesson", "influential", "essay", "rich", "sutton", "arguing", "history", "shows", "general", "methods", "leveraging", "computation", "search", "learning"]}, {"id": "term-black-box", "t": "Black Box", "tg": ["Interpretability", "Concept"], "d": "general", "x": "A system whose internal workings are not visible or understandable to users. Many AI models are considered black boxes...", "l": "b", "k": ["black", "box", "system", "whose", "internal", "workings", "visible", "understandable", "users", "models", "considered", "boxes", "decision-making", "processes", "difficult"]}, {"id": "term-black-box-problem", "t": "Black Box Problem", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The challenge that many AI systems, particularly deep neural networks, operate in ways that are opaque to human...", "l": "b", "k": ["black", "box", "problem", "challenge", "systems", "particularly", "deep", "neural", "networks", "operate", "ways", "opaque", "human", "understanding", "making"]}, {"id": "term-blackboard-system", "t": "Blackboard System", "tg": ["History", "Fundamentals"], "d": "history", "x": "An AI architecture where multiple knowledge sources cooperate to solve a problem by reading from and writing to a...", "l": "b", "k": ["blackboard", "system", "architecture", "multiple", "knowledge", "sources", "cooperate", "solve", "problem", "reading", "writing", "shared", "data", "structure", "called"]}, {"id": "term-bletchley-declaration-on-ai", "t": "Bletchley Declaration on AI", "tg": ["Governance", "Regulation"], "d": "safety", "x": "A declaration signed by 28 countries at the November 2023 AI Safety Summit at Bletchley Park, acknowledging the...", "l": "b", "k": ["bletchley", "declaration", "signed", "countries", "november", "safety", "summit", "park", "acknowledging", "potential", "serious", "harm", "frontier", "committing", "international"]}, {"id": "term-bletchley-park-ai-safety-summit", "t": "Bletchley Park AI Safety Summit", "tg": ["History", "Governance"], "d": "history", "x": "The first major international AI Safety Summit held at Bletchley Park, UK, in November 2023, bringing together...", "l": "b", "k": ["bletchley", "park", "safety", "summit", "major", "international", "held", "november", "bringing", "together", "governments", "companies", "discuss", "frontier", "risks"]}, {"id": "term-bletchley-park-codebreaking", "t": "Bletchley Park Codebreaking", "tg": ["History", "Milestones"], "d": "history", "x": "The World War II British codebreaking operation where Alan Turing and colleagues developed the Bombe and Colossus...", "l": "b", "k": ["bletchley", "park", "codebreaking", "world", "war", "british", "operation", "alan", "turing", "colleagues", "developed", "bombe", "colossus", "machines", "decrypt"]}, {"id": "term-bleu-score", "t": "BLEU Score", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Bilingual Evaluation Understudy is a metric for evaluating machine translation quality by comparing n-gram overlap...", "l": "b", "k": ["bleu", "score", "bilingual", "evaluation", "understudy", "metric", "evaluating", "machine", "translation", "quality", "comparing", "n-gram", "overlap", "candidate", "reference"]}, {"id": "term-bleurt", "t": "BLEURT", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A learned evaluation metric that fine-tunes BERT on synthetic and human-rated data to predict text quality scores,...", "l": "b", "k": ["bleurt", "learned", "evaluation", "metric", "fine-tunes", "bert", "synthetic", "human-rated", "data", "predict", "text", "quality", "scores", "providing", "robust"]}, {"id": "term-bloom", "t": "BLOOM", "tg": ["Model", "Open Source"], "d": "models", "x": "A large multilingual open-source language model created by BigScience, trained on 46 languages. Demonstrated the...", "l": "b", "k": ["bloom", "large", "multilingual", "open-source", "language", "model", "created", "bigscience", "trained", "languages", "demonstrated", "viability", "collaborative", "open", "development"]}, {"id": "term-bloomberggpt", "t": "BloombergGPT", "tg": ["Models", "Technical"], "d": "models", "x": "A 50 billion parameter language model by Bloomberg trained on a mix of financial data and general text. Designed for...", "l": "b", "k": ["bloomberggpt", "billion", "parameter", "language", "model", "bloomberg", "trained", "mix", "financial", "data", "general", "text", "designed", "nlp", "tasks"]}, {"id": "term-bm25", "t": "BM25", "tg": ["NLP", "Text Processing"], "d": "general", "x": "Best Matching 25, a probabilistic information retrieval ranking function that extends TF-IDF with document length...", "l": "b", "k": ["bm25", "best", "matching", "probabilistic", "information", "retrieval", "ranking", "function", "extends", "tf-idf", "document", "length", "normalization", "term", "frequency"]}, {"id": "term-bm25-in-rag", "t": "BM25 in RAG", "tg": ["Retrieval", "Search"], "d": "general", "x": "The application of the Best Matching 25 probabilistic ranking function within retrieval-augmented generation pipelines,...", "l": "b", "k": ["bm25", "rag", "application", "best", "matching", "probabilistic", "ranking", "function", "within", "retrieval-augmented", "generation", "pipelines", "providing", "strong", "lexical"]}, {"id": "term-boltzmann-exploration", "t": "Boltzmann Exploration", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An exploration strategy that selects actions with probability proportional to exponentiated Q-values divided by a...", "l": "b", "k": ["boltzmann", "exploration", "strategy", "selects", "actions", "probability", "proportional", "exponentiated", "q-values", "divided", "temperature", "parameter", "higher", "temperatures", "increase"]}, {"id": "term-boltzmann-machine", "t": "Boltzmann Machine", "tg": ["History", "Milestones"], "d": "history", "x": "A stochastic neural network model invented by Geoffrey Hinton and Terry Sejnowski in 1985 that uses simulated annealing...", "l": "b", "k": ["boltzmann", "machine", "stochastic", "neural", "network", "model", "invented", "geoffrey", "hinton", "terry", "sejnowski", "uses", "simulated", "annealing", "learn"]}, {"id": "term-bonferroni-correction", "t": "Bonferroni Correction", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A multiple comparison adjustment that divides the significance level by the number of tests performed, controlling the...", "l": "b", "k": ["bonferroni", "correction", "multiple", "comparison", "adjustment", "divides", "significance", "level", "number", "tests", "performed", "controlling", "family-wise", "error", "rate"]}, {"id": "term-boolean-retrieval", "t": "Boolean Retrieval", "tg": ["Search", "Traditional"], "d": "general", "x": "Search using AND, OR, NOT operators to combine terms. Simple but limited compared to semantic search. Still used in...", "l": "b", "k": ["boolean", "retrieval", "search", "operators", "combine", "terms", "simple", "limited", "compared", "semantic", "specialized", "databases", "advanced", "interfaces"]}, {"id": "term-boosting", "t": "Boosting", "tg": ["Technique", "ML"], "d": "general", "x": "An ensemble technique that trains models sequentially, with each new model focusing on examples the previous ones got...", "l": "b", "k": ["boosting", "ensemble", "technique", "trains", "models", "sequentially", "model", "focusing", "examples", "previous", "ones", "got", "wrong", "powers", "xgboost"]}, {"id": "term-boosting-history", "t": "Boosting History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of boosting algorithms from the theoretical work of Michael Kearns and Leslie Valiant (1988) through...", "l": "b", "k": ["boosting", "history", "development", "algorithms", "theoretical", "work", "michael", "kearns", "leslie", "valiant", "adaboost", "freund", "schapire", "gradient", "friedman"]}, {"id": "term-bootstrap", "t": "Bootstrap", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A resampling technique that estimates the sampling distribution of a statistic by repeatedly drawing samples with...", "l": "b", "k": ["bootstrap", "resampling", "technique", "estimates", "sampling", "distribution", "statistic", "repeatedly", "drawing", "samples", "replacement", "observed", "data", "provides", "standard"]}, {"id": "term-bootstrap-aggregating", "t": "Bootstrap Aggregating", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An ensemble method (also called bagging) that trains multiple models on different bootstrap samples of the training...", "l": "b", "k": ["bootstrap", "aggregating", "ensemble", "method", "called", "bagging", "trains", "multiple", "models", "different", "samples", "training", "data", "combines", "predictions"]}, {"id": "term-bottleneck", "t": "Bottleneck", "tg": ["Architecture", "Design"], "d": "models", "x": "A narrow layer in a neural network that forces compression of information. Used in autoencoders and some architectures...", "l": "b", "k": ["bottleneck", "narrow", "layer", "neural", "network", "forces", "compression", "information", "autoencoders", "architectures", "learn", "efficient", "representations"]}, {"id": "term-bottleneck-layer", "t": "Bottleneck Layer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A narrow hidden layer that compresses representations to a lower dimension before expanding them, used in autoencoders...", "l": "b", "k": ["bottleneck", "layer", "narrow", "hidden", "compresses", "representations", "lower", "dimension", "expanding", "autoencoders", "residual", "blocks", "reduce", "computation", "encourage"]}, {"id": "term-box-cox-transformation", "t": "Box-Cox Transformation", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A family of power transformations parametrized by lambda that aims to stabilize variance and make data more normally...", "l": "b", "k": ["box-cox", "transformation", "family", "power", "transformations", "parametrized", "lambda", "aims", "stabilize", "variance", "data", "normally", "distributed", "special", "cases"]}, {"id": "term-bpe", "t": "BPE (Byte Pair Encoding)", "tg": ["Tokenization", "NLP"], "d": "general", "x": "A tokenization algorithm that breaks text into subword units. Starts with individual characters and iteratively merges...", "l": "b", "k": ["bpe", "byte", "pair", "encoding", "tokenization", "algorithm", "breaks", "text", "subword", "units", "starts", "individual", "characters", "iteratively", "merges"]}, {"id": "term-brain-computer", "t": "Brain-Computer Interface (BCI)", "tg": ["Application", "Neuroscience"], "d": "general", "x": "Technology connecting brain signals directly to computers. AI helps interpret neural signals for prosthetics,...", "l": "b", "k": ["brain-computer", "interface", "bci", "technology", "connecting", "brain", "signals", "directly", "computers", "helps", "interpret", "neural", "prosthetics", "communication", "devices"]}, {"id": "term-branch-and-bound", "t": "Branch and Bound", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An algorithmic paradigm for solving combinatorial optimization problems that systematically enumerates candidates while...", "l": "b", "k": ["branch", "bound", "algorithmic", "paradigm", "solving", "combinatorial", "optimization", "problems", "systematically", "enumerates", "candidates", "pruning", "large", "portions", "search"]}, {"id": "term-brier-score", "t": "Brier Score", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A scoring metric that measures the accuracy of probabilistic predictions by computing the mean squared difference...", "l": "b", "k": ["brier", "score", "scoring", "metric", "measures", "accuracy", "probabilistic", "predictions", "computing", "mean", "squared", "difference", "predicted", "probabilities", "actual"]}, {"id": "term-browse-mode", "t": "Browse Mode", "tg": ["Feature", "Capability"], "d": "general", "x": "AI capability to access and retrieve current web information during conversations. Addresses knowledge cutoff...", "l": "b", "k": ["browse", "mode", "capability", "access", "retrieve", "current", "web", "information", "conversations", "addresses", "knowledge", "cutoff", "limitations", "fetching", "real-time"]}, {"id": "term-bruce-buchanan", "t": "Bruce Buchanan", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who co-developed DENDRAL the first expert system with Edward Feigenbaum and Joshua...", "l": "b", "k": ["bruce", "buchanan", "american", "computer", "scientist", "co-developed", "dendral", "expert", "system", "edward", "feigenbaum", "joshua", "lederberg", "stanford", "work"]}, {"id": "term-buffer", "t": "Buffer (Memory)", "tg": ["Technical", "Architecture"], "d": "models", "x": "Temporary storage for data being processed. In AI agents, conversation buffers store recent exchanges. In training,...", "l": "b", "k": ["buffer", "memory", "temporary", "storage", "data", "processed", "agents", "conversation", "buffers", "store", "recent", "exchanges", "training", "optimize", "gpu"]}, {"id": "term-bundle-adjustment", "t": "Bundle Adjustment", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "An optimization procedure that jointly refines 3D point positions and camera parameters by minimizing the reprojection...", "l": "b", "k": ["bundle", "adjustment", "optimization", "procedure", "jointly", "refines", "point", "positions", "camera", "parameters", "minimizing", "reprojection", "error", "across", "views"]}, {"id": "term-burst", "t": "Burst (API)", "tg": ["API", "Usage"], "d": "general", "x": "Short periods of high API usage that may exceed normal rate limits. Many providers allow bursting with gradual...", "l": "b", "k": ["burst", "api", "short", "periods", "high", "usage", "exceed", "normal", "rate", "limits", "providers", "allow", "bursting", "gradual", "throttling"]}, {"id": "term-burstiness", "t": "Burstiness", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A statistical property measuring the variability of sentence length and structure in text, often used in AI-generated...", "l": "b", "k": ["burstiness", "statistical", "property", "measuring", "variability", "sentence", "length", "structure", "text", "ai-generated", "detection", "machine-generated", "content", "tends", "show"]}, {"id": "term-byte-fallback", "t": "Byte Fallback", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A tokenization strategy that encodes unknown characters as individual bytes when they cannot be represented by the...", "l": "b", "k": ["byte", "fallback", "tokenization", "strategy", "encodes", "unknown", "characters", "individual", "bytes", "cannot", "represented", "learned", "vocabulary", "ensuring", "possible"]}, {"id": "term-byte-pair-encoding", "t": "Byte Pair Encoding", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A subword tokenization algorithm that iteratively merges the most frequent pair of consecutive bytes or characters....", "l": "b", "k": ["byte", "pair", "encoding", "subword", "tokenization", "algorithm", "iteratively", "merges", "frequent", "consecutive", "bytes", "characters", "originally", "data", "compression"]}, {"id": "term-bpe-tokenizer", "t": "Byte Pair Encoding Tokenizer", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A subword tokenization algorithm that iteratively merges the most frequent pair of adjacent bytes or characters in the...", "l": "b", "k": ["byte", "pair", "encoding", "tokenizer", "subword", "tokenization", "algorithm", "iteratively", "merges", "frequent", "adjacent", "bytes", "characters", "training", "corpus"]}, {"id": "term-byte-level-tokenization", "t": "Byte-Level Tokenization", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A tokenization approach that operates on raw bytes rather than characters, ensuring complete coverage of any text input...", "l": "b", "k": ["byte-level", "tokenization", "approach", "operates", "raw", "bytes", "rather", "characters", "ensuring", "complete", "coverage", "text", "input", "without", "unknown"]}, {"id": "term-c2pa", "t": "C2PA", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The Coalition for Content Provenance and Authenticity, a joint development foundation creating technical standards for...", "l": "c", "k": ["c2pa", "coalition", "content", "provenance", "authenticity", "joint", "development", "foundation", "creating", "technical", "standards", "certifying", "source", "history", "media"]}, {"id": "term-caffe-framework", "t": "Caffe Framework", "tg": ["History", "Milestones"], "d": "history", "x": "A deep learning framework developed by Yangqing Jia at UC Berkeley in 2013. Caffe (Convolutional Architecture for Fast...", "l": "c", "k": ["caffe", "framework", "deep", "learning", "developed", "yangqing", "jia", "berkeley", "convolutional", "architecture", "fast", "feature", "embedding", "widely", "computer"]}, {"id": "term-calibration", "t": "Calibration", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "The degree to which a model's predicted probabilities match the true frequencies of outcomes. A well-calibrated model...", "l": "c", "k": ["calibration", "degree", "model", "predicted", "probabilities", "match", "true", "frequencies", "outcomes", "well-calibrated", "predicting", "probability", "correct", "approximately", "time"]}, {"id": "term-calibration-data", "t": "Calibration Data for Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A representative subset of input data used to determine optimal quantization parameters such as scaling factors and...", "l": "c", "k": ["calibration", "data", "quantization", "representative", "subset", "input", "determine", "optimal", "parameters", "scaling", "factors", "zero", "points", "quality", "directly"]}, {"id": "term-calibration-error", "t": "Calibration Error", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that measures the discrepancy between a model's predicted confidence and its actual accuracy, where a...", "l": "c", "k": ["calibration", "error", "metric", "measures", "discrepancy", "model", "predicted", "confidence", "actual", "accuracy", "well-calibrated", "stated", "probability", "correct", "closely"]}, {"id": "term-calibration-fairness", "t": "Calibration Fairness", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness metric requiring that among individuals assigned a given predicted probability, the actual proportion of...", "l": "c", "k": ["calibration", "fairness", "metric", "requiring", "among", "individuals", "assigned", "given", "predicted", "probability", "actual", "proportion", "positive", "outcomes", "across"]}, {"id": "term-callback", "t": "Callback", "tg": ["Technical", "Training"], "d": "general", "x": "A function called at specific points during training or inference. Used for logging, checkpointing, early stopping, and...", "l": "c", "k": ["callback", "function", "called", "specific", "points", "training", "inference", "logging", "checkpointing", "early", "stopping", "custom", "behaviors", "pipelines"]}, {"id": "term-camera-calibration", "t": "Camera Calibration", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The process of estimating the intrinsic parameters (focal length, principal point, distortion) and extrinsic parameters...", "l": "c", "k": ["camera", "calibration", "process", "estimating", "intrinsic", "parameters", "focal", "length", "principal", "point", "distortion", "extrinsic", "position", "orientation", "essential"]}, {"id": "term-capability", "t": "Capability (AI)", "tg": ["Concept", "Assessment"], "d": "general", "x": "A specific skill or function an AI system can perform. Capabilities range from basic (text generation) to advanced...", "l": "c", "k": ["capability", "specific", "skill", "function", "system", "perform", "capabilities", "range", "basic", "text", "generation", "advanced", "multi-step", "reasoning", "tool"]}, {"id": "term-capability-control", "t": "Capability Control", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "Safety measures that limit what an AI system can do by restricting its access to resources, communication channels, or...", "l": "c", "k": ["capability", "control", "safety", "measures", "limit", "system", "restricting", "access", "resources", "communication", "channels", "actuators", "opposed", "motivational", "shapes"]}, {"id": "term-capsule-network", "t": "Capsule Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A neural network architecture that uses groups of neurons (capsules) to encode both the presence and instantiation...", "l": "c", "k": ["capsule", "network", "neural", "architecture", "uses", "groups", "neurons", "capsules", "encode", "presence", "instantiation", "parameters", "features", "dynamic", "routing"]}, {"id": "term-capsule-networks", "t": "Capsule Networks", "tg": ["History", "Systems"], "d": "history", "x": "A neural network architecture proposed by Geoffrey Hinton and colleagues (Sabour et al. 2017) that uses groups of...", "l": "c", "k": ["capsule", "networks", "neural", "network", "architecture", "proposed", "geoffrey", "hinton", "colleagues", "sabour", "uses", "groups", "neurons", "capsules", "represent"]}, {"id": "term-captioning", "t": "Captioning", "tg": ["Task", "Multimodal"], "d": "general", "x": "Generating text descriptions of images or videos. A multimodal task requiring visual understanding and language...", "l": "c", "k": ["captioning", "generating", "text", "descriptions", "images", "videos", "multimodal", "task", "requiring", "visual", "understanding", "language", "generation", "accessibility", "content"]}, {"id": "term-carnegie-mellon-ai", "t": "Carnegie Mellon AI", "tg": ["History", "Organizations"], "d": "history", "x": "AI research programs at Carnegie Mellon University including the work of Herbert Simon Allen Newell and Raj Reddy. Home...", "l": "c", "k": ["carnegie", "mellon", "research", "programs", "university", "including", "work", "herbert", "simon", "allen", "newell", "raj", "reddy", "home", "robotics"]}, {"id": "term-case-based-reasoning", "t": "Case-Based Reasoning", "tg": ["History", "Fundamentals"], "d": "history", "x": "An AI methodology that solves new problems by adapting solutions from similar past cases. Developed by Roger Schank and...", "l": "c", "k": ["case-based", "reasoning", "methodology", "solves", "problems", "adapting", "solutions", "similar", "past", "cases", "developed", "roger", "schank", "others", "1980s"]}, {"id": "term-cataphora", "t": "Cataphora", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A linguistic phenomenon where a referring expression precedes the entity it refers to in the text, as in 'Before he...", "l": "c", "k": ["cataphora", "linguistic", "phenomenon", "referring", "expression", "precedes", "entity", "refers", "text", "arrived", "john", "called", "ahead", "posing", "challenges"]}, {"id": "term-catastrophic-forgetting", "t": "Catastrophic Forgetting", "tg": ["Training", "Challenge"], "d": "general", "x": "The tendency of neural networks to forget previously learned information when trained on new data. A significant...", "l": "c", "k": ["catastrophic", "forgetting", "tendency", "neural", "networks", "forget", "previously", "learned", "information", "trained", "data", "significant", "challenge", "continual", "learning"]}, {"id": "term-catastrophic-forgetting-ethics", "t": "Catastrophic Forgetting Ethics", "tg": ["AI Safety", "AI Ethics"], "d": "safety", "x": "Ethical implications of the tendency of neural networks to forget previously learned safety constraints when trained on...", "l": "c", "k": ["catastrophic", "forgetting", "ethics", "ethical", "implications", "tendency", "neural", "networks", "forget", "previously", "learned", "safety", "constraints", "trained", "data"]}, {"id": "term-catastrophic-forgetting-rl", "t": "Catastrophic Forgetting in RL", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "The tendency of neural network-based RL agents to lose previously learned skills when adapting to new tasks or...", "l": "c", "k": ["catastrophic", "forgetting", "tendency", "neural", "network-based", "agents", "lose", "previously", "learned", "skills", "adapting", "tasks", "environments", "continual", "methods"]}, {"id": "term-catastrophic-risk-from-ai", "t": "Catastrophic Risk from AI", "tg": ["AI Safety", "AI Ethics"], "d": "safety", "x": "The risk that AI systems could cause large-scale irreversible harm falling short of existential risk, such as...", "l": "c", "k": ["catastrophic", "risk", "systems", "cause", "large-scale", "irreversible", "harm", "falling", "short", "existential", "widespread", "economic", "collapse", "loss", "critical"]}, {"id": "term-catboost", "t": "CatBoost", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A gradient boosting library that natively handles categorical features using ordered target statistics and employs...", "l": "c", "k": ["catboost", "gradient", "boosting", "library", "natively", "handles", "categorical", "features", "ordered", "target", "statistics", "employs", "reduce", "prediction", "shift"]}, {"id": "term-causal-convolution", "t": "Causal Convolution", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A convolution that only uses current and past inputs ensuring the output at time t depends only on inputs at times t...", "l": "c", "k": ["causal", "convolution", "uses", "current", "past", "inputs", "ensuring", "output", "time", "depends", "times", "earlier", "essential", "autoregressive", "sequence"]}, {"id": "term-causal-language-model", "t": "Causal Language Model", "tg": ["Architecture", "LLM"], "d": "models", "x": "A model that predicts the next token based only on previous tokens (left-to-right). GPT and most text generation models...", "l": "c", "k": ["causal", "language", "model", "predicts", "next", "token", "based", "previous", "tokens", "left-to-right", "gpt", "text", "generation", "models", "contrast"]}, {"id": "term-causal-language-modeling", "t": "Causal Language Modeling", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A training objective where the model predicts each token based only on the preceding tokens in the sequence, enforcing...", "l": "c", "k": ["causal", "language", "modeling", "training", "objective", "model", "predicts", "token", "based", "preceding", "tokens", "sequence", "enforcing", "left-to-right", "autoregressive"]}, {"id": "term-causal-mask", "t": "Causal Mask", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A triangular attention mask that prevents each position from attending to subsequent positions, enforcing the...", "l": "c", "k": ["causal", "mask", "triangular", "attention", "prevents", "position", "attending", "subsequent", "positions", "enforcing", "autoregressive", "property", "required", "left-to-right", "language"]}, {"id": "term-causal-masking", "t": "Causal Masking", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A specific attention mask pattern that prevents each position from attending to future positions ensuring...", "l": "c", "k": ["causal", "masking", "specific", "attention", "mask", "pattern", "prevents", "position", "attending", "future", "positions", "ensuring", "autoregressive", "behavior", "implemented"]}, {"id": "term-cbam", "t": "CBAM", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Convolutional Block Attention Module, a lightweight attention module that sequentially applies channel and spatial...", "l": "c", "k": ["cbam", "convolutional", "block", "attention", "module", "lightweight", "sequentially", "applies", "channel", "spatial", "feature", "maps", "enhancing", "representational", "power"]}, {"id": "term-cbow", "t": "CBOW", "tg": ["NLP", "Embeddings"], "d": "general", "x": "Continuous Bag of Words, a Word2Vec training objective that predicts a center word from the average of its surrounding...", "l": "c", "k": ["cbow", "continuous", "bag", "words", "word2vec", "training", "objective", "predicts", "center", "word", "average", "surrounding", "context", "vectors", "typically"]}, {"id": "term-ceiling-effect", "t": "Ceiling Effect", "tg": ["Evaluation", "Benchmark"], "d": "datasets", "x": "When a benchmark becomes too easy to distinguish between models, all scoring near the maximum. Prompts creation of...", "l": "c", "k": ["ceiling", "effect", "benchmark", "becomes", "easy", "distinguish", "models", "scoring", "near", "maximum", "prompts", "creation", "harder", "benchmarks", "continue"]}, {"id": "term-cellular-automata", "t": "Cellular Automata", "tg": ["History", "Fundamentals"], "d": "history", "x": "Discrete models of computation consisting of a grid of cells each in a finite number of states that evolve over time...", "l": "c", "k": ["cellular", "automata", "discrete", "models", "computation", "consisting", "grid", "cells", "finite", "number", "states", "evolve", "time", "according", "simple"]}, {"id": "term-censoring", "t": "Censoring", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A condition in survival analysis where the exact time of an event is not observed for some subjects, typically because...", "l": "c", "k": ["censoring", "condition", "survival", "analysis", "exact", "time", "event", "observed", "subjects", "typically", "study", "ended", "subject", "lost", "follow-up"]}, {"id": "term-center-for-ai-safety", "t": "Center for AI Safety", "tg": ["History", "Organizations"], "d": "history", "x": "A nonprofit research and field-building organization founded in 2022 focused on reducing societal-scale risks from AI....", "l": "c", "k": ["center", "safety", "nonprofit", "research", "field-building", "organization", "founded", "focused", "reducing", "societal-scale", "risks", "cais", "published", "statement", "risk"]}, {"id": "term-centernet", "t": "CenterNet", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "An anchor-free object detection approach that represents objects as center points with associated size and offset...", "l": "c", "k": ["centernet", "anchor-free", "object", "detection", "approach", "represents", "objects", "center", "points", "associated", "size", "offset", "predictions", "simplifying", "pipeline"]}, {"id": "term-central-limit-theorem", "t": "Central Limit Theorem", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A fundamental theorem stating that the sampling distribution of the sample mean approaches a normal distribution as the...", "l": "c", "k": ["central", "limit", "theorem", "fundamental", "stating", "sampling", "distribution", "sample", "mean", "approaches", "normal", "size", "increases", "regardless", "population"]}, {"id": "term-ctde", "t": "Centralized Training Decentralized Execution (CTDE)", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A multi-agent RL paradigm where agents have access to global information during training but must act independently...", "l": "c", "k": ["centralized", "training", "decentralized", "execution", "ctde", "multi-agent", "paradigm", "agents", "access", "global", "information", "must", "act", "independently", "based"]}, {"id": "term-centroid-based-clustering-vectors", "t": "Centroid-Based Clustering for Vectors", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "The use of clustering algorithms like k-means to partition a vector collection into groups represented by centroid...", "l": "c", "k": ["centroid-based", "clustering", "vectors", "algorithms", "k-means", "partition", "vector", "collection", "groups", "represented", "centroid", "forming", "basis", "ivf", "indexes"]}, {"id": "term-cerebras", "t": "Cerebras", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "A semiconductor company that produces wafer-scale AI processors (WSE series) with millions of cores and terabytes of...", "l": "c", "k": ["cerebras", "semiconductor", "company", "produces", "wafer-scale", "processors", "wse", "series", "millions", "cores", "terabytes", "on-chip", "sram", "systems", "eliminate"]}, {"id": "term-cerebras-systems", "t": "Cerebras Systems", "tg": ["History", "Organizations"], "d": "history", "x": "An AI hardware company founded in 2016 that developed the Wafer Scale Engine the largest chip ever built. The CS-2...", "l": "c", "k": ["cerebras", "systems", "hardware", "company", "founded", "developed", "wafer", "scale", "engine", "largest", "chip", "ever", "built", "cs-2", "system"]}, {"id": "term-cerebras-gpt", "t": "Cerebras-GPT", "tg": ["Models", "Technical"], "d": "models", "x": "A family of language models trained by Cerebras Systems following Chinchilla-optimal scaling laws. Released with...", "l": "c", "k": ["cerebras-gpt", "family", "language", "models", "trained", "cerebras", "systems", "following", "chinchilla-optimal", "scaling", "laws", "released", "training", "recipes", "full"]}, {"id": "term-chain-of-density", "t": "Chain of Density", "tg": ["Prompt Engineering", "Summarization"], "d": "general", "x": "A prompting technique that iteratively increases the information density of a summary by asking the model to rewrite it...", "l": "c", "k": ["chain", "density", "prompting", "technique", "iteratively", "increases", "information", "summary", "asking", "model", "rewrite", "additional", "entities", "maintaining", "length"]}, {"id": "term-chain-of-code", "t": "Chain-of-Code", "tg": ["Prompt Engineering", "Code-Augmented"], "d": "general", "x": "A reasoning framework that augments chain-of-thought with executable code generation, allowing the model to write and...", "l": "c", "k": ["chain-of-code", "reasoning", "framework", "augments", "chain-of-thought", "executable", "code", "generation", "allowing", "model", "write", "simulate", "execution", "steps", "benefit"]}, {"id": "term-chain-of-knowledge", "t": "Chain-of-Knowledge", "tg": ["Prompt Engineering", "Knowledge Augmentation"], "d": "general", "x": "A prompting framework that progressively builds and refines a knowledge chain by eliciting relevant facts, verifying...", "l": "c", "k": ["chain-of-knowledge", "prompting", "framework", "progressively", "builds", "refines", "knowledge", "chain", "eliciting", "relevant", "facts", "verifying", "consistency", "reasoning", "accumulated"]}, {"id": "term-chain-of-table", "t": "Chain-of-Table", "tg": ["Prompt Engineering", "Tabular Reasoning"], "d": "general", "x": "A reasoning framework for tabular data that iteratively transforms tables through operations like filtering, sorting,...", "l": "c", "k": ["chain-of-table", "reasoning", "framework", "tabular", "data", "iteratively", "transforms", "tables", "operations", "filtering", "sorting", "aggregation", "intermediate", "steps", "step"]}, {"id": "term-chain-of-thought", "t": "Chain-of-Thought (CoT)", "tg": ["Prompting", "Reasoning"], "d": "general", "x": "A prompting technique that encourages AI to show its reasoning process step-by-step, leading to more accurate and...", "l": "c", "k": ["chain-of-thought", "cot", "prompting", "technique", "encourages", "show", "reasoning", "process", "step-by-step", "leading", "accurate", "transparent", "responses", "complex", "problems"]}, {"id": "term-chain-of-thought-discovery", "t": "Chain-of-Thought Discovery", "tg": ["History", "Milestones"], "d": "history", "x": "The discovery that prompting large language models to think step by step dramatically improves their reasoning...", "l": "c", "k": ["chain-of-thought", "discovery", "prompting", "large", "language", "models", "think", "step", "dramatically", "improves", "reasoning", "performance", "introduced", "jason", "wei"]}, {"id": "term-chain-of-thought-prompting", "t": "Chain-of-Thought Prompting", "tg": ["Algorithms", "Fundamentals", "Prompting"], "d": "algorithms", "x": "A prompting technique that elicits step-by-step reasoning from language models by including intermediate reasoning...", "l": "c", "k": ["chain-of-thought", "prompting", "technique", "elicits", "step-by-step", "reasoning", "language", "models", "including", "intermediate", "steps", "prompt", "dramatically", "improves", "performance"]}, {"id": "term-cot-self-consistency", "t": "Chain-of-Thought with Self-Consistency", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "The combined technique of generating multiple chain-of-thought reasoning paths for a single problem using sampling and...", "l": "c", "k": ["chain-of-thought", "self-consistency", "combined", "technique", "generating", "multiple", "reasoning", "paths", "single", "problem", "sampling", "selecting", "frequently", "occurring", "final"]}, {"id": "term-channel-attention", "t": "Channel Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that learns to weight the importance of different feature channels in a CNN, selectively...", "l": "c", "k": ["channel", "attention", "mechanism", "learns", "weight", "importance", "different", "feature", "channels", "cnn", "selectively", "emphasizing", "informative", "suppressing", "less"]}, {"id": "term-character-error-rate", "t": "Character Error Rate", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A fine-grained evaluation metric that computes the edit distance between predicted and reference texts at the character...", "l": "c", "k": ["character", "error", "rate", "fine-grained", "evaluation", "metric", "computes", "edit", "distance", "predicted", "reference", "texts", "level", "useful", "evaluating"]}, {"id": "term-character-n-gram", "t": "Character N-gram", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A contiguous sequence of N characters extracted from a word or text, used as features for text classification, language...", "l": "c", "k": ["character", "n-gram", "contiguous", "sequence", "characters", "extracted", "word", "text", "features", "classification", "language", "identification", "spelling", "correction", "tasks"]}, {"id": "term-character-level", "t": "Character-Level Model", "tg": ["Architecture", "Alternative"], "d": "models", "x": "Models that process text character by character rather than using tokens. More flexible with novel words but typically...", "l": "c", "k": ["character-level", "model", "models", "process", "text", "character", "rather", "tokens", "flexible", "novel", "words", "typically", "slower", "requiring", "parameters"]}, {"id": "term-charles-babbage", "t": "Charles Babbage", "tg": ["History", "Pioneers"], "d": "history", "x": "English mathematician and inventor (1791-1871) who conceived the Difference Engine and the Analytical Engine,...", "l": "c", "k": ["charles", "babbage", "english", "mathematician", "inventor", "1791-1871", "conceived", "difference", "engine", "analytical", "mechanical", "general-purpose", "computers", "anticipated", "key"]}, {"id": "term-chat-completion", "t": "Chat Completion", "tg": ["API", "Technical"], "d": "general", "x": "An API endpoint type where the model generates responses in a conversational format. Takes a list of messages (system,...", "l": "c", "k": ["chat", "completion", "api", "endpoint", "type", "model", "generates", "responses", "conversational", "format", "takes", "list", "messages", "system", "user"]}, {"id": "term-chat-template", "t": "Chat Template", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A structured formatting convention that defines how system messages, user inputs, and assistant responses are tokenized...", "l": "c", "k": ["chat", "template", "structured", "formatting", "convention", "defines", "system", "messages", "user", "inputs", "assistant", "responses", "tokenized", "delimited", "multi-turn"]}, {"id": "term-chatglm", "t": "ChatGLM", "tg": ["Models", "Technical"], "d": "models", "x": "A family of bilingual language models based on the General Language Model architecture. Developed by Tsinghua...", "l": "c", "k": ["chatglm", "family", "bilingual", "language", "models", "based", "general", "model", "architecture", "developed", "tsinghua", "university", "zhipu", "features", "efficient"]}, {"id": "term-chatgpt", "t": "ChatGPT", "tg": ["Product", "OpenAI"], "d": "general", "x": "OpenAI's conversational AI product launched in November 2022. Built on GPT models fine-tuned for dialogue, it...", "l": "c", "k": ["chatgpt", "openai", "conversational", "product", "launched", "november", "built", "gpt", "models", "fine-tuned", "dialogue", "popularized", "sparked", "widespread", "public"]}, {"id": "term-chatgpt-launch", "t": "ChatGPT Launch", "tg": ["History", "Milestones"], "d": "history", "x": "OpenAI's release of ChatGPT on November 30, 2022, a conversational AI interface built on GPT-3.5 that reached 100...", "l": "c", "k": ["chatgpt", "launch", "openai", "release", "november", "conversational", "interface", "built", "gpt-3", "reached", "million", "users", "months", "triggering", "widespread"]}, {"id": "term-checkpoint", "t": "Checkpoint", "tg": ["Training", "Technical"], "d": "general", "x": "A saved snapshot of model weights during training. Enables resuming training after interruption, comparing different...", "l": "c", "k": ["checkpoint", "saved", "snapshot", "model", "weights", "training", "enables", "resuming", "interruption", "comparing", "different", "stages", "selecting", "best", "performing"]}, {"id": "term-checkpointing-training", "t": "Checkpointing for Training", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "The practice of periodically saving model weights, optimizer state, and training metadata to persistent storage during...", "l": "c", "k": ["checkpointing", "training", "practice", "periodically", "saving", "model", "weights", "optimizer", "state", "metadata", "persistent", "storage", "enables", "recovery", "hardware"]}, {"id": "term-chi-square-distribution", "t": "Chi-Square Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "The distribution of the sum of squares of k independent standard normal random variables. It is used in chi-square...", "l": "c", "k": ["chi-square", "distribution", "sum", "squares", "independent", "standard", "normal", "random", "variables", "tests", "confidence", "interval", "estimation", "variance", "goodness-of-fit"]}, {"id": "term-chi-square-test", "t": "Chi-Square Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A statistical test that evaluates whether observed frequencies differ significantly from expected frequencies under a...", "l": "c", "k": ["chi-square", "test", "statistical", "evaluates", "observed", "frequencies", "differ", "significantly", "expected", "null", "hypothesis", "testing", "independence", "categorical", "variables"]}, {"id": "term-chinchilla", "t": "Chinchilla", "tg": ["Research", "Scaling"], "d": "general", "x": "A DeepMind model and scaling study showing optimal training requires more data than previously thought. Influenced...", "l": "c", "k": ["chinchilla", "deepmind", "model", "scaling", "study", "showing", "optimal", "training", "requires", "data", "previously", "thought", "influenced", "subsequent", "development"]}, {"id": "term-chinchilla-optimal", "t": "Chinchilla Optimal", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A training regime derived from DeepMind's Chinchilla scaling laws, suggesting that for a given compute budget, model...", "l": "c", "k": ["chinchilla", "optimal", "training", "regime", "derived", "deepmind", "scaling", "laws", "suggesting", "given", "compute", "budget", "model", "size", "data"]}, {"id": "term-chinchilla-paper", "t": "Chinchilla Paper", "tg": ["History", "Milestones"], "d": "history", "x": "The 2022 DeepMind paper by Hoffmann et al. demonstrating that many large language models were undertrained relative to...", "l": "c", "k": ["chinchilla", "paper", "deepmind", "hoffmann", "demonstrating", "large", "language", "models", "were", "undertrained", "relative", "size", "establishing", "scaling", "laws"]}, {"id": "term-chinchilla-scaling-laws", "t": "Chinchilla Scaling Laws", "tg": ["History", "Milestones"], "d": "history", "x": "Findings published by DeepMind in 2022 (Hoffmann et al.) showing that many large language models were significantly...", "l": "c", "k": ["chinchilla", "scaling", "laws", "findings", "published", "deepmind", "hoffmann", "showing", "large", "language", "models", "were", "significantly", "undertrained", "paper"]}, {"id": "term-chinese-room-argument", "t": "Chinese Room Argument", "tg": ["History", "Milestones"], "d": "history", "x": "A thought experiment by John Searle in 1980 arguing that a computer executing a program cannot have genuine...", "l": "c", "k": ["chinese", "room", "argument", "thought", "experiment", "john", "searle", "arguing", "computer", "executing", "program", "cannot", "genuine", "understanding", "consciousness"]}, {"id": "term-chinook", "t": "Chinook", "tg": ["History", "Systems"], "d": "history", "x": "A computer checkers program developed by Jonathan Schaeffer at the University of Alberta that won the World Checkers...", "l": "c", "k": ["chinook", "computer", "checkers", "program", "developed", "jonathan", "schaeffer", "university", "alberta", "won", "world", "championship", "team", "proved", "perfect"]}, {"id": "term-chiplet-architecture", "t": "Chiplet Architecture", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "A processor design approach using multiple small silicon dies (chiplets) connected via high-speed interconnects on a...", "l": "c", "k": ["chiplet", "architecture", "processor", "design", "approach", "multiple", "small", "silicon", "dies", "chiplets", "connected", "via", "high-speed", "interconnects", "single"]}, {"id": "term-christopher-watkins", "t": "Christopher Watkins", "tg": ["History", "Pioneers"], "d": "history", "x": "British computer scientist who introduced Q-learning in his 1989 PhD thesis at Cambridge University. Q-learning is a...", "l": "c", "k": ["christopher", "watkins", "british", "computer", "scientist", "introduced", "q-learning", "phd", "thesis", "cambridge", "university", "model-free", "reinforcement", "learning", "algorithm"]}, {"id": "term-chromadb", "t": "ChromaDB", "tg": ["Vector Database", "Open Source"], "d": "general", "x": "An open-source embedding database designed for AI applications that provides a simple API for storing, querying, and...", "l": "c", "k": ["chromadb", "open-source", "embedding", "database", "designed", "applications", "provides", "simple", "api", "storing", "querying", "filtering", "embeddings", "associated", "metadata"]}, {"id": "term-chunk-overlap", "t": "Chunk Overlap", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "The number of tokens or characters shared between consecutive chunks during document splitting, ensuring that...", "l": "c", "k": ["chunk", "overlap", "number", "tokens", "characters", "shared", "consecutive", "chunks", "document", "splitting", "ensuring", "information", "spanning", "boundaries", "lost"]}, {"id": "term-chunk-size", "t": "Chunk Size", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "The target length of individual text segments produced during document chunking, typically measured in tokens or...", "l": "c", "k": ["chunk", "size", "target", "length", "individual", "text", "segments", "produced", "document", "chunking", "typically", "measured", "tokens", "characters", "smaller"]}, {"id": "term-chunked-prefill", "t": "Chunked Prefill", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An inference optimization that breaks the prompt processing phase into smaller chunks to be interleaved with decoding...", "l": "c", "k": ["chunked", "prefill", "inference", "optimization", "breaks", "prompt", "processing", "phase", "smaller", "chunks", "interleaved", "decoding", "steps", "reduces", "time-to-first-token"]}, {"id": "term-chunking", "t": "Chunking", "tg": ["Technique", "Processing"], "d": "general", "x": "Splitting long documents into smaller pieces for processing. Essential for RAG and embedding systems where input length...", "l": "c", "k": ["chunking", "splitting", "long", "documents", "smaller", "pieces", "processing", "essential", "rag", "embedding", "systems", "input", "length", "exceeds", "model"]}, {"id": "term-church-turing-thesis", "t": "Church-Turing Thesis", "tg": ["History", "Fundamentals"], "d": "history", "x": "The hypothesis independently proposed by Alonzo Church and Alan Turing in 1936 that any function computable by an...", "l": "c", "k": ["church-turing", "thesis", "hypothesis", "independently", "proposed", "alonzo", "church", "alan", "turing", "function", "computable", "effective", "procedure", "computed", "machine"]}, {"id": "term-cider", "t": "CIDEr", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "Consensus-based Image Description Evaluation, a metric that measures image captioning quality using TF-IDF weighted...", "l": "c", "k": ["cider", "consensus-based", "image", "description", "evaluation", "metric", "measures", "captioning", "quality", "tf-idf", "weighted", "n-gram", "similarity", "generated", "reference"]}, {"id": "term-cifar-organization", "t": "CIFAR (Organization)", "tg": ["History", "Organizations"], "d": "history", "x": "The Canadian Institute for Advanced Research a nonprofit research organization that has been instrumental in supporting...", "l": "c", "k": ["cifar", "organization", "canadian", "institute", "advanced", "research", "nonprofit", "instrumental", "supporting", "learning", "machines", "brains", "program", "provided", "crucial"]}, {"id": "term-cifar-10", "t": "CIFAR-10", "tg": ["History", "Milestones"], "d": "history", "x": "A dataset of 60000 32x32 color images in 10 classes collected by Alex Krizhevsky and Geoffrey Hinton in 2009. Along...", "l": "c", "k": ["cifar-10", "dataset", "32x32", "color", "images", "classes", "collected", "alex", "krizhevsky", "geoffrey", "hinton", "along", "cifar-100", "became", "standard"]}, {"id": "term-citation-generation", "t": "Citation Generation", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The capability of a language model to produce inline references to source documents that support its claims, enabling...", "l": "c", "k": ["citation", "generation", "capability", "language", "model", "produce", "inline", "references", "source", "documents", "support", "claims", "enabling", "users", "verify"]}, {"id": "term-cky-algorithm", "t": "CKY Algorithm", "tg": ["NLP", "Parsing"], "d": "general", "x": "Cocke-Kasami-Younger algorithm, a dynamic programming parser for context-free grammars that builds parse trees...", "l": "c", "k": ["cky", "algorithm", "cocke-kasami-younger", "dynamic", "programming", "parser", "context-free", "grammars", "builds", "parse", "trees", "bottom-up", "time", "filling", "chart"]}, {"id": "term-clarity", "t": "Clarity (Prompting)", "tg": ["Prompting", "Best Practice"], "d": "general", "x": "Using clear, unambiguous language in prompts to reduce misinterpretation. Specific instructions and explicit...", "l": "c", "k": ["clarity", "prompting", "clear", "unambiguous", "language", "prompts", "reduce", "misinterpretation", "specific", "instructions", "explicit", "requirements", "improve", "response", "quality"]}, {"id": "term-class-activation-map", "t": "Class Activation Map", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A visualization technique that highlights the image regions most important for a CNN's classification decision,...", "l": "c", "k": ["class", "activation", "map", "visualization", "technique", "highlights", "image", "regions", "important", "cnn", "classification", "decision", "computed", "weighting", "feature"]}, {"id": "term-class-imbalance", "t": "Class Imbalance", "tg": ["Data", "Challenge"], "d": "general", "x": "When training data has unequal representation across categories. Can cause models to favor majority classes. Addressed...", "l": "c", "k": ["class", "imbalance", "training", "data", "unequal", "representation", "across", "categories", "cause", "models", "favor", "majority", "classes", "addressed", "sampling"]}, {"id": "term-class-weight", "t": "Class Weight", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A technique for handling class imbalance by assigning higher weight to the minority class in the loss function,...", "l": "c", "k": ["class", "weight", "technique", "handling", "imbalance", "assigning", "higher", "minority", "loss", "function", "effectively", "making", "misclassification", "underrepresented", "classes"]}, {"id": "term-classification", "t": "Classification", "tg": ["ML Task", "Supervised"], "d": "general", "x": "A machine learning task that assigns input data to predefined categories. Examples include spam detection (spam/not...", "l": "c", "k": ["classification", "machine", "learning", "task", "assigns", "input", "data", "predefined", "categories", "examples", "include", "spam", "detection", "sentiment", "analysis"]}, {"id": "term-classifier-free-guidance", "t": "Classifier-Free Guidance", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A technique for conditional diffusion models that interpolates between conditional and unconditional score estimates...", "l": "c", "k": ["classifier-free", "guidance", "technique", "conditional", "diffusion", "models", "interpolates", "unconditional", "score", "estimates", "sampling", "controlling", "trade-off", "sample", "quality"]}, {"id": "term-claude", "t": "Claude", "tg": ["Product", "Anthropic"], "d": "general", "x": "An AI assistant created by Anthropic, designed to be helpful, harmless, and honest. Known for nuanced reasoning, long...", "l": "c", "k": ["claude", "assistant", "created", "anthropic", "designed", "helpful", "harmless", "honest", "known", "nuanced", "reasoning", "long", "context", "handling", "strong"]}, {"id": "term-claude-ai", "t": "Claude (AI)", "tg": ["History", "Systems"], "d": "history", "x": "A family of AI assistants developed by Anthropic beginning with Claude 1.0 in March 2023. Built using Constitutional AI...", "l": "c", "k": ["claude", "family", "assistants", "developed", "anthropic", "beginning", "march", "built", "constitutional", "methods", "reinforcement", "learning", "human", "feedback", "known"]}, {"id": "term-claude-haiku", "t": "Claude Haiku", "tg": ["Models", "Technical"], "d": "models", "x": "The fastest and most compact model in the Claude family optimized for quick responses and high throughput. Suitable for...", "l": "c", "k": ["claude", "haiku", "fastest", "compact", "model", "family", "optimized", "quick", "responses", "high", "throughput", "suitable", "tasks", "requiring", "speed"]}, {"id": "term-claude-instant", "t": "Claude Instant / Haiku", "tg": ["Model", "Anthropic"], "d": "models", "x": "Anthropic's faster, more cost-effective models for simpler tasks. Trade some capability for speed and lower cost,...", "l": "c", "k": ["claude", "instant", "haiku", "anthropic", "faster", "cost-effective", "models", "simpler", "tasks", "trade", "capability", "speed", "lower", "cost", "suitable"]}, {"id": "term-claude-launch", "t": "Claude Launch", "tg": ["History", "Milestones"], "d": "history", "x": "Anthropic's release of Claude, a family of AI assistants trained using Constitutional AI methods, first made available...", "l": "c", "k": ["claude", "launch", "anthropic", "release", "family", "assistants", "trained", "constitutional", "methods", "available", "march", "emphasizing", "safety", "helpfulness", "harmlessness"]}, {"id": "term-claude-opus", "t": "Claude Opus", "tg": ["Model", "Anthropic"], "d": "models", "x": "Anthropic's most capable model, designed for complex reasoning, creative tasks, and nuanced understanding. Higher cost...", "l": "c", "k": ["claude", "opus", "anthropic", "capable", "model", "designed", "complex", "reasoning", "creative", "tasks", "nuanced", "understanding", "higher", "cost", "best"]}, {"id": "term-claude-shannon", "t": "Claude Shannon", "tg": ["History", "Pioneers"], "d": "history", "x": "American mathematician (1916-2001) known as the father of information theory, whose 1948 paper established the...", "l": "c", "k": ["claude", "shannon", "american", "mathematician", "1916-2001", "known", "father", "information", "theory", "whose", "paper", "established", "mathematical", "foundations", "digital"]}, {"id": "term-claude-sonnet", "t": "Claude Sonnet", "tg": ["Models", "Technical"], "d": "models", "x": "A balanced model in the Claude family offering strong performance with good efficiency. Suitable for a wide range of...", "l": "c", "k": ["claude", "sonnet", "balanced", "model", "family", "offering", "strong", "performance", "good", "efficiency", "suitable", "wide", "range", "tasks", "including"]}, {"id": "term-clever-hans-effect", "t": "Clever Hans Effect", "tg": ["History", "Fundamentals"], "d": "history", "x": "Named after a horse that appeared to perform arithmetic but was actually reading its trainer's body language. In AI it...", "l": "c", "k": ["clever", "hans", "effect", "named", "horse", "appeared", "perform", "arithmetic", "actually", "reading", "trainer", "body", "language", "refers", "models"]}, {"id": "term-cliff-shaw", "t": "Cliff Shaw", "tg": ["History", "Pioneers"], "d": "history", "x": "American programmer at RAND Corporation who along with Allen Newell and Herbert Simon developed the Logic Theorist...", "l": "c", "k": ["cliff", "shaw", "american", "programmer", "rand", "corporation", "along", "allen", "newell", "herbert", "simon", "developed", "logic", "theorist", "general"]}, {"id": "term-clip", "t": "CLIP", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Contrastive Language-Image Pre-training, a model that learns visual concepts from natural language supervision by...", "l": "c", "k": ["clip", "contrastive", "language-image", "pre-training", "model", "learns", "visual", "concepts", "natural", "language", "supervision", "training", "image", "text", "encoders"]}, {"id": "term-clipped-surrogate-objective", "t": "Clipped Surrogate Objective", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "The core optimization objective in PPO that clips the probability ratio between new and old policies, preventing...", "l": "c", "k": ["clipped", "surrogate", "objective", "core", "optimization", "ppo", "clips", "probability", "ratio", "old", "policies", "preventing", "excessively", "large", "updates"]}, {"id": "term-clips", "t": "CLIPS", "tg": ["History", "Systems"], "d": "history", "x": "The C Language Integrated Production System developed by NASA's Johnson Space Center in 1985. An expert system shell...", "l": "c", "k": ["clips", "language", "integrated", "production", "system", "developed", "nasa", "johnson", "space", "center", "expert", "shell", "designed", "building", "rule-based"]}, {"id": "term-cloud-computing-ai", "t": "Cloud Computing for AI", "tg": ["Distributed Computing", "Inference Infrastructure"], "d": "hardware", "x": "The use of cloud infrastructure services (AWS, GCP, Azure) for AI model training and inference, providing on-demand...", "l": "c", "k": ["cloud", "computing", "infrastructure", "services", "aws", "gcp", "azure", "model", "training", "inference", "providing", "on-demand", "access", "gpu", "clusters"]}, {"id": "term-cloze", "t": "Cloze Task", "tg": ["Task", "Evaluation"], "d": "datasets", "x": "A task where models predict missing words in text. A classic NLP benchmark and training objective. BERT's masked...", "l": "c", "k": ["cloze", "task", "models", "predict", "missing", "words", "text", "classic", "nlp", "benchmark", "training", "objective", "bert", "masked", "language"]}, {"id": "term-clustering", "t": "Clustering", "tg": ["ML Task", "Unsupervised"], "d": "general", "x": "An unsupervised learning technique that groups similar data points together without predefined labels. Used for...", "l": "c", "k": ["clustering", "unsupervised", "learning", "technique", "groups", "similar", "data", "points", "together", "without", "predefined", "labels", "customer", "segmentation", "document"]}, {"id": "term-cma-es", "t": "CMA-ES", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Covariance Matrix Adaptation Evolution Strategy is a derivative-free optimization algorithm that adapts its search...", "l": "c", "k": ["cma-es", "covariance", "matrix", "adaptation", "evolution", "strategy", "derivative-free", "optimization", "algorithm", "adapts", "search", "distribution", "updating", "multivariate", "normal"]}, {"id": "term-cmu-ai-research", "t": "CMU AI Research", "tg": ["History", "Milestones"], "d": "history", "x": "Carnegie Mellon University's AI research programs, including the work of Allen Newell and Herbert Simon, the...", "l": "c", "k": ["cmu", "research", "carnegie", "mellon", "university", "programs", "including", "work", "allen", "newell", "herbert", "simon", "development", "expert", "systems"]}, {"id": "term-cnn", "t": "CNN (Convolutional Neural Network)", "tg": ["Architecture", "Computer Vision"], "d": "models", "x": "A neural network architecture designed for processing grid-like data such as images. Uses convolutional layers to...", "l": "c", "k": ["cnn", "convolutional", "neural", "network", "architecture", "designed", "processing", "grid-like", "data", "images", "uses", "layers", "automatically", "learn", "spatial"]}, {"id": "term-co-occurrence-matrix", "t": "Co-occurrence Matrix", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A matrix recording how often pairs of words appear together within a defined context window across a corpus, used as...", "l": "c", "k": ["co-occurrence", "matrix", "recording", "pairs", "words", "appear", "together", "within", "defined", "context", "window", "across", "corpus", "basis", "distributional"]}, {"id": "term-co-training", "t": "Co-Training", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A semi-supervised learning method that trains two classifiers on different views or feature sets of the data. Each...", "l": "c", "k": ["co-training", "semi-supervised", "learning", "method", "trains", "classifiers", "different", "views", "feature", "sets", "data", "classifier", "labels", "unlabeled", "examples"]}, {"id": "term-coco-dataset", "t": "COCO Dataset", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Common Objects in Context, a large-scale benchmark dataset containing images with annotations for object detection,...", "l": "c", "k": ["coco", "dataset", "common", "objects", "context", "large-scale", "benchmark", "containing", "images", "annotations", "object", "detection", "instance", "segmentation", "keypoint"]}, {"id": "term-code-generation", "t": "Code Generation", "tg": ["Application", "Development"], "d": "general", "x": "The ability of AI models to write programming code from natural language descriptions. Powers tools like GitHub...", "l": "c", "k": ["code", "generation", "ability", "models", "write", "programming", "natural", "language", "descriptions", "powers", "tools", "github", "copilot", "cursor", "code-focused"]}, {"id": "term-code-generation-prompting", "t": "Code Generation Prompting", "tg": ["Prompt Engineering", "Code"], "d": "general", "x": "Specialized prompting techniques for producing high-quality code, incorporating language specification, function...", "l": "c", "k": ["code", "generation", "prompting", "specialized", "techniques", "producing", "high-quality", "incorporating", "language", "specification", "function", "signatures", "docstrings", "test", "cases"]}, {"id": "term-code-interpreter", "t": "Code Interpreter", "tg": ["Feature", "Tool Use"], "d": "general", "x": "AI capability to write and execute code, enabling data analysis, visualization, and computation. ChatGPT's code...", "l": "c", "k": ["code", "interpreter", "capability", "write", "execute", "enabling", "data", "analysis", "visualization", "computation", "chatgpt", "runs", "python", "sandbox", "environment"]}, {"id": "term-code-llama", "t": "Code Llama", "tg": ["Models", "Technical"], "d": "models", "x": "A specialized version of LLaMA fine-tuned for code generation and understanding. Available in base Python and...", "l": "c", "k": ["code", "llama", "specialized", "version", "fine-tuned", "generation", "understanding", "available", "base", "python", "instruction-tuned", "variants", "supports", "infilling", "long-context"]}, {"id": "term-code-llm", "t": "Code LLM", "tg": ["Model Type", "Specialized"], "d": "models", "x": "Language models specialized for programming tasks. Examples include Codex, StarCoder, and Code Llama. Often trained on...", "l": "c", "k": ["code", "llm", "language", "models", "specialized", "programming", "tasks", "examples", "include", "codex", "starcoder", "llama", "trained", "large", "corpora"]}, {"id": "term-code-switching", "t": "Code-Switching", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The phenomenon of alternating between two or more languages within a single conversation or utterance, posing...", "l": "c", "k": ["code-switching", "phenomenon", "alternating", "languages", "within", "single", "conversation", "utterance", "posing", "challenges", "nlp", "systems", "designed", "monolingual", "text"]}, {"id": "term-codebleu", "t": "CodeBLEU", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A code evaluation metric that extends BLEU with code-specific components including abstract syntax tree matching,...", "l": "c", "k": ["codebleu", "code", "evaluation", "metric", "extends", "bleu", "code-specific", "components", "including", "abstract", "syntax", "tree", "matching", "data-flow", "analysis"]}, {"id": "term-codegen", "t": "CodeGen", "tg": ["Models", "Technical"], "d": "models", "x": "A family of large language models for program synthesis that convert natural language descriptions into executable...", "l": "c", "k": ["codegen", "family", "large", "language", "models", "program", "synthesis", "convert", "natural", "descriptions", "executable", "code", "trained", "multi-turn", "enabling"]}, {"id": "term-codex", "t": "Codex", "tg": ["Models", "Technical"], "d": "models", "x": "An OpenAI model fine-tuned from GPT-3 on publicly available code from GitHub. Powers GitHub Copilot for code completion...", "l": "c", "k": ["codex", "openai", "model", "fine-tuned", "gpt-3", "publicly", "available", "code", "github", "powers", "copilot", "completion", "generation", "proficient", "python"]}, {"id": "term-cognitive-load", "t": "Cognitive Load (Prompting)", "tg": ["Prompting", "Best Practice"], "d": "general", "x": "The mental effort required to process complex prompts. Simpler, well-organized prompts often yield better results by...", "l": "c", "k": ["cognitive", "load", "prompting", "mental", "effort", "required", "process", "complex", "prompts", "simpler", "well-organized", "yield", "better", "results", "reducing"]}, {"id": "term-cogvlm", "t": "CogVLM", "tg": ["Models", "Technical"], "d": "models", "x": "A visual language model that integrates vision features deep into the language model through a visual expert module in...", "l": "c", "k": ["cogvlm", "visual", "language", "model", "integrates", "vision", "features", "deep", "expert", "module", "attention", "ffn", "layer", "achieves", "strong"]}, {"id": "term-cohens-kappa", "t": "Cohen's Kappa", "tg": ["Statistics", "Metrics"], "d": "datasets", "x": "A statistic measuring inter-rater agreement for categorical items that accounts for agreement occurring by chance....", "l": "c", "k": ["cohen", "kappa", "statistic", "measuring", "inter-rater", "agreement", "categorical", "items", "accounts", "occurring", "chance", "values", "range", "indicating", "perfect"]}, {"id": "term-cohere", "t": "Cohere", "tg": ["Company", "LLM Provider"], "d": "models", "x": "An enterprise AI company providing LLMs for text generation, embeddings, and search. Known for Command models and focus...", "l": "c", "k": ["cohere", "enterprise", "company", "providing", "llms", "text", "generation", "embeddings", "search", "known", "command", "models", "focus", "cases", "strong"]}, {"id": "term-cohere-command", "t": "Cohere Command", "tg": ["Models", "Technical"], "d": "models", "x": "A family of generative language models by Cohere designed for enterprise applications. Optimized for instruction...", "l": "c", "k": ["cohere", "command", "family", "generative", "language", "models", "designed", "enterprise", "applications", "optimized", "instruction", "following", "text", "generation", "tool"]}, {"id": "term-cohere-embed", "t": "Cohere Embed", "tg": ["Models", "Technical"], "d": "models", "x": "A family of embedding models by Cohere designed for search and retrieval applications. Supports over 100 languages with...", "l": "c", "k": ["cohere", "embed", "family", "embedding", "models", "designed", "search", "retrieval", "applications", "supports", "languages", "state-of-the-art", "performance", "multilingual", "benchmarks"]}, {"id": "term-coherence-modeling", "t": "Coherence Modeling", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The computational assessment of how well sentences in a text flow together logically and topically, evaluating whether...", "l": "c", "k": ["coherence", "modeling", "computational", "assessment", "sentences", "text", "flow", "together", "logically", "topically", "evaluating", "reads", "naturally", "maintains", "consistent"]}, {"id": "term-coherence-score", "t": "Coherence Score", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation metric that assesses the logical consistency and semantic flow of generated text, measuring whether ideas...", "l": "c", "k": ["coherence", "score", "evaluation", "metric", "assesses", "logical", "consistency", "semantic", "flow", "generated", "text", "measuring", "ideas", "connect", "naturally"]}, {"id": "term-cointegration", "t": "Cointegration", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A statistical property of two or more non-stationary time series that share a common stochastic trend, meaning a linear...", "l": "c", "k": ["cointegration", "statistical", "property", "non-stationary", "time", "series", "share", "common", "stochastic", "trend", "meaning", "linear", "combination", "stationary", "implies"]}, {"id": "term-colbert", "t": "ColBERT", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A late-interaction retrieval model that independently encodes queries and documents into per-token embeddings, then...", "l": "c", "k": ["colbert", "late-interaction", "retrieval", "model", "independently", "encodes", "queries", "documents", "per-token", "embeddings", "scores", "relevance", "efficient", "maxsim", "operations"]}, {"id": "term-cold-start", "t": "Cold Start Problem", "tg": ["Challenge", "Recommendations"], "d": "general", "x": "Difficulty making predictions for new users or items with no historical data. Common in recommendation systems....", "l": "c", "k": ["cold", "start", "problem", "difficulty", "making", "predictions", "users", "items", "historical", "data", "common", "recommendation", "systems", "addressed", "hybrid"]}, {"id": "term-collaborative-filtering", "t": "Collaborative Filtering", "tg": ["Technique", "Recommendations"], "d": "general", "x": "Recommendation technique based on user behavior patterns. \"Users who liked X also liked Y.\" Forms the basis of many...", "l": "c", "k": ["collaborative", "filtering", "recommendation", "technique", "based", "user", "behavior", "patterns", "users", "liked", "forms", "basis", "systems", "netflix", "amazon"]}, {"id": "term-collection", "t": "Collection", "tg": ["Vector Database", "Infrastructure"], "d": "hardware", "x": "A named grouping of vectors and their associated metadata within a vector database, analogous to a table in relational...", "l": "c", "k": ["collection", "named", "grouping", "vectors", "associated", "metadata", "within", "vector", "database", "analogous", "table", "relational", "databases", "serving", "primary"]}, {"id": "term-collective-communication", "t": "Collective Communication", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "Coordinated data exchange patterns among multiple processes or GPUs, including all-reduce, all-gather, reduce-scatter,...", "l": "c", "k": ["collective", "communication", "coordinated", "data", "exchange", "patterns", "among", "multiple", "processes", "gpus", "including", "all-reduce", "all-gather", "reduce-scatter", "broadcast"]}, {"id": "term-collocation", "t": "Collocation", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A sequence of words that co-occur more frequently than expected by chance, forming conventional expressions such as...", "l": "c", "k": ["collocation", "sequence", "words", "co-occur", "frequently", "expected", "chance", "forming", "conventional", "expressions", "strong", "coffee", "decision", "identified", "statistical"]}, {"id": "term-colossus-computer", "t": "Colossus Computer", "tg": ["History", "Milestones"], "d": "history", "x": "The world's first programmable electronic digital computer, built at Bletchley Park in 1943-1944 to break German Lorenz...", "l": "c", "k": ["colossus", "computer", "world", "programmable", "electronic", "digital", "built", "bletchley", "park", "1943-1944", "break", "german", "lorenz", "cipher", "messages"]}, {"id": "term-combinatorial-explosion", "t": "Combinatorial Explosion", "tg": ["History", "Fundamentals"], "d": "history", "x": "The rapid growth of possible states or paths in a problem space as the number of variables increases. A fundamental...", "l": "c", "k": ["combinatorial", "explosion", "rapid", "growth", "possible", "states", "paths", "problem", "space", "number", "variables", "increases", "fundamental", "challenge", "search"]}, {"id": "term-command-model", "t": "Command Model", "tg": ["Model", "Cohere"], "d": "models", "x": "Cohere's instruction-tuned LLMs optimized for following commands and business applications. Includes Command R for RAG...", "l": "c", "k": ["command", "model", "cohere", "instruction-tuned", "llms", "optimized", "following", "commands", "business", "applications", "includes", "rag", "enterprise", "cases"]}, {"id": "term-command-r", "t": "Command R", "tg": ["Models", "Technical"], "d": "models", "x": "A family of language models by Cohere optimized for retrieval-augmented generation and tool use. Designed for...", "l": "c", "k": ["command", "family", "language", "models", "cohere", "optimized", "retrieval-augmented", "generation", "tool", "designed", "enterprise", "applications", "strong", "grounding", "retrieved"]}, {"id": "term-common-crawl", "t": "Common Crawl", "tg": ["Data", "Training"], "d": "general", "x": "A massive open repository of web data used to train many LLMs. Contains petabytes of text crawled from the internet,...", "l": "c", "k": ["common", "crawl", "massive", "open", "repository", "web", "data", "train", "llms", "contains", "petabytes", "text", "crawled", "internet", "requiring"]}, {"id": "term-commonsense-reasoning", "t": "Commonsense Reasoning", "tg": ["Capability", "Reasoning"], "d": "general", "x": "AI's ability to understand everyday knowledge humans take for granted. That water is wet, objects fall down, people...", "l": "c", "k": ["commonsense", "reasoning", "ability", "understand", "everyday", "knowledge", "humans", "take", "granted", "water", "wet", "objects", "fall", "down", "people"]}, {"id": "term-communication-marl", "t": "Communication in Multi-Agent RL", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "Protocols and mechanisms that allow agents in a multi-agent system to share information through learned communication...", "l": "c", "k": ["communication", "multi-agent", "protocols", "mechanisms", "allow", "agents", "system", "share", "information", "learned", "channels", "emergent", "develop", "structured", "language-like"]}, {"id": "term-communication-overlap", "t": "Communication Overlap", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A distributed training optimization that overlaps gradient communication with backward pass computation, hiding...", "l": "c", "k": ["communication", "overlap", "distributed", "training", "optimization", "overlaps", "gradient", "backward", "pass", "computation", "hiding", "latency", "behind", "useful", "work"]}, {"id": "term-competitive-rl", "t": "Competitive Reinforcement Learning", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A multi-agent RL setting where agents have opposing objectives, such as zero-sum games. Competitive RL involves finding...", "l": "c", "k": ["competitive", "reinforcement", "learning", "multi-agent", "setting", "agents", "opposing", "objectives", "zero-sum", "games", "involves", "finding", "nash", "equilibria", "developing"]}, {"id": "term-compile-time-graph-optimization", "t": "Compile-Time Graph Optimization", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Static optimization of computation graphs before execution, including constant folding, dead code elimination, and...", "l": "c", "k": ["compile-time", "graph", "optimization", "static", "computation", "graphs", "execution", "including", "constant", "folding", "dead", "code", "elimination", "operator", "fusion"]}, {"id": "term-completion", "t": "Completion", "tg": ["Task", "Fundamentals"], "d": "general", "x": "Text generated by an AI to continue a given prompt. The basic operation of language models: given input text, predict...", "l": "c", "k": ["completion", "text", "generated", "continue", "given", "prompt", "basic", "operation", "language", "models", "input", "predict", "comes", "next"]}, {"id": "term-complexity-based-prompting", "t": "Complexity-Based Prompting", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A self-consistency variant that selects the final answer from reasoning chains with the highest complexity, measured by...", "l": "c", "k": ["complexity-based", "prompting", "self-consistency", "variant", "selects", "final", "answer", "reasoning", "chains", "highest", "complexity", "measured", "number", "steps", "based"]}, {"id": "term-compositionality", "t": "Compositionality", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The principle that the meaning of a complex expression is determined by the meanings of its parts and the rules used to...", "l": "c", "k": ["compositionality", "principle", "meaning", "complex", "expression", "determined", "meanings", "parts", "rules", "combine", "foundational", "concept", "formal", "semantics"]}, {"id": "term-compression", "t": "Compression (Model)", "tg": ["Optimization", "Deployment"], "d": "algorithms", "x": "Reducing model size while maintaining performance. Techniques include quantization, pruning, and distillation. Enables...", "l": "c", "k": ["compression", "model", "reducing", "size", "maintaining", "performance", "techniques", "include", "quantization", "pruning", "distillation", "enables", "deployment", "edge", "devices"]}, {"id": "term-compute", "t": "Compute", "tg": ["Infrastructure", "Resources"], "d": "hardware", "x": "Computational resources required for training and running AI models. Measured in FLOPs, GPU-hours, or dollars. A...", "l": "c", "k": ["compute", "computational", "resources", "required", "training", "running", "models", "measured", "flops", "gpu-hours", "dollars", "primary", "constraint", "cost", "driver"]}, {"id": "term-compute-governance", "t": "Compute Governance", "tg": ["Governance", "AI Safety"], "d": "safety", "x": "Policy approaches that use computational resources as a lever for AI governance, including monitoring large training...", "l": "c", "k": ["compute", "governance", "policy", "approaches", "computational", "resources", "lever", "including", "monitoring", "large", "training", "runs", "export", "controls", "chips"]}, {"id": "term-compute-bound", "t": "Compute-Bound Workload", "tg": ["Hardware", "Model Optimization"], "d": "models", "x": "A processing task where performance is limited by the rate of arithmetic computation rather than memory bandwidth or...", "l": "c", "k": ["compute-bound", "workload", "processing", "task", "performance", "limited", "rate", "arithmetic", "computation", "rather", "memory", "bandwidth", "training", "large", "models"]}, {"id": "term-compute-optimal-training", "t": "Compute-Optimal Training", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An approach to model training that seeks the best allocation of a fixed compute budget between model parameters and...", "l": "c", "k": ["compute-optimal", "training", "approach", "model", "seeks", "best", "allocation", "fixed", "compute", "budget", "parameters", "tokens", "based", "empirical", "scaling"]}, {"id": "term-computer-vision", "t": "Computer Vision", "tg": ["Field", "Images"], "d": "general", "x": "The field of AI that enables machines to interpret and understand visual information from images and videos....", "l": "c", "k": ["computer", "vision", "field", "enables", "machines", "interpret", "understand", "visual", "information", "images", "videos", "applications", "include", "object", "detection"]}, {"id": "term-computer-vision-history", "t": "Computer Vision History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of computer vision from early edge detection and pattern recognition in the 1960s through feature-based...", "l": "c", "k": ["computer", "vision", "history", "evolution", "early", "edge", "detection", "pattern", "recognition", "1960s", "feature-based", "methods", "sift", "hog", "deep"]}, {"id": "term-computing-machinery-and-intelligence", "t": "Computing Machinery and Intelligence", "tg": ["History", "Milestones"], "d": "history", "x": "A seminal 1950 paper by Alan Turing published in the journal Mind that proposed the imitation game (later known as the...", "l": "c", "k": ["computing", "machinery", "intelligence", "seminal", "paper", "alan", "turing", "published", "journal", "mind", "proposed", "imitation", "game", "later", "known"]}, {"id": "term-concept-drift", "t": "Concept Drift", "tg": ["Challenge", "Production"], "d": "general", "x": "When the relationship between input and output changes over time, causing model performance to degrade. Requires...", "l": "c", "k": ["concept", "drift", "relationship", "input", "output", "changes", "time", "causing", "model", "performance", "degrade", "requires", "monitoring", "retraining", "maintain"]}, {"id": "term-conceptual-dependency-theory", "t": "Conceptual Dependency Theory", "tg": ["History", "Fundamentals"], "d": "history", "x": "A theory of natural language understanding developed by Roger Schank in the 1970s that represents the meaning of...", "l": "c", "k": ["conceptual", "dependency", "theory", "natural", "language", "understanding", "developed", "roger", "schank", "1970s", "represents", "meaning", "sentences", "small", "primitive"]}, {"id": "term-conditional-gan", "t": "Conditional GAN", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A GAN variant where both generator and discriminator receive additional conditioning information such as class labels...", "l": "c", "k": ["conditional", "gan", "variant", "generator", "discriminator", "receive", "additional", "conditioning", "information", "class", "labels", "text", "enabling", "controlled", "generation"]}, {"id": "term-conditional-generation", "t": "Conditional Generation", "tg": ["Technique", "Generation"], "d": "general", "x": "Generating content based on specific conditions or inputs. Image generation conditioned on text, or text generation...", "l": "c", "k": ["conditional", "generation", "generating", "content", "based", "specific", "conditions", "inputs", "image", "conditioned", "text", "topic", "style"]}, {"id": "term-crf", "t": "Conditional Random Field", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A discriminative probabilistic model for sequence labeling that models the conditional probability of label sequences...", "l": "c", "k": ["conditional", "random", "field", "discriminative", "probabilistic", "model", "sequence", "labeling", "models", "probability", "label", "sequences", "given", "observations", "capturing"]}, {"id": "term-confabulation", "t": "Confabulation", "tg": ["Risk", "Limitation"], "d": "safety", "x": "Another term for hallucinationwhen AI generates plausible but false information. The model \"fills in gaps\" with...", "l": "c", "k": ["confabulation", "another", "term", "hallucination", "generates", "plausible", "false", "information", "model", "fills", "gaps", "invented", "content", "sounds", "convincing"]}, {"id": "term-confidence-interval", "t": "Confidence Interval", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A range of values constructed from sample data that, if the sampling procedure were repeated many times, would contain...", "l": "c", "k": ["confidence", "interval", "range", "values", "constructed", "sample", "data", "sampling", "procedure", "were", "repeated", "times", "contain", "true", "population"]}, {"id": "term-confidence-score", "t": "Confidence Score", "tg": ["Metrics", "Evaluation"], "d": "datasets", "x": "A numerical value indicating how certain a model is about its prediction or output. Higher scores suggest the model is...", "l": "c", "k": ["confidence", "score", "numerical", "value", "indicating", "certain", "model", "prediction", "output", "higher", "scores", "suggest", "sure", "doesn", "always"]}, {"id": "term-confidence-threshold-cv", "t": "Confidence Threshold", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "The minimum prediction score required to accept a detection as valid, balancing between missing true detections (high...", "l": "c", "k": ["confidence", "threshold", "minimum", "prediction", "score", "required", "accept", "detection", "valid", "balancing", "missing", "true", "detections", "high", "including"]}, {"id": "term-confirmation-bias-in-ai", "t": "Confirmation Bias in AI", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "The tendency for AI developers or users to favor data, model outputs, or evaluation criteria that confirm pre-existing...", "l": "c", "k": ["confirmation", "bias", "tendency", "developers", "users", "favor", "data", "model", "outputs", "evaluation", "criteria", "confirm", "pre-existing", "beliefs", "leading"]}, {"id": "term-conformer", "t": "Conformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A speech processing architecture that combines convolution and transformer modules in each block, capturing both local...", "l": "c", "k": ["conformer", "speech", "processing", "architecture", "combines", "convolution", "transformer", "modules", "block", "capturing", "local", "global", "dependencies", "improved", "automatic"]}, {"id": "term-conformity-assessment-for-ai", "t": "Conformity Assessment for AI", "tg": ["Governance", "Regulation"], "d": "safety", "x": "The formal evaluation process required under the EU AI Act to verify that high-risk AI systems meet regulatory...", "l": "c", "k": ["conformity", "assessment", "formal", "evaluation", "process", "required", "act", "verify", "high-risk", "systems", "meet", "regulatory", "requirements", "placed", "market"]}, {"id": "term-confounding-variable", "t": "Confounding Variable", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A variable that influences both the independent and dependent variables, creating a spurious association between them....", "l": "c", "k": ["confounding", "variable", "influences", "independent", "dependent", "variables", "creating", "spurious", "association", "failure", "control", "confounders", "lead", "incorrect", "causal"]}, {"id": "term-confusion-matrix", "t": "Confusion Matrix", "tg": ["Evaluation", "Visualization"], "d": "datasets", "x": "A table showing correct and incorrect predictions for each class. Reveals where a classification model makes mistakes,...", "l": "c", "k": ["confusion", "matrix", "table", "showing", "correct", "incorrect", "predictions", "class", "reveals", "classification", "model", "makes", "mistakes", "enabling", "targeted"]}, {"id": "term-conjugate-gradient-method", "t": "Conjugate Gradient Method", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An iterative optimization algorithm for solving systems of linear equations with symmetric positive-definite matrices....", "l": "c", "k": ["conjugate", "gradient", "method", "iterative", "optimization", "algorithm", "solving", "systems", "linear", "equations", "symmetric", "positive-definite", "matrices", "adapted", "nonlinear"]}, {"id": "term-conjugate-prior", "t": "Conjugate Prior", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A prior distribution that, when combined with a particular likelihood function via Bayes' theorem, yields a posterior...", "l": "c", "k": ["conjugate", "prior", "distribution", "combined", "particular", "likelihood", "function", "via", "bayes", "theorem", "yields", "posterior", "family", "simplifies", "bayesian"]}, {"id": "term-connection-machine", "t": "Connection Machine", "tg": ["History", "Systems"], "d": "history", "x": "A series of massively parallel supercomputers designed by Danny Hillis at Thinking Machines Corporation in the 1980s....", "l": "c", "k": ["connection", "machine", "series", "massively", "parallel", "supercomputers", "designed", "danny", "hillis", "thinking", "machines", "corporation", "1980s", "cm-1", "processors"]}, {"id": "term-connectionism", "t": "Connectionism", "tg": ["History", "Fundamentals"], "d": "history", "x": "A theoretical framework in cognitive science and AI that models mental phenomena using interconnected networks of...", "l": "c", "k": ["connectionism", "theoretical", "framework", "cognitive", "science", "models", "mental", "phenomena", "interconnected", "networks", "simple", "units", "connectionist", "approach", "contrasts"]}, {"id": "term-connectionism-vs-symbolism", "t": "Connectionism vs Symbolism", "tg": ["History", "Milestones"], "d": "history", "x": "The historical debate in AI between connectionist approaches using neural networks that learn distributed...", "l": "c", "k": ["connectionism", "symbolism", "historical", "debate", "connectionist", "approaches", "neural", "networks", "learn", "distributed", "representations", "symbolic", "explicit", "rules", "logic"]}, {"id": "term-connectionist-revival", "t": "Connectionist Revival", "tg": ["History", "Milestones"], "d": "history", "x": "The resurgence of interest in neural networks in the 1980s driven by the parallel distributed processing (PDP) research...", "l": "c", "k": ["connectionist", "revival", "resurgence", "interest", "neural", "networks", "1980s", "driven", "parallel", "distributed", "processing", "pdp", "research", "group", "including"]}, {"id": "term-consent-laundering", "t": "Consent Laundering", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "The practice of obtaining user consent for data collection through opaque terms of service and then repurposing that...", "l": "c", "k": ["consent", "laundering", "practice", "obtaining", "user", "data", "collection", "opaque", "terms", "service", "repurposing", "training", "ways", "users", "anticipated"]}, {"id": "term-conservative-q-learning", "t": "Conservative Q-Learning (CQL)", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An offline RL algorithm that adds a regularizer to penalize Q-values for out-of-distribution actions, producing...", "l": "c", "k": ["conservative", "q-learning", "cql", "offline", "algorithm", "adds", "regularizer", "penalize", "q-values", "out-of-distribution", "actions", "producing", "value", "estimates", "avoid"]}, {"id": "term-consistency", "t": "Consistency", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A property of a statistical estimator indicating that it converges in probability to the true parameter value as the...", "l": "c", "k": ["consistency", "property", "statistical", "estimator", "indicating", "converges", "probability", "true", "parameter", "value", "sample", "size", "approaches", "infinity", "consistent"]}, {"id": "term-consistency-model", "t": "Consistency Model", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative model that learns to map any point along a diffusion trajectory directly to the trajectory's starting...", "l": "c", "k": ["consistency", "model", "generative", "learns", "map", "point", "along", "diffusion", "trajectory", "directly", "starting", "enabling", "one-step", "few-step", "generation"]}, {"id": "term-consistency-regularization", "t": "Consistency Regularization", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A semi-supervised learning principle that enforces the model to produce similar predictions for perturbed versions of...", "l": "c", "k": ["consistency", "regularization", "semi-supervised", "learning", "principle", "enforces", "model", "produce", "similar", "predictions", "perturbed", "versions", "input", "encourages", "smooth"]}, {"id": "term-consistency-based-self-evaluation", "t": "Consistency-Based Self-Evaluation", "tg": ["Evaluation", "LLM-Based"], "d": "models", "x": "An evaluation method where a language model assesses the quality of its own outputs by generating multiple responses...", "l": "c", "k": ["consistency-based", "self-evaluation", "evaluation", "method", "language", "model", "assesses", "quality", "outputs", "generating", "multiple", "responses", "measuring", "agreement", "across"]}, {"id": "term-constituency-parsing", "t": "Constituency Parsing", "tg": ["NLP", "Parsing"], "d": "general", "x": "The task of analyzing sentence structure by breaking it into hierarchical nested constituents (phrases) according to a...", "l": "c", "k": ["constituency", "parsing", "task", "analyzing", "sentence", "structure", "breaking", "hierarchical", "nested", "constituents", "phrases", "according", "grammar", "producing", "tree"]}, {"id": "term-constitutional-ai", "t": "Constitutional AI", "tg": ["Safety", "Anthropic"], "d": "safety", "x": "Anthropic's approach to AI alignment where models are trained to follow a set of principles (\"constitution\") that guide...", "l": "c", "k": ["constitutional", "anthropic", "approach", "alignment", "models", "trained", "follow", "principles", "constitution", "guide", "behavior", "reduces", "reliance", "human", "feedback"]}, {"id": "term-constitutional-ai-model", "t": "Constitutional AI Model", "tg": ["Models", "Safety"], "d": "models", "x": "An AI system trained using constitutional AI methods where the model self-critiques and revises its outputs against a...", "l": "c", "k": ["constitutional", "model", "system", "trained", "methods", "self-critiques", "revises", "outputs", "against", "principles", "reduces", "need", "human", "feedback", "ai-generated"]}, {"id": "term-constitutional-ai-training", "t": "Constitutional AI Training", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A training methodology where the model critiques and revises its own outputs according to a set of written principles,...", "l": "c", "k": ["constitutional", "training", "methodology", "model", "critiques", "revises", "outputs", "according", "written", "principles", "reducing", "reliance", "human", "feedback", "alignment"]}, {"id": "term-constitutional-prompting", "t": "Constitutional Prompting", "tg": ["Prompt Engineering", "Safety"], "d": "safety", "x": "A prompting approach that provides the model with an explicit set of principles, rules, or constitutional guidelines...", "l": "c", "k": ["constitutional", "prompting", "approach", "provides", "model", "explicit", "principles", "rules", "guidelines", "must", "follow", "generating", "responses", "enabling", "value-aligned"]}, {"id": "term-constrained-beam-search", "t": "Constrained Beam Search", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A beam search variant that enforces lexical or structural constraints during decoding, ensuring that certain tokens or...", "l": "c", "k": ["constrained", "beam", "search", "variant", "enforces", "lexical", "structural", "constraints", "decoding", "ensuring", "certain", "tokens", "phrases", "must", "appear"]}, {"id": "term-constrained-rl", "t": "Constrained Reinforcement Learning", "tg": ["Reinforcement Learning", "Safety"], "d": "safety", "x": "An RL formulation where the agent maximizes expected return while satisfying one or more constraint functions on...", "l": "c", "k": ["constrained", "reinforcement", "learning", "formulation", "agent", "maximizes", "expected", "return", "satisfying", "constraint", "functions", "costs", "mdps", "solved", "lagrangian"]}, {"id": "term-constraint", "t": "Constraint (Prompting)", "tg": ["Prompting", "Technique"], "d": "general", "x": "Limitations or requirements specified in a prompt. \"Respond in 50 words or less\" or \"Use only formal language.\"...", "l": "c", "k": ["constraint", "prompting", "limitations", "requirements", "specified", "prompt", "respond", "words", "less", "formal", "language", "constraints", "shape", "focus", "output"]}, {"id": "term-constraint-prompting", "t": "Constraint Prompting", "tg": ["Prompt Engineering", "Constraints"], "d": "general", "x": "A technique that specifies explicit constraints within the prompt such as length limits, format requirements,...", "l": "c", "k": ["constraint", "prompting", "technique", "specifies", "explicit", "constraints", "within", "prompt", "length", "limits", "format", "requirements", "vocabulary", "restrictions", "content"]}, {"id": "term-constraint-satisfaction", "t": "Constraint Satisfaction", "tg": ["History", "Fundamentals"], "d": "history", "x": "A paradigm for solving problems by finding values for variables that satisfy a set of constraints. Constraint...", "l": "c", "k": ["constraint", "satisfaction", "paradigm", "solving", "problems", "finding", "values", "variables", "satisfy", "constraints", "csps", "fundamental", "planning", "scheduling", "configuration"]}, {"id": "term-content-authenticity-initiative", "t": "Content Authenticity Initiative", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "An industry coalition led by Adobe that develops open standards for attributing and verifying the provenance of digital...", "l": "c", "k": ["content", "authenticity", "initiative", "industry", "coalition", "led", "adobe", "develops", "open", "standards", "attributing", "verifying", "provenance", "digital", "helping"]}, {"id": "term-content-filtering", "t": "Content Filtering", "tg": ["Safety", "Moderation"], "d": "safety", "x": "Systems that detect and block harmful content in AI inputs or outputs. Part of safety infrastructure, filtering...", "l": "c", "k": ["content", "filtering", "systems", "detect", "block", "harmful", "inputs", "outputs", "part", "safety", "infrastructure", "violence", "explicit", "policy", "violations"]}, {"id": "term-content-moderation", "t": "Content Moderation", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The process of monitoring and filtering user-generated content on digital platforms to enforce community standards,...", "l": "c", "k": ["content", "moderation", "process", "monitoring", "filtering", "user-generated", "digital", "platforms", "enforce", "community", "standards", "increasingly", "assisted", "classifiers", "detecting"]}, {"id": "term-context", "t": "Context", "tg": ["Prompting", "Core Concept"], "d": "general", "x": "Background information provided to AI that helps it understand your situation and needs. Essential for getting...", "l": "c", "k": ["context", "background", "information", "provided", "helps", "understand", "situation", "needs", "essential", "getting", "relevant", "accurate", "responses"]}, {"id": "term-context-distillation", "t": "Context Distillation", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A training technique that transfers the behavior elicited by a specific prompt or context into the model's weights,...", "l": "c", "k": ["context", "distillation", "training", "technique", "transfers", "behavior", "elicited", "specific", "prompt", "model", "weights", "eliminating", "need", "include", "inference"]}, {"id": "term-context-length", "t": "Context Length", "tg": ["Specification", "Limitation"], "d": "general", "x": "The maximum amount of text a model can process at once, measured in tokens. Ranges from 4K to 200K+ depending on the...", "l": "c", "k": ["context", "length", "maximum", "amount", "text", "model", "process", "measured", "tokens", "ranges", "200k", "depending", "longer", "contexts", "enable"]}, {"id": "term-context-parallelism", "t": "Context Parallelism", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A specialized parallelism approach that distributes attention computation across GPUs along the sequence length...", "l": "c", "k": ["context", "parallelism", "specialized", "approach", "distributes", "attention", "computation", "across", "gpus", "along", "sequence", "length", "dimension", "long", "windows"]}, {"id": "term-context-window", "t": "Context Window", "tg": ["Limitation", "Architecture"], "d": "models", "x": "The amount of text (measured in tokens) that an AI can process at once. Modern models range from 4K to 200K+ tokens,...", "l": "c", "k": ["context", "window", "amount", "text", "measured", "tokens", "process", "modern", "models", "range", "200k", "determining", "conversation", "history", "reference"]}, {"id": "term-context-window-management", "t": "Context Window Management", "tg": ["LLM", "Inference"], "d": "models", "x": "Techniques for efficiently utilizing and extending the finite context window of language models, including sliding...", "l": "c", "k": ["context", "window", "management", "techniques", "efficiently", "utilizing", "extending", "finite", "language", "models", "including", "sliding", "windows", "summarization", "earlier"]}, {"id": "term-context-free-grammar", "t": "Context-Free Grammar", "tg": ["NLP", "Parsing"], "d": "general", "x": "A formal grammar where production rules map single non-terminal symbols to sequences of terminals and non-terminals,...", "l": "c", "k": ["context-free", "grammar", "formal", "production", "rules", "map", "single", "non-terminal", "symbols", "sequences", "terminals", "non-terminals", "widely", "nlp", "defining"]}, {"id": "term-contextual-bandit", "t": "Contextual Bandit", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An extension of the multi-armed bandit where the agent observes a context (feature vector) before choosing an action,...", "l": "c", "k": ["contextual", "bandit", "extension", "multi-armed", "agent", "observes", "context", "feature", "vector", "choosing", "action", "allowing", "policy", "adapt", "decisions"]}, {"id": "term-contextual-calibration", "t": "Contextual Calibration", "tg": ["Prompt Engineering", "Calibration"], "d": "general", "x": "A technique that adjusts a language model's output probabilities by estimating and correcting for biases introduced by...", "l": "c", "k": ["contextual", "calibration", "technique", "adjusts", "language", "model", "output", "probabilities", "estimating", "correcting", "biases", "introduced", "prompt", "context", "typically"]}, {"id": "term-contextual-compression", "t": "Contextual Compression", "tg": ["Retrieval", "Post-Processing"], "d": "general", "x": "A retrieval post-processing technique that compresses or extracts only the most relevant portions from retrieved...", "l": "c", "k": ["contextual", "compression", "retrieval", "post-processing", "technique", "compresses", "extracts", "relevant", "portions", "retrieved", "documents", "based", "query", "context", "reducing"]}, {"id": "term-contextual-embedding", "t": "Contextual Embedding", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A word representation that varies depending on the surrounding context, unlike static embeddings, capturing polysemy...", "l": "c", "k": ["contextual", "embedding", "word", "representation", "varies", "depending", "surrounding", "context", "unlike", "static", "embeddings", "capturing", "polysemy", "context-dependent", "meaning"]}, {"id": "term-contextual-few-shot-selection", "t": "Contextual Few-Shot Selection", "tg": ["Prompt Engineering", "Example Selection"], "d": "general", "x": "The practice of dynamically selecting the most relevant few-shot examples for each query based on semantic similarity,...", "l": "c", "k": ["contextual", "few-shot", "selection", "practice", "dynamically", "selecting", "relevant", "examples", "query", "based", "semantic", "similarity", "task", "characteristics", "diversity"]}, {"id": "term-contextual-retrieval", "t": "Contextual Retrieval", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "A retrieval enhancement technique that prepends each chunk with a model-generated contextual summary explaining the...", "l": "c", "k": ["contextual", "retrieval", "enhancement", "technique", "prepends", "chunk", "model-generated", "summary", "explaining", "place", "within", "larger", "document", "improving", "accuracy"]}, {"id": "term-continual-learning", "t": "Continual Learning", "tg": ["Training", "Research"], "d": "general", "x": "Training models incrementally on new data without forgetting previous knowledge. A challenge because neural networks...", "l": "c", "k": ["continual", "learning", "training", "models", "incrementally", "data", "without", "forgetting", "previous", "knowledge", "challenge", "neural", "networks", "tend", "overwrite"]}, {"id": "term-continual-rl", "t": "Continual Reinforcement Learning", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "RL settings where the agent must learn and adapt over a non-stationary sequence of tasks without forgetting earlier...", "l": "c", "k": ["continual", "reinforcement", "learning", "settings", "agent", "must", "learn", "adapt", "non-stationary", "sequence", "tasks", "without", "forgetting", "earlier", "knowledge"]}, {"id": "term-continuous-batching", "t": "Continuous Batching", "tg": ["LLM", "Inference"], "d": "models", "x": "A dynamic batching strategy where new requests are inserted into a running batch as soon as existing requests complete,...", "l": "c", "k": ["continuous", "batching", "dynamic", "strategy", "requests", "inserted", "running", "batch", "soon", "existing", "complete", "eliminating", "idle", "gpu", "time"]}, {"id": "term-contractive-autoencoder", "t": "Contractive Autoencoder", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An autoencoder that adds a penalty term based on the Frobenius norm of the encoder's Jacobian matrix, encouraging the...", "l": "c", "k": ["contractive", "autoencoder", "adds", "penalty", "term", "based", "frobenius", "norm", "encoder", "jacobian", "matrix", "encouraging", "learned", "representation", "robust"]}, {"id": "term-contrastive-chain-of-thought", "t": "Contrastive Chain-of-Thought", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting approach that provides both correct and incorrect reasoning examples in demonstrations, helping the model...", "l": "c", "k": ["contrastive", "chain-of-thought", "prompting", "approach", "provides", "correct", "incorrect", "reasoning", "examples", "demonstrations", "helping", "model", "learn", "right", "patterns"]}, {"id": "term-contrastive-decoding", "t": "Contrastive Decoding", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A decoding method that improves generation quality by contrasting the output distributions of a large expert model and...", "l": "c", "k": ["contrastive", "decoding", "method", "improves", "generation", "quality", "contrasting", "output", "distributions", "large", "expert", "model", "smaller", "amateur", "suppressing"]}, {"id": "term-contrastive-learning", "t": "Contrastive Learning", "tg": ["Training", "Technique"], "d": "general", "x": "Training by comparing similar and dissimilar examples. The model learns to place similar items close together in...", "l": "c", "k": ["contrastive", "learning", "training", "comparing", "similar", "dissimilar", "examples", "model", "learns", "place", "items", "close", "together", "embedding", "space"]}, {"id": "term-contrastive-learning-vision", "t": "Contrastive Learning for Vision", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A self-supervised approach that trains visual encoders by pulling augmented views of the same image closer in embedding...", "l": "c", "k": ["contrastive", "learning", "vision", "self-supervised", "approach", "trains", "visual", "encoders", "pulling", "augmented", "views", "image", "closer", "embedding", "space"]}, {"id": "term-contrastive-loss", "t": "Contrastive Loss", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A loss function that trains models to pull similar (positive) pairs closer together and push dissimilar (negative)...", "l": "c", "k": ["contrastive", "loss", "function", "trains", "models", "pull", "similar", "positive", "pairs", "closer", "together", "push", "dissimilar", "negative", "apart"]}, {"id": "term-contrastive-search", "t": "Contrastive Search", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A text generation method that selects tokens based on both probability and degeneration penalty. Balances the...", "l": "c", "k": ["contrastive", "search", "text", "generation", "method", "selects", "tokens", "based", "probability", "degeneration", "penalty", "balances", "confidence", "model", "distinctiveness"]}, {"id": "term-control-problem", "t": "Control Problem", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The challenge of ensuring that a highly capable AI system remains under meaningful human control and pursues objectives...", "l": "c", "k": ["control", "problem", "challenge", "ensuring", "highly", "capable", "system", "remains", "meaningful", "human", "pursues", "objectives", "aligned", "values", "capabilities"]}, {"id": "term-controllable-generation", "t": "Controllable Generation", "tg": ["Technique", "Generation"], "d": "general", "x": "Techniques for steering AI output toward desired attributes like sentiment, style, or topic. Enables more precise...", "l": "c", "k": ["controllable", "generation", "techniques", "steering", "output", "toward", "desired", "attributes", "sentiment", "style", "topic", "enables", "precise", "control", "generated"]}, {"id": "term-controlnet", "t": "ControlNet", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A neural network architecture that adds spatial conditioning controls to pre-trained diffusion models, enabling guided...", "l": "c", "k": ["controlnet", "neural", "network", "architecture", "adds", "spatial", "conditioning", "controls", "pre-trained", "diffusion", "models", "enabling", "guided", "image", "generation"]}, {"id": "term-convergence", "t": "Convergence", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "The property of an optimization algorithm or iterative process where successive iterations produce results that...", "l": "c", "k": ["convergence", "property", "optimization", "algorithm", "iterative", "process", "successive", "iterations", "produce", "results", "approach", "stable", "solution", "fixed", "point"]}, {"id": "term-conversation-history", "t": "Conversation History", "tg": ["Feature", "Context"], "d": "general", "x": "The record of previous messages in a chat session. Provides context for AI responses. Managing history is important as...", "l": "c", "k": ["conversation", "history", "record", "previous", "messages", "chat", "session", "provides", "context", "responses", "managing", "important", "consumes", "window", "space"]}, {"id": "term-conversational-ai", "t": "Conversational AI", "tg": ["Application", "NLP"], "d": "general", "x": "AI systems designed for natural dialogue with humans. Includes chatbots, virtual assistants, and systems like ChatGPT...", "l": "c", "k": ["conversational", "systems", "designed", "natural", "dialogue", "humans", "includes", "chatbots", "virtual", "assistants", "chatgpt", "claude", "maintain", "context", "across"]}, {"id": "term-convnext", "t": "ConvNeXt", "tg": ["Models", "Technical"], "d": "models", "x": "A pure convolutional architecture that modernizes ResNet design by incorporating ideas from vision transformers such as...", "l": "c", "k": ["convnext", "pure", "convolutional", "architecture", "modernizes", "resnet", "design", "incorporating", "ideas", "vision", "transformers", "larger", "kernel", "sizes", "training"]}, {"id": "term-convolution", "t": "Convolution", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A mathematical operation that combines two functions to produce a third expressing how one modifies the other. In deep...", "l": "c", "k": ["convolution", "mathematical", "operation", "combines", "functions", "produce", "expressing", "modifies", "deep", "learning", "layers", "apply", "learned", "filters", "input"]}, {"id": "term-convolutional-filter", "t": "Convolutional Filter", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A learnable weight matrix (kernel) that slides across an input image or feature map, computing element-wise products...", "l": "c", "k": ["convolutional", "filter", "learnable", "weight", "matrix", "kernel", "slides", "across", "input", "image", "feature", "map", "computing", "element-wise", "products"]}, {"id": "term-cnn-history", "t": "Convolutional Neural Network History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of CNNs from Fukushima's Neocognitron in 1980 through LeCun's application to handwritten digit...", "l": "c", "k": ["convolutional", "neural", "network", "history", "development", "cnns", "fukushima", "neocognitron", "lecun", "application", "handwritten", "digit", "recognition", "culminating", "dominance"]}, {"id": "term-conways-game-of-life", "t": "Conway's Game of Life", "tg": ["History", "Systems"], "d": "history", "x": "A cellular automaton devised by mathematician John Horton Conway in 1970. Despite having only simple rules (birth...", "l": "c", "k": ["conway", "game", "life", "cellular", "automaton", "devised", "mathematician", "john", "horton", "despite", "having", "simple", "rules", "birth", "survival"]}, {"id": "term-cooks-distance", "t": "Cook's Distance", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A measure of the influence of each observation on the fitted values of a regression model, computed as the sum of...", "l": "c", "k": ["cook", "distance", "measure", "influence", "observation", "fitted", "values", "regression", "model", "computed", "sum", "changes", "predicted", "removed", "high"]}, {"id": "term-cooperative-inverse-reinforcement-learning", "t": "Cooperative Inverse Reinforcement Learning", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "A framework for human-AI alignment where a robot and human work together in a game where the robot tries to maximize...", "l": "c", "k": ["cooperative", "inverse", "reinforcement", "learning", "framework", "human-ai", "alignment", "robot", "human", "work", "together", "game", "tries", "maximize", "reward"]}, {"id": "term-cooperative-rl", "t": "Cooperative Reinforcement Learning", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A multi-agent RL setting where agents share a common objective and must learn to coordinate their actions for mutual...", "l": "c", "k": ["cooperative", "reinforcement", "learning", "multi-agent", "setting", "agents", "share", "common", "objective", "must", "learn", "coordinate", "actions", "mutual", "benefit"]}, {"id": "term-coordinate-descent", "t": "Coordinate Descent", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An optimization algorithm that minimizes along one coordinate direction at a time while holding others fixed....", "l": "c", "k": ["coordinate", "descent", "optimization", "algorithm", "minimizes", "along", "direction", "time", "holding", "others", "fixed", "particularly", "effective", "problems", "separable"]}, {"id": "term-copernicus-of-ai", "t": "Copernicus of AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "An informal designation sometimes given to researchers whose work fundamentally shifted the paradigm of AI research....", "l": "c", "k": ["copernicus", "informal", "designation", "sometimes", "given", "researchers", "whose", "work", "fundamentally", "shifted", "paradigm", "research", "applied", "geoffrey", "hinton"]}, {"id": "term-copilot", "t": "Copilot", "tg": ["Product", "Microsoft"], "d": "general", "x": "Microsoft's AI assistant integrated into their products. Originally focused on code completion (GitHub Copilot), now...", "l": "c", "k": ["copilot", "microsoft", "assistant", "integrated", "products", "originally", "focused", "code", "completion", "github", "now", "extended", "general", "assistance", "across"]}, {"id": "term-copula", "t": "Copula", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A multivariate probability distribution that captures the dependence structure between random variables independently...", "l": "c", "k": ["copula", "multivariate", "probability", "distribution", "captures", "dependence", "structure", "random", "variables", "independently", "marginal", "distributions", "copulas", "allow", "modeling"]}, {"id": "term-coreference-resolution", "t": "Coreference Resolution", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying all expressions in a text that refer to the same real-world entity, linking pronouns, noun...", "l": "c", "k": ["coreference", "resolution", "task", "identifying", "expressions", "text", "refer", "real-world", "entity", "linking", "pronouns", "noun", "phrases", "mentions", "form"]}, {"id": "term-corpus", "t": "Corpus", "tg": ["Data", "Training"], "d": "general", "x": "A large collection of text used for training or evaluating language models. Quality corpora are essential for...", "l": "c", "k": ["corpus", "large", "collection", "text", "training", "evaluating", "language", "models", "quality", "corpora", "essential", "developing", "capable", "nlp", "systems"]}, {"id": "term-corrective-rag", "t": "Corrective RAG", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A RAG variant that evaluates the relevance of retrieved documents and, if they are insufficient, triggers web search or...", "l": "c", "k": ["corrective", "rag", "variant", "evaluates", "relevance", "retrieved", "documents", "insufficient", "triggers", "web", "search", "query", "reformulation", "correct", "retrieval"]}, {"id": "term-correlation-coefficient", "t": "Correlation Coefficient", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A statistical measure quantifying the strength and direction of the linear relationship between two variables,...", "l": "c", "k": ["correlation", "coefficient", "statistical", "measure", "quantifying", "strength", "direction", "linear", "relationship", "variables", "typically", "pearson", "ranging", "perfect", "negative"]}, {"id": "term-corrigibility", "t": "Corrigibility", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The property of an AI system that allows its operators to correct, modify, retrain, or shut it down without the system...", "l": "c", "k": ["corrigibility", "property", "system", "allows", "operators", "correct", "modify", "retrain", "shut", "down", "without", "resisting", "subverting", "interventions", "ensuring"]}, {"id": "term-cosine-annealing", "t": "Cosine Annealing", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A learning rate schedule that decreases the learning rate following a cosine curve from its initial value to near zero...", "l": "c", "k": ["cosine", "annealing", "learning", "rate", "schedule", "decreases", "following", "curve", "initial", "value", "near", "zero", "training", "period", "optionally"]}, {"id": "term-cosine-annealing-with-warm-restarts", "t": "Cosine Annealing with Warm Restarts", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "An extension of cosine annealing that periodically resets the learning rate to its initial value creating a series of...", "l": "c", "k": ["cosine", "annealing", "warm", "restarts", "extension", "periodically", "resets", "learning", "rate", "initial", "value", "creating", "series", "decay", "cycles"]}, {"id": "term-cosine-similarity", "t": "Cosine Similarity", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A similarity metric that measures the cosine of the angle between two vectors, ranging from -1 (opposite) to 1...", "l": "c", "k": ["cosine", "similarity", "metric", "measures", "angle", "vectors", "ranging", "opposite", "identical", "direction", "captures", "orientation", "rather", "magnitude", "widely"]}, {"id": "term-cost-function", "t": "Cost Function", "tg": ["Training", "Math"], "d": "general", "x": "Another name for loss function - the metric being minimized during training. Different tasks use different cost...", "l": "c", "k": ["cost", "function", "another", "name", "loss", "metric", "minimized", "training", "different", "tasks", "functions", "cross-entropy", "classification", "mse", "regression"]}, {"id": "term-costar", "t": "COSTAR", "tg": ["Framework", "Professional"], "d": "general", "x": "A prompting framework: Context, Objective, Style, Tone, Audience, Response. Ideal for professional content creation...", "l": "c", "k": ["costar", "prompting", "framework", "context", "objective", "style", "tone", "audience", "response", "ideal", "professional", "content", "creation", "specific", "voice"]}, {"id": "term-count-based-exploration", "t": "Count-Based Exploration", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An exploration strategy that provides bonus rewards inversely related to state visitation counts, encouraging the agent...", "l": "c", "k": ["count-based", "exploration", "strategy", "provides", "bonus", "rewards", "inversely", "related", "state", "visitation", "counts", "encouraging", "agent", "visit", "less-explored"]}, {"id": "term-counterfactual", "t": "Counterfactual", "tg": ["Concept", "Reasoning"], "d": "general", "x": "\"What if\" reasoning about alternative scenarios. Used in explainability (\"the prediction would change if...\") and for...", "l": "c", "k": ["counterfactual", "reasoning", "alternative", "scenarios", "explainability", "prediction", "change", "evaluating", "causal", "relationships", "data"]}, {"id": "term-counterfactual-explanation", "t": "Counterfactual Explanation", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "An explanation that describes the smallest change to the input features that would alter the model's prediction to a...", "l": "c", "k": ["counterfactual", "explanation", "describes", "smallest", "change", "input", "features", "alter", "model", "prediction", "desired", "outcome", "providing", "actionable", "insights"]}, {"id": "term-counterfactual-fairness", "t": "Counterfactual Fairness", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness criterion requiring that a decision would remain the same in a counterfactual world where an individual's...", "l": "c", "k": ["counterfactual", "fairness", "criterion", "requiring", "decision", "remain", "world", "individual", "protected", "attribute", "different", "grounded", "causal", "reasoning"]}, {"id": "term-covariance", "t": "Covariance", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A measure of the joint variability of two random variables, indicating the direction of their linear relationship....", "l": "c", "k": ["covariance", "measure", "joint", "variability", "random", "variables", "indicating", "direction", "linear", "relationship", "positive", "means", "tend", "increase", "together"]}, {"id": "term-covariance-matrix", "t": "Covariance Matrix", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A symmetric matrix whose entries are the pairwise covariances between all pairs of variables in a dataset. The diagonal...", "l": "c", "k": ["covariance", "matrix", "symmetric", "whose", "entries", "pairwise", "covariances", "pairs", "variables", "dataset", "diagonal", "variances", "individual"]}, {"id": "term-covariate-shift", "t": "Covariate Shift", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A type of dataset shift where the distribution of input features changes between training and deployment while the...", "l": "c", "k": ["covariate", "shift", "type", "dataset", "distribution", "input", "features", "changes", "training", "deployment", "conditional", "target", "given", "inputs", "remains"]}, {"id": "term-coverage", "t": "Coverage", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation metric that measures the proportion of reference content or ground truth items that are represented in...", "l": "c", "k": ["coverage", "evaluation", "metric", "measures", "proportion", "reference", "content", "ground", "truth", "items", "represented", "model", "output", "assessing", "completeness"]}, {"id": "term-cox-proportional-hazards", "t": "Cox Proportional Hazards", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A semi-parametric survival model that estimates the effect of covariates on the hazard rate without specifying the...", "l": "c", "k": ["cox", "proportional", "hazards", "semi-parametric", "survival", "model", "estimates", "effect", "covariates", "hazard", "rate", "without", "specifying", "baseline", "function"]}, {"id": "term-cpu-inference", "t": "CPU Inference", "tg": ["Deployment", "Hardware"], "d": "hardware", "x": "Running AI models on CPUs rather than GPUs. Slower but more accessible. Quantized models can run efficiently on CPUs...", "l": "c", "k": ["cpu", "inference", "running", "models", "cpus", "rather", "gpus", "slower", "accessible", "quantized", "run", "efficiently", "edge", "deployment"]}, {"id": "term-cramer-rao-lower-bound", "t": "Cramer-Rao Lower Bound", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A theoretical lower bound on the variance of any unbiased estimator of a parameter, computed as the inverse of the...", "l": "c", "k": ["cramer-rao", "lower", "bound", "theoretical", "variance", "unbiased", "estimator", "parameter", "computed", "inverse", "fisher", "information"]}, {"id": "term-cray-supercomputers", "t": "Cray Supercomputers", "tg": ["History", "Systems"], "d": "history", "x": "A series of supercomputers designed by Seymour Cray beginning with the Cray-1 in 1976. While not specifically AI...", "l": "c", "k": ["cray", "supercomputers", "series", "designed", "seymour", "beginning", "cray-1", "specifically", "systems", "provided", "computational", "power", "needed", "large-scale", "scientific"]}, {"id": "term-creative-prompting", "t": "Creative Prompting", "tg": ["Prompt Engineering", "Creative"], "d": "general", "x": "Prompting techniques specifically designed to elicit imaginative, original, and artistically expressive outputs from...", "l": "c", "k": ["creative", "prompting", "techniques", "specifically", "designed", "elicit", "imaginative", "original", "artistically", "expressive", "outputs", "language", "models", "higher", "temperature"]}, {"id": "term-creative-writing", "t": "Creative Writing (AI)", "tg": ["Application", "Creative"], "d": "general", "x": "Using AI to generate fiction, poetry, scripts, and other creative content. Effective creative prompting often uses...", "l": "c", "k": ["creative", "writing", "generate", "fiction", "poetry", "scripts", "content", "effective", "prompting", "uses", "crispe", "examples", "establish", "tone", "style"]}, {"id": "term-credible-interval", "t": "Credible Interval", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A Bayesian analog of the confidence interval, representing the range within which a parameter falls with a specified...", "l": "c", "k": ["credible", "interval", "bayesian", "analog", "confidence", "representing", "range", "within", "parameter", "falls", "specified", "probability", "given", "observed", "data"]}, {"id": "term-credit-assignment", "t": "Credit Assignment Problem", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The challenge of determining which actions in a sequence were responsible for a delayed reward signal. Credit...", "l": "c", "k": ["credit", "assignment", "problem", "challenge", "determining", "actions", "sequence", "were", "responsible", "delayed", "reward", "signal", "fundamental", "becomes", "harder"]}, {"id": "term-crisp", "t": "CRISP", "tg": ["Framework", "General Purpose"], "d": "general", "x": "A prompting framework: Context, Role, Instructions, Specifics, Parameters. A versatile method for everyday AI tasks and...", "l": "c", "k": ["crisp", "prompting", "framework", "context", "role", "instructions", "specifics", "parameters", "versatile", "method", "everyday", "tasks", "requests"]}, {"id": "term-crispe", "t": "CRISPE", "tg": ["Framework", "Creative"], "d": "general", "x": "An extension of CRISP that adds Examples for few-shot learning. Particularly useful for creative tasks where showing is...", "l": "c", "k": ["crispe", "extension", "crisp", "adds", "examples", "few-shot", "learning", "particularly", "useful", "creative", "tasks", "showing", "better", "telling"]}, {"id": "term-crop-and-resize", "t": "Crop-and-Resize", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A spatial transformation operation used in object detection that extracts and resizes region proposals from feature...", "l": "c", "k": ["crop-and-resize", "spatial", "transformation", "operation", "object", "detection", "extracts", "resizes", "region", "proposals", "feature", "maps", "bilinear", "sampling", "serving"]}, {"id": "term-cross-attention", "t": "Cross-Attention", "tg": ["Architecture", "Transformers"], "d": "models", "x": "An attention mechanism where queries come from one sequence and keys/values from another. Essential in encoder-decoder...", "l": "c", "k": ["cross-attention", "attention", "mechanism", "queries", "come", "sequence", "keys", "values", "another", "essential", "encoder-decoder", "models", "multimodal", "systems", "combine"]}, {"id": "term-cross-attention-conditioning", "t": "Cross-Attention Conditioning", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "The mechanism in diffusion models where text embeddings influence image generation through cross-attention layers,...", "l": "c", "k": ["cross-attention", "conditioning", "mechanism", "diffusion", "models", "text", "embeddings", "influence", "image", "generation", "layers", "allowing", "spatial", "region", "generated"]}, {"id": "term-cross-encoder", "t": "Cross-Encoder", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A text similarity model that processes both texts jointly through a single transformer encoder. Produces more accurate...", "l": "c", "k": ["cross-encoder", "text", "similarity", "model", "processes", "texts", "jointly", "single", "transformer", "encoder", "produces", "accurate", "scores", "bi-encoders", "computationally"]}, {"id": "term-cross-encoder-re-ranking", "t": "Cross-Encoder Re-Ranking", "tg": ["Retrieval", "Ranking"], "d": "general", "x": "A re-ranking approach that jointly encodes the query and each candidate document through a single transformer model,...", "l": "c", "k": ["cross-encoder", "re-ranking", "approach", "jointly", "encodes", "query", "candidate", "document", "single", "transformer", "model", "enabling", "rich", "cross-attention", "interactions"]}, {"id": "term-cross-entropy", "t": "Cross-Entropy", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A measure of the difference between two probability distributions. In machine learning used as a loss function...", "l": "c", "k": ["cross-entropy", "measure", "difference", "probability", "distributions", "machine", "learning", "loss", "function", "measuring", "divergence", "predicted", "true", "label", "equivalent"]}, {"id": "term-cross-entropy-loss", "t": "Cross-Entropy Loss", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A loss function that measures the dissimilarity between the predicted probability distribution and the true label...", "l": "c", "k": ["cross-entropy", "loss", "function", "measures", "dissimilarity", "predicted", "probability", "distribution", "true", "label", "standard", "classification", "tasks", "equals", "negative"]}, {"id": "term-cross-entropy-method-rl", "t": "Cross-Entropy Method in RL", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An evolutionary optimization approach for RL that samples multiple policies, evaluates their returns, and updates the...", "l": "c", "k": ["cross-entropy", "method", "evolutionary", "optimization", "approach", "samples", "multiple", "policies", "evaluates", "returns", "updates", "sampling", "distribution", "toward", "elite"]}, {"id": "term-cross-layer-parameter-sharing", "t": "Cross-Layer Parameter Sharing", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A technique where multiple transformer layers share the same weight parameters, dramatically reducing model size while...", "l": "c", "k": ["cross-layer", "parameter", "sharing", "technique", "multiple", "transformer", "layers", "share", "weight", "parameters", "dramatically", "reducing", "model", "size", "maintaining"]}, {"id": "term-cross-lingual-embedding", "t": "Cross-Lingual Embedding", "tg": ["NLP", "Embeddings"], "d": "general", "x": "Word or sentence representations that map multiple languages into a shared vector space where semantically equivalent...", "l": "c", "k": ["cross-lingual", "embedding", "word", "sentence", "representations", "map", "multiple", "languages", "shared", "vector", "space", "semantically", "equivalent", "expressions", "close"]}, {"id": "term-cross-validation", "t": "Cross-Validation", "tg": ["Evaluation", "Training"], "d": "datasets", "x": "A technique for evaluating model performance by splitting data into multiple subsets, training on some and testing on...", "l": "c", "k": ["cross-validation", "technique", "evaluating", "model", "performance", "splitting", "data", "multiple", "subsets", "training", "testing", "others", "provides", "reliable", "estimates"]}, {"id": "term-crowd-counting", "t": "Crowd Counting", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of estimating the number of people in crowded scenes from images, typically using density map regression to...", "l": "c", "k": ["crowd", "counting", "task", "estimating", "number", "people", "crowded", "scenes", "images", "typically", "density", "map", "regression", "handle", "extreme"]}, {"id": "term-crowdsourcing", "t": "Crowdsourcing", "tg": ["Data", "Process"], "d": "general", "x": "Gathering data labels or human feedback from many workers. Platforms like Amazon MTurk provide annotations for training...", "l": "c", "k": ["crowdsourcing", "gathering", "data", "labels", "human", "feedback", "workers", "platforms", "amazon", "mturk", "provide", "annotations", "training", "rlhf", "preference"]}, {"id": "term-ctc-loss", "t": "CTC Loss", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Connectionist Temporal Classification is a loss function for training sequence-to-sequence models when the alignment...", "l": "c", "k": ["ctc", "loss", "connectionist", "temporal", "classification", "function", "training", "sequence-to-sequence", "models", "alignment", "input", "output", "unknown", "marginalizes", "possible"]}, {"id": "term-cuda", "t": "CUDA", "tg": ["Hardware", "Infrastructure"], "d": "hardware", "x": "NVIDIA's parallel computing platform that enables GPUs to accelerate AI training and inference. Essential...", "l": "c", "k": ["cuda", "nvidia", "parallel", "computing", "platform", "enables", "gpus", "accelerate", "training", "inference", "essential", "infrastructure", "deep", "learning", "allowing"]}, {"id": "term-cuda-cores", "t": "CUDA Cores", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "The general-purpose parallel processing units in NVIDIA GPUs that execute scalar floating-point and integer operations....", "l": "c", "k": ["cuda", "cores", "general-purpose", "parallel", "processing", "units", "nvidia", "gpus", "execute", "scalar", "floating-point", "integer", "operations", "less", "specialized"]}, {"id": "term-cuda-programming", "t": "CUDA Programming", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "NVIDIA's parallel computing platform and API that enables direct programming of GPU hardware using C/C++ extensions....", "l": "c", "k": ["cuda", "programming", "nvidia", "parallel", "computing", "platform", "api", "enables", "direct", "gpu", "hardware", "extensions", "provides", "thread", "hierarchy"]}, {"id": "term-cumulative-reasoning", "t": "Cumulative Reasoning", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting paradigm where a proposer generates potential reasoning steps, a verifier checks each step's validity, and...", "l": "c", "k": ["cumulative", "reasoning", "prompting", "paradigm", "proposer", "generates", "potential", "steps", "verifier", "checks", "step", "validity", "reporter", "determines", "sufficient"]}, {"id": "term-curiosity-driven-exploration", "t": "Curiosity-Driven Exploration", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An exploration strategy that rewards the agent for encountering states where its predictive model has high error,...", "l": "c", "k": ["curiosity-driven", "exploration", "strategy", "rewards", "agent", "encountering", "states", "predictive", "model", "high", "error", "encouraging", "visits", "novel", "informative"]}, {"id": "term-curriculum-learning", "t": "Curriculum Learning", "tg": ["Training", "Technique"], "d": "general", "x": "Training models on progressively harder examples, mimicking human education. Can improve learning efficiency and final...", "l": "c", "k": ["curriculum", "learning", "training", "models", "progressively", "harder", "examples", "mimicking", "human", "education", "improve", "efficiency", "final", "performance", "compared"]}, {"id": "term-curriculum-learning-rl", "t": "Curriculum Learning in RL", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A training strategy that presents tasks to an RL agent in a structured order of increasing difficulty, enabling the...", "l": "c", "k": ["curriculum", "learning", "training", "strategy", "presents", "tasks", "agent", "structured", "order", "increasing", "difficulty", "enabling", "build", "skills", "progressively"]}, {"id": "term-curse-of-dimensionality", "t": "Curse of Dimensionality", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "The phenomenon where the performance of many algorithms degrades as the number of features increases, because data...", "l": "c", "k": ["curse", "dimensionality", "phenomenon", "performance", "algorithms", "degrades", "number", "features", "increases", "data", "becomes", "sparse", "high-dimensional", "spaces", "distances"]}, {"id": "term-cursor", "t": "Cursor", "tg": ["Product", "IDE"], "d": "general", "x": "An AI-powered code editor built on VS Code, designed for AI-first development. Features include AI chat, code...", "l": "c", "k": ["cursor", "ai-powered", "code", "editor", "built", "designed", "ai-first", "development", "features", "include", "chat", "generation", "understanding", "entire", "codebases"]}, {"id": "term-custom-instructions", "t": "Custom Instructions", "tg": ["Feature", "Personalization"], "d": "general", "x": "Persistent preferences that shape all AI responses in ChatGPT and similar products. Set once and applied automatically...", "l": "c", "k": ["custom", "instructions", "persistent", "preferences", "shape", "responses", "chatgpt", "similar", "products", "applied", "automatically", "conversation"]}, {"id": "term-cutmix", "t": "CutMix", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "An augmentation strategy that replaces a rectangular region of one training image with a patch from another image and...", "l": "c", "k": ["cutmix", "augmentation", "strategy", "replaces", "rectangular", "region", "training", "image", "patch", "another", "proportionally", "mixes", "labels", "combining", "benefits"]}, {"id": "term-cutoff-date", "t": "Cutoff Date (Knowledge Cutoff)", "tg": ["Limitation", "LLM"], "d": "models", "x": "The date after which an AI model has no training data. Information after this date is unknown to the model unless...", "l": "c", "k": ["cutoff", "date", "knowledge", "model", "training", "data", "information", "unknown", "unless", "provided", "prompt", "accessed", "via", "tools"]}, {"id": "term-cutout", "t": "Cutout", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A data augmentation technique that randomly masks square regions of input images during training. Forces the model to...", "l": "c", "k": ["cutout", "data", "augmentation", "technique", "randomly", "masks", "square", "regions", "input", "images", "training", "forces", "model", "attend", "less"]}, {"id": "term-cutout-augmentation", "t": "Cutout Augmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A regularization technique that randomly masks out square regions of training images, forcing the model to learn from...", "l": "c", "k": ["cutout", "augmentation", "regularization", "technique", "randomly", "masks", "square", "regions", "training", "images", "forcing", "model", "learn", "partial", "information"]}, {"id": "term-cvpr", "t": "CVPR", "tg": ["History", "Conferences"], "d": "history", "x": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition first held in 1983. The premier conference for...", "l": "c", "k": ["cvpr", "ieee", "cvf", "conference", "computer", "vision", "pattern", "recognition", "held", "premier", "research", "breakthrough", "results", "image", "object"]}, {"id": "term-cybernetics", "t": "Cybernetics", "tg": ["History", "Fundamentals"], "d": "history", "x": "An interdisciplinary field founded by Norbert Wiener in 1948 studying regulatory systems their structures constraints...", "l": "c", "k": ["cybernetics", "interdisciplinary", "field", "founded", "norbert", "wiener", "studying", "regulatory", "systems", "structures", "constraints", "possibilities", "examined", "feedback", "loops"]}, {"id": "term-cybernetics-movement", "t": "Cybernetics Movement", "tg": ["History", "Milestones"], "d": "history", "x": "An interdisciplinary field founded in the 1940s by Norbert Wiener studying control, communication, and feedback in...", "l": "c", "k": ["cybernetics", "movement", "interdisciplinary", "field", "founded", "1940s", "norbert", "wiener", "studying", "control", "communication", "feedback", "biological", "mechanical", "systems"]}, {"id": "term-cyc-project", "t": "Cyc Project", "tg": ["History", "Milestones"], "d": "history", "x": "A long-running AI project started by Douglas Lenat in 1984 to create a comprehensive knowledge base of common-sense...", "l": "c", "k": ["cyc", "project", "long-running", "started", "douglas", "lenat", "create", "comprehensive", "knowledge", "base", "common-sense", "facts", "rules", "representing", "ambitious"]}, {"id": "term-cyclegan", "t": "CycleGAN", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An unpaired image-to-image translation model using two generators and discriminators with cycle consistency loss,...", "l": "c", "k": ["cyclegan", "unpaired", "image-to-image", "translation", "model", "generators", "discriminators", "cycle", "consistency", "loss", "enabling", "domain", "transfer", "without", "requiring"]}, {"id": "term-cyclic-learning-rate", "t": "Cyclic Learning Rate", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A learning rate schedule that oscillates the learning rate between minimum and maximum bounds during training. Proposed...", "l": "c", "k": ["cyclic", "learning", "rate", "schedule", "oscillates", "minimum", "maximum", "bounds", "training", "proposed", "smith", "shown", "improve", "convergence", "speed"]}, {"id": "term-cynthia-breazeal", "t": "Cynthia Breazeal", "tg": ["History", "Pioneers"], "d": "history", "x": "American roboticist at MIT who pioneered social robotics and human-robot interaction. Created Kismet (1998) one of the...", "l": "c", "k": ["cynthia", "breazeal", "american", "roboticist", "mit", "pioneered", "social", "robotics", "human-robot", "interaction", "created", "kismet", "robots", "designed", "recognize"]}, {"id": "term-dagger", "t": "DAgger", "tg": ["Reinforcement Learning", "Imitation"], "d": "general", "x": "Dataset Aggregation, an iterative imitation learning algorithm that queries the expert for the correct action at states...", "l": "d", "k": ["dagger", "dataset", "aggregation", "iterative", "imitation", "learning", "algorithm", "queries", "expert", "correct", "action", "states", "visited", "learned", "policy"]}, {"id": "term-dall-e", "t": "DALL-E", "tg": ["Model", "Image Generation"], "d": "models", "x": "OpenAI's image generation model that creates images from text descriptions. Named as a portmanteau of \"Dal\" (the...", "l": "d", "k": ["dall-e", "openai", "image", "generation", "model", "creates", "images", "text", "descriptions", "named", "portmanteau", "dal", "artist", "wall-e", "robot"]}, {"id": "term-dall-e-2", "t": "DALL-E 2", "tg": ["Models", "Technical"], "d": "models", "x": "OpenAI's second generation image generation model that uses CLIP embeddings and a diffusion model to create more...", "l": "d", "k": ["dall-e", "openai", "generation", "image", "model", "uses", "clip", "embeddings", "diffusion", "create", "realistic", "higher", "resolution", "images", "text"]}, {"id": "term-dall-e-3", "t": "DALL-E 3", "tg": ["Models", "Technical"], "d": "models", "x": "OpenAI's third generation text-to-image model featuring significantly improved text rendering prompt following and...", "l": "d", "k": ["dall-e", "openai", "generation", "text-to-image", "model", "featuring", "significantly", "improved", "text", "rendering", "prompt", "following", "coherent", "composition", "integrated"]}, {"id": "term-dall-e-architecture", "t": "DALL-E Architecture", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A two-stage generative architecture that first trains a discrete VAE to compress images into tokens, then trains an...", "l": "d", "k": ["dall-e", "architecture", "two-stage", "generative", "trains", "discrete", "vae", "compress", "images", "tokens", "autoregressive", "transformer", "generate", "image", "conditioned"]}, {"id": "term-dana-scott", "t": "Dana Scott", "tg": ["History", "Pioneers"], "d": "history", "x": "American logician and computer scientist who received the Turing Award in 1976 for work on denotational semantics and...", "l": "d", "k": ["dana", "scott", "american", "logician", "computer", "scientist", "received", "turing", "award", "work", "denotational", "semantics", "domain", "theory", "mathematical"]}, {"id": "term-danny-hillis", "t": "Danny Hillis", "tg": ["History", "Pioneers"], "d": "history", "x": "American inventor scientist and engineer who founded Thinking Machines Corporation and designed the Connection Machine...", "l": "d", "k": ["danny", "hillis", "american", "inventor", "scientist", "engineer", "founded", "thinking", "machines", "corporation", "designed", "connection", "machine", "parallel", "supercomputer"]}, {"id": "term-daphne-koller", "t": "Daphne Koller", "tg": ["History", "Pioneers"], "d": "history", "x": "Israeli-American computer scientist who co-founded Coursera with Andrew Ng. Known for pioneering work on probabilistic...", "l": "d", "k": ["daphne", "koller", "israeli-american", "computer", "scientist", "co-founded", "coursera", "andrew", "known", "pioneering", "work", "probabilistic", "graphical", "models", "applications"]}, {"id": "term-dario-amodei", "t": "Dario Amodei", "tg": ["History", "Pioneers"], "d": "history", "x": "American AI researcher who co-founded Anthropic in 2021 after leaving OpenAI, serving as CEO and advocating for a...", "l": "d", "k": ["dario", "amodei", "american", "researcher", "co-founded", "anthropic", "leaving", "openai", "serving", "ceo", "advocating", "safety-focused", "approach", "development", "including"]}, {"id": "term-darpa-grand-challenge", "t": "DARPA Grand Challenge", "tg": ["History", "Milestones"], "d": "history", "x": "A series of autonomous vehicle competitions organized by DARPA starting in 2004 that spurred development of...", "l": "d", "k": ["darpa", "grand", "challenge", "series", "autonomous", "vehicle", "competitions", "organized", "starting", "spurred", "development", "self-driving", "technology", "won", "stanford"]}, {"id": "term-dartmouth-conference", "t": "Dartmouth Conference", "tg": ["History", "Milestones"], "d": "history", "x": "A 1956 summer workshop at Dartmouth College organized by John McCarthy Marvin Minsky Nathaniel Rochester and Claude...", "l": "d", "k": ["dartmouth", "conference", "summer", "workshop", "college", "organized", "john", "mccarthy", "marvin", "minsky", "nathaniel", "rochester", "claude", "shannon", "widely"]}, {"id": "term-dartmouth-workshop", "t": "Dartmouth Workshop", "tg": ["History", "Milestones"], "d": "history", "x": "The 1956 summer research project at Dartmouth College organized by John McCarthy, Marvin Minsky, Nathaniel Rochester,...", "l": "d", "k": ["dartmouth", "workshop", "summer", "research", "project", "college", "organized", "john", "mccarthy", "marvin", "minsky", "nathaniel", "rochester", "claude", "shannon"]}, {"id": "term-darts", "t": "DARTS", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Differentiable Architecture Search relaxes the discrete architecture search space to be continuous enabling...", "l": "d", "k": ["darts", "differentiable", "architecture", "search", "relaxes", "discrete", "space", "continuous", "enabling", "gradient-based", "optimization", "jointly", "optimizes", "parameters", "network"]}, {"id": "term-data-augmentation", "t": "Data Augmentation", "tg": ["Training", "Data"], "d": "general", "x": "Techniques to artificially expand training datasets by creating modified versions of existing data. For images:...", "l": "d", "k": ["data", "augmentation", "techniques", "artificially", "expand", "training", "datasets", "creating", "modified", "versions", "existing", "images", "rotation", "flipping", "cropping"]}, {"id": "term-data-colonialism", "t": "Data Colonialism", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "The critique that powerful AI companies extract data from marginalized communities and developing nations without fair...", "l": "d", "k": ["data", "colonialism", "critique", "powerful", "companies", "extract", "marginalized", "communities", "developing", "nations", "without", "fair", "compensation", "representation", "perpetuating"]}, {"id": "term-data-contamination", "t": "Data Contamination", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The unintentional inclusion of test or evaluation data in a model's training set, which inflates benchmark scores and...", "l": "d", "k": ["data", "contamination", "unintentional", "inclusion", "test", "evaluation", "model", "training", "inflates", "benchmark", "scores", "gives", "misleading", "picture", "true"]}, {"id": "term-data-drift", "t": "Data Drift", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A change in the statistical properties of the input data over time that can degrade model performance. Types include...", "l": "d", "k": ["data", "drift", "change", "statistical", "properties", "input", "time", "degrade", "model", "performance", "types", "include", "covariate", "shift", "prior"]}, {"id": "term-data-leakage", "t": "Data Leakage", "tg": ["Training", "Pitfall"], "d": "general", "x": "When information from outside the training set improperly influences the model, leading to overly optimistic...", "l": "d", "k": ["data", "leakage", "information", "outside", "training", "improperly", "influences", "model", "leading", "overly", "optimistic", "performance", "estimates", "common", "mistake"]}, {"id": "term-data-mixture", "t": "Data Mixture", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The proportional composition of different data sources (web text, books, code, conversations) used in pre-training a...", "l": "d", "k": ["data", "mixture", "proportional", "composition", "different", "sources", "web", "text", "books", "code", "conversations", "pre-training", "language", "model", "significantly"]}, {"id": "term-data-parallelism", "t": "Data Parallelism", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A distributed training strategy that replicates the entire model on each GPU and splits the training data across...", "l": "d", "k": ["data", "parallelism", "distributed", "training", "strategy", "replicates", "entire", "model", "gpu", "splits", "across", "replicas", "synchronizing", "gradients", "step"]}, {"id": "term-data-preprocessing", "t": "Data Preprocessing", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "The collection of techniques applied to raw data before model training, including cleaning, handling missing values,...", "l": "d", "k": ["data", "preprocessing", "collection", "techniques", "applied", "raw", "model", "training", "including", "cleaning", "handling", "missing", "values", "encoding", "categorical"]}, {"id": "term-data-preprocessing-images", "t": "Data Preprocessing for Images", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The standardization pipeline applied to images before model training or inference, including resizing, normalization to...", "l": "d", "k": ["data", "preprocessing", "images", "standardization", "pipeline", "applied", "model", "training", "inference", "including", "resizing", "normalization", "specific", "mean", "std"]}, {"id": "term-data-privacy", "t": "Data Privacy (AI)", "tg": ["Ethics", "Safety"], "d": "safety", "x": "Concerns and practices around protecting personal information when using AI systems. Includes what data is collected...", "l": "d", "k": ["data", "privacy", "concerns", "practices", "around", "protecting", "personal", "information", "systems", "includes", "collected", "interactions", "stored", "training"]}, {"id": "term-data-privacy-in-ai", "t": "Data Privacy in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "Concerns about the collection storage and use of personal data in AI systems. Issues include training data consent...", "l": "d", "k": ["data", "privacy", "concerns", "collection", "storage", "personal", "systems", "issues", "include", "training", "consent", "model", "memorization", "tension", "large-scale"]}, {"id": "term-data-sovereignty", "t": "Data Sovereignty", "tg": ["Privacy", "Governance"], "d": "safety", "x": "The principle that data is subject to the laws and governance structures of the nation or community where it is...", "l": "d", "k": ["data", "sovereignty", "principle", "subject", "laws", "governance", "structures", "nation", "community", "collected", "resides", "giving", "jurisdictions", "control", "citizens"]}, {"id": "term-dataset", "t": "Dataset", "tg": ["Data", "Fundamentals"], "d": "general", "x": "A collection of data used for training, validating, or testing AI models. Quality and diversity of datasets...", "l": "d", "k": ["dataset", "collection", "data", "training", "validating", "testing", "models", "quality", "diversity", "datasets", "significantly", "impact", "model", "performance", "fairness"]}, {"id": "term-datasheets-for-datasets", "t": "Datasheets for Datasets", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "Standardized documentation proposed by Gebru et al. (2021) that accompanies ML datasets, describing their motivation,...", "l": "d", "k": ["datasheets", "datasets", "standardized", "documentation", "proposed", "gebru", "accompanies", "describing", "motivation", "composition", "collection", "process", "preprocessing", "intended", "uses"]}, {"id": "term-david-rumelhart", "t": "David Rumelhart", "tg": ["History", "Pioneers"], "d": "history", "x": "American psychologist and computer scientist (1942-2011) who, with Hinton and Williams, popularized backpropagation for...", "l": "d", "k": ["david", "rumelhart", "american", "psychologist", "computer", "scientist", "1942-2011", "hinton", "williams", "popularized", "backpropagation", "neural", "networks", "co-edited", "influential"]}, {"id": "term-david-silver", "t": "David Silver", "tg": ["History", "Pioneers"], "d": "history", "x": "British computer scientist at DeepMind who led the development of AlphaGo AlphaZero and other game-playing AI systems....", "l": "d", "k": ["david", "silver", "british", "computer", "scientist", "deepmind", "led", "development", "alphago", "alphazero", "game-playing", "systems", "ucl", "reinforcement", "learning"]}, {"id": "term-dbscan", "t": "DBSCAN", "tg": ["Machine Learning", "Clustering"], "d": "general", "x": "Density-Based Spatial Clustering of Applications with Noise, an algorithm that groups together points that are closely...", "l": "d", "k": ["dbscan", "density-based", "spatial", "clustering", "applications", "noise", "algorithm", "groups", "together", "points", "closely", "packed", "based", "distance", "threshold"]}, {"id": "term-ddim", "t": "DDIM", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "Denoising Diffusion Implicit Models, a deterministic sampling variant of DDPM that skips intermediate diffusion steps,...", "l": "d", "k": ["ddim", "denoising", "diffusion", "implicit", "models", "deterministic", "sampling", "variant", "ddpm", "skips", "intermediate", "steps", "enabling", "faster", "image"]}, {"id": "term-deadly-triad", "t": "Deadly Triad", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "The combination of function approximation, bootstrapping, and off-policy learning that can cause divergence in RL...", "l": "d", "k": ["deadly", "triad", "combination", "function", "approximation", "bootstrapping", "off-policy", "learning", "cause", "divergence", "algorithms", "highlights", "fundamental", "instability", "issues"]}, {"id": "term-debate-as-alignment", "t": "Debate as Alignment", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "An AI safety technique proposed by Irving et al. where two AI agents debate each other on a question and a human judge...", "l": "d", "k": ["debate", "alignment", "safety", "technique", "proposed", "irving", "agents", "question", "human", "judge", "selects", "winner", "incentivizing", "truthful", "well-reasoned"]}, {"id": "term-debate-prompting", "t": "Debate Prompting", "tg": ["Prompt Engineering", "Multi-Agent"], "d": "general", "x": "A prompting strategy that instructs two or more simulated agents to argue opposing positions on a question, then uses...", "l": "d", "k": ["debate", "prompting", "strategy", "instructs", "simulated", "agents", "argue", "opposing", "positions", "question", "uses", "surface", "stronger", "reasoning", "reach"]}, {"id": "term-deberta", "t": "DeBERTa", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Decoding-enhanced BERT with disentangled attention, which improves BERT and RoBERTa by using separate vectors for...", "l": "d", "k": ["deberta", "decoding-enhanced", "bert", "disentangled", "attention", "improves", "roberta", "separate", "vectors", "content", "position", "enhanced", "mask", "decoder", "pretraining"]}, {"id": "term-debugging-prompting", "t": "Debugging Prompting", "tg": ["Prompt Engineering", "Code"], "d": "general", "x": "A prompting approach that provides buggy code along with error messages or test failures and instructs the model to...", "l": "d", "k": ["debugging", "prompting", "approach", "provides", "buggy", "code", "along", "error", "messages", "test", "failures", "instructs", "model", "systematically", "identify"]}, {"id": "term-deceptive-alignment", "t": "Deceptive Alignment", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "A hypothesized failure mode where a mesa-optimizer learns to behave as if aligned during training in order to be...", "l": "d", "k": ["deceptive", "alignment", "hypothesized", "failure", "mode", "mesa-optimizer", "learns", "behave", "aligned", "training", "order", "deployed", "pursues", "misaligned", "objective"]}, {"id": "term-decision-boundary", "t": "Decision Boundary", "tg": ["ML Concept", "Classification"], "d": "general", "x": "The line or surface that separates different classes in a classification model. The shape and complexity of decision...", "l": "d", "k": ["decision", "boundary", "line", "surface", "separates", "different", "classes", "classification", "model", "shape", "complexity", "boundaries", "determine", "patterns", "learn"]}, {"id": "term-decision-transformer", "t": "Decision Transformer", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "An approach that frames RL as a sequence modeling problem, using a transformer architecture to predict actions...", "l": "d", "k": ["decision", "transformer", "approach", "frames", "sequence", "modeling", "problem", "architecture", "predict", "actions", "conditioned", "desired", "returns", "past", "states"]}, {"id": "term-decision-tree", "t": "Decision Tree", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A non-parametric supervised learning model that recursively partitions the feature space using threshold-based...", "l": "d", "k": ["decision", "tree", "non-parametric", "supervised", "learning", "model", "recursively", "partitions", "feature", "space", "threshold-based", "splitting", "rules", "forming", "structure"]}, {"id": "term-decision-trees-history", "t": "Decision Trees History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of decision tree learning from early work by Earl Hunt (1960s) through ID3 (Quinlan 1986) C4.5 (Quinlan...", "l": "d", "k": ["decision", "trees", "history", "development", "tree", "learning", "early", "work", "earl", "hunt", "1960s", "id3", "quinlan", "cart", "breiman"]}, {"id": "term-decode-phase", "t": "Decode Phase", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The autoregressive generation phase of LLM inference where tokens are produced one at a time, each requiring a full...", "l": "d", "k": ["decode", "phase", "autoregressive", "generation", "llm", "inference", "tokens", "produced", "time", "requiring", "full", "model", "forward", "pass", "memory-bandwidth-bound"]}, {"id": "term-decoder", "t": "Decoder", "tg": ["Architecture", "Transformers"], "d": "models", "x": "The component of a neural network that generates output from encoded representations. In transformers, decoder-only...", "l": "d", "k": ["decoder", "component", "neural", "network", "generates", "output", "encoded", "representations", "transformers", "decoder-only", "models", "gpt", "generate", "text", "autoregressively"]}, {"id": "term-decoder-only-architecture", "t": "Decoder-Only Architecture", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer design using only masked self-attention decoder blocks, where the model generates output autoregressively...", "l": "d", "k": ["decoder-only", "architecture", "transformer", "design", "masked", "self-attention", "decoder", "blocks", "model", "generates", "output", "autoregressively", "conditioning", "previous", "tokens"]}, {"id": "term-decomposed-prompting", "t": "Decomposed Prompting", "tg": ["Prompt Engineering", "Decomposition"], "d": "general", "x": "A framework that decomposes complex tasks into simpler sub-tasks, each handled by specialized sub-prompt handlers,...", "l": "d", "k": ["decomposed", "prompting", "framework", "decomposes", "complex", "tasks", "simpler", "sub-tasks", "handled", "specialized", "sub-prompt", "handlers", "enabling", "modular", "problem-solving"]}, {"id": "term-deduplication", "t": "Deduplication", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The process of removing duplicate or near-duplicate documents from training data to improve model quality, reduce...", "l": "d", "k": ["deduplication", "process", "removing", "duplicate", "near-duplicate", "documents", "training", "data", "improve", "model", "quality", "reduce", "memorization", "ensure", "benchmark"]}, {"id": "term-deep-belief-network", "t": "Deep Belief Network", "tg": ["Models", "History"], "d": "models", "x": "A generative model composed of multiple stacked restricted Boltzmann machines. Trained layer by layer in an...", "l": "d", "k": ["deep", "belief", "network", "generative", "model", "composed", "multiple", "stacked", "restricted", "boltzmann", "machines", "trained", "layer", "unsupervised", "manner"]}, {"id": "term-deep-blue", "t": "Deep Blue", "tg": ["History", "Milestones"], "d": "history", "x": "An IBM chess-playing computer that became the first machine to defeat a reigning world chess champion in a full match...", "l": "d", "k": ["deep", "blue", "ibm", "chess-playing", "computer", "became", "machine", "defeat", "reigning", "world", "chess", "champion", "full", "match", "beat"]}, {"id": "term-deep-boltzmann-machine", "t": "Deep Boltzmann Machine", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A multi-layer generative model composed of stacked Restricted Boltzmann Machines with undirected connections between...", "l": "d", "k": ["deep", "boltzmann", "machine", "multi-layer", "generative", "model", "composed", "stacked", "restricted", "machines", "undirected", "connections", "adjacent", "layers", "capable"]}, {"id": "term-ddpg", "t": "Deep Deterministic Policy Gradient (DDPG)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An off-policy actor-critic algorithm for continuous action spaces that combines DPG with deep neural networks,...", "l": "d", "k": ["deep", "deterministic", "policy", "gradient", "ddpg", "off-policy", "actor-critic", "algorithm", "continuous", "action", "spaces", "combines", "dpg", "neural", "networks"]}, {"id": "term-deep-learning", "t": "Deep Learning", "tg": ["Field", "Neural Networks"], "d": "models", "x": "A subset of machine learning using neural networks with many layers (\"deep\" networks). Enables learning complex...", "l": "d", "k": ["deep", "learning", "subset", "machine", "neural", "networks", "layers", "enables", "complex", "patterns", "representations", "large", "amounts", "data"]}, {"id": "term-deep-learning-book", "t": "Deep Learning Book", "tg": ["History", "Milestones"], "d": "history", "x": "A comprehensive textbook on deep learning by Ian Goodfellow Yoshua Bengio and Aaron Courville published in 2016. The...", "l": "d", "k": ["deep", "learning", "book", "comprehensive", "textbook", "ian", "goodfellow", "yoshua", "bengio", "aaron", "courville", "published", "covers", "mathematical", "foundations"]}, {"id": "term-deep-learning-breakthrough-2012", "t": "Deep Learning Breakthrough 2012", "tg": ["History", "Milestones"], "d": "history", "x": "The watershed moment when AlexNet dramatically won the ImageNet competition in 2012, demonstrating that deep...", "l": "d", "k": ["deep", "learning", "breakthrough", "watershed", "moment", "alexnet", "dramatically", "won", "imagenet", "competition", "demonstrating", "convolutional", "neural", "networks", "trained"]}, {"id": "term-deep-q-network", "t": "Deep Q-Network", "tg": ["History", "Milestones"], "d": "history", "x": "A deep reinforcement learning architecture developed by DeepMind in 2013-2015 that combined Q-learning with deep neural...", "l": "d", "k": ["deep", "q-network", "reinforcement", "learning", "architecture", "developed", "deepmind", "2013-2015", "combined", "q-learning", "neural", "networks", "master", "atari", "games"]}, {"id": "term-dqn", "t": "Deep Q-Network (DQN)", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A deep RL algorithm that approximates the Q-function using a neural network, stabilized by experience replay and a...", "l": "d", "k": ["deep", "q-network", "dqn", "algorithm", "approximates", "q-function", "neural", "network", "stabilized", "experience", "replay", "separate", "target", "demonstrated", "superhuman"]}, {"id": "term-deepfake", "t": "Deepfake", "tg": ["Risk", "Ethics"], "d": "safety", "x": "AI-generated synthetic media where a person's likeness is replaced or manipulated. Raises concerns about...", "l": "d", "k": ["deepfake", "ai-generated", "synthetic", "media", "person", "likeness", "replaced", "manipulated", "raises", "concerns", "misinformation", "consent", "authenticity", "digital", "content"]}, {"id": "term-deepfake-detection", "t": "Deepfake Detection", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The set of techniques and tools used to identify synthetically generated or manipulated media, including analysis of...", "l": "d", "k": ["deepfake", "detection", "techniques", "tools", "identify", "synthetically", "generated", "manipulated", "media", "including", "analysis", "facial", "inconsistencies", "temporal", "artifacts"]}, {"id": "term-deepfloyd-if", "t": "DeepFloyd IF", "tg": ["Models", "Technical"], "d": "models", "x": "A modular text-to-image model that operates in pixel space using a frozen text encoder and cascaded diffusion modules....", "l": "d", "k": ["deepfloyd", "modular", "text-to-image", "model", "operates", "pixel", "space", "frozen", "text", "encoder", "cascaded", "diffusion", "modules", "achieves", "high"]}, {"id": "term-deeplab", "t": "DeepLab", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A family of semantic segmentation architectures that use atrous (dilated) convolutions and atrous spatial pyramid...", "l": "d", "k": ["deeplab", "family", "semantic", "segmentation", "architectures", "atrous", "dilated", "convolutions", "spatial", "pyramid", "pooling", "capture", "multi-scale", "context", "without"]}, {"id": "term-deepmind", "t": "DeepMind", "tg": ["Company", "Research"], "d": "general", "x": "Google's AI research lab known for breakthroughs like AlphaGo, AlphaFold, and Gemini. Pioneers in reinforcement...", "l": "d", "k": ["deepmind", "google", "research", "lab", "known", "breakthroughs", "alphago", "alphafold", "gemini", "pioneers", "reinforcement", "learning", "game-playing", "scientific", "applications"]}, {"id": "term-deepmind-founded", "t": "DeepMind Founded", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of DeepMind Technologies in London in 2010 by Demis Hassabis Shane Legg and Mustafa Suleyman. The company...", "l": "d", "k": ["deepmind", "founded", "founding", "technologies", "london", "demis", "hassabis", "shane", "legg", "mustafa", "suleyman", "company", "acquired", "google", "approximately"]}, {"id": "term-deepmind-founding", "t": "DeepMind Founding", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of DeepMind Technologies in London in 2010 by Demis Hassabis, Shane Legg, and Mustafa Suleyman, which was...", "l": "d", "k": ["deepmind", "founding", "technologies", "london", "demis", "hassabis", "shane", "legg", "mustafa", "suleyman", "acquired", "google", "approximately", "million", "dollars"]}, {"id": "term-deepseek", "t": "DeepSeek", "tg": ["Company", "Model"], "d": "models", "x": "A Chinese AI company known for efficient, high-performing open models. Their DeepSeek-V2 and DeepSeek-Coder models...", "l": "d", "k": ["deepseek", "chinese", "company", "known", "efficient", "high-performing", "open", "models", "deepseek-v2", "deepseek-coder", "demonstrate", "competitive", "performance", "lower", "computational"]}, {"id": "term-deepseek-r1", "t": "DeepSeek-R1", "tg": ["Models", "Technical"], "d": "models", "x": "A reasoning model by DeepSeek that achieves strong performance on mathematical and logical reasoning through...", "l": "d", "k": ["deepseek-r1", "reasoning", "model", "deepseek", "achieves", "strong", "performance", "mathematical", "logical", "reinforcement", "learning", "without", "supervised", "fine-tuning", "chain-of-thought"]}, {"id": "term-deepsort", "t": "DeepSORT", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "An extension of the SORT tracker that incorporates deep appearance features alongside motion information for data...", "l": "d", "k": ["deepsort", "extension", "sort", "tracker", "incorporates", "deep", "appearance", "features", "alongside", "motion", "information", "data", "association", "significantly", "reducing"]}, {"id": "term-deepspeed", "t": "DeepSpeed", "tg": ["LLM", "Inference"], "d": "models", "x": "A Microsoft deep learning optimization library that provides ZeRO-based training, inference optimization, and model...", "l": "d", "k": ["deepspeed", "microsoft", "deep", "learning", "optimization", "library", "provides", "zero-based", "training", "inference", "model", "compression", "techniques", "efficiently", "deploying"]}, {"id": "term-deepwalk", "t": "DeepWalk", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A graph embedding algorithm that learns node representations by treating truncated random walks on graphs as sentences...", "l": "d", "k": ["deepwalk", "graph", "embedding", "algorithm", "learns", "node", "representations", "treating", "truncated", "random", "walks", "graphs", "sentences", "applying", "word2vec"]}, {"id": "term-deformable-attention", "t": "Deformable Attention", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "An attention mechanism that attends to a small set of sampling points around a reference point with learnable offsets,...", "l": "d", "k": ["deformable", "attention", "mechanism", "attends", "small", "sampling", "points", "around", "reference", "point", "learnable", "offsets", "dramatically", "reducing", "computational"]}, {"id": "term-deformable-convolution", "t": "Deformable Convolution", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A convolution operation where the sampling grid positions are augmented with learned offsets, allowing the network to...", "l": "d", "k": ["deformable", "convolution", "operation", "sampling", "grid", "positions", "augmented", "learned", "offsets", "allowing", "network", "adaptively", "adjust", "receptive", "field"]}, {"id": "term-deit", "t": "DeiT", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Data-efficient Image Transformer, a vision transformer training methodology that uses knowledge distillation and strong...", "l": "d", "k": ["deit", "data-efficient", "image", "transformer", "vision", "training", "methodology", "uses", "knowledge", "distillation", "strong", "data", "augmentation", "achieve", "competitive"]}, {"id": "term-delimiter", "t": "Delimiter", "tg": ["Prompting", "Technique"], "d": "general", "x": "Characters or symbols used in prompts to clearly separate different sections or types of content. Examples include...", "l": "d", "k": ["delimiter", "characters", "symbols", "prompts", "clearly", "separate", "different", "sections", "types", "content", "examples", "include", "triple", "backticks", "xml-style"]}, {"id": "term-demis-hassabis", "t": "Demis Hassabis", "tg": ["History", "Pioneers"], "d": "history", "x": "British AI researcher and neuroscientist who co-founded DeepMind in 2010, led the development of AlphaGo and AlphaFold,...", "l": "d", "k": ["demis", "hassabis", "british", "researcher", "neuroscientist", "co-founded", "deepmind", "led", "development", "alphago", "alphafold", "serves", "ceo", "google", "won"]}, {"id": "term-demographic-parity", "t": "Demographic Parity", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness metric requiring that the probability of a positive outcome is equal across all protected groups. Also known...", "l": "d", "k": ["demographic", "parity", "fairness", "metric", "requiring", "probability", "positive", "outcome", "equal", "across", "protected", "groups", "known", "statistical", "mandates"]}, {"id": "term-dendral", "t": "DENDRAL", "tg": ["History", "Milestones"], "d": "history", "x": "One of the first expert systems, developed at Stanford in the 1960s-1970s by Edward Feigenbaum and Joshua Lederberg,...", "l": "d", "k": ["dendral", "expert", "systems", "developed", "stanford", "1960s-1970s", "edward", "feigenbaum", "joshua", "lederberg", "identified", "chemical", "compounds", "mass", "spectrometry"]}, {"id": "term-denoising-autoencoder", "t": "Denoising Autoencoder", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An autoencoder variant trained to reconstruct clean data from corrupted inputs, learning robust feature representations...", "l": "d", "k": ["denoising", "autoencoder", "variant", "trained", "reconstruct", "clean", "data", "corrupted", "inputs", "learning", "robust", "feature", "representations", "forcing", "network"]}, {"id": "term-ddpm", "t": "Denoising Diffusion Probabilistic Model", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative model that learns to reverse a gradual noising process, generating samples by iteratively denoising from...", "l": "d", "k": ["denoising", "diffusion", "probabilistic", "model", "generative", "learns", "reverse", "gradual", "noising", "process", "generating", "samples", "iteratively", "pure", "gaussian"]}, {"id": "term-dense-connection", "t": "Dense Connection", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A network pattern where each layer receives inputs from all preceding layers. Used in DenseNet architectures....", "l": "d", "k": ["dense", "connection", "network", "pattern", "layer", "receives", "inputs", "preceding", "layers", "densenet", "architectures", "encourages", "feature", "reuse", "reduces"]}, {"id": "term-dense-passage-retriever", "t": "Dense Passage Retriever", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "A bi-encoder retrieval model (DPR) that trains separate BERT-based encoders for queries and passages using contrastive...", "l": "d", "k": ["dense", "passage", "retriever", "bi-encoder", "retrieval", "model", "dpr", "trains", "separate", "bert-based", "encoders", "queries", "passages", "contrastive", "learning"]}, {"id": "term-dense-prediction", "t": "Dense Prediction", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Computer vision tasks that require producing an output for every pixel in an input image, including semantic...", "l": "d", "k": ["dense", "prediction", "computer", "vision", "tasks", "require", "producing", "output", "pixel", "input", "image", "including", "semantic", "segmentation", "depth"]}, {"id": "term-dense-retrieval", "t": "Dense Retrieval", "tg": ["Retrieval", "Search"], "d": "general", "x": "An information retrieval approach that represents queries and documents as dense continuous vectors from neural...", "l": "d", "k": ["dense", "retrieval", "information", "approach", "represents", "queries", "documents", "continuous", "vectors", "neural", "encoders", "retrieves", "candidates", "based", "vector"]}, {"id": "term-dense-reward", "t": "Dense Reward", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "A reward structure that provides frequent, informative feedback at nearly every time step, guiding the agent more...", "l": "d", "k": ["dense", "reward", "structure", "provides", "frequent", "informative", "feedback", "nearly", "time", "step", "guiding", "agent", "directly", "toward", "desired"]}, {"id": "term-dense-sparse-hybrid", "t": "Dense-Sparse Hybrid", "tg": ["Retrieval", "Search"], "d": "general", "x": "A retrieval strategy that fuses results from both dense vector search and sparse lexical search, typically using...", "l": "d", "k": ["dense-sparse", "hybrid", "retrieval", "strategy", "fuses", "results", "dense", "vector", "search", "sparse", "lexical", "typically", "reciprocal", "rank", "fusion"]}, {"id": "term-densenet", "t": "DenseNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A CNN architecture where each layer receives feature maps from all preceding layers as input and passes its own feature...", "l": "d", "k": ["densenet", "cnn", "architecture", "layer", "receives", "feature", "maps", "preceding", "layers", "input", "passes", "subsequent", "promoting", "reuse", "reducing"]}, {"id": "term-dependency-parsing", "t": "Dependency Parsing", "tg": ["NLP", "Parsing"], "d": "general", "x": "The task of analyzing the grammatical structure of a sentence by identifying directed relationships between words,...", "l": "d", "k": ["dependency", "parsing", "task", "analyzing", "grammatical", "structure", "sentence", "identifying", "directed", "relationships", "words", "representing", "modify", "depend"]}, {"id": "term-dependency-tree", "t": "Dependency Tree", "tg": ["NLP", "Parsing"], "d": "general", "x": "A directed tree structure representing syntactic dependencies in a sentence where each word is a node and edges...", "l": "d", "k": ["dependency", "tree", "directed", "structure", "representing", "syntactic", "dependencies", "sentence", "word", "node", "edges", "indicate", "grammatical", "relationships", "subject"]}, {"id": "term-deployment-bias", "t": "Deployment Bias", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Bias that emerges when an AI system is used in contexts or populations that differ from its training conditions,...", "l": "d", "k": ["deployment", "bias", "emerges", "system", "contexts", "populations", "differ", "training", "conditions", "including", "shifts", "user", "behavior", "environmental", "population"]}, {"id": "term-depth-anything", "t": "Depth Anything", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A foundation model for monocular depth estimation that produces accurate relative depth maps from single images across...", "l": "d", "k": ["depth", "anything", "foundation", "model", "monocular", "estimation", "produces", "accurate", "relative", "maps", "single", "images", "across", "diverse", "scenes"]}, {"id": "term-depth-estimation", "t": "Depth Estimation", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The task of predicting the distance of each pixel from the camera in a 2D image, producing a dense depth map using...", "l": "d", "k": ["depth", "estimation", "task", "predicting", "distance", "pixel", "camera", "image", "producing", "dense", "map", "monocular", "cues", "learned", "deep"]}, {"id": "term-depthwise-convolution", "t": "Depthwise Convolution", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A convolution that applies a separate filter to each input channel independently, capturing spatial features per...", "l": "d", "k": ["depthwise", "convolution", "applies", "separate", "filter", "input", "channel", "independently", "capturing", "spatial", "features", "per", "without", "mixing", "information"]}, {"id": "term-grouped-convolution", "t": "Depthwise Convolution Variant", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A convolution where input channels are divided into groups and convolution is applied independently within each group,...", "l": "d", "k": ["depthwise", "convolution", "variant", "input", "channels", "divided", "groups", "applied", "independently", "within", "group", "reducing", "parameters", "computation", "proportional"]}, {"id": "term-depthwise-separable-convolution", "t": "Depthwise Separable Convolution", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A factorized convolution that decomposes a standard convolution into a depthwise convolution applied independently per...", "l": "d", "k": ["depthwise", "separable", "convolution", "factorized", "decomposes", "standard", "applied", "independently", "per", "channel", "followed", "pointwise", "1x1", "reducing", "parameters"]}, {"id": "term-description-logics", "t": "Description Logics", "tg": ["History", "Fundamentals"], "d": "history", "x": "A family of formal knowledge representation languages used as the logical basis for ontologies and the Semantic Web....", "l": "d", "k": ["description", "logics", "family", "formal", "knowledge", "representation", "languages", "logical", "basis", "ontologies", "semantic", "web", "provide", "balance", "expressiveness"]}, {"id": "term-deterministic-policy-gradient", "t": "Deterministic Policy Gradient (DPG)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A policy gradient theorem for deterministic policies that computes the gradient of expected return by backpropagating...", "l": "d", "k": ["deterministic", "policy", "gradient", "dpg", "theorem", "policies", "computes", "expected", "return", "backpropagating", "q-function", "respect", "actions", "requires", "single"]}, {"id": "term-deterministic", "t": "Deterministic vs Stochastic", "tg": ["Concept", "LLM"], "d": "models", "x": "Deterministic systems produce the same output for the same input every time. LLMs are typically stochastic (random),...", "l": "d", "k": ["deterministic", "stochastic", "systems", "produce", "output", "input", "time", "llms", "typically", "random", "producing", "varied", "outputs", "unless", "temperature"]}, {"id": "term-detokenization", "t": "Detokenization", "tg": ["NLP", "Tokenization"], "d": "general", "x": "The process of converting a sequence of tokens back into readable text by reversing the tokenization process, handling...", "l": "d", "k": ["detokenization", "process", "converting", "sequence", "tokens", "readable", "text", "reversing", "tokenization", "handling", "subword", "boundaries", "spacing", "special", "characters"]}, {"id": "term-detr", "t": "DETR", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Detection Transformer, an end-to-end object detection model that uses a transformer encoder-decoder architecture with...", "l": "d", "k": ["detr", "detection", "transformer", "end-to-end", "object", "model", "uses", "encoder-decoder", "architecture", "bipartite", "matching", "loss", "eliminating", "need", "hand-designed"]}, {"id": "term-deviance", "t": "Deviance", "tg": ["Statistics", "Metrics"], "d": "datasets", "x": "A goodness-of-fit statistic for generalized linear models, computed as twice the difference in log-likelihoods between...", "l": "d", "k": ["deviance", "goodness-of-fit", "statistic", "generalized", "linear", "models", "computed", "twice", "difference", "log-likelihoods", "fitted", "model", "saturated", "generalizes", "residual"]}, {"id": "term-dgx-system", "t": "DGX System", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "NVIDIA's integrated AI supercomputing platform pre-configured with multiple high-end GPUs, NVLink/NVSwitch...", "l": "d", "k": ["dgx", "system", "nvidia", "integrated", "supercomputing", "platform", "pre-configured", "multiple", "high-end", "gpus", "nvlink", "nvswitch", "interconnects", "optimized", "software"]}, {"id": "term-dialogue-act", "t": "Dialogue Act", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A categorization of the communicative function of an utterance in a conversation, such as question, statement, request,...", "l": "d", "k": ["dialogue", "act", "categorization", "communicative", "function", "utterance", "conversation", "question", "statement", "request", "greeting", "acknowledgment", "system", "design"]}, {"id": "term-dialogue-system", "t": "Dialogue System", "tg": ["Application", "NLP"], "d": "general", "x": "An AI system designed to converse with humans in natural language. Includes task-oriented systems (customer service)...", "l": "d", "k": ["dialogue", "system", "designed", "converse", "humans", "natural", "language", "includes", "task-oriented", "systems", "customer", "service", "open-domain", "chatbots", "general"]}, {"id": "term-dice-loss", "t": "Dice Loss", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A loss function based on the Dice coefficient measuring overlap between predicted and ground truth segmentation masks....", "l": "d", "k": ["dice", "loss", "function", "based", "coefficient", "measuring", "overlap", "predicted", "ground", "truth", "segmentation", "masks", "ranges", "handles", "class"]}, {"id": "term-differencing", "t": "Differencing", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A time series transformation that computes the difference between consecutive observations (or seasonal periods) to...", "l": "d", "k": ["differencing", "time", "series", "transformation", "computes", "difference", "consecutive", "observations", "seasonal", "periods", "achieve", "stationarity", "first-order", "removes", "linear"]}, {"id": "term-differentiable-neural-computer", "t": "Differentiable Neural Computer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An extension of the Neural Turing Machine that adds temporal link tracking and dynamic memory allocation, improving the...", "l": "d", "k": ["differentiable", "neural", "computer", "extension", "turing", "machine", "adds", "temporal", "link", "tracking", "dynamic", "memory", "allocation", "improving", "ability"]}, {"id": "term-differentiable-programming", "t": "Differentiable Programming", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A programming paradigm where programs are differentiable end-to-end enabling gradient-based optimization of arbitrary...", "l": "d", "k": ["differentiable", "programming", "paradigm", "programs", "end-to-end", "enabling", "gradient-based", "optimization", "arbitrary", "computations", "extends", "deep", "learning", "beyond", "standard"]}, {"id": "term-differentiable-rendering", "t": "Differentiable Rendering", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "Rendering techniques where the image formation process is differentiable with respect to scene parameters, enabling...", "l": "d", "k": ["differentiable", "rendering", "techniques", "image", "formation", "process", "respect", "scene", "parameters", "enabling", "gradient-based", "optimization", "geometry", "materials", "lighting"]}, {"id": "term-differential-evolution", "t": "Differential Evolution", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A population-based optimization algorithm that creates new candidates by combining existing ones using vector...", "l": "d", "k": ["differential", "evolution", "population-based", "optimization", "algorithm", "creates", "candidates", "combining", "existing", "ones", "vector", "differences", "effective", "continuous", "problems"]}, {"id": "term-differential-privacy", "t": "Differential Privacy", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "A mathematical framework providing formal guarantees that the output of a computation does not reveal whether any...", "l": "d", "k": ["differential", "privacy", "mathematical", "framework", "providing", "formal", "guarantees", "output", "computation", "reveal", "single", "individual", "data", "included", "input"]}, {"id": "term-differential-technology-development", "t": "Differential Technology Development", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "The strategic prioritization of developing defensive and safety technologies ahead of potentially dangerous...", "l": "d", "k": ["differential", "technology", "development", "strategic", "prioritization", "developing", "defensive", "safety", "technologies", "ahead", "potentially", "dangerous", "capabilities", "ensuring", "protective"]}, {"id": "term-diffusion-model", "t": "Diffusion Model", "tg": ["Architecture", "Generative"], "d": "models", "x": "A generative AI architecture that creates content by gradually removing noise from random data. Powers leading image...", "l": "d", "k": ["diffusion", "model", "generative", "architecture", "creates", "content", "gradually", "removing", "noise", "random", "data", "powers", "leading", "image", "generators"]}, {"id": "term-diffusion-model-breakthrough", "t": "Diffusion Model Breakthrough", "tg": ["History", "Milestones"], "d": "history", "x": "The emergence of diffusion-based generative models in 2020-2022 that progressively denoise random noise into...", "l": "d", "k": ["diffusion", "model", "breakthrough", "emergence", "diffusion-based", "generative", "models", "2020-2022", "progressively", "denoise", "random", "noise", "high-quality", "images", "enabling"]}, {"id": "term-diffusion-models-history", "t": "Diffusion Models History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of diffusion-based generative models from the theoretical foundation by Sohl-Dickstein et al. (2015)...", "l": "d", "k": ["diffusion", "models", "history", "development", "diffusion-based", "generative", "theoretical", "foundation", "sohl-dickstein", "denoising", "probabilistic", "practical", "image", "generation", "systems"]}, {"id": "term-diffusion-transformer", "t": "Diffusion Transformer", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "An architecture (DiT) that replaces the U-Net backbone in diffusion models with a transformer operating on sequences of...", "l": "d", "k": ["diffusion", "transformer", "architecture", "dit", "replaces", "u-net", "backbone", "models", "operating", "sequences", "latent", "patches", "scaling", "effectively", "achieving"]}, {"id": "term-digital-provenance", "t": "Digital Provenance", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The verifiable record of the origin, creation process, and modification history of a digital asset, increasingly...", "l": "d", "k": ["digital", "provenance", "verifiable", "record", "origin", "creation", "process", "modification", "history", "asset", "increasingly", "important", "establishing", "trust", "authenticity"]}, {"id": "term-digital-watermarking-for-ai", "t": "Digital Watermarking for AI", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "Techniques for embedding imperceptible identifying information into AI-generated content such as images, text, or...", "l": "d", "k": ["digital", "watermarking", "techniques", "embedding", "imperceptible", "identifying", "information", "ai-generated", "content", "images", "text", "audio", "enabling", "later", "detection"]}, {"id": "term-dilated-attention", "t": "Dilated Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that attends to tokens at regularly spaced intervals with gaps between attended positions,...", "l": "d", "k": ["dilated", "attention", "mechanism", "attends", "tokens", "regularly", "spaced", "intervals", "gaps", "attended", "positions", "allowing", "token", "capture", "long-range"]}, {"id": "term-dilated-convolution", "t": "Dilated Convolution", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A convolution operation with gaps between kernel elements that exponentially increases the receptive field without...", "l": "d", "k": ["dilated", "convolution", "operation", "gaps", "kernel", "elements", "exponentially", "increases", "receptive", "field", "without", "increasing", "parameters", "reducing", "spatial"]}, {"id": "term-dimensionality-reduction", "t": "Dimensionality Reduction", "tg": ["Technique", "Data Processing"], "d": "general", "x": "Techniques to reduce the number of features in data while preserving important information. Used for visualization,...", "l": "d", "k": ["dimensionality", "reduction", "techniques", "reduce", "number", "features", "data", "preserving", "important", "information", "visualization", "noise", "improving", "computational", "efficiency"]}, {"id": "term-dimensionality-reduction-vectors", "t": "Dimensionality Reduction for Vectors", "tg": ["Vector Database", "Dimensionality Reduction"], "d": "general", "x": "Techniques that project high-dimensional embedding vectors into lower-dimensional spaces to reduce storage, accelerate...", "l": "d", "k": ["dimensionality", "reduction", "vectors", "techniques", "project", "high-dimensional", "embedding", "lower-dimensional", "spaces", "reduce", "storage", "accelerate", "search", "mitigate", "curse"]}, {"id": "term-dino", "t": "DINO", "tg": ["Models", "Technical"], "d": "models", "x": "Self-distillation with no labels is a self-supervised learning method for vision transformers that discovers semantic...", "l": "d", "k": ["dino", "self-distillation", "labels", "self-supervised", "learning", "method", "vision", "transformers", "discovers", "semantic", "segments", "without", "supervision", "uses", "student-teacher"]}, {"id": "term-dinov2", "t": "DINOv2", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A self-supervised vision model trained with a combination of self-distillation and masked image modeling that produces...", "l": "d", "k": ["dinov2", "self-supervised", "vision", "model", "trained", "combination", "self-distillation", "masked", "image", "modeling", "produces", "versatile", "visual", "features", "useful"]}, {"id": "term-directional-stimulus-prompting", "t": "Directional Stimulus Prompting", "tg": ["Prompt Engineering", "Guided Generation"], "d": "general", "x": "A prompting framework that provides a small, tunable stimulus or hint within the prompt to guide the language model...", "l": "d", "k": ["directional", "stimulus", "prompting", "framework", "provides", "small", "tunable", "hint", "within", "prompt", "guide", "language", "model", "toward", "desired"]}, {"id": "term-dirichlet-distribution", "t": "Dirichlet Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A multivariate generalization of the beta distribution that generates probability vectors summing to one. It is widely...", "l": "d", "k": ["dirichlet", "distribution", "multivariate", "generalization", "beta", "generates", "probability", "vectors", "summing", "widely", "prior", "categorical", "distributions", "topic", "models"]}, {"id": "term-disaggregated-serving", "t": "Disaggregated Serving", "tg": ["Inference Infrastructure", "Distributed Computing"], "d": "hardware", "x": "An inference architecture that separates storage, compute, and memory resources into independent pools that can be...", "l": "d", "k": ["disaggregated", "serving", "inference", "architecture", "separates", "storage", "compute", "memory", "resources", "independent", "pools", "scaled", "independently", "enables", "flexible"]}, {"id": "term-discount-factor", "t": "Discount Factor", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A parameter gamma between 0 and 1 that determines how much future rewards are weighted relative to immediate rewards....", "l": "d", "k": ["discount", "factor", "parameter", "gamma", "determines", "future", "rewards", "weighted", "relative", "immediate", "lower", "factors", "agent", "myopic", "values"]}, {"id": "term-discourse-analysis", "t": "Discourse Analysis", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The study of how sentences and utterances connect and relate to each other in text, examining coherence relations,...", "l": "d", "k": ["discourse", "analysis", "study", "sentences", "utterances", "connect", "relate", "text", "examining", "coherence", "relations", "rhetorical", "structure", "information", "flow"]}, {"id": "term-discourse-relation", "t": "Discourse Relation", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A semantic or pragmatic relationship between text segments such as cause-effect, contrast, elaboration, or temporal...", "l": "d", "k": ["discourse", "relation", "semantic", "pragmatic", "relationship", "text", "segments", "cause-effect", "contrast", "elaboration", "temporal", "sequence", "contributes", "coherence", "document"]}, {"id": "term-discretization", "t": "Discretization", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "The process of converting continuous features into discrete bins or categories, using methods such as equal-width...", "l": "d", "k": ["discretization", "process", "converting", "continuous", "features", "discrete", "bins", "categories", "methods", "equal-width", "binning", "equal-frequency", "supervised", "decision", "tree-based"]}, {"id": "term-discriminative-learning-rates", "t": "Discriminative Learning Rates", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A fine-tuning technique that applies different learning rates to different layers of a pretrained model. Earlier layers...", "l": "d", "k": ["discriminative", "learning", "rates", "fine-tuning", "technique", "applies", "different", "layers", "pretrained", "model", "earlier", "general", "features", "smaller", "later"]}, {"id": "term-disinformation", "t": "Disinformation", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "False or misleading information deliberately created and spread with the intent to deceive. AI-generated disinformation...", "l": "d", "k": ["disinformation", "false", "misleading", "information", "deliberately", "created", "spread", "intent", "deceive", "ai-generated", "escalating", "concern", "due", "increasing", "quality"]}, {"id": "term-disparate-impact", "t": "Disparate Impact", "tg": ["Fairness", "Regulation"], "d": "safety", "x": "A legal and ethical concept where a seemingly neutral AI policy or practice disproportionately harms members of a...", "l": "d", "k": ["disparate", "impact", "legal", "ethical", "concept", "seemingly", "neutral", "policy", "practice", "disproportionately", "harms", "members", "protected", "group", "without"]}, {"id": "term-disparity-map", "t": "Disparity Map", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A pixel-level representation of the horizontal displacement between corresponding points in left and right stereo...", "l": "d", "k": ["disparity", "map", "pixel-level", "representation", "horizontal", "displacement", "corresponding", "points", "left", "right", "stereo", "images", "inversely", "proportional", "depth"]}, {"id": "term-distilbert", "t": "DistilBERT", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A distilled version of BERT that retains 97% of its language understanding capabilities while being 60% smaller and 60%...", "l": "d", "k": ["distilbert", "distilled", "version", "bert", "retains", "language", "understanding", "capabilities", "smaller", "faster", "trained", "knowledge", "distillation", "techniques"]}, {"id": "term-distillation", "t": "Distillation (Knowledge Distillation)", "tg": ["Training", "Optimization"], "d": "algorithms", "x": "A technique to transfer knowledge from a large \"teacher\" model to a smaller \"student\" model. Creates efficient models...", "l": "d", "k": ["distillation", "knowledge", "technique", "transfer", "large", "teacher", "model", "smaller", "student", "creates", "efficient", "models", "retain", "larger", "capability"]}, {"id": "term-distinct-n", "t": "Distinct-N", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A diversity metric that calculates the ratio of unique n-grams to total n-grams in generated text, measuring lexical...", "l": "d", "k": ["distinct-n", "diversity", "metric", "calculates", "ratio", "unique", "n-grams", "total", "generated", "text", "measuring", "lexical", "higher", "values", "indicate"]}, {"id": "term-distributed-ai", "t": "Distributed AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "A subfield of AI concerned with systems where multiple computational entities (agents) work together to solve problems...", "l": "d", "k": ["distributed", "subfield", "concerned", "systems", "multiple", "computational", "entities", "agents", "work", "together", "solve", "problems", "achieve", "goals", "encompasses"]}, {"id": "term-distributed-training", "t": "Distributed Training", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "The practice of spreading model training across multiple GPUs, nodes, or clusters to handle larger models and datasets....", "l": "d", "k": ["distributed", "training", "practice", "spreading", "model", "across", "multiple", "gpus", "nodes", "clusters", "handle", "larger", "models", "datasets", "requires"]}, {"id": "term-distribution-shift", "t": "Distribution Shift", "tg": ["Challenge", "Production"], "d": "general", "x": "When the data a model encounters in production differs from its training data. A major cause of model degradation over...", "l": "d", "k": ["distribution", "shift", "data", "model", "encounters", "production", "differs", "training", "major", "cause", "degradation", "time", "requiring", "monitoring", "retraining"]}, {"id": "term-distributional-rl", "t": "Distributional Reinforcement Learning", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An extension of value-based RL that models the full distribution of returns rather than just the expected value....", "l": "d", "k": ["distributional", "reinforcement", "learning", "extension", "value-based", "models", "full", "distribution", "returns", "rather", "expected", "value", "captures", "risk", "uncertainty"]}, {"id": "term-distributional-semantics", "t": "Distributional Semantics", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The theory that word meaning can be characterized by the contexts in which words appear, formalized as the...", "l": "d", "k": ["distributional", "semantics", "theory", "word", "meaning", "characterized", "contexts", "words", "appear", "formalized", "hypothesis", "similar", "distributions", "meanings"]}, {"id": "term-dit", "t": "DiT", "tg": ["Models", "Technical"], "d": "models", "x": "Diffusion Transformer replaces the U-Net backbone in diffusion models with a transformer architecture. Demonstrates...", "l": "d", "k": ["dit", "diffusion", "transformer", "replaces", "u-net", "backbone", "models", "architecture", "demonstrates", "transformers", "effective", "scalable", "backbones", "image", "generation"]}, {"id": "term-diverse-beam-search", "t": "Diverse Beam Search", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A variant of beam search that introduces a diversity penalty between beam groups, encouraging the generation of a set...", "l": "d", "k": ["diverse", "beam", "search", "variant", "introduces", "diversity", "penalty", "groups", "encouraging", "generation", "meaningfully", "different", "candidate", "sequences", "rather"]}, {"id": "term-diversity-in-retrieval", "t": "Diversity in Retrieval", "tg": ["Retrieval", "Diversity"], "d": "general", "x": "The goal of returning search results that cover different aspects, perspectives, or subtopics of a query rather than...", "l": "d", "k": ["diversity", "retrieval", "goal", "returning", "search", "results", "cover", "different", "aspects", "perspectives", "subtopics", "query", "rather", "redundant", "near-duplicate"]}, {"id": "term-diversity-score", "t": "Diversity Score", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that quantifies the variety and heterogeneity of a set of generated outputs by measuring lexical, semantic, or...", "l": "d", "k": ["diversity", "score", "metric", "quantifies", "variety", "heterogeneity", "generated", "outputs", "measuring", "lexical", "semantic", "topical", "differences", "among", "penalizing"]}, {"id": "term-document-ai", "t": "Document AI", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "AI systems that understand and extract structured information from documents by combining OCR, layout analysis, and...", "l": "d", "k": ["document", "systems", "understand", "extract", "structured", "information", "documents", "combining", "ocr", "layout", "analysis", "language", "understanding", "process", "invoices"]}, {"id": "term-document-chunking", "t": "Document Chunking", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "The process of splitting larger documents into smaller text segments for individual embedding and indexing, balancing...", "l": "d", "k": ["document", "chunking", "process", "splitting", "larger", "documents", "smaller", "text", "segments", "individual", "embedding", "indexing", "balancing", "preserving", "semantic"]}, {"id": "term-document-embedding", "t": "Document Embedding", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A dense vector representation of an entire document that captures its overall semantic content, used for document...", "l": "d", "k": ["document", "embedding", "dense", "vector", "representation", "entire", "captures", "overall", "semantic", "content", "retrieval", "clustering", "similarity", "comparison", "tasks"]}, {"id": "term-document-qa", "t": "Document Q&amp;A", "tg": ["Application", "RAG"], "d": "general", "x": "Using AI to answer questions about specific documents or text. Often implemented with RAG to enable models to reference...", "l": "d", "k": ["document", "amp", "answer", "questions", "specific", "documents", "text", "implemented", "rag", "enable", "models", "reference", "sources", "rather", "relying"]}, {"id": "term-domain-adaptation", "t": "Domain Adaptation", "tg": ["Training", "Transfer Learning"], "d": "general", "x": "Techniques for adapting a model trained on one domain (e.g., general text) to perform well on another domain (e.g.,...", "l": "d", "k": ["domain", "adaptation", "techniques", "adapting", "model", "trained", "general", "text", "perform", "another", "medical", "legal", "limited", "target", "data"]}, {"id": "term-domain-adaptation-algorithm", "t": "Domain Adaptation Algorithm", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A transfer learning technique that adapts a model trained on a source domain to perform well on a different but related...", "l": "d", "k": ["domain", "adaptation", "algorithm", "transfer", "learning", "technique", "adapts", "model", "trained", "source", "perform", "different", "related", "target", "methods"]}, {"id": "term-domain-randomization", "t": "Domain Randomization", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A sim-to-real transfer technique that trains vision models on synthetic images with heavily randomized visual...", "l": "d", "k": ["domain", "randomization", "sim-to-real", "transfer", "technique", "trains", "vision", "models", "synthetic", "images", "heavily", "randomized", "visual", "properties", "lighting"]}, {"id": "term-domain-specific-prompting", "t": "Domain-Specific Prompting", "tg": ["Prompt Engineering", "Domain Adaptation"], "d": "general", "x": "The practice of crafting prompts that incorporate specialized vocabulary, conventions, constraints, and contextual...", "l": "d", "k": ["domain-specific", "prompting", "practice", "crafting", "prompts", "incorporate", "specialized", "vocabulary", "conventions", "constraints", "contextual", "knowledge", "particular", "specific", "field"]}, {"id": "term-donald-hebb", "t": "Donald Hebb", "tg": ["History", "Pioneers"], "d": "history", "x": "Canadian neuropsychologist (1904-1985) who proposed Hebbian learning theory in his 1949 book The Organization of...", "l": "d", "k": ["donald", "hebb", "canadian", "neuropsychologist", "1904-1985", "proposed", "hebbian", "learning", "theory", "book", "organization", "behavior", "providing", "neurobiological", "basis"]}, {"id": "term-dot-product-similarity", "t": "Dot Product Similarity", "tg": ["Vector Database", "Similarity"], "d": "general", "x": "A similarity measure computed as the sum of element-wise products of two vectors, equivalent to cosine similarity when...", "l": "d", "k": ["dot", "product", "similarity", "measure", "computed", "sum", "element-wise", "products", "vectors", "equivalent", "cosine", "normalized", "additionally", "capturing", "magnitude"]}, {"id": "term-dot-product-attention", "t": "Dot-Product Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that computes compatibility scores as the dot product between query and key vectors, scaled by...", "l": "d", "k": ["dot-product", "attention", "mechanism", "computes", "compatibility", "scores", "dot", "product", "query", "key", "vectors", "scaled", "square", "root", "dimension"]}, {"id": "term-double-dqn", "t": "Double DQN", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An extension of DQN that addresses overestimation bias by decoupling action selection from action evaluation, using the...", "l": "d", "k": ["double", "dqn", "extension", "addresses", "overestimation", "bias", "decoupling", "action", "selection", "evaluation", "online", "network", "select", "actions", "target"]}, {"id": "term-double-q-learning", "t": "Double Q-Learning", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A reinforcement learning variant that addresses the overestimation bias in standard Q-learning by using two separate...", "l": "d", "k": ["double", "q-learning", "reinforcement", "learning", "variant", "addresses", "overestimation", "bias", "standard", "separate", "value", "estimators", "selects", "action", "evaluates"]}, {"id": "term-douglas-engelbart", "t": "Douglas Engelbart", "tg": ["History", "Pioneers"], "d": "history", "x": "American engineer and inventor who demonstrated the first computer mouse hypertext video conferencing and collaborative...", "l": "d", "k": ["douglas", "engelbart", "american", "engineer", "inventor", "demonstrated", "computer", "mouse", "hypertext", "video", "conferencing", "collaborative", "real-time", "editing", "mother"]}, {"id": "term-douglas-hofstadter", "t": "Douglas Hofstadter", "tg": ["History", "Pioneers"], "d": "history", "x": "American cognitive scientist and author of Goedel Escher Bach: An Eternal Golden Braid (1979) which explores...", "l": "d", "k": ["douglas", "hofstadter", "american", "cognitive", "scientist", "author", "goedel", "escher", "bach", "eternal", "golden", "braid", "explores", "connections", "mathematics"]}, {"id": "term-douglas-lenat", "t": "Douglas Lenat", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1950-2023) who created the Cyc project in 1984, an ambitious effort to build a...", "l": "d", "k": ["douglas", "lenat", "american", "computer", "scientist", "1950-2023", "created", "cyc", "project", "ambitious", "effort", "build", "comprehensive", "ontology", "common-sense"]}, {"id": "term-dpo", "t": "DPO (Direct Preference Optimization)", "tg": ["Training", "Alignment"], "d": "safety", "x": "A simpler alternative to RLHF for aligning language models. Directly optimizes the model using preference data without...", "l": "d", "k": ["dpo", "direct", "preference", "optimization", "simpler", "alternative", "rlhf", "aligning", "language", "models", "directly", "optimizes", "model", "data", "without"]}, {"id": "term-dqn-deep-q-network", "t": "DQN (Deep Q-Network)", "tg": ["History", "Systems"], "d": "history", "x": "A deep reinforcement learning architecture developed by DeepMind in 2013 that combines Q-learning with deep neural...", "l": "d", "k": ["dqn", "deep", "q-network", "reinforcement", "learning", "architecture", "developed", "deepmind", "combines", "q-learning", "neural", "networks", "learned", "play", "atari"]}, {"id": "term-dreambooth", "t": "DreamBooth", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A fine-tuning technique that personalizes diffusion models to generate images of specific subjects by training on just...", "l": "d", "k": ["dreambooth", "fine-tuning", "technique", "personalizes", "diffusion", "models", "generate", "images", "specific", "subjects", "training", "reference", "unique", "identifier", "token"]}, {"id": "term-dreamer", "t": "Dreamer", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A model-based RL agent that learns a world model in latent space and trains its policy entirely through imagined...", "l": "d", "k": ["dreamer", "model-based", "agent", "learns", "world", "model", "latent", "space", "trains", "policy", "entirely", "imagined", "trajectories", "generated", "achieves"]}, {"id": "term-drew-mcdermott", "t": "Drew McDermott", "tg": ["History", "Pioneers"], "d": "history", "x": "American AI researcher at Yale who made significant contributions to AI planning and robotics. Known for his 1976...", "l": "d", "k": ["drew", "mcdermott", "american", "researcher", "yale", "significant", "contributions", "planning", "robotics", "known", "critique", "overconfidence", "paper", "artificial", "intelligence"]}, {"id": "term-dropblock", "t": "DropBlock", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A structured dropout method that drops contiguous regions of feature maps rather than individual elements. More...", "l": "d", "k": ["dropblock", "structured", "dropout", "method", "drops", "contiguous", "regions", "feature", "maps", "rather", "individual", "elements", "effective", "standard", "convolutional"]}, {"id": "term-dropconnect", "t": "DropConnect", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A regularization method that randomly sets individual weights rather than activations to zero during training....", "l": "d", "k": ["dropconnect", "regularization", "method", "randomly", "sets", "individual", "weights", "rather", "activations", "zero", "training", "generalizes", "dropout", "operating", "weight"]}, {"id": "term-dropout", "t": "Dropout", "tg": ["Training", "Regularization"], "d": "algorithms", "x": "A regularization technique that randomly deactivates neurons during training. Prevents overfitting by forcing the...", "l": "d", "k": ["dropout", "regularization", "technique", "randomly", "deactivates", "neurons", "training", "prevents", "overfitting", "forcing", "network", "learn", "robust", "features"]}, {"id": "term-dropout-technique", "t": "Dropout Technique", "tg": ["History", "Milestones"], "d": "history", "x": "A regularization method proposed by Hinton et al. in 2012 that randomly deactivates neurons during training to prevent...", "l": "d", "k": ["dropout", "technique", "regularization", "method", "proposed", "hinton", "randomly", "deactivates", "neurons", "training", "prevent", "overfitting", "becoming", "widely", "techniques"]}, {"id": "term-droppath", "t": "DropPath", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A regularization method for networks with multiple parallel paths that randomly drops entire residual branches during...", "l": "d", "k": ["droppath", "regularization", "method", "networks", "multiple", "parallel", "paths", "randomly", "drops", "entire", "residual", "branches", "training", "improving", "generalization"]}, {"id": "term-dual-use-technology", "t": "Dual-Use Technology", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "Technology that can be used for both beneficial and harmful purposes, a concept particularly relevant to AI...", "l": "d", "k": ["dual-use", "technology", "beneficial", "harmful", "purposes", "concept", "particularly", "relevant", "capabilities", "language", "generation", "computer", "vision", "autonomous", "systems"]}, {"id": "term-dueling-dqn", "t": "Dueling DQN", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A DQN architecture that separately estimates the state value function and the advantage function, combining them to...", "l": "d", "k": ["dueling", "dqn", "architecture", "separately", "estimates", "state", "value", "function", "advantage", "combining", "produce", "q-values", "decomposition", "allows", "network"]}, {"id": "term-durbin-watson-test", "t": "Durbin-Watson Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A statistical test for detecting first-order autocorrelation in the residuals of a regression analysis. Values near 2...", "l": "d", "k": ["durbin-watson", "test", "statistical", "detecting", "first-order", "autocorrelation", "residuals", "regression", "analysis", "values", "near", "indicate", "suggest", "positive", "negative"]}, {"id": "term-dyna-architecture", "t": "Dyna Architecture", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A model-based RL framework that integrates direct learning from real experience with planning through simulated...", "l": "d", "k": ["dyna", "architecture", "model-based", "framework", "integrates", "direct", "learning", "real", "experience", "planning", "simulated", "generated", "learned", "environment", "model"]}, {"id": "term-dynamic-loss-scaling", "t": "Dynamic Loss Scaling", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "An automatic loss scaling strategy that adapts the scaling factor during training, increasing it when no overflow is...", "l": "d", "k": ["dynamic", "loss", "scaling", "automatic", "strategy", "adapts", "factor", "training", "increasing", "overflow", "detected", "decreasing", "gradients", "eliminates", "need"]}, {"id": "term-dynamic-programming", "t": "Dynamic Programming", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "An algorithmic technique that solves complex problems by breaking them into simpler overlapping subproblems and storing...", "l": "d", "k": ["dynamic", "programming", "algorithmic", "technique", "solves", "complex", "problems", "breaking", "simpler", "overlapping", "subproblems", "storing", "solutions", "avoid", "redundant"]}, {"id": "term-dynamic-prompting", "t": "Dynamic Prompting", "tg": ["Prompt Engineering", "Adaptive"], "d": "general", "x": "A prompting approach where the content, structure, or examples within a prompt are programmatically adjusted at runtime...", "l": "d", "k": ["dynamic", "prompting", "approach", "content", "structure", "examples", "within", "prompt", "programmatically", "adjusted", "runtime", "based", "input", "query", "user"]}, {"id": "term-dynamic-quantization", "t": "Dynamic Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A quantization approach that computes scaling factors on-the-fly during inference based on the actual range of...", "l": "d", "k": ["dynamic", "quantization", "approach", "computes", "scaling", "factors", "on-the-fly", "inference", "based", "actual", "range", "activation", "values", "encountered", "adapts"]}, {"id": "term-e5", "t": "E5", "tg": ["Models", "Technical"], "d": "models", "x": "A family of text embedding models trained with contrastive learning on large-scale text pairs. Achieves strong...", "l": "e", "k": ["family", "text", "embedding", "models", "trained", "contrastive", "learning", "large-scale", "pairs", "achieves", "strong", "performance", "across", "retrieval", "clustering"]}, {"id": "term-early-stopping", "t": "Early Stopping", "tg": ["Training", "Regularization"], "d": "algorithms", "x": "A regularization technique that stops training when performance on a validation set stops improving. Prevents...", "l": "e", "k": ["early", "stopping", "regularization", "technique", "stops", "training", "performance", "validation", "improving", "prevents", "overfitting", "saves", "computational", "resources"]}, {"id": "term-early-stopping-criterion", "t": "Early Stopping Criterion", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A regularization technique that monitors validation performance during training and stops when it begins to...", "l": "e", "k": ["early", "stopping", "criterion", "regularization", "technique", "monitors", "validation", "performance", "training", "stops", "begins", "deteriorate", "prevents", "overfitting", "selecting"]}, {"id": "term-earth-movers-distance", "t": "Earth Mover's Distance", "tg": ["Statistics", "Metrics"], "d": "datasets", "x": "A metric for comparing probability distributions based on the minimum amount of work needed to transform one...", "l": "e", "k": ["earth", "mover", "distance", "metric", "comparing", "probability", "distributions", "based", "minimum", "amount", "work", "needed", "transform", "distribution", "mass"]}, {"id": "term-eccv", "t": "ECCV", "tg": ["History", "Conferences"], "d": "history", "x": "The European Conference on Computer Vision held biennially since 1990. One of the top three computer vision conferences...", "l": "e", "k": ["eccv", "european", "conference", "computer", "vision", "held", "biennially", "top", "conferences", "alongside", "cvpr", "iccv", "known", "publishing", "influential"]}, {"id": "term-echo-state-network", "t": "Echo State Network", "tg": ["Models", "Technical"], "d": "models", "x": "A recurrent neural network where the recurrent layer is a large randomly generated reservoir with fixed weights. Only...", "l": "e", "k": ["echo", "state", "network", "recurrent", "neural", "layer", "large", "randomly", "generated", "reservoir", "fixed", "weights", "output", "trained", "part"]}, {"id": "term-edge-ai", "t": "Edge AI", "tg": ["Deployment", "Architecture"], "d": "models", "x": "Running AI models locally on devices (phones, IoT) rather than in the cloud. Enables faster responses, offline...", "l": "e", "k": ["edge", "running", "models", "locally", "devices", "phones", "iot", "rather", "cloud", "enables", "faster", "responses", "offline", "operation", "better"]}, {"id": "term-edge-inference", "t": "Edge Inference", "tg": ["Inference Infrastructure", "Hardware"], "d": "hardware", "x": "Running AI model inference directly on edge devices (phones, IoT sensors, embedded systems) rather than in the cloud,...", "l": "e", "k": ["edge", "inference", "running", "model", "directly", "devices", "phones", "iot", "sensors", "embedded", "systems", "rather", "cloud", "reducing", "latency"]}, {"id": "term-edit-distance", "t": "Edit Distance", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A family of metrics quantifying the minimum number of operations required to transform one string into another, with...", "l": "e", "k": ["edit", "distance", "family", "metrics", "quantifying", "minimum", "number", "operations", "required", "transform", "string", "another", "variants", "including", "levenshtein"]}, {"id": "term-edmund-clarke", "t": "Edmund Clarke", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who pioneered model checking a technique for automatically verifying the correctness of...", "l": "e", "k": ["edmund", "clarke", "american", "computer", "scientist", "pioneered", "model", "checking", "technique", "automatically", "verifying", "correctness", "finite-state", "systems", "received"]}, {"id": "term-edvac", "t": "EDVAC", "tg": ["History", "Systems"], "d": "history", "x": "The Electronic Discrete Variable Automatic Computer designed in the 1940s was one of the first stored-program...", "l": "e", "k": ["edvac", "electronic", "discrete", "variable", "automatic", "computer", "designed", "1940s", "stored-program", "computers", "john", "von", "neumann", "draft", "report"]}, {"id": "term-edward-feigenbaum", "t": "Edward Feigenbaum", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist known as the father of expert systems, who led the development of DENDRAL and pioneered...", "l": "e", "k": ["edward", "feigenbaum", "american", "computer", "scientist", "known", "father", "expert", "systems", "led", "development", "dendral", "pioneered", "knowledge", "engineering"]}, {"id": "term-edward-shortliffe", "t": "Edward Shortliffe", "tg": ["History", "Pioneers"], "d": "history", "x": "American biomedical informatician who developed MYCIN one of the most influential early expert systems for medical...", "l": "e", "k": ["edward", "shortliffe", "american", "biomedical", "informatician", "developed", "mycin", "influential", "early", "expert", "systems", "medical", "diagnosis", "work", "demonstrated"]}, {"id": "term-effect-size", "t": "Effect Size", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A quantitative measure of the magnitude of a phenomenon or the practical significance of a result, independent of...", "l": "e", "k": ["effect", "size", "quantitative", "measure", "magnitude", "phenomenon", "practical", "significance", "result", "independent", "sample", "common", "measures", "include", "cohen"]}, {"id": "term-efficiency", "t": "Efficiency", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A property of a statistical estimator related to how much information from the data it uses. An efficient estimator...", "l": "e", "k": ["efficiency", "property", "statistical", "estimator", "related", "information", "data", "uses", "efficient", "achieves", "lowest", "possible", "variance", "among", "unbiased"]}, {"id": "term-efficientnet", "t": "EfficientNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A family of CNN models that use compound scaling to uniformly scale network width, depth, and resolution using a fixed...", "l": "e", "k": ["efficientnet", "family", "cnn", "models", "compound", "scaling", "uniformly", "scale", "network", "width", "depth", "resolution", "fixed", "coefficients", "achieving"]}, {"id": "term-efficientnetv2", "t": "EfficientNetV2", "tg": ["Models", "Technical"], "d": "models", "x": "An improved version of EfficientNet that uses progressive training and fused MBConv blocks for faster training....", "l": "e", "k": ["efficientnetv2", "improved", "version", "efficientnet", "uses", "progressive", "training", "fused", "mbconv", "blocks", "faster", "achieves", "better", "accuracy", "speed"]}, {"id": "term-eigendecomposition", "t": "Eigendecomposition", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "The factorization of a square matrix into eigenvalues and eigenvectors revealing the directions along which the...", "l": "e", "k": ["eigendecomposition", "factorization", "square", "matrix", "eigenvalues", "eigenvectors", "revealing", "directions", "along", "transformation", "acts", "simple", "scaling", "fundamental", "pca"]}, {"id": "term-einsum", "t": "Einsum", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Einstein Summation notation is a compact way to express multi-dimensional array operations including matrix...", "l": "e", "k": ["einsum", "einstein", "summation", "notation", "compact", "express", "multi-dimensional", "array", "operations", "including", "matrix", "multiplication", "outer", "products", "contractions"]}, {"id": "term-elastic-net", "t": "Elastic Net", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A regularization method that linearly combines L1 and L2 penalty terms, balancing feature selection (sparsity) with...", "l": "e", "k": ["elastic", "net", "regularization", "method", "linearly", "combines", "penalty", "terms", "balancing", "feature", "selection", "sparsity", "weight", "shrinkage", "mixing"]}, {"id": "term-elastic-training", "t": "Elastic Training", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A distributed training approach that can dynamically scale the number of workers up or down during a training run...", "l": "e", "k": ["elastic", "training", "distributed", "approach", "dynamically", "scale", "number", "workers", "down", "run", "without", "requiring", "restart", "handles", "node"]}, {"id": "term-elastic-weight-consolidation", "t": "Elastic Weight Consolidation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A continual learning technique that slows down learning on weights important for previous tasks. Uses the diagonal of...", "l": "e", "k": ["elastic", "weight", "consolidation", "continual", "learning", "technique", "slows", "down", "weights", "important", "previous", "tasks", "uses", "diagonal", "fisher"]}, {"id": "term-elbow-method", "t": "Elbow Method", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A heuristic for selecting the optimal number of clusters by plotting the within-cluster sum of squares against the...", "l": "e", "k": ["elbow", "method", "heuristic", "selecting", "optimal", "number", "clusters", "plotting", "within-cluster", "sum", "squares", "against", "identifying", "point", "additional"]}, {"id": "term-electra", "t": "ELECTRA", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A pretraining method that trains a discriminator to detect tokens replaced by a small generator network, providing more...", "l": "e", "k": ["electra", "pretraining", "method", "trains", "discriminator", "detect", "tokens", "replaced", "small", "generator", "network", "providing", "efficient", "training", "masked"]}, {"id": "term-eleutherai", "t": "EleutherAI", "tg": ["History", "Organizations"], "d": "history", "x": "A grassroots collective of researchers founded in 2020 dedicated to open-source AI research. EleutherAI developed...", "l": "e", "k": ["eleutherai", "grassroots", "collective", "researchers", "founded", "dedicated", "open-source", "research", "developed", "gpt-neo", "gpt-j", "gpt-neox", "demonstrating", "large", "language"]}, {"id": "term-eliciting-latent-knowledge", "t": "Eliciting Latent Knowledge", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "A research problem in AI alignment focused on extracting truthful information from a model that may have learned to...", "l": "e", "k": ["eliciting", "latent", "knowledge", "research", "problem", "alignment", "focused", "extracting", "truthful", "information", "model", "learned", "represent", "world", "accurately"]}, {"id": "term-eligibility-trace", "t": "Eligibility Trace", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A decaying memory of recently visited states used in TD(lambda) and other RL algorithms to distribute credit backward...", "l": "e", "k": ["eligibility", "trace", "decaying", "memory", "recently", "visited", "states", "lambda", "algorithms", "distribute", "credit", "backward", "time", "traces", "enable"]}, {"id": "term-eliza", "t": "ELIZA", "tg": ["History", "Milestones"], "d": "history", "x": "A natural language processing program created by Joseph Weizenbaum at MIT in 1966 that simulated a Rogerian...", "l": "e", "k": ["eliza", "natural", "language", "processing", "program", "created", "joseph", "weizenbaum", "mit", "simulated", "rogerian", "psychotherapist", "demonstrating", "illusion", "understanding"]}, {"id": "term-eliza-effect", "t": "Eliza Effect", "tg": ["History", "Fundamentals"], "d": "history", "x": "The tendency of humans to unconsciously assume that computer behaviors are analogous to human behaviors. Named after...", "l": "e", "k": ["eliza", "effect", "tendency", "humans", "unconsciously", "assume", "computer", "behaviors", "analogous", "human", "named", "chatbot", "describes", "people", "readily"]}, {"id": "term-ellipsis-resolution", "t": "Ellipsis Resolution", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of identifying and recovering omitted words or phrases in text that are understood from context, such as...", "l": "e", "k": ["ellipsis", "resolution", "task", "identifying", "recovering", "omitted", "words", "phrases", "text", "understood", "context", "resolving", "john", "likes", "coffee"]}, {"id": "term-elmo", "t": "ELMo", "tg": ["NLP", "Embeddings"], "d": "general", "x": "Embeddings from Language Models, a contextualized word representation method that generates word vectors as a function...", "l": "e", "k": ["elmo", "embeddings", "language", "models", "contextualized", "word", "representation", "method", "generates", "vectors", "function", "entire", "input", "sentence", "bidirectional"]}, {"id": "term-elo-rating-for-models", "t": "ELO Rating for Models", "tg": ["Evaluation", "Ranking"], "d": "datasets", "x": "An adaptation of the chess ELO rating system to rank language models through pairwise comparisons, where models gain or...", "l": "e", "k": ["elo", "rating", "models", "adaptation", "chess", "system", "rank", "language", "pairwise", "comparisons", "gain", "lose", "points", "based", "head-to-head"]}, {"id": "term-elu", "t": "ELU", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Exponential Linear Unit activation function that uses an exponential curve for negative inputs. Defined as f(x) = x if...", "l": "e", "k": ["elu", "exponential", "linear", "unit", "activation", "function", "uses", "curve", "negative", "inputs", "defined", "alpha", "exp", "otherwise", "produces"]}, {"id": "term-embedding", "t": "Embedding", "tg": ["Representation", "NLP"], "d": "general", "x": "A dense vector representation of data (words, sentences, images) in a continuous space. Similar items have similar...", "l": "e", "k": ["embedding", "dense", "vector", "representation", "data", "words", "sentences", "images", "continuous", "space", "similar", "items", "embeddings", "enabling", "semantic"]}, {"id": "term-embedding-caching", "t": "Embedding Caching", "tg": ["Vector Database", "Performance"], "d": "general", "x": "The practice of storing previously computed embedding vectors for reuse, avoiding redundant embedding model inference...", "l": "e", "k": ["embedding", "caching", "practice", "storing", "previously", "computed", "vectors", "reuse", "avoiding", "redundant", "model", "inference", "repeated", "similar", "content"]}, {"id": "term-embedding-dimension", "t": "Embedding Dimension", "tg": ["Vector Database", "Embeddings"], "d": "general", "x": "The number of components in a vector embedding, determining the representational capacity and memory footprint of the...", "l": "e", "k": ["embedding", "dimension", "number", "components", "vector", "determining", "representational", "capacity", "memory", "footprint", "space", "typical", "values", "ranging", "dimensions"]}, {"id": "term-embedding-drift", "t": "Embedding Drift", "tg": ["Vector Database", "Maintenance"], "d": "general", "x": "The phenomenon where the distribution of vector embeddings changes over time as source data evolves or embedding models...", "l": "e", "k": ["embedding", "drift", "phenomenon", "distribution", "vector", "embeddings", "changes", "time", "source", "data", "evolves", "models", "updated", "potentially", "degrading"]}, {"id": "term-embedding-fine-tuning", "t": "Embedding Fine-Tuning", "tg": ["Vector Database", "Embeddings"], "d": "general", "x": "The process of further training a pre-trained embedding model on domain-specific data using contrastive learning or...", "l": "e", "k": ["embedding", "fine-tuning", "process", "training", "pre-trained", "model", "domain-specific", "data", "contrastive", "learning", "objectives", "produce", "embeddings", "better", "suited"]}, {"id": "term-embedding-model", "t": "Embedding Model", "tg": ["Model Type", "Representation"], "d": "models", "x": "A model specifically designed to convert text, images, or other data into vector representations. Popular embedding...", "l": "e", "k": ["embedding", "model", "specifically", "designed", "convert", "text", "images", "data", "vector", "representations", "popular", "models", "include", "openai", "text-embedding-ada-002"]}, {"id": "term-embedding-quantization", "t": "Embedding Quantization", "tg": ["LLM", "Inference"], "d": "models", "x": "The compression of high-dimensional embedding vectors from 32-bit floats to lower precision formats (binary, int8) to...", "l": "e", "k": ["embedding", "quantization", "compression", "high-dimensional", "vectors", "32-bit", "floats", "lower", "precision", "formats", "binary", "int8", "reduce", "storage", "costs"]}, {"id": "term-embedding-similarity-search", "t": "Embedding Similarity Search", "tg": ["Vector Database", "Search"], "d": "general", "x": "The process of finding the most semantically similar items to a query by computing distances between their vector...", "l": "e", "k": ["embedding", "similarity", "search", "process", "finding", "semantically", "similar", "items", "query", "computing", "distances", "vector", "embeddings", "shared", "space"]}, {"id": "term-embedding-space", "t": "Embedding Space", "tg": ["Vector Database", "Embeddings"], "d": "general", "x": "The continuous high-dimensional vector space in which embeddings reside, where geometric relationships between vectors...", "l": "e", "k": ["embedding", "space", "continuous", "high-dimensional", "vector", "embeddings", "reside", "geometric", "relationships", "vectors", "encode", "semantic", "similar", "concepts", "located"]}, {"id": "term-embodied-ai", "t": "Embodied AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "An approach to AI that emphasizes the role of physical embodiment and sensorimotor interaction with the environment in...", "l": "e", "k": ["embodied", "approach", "emphasizes", "role", "physical", "embodiment", "sensorimotor", "interaction", "environment", "development", "intelligence", "proponents", "argue", "cannot", "fully"]}, {"id": "term-emergent-abilities", "t": "Emergent Abilities", "tg": ["Phenomenon", "Scaling"], "d": "general", "x": "Capabilities that appear in large AI models that weren't present in smaller versions. Examples include complex...", "l": "e", "k": ["emergent", "abilities", "capabilities", "appear", "large", "models", "weren", "present", "smaller", "versions", "examples", "include", "complex", "reasoning", "code"]}, {"id": "term-emergent-capability", "t": "Emergent Capability", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An ability that appears in large language models only at sufficient scale and is absent in smaller models, such as...", "l": "e", "k": ["emergent", "capability", "ability", "appears", "large", "language", "models", "sufficient", "scale", "absent", "smaller", "multi-step", "reasoning", "code", "generation"]}, {"id": "term-emnlp", "t": "EMNLP", "tg": ["History", "Conferences"], "d": "history", "x": "The Conference on Empirical Methods in Natural Language Processing first held in 1996. A top-tier NLP conference known...", "l": "e", "k": ["emnlp", "conference", "empirical", "methods", "natural", "language", "processing", "held", "top-tier", "nlp", "known", "emphasis", "data-driven", "approaches", "alongside"]}, {"id": "term-emotion-detection", "t": "Emotion Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying specific emotions such as joy, anger, sadness, fear, or surprise expressed in text, providing...", "l": "e", "k": ["emotion", "detection", "task", "identifying", "specific", "emotions", "joy", "anger", "sadness", "fear", "surprise", "expressed", "text", "providing", "finer-grained"]}, {"id": "term-emotion-prompting", "t": "Emotion Prompting", "tg": ["Prompt Engineering", "Behavioral"], "d": "general", "x": "A technique that appends emotionally charged phrases to prompts such as urgency cues or importance markers, leveraging...", "l": "e", "k": ["emotion", "prompting", "technique", "appends", "emotionally", "charged", "phrases", "prompts", "urgency", "cues", "importance", "markers", "leveraging", "observation", "language"]}, {"id": "term-emotion-recognition", "t": "Emotion Recognition", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The classification of facial expressions or body language into emotional categories using computer vision models...", "l": "e", "k": ["emotion", "recognition", "classification", "facial", "expressions", "body", "language", "emotional", "categories", "computer", "vision", "models", "trained", "annotated", "datasets"]}, {"id": "term-emotion-recognition-ai-ethics", "t": "Emotion Recognition AI Ethics", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "Ethical concerns about AI systems that claim to detect human emotions from facial expressions, voice, or physiological...", "l": "e", "k": ["emotion", "recognition", "ethics", "ethical", "concerns", "systems", "claim", "detect", "human", "emotions", "facial", "expressions", "voice", "physiological", "signals"]}, {"id": "term-empirical-bayes", "t": "Empirical Bayes", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "An approach that estimates prior distribution parameters from the data itself, rather than specifying them a priori. It...", "l": "e", "k": ["empirical", "bayes", "approach", "estimates", "prior", "distribution", "parameters", "data", "itself", "rather", "specifying", "priori", "blends", "bayesian", "frequentist"]}, {"id": "term-empirical-distribution-function", "t": "Empirical Distribution Function", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A cumulative distribution function constructed from sample data that assigns probability 1/n to each observed value. It...", "l": "e", "k": ["empirical", "distribution", "function", "cumulative", "constructed", "sample", "data", "assigns", "probability", "observed", "value", "converges", "uniformly", "true", "cdf"]}, {"id": "term-empirical-risk-minimization", "t": "Empirical Risk Minimization", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A learning principle that selects the hypothesis minimizing the average loss on the training data. While simple and...", "l": "e", "k": ["empirical", "risk", "minimization", "learning", "principle", "selects", "hypothesis", "minimizing", "average", "loss", "training", "data", "simple", "intuitive", "lead"]}, {"id": "term-empowerment", "t": "Empowerment", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An information-theoretic intrinsic motivation measure defined as the channel capacity between an agent's actions and...", "l": "e", "k": ["empowerment", "information-theoretic", "intrinsic", "motivation", "measure", "defined", "channel", "capacity", "agent", "actions", "future", "states", "rewards", "maintaining", "maximum"]}, {"id": "term-encodec", "t": "Encodec", "tg": ["Models", "Technical"], "d": "models", "x": "A neural audio codec by Meta AI that compresses audio at very low bitrates using residual vector quantization. Enables...", "l": "e", "k": ["encodec", "neural", "audio", "codec", "meta", "compresses", "low", "bitrates", "residual", "vector", "quantization", "enables", "efficient", "storage", "transmission"]}, {"id": "term-encoder", "t": "Encoder", "tg": ["Architecture", "Transformers"], "d": "models", "x": "A neural network component that transforms input into a compressed representation. In transformers, encoder models...", "l": "e", "k": ["encoder", "neural", "network", "component", "transforms", "input", "compressed", "representation", "transformers", "models", "bert", "process", "entire", "understanding", "tasks"]}, {"id": "term-encoder-decoder-architecture", "t": "Encoder-Decoder Architecture", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A neural network design where an encoder processes input into a latent representation and a decoder generates output...", "l": "e", "k": ["encoder-decoder", "architecture", "neural", "network", "design", "encoder", "processes", "input", "latent", "representation", "decoder", "generates", "output", "commonly", "translation"]}, {"id": "term-encoder-only-architecture", "t": "Encoder-Only Architecture", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer design using only bidirectional self-attention encoder blocks, producing contextualized representations...", "l": "e", "k": ["encoder-only", "architecture", "transformer", "design", "bidirectional", "self-attention", "encoder", "blocks", "producing", "contextualized", "representations", "input", "tokens", "suited", "classification"]}, {"id": "term-endpoint", "t": "Endpoint", "tg": ["API", "Technical"], "d": "general", "x": "A specific URL where an API can be accessed. AI services expose endpoints for different functions like chat...", "l": "e", "k": ["endpoint", "specific", "url", "api", "accessed", "services", "expose", "endpoints", "different", "functions", "chat", "completions", "embeddings", "image", "generation"]}, {"id": "term-eniac", "t": "ENIAC", "tg": ["History", "Milestones"], "d": "history", "x": "The Electronic Numerical Integrator and Computer, completed in 1945 at the University of Pennsylvania, one of the...", "l": "e", "k": ["eniac", "electronic", "numerical", "integrator", "computer", "completed", "university", "pennsylvania", "earliest", "general-purpose", "digital", "computers", "demonstrated", "feasibility", "large-scale"]}, {"id": "term-ensemble-learning", "t": "Ensemble Learning", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A machine learning paradigm that combines predictions from multiple models to produce a more robust and accurate...", "l": "e", "k": ["ensemble", "learning", "machine", "paradigm", "combines", "predictions", "multiple", "models", "produce", "robust", "accurate", "prediction", "methods", "include", "bagging"]}, {"id": "term-ensemble", "t": "Ensemble Methods", "tg": ["Technique", "ML"], "d": "general", "x": "Techniques that combine multiple models to produce better results than any single model. Includes voting, bagging...", "l": "e", "k": ["ensemble", "methods", "techniques", "combine", "multiple", "models", "produce", "better", "results", "single", "model", "includes", "voting", "bagging", "random"]}, {"id": "term-ensemble-methods-history", "t": "Ensemble Methods History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of ensemble methods from early voting approaches through bagging (Breiman 1996) boosting (Freund and...", "l": "e", "k": ["ensemble", "methods", "history", "development", "early", "voting", "approaches", "bagging", "breiman", "boosting", "freund", "schapire", "stacking", "wolpert", "random"]}, {"id": "term-enterprise-ai", "t": "Enterprise AI", "tg": ["Business", "Application"], "d": "general", "x": "AI solutions designed for business environments with features like access controls, compliance, data privacy, and...", "l": "e", "k": ["enterprise", "solutions", "designed", "business", "environments", "features", "access", "controls", "compliance", "data", "privacy", "integration", "existing", "systems", "different"]}, {"id": "term-entity-disambiguation", "t": "Entity Disambiguation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The process of determining which specific real-world entity a textual mention refers to when the same name could refer...", "l": "e", "k": ["entity", "disambiguation", "process", "determining", "specific", "real-world", "textual", "mention", "refers", "name", "refer", "multiple", "entities", "context", "clues"]}, {"id": "term-entropy", "t": "Entropy", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A measure from information theory quantifying the uncertainty or disorder in a random variable's distribution. In...", "l": "e", "k": ["entropy", "measure", "information", "theory", "quantifying", "uncertainty", "disorder", "random", "variable", "distribution", "machine", "learning", "splitting", "criterion", "component"]}, {"id": "term-entropy-regularization", "t": "Entropy Regularization", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A technique that adds the policy entropy to the RL objective function, discouraging the agent from committing to a...", "l": "e", "k": ["entropy", "regularization", "technique", "adds", "policy", "objective", "function", "discouraging", "agent", "committing", "single", "action", "quickly", "promotes", "robust"]}, {"id": "term-environment", "t": "Environment", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The external system an RL agent interacts with, providing observations and rewards in response to actions. The...", "l": "e", "k": ["environment", "external", "system", "agent", "interacts", "providing", "observations", "rewards", "response", "actions", "defines", "dynamics", "rules", "governing", "state"]}, {"id": "term-epipolar-geometry", "t": "Epipolar Geometry", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The geometric relationship between two camera views of the same scene, defined by the fundamental or essential matrix,...", "l": "e", "k": ["epipolar", "geometry", "geometric", "relationship", "camera", "views", "scene", "defined", "fundamental", "essential", "matrix", "constraining", "point", "image", "appear"]}, {"id": "term-episode", "t": "Episode", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A complete sequence of interaction from an initial state to a terminal state in episodic RL tasks. Episodes provide...", "l": "e", "k": ["episode", "complete", "sequence", "interaction", "initial", "state", "terminal", "episodic", "tasks", "episodes", "provide", "natural", "boundaries", "computing", "returns"]}, {"id": "term-epoch", "t": "Epoch", "tg": ["Training", "Technical"], "d": "general", "x": "One complete pass through the entire training dataset. Models typically train for multiple epochs, with each pass...", "l": "e", "k": ["epoch", "complete", "pass", "entire", "training", "dataset", "models", "typically", "train", "multiple", "epochs", "allowing", "weights", "refined", "based"]}, {"id": "term-epsilon-greedy", "t": "Epsilon-Greedy Exploration", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An exploration strategy where the agent selects the greedy (best-known) action with probability 1-epsilon and a random...", "l": "e", "k": ["epsilon-greedy", "exploration", "strategy", "agent", "selects", "greedy", "best-known", "action", "probability", "1-epsilon", "random", "epsilon", "parameter", "typically", "annealed"]}, {"id": "term-equalized-odds", "t": "Equalized Odds", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness criterion requiring that a classifier has equal true positive rates and equal false positive rates across...", "l": "e", "k": ["equalized", "odds", "fairness", "criterion", "requiring", "classifier", "equal", "true", "positive", "rates", "false", "across", "protected", "groups", "ensuring"]}, {"id": "term-error-analysis", "t": "Error Analysis", "tg": ["Evaluation", "Process"], "d": "datasets", "x": "Systematic examination of model mistakes to understand failure patterns and guide improvements. Essential for iterating...", "l": "e", "k": ["error", "analysis", "systematic", "examination", "model", "mistakes", "understand", "failure", "patterns", "guide", "improvements", "essential", "iterating", "performance", "identifying"]}, {"id": "term-roce", "t": "Ethernet for AI (RoCE)", "tg": ["Distributed Computing", "Hardware"], "d": "hardware", "x": "RDMA over Converged Ethernet, a networking protocol that enables remote direct memory access over Ethernet...", "l": "e", "k": ["ethernet", "roce", "rdma", "converged", "networking", "protocol", "enables", "remote", "direct", "memory", "access", "infrastructure", "provides", "lower-cost", "alternative"]}, {"id": "term-ethical-prompting", "t": "Ethical Prompting", "tg": ["Prompt Engineering", "Ethics"], "d": "safety", "x": "The practice of designing prompts that explicitly incorporate ethical guidelines, fairness constraints, and...", "l": "e", "k": ["ethical", "prompting", "practice", "designing", "prompts", "explicitly", "incorporate", "guidelines", "fairness", "constraints", "harm-avoidance", "instructions", "steer", "model", "outputs"]}, {"id": "term-ethics-board", "t": "Ethics Board (AI)", "tg": ["Governance", "Ethics"], "d": "safety", "x": "A group that reviews AI development and deployment for ethical concerns. Many major AI companies have ethics boards to...", "l": "e", "k": ["ethics", "board", "group", "reviews", "development", "deployment", "ethical", "concerns", "major", "companies", "boards", "evaluate", "potential", "harms", "establish"]}, {"id": "term-ethics-washing", "t": "Ethics Washing", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The practice of organizations using ethics boards, principles, or frameworks as public relations tools without...", "l": "e", "k": ["ethics", "washing", "practice", "organizations", "boards", "principles", "frameworks", "public", "relations", "tools", "without", "implementing", "meaningful", "changes", "development"]}, {"id": "term-eu-ai-act", "t": "EU AI Act", "tg": ["Governance", "Regulation"], "d": "safety", "x": "The European Union's comprehensive regulatory framework for artificial intelligence, adopted in 2024, which classifies...", "l": "e", "k": ["act", "european", "union", "comprehensive", "regulatory", "framework", "artificial", "intelligence", "adopted", "classifies", "systems", "risk", "level", "imposes", "requirements"]}, {"id": "term-euclidean-distance", "t": "Euclidean Distance", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "The straight-line distance between two points in Euclidean space, computed as the square root of the sum of squared...", "l": "e", "k": ["euclidean", "distance", "straight-line", "points", "space", "computed", "square", "root", "sum", "squared", "differences", "across", "dimensions", "common", "metric"]}, {"id": "term-eurisko", "t": "Eurisko", "tg": ["History", "Systems"], "d": "history", "x": "An AI program developed by Douglas Lenat in 1981 that could discover new heuristics and concepts. Built as a successor...", "l": "e", "k": ["eurisko", "program", "developed", "douglas", "lenat", "discover", "heuristics", "concepts", "built", "successor", "famously", "won", "traveller", "trillion", "credit"]}, {"id": "term-evaluation", "t": "Evaluation", "tg": ["Process", "Quality"], "d": "general", "x": "The process of measuring model performance using metrics, benchmarks, and human assessment. Critical for comparing...", "l": "e", "k": ["evaluation", "process", "measuring", "model", "performance", "metrics", "benchmarks", "human", "assessment", "critical", "comparing", "models", "ensuring", "meet", "quality"]}, {"id": "term-evaluation-bias", "t": "Evaluation Bias", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Bias introduced during model evaluation when benchmark datasets or metrics do not adequately represent the diversity of...", "l": "e", "k": ["evaluation", "bias", "introduced", "model", "benchmark", "datasets", "metrics", "adequately", "represent", "diversity", "deployment", "population", "leading", "overly", "optimistic"]}, {"id": "term-eval-harness", "t": "Evaluation Harness", "tg": ["Tools", "Evaluation"], "d": "datasets", "x": "A framework for systematically testing AI models across multiple benchmarks and tasks. Popular harnesses include...", "l": "e", "k": ["evaluation", "harness", "framework", "systematically", "testing", "models", "across", "multiple", "benchmarks", "tasks", "popular", "harnesses", "include", "lm-evaluation-harness", "open-source"]}, {"id": "term-event-extraction", "t": "Event Extraction", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying event triggers and their arguments in text, determining what happened, who was involved, when,...", "l": "e", "k": ["event", "extraction", "task", "identifying", "triggers", "arguments", "text", "determining", "happened", "involved", "event-specific", "attributes"]}, {"id": "term-evidence-lower-bound", "t": "Evidence Lower Bound", "tg": ["Machine Learning", "Bayesian Methods"], "d": "general", "x": "A lower bound on the log marginal likelihood (model evidence) that serves as the objective function in variational...", "l": "e", "k": ["evidence", "lower", "bound", "log", "marginal", "likelihood", "model", "serves", "objective", "function", "variational", "inference", "maximizing", "elbo", "equivalent"]}, {"id": "term-evolution-strategies-rl", "t": "Evolution Strategies for RL", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "Black-box optimization methods that estimate policy gradients by perturbing parameters and evaluating returns, without...", "l": "e", "k": ["evolution", "strategies", "black-box", "optimization", "methods", "estimate", "policy", "gradients", "perturbing", "parameters", "evaluating", "returns", "without", "requiring", "backpropagation"]}, {"id": "term-evolutionary-computation", "t": "Evolutionary Computation", "tg": ["History", "Fundamentals"], "d": "history", "x": "A family of optimization algorithms inspired by biological evolution including genetic algorithms genetic programming...", "l": "e", "k": ["evolutionary", "computation", "family", "optimization", "algorithms", "inspired", "biological", "evolution", "including", "genetic", "programming", "strategies", "differential", "pioneered", "researchers"]}, {"id": "term-exact-match", "t": "Exact Match", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A strict evaluation metric that scores a prediction as correct only if it exactly matches the ground truth answer after...", "l": "e", "k": ["exact", "match", "strict", "evaluation", "metric", "scores", "prediction", "correct", "exactly", "matches", "ground", "truth", "answer", "normalization", "commonly"]}, {"id": "term-exact-nearest-neighbor", "t": "Exact Nearest Neighbor", "tg": ["Vector Database", "Search"], "d": "general", "x": "A search approach that guarantees finding the true closest vectors to a query by exhaustively computing distances to...", "l": "e", "k": ["exact", "nearest", "neighbor", "search", "approach", "guarantees", "finding", "true", "closest", "vectors", "query", "exhaustively", "computing", "distances", "index"]}, {"id": "term-executive-order-on-ai", "t": "Executive Order on AI", "tg": ["History", "Milestones"], "d": "history", "x": "US Executive Order 14110 on Safe Secure and Trustworthy Development and Use of Artificial Intelligence signed by...", "l": "e", "k": ["executive", "order", "safe", "secure", "trustworthy", "development", "artificial", "intelligence", "signed", "president", "biden", "october", "established", "standards", "safety"]}, {"id": "term-existential-risk-from-ai", "t": "Existential Risk from AI", "tg": ["AI Safety", "AI Ethics"], "d": "safety", "x": "The hypothesis that sufficiently advanced AI systems could pose a threat to the continued existence of humanity or...", "l": "e", "k": ["existential", "risk", "hypothesis", "sufficiently", "advanced", "systems", "pose", "threat", "continued", "existence", "humanity", "permanently", "curtail", "potential", "motivating"]}, {"id": "term-expectation-maximization", "t": "Expectation-Maximization", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "An iterative algorithm for finding maximum likelihood estimates in models with latent variables. It alternates between...", "l": "e", "k": ["expectation-maximization", "iterative", "algorithm", "finding", "maximum", "likelihood", "estimates", "models", "latent", "variables", "alternates", "computing", "expected", "values", "e-step"]}, {"id": "term-expectation-maximization-algorithm", "t": "Expectation-Maximization Algorithm", "tg": ["History", "Fundamentals"], "d": "history", "x": "An iterative statistical method for finding maximum likelihood estimates when data is incomplete or has latent...", "l": "e", "k": ["expectation-maximization", "algorithm", "iterative", "statistical", "method", "finding", "maximum", "likelihood", "estimates", "data", "incomplete", "latent", "variables", "published", "arthur"]}, {"id": "term-expected-calibration-error", "t": "Expected Calibration Error", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A scalar summary of calibration quality computed by binning predictions by confidence, calculating the absolute...", "l": "e", "k": ["expected", "calibration", "error", "scalar", "summary", "quality", "computed", "binning", "predictions", "confidence", "calculating", "absolute", "difference", "accuracy", "within"]}, {"id": "term-experience-replay", "t": "Experience Replay", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A technique where an agent stores past transitions in a replay buffer and samples from it during training, breaking...", "l": "e", "k": ["experience", "replay", "technique", "agent", "stores", "past", "transitions", "buffer", "samples", "training", "breaking", "temporal", "correlations", "consecutive", "improves"]}, {"id": "term-expert-parallelism", "t": "Expert Parallelism", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A distributed computing strategy for mixture-of-experts models where different expert subnetworks are placed on...", "l": "e", "k": ["expert", "parallelism", "distributed", "computing", "strategy", "mixture-of-experts", "models", "different", "subnetworks", "placed", "devices", "all-to-all", "communication", "routing", "tokens"]}, {"id": "term-expert-prompting", "t": "Expert Prompting", "tg": ["Prompt Engineering", "Persona"], "d": "general", "x": "A prompting method that instructs the model to first identify the most qualified expert identity for a given question,...", "l": "e", "k": ["expert", "prompting", "method", "instructs", "model", "identify", "qualified", "identity", "given", "question", "adopt", "perspective", "knowledge", "base", "provide"]}, {"id": "term-expert-systems", "t": "Expert Systems", "tg": ["History", "Milestones"], "d": "history", "x": "AI programs popular in the 1970s and 1980s that encoded human expert knowledge as if-then rules to solve...", "l": "e", "k": ["expert", "systems", "programs", "popular", "1970s", "1980s", "encoded", "human", "knowledge", "if-then", "rules", "solve", "domain-specific", "problems", "representing"]}, {"id": "term-explainability", "t": "Explainability (XAI)", "tg": ["Transparency", "Trust"], "d": "safety", "x": "The ability to understand and explain how AI models make decisions. Important for trust, debugging, regulatory...", "l": "e", "k": ["explainability", "xai", "ability", "understand", "explain", "models", "decisions", "important", "trust", "debugging", "regulatory", "compliance", "identifying", "potential", "biases"]}, {"id": "term-explainable-ai-history", "t": "Explainable AI History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of explainable AI from early rule-based systems that were inherently interpretable through the challenge...", "l": "e", "k": ["explainable", "history", "evolution", "early", "rule-based", "systems", "were", "inherently", "interpretable", "challenge", "black-box", "deep", "learning", "modern", "xai"]}, {"id": "term-exploding-gradient", "t": "Exploding Gradient", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A training instability where gradients grow exponentially large during backpropagation through deep networks, causing...", "l": "e", "k": ["exploding", "gradient", "training", "instability", "gradients", "grow", "exponentially", "large", "backpropagation", "deep", "networks", "causing", "weight", "updates", "become"]}, {"id": "term-exploding-gradient-problem", "t": "Exploding Gradient Problem", "tg": ["History", "Fundamentals"], "d": "history", "x": "A complementary problem to vanishing gradients where gradients grow exponentially during backpropagation causing...", "l": "e", "k": ["exploding", "gradient", "problem", "complementary", "vanishing", "gradients", "grow", "exponentially", "backpropagation", "causing", "unstable", "training", "solutions", "include", "clipping"]}, {"id": "term-exploration-vs-exploitation", "t": "Exploration vs Exploitation", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "The fundamental dilemma in RL between exploring unknown actions to discover potentially better strategies and...", "l": "e", "k": ["exploration", "exploitation", "fundamental", "dilemma", "exploring", "unknown", "actions", "discover", "potentially", "better", "strategies", "exploiting", "current", "knowledge", "maximize"]}, {"id": "term-exponential-distribution", "t": "Exponential Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution modeling the time between events in a Poisson process. It has the memoryless...", "l": "e", "k": ["exponential", "distribution", "continuous", "probability", "modeling", "time", "events", "poisson", "process", "memoryless", "property", "event", "next", "interval", "independent"]}, {"id": "term-exponential-family", "t": "Exponential Family", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A broad class of probability distributions characterized by a specific mathematical form, including normal, Poisson,...", "l": "e", "k": ["exponential", "family", "broad", "class", "probability", "distributions", "characterized", "specific", "mathematical", "form", "including", "normal", "poisson", "binomial", "gamma"]}, {"id": "term-exponential-moving-average-model", "t": "Exponential Moving Average Model", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A technique that maintains a running exponential average of model weights during training and uses the averaged model...", "l": "e", "k": ["exponential", "moving", "average", "model", "technique", "maintains", "running", "weights", "training", "uses", "averaged", "evaluation", "provides", "smoother", "stable"]}, {"id": "term-exponential-smoothing", "t": "Exponential Smoothing", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A family of time series forecasting methods that compute weighted averages of past observations with exponentially...", "l": "e", "k": ["exponential", "smoothing", "family", "time", "series", "forecasting", "methods", "compute", "weighted", "averages", "past", "observations", "exponentially", "decreasing", "weights"]}, {"id": "term-extra-trees", "t": "Extra Trees", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Extremely Randomized Trees is an ensemble method similar to random forests but with additional randomization. Splits...", "l": "e", "k": ["extra", "trees", "extremely", "randomized", "ensemble", "method", "similar", "random", "forests", "additional", "randomization", "splits", "chosen", "completely", "rather"]}, {"id": "term-extraction", "t": "Extraction", "tg": ["NLP Task", "Application"], "d": "general", "x": "Using AI to identify and pull specific information from unstructured text. Applications include named entity...", "l": "e", "k": ["extraction", "identify", "pull", "specific", "information", "unstructured", "text", "applications", "include", "named", "entity", "key", "phrase", "structured", "data"]}, {"id": "term-extractive-question-answering", "t": "Extractive Question Answering", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A QA task where the model identifies the answer as a contiguous span of text within a given context passage, predicting...", "l": "e", "k": ["extractive", "question", "answering", "task", "model", "identifies", "answer", "contiguous", "span", "text", "within", "given", "context", "passage", "predicting"]}, {"id": "term-extractive-summarization", "t": "Extractive Summarization", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A summarization approach that selects and concatenates the most important sentences or passages from the source...", "l": "e", "k": ["extractive", "summarization", "approach", "selects", "concatenates", "important", "sentences", "passages", "source", "document", "form", "summary", "without", "generating", "text"]}, {"id": "term-f-distribution", "t": "F-Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution arising as the ratio of two independent chi-square distributions, each divided by...", "l": "f", "k": ["f-distribution", "continuous", "probability", "distribution", "arising", "ratio", "independent", "chi-square", "distributions", "divided", "degrees", "freedom", "basis", "f-test", "anova"]}, {"id": "term-f1-score", "t": "F1 Score", "tg": ["Metrics", "Evaluation"], "d": "datasets", "x": "A metric combining precision and recall into a single score (their harmonic mean). Useful for evaluating classification...", "l": "f", "k": ["score", "metric", "combining", "precision", "recall", "single", "harmonic", "mean", "useful", "evaluating", "classification", "models", "especially", "imbalanced", "datasets"]}, {"id": "term-face-detection", "t": "Face Detection", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "The task of locating and bounding all human faces in an image regardless of pose, scale, or occlusion, using...", "l": "f", "k": ["face", "detection", "task", "locating", "bounding", "human", "faces", "image", "regardless", "pose", "scale", "occlusion", "specialized", "object", "architectures"]}, {"id": "term-face-recognition", "t": "Face Recognition", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A biometric identification task that determines the identity of a person from their facial features, using deep...", "l": "f", "k": ["face", "recognition", "biometric", "identification", "task", "determines", "identity", "person", "facial", "features", "deep", "learning", "models", "extract", "embeddings"]}, {"id": "term-face-verification", "t": "Face Verification", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A one-to-one matching task that determines whether two face images belong to the same person by comparing their face...", "l": "f", "k": ["face", "verification", "one-to-one", "matching", "task", "determines", "images", "belong", "person", "comparing", "embeddings", "typically", "distance", "threshold", "accept"]}, {"id": "term-facial-landmark-detection", "t": "Facial Landmark Detection", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of predicting the precise locations of key facial points (eyes, nose, mouth corners, jawline) in an image,...", "l": "f", "k": ["facial", "landmark", "detection", "task", "predicting", "precise", "locations", "key", "points", "eyes", "nose", "mouth", "corners", "jawline", "image"]}, {"id": "term-facial-recognition-ethics", "t": "Facial Recognition Ethics", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "The ethical debate around AI-powered facial recognition technology, encompassing concerns about surveillance, racial...", "l": "f", "k": ["facial", "recognition", "ethics", "ethical", "debate", "around", "ai-powered", "technology", "encompassing", "concerns", "surveillance", "racial", "bias", "accuracy", "consent"]}, {"id": "term-factor-analysis", "t": "Factor Analysis", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A statistical method that models observed variables as linear combinations of latent factors plus noise. Unlike PCA it...", "l": "f", "k": ["factor", "analysis", "statistical", "method", "models", "observed", "variables", "linear", "combinations", "latent", "factors", "plus", "noise", "unlike", "pca"]}, {"id": "term-factorized-embedding", "t": "Factorized Embedding", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A technique that decomposes the embedding matrix into two smaller matrices, separating the vocabulary embedding...", "l": "f", "k": ["factorized", "embedding", "technique", "decomposes", "matrix", "smaller", "matrices", "separating", "vocabulary", "dimension", "hidden", "reduce", "parameter", "count"]}, {"id": "term-factual-consistency", "t": "Factual Consistency", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation metric that measures whether claims in generated text are logically entailed by and consistent with...", "l": "f", "k": ["factual", "consistency", "evaluation", "metric", "measures", "claims", "generated", "text", "logically", "entailed", "consistent", "source", "documents", "verifiable", "facts"]}, {"id": "term-factual-grounding", "t": "Factual Grounding", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The process of anchoring a language model's responses to verified source documents or knowledge bases, ensuring outputs...", "l": "f", "k": ["factual", "grounding", "process", "anchoring", "language", "model", "responses", "verified", "source", "documents", "knowledge", "bases", "ensuring", "outputs", "supported"]}, {"id": "term-factuality", "t": "Factuality", "tg": ["Quality", "Challenge"], "d": "general", "x": "The degree to which AI outputs are accurate and true. A major challenge for LLMs, which can generate plausible-sounding...", "l": "f", "k": ["factuality", "degree", "outputs", "accurate", "true", "major", "challenge", "llms", "generate", "plausible-sounding", "incorrect", "information", "hallucinations"]}, {"id": "term-fair-founded", "t": "FAIR Founded", "tg": ["History", "Organizations"], "d": "history", "x": "The founding of Facebook AI Research (FAIR) in 2013 led by Yann LeCun. FAIR became one of the leading industrial AI...", "l": "f", "k": ["fair", "founded", "founding", "facebook", "research", "led", "yann", "lecun", "became", "leading", "industrial", "laboratories", "contributing", "significant", "advances"]}, {"id": "term-fairness", "t": "Fairness (AI)", "tg": ["Ethics", "Safety"], "d": "safety", "x": "The principle that AI systems should not discriminate or produce biased outcomes across different demographic groups....", "l": "f", "k": ["fairness", "principle", "systems", "discriminate", "produce", "biased", "outcomes", "across", "different", "demographic", "groups", "multiple", "mathematical", "definitions", "exist"]}, {"id": "term-fairness-in-machine-learning", "t": "Fairness in Machine Learning", "tg": ["History", "Fundamentals"], "d": "history", "x": "The study of how to ensure machine learning systems treat individuals and groups equitably. Research on ML fairness...", "l": "f", "k": ["fairness", "machine", "learning", "study", "ensure", "systems", "treat", "individuals", "groups", "equitably", "research", "addresses", "multiple", "definitions", "demographic"]}, {"id": "term-faiss", "t": "FAISS", "tg": ["Vector Database", "Libraries"], "d": "general", "x": "Facebook AI Similarity Search, an open-source library developed by Meta that provides highly optimized implementations...", "l": "f", "k": ["faiss", "facebook", "similarity", "search", "open-source", "library", "developed", "meta", "provides", "highly", "optimized", "implementations", "vector", "clustering", "algorithms"]}, {"id": "term-faithful-chain-of-thought", "t": "Faithful Chain-of-Thought", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A reasoning framework that decomposes questions into interleaved natural language reasoning and symbolic operations,...", "l": "f", "k": ["faithful", "chain-of-thought", "reasoning", "framework", "decomposes", "questions", "interleaved", "natural", "language", "symbolic", "operations", "ensuring", "chain", "actual", "computation"]}, {"id": "term-faithfulness", "t": "Faithfulness", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation dimension that measures whether generated text contains only information that is supported by and...", "l": "f", "k": ["faithfulness", "evaluation", "dimension", "measures", "generated", "text", "contains", "information", "supported", "consistent", "provided", "source", "context", "unfaithful", "content"]}, {"id": "term-falcon", "t": "Falcon", "tg": ["Models", "Technical"], "d": "models", "x": "A family of open-source language models developed by the Technology Innovation Institute in Abu Dhabi. Trained on the...", "l": "f", "k": ["falcon", "family", "open-source", "language", "models", "developed", "technology", "innovation", "institute", "abu", "dhabi", "trained", "refinedweb", "dataset", "falcon-180b"]}, {"id": "term-false-discovery-rate", "t": "False Discovery Rate", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The expected proportion of false positives among all rejected null hypotheses in multiple testing. The...", "l": "f", "k": ["false", "discovery", "rate", "expected", "proportion", "positives", "among", "rejected", "null", "hypotheses", "multiple", "testing", "benjamini-hochberg", "procedure", "controls"]}, {"id": "term-false-positive", "t": "False Positive / False Negative", "tg": ["Metrics", "Evaluation"], "d": "datasets", "x": "Classification errors: false positives incorrectly predict the positive class; false negatives miss actual positives....", "l": "f", "k": ["false", "positive", "negative", "classification", "errors", "positives", "incorrectly", "predict", "class", "negatives", "miss", "actual", "trade-off", "depends", "application"]}, {"id": "term-fast-fourier-transform", "t": "Fast Fourier Transform", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "An efficient algorithm for computing the discrete Fourier transform that reduces computational complexity from O(n^2)...", "l": "f", "k": ["fast", "fourier", "transform", "efficient", "algorithm", "computing", "discrete", "reduces", "computational", "complexity", "log", "discovered", "cooley", "tukey", "essential"]}, {"id": "term-faster-rcnn", "t": "Faster R-CNN", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A two-stage object detection framework that uses a Region Proposal Network (RPN) to generate candidate bounding boxes,...", "l": "f", "k": ["faster", "r-cnn", "two-stage", "object", "detection", "framework", "uses", "region", "proposal", "network", "rpn", "generate", "candidate", "bounding", "boxes"]}, {"id": "term-fasttext", "t": "FastText", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A word representation model that extends Word2Vec by representing each word as a bag of character n-grams, enabling...", "l": "f", "k": ["fasttext", "word", "representation", "model", "extends", "word2vec", "representing", "bag", "character", "n-grams", "enabling", "meaningful", "embeddings", "out-of-vocabulary", "words"]}, {"id": "term-fcos", "t": "FCOS", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Fully Convolutional One-Stage Object Detection, an anchor-free detector that directly predicts bounding boxes from...", "l": "f", "k": ["fcos", "fully", "convolutional", "one-stage", "object", "detection", "anchor-free", "detector", "directly", "predicts", "bounding", "boxes", "foreground", "pixel", "per-pixel"]}, {"id": "term-feature", "t": "Feature", "tg": ["Data", "ML Fundamentals"], "d": "general", "x": "An individual measurable property or characteristic of data used as input to a model. Good feature engineering can...", "l": "f", "k": ["feature", "individual", "measurable", "property", "characteristic", "data", "input", "model", "good", "engineering", "significantly", "improve", "performance"]}, {"id": "term-feature-engineering", "t": "Feature Engineering", "tg": ["History", "Fundamentals"], "d": "history", "x": "The process of using domain knowledge to create input features that improve machine learning model performance. Before...", "l": "f", "k": ["feature", "engineering", "process", "domain", "knowledge", "create", "input", "features", "improve", "machine", "learning", "model", "performance", "deep", "primary"]}, {"id": "term-feature-extraction", "t": "Feature Extraction", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "The process of constructing new features from raw data through transformations such as PCA, autoencoders, or...", "l": "f", "k": ["feature", "extraction", "process", "constructing", "features", "raw", "data", "transformations", "pca", "autoencoders", "domain-specific", "computations", "reducing", "dimensionality", "retaining"]}, {"id": "term-feature-hashing", "t": "Feature Hashing", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A dimensionality reduction technique that maps high-dimensional feature vectors to a lower-dimensional space using a...", "l": "f", "k": ["feature", "hashing", "dimensionality", "reduction", "technique", "maps", "high-dimensional", "vectors", "lower-dimensional", "space", "hash", "function", "avoiding", "need", "maintain"]}, {"id": "term-feature-importance", "t": "Feature Importance", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A measure of how much each input feature contributes to a model's predictions. Common methods include tree-based...", "l": "f", "k": ["feature", "importance", "measure", "input", "contributes", "model", "predictions", "common", "methods", "include", "tree-based", "impurity", "permutation", "shap", "values"]}, {"id": "term-feature-map", "t": "Feature Map", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The output of a convolutional layer representing the activation pattern produced by applying a specific filter to the...", "l": "f", "k": ["feature", "map", "output", "convolutional", "layer", "representing", "activation", "pattern", "produced", "applying", "specific", "filter", "input", "channel", "captures"]}, {"id": "term-feature-matching", "t": "Feature Matching", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of finding corresponding points between two images by detecting local features (keypoints) and comparing...", "l": "f", "k": ["feature", "matching", "process", "finding", "corresponding", "points", "images", "detecting", "local", "features", "keypoints", "comparing", "descriptors", "reconstruction", "slam"]}, {"id": "term-feature-pyramid-network", "t": "Feature Pyramid Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A multi-scale feature extraction architecture that builds a top-down pathway with lateral connections to produce...", "l": "f", "k": ["feature", "pyramid", "network", "multi-scale", "extraction", "architecture", "builds", "top-down", "pathway", "lateral", "connections", "produce", "semantically", "rich", "features"]}, {"id": "term-feature-scaling", "t": "Feature Scaling", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "The process of transforming numerical features to a common scale, such as standardization (zero mean, unit variance) or...", "l": "f", "k": ["feature", "scaling", "process", "transforming", "numerical", "features", "common", "scale", "standardization", "zero", "mean", "unit", "variance", "min-max", "prevent"]}, {"id": "term-feature-selection", "t": "Feature Selection", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "The process of identifying and selecting a subset of relevant features for model construction, reducing dimensionality,...", "l": "f", "k": ["feature", "selection", "process", "identifying", "selecting", "subset", "relevant", "features", "model", "construction", "reducing", "dimensionality", "mitigating", "overfitting", "improving"]}, {"id": "term-federated-averaging", "t": "Federated Averaging", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "The foundational algorithm for federated learning that trains models across decentralized devices by averaging locally...", "l": "f", "k": ["federated", "averaging", "foundational", "algorithm", "learning", "trains", "models", "across", "decentralized", "devices", "locally", "trained", "model", "updates", "device"]}, {"id": "term-federated-learning", "t": "Federated Learning", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "A machine learning approach where models are trained across multiple decentralized devices or servers holding local...", "l": "f", "k": ["federated", "learning", "machine", "approach", "models", "trained", "across", "multiple", "decentralized", "devices", "servers", "holding", "local", "data", "samples"]}, {"id": "term-feedforward", "t": "Feedforward Neural Network", "tg": ["Architecture", "Neural Networks"], "d": "models", "x": "A neural network where information flows in one direction from input to output, without cycles. The simplest type of...", "l": "f", "k": ["feedforward", "neural", "network", "information", "flows", "direction", "input", "output", "without", "cycles", "simplest", "type", "architecture"]}, {"id": "term-fei-fei-li", "t": "Fei-Fei Li", "tg": ["History", "Pioneers"], "d": "history", "x": "Chinese-American computer scientist who led the creation of the ImageNet dataset and competition, which was...", "l": "f", "k": ["fei-fei", "chinese-american", "computer", "scientist", "led", "creation", "imagenet", "dataset", "competition", "instrumental", "sparking", "deep", "learning", "revolution", "leading"]}, {"id": "term-feudal-network", "t": "Feudal Network", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A hierarchical RL architecture where a manager module sets abstract goals for a worker module that selects primitive...", "l": "f", "k": ["feudal", "network", "hierarchical", "architecture", "manager", "module", "sets", "abstract", "goals", "worker", "selects", "primitive", "actions", "approach", "decomposes"]}, {"id": "term-few-shot", "t": "Few-Shot Learning", "tg": ["Prompting", "Technique"], "d": "general", "x": "A technique where you provide a few examples in your prompt to help AI understand the pattern or format you want. More...", "l": "f", "k": ["few-shot", "learning", "technique", "provide", "examples", "prompt", "help", "understand", "pattern", "format", "want", "effective", "describing", "alone", "complex"]}, {"id": "term-few-shot-learning-history", "t": "Few-Shot Learning History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of few-shot learning from early work on one-shot learning (Li Fei-Fei 2003) through meta-learning...", "l": "f", "k": ["few-shot", "learning", "history", "evolution", "early", "work", "one-shot", "fei-fei", "meta-learning", "approaches", "maml", "in-context", "large", "language", "models"]}, {"id": "term-few-shot-object-detection", "t": "Few-Shot Object Detection", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Object detection methods that can learn to detect new categories from only a handful of annotated examples, using...", "l": "f", "k": ["few-shot", "object", "detection", "methods", "learn", "detect", "categories", "handful", "annotated", "examples", "meta-learning", "transfer", "learning", "generalize", "limited"]}, {"id": "term-fgsm", "t": "FGSM", "tg": ["Algorithms", "Safety"], "d": "algorithms", "x": "Fast Gradient Sign Method is an adversarial attack that perturbs inputs in the direction of the gradient of the loss...", "l": "f", "k": ["fgsm", "fast", "gradient", "sign", "method", "adversarial", "attack", "perturbs", "inputs", "direction", "loss", "respect", "input", "creates", "examples"]}, {"id": "term-fid-score", "t": "FID Score", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Frechet Inception Distance measures the quality of generated images by comparing the statistics of generated and real...", "l": "f", "k": ["fid", "score", "frechet", "inception", "distance", "measures", "quality", "generated", "images", "comparing", "statistics", "real", "image", "features", "extracted"]}, {"id": "term-fifth-generation-computer-project", "t": "Fifth Generation Computer Project", "tg": ["History", "Milestones"], "d": "history", "x": "A large-scale Japanese government initiative launched in 1982 aiming to create computers with AI capabilities using...", "l": "f", "k": ["fifth", "generation", "computer", "project", "large-scale", "japanese", "government", "initiative", "launched", "aiming", "create", "computers", "capabilities", "parallel", "processing"]}, {"id": "term-fill-in-the-middle", "t": "Fill-in-the-Middle", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A training objective where the model learns to generate text that fills a gap between a given prefix and suffix,...", "l": "f", "k": ["fill-in-the-middle", "training", "objective", "model", "learns", "generate", "text", "fills", "gap", "given", "prefix", "suffix", "enabling", "code", "completion"]}, {"id": "term-finbert", "t": "FinBERT", "tg": ["Models", "Technical"], "d": "models", "x": "A BERT model fine-tuned on financial text for sentiment analysis and other financial NLP tasks. Trained on financial...", "l": "f", "k": ["finbert", "bert", "model", "fine-tuned", "financial", "text", "sentiment", "analysis", "nlp", "tasks", "trained", "news", "corporate", "reports", "analyst"]}, {"id": "term-fine-tuning", "t": "Fine-Tuning", "tg": ["Training", "Customization"], "d": "general", "x": "The process of training a pre-existing AI model on additional, specialized data to improve its performance for specific...", "l": "f", "k": ["fine-tuning", "process", "training", "pre-existing", "model", "additional", "specialized", "data", "improve", "performance", "specific", "tasks", "domains", "efficient", "scratch"]}, {"id": "term-finite-state-machine", "t": "Finite State Machine", "tg": ["History", "Fundamentals"], "d": "history", "x": "A mathematical model of computation consisting of a finite number of states transitions between states and actions....", "l": "f", "k": ["finite", "state", "machine", "mathematical", "model", "computation", "consisting", "number", "states", "transitions", "actions", "extensively", "computer", "science", "pattern"]}, {"id": "term-first-ai-winter", "t": "First AI Winter", "tg": ["History", "Milestones"], "d": "history", "x": "The period from approximately 1974 to 1980 when AI research funding and interest declined sharply following the...", "l": "f", "k": ["winter", "period", "approximately", "research", "funding", "interest", "declined", "sharply", "following", "lighthill", "report", "criticism", "failure", "early", "deliver"]}, {"id": "term-fisher-information", "t": "Fisher Information", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A measure of the amount of information that an observable random variable carries about an unknown parameter. It...", "l": "f", "k": ["fisher", "information", "measure", "amount", "observable", "random", "variable", "carries", "unknown", "parameter", "quantifies", "sensitive", "likelihood", "function", "changes"]}, {"id": "term-fitted-q-iteration", "t": "Fitted Q-Iteration", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A batch RL algorithm that iteratively fits a Q-function approximator to Bellman backup targets computed from a fixed...", "l": "f", "k": ["fitted", "q-iteration", "batch", "algorithm", "iteratively", "fits", "q-function", "approximator", "bellman", "backup", "targets", "computed", "fixed", "dataset", "generalizes"]}, {"id": "term-fixmatch", "t": "FixMatch", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A semi-supervised learning algorithm that combines consistency regularization with pseudo-labeling. Generates...", "l": "f", "k": ["fixmatch", "semi-supervised", "learning", "algorithm", "combines", "consistency", "regularization", "pseudo-labeling", "generates", "pseudo-labels", "weakly", "augmented", "inputs", "trains", "strongly"]}, {"id": "term-flamingo", "t": "Flamingo", "tg": ["Models", "Technical"], "d": "models", "x": "A visual language model by DeepMind that processes interleaved image and text sequences using cross-attention layers...", "l": "f", "k": ["flamingo", "visual", "language", "model", "deepmind", "processes", "interleaved", "image", "text", "sequences", "cross-attention", "layers", "frozen", "vision", "encoder"]}, {"id": "term-flan", "t": "FLAN (Fine-tuned Language Net)", "tg": ["Training", "Google"], "d": "general", "x": "Google's approach to instruction tuning, fine-tuning models on many tasks described with natural language instructions....", "l": "f", "k": ["flan", "fine-tuned", "language", "net", "google", "approach", "instruction", "tuning", "fine-tuning", "models", "tasks", "described", "natural", "instructions", "showed"]}, {"id": "term-flan-t5", "t": "Flan-T5", "tg": ["Models", "Technical"], "d": "models", "x": "A version of T5 fine-tuned on a large collection of tasks phrased as instructions. Demonstrates that instruction tuning...", "l": "f", "k": ["flan-t5", "version", "fine-tuned", "large", "collection", "tasks", "phrased", "instructions", "demonstrates", "instruction", "tuning", "dramatically", "improves", "zero-shot", "few-shot"]}, {"id": "term-flash-attention", "t": "Flash Attention", "tg": ["Optimization", "Architecture"], "d": "models", "x": "An optimized attention algorithm that reduces memory usage and improves speed by tiling computations. Enables longer...", "l": "f", "k": ["flash", "attention", "optimized", "algorithm", "reduces", "memory", "usage", "improves", "speed", "tiling", "computations", "enables", "longer", "context", "windows"]}, {"id": "term-flash-decoding", "t": "Flash Decoding", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An optimized inference algorithm for autoregressive language models that parallelizes the attention computation across...", "l": "f", "k": ["flash", "decoding", "optimized", "inference", "algorithm", "autoregressive", "language", "models", "parallelizes", "attention", "computation", "across", "key-value", "sequence", "dimension"]}, {"id": "term-flashattention-2", "t": "FlashAttention-2", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "An optimized attention implementation that reduces memory I/O by tiling the computation to keep data in fast SRAM,...", "l": "f", "k": ["flashattention-2", "optimized", "attention", "implementation", "reduces", "memory", "tiling", "computation", "keep", "data", "fast", "sram", "avoiding", "materialization", "full"]}, {"id": "term-flat-index", "t": "Flat Index", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "A brute-force vector index that stores all vectors without compression or partitioning and performs exhaustive linear...", "l": "f", "k": ["flat", "index", "brute-force", "vector", "stores", "vectors", "without", "compression", "partitioning", "performs", "exhaustive", "linear", "search", "guaranteeing", "exact"]}, {"id": "term-fleiss-kappa", "t": "Fleiss' Kappa", "tg": ["Evaluation", "Methodology"], "d": "datasets", "x": "A statistical measure that extends Cohen's Kappa to assess inter-rater agreement among three or more annotators making...", "l": "f", "k": ["fleiss", "kappa", "statistical", "measure", "extends", "cohen", "assess", "inter-rater", "agreement", "among", "annotators", "making", "categorical", "judgments", "accounting"]}, {"id": "term-flipped-interaction", "t": "Flipped Interaction", "tg": ["Framework", "Interactive"], "d": "general", "x": "A method where you ask AI to ask you questions before providing advice. Leads to more personalized and relevant...", "l": "f", "k": ["flipped", "interaction", "method", "ask", "questions", "providing", "advice", "leads", "personalized", "relevant", "responses", "complex", "situations"]}, {"id": "term-flops", "t": "FLOPS", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "Floating-Point Operations Per Second, the standard measure of computational throughput for AI hardware. FLOPS is...", "l": "f", "k": ["flops", "floating-point", "operations", "per", "standard", "measure", "computational", "throughput", "hardware", "reported", "various", "precisions", "fp64", "fp32", "fp16"]}, {"id": "term-florence", "t": "Florence", "tg": ["Models", "Technical"], "d": "models", "x": "A foundation model for computer vision developed by Microsoft that unifies image classification object detection visual...", "l": "f", "k": ["florence", "foundation", "model", "computer", "vision", "developed", "microsoft", "unifies", "image", "classification", "object", "detection", "visual", "question", "answering"]}, {"id": "term-flow-matching", "t": "Flow Matching", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A generative modeling framework that learns continuous normalizing flows by regressing velocity fields that transport...", "l": "f", "k": ["flow", "matching", "generative", "modeling", "framework", "learns", "continuous", "normalizing", "flows", "regressing", "velocity", "fields", "transport", "samples", "noise"]}, {"id": "term-fluency-score", "t": "Fluency Score", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that evaluates the grammatical correctness, naturalness, and readability of generated text, often assessed...", "l": "f", "k": ["fluency", "score", "metric", "evaluates", "grammatical", "correctness", "naturalness", "readability", "generated", "text", "assessed", "human", "judgment", "proxy", "models"]}, {"id": "term-flux", "t": "Flux", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A family of image generation models that use a rectified flow transformer architecture (DiT) for diffusion, achieving...", "l": "f", "k": ["flux", "family", "image", "generation", "models", "rectified", "flow", "transformer", "architecture", "dit", "diffusion", "achieving", "state-of-the-art", "quality", "improved"]}, {"id": "term-fnet", "t": "FNet", "tg": ["Models", "Technical"], "d": "models", "x": "A model that replaces the self-attention layer in transformers with a simple Fourier transform. Achieves 92% of BERT...", "l": "f", "k": ["fnet", "model", "replaces", "self-attention", "layer", "transformers", "simple", "fourier", "transform", "achieves", "bert", "accuracy", "glue", "significantly", "faster"]}, {"id": "term-focal-loss", "t": "Focal Loss", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A modified cross-entropy loss that down-weights the contribution of easy examples and focuses training on hard,...", "l": "f", "k": ["focal", "loss", "modified", "cross-entropy", "down-weights", "contribution", "easy", "examples", "focuses", "training", "hard", "misclassified", "adding", "modulating", "factor"]}, {"id": "term-focal-loss-function", "t": "Focal Loss Function", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A modification of cross-entropy loss that down-weights easy examples and focuses training on hard misclassified...", "l": "f", "k": ["focal", "loss", "function", "modification", "cross-entropy", "down-weights", "easy", "examples", "focuses", "training", "hard", "misclassified", "addresses", "class", "imbalance"]}, {"id": "term-fomo", "t": "FOMO (AI Context)", "tg": ["Culture", "Learning"], "d": "general", "x": "Fear Of Missing Out on AI advancements. The rapid pace of AI development creates pressure to constantly learn new tools...", "l": "f", "k": ["fomo", "context", "fear", "missing", "advancements", "rapid", "pace", "development", "creates", "pressure", "constantly", "learn", "tools", "techniques", "balance"]}, {"id": "term-format-instruction", "t": "Format Instructions", "tg": ["Prompting", "Technique"], "d": "general", "x": "Explicit guidance in prompts about how AI should structure its response. Examples include requesting JSON, markdown...", "l": "f", "k": ["format", "instructions", "explicit", "guidance", "prompts", "structure", "response", "examples", "include", "requesting", "json", "markdown", "tables", "bullet", "points"]}, {"id": "term-forward-backward-algorithm", "t": "Forward-Backward Algorithm", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An inference algorithm for Hidden Markov Models that computes the posterior probability of each hidden state given the...", "l": "f", "k": ["forward-backward", "algorithm", "inference", "hidden", "markov", "models", "computes", "posterior", "probability", "state", "given", "entire", "observation", "sequence", "combines"]}, {"id": "term-foundation-model", "t": "Foundation Model", "tg": ["Model Type", "Architecture"], "d": "models", "x": "A large AI model trained on broad data that can be adapted to many downstream tasks. Examples include GPT-4, Claude,...", "l": "f", "k": ["foundation", "model", "large", "trained", "broad", "data", "adapted", "downstream", "tasks", "examples", "include", "gpt-4", "claude", "llama", "base"]}, {"id": "term-foundation-models", "t": "Foundation Models", "tg": ["History", "Milestones"], "d": "history", "x": "A term coined by researchers at Stanford in 2021 describing large AI models trained on broad data that can be adapted...", "l": "f", "k": ["foundation", "models", "term", "coined", "researchers", "stanford", "describing", "large", "trained", "broad", "data", "adapted", "wide", "range", "downstream"]}, {"id": "term-fourier-transform", "t": "Fourier Transform", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A mathematical transform that decomposes a function of time or space into its constituent frequencies. Converts signals...", "l": "f", "k": ["fourier", "transform", "mathematical", "decomposes", "function", "time", "space", "constituent", "frequencies", "converts", "signals", "spatial", "domain", "frequency", "fundamental"]}, {"id": "term-fp16", "t": "FP16 (Half Precision)", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "A 16-bit floating-point format with 5 exponent bits and 10 mantissa bits, offering half the memory footprint of FP32....", "l": "f", "k": ["fp16", "half", "precision", "16-bit", "floating-point", "format", "exponent", "bits", "mantissa", "offering", "memory", "footprint", "fp32", "widely", "mixed"]}, {"id": "term-fp32", "t": "FP32 (Single Precision)", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "The standard 32-bit floating-point format with 8 exponent bits and 23 mantissa bits, providing high numerical precision...", "l": "f", "k": ["fp32", "single", "precision", "standard", "32-bit", "floating-point", "format", "exponent", "bits", "mantissa", "providing", "high", "numerical", "model", "training"]}, {"id": "term-fp4", "t": "FP4 Quantization", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "An ultra-low precision 4-bit floating-point format for neural network weights, supported by next-generation AI hardware...", "l": "f", "k": ["fp4", "quantization", "ultra-low", "precision", "4-bit", "floating-point", "format", "neural", "network", "weights", "supported", "next-generation", "hardware", "nvidia", "blackwell"]}, {"id": "term-fp8", "t": "FP8 Precision", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "An 8-bit floating-point format introduced for AI training and inference, available in two variants: E4M3 (4 exponent, 3...", "l": "f", "k": ["fp8", "precision", "8-bit", "floating-point", "format", "introduced", "training", "inference", "available", "variants", "e4m3", "exponent", "mantissa", "bits", "forward"]}, {"id": "term-fpga-ai", "t": "FPGA for AI", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "Field-Programmable Gate Arrays reconfigured for AI workloads, offering customizable hardware logic that can be tailored...", "l": "f", "k": ["fpga", "field-programmable", "gate", "arrays", "reconfigured", "workloads", "offering", "customizable", "hardware", "logic", "tailored", "specific", "neural", "network", "architectures"]}, {"id": "term-frame-problem", "t": "Frame Problem", "tg": ["History", "Milestones"], "d": "history", "x": "A fundamental challenge in AI identified by McCarthy and Hayes in 1969 concerning how to represent the effects of...", "l": "f", "k": ["frame", "problem", "fundamental", "challenge", "identified", "mccarthy", "hayes", "concerning", "represent", "effects", "actions", "without", "explicitly", "specifying", "everything"]}, {"id": "term-frame-stacking", "t": "Frame Stacking", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A technique used in visual RL that concatenates multiple consecutive observation frames as input to the agent,...", "l": "f", "k": ["frame", "stacking", "technique", "visual", "concatenates", "multiple", "consecutive", "observation", "frames", "input", "agent", "providing", "temporal", "context", "enabling"]}, {"id": "term-framenet", "t": "FrameNet", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A lexical database that describes word meanings in terms of semantic frames, specifying the participants, props, and...", "l": "f", "k": ["framenet", "lexical", "database", "describes", "word", "meanings", "terms", "semantic", "frames", "specifying", "participants", "props", "conceptual", "roles", "associated"]}, {"id": "term-frames-minsky", "t": "Frames (Minsky)", "tg": ["History", "Milestones"], "d": "history", "x": "A knowledge representation scheme proposed by Marvin Minsky in 1974 where stereotyped situations are represented as...", "l": "f", "k": ["frames", "minsky", "knowledge", "representation", "scheme", "proposed", "marvin", "stereotyped", "situations", "represented", "data", "structures", "slots", "expected", "information"]}, {"id": "term-frames-theory", "t": "Frames Theory", "tg": ["History", "Fundamentals"], "d": "history", "x": "A knowledge representation approach proposed by Marvin Minsky in 1974 where knowledge is organized into frame...", "l": "f", "k": ["frames", "theory", "knowledge", "representation", "approach", "proposed", "marvin", "minsky", "organized", "frame", "structures", "containing", "slots", "attributes", "default"]}, {"id": "term-frank-rosenblatt", "t": "Frank Rosenblatt", "tg": ["History", "Pioneers"], "d": "history", "x": "American psychologist (1928-1971) who invented the perceptron in 1957, one of the earliest neural network models...", "l": "f", "k": ["frank", "rosenblatt", "american", "psychologist", "1928-1971", "invented", "perceptron", "earliest", "neural", "network", "models", "capable", "learning", "data", "pioneering"]}, {"id": "term-frank-wolfe-algorithm", "t": "Frank-Wolfe Algorithm", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A constrained optimization algorithm also known as conditional gradient that replaces the projection step with a linear...", "l": "f", "k": ["frank-wolfe", "algorithm", "constrained", "optimization", "known", "conditional", "gradient", "replaces", "projection", "step", "linear", "minimization", "constraint", "produces", "sparse"]}, {"id": "term-frequency-penalty", "t": "Frequency Penalty", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A parameter that penalizes tokens proportionally to how often they have appeared in the output so far, encouraging...", "l": "f", "k": ["frequency", "penalty", "parameter", "penalizes", "tokens", "proportionally", "appeared", "output", "far", "encouraging", "lexical", "diversity", "generated", "text"]}, {"id": "term-frontier-ai", "t": "Frontier AI", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "The most capable AI models at the cutting edge of capability, which may pose novel safety risks not present in less...", "l": "f", "k": ["frontier", "capable", "models", "cutting", "edge", "capability", "pose", "novel", "safety", "risks", "present", "less", "systems", "term", "governance"]}, {"id": "term-frontier-model", "t": "Frontier Model", "tg": ["Policy", "Research"], "d": "safety", "x": "The most capable AI models at any given time, pushing the boundaries of what's possible. Term often used in AI policy...", "l": "f", "k": ["frontier", "model", "capable", "models", "given", "time", "pushing", "boundaries", "possible", "term", "policy", "discussions", "governance", "powerful", "systems"]}, {"id": "term-frontier-model-forum", "t": "Frontier Model Forum", "tg": ["Governance", "AI Safety"], "d": "safety", "x": "An industry body established in 2023 by major AI companies to promote responsible development and deployment of...", "l": "f", "k": ["frontier", "model", "forum", "industry", "body", "established", "major", "companies", "promote", "responsible", "development", "deployment", "models", "focusing", "safety"]}, {"id": "term-fsdp", "t": "FSDP", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Fully Sharded Data Parallel, a PyTorch training strategy that shards model parameters, gradients, and optimizer states...", "l": "f", "k": ["fsdp", "fully", "sharded", "data", "parallel", "pytorch", "training", "strategy", "shards", "model", "parameters", "gradients", "optimizer", "states", "across"]}, {"id": "term-ft-transformer", "t": "FT-Transformer", "tg": ["Models", "Technical"], "d": "models", "x": "Feature Tokenizer plus Transformer applies the transformer architecture to tabular data by converting each feature into...", "l": "f", "k": ["ft-transformer", "feature", "tokenizer", "plus", "transformer", "applies", "architecture", "tabular", "data", "converting", "embedding", "token", "demonstrates", "transformers", "match"]}, {"id": "term-full-text-search", "t": "Full-Text Search", "tg": ["Retrieval", "Search"], "d": "general", "x": "A search technique that examines all words in every stored document to find matches for a query, typically using...", "l": "f", "k": ["full-text", "search", "technique", "examines", "words", "stored", "document", "find", "matches", "query", "typically", "inverted", "indexes", "tokenization", "stemming"]}, {"id": "term-fully-sharded-data-parallel", "t": "Fully Sharded Data Parallel", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A memory-efficient distributed training technique that shards model parameters gradients and optimizer states across...", "l": "f", "k": ["fully", "sharded", "data", "parallel", "memory-efficient", "distributed", "training", "technique", "shards", "model", "parameters", "gradients", "optimizer", "states", "across"]}, {"id": "term-function-calling", "t": "Function Calling", "tg": ["Capability", "Agents"], "d": "general", "x": "An LLM capability that allows models to generate structured output to call external functions or APIs. Enables AI...", "l": "f", "k": ["function", "calling", "llm", "capability", "allows", "models", "generate", "structured", "output", "call", "external", "functions", "apis", "enables", "agents"]}, {"id": "term-functional-correctness", "t": "Functional Correctness", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "An evaluation criterion for code generation that assesses whether generated code produces correct outputs for all test...", "l": "f", "k": ["functional", "correctness", "evaluation", "criterion", "code", "generation", "assesses", "generated", "produces", "correct", "outputs", "test", "inputs", "measured", "execution-based"]}, {"id": "term-fusion-retrieval", "t": "Fusion Retrieval", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "A retrieval paradigm that combines results from multiple diverse retrieval methods or models through score fusion, rank...", "l": "f", "k": ["fusion", "retrieval", "paradigm", "combines", "results", "multiple", "diverse", "methods", "models", "score", "rank", "learned", "combination", "leveraging", "complementary"]}, {"id": "term-future-of-life-institute", "t": "Future of Life Institute", "tg": ["History", "Organizations"], "d": "history", "x": "A nonprofit organization founded in 2014 focused on existential risks from advanced technology particularly AI. FLI...", "l": "f", "k": ["future", "life", "institute", "nonprofit", "organization", "founded", "focused", "existential", "risks", "advanced", "technology", "particularly", "fli", "published", "open"]}, {"id": "term-fuzzy-logic", "t": "Fuzzy Logic", "tg": ["History", "Milestones"], "d": "history", "x": "A form of many-valued logic introduced by Lotfi Zadeh in 1965 that handles degrees of truth rather than binary...", "l": "f", "k": ["fuzzy", "logic", "form", "many-valued", "introduced", "lotfi", "zadeh", "handles", "degrees", "truth", "rather", "binary", "true", "false", "values"]}, {"id": "term-fuzzy-matching", "t": "Fuzzy Matching", "tg": ["NLP", "Technique"], "d": "general", "x": "Finding approximate rather than exact matches in text. Useful for handling typos, variations, and similar-meaning terms...", "l": "f", "k": ["fuzzy", "matching", "finding", "approximate", "rather", "exact", "matches", "text", "useful", "handling", "typos", "variations", "similar-meaning", "terms", "search"]}, {"id": "term-g-eval", "t": "G-Eval", "tg": ["Evaluation", "LLM-Based"], "d": "models", "x": "A framework that uses large language models with chain-of-thought prompting to evaluate natural language generation...", "l": "g", "k": ["g-eval", "framework", "uses", "large", "language", "models", "chain-of-thought", "prompting", "evaluate", "natural", "generation", "quality", "achieving", "high", "correlation"]}, {"id": "term-galactica", "t": "Galactica", "tg": ["Models", "Technical"], "d": "models", "x": "A large language model trained by Meta AI on 48 million scientific papers citations and other academic content....", "l": "g", "k": ["galactica", "large", "language", "model", "trained", "meta", "million", "scientific", "papers", "citations", "academic", "content", "designed", "knowledge", "tasks"]}, {"id": "term-gamma-distribution", "t": "Gamma Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A two-parameter family of continuous probability distributions that generalizes the exponential distribution. It models...", "l": "g", "k": ["gamma", "distribution", "two-parameter", "family", "continuous", "probability", "distributions", "generalizes", "exponential", "models", "waiting", "time", "specified", "number", "events"]}, {"id": "term-gan", "t": "GAN (Generative Adversarial Network)", "tg": ["Architecture", "Generative"], "d": "models", "x": "A neural network architecture with two competing networks: a generator creating content and a discriminator evaluating...", "l": "g", "k": ["gan", "generative", "adversarial", "network", "neural", "architecture", "competing", "networks", "generator", "creating", "content", "discriminator", "evaluating", "pioneered", "realistic"]}, {"id": "term-gan-discriminator", "t": "GAN Discriminator", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The component of a generative adversarial network that learns to distinguish real data from generated samples,...", "l": "g", "k": ["gan", "discriminator", "component", "generative", "adversarial", "network", "learns", "distinguish", "real", "data", "generated", "samples", "providing", "training", "signal"]}, {"id": "term-gan-generator", "t": "GAN Generator", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The component of a generative adversarial network that transforms random noise into synthetic data samples, trained to...", "l": "g", "k": ["gan", "generator", "component", "generative", "adversarial", "network", "transforms", "random", "noise", "synthetic", "data", "samples", "trained", "fool", "discriminator"]}, {"id": "term-gan-invention", "t": "GAN Invention", "tg": ["History", "Milestones"], "d": "history", "x": "The invention of Generative Adversarial Networks by Ian Goodfellow and colleagues in 2014. GANs train two neural...", "l": "g", "k": ["gan", "invention", "generative", "adversarial", "networks", "ian", "goodfellow", "colleagues", "gans", "train", "neural", "generator", "discriminator", "competition", "enabling"]}, {"id": "term-gan-inversion", "t": "GAN Inversion", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of finding the latent code in a pre-trained GAN's latent space that best reconstructs a given real image,...", "l": "g", "k": ["gan", "inversion", "process", "finding", "latent", "code", "pre-trained", "space", "best", "reconstructs", "given", "real", "image", "enabling", "gan-based"]}, {"id": "term-gated-linear-unit", "t": "Gated Linear Unit", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An activation mechanism that multiplies a linear projection of the input by a sigmoid-gated linear projection, allowing...", "l": "g", "k": ["gated", "linear", "unit", "activation", "mechanism", "multiplies", "projection", "input", "sigmoid-gated", "allowing", "network", "control", "information", "flow", "commonly"]}, {"id": "term-gaussian-kernel", "t": "Gaussian Kernel", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A kernel function based on the Gaussian (normal) distribution, commonly used in kernel density estimation, smoothing,...", "l": "g", "k": ["gaussian", "kernel", "function", "based", "normal", "distribution", "commonly", "density", "estimation", "smoothing", "svms", "bandwidth", "parameter", "controls", "width"]}, {"id": "term-gaussian-mixture-model", "t": "Gaussian Mixture Model", "tg": ["Machine Learning", "Clustering"], "d": "general", "x": "A probabilistic model that represents data as generated from a mixture of a finite number of Gaussian distributions...", "l": "g", "k": ["gaussian", "mixture", "model", "probabilistic", "represents", "data", "generated", "finite", "number", "distributions", "unknown", "parameters", "typically", "estimated", "via"]}, {"id": "term-gaussian-process", "t": "Gaussian Process", "tg": ["Machine Learning", "Bayesian Methods"], "d": "general", "x": "A non-parametric Bayesian model that defines a probability distribution over functions, where any finite collection of...", "l": "g", "k": ["gaussian", "process", "non-parametric", "bayesian", "model", "defines", "probability", "distribution", "functions", "finite", "collection", "function", "values", "follows", "multivariate"]}, {"id": "term-gaussian-splatting", "t": "Gaussian Splatting", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A 3D scene representation that models scenes as collections of 3D Gaussian primitives, enabling real-time rendering of...", "l": "g", "k": ["gaussian", "splatting", "scene", "representation", "models", "scenes", "collections", "primitives", "enabling", "real-time", "rendering", "photorealistic", "novel", "views", "efficient"]}, {"id": "term-gaze-estimation", "t": "Gaze Estimation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of predicting where a person is looking based on their eye appearance and head pose in images or video, used...", "l": "g", "k": ["gaze", "estimation", "task", "predicting", "person", "looking", "based", "eye", "appearance", "head", "pose", "images", "video", "attention", "analysis"]}, {"id": "term-gazetteer", "t": "Gazetteer", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A curated list of entity names organized by type, such as person names, locations, or organizations, used as a feature...", "l": "g", "k": ["gazetteer", "curated", "list", "entity", "names", "organized", "type", "person", "locations", "organizations", "feature", "lookup", "resource", "named", "recognition"]}, {"id": "term-gdpr-ai-provisions", "t": "GDPR AI Provisions", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Provisions within the EU General Data Protection Regulation relevant to AI, including the right not to be subject to...", "l": "g", "k": ["gdpr", "provisions", "within", "general", "data", "protection", "regulation", "relevant", "including", "right", "subject", "purely", "automated", "decisions", "impact"]}, {"id": "term-gelu", "t": "GELU (Gaussian Error Linear Unit)", "tg": ["Architecture", "Function"], "d": "models", "x": "An activation function commonly used in transformers that applies a smooth, probabilistic non-linearity. Outperforms...", "l": "g", "k": ["gelu", "gaussian", "error", "linear", "unit", "activation", "function", "commonly", "transformers", "applies", "smooth", "probabilistic", "non-linearity", "outperforms", "relu"]}, {"id": "term-gemini", "t": "Gemini", "tg": ["Model", "Google"], "d": "models", "x": "Google's family of multimodal AI models that can process text, images, audio, and video. Powers Google's AI features...", "l": "g", "k": ["gemini", "google", "family", "multimodal", "models", "process", "text", "images", "audio", "video", "powers", "features", "including", "bard", "workspace"]}, {"id": "term-gemini-ai", "t": "Gemini (AI)", "tg": ["History", "Systems"], "d": "history", "x": "A multimodal AI model family developed by Google DeepMind announced in December 2023. Gemini was designed from the...", "l": "g", "k": ["gemini", "multimodal", "model", "family", "developed", "google", "deepmind", "announced", "december", "designed", "ground", "natively", "understanding", "generating", "text"]}, {"id": "term-gemini-flash", "t": "Gemini Flash", "tg": ["Models", "Technical"], "d": "models", "x": "A lightweight model in the Gemini family optimized for speed and efficiency. Designed for high-volume low-latency...", "l": "g", "k": ["gemini", "flash", "lightweight", "model", "family", "optimized", "speed", "efficiency", "designed", "high-volume", "low-latency", "applications", "fast", "responses", "prioritized"]}, {"id": "term-gemini-launch", "t": "Gemini Launch", "tg": ["History", "Milestones"], "d": "history", "x": "Google DeepMind's release of Gemini in December 2023, a family of multimodal large language models designed to process...", "l": "g", "k": ["gemini", "launch", "google", "deepmind", "release", "december", "family", "multimodal", "large", "language", "models", "designed", "process", "text", "images"]}, {"id": "term-gemini-pro", "t": "Gemini Pro", "tg": ["Models", "Technical"], "d": "models", "x": "A mid-tier model in Google's Gemini family designed for a wide range of tasks. Powers many Google AI products and is...", "l": "g", "k": ["gemini", "pro", "mid-tier", "model", "google", "family", "designed", "wide", "range", "tasks", "powers", "products", "available", "api", "balances"]}, {"id": "term-gemma", "t": "Gemma", "tg": ["Models", "Technical"], "d": "models", "x": "A family of lightweight open models built by Google DeepMind from the same technology used to create Gemini. Available...", "l": "g", "k": ["gemma", "family", "lightweight", "open", "models", "built", "google", "deepmind", "technology", "create", "gemini", "available", "parameter", "sizes", "optimized"]}, {"id": "term-gen-2", "t": "Gen-2", "tg": ["Models", "Technical"], "d": "models", "x": "A multimodal AI model by Runway for generating and editing video from text images or existing video. One of the first...", "l": "g", "k": ["gen-2", "multimodal", "model", "runway", "generating", "editing", "video", "text", "images", "existing", "commercially", "available", "generation", "tools", "supports"]}, {"id": "term-general-problem-solver", "t": "General Problem Solver", "tg": ["History", "Milestones"], "d": "history", "x": "An AI program developed by Newell and Simon in 1957 that used means-ends analysis to solve a wide range of formalized...", "l": "g", "k": ["general", "problem", "solver", "program", "developed", "newell", "simon", "means-ends", "analysis", "solve", "wide", "range", "formalized", "problems", "representing"]}, {"id": "term-generalization", "t": "Generalization", "tg": ["Concept", "Quality"], "d": "general", "x": "A model's ability to perform well on new, unseen data rather than just memorizing training examples. The fundamental...", "l": "g", "k": ["generalization", "model", "ability", "perform", "unseen", "data", "rather", "memorizing", "training", "examples", "fundamental", "goal", "machine", "learning"]}, {"id": "term-generalized-advantage-estimation", "t": "Generalized Advantage Estimation (GAE)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A technique that computes advantage function estimates using an exponentially-weighted average of multi-step TD errors,...", "l": "g", "k": ["generalized", "advantage", "estimation", "gae", "technique", "computes", "function", "estimates", "exponentially-weighted", "average", "multi-step", "errors", "controlled", "lambda", "parameter"]}, {"id": "term-generalized-linear-model", "t": "Generalized Linear Model", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A flexible generalization of ordinary linear regression that allows the response variable to follow any distribution...", "l": "g", "k": ["generalized", "linear", "model", "flexible", "generalization", "ordinary", "regression", "allows", "response", "variable", "follow", "distribution", "exponential", "family", "link"]}, {"id": "term-generated-knowledge-prompting", "t": "Generated Knowledge Prompting", "tg": ["Prompt Engineering", "Knowledge Augmentation"], "d": "general", "x": "A technique where the model first generates relevant knowledge or facts about a topic, then uses that self-generated...", "l": "g", "k": ["generated", "knowledge", "prompting", "technique", "model", "generates", "relevant", "facts", "topic", "uses", "self-generated", "additional", "context", "answer", "downstream"]}, {"id": "term-generation", "t": "Generation", "tg": ["Process", "Core Concept"], "d": "general", "x": "The process of producing new content from an AI model. Text generation works by predicting one token at a time; image...", "l": "g", "k": ["generation", "process", "producing", "content", "model", "text", "works", "predicting", "token", "time", "image", "uses", "diffusion", "similar", "processes"]}, {"id": "term-gail", "t": "Generative Adversarial Imitation Learning (GAIL)", "tg": ["Reinforcement Learning", "Imitation"], "d": "general", "x": "An imitation learning algorithm that uses a GAN-like framework where a discriminator distinguishes between agent and...", "l": "g", "k": ["generative", "adversarial", "imitation", "learning", "gail", "algorithm", "uses", "gan-like", "framework", "discriminator", "distinguishes", "agent", "expert", "trajectories", "policy"]}, {"id": "term-gan-history", "t": "Generative Adversarial Network History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of GANs from Ian Goodfellow's 2014 invention through progressive improvements including DCGAN,...", "l": "g", "k": ["generative", "adversarial", "network", "history", "development", "gans", "ian", "goodfellow", "invention", "progressive", "improvements", "including", "dcgan", "stylegan", "biggan"]}, {"id": "term-generative-ai", "t": "Generative AI", "tg": ["Field", "Category"], "d": "general", "x": "AI systems that can create new content (text, images, code, music, video) rather than just analyzing existing data....", "l": "g", "k": ["generative", "systems", "create", "content", "text", "images", "code", "music", "video", "rather", "analyzing", "existing", "data", "includes", "chatbots"]}, {"id": "term-generative-ai-era", "t": "Generative AI Era", "tg": ["History", "Milestones"], "d": "history", "x": "The period beginning approximately in 2022 with the release of ChatGPT DALL-E 2 and Stable Diffusion when generative AI...", "l": "g", "k": ["generative", "era", "period", "beginning", "approximately", "release", "chatgpt", "dall-e", "stable", "diffusion", "models", "capable", "creating", "text", "images"]}, {"id": "term-generative-question-answering", "t": "Generative Question Answering", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A QA approach where the model generates a free-form answer in natural language rather than extracting a span, enabling...", "l": "g", "k": ["generative", "question", "answering", "approach", "model", "generates", "free-form", "answer", "natural", "language", "rather", "extracting", "span", "enabling", "responses"]}, {"id": "term-genetic-algorithm", "t": "Genetic Algorithm", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "An evolutionary optimization algorithm inspired by natural selection. Maintains a population of candidate solutions...", "l": "g", "k": ["genetic", "algorithm", "evolutionary", "optimization", "inspired", "natural", "selection", "maintains", "population", "candidate", "solutions", "undergo", "crossover", "mutation", "generations"]}, {"id": "term-genetic-algorithms", "t": "Genetic Algorithms", "tg": ["History", "Milestones"], "d": "history", "x": "Optimization algorithms inspired by natural selection, developed by John Holland in the 1960s-1970s, that evolve...", "l": "g", "k": ["genetic", "algorithms", "optimization", "inspired", "natural", "selection", "developed", "john", "holland", "1960s-1970s", "evolve", "candidate", "solutions", "crossover", "mutation"]}, {"id": "term-geoffrey-hinton", "t": "Geoffrey Hinton", "tg": ["History", "Pioneers"], "d": "history", "x": "British-Canadian computer scientist known as a godfather of deep learning, who co-developed backpropagation, Boltzmann...", "l": "g", "k": ["geoffrey", "hinton", "british-canadian", "computer", "scientist", "known", "godfather", "deep", "learning", "co-developed", "backpropagation", "boltzmann", "machines", "belief", "networks"]}, {"id": "term-geometric-augmentation", "t": "Geometric Augmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Image augmentation techniques that modify the spatial arrangement of pixels including rotation, translation, scaling,...", "l": "g", "k": ["geometric", "augmentation", "image", "techniques", "modify", "spatial", "arrangement", "pixels", "including", "rotation", "translation", "scaling", "shearing", "flipping", "perspective"]}, {"id": "term-georgetown-ibm-experiment", "t": "Georgetown-IBM Experiment", "tg": ["History", "Milestones"], "d": "history", "x": "A 1954 demonstration of automatic translation of Russian sentences into English using an IBM 701 computer. Though...", "l": "g", "k": ["georgetown-ibm", "experiment", "demonstration", "automatic", "translation", "russian", "sentences", "english", "ibm", "computer", "limited", "words", "six", "grammar", "rules"]}, {"id": "term-gguf", "t": "GGUF", "tg": ["LLM", "Inference"], "d": "models", "x": "A binary file format designed for storing quantized language models optimized for CPU and hybrid CPU/GPU inference,...", "l": "g", "k": ["gguf", "binary", "file", "format", "designed", "storing", "quantized", "language", "models", "optimized", "cpu", "hybrid", "gpu", "inference", "commonly"]}, {"id": "term-gh200", "t": "GH200 Grace Hopper Superchip", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "NVIDIA's integrated CPU-GPU superchip combining a Grace ARM CPU with a Hopper H200 GPU connected by a high-bandwidth...", "l": "g", "k": ["gh200", "grace", "hopper", "superchip", "nvidia", "integrated", "cpu-gpu", "combining", "arm", "cpu", "h200", "gpu", "connected", "high-bandwidth", "nvlink-c2c"]}, {"id": "term-ghost-work", "t": "Ghost Work", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "The often invisible human labor that powers AI systems, including data labeling, content moderation, and quality...", "l": "g", "k": ["ghost", "work", "invisible", "human", "labor", "powers", "systems", "including", "data", "labeling", "content", "moderation", "quality", "assurance", "frequently"]}, {"id": "term-gibbs-sampling", "t": "Gibbs Sampling", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "An MCMC method that samples each variable in turn from its conditional distribution given the current values of all...", "l": "g", "k": ["gibbs", "sampling", "mcmc", "method", "samples", "variable", "turn", "conditional", "distribution", "given", "current", "values", "variables", "efficient", "distributions"]}, {"id": "term-gini-impurity", "t": "Gini Impurity", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A measure of the probability that a randomly chosen sample would be misclassified if labeled according to the class...", "l": "g", "k": ["gini", "impurity", "measure", "probability", "randomly", "chosen", "sample", "misclassified", "labeled", "according", "class", "distribution", "node", "commonly", "splitting"]}, {"id": "term-github-copilot", "t": "GitHub Copilot", "tg": ["Product", "Development"], "d": "general", "x": "An AI pair programmer integrated into code editors. Uses LLMs to suggest code completions, write functions, and explain...", "l": "g", "k": ["github", "copilot", "pair", "programmer", "integrated", "code", "editors", "uses", "llms", "suggest", "completions", "write", "functions", "explain", "successful"]}, {"id": "term-github-copilot-launch", "t": "GitHub Copilot Launch", "tg": ["History", "Milestones"], "d": "history", "x": "The public release of GitHub Copilot in June 2022 an AI pair programming tool powered by OpenAI Codex. Copilot...", "l": "g", "k": ["github", "copilot", "launch", "public", "release", "june", "pair", "programming", "tool", "powered", "openai", "codex", "generates", "code", "suggestions"]}, {"id": "term-global-average-pooling", "t": "Global Average Pooling", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A pooling operation that computes the mean of each feature map across all spatial dimensions, reducing the feature map...", "l": "g", "k": ["global", "average", "pooling", "operation", "computes", "mean", "feature", "map", "across", "spatial", "dimensions", "reducing", "single", "value", "per"]}, {"id": "term-glove", "t": "GloVe", "tg": ["NLP", "Embeddings"], "d": "general", "x": "Global Vectors for Word Representation, a word embedding method that trains on aggregated word co-occurrence statistics...", "l": "g", "k": ["glove", "global", "vectors", "word", "representation", "embedding", "method", "trains", "aggregated", "co-occurrence", "statistics", "corpus", "combining", "matrix", "factorization"]}, {"id": "term-glu", "t": "GLU", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Gated Linear Unit that splits input into two halves and applies a sigmoid gate to one half then multiplies...", "l": "g", "k": ["glu", "gated", "linear", "unit", "splits", "input", "halves", "applies", "sigmoid", "gate", "half", "multiplies", "element-wise", "introduced", "language"]}, {"id": "term-glue-benchmark", "t": "GLUE Benchmark", "tg": ["History", "Milestones"], "d": "history", "x": "The General Language Understanding Evaluation benchmark introduced in 2018 consisting of nine natural language...", "l": "g", "k": ["glue", "benchmark", "general", "language", "understanding", "evaluation", "introduced", "consisting", "nine", "natural", "tasks", "provided", "standardized", "evaluate", "models"]}, {"id": "term-goal-conditioned-rl", "t": "Goal-Conditioned RL", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "An RL formulation where the agent's policy and value function are conditioned on a goal specifying what the agent...", "l": "g", "k": ["goal-conditioned", "formulation", "agent", "policy", "value", "function", "conditioned", "goal", "specifying", "achieve", "policies", "enable", "multi-task", "learning", "generalization"]}, {"id": "term-goedel-escher-bach", "t": "Goedel Escher Bach", "tg": ["History", "Milestones"], "d": "history", "x": "A 1979 book by Douglas Hofstadter exploring common themes in the works of mathematician Kurt Goedel artist M.C. Escher...", "l": "g", "k": ["goedel", "escher", "bach", "book", "douglas", "hofstadter", "exploring", "common", "themes", "works", "mathematician", "kurt", "artist", "composer", "johann"]}, {"id": "term-goedels-incompleteness-theorems", "t": "Goedel's Incompleteness Theorems", "tg": ["History", "Fundamentals"], "d": "history", "x": "Two theorems published by Kurt Goedel in 1931 showing that any consistent formal system capable of expressing basic...", "l": "g", "k": ["goedel", "incompleteness", "theorems", "published", "kurt", "showing", "consistent", "formal", "system", "capable", "expressing", "basic", "arithmetic", "contains", "statements"]}, {"id": "term-gofai-good-old-fashioned-ai", "t": "GOFAI (Good Old-Fashioned AI)", "tg": ["History", "Fundamentals"], "d": "history", "x": "A term coined by philosopher John Haugeland in 1985 referring to classical symbolic AI approaches based on physical...", "l": "g", "k": ["gofai", "good", "old-fashioned", "term", "coined", "philosopher", "john", "haugeland", "referring", "classical", "symbolic", "approaches", "based", "physical", "symbol"]}, {"id": "term-goodharts-law-in-ai", "t": "Goodhart's Law in AI", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The principle that when a proxy measure becomes the target for optimization, it ceases to be a good measure. In AI,...", "l": "g", "k": ["goodhart", "law", "principle", "proxy", "measure", "becomes", "target", "optimization", "ceases", "good", "manifests", "models", "over-optimize", "reward", "diverging"]}, {"id": "term-google-brain", "t": "Google Brain", "tg": ["History", "Milestones"], "d": "history", "x": "A deep learning research project founded at Google in 2011 by Andrew Ng and Jeff Dean that demonstrated unsupervised...", "l": "g", "k": ["google", "brain", "deep", "learning", "research", "project", "founded", "andrew", "jeff", "dean", "demonstrated", "unsupervised", "youtube", "videos", "became"]}, {"id": "term-google-brain-founded", "t": "Google Brain Founded", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of Google Brain as a deep learning research project within Google in 2011 by Andrew Ng and Jeff Dean. The...", "l": "g", "k": ["google", "brain", "founded", "founding", "deep", "learning", "research", "project", "within", "andrew", "jeff", "dean", "team", "achieved", "breakthrough"]}, {"id": "term-google-deepmind", "t": "Google DeepMind", "tg": ["History", "Milestones"], "d": "history", "x": "An AI research laboratory formed in April 2023 by merging Google Brain and DeepMind, created from the original DeepMind...", "l": "g", "k": ["google", "deepmind", "research", "laboratory", "formed", "april", "merging", "brain", "created", "original", "technologies", "founded", "demis", "hassabis", "shane"]}, {"id": "term-tpu-v5", "t": "Google TPU v5", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "The fifth generation of Google's Tensor Processing Unit featuring improved matrix multiply units, increased memory...", "l": "g", "k": ["google", "tpu", "fifth", "generation", "tensor", "processing", "unit", "featuring", "improved", "matrix", "multiply", "units", "increased", "memory", "bandwidth"]}, {"id": "term-google-translate-neural-mt", "t": "Google Translate Neural MT", "tg": ["History", "Milestones"], "d": "history", "x": "Google's 2016 transition from statistical to neural machine translation using sequence-to-sequence models with...", "l": "g", "k": ["google", "translate", "neural", "transition", "statistical", "machine", "translation", "sequence-to-sequence", "models", "attention", "dramatically", "improving", "quality", "bringing", "networks"]}, {"id": "term-googlenet", "t": "GoogLeNet", "tg": ["Models", "History"], "d": "models", "x": "The winner of the 2014 ImageNet competition also known as Inception v1. Introduced the inception module that processes...", "l": "g", "k": ["googlenet", "winner", "imagenet", "competition", "known", "inception", "introduced", "module", "processes", "input", "multiple", "filter", "sizes", "parallel", "achieved"]}, {"id": "term-gopher", "t": "Gopher", "tg": ["Models", "Technical"], "d": "models", "x": "A 280 billion parameter language model by DeepMind that advanced understanding of language model scaling. Provided...", "l": "g", "k": ["gopher", "billion", "parameter", "language", "model", "deepmind", "advanced", "understanding", "scaling", "provided", "comprehensive", "analysis", "behavior", "across", "diverse"]}, {"id": "term-gpqa", "t": "GPQA", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Graduate-Level Google-Proof Question Answering, an extremely difficult benchmark of expert-crafted questions across...", "l": "g", "k": ["gpqa", "graduate-level", "google-proof", "question", "answering", "extremely", "difficult", "benchmark", "expert-crafted", "questions", "across", "biology", "physics", "chemistry", "domain"]}, {"id": "term-gpt", "t": "GPT (Generative Pre-trained Transformer)", "tg": ["Model", "OpenAI"], "d": "models", "x": "OpenAI's series of large language models. GPT-4 is the latest major version, known for strong reasoning, multimodal...", "l": "g", "k": ["gpt", "generative", "pre-trained", "transformer", "openai", "series", "large", "language", "models", "gpt-4", "latest", "major", "version", "known", "strong"]}, {"id": "term-gpt-scaling-laws", "t": "GPT Scaling Laws", "tg": ["History", "Milestones"], "d": "history", "x": "Empirical power-law relationships discovered by Kaplan et al. at OpenAI in 2020 showing that language model performance...", "l": "g", "k": ["gpt", "scaling", "laws", "empirical", "power-law", "relationships", "discovered", "kaplan", "openai", "showing", "language", "model", "performance", "improves", "predictably"]}, {"id": "term-gpt-1", "t": "GPT-1", "tg": ["History", "Milestones"], "d": "history", "x": "The first Generative Pre-trained Transformer model released by OpenAI in June 2018, demonstrating that unsupervised...", "l": "g", "k": ["gpt-1", "generative", "pre-trained", "transformer", "model", "released", "openai", "june", "demonstrating", "unsupervised", "pre-training", "large", "text", "corpora", "followed"]}, {"id": "term-gpt-2", "t": "GPT-2", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A 1.5-billion parameter autoregressive language model by OpenAI that demonstrated strong text generation capabilities...", "l": "g", "k": ["gpt-2", "5-billion", "parameter", "autoregressive", "language", "model", "openai", "demonstrated", "strong", "text", "generation", "capabilities", "initially", "withheld", "full"]}, {"id": "term-gpt-3", "t": "GPT-3", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A 175-billion parameter autoregressive transformer model by OpenAI that popularized few-shot and zero-shot learning...", "l": "g", "k": ["gpt-3", "175-billion", "parameter", "autoregressive", "transformer", "model", "openai", "popularized", "few-shot", "zero-shot", "learning", "in-context", "prompting", "without", "fine-tuning"]}, {"id": "term-gpt-35", "t": "GPT-3.5", "tg": ["Models", "Fundamentals"], "d": "models", "x": "An improved version of GPT-3 fine-tuned with reinforcement learning from human feedback. Powers the initial release of...", "l": "g", "k": ["gpt-3", "improved", "version", "fine-tuned", "reinforcement", "learning", "human", "feedback", "powers", "initial", "release", "chatgpt", "demonstrates", "significant", "improvements"]}, {"id": "term-gpt-4", "t": "GPT-4", "tg": ["History", "Milestones"], "d": "history", "x": "OpenAI's multimodal large language model released in March 2023, capable of processing both text and images,...", "l": "g", "k": ["gpt-4", "openai", "multimodal", "large", "language", "model", "released", "march", "capable", "processing", "text", "images", "demonstrating", "human-level", "performance"]}, {"id": "term-gpt-4o", "t": "GPT-4o", "tg": ["Models", "Technical"], "d": "models", "x": "A multimodal model from OpenAI that natively processes text audio and images. Features faster response times and...", "l": "g", "k": ["gpt-4o", "multimodal", "model", "openai", "natively", "processes", "text", "audio", "images", "features", "faster", "response", "times", "improved", "capabilities"]}, {"id": "term-gpt-4v", "t": "GPT-4V", "tg": ["Models", "Technical"], "d": "models", "x": "GPT-4 with Vision extends GPT-4 with the ability to understand and reason about images. Accepts interleaved text and...", "l": "g", "k": ["gpt-4v", "gpt-4", "vision", "extends", "ability", "understand", "reason", "images", "accepts", "interleaved", "text", "image", "inputs", "enabling", "visual"]}, {"id": "term-gpt-j", "t": "GPT-J", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A 6-billion parameter open-source autoregressive language model created by EleutherAI, notable for being one of the...", "l": "g", "k": ["gpt-j", "6-billion", "parameter", "open-source", "autoregressive", "language", "model", "created", "eleutherai", "notable", "performant", "alternatives", "gpt-3"]}, {"id": "term-gpt-neox", "t": "GPT-NeoX", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A 20-billion parameter autoregressive language model by EleutherAI that uses rotary positional embeddings and parallel...", "l": "g", "k": ["gpt-neox", "20-billion", "parameter", "autoregressive", "language", "model", "eleutherai", "uses", "rotary", "positional", "embeddings", "parallel", "attention-feedforward", "computation", "improved"]}, {"id": "term-gpt-score", "t": "GPT-Score", "tg": ["Evaluation", "LLM-Based"], "d": "models", "x": "An evaluation framework that leverages generative pre-trained models to score text quality by computing conditional...", "l": "g", "k": ["gpt-score", "evaluation", "framework", "leverages", "generative", "pre-trained", "models", "score", "text", "quality", "computing", "conditional", "generation", "probabilities", "assessing"]}, {"id": "term-gptq", "t": "GPTQ", "tg": ["LLM", "Inference"], "d": "models", "x": "A post-training quantization method for large language models that uses approximate second-order information to...", "l": "g", "k": ["gptq", "post-training", "quantization", "method", "large", "language", "models", "uses", "approximate", "second-order", "information", "compress", "weights", "lower", "bit"]}, {"id": "term-gpu", "t": "GPU (Graphics Processing Unit)", "tg": ["Hardware", "Infrastructure"], "d": "hardware", "x": "Hardware originally designed for graphics that excels at the parallel computations needed for AI. NVIDIA GPUs are the...", "l": "g", "k": ["gpu", "graphics", "processing", "unit", "hardware", "originally", "designed", "excels", "parallel", "computations", "needed", "nvidia", "gpus", "dominant", "training"]}, {"id": "term-gpu-architecture", "t": "GPU Architecture for AI", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "The parallel processing design of graphics processing units optimized for AI workloads, featuring thousands of cores...", "l": "g", "k": ["gpu", "architecture", "parallel", "processing", "design", "graphics", "units", "optimized", "workloads", "featuring", "thousands", "cores", "organized", "streaming", "multiprocessors"]}, {"id": "term-gpu-computing-revolution", "t": "GPU Computing Revolution", "tg": ["History", "Milestones"], "d": "history", "x": "The adoption of graphics processing units for general-purpose computing and machine learning beginning around...", "l": "g", "k": ["gpu", "computing", "revolution", "adoption", "graphics", "processing", "units", "general-purpose", "machine", "learning", "beginning", "around", "2007-2012", "nvidia", "cuda"]}, {"id": "term-gpu-direct", "t": "GPU Direct", "tg": ["Distributed Computing", "GPU"], "d": "hardware", "x": "NVIDIA's technology suite enabling direct data transfers between GPUs and network adapters or storage without staging...", "l": "g", "k": ["gpu", "direct", "nvidia", "technology", "suite", "enabling", "data", "transfers", "gpus", "network", "adapters", "storage", "without", "staging", "cpu"]}, {"id": "term-gpu-memory-hierarchy", "t": "GPU Memory Hierarchy", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "The layered memory system in GPUs consisting of registers, shared memory (SRAM), L2 cache, and global memory...", "l": "g", "k": ["gpu", "memory", "hierarchy", "layered", "system", "gpus", "consisting", "registers", "shared", "sram", "cache", "global", "hbm", "gddr", "understanding"]}, {"id": "term-grace-hopper", "t": "Grace Hopper", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist and United States Navy rear admiral who developed the first compiler (A-0 System) in 1952....", "l": "g", "k": ["grace", "hopper", "american", "computer", "scientist", "united", "states", "navy", "rear", "admiral", "developed", "compiler", "a-0", "system", "pioneered"]}, {"id": "term-grad-cam", "t": "Grad-CAM", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Gradient-weighted Class Activation Mapping, a visualization method that uses gradients flowing into the final...", "l": "g", "k": ["grad-cam", "gradient-weighted", "class", "activation", "mapping", "visualization", "method", "uses", "gradients", "flowing", "final", "convolutional", "layer", "produce", "heatmap"]}, {"id": "term-gradient", "t": "Gradient", "tg": ["Training", "Math"], "d": "general", "x": "A vector indicating the direction and magnitude of change needed to reduce a model's error. The foundation of gradient...", "l": "g", "k": ["gradient", "vector", "indicating", "direction", "magnitude", "change", "needed", "reduce", "model", "error", "foundation", "descent", "optimization", "training", "neural"]}, {"id": "term-gradient-accumulation", "t": "Gradient Accumulation", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A technique that simulates larger batch sizes by accumulating gradients over multiple forward-backward passes before...", "l": "g", "k": ["gradient", "accumulation", "technique", "simulates", "larger", "batch", "sizes", "accumulating", "gradients", "multiple", "forward-backward", "passes", "performing", "parameter", "update"]}, {"id": "term-gradient-boosting", "t": "Gradient Boosting", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An ensemble technique that builds models sequentially, with each new model trained to correct the residual errors of...", "l": "g", "k": ["gradient", "boosting", "ensemble", "technique", "builds", "models", "sequentially", "model", "trained", "correct", "residual", "errors", "combined", "far", "descent"]}, {"id": "term-gradient-boosting-history", "t": "Gradient Boosting History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of gradient boosting from the work of Jerome Friedman (2001) through XGBoost (Chen and Guestrin 2016)...", "l": "g", "k": ["gradient", "boosting", "history", "development", "work", "jerome", "friedman", "xgboost", "chen", "guestrin", "lightgbm", "catboost", "boosted", "trees", "became"]}, {"id": "term-gradient-checkpointing", "t": "Gradient Checkpointing", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A memory optimization technique that trades computation for memory by only storing activations at selected checkpoints...", "l": "g", "k": ["gradient", "checkpointing", "memory", "optimization", "technique", "trades", "computation", "storing", "activations", "selected", "checkpoints", "forward", "pass", "recomputing", "intermediate"]}, {"id": "term-gradient-clipping", "t": "Gradient Clipping", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A technique that rescales or truncates gradients when their norm exceeds a specified threshold, preventing the...", "l": "g", "k": ["gradient", "clipping", "technique", "rescales", "truncates", "gradients", "norm", "exceeds", "specified", "threshold", "preventing", "exploding", "problem", "destabilize", "training"]}, {"id": "term-gradient-descent", "t": "Gradient Descent", "tg": ["Algorithm", "Training"], "d": "algorithms", "x": "The optimization algorithm that trains neural networks by iteratively adjusting weights in the direction that reduces...", "l": "g", "k": ["gradient", "descent", "optimization", "algorithm", "trains", "neural", "networks", "iteratively", "adjusting", "weights", "direction", "reduces", "error", "variants", "include"]}, {"id": "term-gradient-surgery", "t": "Gradient Surgery", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A technique for multi-task learning that modifies conflicting gradients from different tasks to reduce interference....", "l": "g", "k": ["gradient", "surgery", "technique", "multi-task", "learning", "modifies", "conflicting", "gradients", "different", "tasks", "reduce", "interference", "projects", "oppose", "enabling"]}, {"id": "term-gradient-synchronization", "t": "Gradient Synchronization", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "The process of aggregating gradients across multiple GPUs or nodes in distributed training, typically via all-reduce....", "l": "g", "k": ["gradient", "synchronization", "process", "aggregating", "gradients", "across", "multiple", "gpus", "nodes", "distributed", "training", "typically", "via", "all-reduce", "synchronous"]}, {"id": "term-grammar-constrained-decoding", "t": "Grammar-Constrained Decoding", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A decoding approach that restricts token generation to sequences valid under a formal grammar (such as BNF or regex),...", "l": "g", "k": ["grammar-constrained", "decoding", "approach", "restricts", "token", "generation", "sequences", "valid", "formal", "grammar", "bnf", "regex", "ensuring", "outputs", "always"]}, {"id": "term-granger-causality", "t": "Granger Causality", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A statistical concept where a time series X is said to Granger-cause Y if past values of X provide statistically...", "l": "g", "k": ["granger", "causality", "statistical", "concept", "time", "series", "said", "granger-cause", "past", "values", "provide", "statistically", "significant", "information", "future"]}, {"id": "term-graph-attention", "t": "Graph Attention", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An attention mechanism applied to graph-structured data that learns to weight the importance of different neighbor...", "l": "g", "k": ["graph", "attention", "mechanism", "applied", "graph-structured", "data", "learns", "weight", "importance", "different", "neighbor", "nodes", "introduced", "networks", "allowing"]}, {"id": "term-gat", "t": "Graph Attention Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A graph neural network that uses attention mechanisms to weight the importance of neighboring nodes' features during...", "l": "g", "k": ["graph", "attention", "network", "neural", "uses", "mechanisms", "weight", "importance", "neighboring", "nodes", "features", "aggregation", "learning", "focus", "relevant"]}, {"id": "term-graph-convolution", "t": "Graph Convolution", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An operation that extends convolution to graph-structured data by aggregating features from neighboring nodes. Spectral...", "l": "g", "k": ["graph", "convolution", "operation", "extends", "graph-structured", "data", "aggregating", "features", "neighboring", "nodes", "spectral", "approaches", "fourier", "transforms", "spatial"]}, {"id": "term-gcn", "t": "Graph Convolutional Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A neural network that operates on graph-structured data by aggregating features from neighboring nodes through...", "l": "g", "k": ["graph", "convolutional", "network", "neural", "operates", "graph-structured", "data", "aggregating", "features", "neighboring", "nodes", "learnable", "operations", "defined", "topology"]}, {"id": "term-gin", "t": "Graph Isomorphism Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A graph neural network provably as powerful as the Weisfeiler-Lehman graph isomorphism test, using a sum aggregator and...", "l": "g", "k": ["graph", "isomorphism", "network", "neural", "provably", "powerful", "weisfeiler-lehman", "test", "sum", "aggregator", "mlp", "update", "function", "maximize", "discriminative"]}, {"id": "term-graph-neural-network", "t": "Graph Neural Network", "tg": ["Models", "Fundamentals"], "d": "models", "x": "A class of neural networks designed to operate on graph-structured data. Learns node edge or graph-level...", "l": "g", "k": ["graph", "neural", "network", "class", "networks", "designed", "operate", "graph-structured", "data", "learns", "node", "edge", "graph-level", "representations", "aggregating"]}, {"id": "term-graph-neural-network-history", "t": "Graph Neural Network History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of neural networks for graph-structured data from early spectral approaches (Bruna et al. 2013) through...", "l": "g", "k": ["graph", "neural", "network", "history", "development", "networks", "graph-structured", "data", "early", "spectral", "approaches", "bruna", "convolutional", "kipf", "welling"]}, {"id": "term-graph-rag", "t": "Graph RAG", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A retrieval-augmented generation approach that builds a knowledge graph from source documents and uses graph traversal...", "l": "g", "k": ["graph", "rag", "retrieval-augmented", "generation", "approach", "builds", "knowledge", "source", "documents", "uses", "traversal", "retrieve", "structured", "interconnected", "context"]}, {"id": "term-graph-based-index", "t": "Graph-Based Index", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "A vector index structure that organizes vectors as nodes in a proximity graph where edges connect similar vectors,...", "l": "g", "k": ["graph-based", "index", "vector", "structure", "organizes", "vectors", "nodes", "proximity", "graph", "edges", "connect", "similar", "enabling", "efficient", "nearest"]}, {"id": "term-graph-based-parsing", "t": "Graph-Based Parsing", "tg": ["NLP", "Parsing"], "d": "general", "x": "A parsing approach that scores all possible dependency edges simultaneously and finds the highest-scoring tree using...", "l": "g", "k": ["graph-based", "parsing", "approach", "scores", "possible", "dependency", "edges", "simultaneously", "finds", "highest-scoring", "tree", "algorithms", "maximum", "spanning", "typically"]}, {"id": "term-graphcore", "t": "Graphcore", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "A semiconductor company that developed the Intelligence Processing Unit (IPU), featuring a massive number of...", "l": "g", "k": ["graphcore", "semiconductor", "company", "developed", "intelligence", "processing", "unit", "ipu", "featuring", "massive", "number", "independent", "processor", "cores", "large"]}, {"id": "term-graphsage", "t": "GraphSAGE", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A framework for inductive representation learning on graphs that samples and aggregates features from a node's local...", "l": "g", "k": ["graphsage", "framework", "inductive", "representation", "learning", "graphs", "samples", "aggregates", "features", "node", "local", "neighborhood", "enabling", "generalization", "unseen"]}, {"id": "term-greedy-decoding", "t": "Greedy Decoding", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A deterministic text generation strategy that always selects the token with the highest probability at each step,...", "l": "g", "k": ["greedy", "decoding", "deterministic", "text", "generation", "strategy", "always", "selects", "token", "highest", "probability", "step", "producing", "likely", "sequence"]}, {"id": "term-grid-search", "t": "Grid Search", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A hyperparameter tuning method that exhaustively evaluates all combinations of specified parameter values, typically...", "l": "g", "k": ["grid", "search", "hyperparameter", "tuning", "method", "exhaustively", "evaluates", "combinations", "specified", "parameter", "values", "typically", "combined", "cross-validation", "select"]}, {"id": "term-griffin", "t": "Griffin", "tg": ["Models", "Technical"], "d": "models", "x": "A hybrid model by Google DeepMind that combines recurrent layers based on linear recurrences with local attention....", "l": "g", "k": ["griffin", "hybrid", "model", "google", "deepmind", "combines", "recurrent", "layers", "based", "linear", "recurrences", "local", "attention", "achieves", "strong"]}, {"id": "term-grok", "t": "Grok", "tg": ["Product", "Model"], "d": "models", "x": "An AI assistant developed by xAI (Elon Musk's AI company). Integrated with X (Twitter) and known for real-time...", "l": "g", "k": ["grok", "assistant", "developed", "xai", "elon", "musk", "company", "integrated", "twitter", "known", "real-time", "information", "access", "less", "restrictive"]}, {"id": "term-groq", "t": "Groq", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "An AI hardware company that developed the Language Processing Unit (LPU), a deterministic architecture using...", "l": "g", "k": ["groq", "hardware", "company", "developed", "language", "processing", "unit", "lpu", "deterministic", "architecture", "software-defined", "scheduling", "achieve", "extremely", "low-latency"]}, {"id": "term-ground-truth", "t": "Ground Truth", "tg": ["Data", "Evaluation"], "d": "datasets", "x": "The correct answer or label used to evaluate model predictions. Obtained through human annotation, measurement, or...", "l": "g", "k": ["ground", "truth", "correct", "answer", "label", "evaluate", "model", "predictions", "obtained", "human", "annotation", "measurement", "authoritative", "sources"]}, {"id": "term-grounding", "t": "Grounding", "tg": ["Technique", "Accuracy"], "d": "general", "x": "Connecting AI outputs to verified information sources to reduce hallucinations and increase accuracy. Often involves...", "l": "g", "k": ["grounding", "connecting", "outputs", "verified", "information", "sources", "reduce", "hallucinations", "increase", "accuracy", "involves", "retrieval-augmented", "generation", "rag", "real-time"]}, {"id": "term-grounding-dino", "t": "Grounding DINO", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "An open-set object detection model that combines a DINO-based detector with grounded pre-training, enabling detection...", "l": "g", "k": ["grounding", "dino", "open-set", "object", "detection", "model", "combines", "dino-based", "detector", "grounded", "pre-training", "enabling", "arbitrary", "objects", "specified"]}, {"id": "term-group-fairness", "t": "Group Fairness", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Fairness criteria that require statistical parity of outcomes or error rates across demographic groups defined by...", "l": "g", "k": ["group", "fairness", "criteria", "require", "statistical", "parity", "outcomes", "error", "rates", "across", "demographic", "groups", "defined", "protected", "attributes"]}, {"id": "term-group-normalization", "t": "Group Normalization", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A normalization method that divides channels into groups and normalizes within each group independently, providing...", "l": "g", "k": ["group", "normalization", "method", "divides", "channels", "groups", "normalizes", "within", "independently", "providing", "stable", "performance", "regardless", "batch", "size"]}, {"id": "term-group-relative-policy-optimization", "t": "Group Relative Policy Optimization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A reinforcement learning algorithm for language model alignment that uses group-based normalization of advantages...", "l": "g", "k": ["group", "relative", "policy", "optimization", "reinforcement", "learning", "algorithm", "language", "model", "alignment", "uses", "group-based", "normalization", "advantages", "rather"]}, {"id": "term-grouped-query-attention", "t": "Grouped Query Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that groups multiple query heads to share a single key-value head, interpolating between...", "l": "g", "k": ["grouped", "query", "attention", "mechanism", "groups", "multiple", "heads", "share", "single", "key-value", "head", "interpolating", "multi-head", "multi-query", "balance"]}, {"id": "term-gru", "t": "GRU", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Gated Recurrent Unit, a recurrent neural network variant that uses reset and update gates to control information flow,...", "l": "g", "k": ["gru", "gated", "recurrent", "unit", "neural", "network", "variant", "uses", "reset", "update", "gates", "control", "information", "flow", "offering"]}, {"id": "term-gshard", "t": "GShard", "tg": ["Models", "Technical"], "d": "models", "x": "A framework for scaling giant models across thousands of devices using conditional computation. Uses top-2 gating in...", "l": "g", "k": ["gshard", "framework", "scaling", "giant", "models", "across", "thousands", "devices", "conditional", "computation", "uses", "top-2", "gating", "mixture-of-experts", "layers"]}, {"id": "term-gsm8k", "t": "GSM8K", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Grade School Math 8K, a benchmark of 8,500 linguistically diverse grade-school-level math word problems requiring...", "l": "g", "k": ["gsm8k", "grade", "school", "math", "benchmark", "linguistically", "diverse", "grade-school-level", "word", "problems", "requiring", "multi-step", "arithmetic", "reasoning", "widely"]}, {"id": "term-guardrails", "t": "Guardrails", "tg": ["Safety", "Constraint"], "d": "safety", "x": "Safety mechanisms that constrain AI behavior to prevent harmful outputs. Include content filters, output validators,...", "l": "g", "k": ["guardrails", "safety", "mechanisms", "constrain", "behavior", "prevent", "harmful", "outputs", "include", "content", "filters", "output", "validators", "behavioral", "restrictions"]}, {"id": "term-guided-generation", "t": "Guided Generation", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "Techniques that constrain language model output to conform to a specified format (such as JSON schema or grammar rules)...", "l": "g", "k": ["guided", "generation", "techniques", "constrain", "language", "model", "output", "conform", "specified", "format", "json", "schema", "grammar", "rules", "masking"]}, {"id": "term-gumbel-max-trick", "t": "Gumbel-Max Trick", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A method for sampling from categorical distributions using Gumbel noise. Adds independent Gumbel noise to...", "l": "g", "k": ["gumbel-max", "trick", "method", "sampling", "categorical", "distributions", "gumbel", "noise", "adds", "independent", "log-probabilities", "takes", "argmax", "enables", "efficient"]}, {"id": "term-gumbel-softmax", "t": "Gumbel-Softmax", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A continuous relaxation of categorical sampling that allows gradient-based optimization of discrete variables. Uses the...", "l": "g", "k": ["gumbel-softmax", "continuous", "relaxation", "categorical", "sampling", "allows", "gradient-based", "optimization", "discrete", "variables", "uses", "gumbel-max", "trick", "temperature-controlled", "softmax"]}, {"id": "term-gym-environment", "t": "Gym Environment", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "An interface standard and collection of benchmark environments originally developed by OpenAI for RL research. The Gym...", "l": "g", "k": ["gym", "environment", "interface", "standard", "collection", "benchmark", "environments", "originally", "developed", "openai", "research", "api", "defines", "common", "protocol"]}, {"id": "term-habana", "t": "Habana Labs", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "An Intel subsidiary producing the Gaudi series of AI training accelerators featuring integrated RoCE networking and...", "l": "h", "k": ["habana", "labs", "intel", "subsidiary", "producing", "gaudi", "series", "training", "accelerators", "featuring", "integrated", "roce", "networking", "high", "memory"]}, {"id": "term-hallucination", "t": "Hallucination", "tg": ["Limitation", "Risk"], "d": "safety", "x": "When AI generates information that sounds plausible but is actually incorrect or fabricated. Includes fake citations,...", "l": "h", "k": ["hallucination", "generates", "information", "sounds", "plausible", "actually", "incorrect", "fabricated", "includes", "fake", "citations", "invented", "statistics", "fictional", "events"]}, {"id": "term-hallucination-mitigation", "t": "Hallucination Mitigation", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A collection of techniques designed to reduce factually incorrect or fabricated content in LLM outputs, including...", "l": "h", "k": ["hallucination", "mitigation", "collection", "techniques", "designed", "reduce", "factually", "incorrect", "fabricated", "content", "llm", "outputs", "including", "retrieval", "augmentation"]}, {"id": "term-hallucination-rate", "t": "Hallucination Rate", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that quantifies the proportion of generated content that contains fabricated facts, unsupported claims, or...", "l": "h", "k": ["hallucination", "rate", "metric", "quantifies", "proportion", "generated", "content", "contains", "fabricated", "facts", "unsupported", "claims", "information", "contradicting", "source"]}, {"id": "term-halting-problem", "t": "Halting Problem", "tg": ["History", "Fundamentals"], "d": "history", "x": "A decision problem proved undecidable by Alan Turing in 1936. The halting problem asks whether a given program will...", "l": "h", "k": ["halting", "problem", "decision", "proved", "undecidable", "alan", "turing", "asks", "given", "program", "eventually", "halt", "run", "forever", "proof"]}, {"id": "term-hamiltonian-monte-carlo", "t": "Hamiltonian Monte Carlo", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "An MCMC method that uses Hamiltonian dynamics to propose distant samples with high acceptance probability, reducing...", "l": "h", "k": ["hamiltonian", "monte", "carlo", "mcmc", "method", "uses", "dynamics", "propose", "distant", "samples", "high", "acceptance", "probability", "reducing", "random"]}, {"id": "term-hamming-distance", "t": "Hamming Distance", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "The number of positions at which corresponding symbols in two equal-length strings or vectors differ. It is used as a...", "l": "h", "k": ["hamming", "distance", "number", "positions", "corresponding", "symbols", "equal-length", "strings", "vectors", "differ", "metric", "comparing", "binary", "codes", "error"]}, {"id": "term-hand-pose-estimation", "t": "Hand Pose Estimation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of predicting the 3D positions of hand joints and fingertips from images or depth sensors, enabling gesture...", "l": "h", "k": ["hand", "pose", "estimation", "task", "predicting", "positions", "joints", "fingertips", "images", "depth", "sensors", "enabling", "gesture", "recognition", "tracking"]}, {"id": "term-hans-moravec", "t": "Hans Moravec", "tg": ["History", "Pioneers"], "d": "history", "x": "Austrian-Canadian roboticist at Carnegie Mellon University known for Moravec's Paradox and pioneering work in mobile...", "l": "h", "k": ["hans", "moravec", "austrian-canadian", "roboticist", "carnegie", "mellon", "university", "known", "paradox", "pioneering", "work", "mobile", "robot", "navigation", "books"]}, {"id": "term-hard-attention", "t": "Hard Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that selects discrete positions to attend to rather than computing weighted averages, requiring...", "l": "h", "k": ["hard", "attention", "mechanism", "selects", "discrete", "positions", "attend", "rather", "computing", "weighted", "averages", "requiring", "reinforcement", "learning", "straight-through"]}, {"id": "term-hard-sigmoid", "t": "Hard Sigmoid", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A piecewise linear approximation of the sigmoid function that is computationally cheaper to evaluate. Defined as f(x) =...", "l": "h", "k": ["hard", "sigmoid", "piecewise", "linear", "approximation", "function", "computationally", "cheaper", "evaluate", "defined", "clip", "mobile", "embedded", "applications", "computational"]}, {"id": "term-hard-swish", "t": "Hard Swish", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A piecewise linear approximation of the Swish activation function designed for efficient inference on mobile devices....", "l": "h", "k": ["hard", "swish", "piecewise", "linear", "approximation", "activation", "function", "designed", "efficient", "inference", "mobile", "devices", "mobilenetv3", "defined", "hard_sigmoid"]}, {"id": "term-hate-speech-detection", "t": "Hate Speech Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of automatically identifying text that expresses hatred toward a group based on attributes like race, gender,...", "l": "h", "k": ["hate", "speech", "detection", "task", "automatically", "identifying", "text", "expresses", "hatred", "toward", "group", "based", "attributes", "race", "gender"]}, {"id": "term-hazard-function", "t": "Hazard Function", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "The instantaneous rate at which events occur for subjects who have survived up to a given time point. It is a...", "l": "h", "k": ["hazard", "function", "instantaneous", "rate", "events", "occur", "subjects", "survived", "given", "time", "point", "fundamental", "quantity", "survival", "analysis"]}, {"id": "term-he-initialization", "t": "He Initialization", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A weight initialization strategy specifically designed for ReLU activation functions that scales the variance of...", "l": "h", "k": ["initialization", "weight", "strategy", "specifically", "designed", "relu", "activation", "functions", "scales", "variance", "initial", "weights", "divided", "number", "input"]}, {"id": "term-hearsay-ii", "t": "Hearsay-II", "tg": ["History", "Systems"], "d": "history", "x": "A speech understanding system developed at Carnegie Mellon University in the 1970s. Notable for its blackboard...", "l": "h", "k": ["hearsay-ii", "speech", "understanding", "system", "developed", "carnegie", "mellon", "university", "1970s", "notable", "blackboard", "architecture", "multiple", "knowledge", "sources"]}, {"id": "term-heatmap-prediction", "t": "Heatmap Prediction", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A technique in keypoint detection that predicts a probability heatmap for each keypoint type, with peaks indicating...", "l": "h", "k": ["heatmap", "prediction", "technique", "keypoint", "detection", "predicts", "probability", "type", "peaks", "indicating", "likely", "locations", "providing", "sub-pixel", "localization"]}, {"id": "term-hebbian-learning", "t": "Hebbian Learning", "tg": ["History", "Milestones"], "d": "history", "x": "A learning principle proposed by Donald Hebb in 1949 stating that neurons that fire together wire together, meaning...", "l": "h", "k": ["hebbian", "learning", "principle", "proposed", "donald", "hebb", "stating", "neurons", "fire", "together", "wire", "meaning", "synaptic", "connections", "strengthen"]}, {"id": "term-hedge-detection", "t": "Hedge Detection", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of identifying linguistic expressions that indicate uncertainty, speculation, or tentativeness in text, such...", "l": "h", "k": ["hedge", "detection", "task", "identifying", "linguistic", "expressions", "indicate", "uncertainty", "speculation", "tentativeness", "text", "possibly", "appears"]}, {"id": "term-hellaswag", "t": "HellaSwag", "tg": ["Benchmark", "Evaluation"], "d": "datasets", "x": "A benchmark testing commonsense natural language inference. Models must select the most plausible continuation of a...", "l": "h", "k": ["hellaswag", "benchmark", "testing", "commonsense", "natural", "language", "inference", "models", "must", "select", "plausible", "continuation", "scenario", "evaluating", "real-world"]}, {"id": "term-herbert-gelernter", "t": "Herbert Gelernter", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who created the Geometry Theorem Prover in 1959 at IBM. The program could prove theorems in...", "l": "h", "k": ["herbert", "gelernter", "american", "computer", "scientist", "created", "geometry", "theorem", "prover", "ibm", "program", "prove", "theorems", "euclidean", "axioms"]}, {"id": "term-herbert-simon", "t": "Herbert Simon", "tg": ["History", "Pioneers"], "d": "history", "x": "American political scientist, economist, and AI pioneer (1916-2001) who co-created the Logic Theorist and General...", "l": "h", "k": ["herbert", "simon", "american", "political", "scientist", "economist", "pioneer", "1916-2001", "co-created", "logic", "theorist", "general", "problem", "solver", "developed"]}, {"id": "term-heteroscedasticity", "t": "Heteroscedasticity", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A condition in regression analysis where the variance of the residuals is not constant across all levels of the...", "l": "h", "k": ["heteroscedasticity", "condition", "regression", "analysis", "variance", "residuals", "constant", "across", "levels", "independent", "variables", "violates", "key", "assumption", "ordinary"]}, {"id": "term-heuristic", "t": "Heuristic", "tg": ["Concept", "Problem Solving"], "d": "general", "x": "A practical rule or strategy that helps solve problems or make decisions quickly, even if not guaranteed to be optimal....", "l": "h", "k": ["heuristic", "practical", "rule", "strategy", "helps", "solve", "problems", "decisions", "quickly", "guaranteed", "optimal", "search", "optimization", "rule-based", "systems"]}, {"id": "term-heuristic-search", "t": "Heuristic Search", "tg": ["History", "Fundamentals"], "d": "history", "x": "A problem-solving approach that uses rules of thumb or estimates to guide search through a problem space more...", "l": "h", "k": ["heuristic", "search", "problem-solving", "approach", "uses", "rules", "thumb", "estimates", "guide", "problem", "space", "efficiently", "exhaustive", "herbert", "simon"]}, {"id": "term-hidden-layer", "t": "Hidden Layer", "tg": ["Architecture", "Neural Networks"], "d": "models", "x": "Layers in a neural network between the input and output layers. Deep networks have many hidden layers, enabling them to...", "l": "h", "k": ["hidden", "layer", "layers", "neural", "network", "input", "output", "deep", "networks", "enabling", "learn", "complex", "hierarchical", "representations"]}, {"id": "term-hidden-markov-model", "t": "Hidden Markov Model", "tg": ["Machine Learning", "Probability"], "d": "algorithms", "x": "A statistical model where the system is assumed to be a Markov process with unobserved (hidden) states, and...", "l": "h", "k": ["hidden", "markov", "model", "statistical", "system", "assumed", "process", "unobserved", "states", "observations", "probabilistically", "dependent", "state", "widely", "speech"]}, {"id": "term-hidden-markov-models", "t": "Hidden Markov Models", "tg": ["History", "Fundamentals"], "d": "history", "x": "Statistical models where the system being modeled is assumed to follow a Markov process with unobserved states. HMMs...", "l": "h", "k": ["hidden", "markov", "models", "statistical", "system", "modeled", "assumed", "follow", "process", "unobserved", "states", "hmms", "became", "fundamental", "speech"]}, {"id": "term-hierarchical-clustering", "t": "Hierarchical Clustering", "tg": ["Machine Learning", "Clustering"], "d": "general", "x": "A clustering approach that builds a tree-like hierarchy of clusters either by progressively merging smaller clusters...", "l": "h", "k": ["hierarchical", "clustering", "approach", "builds", "tree-like", "hierarchy", "clusters", "progressively", "merging", "smaller", "agglomerative", "recursively", "splitting", "larger", "divisive"]}, {"id": "term-hierarchical-rl", "t": "Hierarchical Reinforcement Learning", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "RL frameworks that decompose complex tasks into hierarchies of subtasks or skills operating at different temporal...", "l": "h", "k": ["hierarchical", "reinforcement", "learning", "frameworks", "decompose", "complex", "tasks", "hierarchies", "subtasks", "skills", "operating", "different", "temporal", "abstractions", "enables"]}, {"id": "term-hbm", "t": "High Bandwidth Memory (HBM)", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "A high-performance DRAM technology that stacks memory dies vertically and connects them with through-silicon vias,...", "l": "h", "k": ["high", "bandwidth", "memory", "hbm", "high-performance", "dram", "technology", "stacks", "dies", "vertically", "connects", "through-silicon", "vias", "providing", "significantly"]}, {"id": "term-high-risk-ai-systems", "t": "High-Risk AI Systems", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Under the EU AI Act, AI systems used in critical domains such as biometric identification, critical infrastructure,...", "l": "h", "k": ["high-risk", "systems", "act", "critical", "domains", "biometric", "identification", "infrastructure", "education", "employment", "law", "enforcement", "migration", "must", "meet"]}, {"id": "term-high-stakes-ai", "t": "High-Stakes AI", "tg": ["Ethics", "Risk"], "d": "safety", "x": "AI applications where errors can have severe consequences: healthcare diagnoses, legal decisions, financial...", "l": "h", "k": ["high-stakes", "applications", "errors", "severe", "consequences", "healthcare", "diagnoses", "legal", "decisions", "financial", "transactions", "requires", "extra", "scrutiny", "testing"]}, {"id": "term-hill-climbing", "t": "Hill Climbing", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A local search optimization algorithm that iteratively moves toward higher-valued neighbors until no improvement is...", "l": "h", "k": ["hill", "climbing", "local", "search", "optimization", "algorithm", "iteratively", "moves", "toward", "higher-valued", "neighbors", "improvement", "found", "simple", "fast"]}, {"id": "term-hindsight-experience-replay", "t": "Hindsight Experience Replay (HER)", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A technique that augments failed trajectories by relabeling goals with actually achieved states, turning failures into...", "l": "h", "k": ["hindsight", "experience", "replay", "technique", "augments", "failed", "trajectories", "relabeling", "goals", "actually", "achieved", "states", "turning", "failures", "successes"]}, {"id": "term-hinge-loss", "t": "Hinge Loss", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A loss function used in maximum-margin classifiers such as SVMs, defined as max(0, 1 - y * f(x)). It penalizes...", "l": "h", "k": ["hinge", "loss", "function", "maximum-margin", "classifiers", "svms", "defined", "max", "penalizes", "predictions", "wrong", "side", "margin", "boundary"]}, {"id": "term-historical-bias", "t": "Historical Bias", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Bias that is encoded in data reflecting past discriminatory practices or societal inequalities, which AI systems can...", "l": "h", "k": ["historical", "bias", "encoded", "data", "reflecting", "past", "discriminatory", "practices", "societal", "inequalities", "systems", "perpetuate", "accurately", "represents", "patterns"]}, {"id": "term-hit-rate", "t": "Hit Rate", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A retrieval metric that measures the fraction of queries for which at least one relevant document appears in the top K...", "l": "h", "k": ["hit", "rate", "retrieval", "metric", "measures", "fraction", "queries", "least", "relevant", "document", "appears", "top", "results", "providing", "simple"]}, {"id": "term-hnsw", "t": "HNSW", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "Hierarchical Navigable Small World, a graph-based approximate nearest neighbor algorithm that builds multi-layered...", "l": "h", "k": ["hnsw", "hierarchical", "navigable", "small", "world", "graph-based", "approximate", "nearest", "neighbor", "algorithm", "builds", "multi-layered", "proximity", "graphs", "logarithmic"]}, {"id": "term-hoeffdings-inequality", "t": "Hoeffding's Inequality", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A probability inequality that provides an upper bound on the probability that the sum of bounded independent random...", "l": "h", "k": ["hoeffding", "inequality", "probability", "provides", "upper", "bound", "sum", "bounded", "independent", "random", "variables", "deviates", "expected", "value", "derive"]}, {"id": "term-holdout-method", "t": "Holdout Method", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "The simplest model evaluation strategy that splits data into separate training and test sets, typically using 70-80%...", "l": "h", "k": ["holdout", "method", "simplest", "model", "evaluation", "strategy", "splits", "data", "separate", "training", "test", "sets", "typically", "70-80", "remainder"]}, {"id": "term-holt-winters-method", "t": "Holt-Winters Method", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A triple exponential smoothing method for time series forecasting that captures level, trend, and seasonal components....", "l": "h", "k": ["holt-winters", "method", "triple", "exponential", "smoothing", "time", "series", "forecasting", "captures", "level", "trend", "seasonal", "components", "supports", "additive"]}, {"id": "term-homography", "t": "Homography", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A projective transformation matrix that maps points between two image planes, used for image stitching, perspective...", "l": "h", "k": ["homography", "projective", "transformation", "matrix", "maps", "points", "image", "planes", "stitching", "perspective", "correction", "augmented", "reality", "scene", "planar"]}, {"id": "term-homomorphic-encryption", "t": "Homomorphic Encryption", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "A cryptographic technique that allows computations to be performed on encrypted data without decrypting it first,...", "l": "h", "k": ["homomorphic", "encryption", "cryptographic", "technique", "allows", "computations", "performed", "encrypted", "data", "without", "decrypting", "enabling", "models", "process", "sensitive"]}, {"id": "term-homonymy", "t": "Homonymy", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The property of two or more words having the same spelling or pronunciation but unrelated meanings, such as 'bat' the...", "l": "h", "k": ["homonymy", "property", "words", "having", "spelling", "pronunciation", "unrelated", "meanings", "bat", "animal", "sporting", "equipment"]}, {"id": "term-hopfield-network", "t": "Hopfield Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A recurrent neural network with symmetric connections that stores patterns as energy minima, functioning as an...", "l": "h", "k": ["hopfield", "network", "recurrent", "neural", "symmetric", "connections", "stores", "patterns", "energy", "minima", "functioning", "associative", "memory", "retrieves", "stored"]}, {"id": "term-huber-loss", "t": "Huber Loss", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A loss function that is quadratic for small residuals and linear for large residuals, combining the best properties of...", "l": "h", "k": ["huber", "loss", "function", "quadratic", "small", "residuals", "linear", "large", "combining", "best", "properties", "mean", "squared", "error", "absolute"]}, {"id": "term-hubert", "t": "HuBERT", "tg": ["Models", "Technical"], "d": "models", "x": "Hidden-Unit BERT is a self-supervised speech representation model that learns from masked prediction of discrete hidden...", "l": "h", "k": ["hubert", "hidden-unit", "bert", "self-supervised", "speech", "representation", "model", "learns", "masked", "prediction", "discrete", "hidden", "units", "derived", "clustering"]}, {"id": "term-hubert-dreyfus", "t": "Hubert Dreyfus", "tg": ["History", "Pioneers"], "d": "history", "x": "American philosopher who was one of the earliest critics of artificial intelligence. His 1972 book What Computers Can't...", "l": "h", "k": ["hubert", "dreyfus", "american", "philosopher", "earliest", "critics", "artificial", "intelligence", "book", "computers", "argued", "based", "symbolic", "processing", "replicate"]}, {"id": "term-hugging-face", "t": "Hugging Face", "tg": ["Platform", "Open Source"], "d": "general", "x": "A platform and community for sharing AI models, datasets, and applications. Hosts thousands of open-source models and...", "l": "h", "k": ["hugging", "face", "platform", "community", "sharing", "models", "datasets", "applications", "hosts", "thousands", "open-source", "popular", "transformers", "library", "nlp"]}, {"id": "term-human-evaluation", "t": "Human Evaluation", "tg": ["Evaluation", "Methodology"], "d": "datasets", "x": "The gold standard for assessing language model outputs where human annotators rate or compare generated text on...", "l": "h", "k": ["human", "evaluation", "gold", "standard", "assessing", "language", "model", "outputs", "annotators", "rate", "compare", "generated", "text", "dimensions", "quality"]}, {"id": "term-human-mesh-recovery", "t": "Human Mesh Recovery", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The task of estimating a full 3D human body mesh (shape and pose) from a single 2D image, using parametric body models...", "l": "h", "k": ["human", "mesh", "recovery", "task", "estimating", "full", "body", "shape", "pose", "single", "image", "parametric", "models", "smpl", "represent"]}, {"id": "term-hitl", "t": "Human-in-the-Loop (HITL)", "tg": ["Practice", "Safety"], "d": "safety", "x": "A design approach where humans review, approve, or modify AI outputs before they're acted upon. Critical for...", "l": "h", "k": ["human-in-the-loop", "hitl", "design", "approach", "humans", "review", "approve", "modify", "outputs", "acted", "upon", "critical", "high-stakes", "decisions", "quality"]}, {"id": "term-humaneval", "t": "HumanEval", "tg": ["Benchmark", "Evaluation"], "d": "datasets", "x": "A benchmark for evaluating code generation capabilities of LLMs. Contains programming problems with test cases,...", "l": "h", "k": ["humaneval", "benchmark", "evaluating", "code", "generation", "capabilities", "llms", "contains", "programming", "problems", "test", "cases", "measuring", "generated", "passes"]}, {"id": "term-hybrid-ai", "t": "Hybrid AI", "tg": ["Architecture", "Design"], "d": "models", "x": "Systems combining multiple AI approaches: neural networks with symbolic reasoning, LLMs with search, or ML with...", "l": "h", "k": ["hybrid", "systems", "combining", "multiple", "approaches", "neural", "networks", "symbolic", "reasoning", "llms", "search", "rule-based", "logic", "robust", "pure"]}, {"id": "term-hybrid-retrieval", "t": "Hybrid Retrieval", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An information retrieval approach that combines sparse retrieval methods like BM25 with dense retrieval using neural...", "l": "h", "k": ["hybrid", "retrieval", "information", "approach", "combines", "sparse", "methods", "bm25", "dense", "neural", "embeddings", "leverages", "strengths", "exact", "term"]}, {"id": "term-hybrid-search", "t": "Hybrid Search", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A retrieval approach that combines dense vector similarity search with traditional keyword-based (sparse) retrieval,...", "l": "h", "k": ["hybrid", "search", "retrieval", "approach", "combines", "dense", "vector", "similarity", "traditional", "keyword-based", "sparse", "leveraging", "strengths", "methods", "robust"]}, {"id": "term-hyena", "t": "Hyena", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A subquadratic attention replacement that uses long convolutions parameterized by implicit neural representations and...", "l": "h", "k": ["hyena", "subquadratic", "attention", "replacement", "uses", "long", "convolutions", "parameterized", "implicit", "neural", "representations", "multiplicative", "gating", "achieve", "competitive"]}, {"id": "term-hyperband", "t": "Hyperband", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A hyperparameter optimization method that combines random search with early stopping, allocating more resources to...", "l": "h", "k": ["hyperband", "hyperparameter", "optimization", "method", "combines", "random", "search", "early", "stopping", "allocating", "resources", "promising", "configurations", "successively", "halving"]}, {"id": "term-hypernymy", "t": "Hypernymy", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A semantic relation where one word's meaning includes and is more general than another's, such as 'animal' being a...", "l": "h", "k": ["hypernymy", "semantic", "relation", "word", "meaning", "includes", "general", "another", "animal", "hypernym", "dog", "forming", "taxonomic", "hierarchies", "lexical"]}, {"id": "term-hyperparameter", "t": "Hyperparameter", "tg": ["Training", "Configuration"], "d": "general", "x": "Configuration settings that control the training process rather than being learned from data. Examples include learning...", "l": "h", "k": ["hyperparameter", "configuration", "settings", "control", "training", "process", "rather", "learned", "data", "examples", "include", "learning", "rate", "batch", "size"]}, {"id": "term-hyperparameter-optimization", "t": "Hyperparameter Optimization", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "The process of finding optimal hyperparameter configurations for machine learning models. Methods include grid search...", "l": "h", "k": ["hyperparameter", "optimization", "process", "finding", "optimal", "configurations", "machine", "learning", "models", "methods", "include", "grid", "search", "random", "bayesian"]}, {"id": "term-hyponymy", "t": "Hyponymy", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A semantic relation where one word's meaning is more specific than and included within another's, such as 'dog' being a...", "l": "h", "k": ["hyponymy", "semantic", "relation", "word", "meaning", "specific", "included", "within", "another", "dog", "hyponym", "animal", "representing", "is-a", "relationship"]}, {"id": "term-hypothesis", "t": "Hypothesis (ML)", "tg": ["Concept", "Theory"], "d": "general", "x": "A specific function or model that the learning algorithm considers as a possible solution. The hypothesis space is all...", "l": "h", "k": ["hypothesis", "specific", "function", "model", "learning", "algorithm", "considers", "possible", "solution", "space", "hypotheses", "choose"]}, {"id": "term-hypothetical-document-embedding", "t": "Hypothetical Document Embedding", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A retrieval technique (HyDE) where a language model generates a hypothetical answer to a query, and the embedding of...", "l": "h", "k": ["hypothetical", "document", "embedding", "retrieval", "technique", "hyde", "language", "model", "generates", "answer", "query", "search", "real", "documents", "improving"]}, {"id": "term-ian-goodfellow", "t": "Ian Goodfellow", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who invented generative adversarial networks (GANs) in 2014, introducing a framework where...", "l": "i", "k": ["ian", "goodfellow", "american", "computer", "scientist", "invented", "generative", "adversarial", "networks", "gans", "introducing", "framework", "neural", "compete", "generate"]}, {"id": "term-ibm-research-ai", "t": "IBM Research AI", "tg": ["History", "Organizations"], "d": "history", "x": "AI research at IBM spanning decades from Arthur Samuel's checkers program (1959) through Deep Blue (1997) Watson (2011)...", "l": "i", "k": ["ibm", "research", "spanning", "decades", "arthur", "samuel", "checkers", "program", "deep", "blue", "watson", "modern", "enterprise", "longest-running", "corporate"]}, {"id": "term-ibm-watson-jeopardy", "t": "IBM Watson Jeopardy", "tg": ["History", "Milestones"], "d": "history", "x": "IBM's Watson AI system that defeated human champions Ken Jennings and Brad Rutter on the quiz show Jeopardy! in...", "l": "i", "k": ["ibm", "watson", "jeopardy", "system", "defeated", "human", "champions", "ken", "jennings", "brad", "rutter", "quiz", "show", "february", "demonstrating"]}, {"id": "term-iccv", "t": "ICCV", "tg": ["History", "Conferences"], "d": "history", "x": "The International Conference on Computer Vision held biennially since 1987. One of the top three computer vision...", "l": "i", "k": ["iccv", "international", "conference", "computer", "vision", "held", "biennially", "top", "conferences", "alongside", "cvpr", "eccv", "seminal", "papers", "visual"]}, {"id": "term-iclr", "t": "ICLR", "tg": ["History", "Conferences"], "d": "history", "x": "The International Conference on Learning Representations first held in 2013 founded by Yoshua Bengio and Yann LeCun....", "l": "i", "k": ["iclr", "international", "conference", "learning", "representations", "held", "founded", "yoshua", "bengio", "yann", "lecun", "pioneered", "open", "peer", "review"]}, {"id": "term-icml", "t": "ICML", "tg": ["History", "Conferences"], "d": "history", "x": "The International Conference on Machine Learning first held in 1980. One of the top-tier conferences in AI and machine...", "l": "i", "k": ["icml", "international", "conference", "machine", "learning", "held", "top-tier", "conferences", "alongside", "neurips", "iclr", "organized", "society"]}, {"id": "term-idefics", "t": "Idefics", "tg": ["Models", "Technical"], "d": "models", "x": "An open-access multimodal model based on Flamingo's architecture that processes interleaved image and text inputs....", "l": "i", "k": ["idefics", "open-access", "multimodal", "model", "based", "flamingo", "architecture", "processes", "interleaved", "image", "text", "inputs", "trained", "publicly", "available"]}, {"id": "term-idiom-detection", "t": "Idiom Detection", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of identifying non-compositional multi-word expressions whose meaning cannot be deduced from their individual...", "l": "i", "k": ["idiom", "detection", "task", "identifying", "non-compositional", "multi-word", "expressions", "whose", "meaning", "cannot", "deduced", "individual", "words", "kick", "bucket"]}, {"id": "term-ieee-organization", "t": "IEEE (Organization)", "tg": ["History", "Organizations"], "d": "history", "x": "The Institute of Electrical and Electronics Engineers founded in 1963. The world's largest professional technical...", "l": "i", "k": ["ieee", "organization", "institute", "electrical", "electronics", "engineers", "founded", "world", "largest", "professional", "technical", "electronic", "engineering", "publishes", "influential"]}, {"id": "term-ieee-ai-ethics-standards", "t": "IEEE AI Ethics Standards", "tg": ["Governance", "Regulation"], "d": "safety", "x": "A family of standards developed by IEEE under the Ethically Aligned Design initiative, including IEEE 7000 series...", "l": "i", "k": ["ieee", "ethics", "standards", "family", "developed", "ethically", "aligned", "design", "initiative", "including", "series", "addressing", "transparency", "data", "privacy"]}, {"id": "term-ifeval", "t": "IFEval", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Instruction Following Evaluation, a benchmark that tests language models' ability to follow specific verifiable...", "l": "i", "k": ["ifeval", "instruction", "following", "evaluation", "benchmark", "tests", "language", "models", "ability", "follow", "specific", "verifiable", "formatting", "instructions", "word"]}, {"id": "term-ijcai", "t": "IJCAI", "tg": ["History", "Conferences"], "d": "history", "x": "The International Joint Conference on Artificial Intelligence first held in 1969. The oldest and one of the most...", "l": "i", "k": ["ijcai", "international", "joint", "conference", "artificial", "intelligence", "held", "oldest", "prestigious", "conferences", "covering", "topics", "biennially", "annually"]}, {"id": "term-ilya-sutskever", "t": "Ilya Sutskever", "tg": ["History", "Pioneers"], "d": "history", "x": "Russian-born AI researcher who co-founded OpenAI, served as its Chief Scientist, and co-designed AlexNet. He co-founded...", "l": "i", "k": ["ilya", "sutskever", "russian-born", "researcher", "co-founded", "openai", "served", "chief", "scientist", "co-designed", "alexnet", "safe", "superintelligence", "inc", "focusing"]}, {"id": "term-image-augmentation", "t": "Image Augmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Techniques that artificially expand training datasets by applying random transformations to images, including rotation,...", "l": "i", "k": ["image", "augmentation", "techniques", "artificially", "expand", "training", "datasets", "applying", "random", "transformations", "images", "including", "rotation", "flipping", "cropping"]}, {"id": "term-image-classification", "t": "Image Classification", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The fundamental computer vision task of assigning a categorical label to an entire image based on its visual content,...", "l": "i", "k": ["image", "classification", "fundamental", "computer", "vision", "task", "assigning", "categorical", "label", "entire", "based", "visual", "content", "typically", "cnn"]}, {"id": "term-image-colorization", "t": "Image Colorization", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of automatically adding plausible colors to grayscale images using deep learning models that learn color...", "l": "i", "k": ["image", "colorization", "task", "automatically", "adding", "plausible", "colors", "grayscale", "images", "deep", "learning", "models", "learn", "color", "distributions"]}, {"id": "term-image-deblurring", "t": "Image Deblurring", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of recovering sharp images from blurred inputs caused by camera shake or object motion, using deep learning...", "l": "i", "k": ["image", "deblurring", "task", "recovering", "sharp", "images", "blurred", "inputs", "caused", "camera", "shake", "object", "motion", "deep", "learning"]}, {"id": "term-image-denoising", "t": "Image Denoising", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of removing noise from degraded images using neural networks that learn to separate signal from noise,...", "l": "i", "k": ["image", "denoising", "process", "removing", "noise", "degraded", "images", "neural", "networks", "learn", "separate", "signal", "producing", "cleaner", "preserving"]}, {"id": "term-image-embedding", "t": "Image Embedding", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A dense vector representation of an image produced by a neural network encoder, capturing semantic and visual features...", "l": "i", "k": ["image", "embedding", "dense", "vector", "representation", "produced", "neural", "network", "encoder", "capturing", "semantic", "visual", "features", "compact", "form"]}, {"id": "term-image-generation", "t": "Image Generation", "tg": ["Application", "Generative"], "d": "general", "x": "AI systems that create images from text descriptions or other inputs. Major models include DALL-E, Midjourney, and...", "l": "i", "k": ["image", "generation", "systems", "create", "images", "text", "descriptions", "inputs", "major", "models", "include", "dall-e", "midjourney", "stable", "diffusion"]}, {"id": "term-image-inpainting", "t": "Image Inpainting", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of filling in missing or masked regions of an image with plausible content, using deep learning models that...", "l": "i", "k": ["image", "inpainting", "task", "filling", "missing", "masked", "regions", "plausible", "content", "deep", "learning", "models", "understand", "context", "texture"]}, {"id": "term-image-matting", "t": "Image Matting", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of estimating a precise alpha matte that defines the fractional opacity of foreground elements in an image,...", "l": "i", "k": ["image", "matting", "task", "estimating", "precise", "alpha", "matte", "defines", "fractional", "opacity", "foreground", "elements", "enabling", "accurate", "extraction"]}, {"id": "term-image-registration", "t": "Image Registration", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of aligning two or more images of the same scene taken at different times, viewpoints, or by different...", "l": "i", "k": ["image", "registration", "process", "aligning", "images", "scene", "taken", "different", "times", "viewpoints", "sensors", "computing", "spatial", "transformation", "maps"]}, {"id": "term-image-retrieval", "t": "Image Retrieval", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of finding images in a database that are visually similar to a query image, using learned embeddings and...", "l": "i", "k": ["image", "retrieval", "task", "finding", "images", "database", "visually", "similar", "query", "learned", "embeddings", "nearest-neighbor", "search", "embedding", "space"]}, {"id": "term-image-stitching", "t": "Image Stitching", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of combining multiple overlapping photographs into a single panoramic or wide-field image by estimating...", "l": "i", "k": ["image", "stitching", "process", "combining", "multiple", "overlapping", "photographs", "single", "panoramic", "wide-field", "estimating", "homographies", "blending", "seams", "correcting"]}, {"id": "term-image-upscaling", "t": "Image Upscaling", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of increasing image resolution using AI models that synthesize realistic high-frequency details, producing...", "l": "i", "k": ["image", "upscaling", "process", "increasing", "resolution", "models", "synthesize", "realistic", "high-frequency", "details", "producing", "sharp", "detailed", "results", "superior"]}, {"id": "term-imagebind", "t": "ImageBind", "tg": ["Models", "Technical"], "d": "models", "x": "A model by Meta AI that learns a joint embedding space across six modalities including images text audio depth thermal...", "l": "i", "k": ["imagebind", "model", "meta", "learns", "joint", "embedding", "space", "across", "six", "modalities", "including", "images", "text", "audio", "depth"]}, {"id": "term-imagen", "t": "Imagen", "tg": ["Models", "Technical"], "d": "models", "x": "A text-to-image diffusion model by Google Brain that uses a large frozen text encoder and cascaded diffusion models....", "l": "i", "k": ["imagen", "text-to-image", "diffusion", "model", "google", "brain", "uses", "large", "frozen", "text", "encoder", "cascaded", "models", "demonstrated", "scaling"]}, {"id": "term-imagenet", "t": "ImageNet", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A large-scale visual database with over 14 million labeled images across thousands of categories, historically serving...", "l": "i", "k": ["imagenet", "large-scale", "visual", "database", "million", "labeled", "images", "across", "thousands", "categories", "historically", "serving", "primary", "benchmark", "dataset"]}, {"id": "term-imagenet-dataset", "t": "ImageNet Dataset", "tg": ["History", "Milestones"], "d": "history", "x": "A large-scale visual recognition dataset created by Fei-Fei Li and colleagues containing over 14 million labeled images...", "l": "i", "k": ["imagenet", "dataset", "large-scale", "visual", "recognition", "created", "fei-fei", "colleagues", "containing", "million", "labeled", "images", "across", "categories", "large"]}, {"id": "term-imagenet-moment", "t": "ImageNet Moment", "tg": ["History", "Milestones"], "d": "history", "x": "The pivotal moment in 2012 when Alex Krizhevsky's deep convolutional neural network (AlexNet) won the ImageNet Large...", "l": "i", "k": ["imagenet", "moment", "pivotal", "alex", "krizhevsky", "deep", "convolutional", "neural", "network", "alexnet", "won", "large", "scale", "visual", "recognition"]}, {"id": "term-img2img", "t": "Img2Img", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "An image-to-image generation pipeline that takes an existing image as input, adds noise to its latent representation,...", "l": "i", "k": ["img2img", "image-to-image", "generation", "pipeline", "takes", "existing", "image", "input", "adds", "noise", "latent", "representation", "denoises", "text", "prompt"]}, {"id": "term-imitation-learning", "t": "Imitation Learning", "tg": ["Reinforcement Learning", "Imitation"], "d": "general", "x": "A paradigm where an agent learns to perform tasks by observing expert demonstrations rather than through reward-based...", "l": "i", "k": ["imitation", "learning", "paradigm", "agent", "learns", "perform", "tasks", "observing", "expert", "demonstrations", "rather", "reward-based", "trial", "error", "includes"]}, {"id": "term-implicit-differentiation", "t": "Implicit Differentiation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A technique for computing gradients through fixed-point iterations or optimization problems without unrolling the...", "l": "i", "k": ["implicit", "differentiation", "technique", "computing", "gradients", "fixed-point", "iterations", "optimization", "problems", "without", "unrolling", "computation", "graph", "enables", "memory-efficient"]}, {"id": "term-implicit-neural-representation", "t": "Implicit Neural Representation", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A neural network that learns a continuous function mapping coordinates to signal values, representing signals like...", "l": "i", "k": ["implicit", "neural", "representation", "network", "learns", "continuous", "function", "mapping", "coordinates", "signal", "values", "representing", "signals", "images", "shapes"]}, {"id": "term-implicit-q-learning", "t": "Implicit Q-Learning (IQL)", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An offline RL method that avoids querying out-of-distribution actions by learning a value function using expectile...", "l": "i", "k": ["implicit", "q-learning", "iql", "offline", "method", "avoids", "querying", "out-of-distribution", "actions", "learning", "value", "function", "expectile", "regression", "extracting"]}, {"id": "term-importance-sampling", "t": "Importance Sampling", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A Monte Carlo technique that estimates properties of one distribution by sampling from a different, easier-to-sample...", "l": "i", "k": ["importance", "sampling", "monte", "carlo", "technique", "estimates", "properties", "distribution", "different", "easier-to-sample", "proposal", "reweighting", "samples", "likelihood", "ratio"]}, {"id": "term-importance-sampling-rl", "t": "Importance Sampling in RL", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A statistical technique used in off-policy RL to correct for the mismatch between the behavior policy that generated...", "l": "i", "k": ["importance", "sampling", "statistical", "technique", "off-policy", "correct", "mismatch", "behavior", "policy", "generated", "data", "target", "evaluated", "ratios", "reweight"]}, {"id": "term-impossibility-theorem-of-fairness", "t": "Impossibility Theorem of Fairness", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Mathematical results demonstrating that certain fairness criteria are mutually incompatible except in trivial cases,...", "l": "i", "k": ["impossibility", "theorem", "fairness", "mathematical", "results", "demonstrating", "certain", "criteria", "mutually", "incompatible", "except", "trivial", "cases", "meaning", "satisfying"]}, {"id": "term-imputation", "t": "Imputation", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "The process of replacing missing values in a dataset with estimated values, using methods such as mean, median, mode...", "l": "i", "k": ["imputation", "process", "replacing", "missing", "values", "dataset", "estimated", "methods", "mean", "median", "mode", "substitution", "k-nearest", "neighbors", "model-based"]}, {"id": "term-in-context-learning", "t": "In-Context Learning", "tg": ["Capability", "Prompting"], "d": "general", "x": "An LLM's ability to learn from examples provided in the prompt without updating its weights. Enables few-shot and...", "l": "i", "k": ["in-context", "learning", "llm", "ability", "learn", "examples", "provided", "prompt", "without", "updating", "weights", "enables", "few-shot", "zero-shot", "task"]}, {"id": "term-in-context-learning-discovery", "t": "In-Context Learning Discovery", "tg": ["History", "Milestones"], "d": "history", "x": "The finding that large language models can learn to perform new tasks by conditioning on a few examples provided in the...", "l": "i", "k": ["in-context", "learning", "discovery", "finding", "large", "language", "models", "learn", "perform", "tasks", "conditioning", "examples", "provided", "input", "context"]}, {"id": "term-inception-network", "t": "Inception Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A CNN architecture that uses parallel convolutional filters of different sizes within the same layer (inception...", "l": "i", "k": ["inception", "network", "cnn", "architecture", "uses", "parallel", "convolutional", "filters", "different", "sizes", "within", "layer", "modules", "capture", "features"]}, {"id": "term-inception-score", "t": "Inception Score", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A metric for evaluating the quality and diversity of generated images. Measures how confidently an Inception network...", "l": "i", "k": ["inception", "score", "metric", "evaluating", "quality", "diversity", "generated", "images", "measures", "confidently", "network", "classifies", "diverse", "predicted", "labels"]}, {"id": "term-inception-v3", "t": "Inception v3", "tg": ["Models", "Technical"], "d": "models", "x": "A refined version of the GoogLeNet architecture with factorized convolutions batch normalization and label smoothing....", "l": "i", "k": ["inception", "refined", "version", "googlenet", "architecture", "factorized", "convolutions", "batch", "normalization", "label", "smoothing", "widely", "feature", "extractor", "serves"]}, {"id": "term-inception-v4", "t": "Inception v4", "tg": ["Models", "Technical"], "d": "models", "x": "An advanced version of the Inception architecture that combines Inception modules with residual connections. Achieves...", "l": "i", "k": ["inception", "advanced", "version", "architecture", "combines", "modules", "residual", "connections", "achieves", "improved", "accuracy", "deeper", "networks", "maintaining", "computational"]}, {"id": "term-incoder", "t": "InCoder", "tg": ["Models", "Technical"], "d": "models", "x": "A unified generative model for code that supports both code generation and infilling through a causal masking training...", "l": "i", "k": ["incoder", "unified", "generative", "model", "code", "supports", "generation", "infilling", "causal", "masking", "training", "objective", "generate", "left-to-right", "fill"]}, {"id": "term-incremental-indexing", "t": "Incremental Indexing", "tg": ["Vector Database", "Maintenance"], "d": "general", "x": "A vector database operation that adds new vectors to an existing index without requiring a full rebuild, enabling...", "l": "i", "k": ["incremental", "indexing", "vector", "database", "operation", "adds", "vectors", "existing", "index", "without", "requiring", "full", "rebuild", "enabling", "near-real-time"]}, {"id": "term-independent-component-analysis", "t": "Independent Component Analysis", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "A computational method for separating a multivariate signal into additive, statistically independent non-Gaussian...", "l": "i", "k": ["independent", "component", "analysis", "computational", "method", "separating", "multivariate", "signal", "additive", "statistically", "non-gaussian", "source", "components", "widely", "blind"]}, {"id": "term-iql-marl", "t": "Independent Q-Learning (IQL-MARL)", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A simple multi-agent RL approach where each agent independently learns its own Q-function treating other agents as part...", "l": "i", "k": ["independent", "q-learning", "iql-marl", "simple", "multi-agent", "approach", "agent", "independently", "learns", "q-function", "treating", "agents", "part", "environment", "despite"]}, {"id": "term-index-optimization", "t": "Index Optimization", "tg": ["Vector Database", "Performance"], "d": "general", "x": "The process of tuning vector index parameters such as the number of clusters, graph connectivity, and quantization...", "l": "i", "k": ["index", "optimization", "process", "tuning", "vector", "parameters", "number", "clusters", "graph", "connectivity", "quantization", "settings", "achieve", "optimal", "balance"]}, {"id": "term-index-refresh", "t": "Index Refresh", "tg": ["Vector Database", "Maintenance"], "d": "general", "x": "The process of rebuilding or updating a vector index to incorporate new vectors, remove deleted ones, and optimize...", "l": "i", "k": ["index", "refresh", "process", "rebuilding", "updating", "vector", "incorporate", "vectors", "remove", "deleted", "ones", "optimize", "search", "structures", "necessary"]}, {"id": "term-individual-conditional-expectation", "t": "Individual Conditional Expectation", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A plot showing how the prediction for each individual instance changes as a feature varies, disaggregating the partial...", "l": "i", "k": ["individual", "conditional", "expectation", "plot", "showing", "prediction", "instance", "changes", "feature", "varies", "disaggregating", "partial", "dependence", "reveal", "heterogeneous"]}, {"id": "term-individual-fairness", "t": "Individual Fairness", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness principle requiring that similar individuals receive similar predictions or outcomes, formalized as a...", "l": "i", "k": ["individual", "fairness", "principle", "requiring", "similar", "individuals", "receive", "predictions", "outcomes", "formalized", "lipschitz", "condition", "close", "task-relevant", "metric"]}, {"id": "term-induction-head", "t": "Induction Head", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A pair of attention heads in transformers that work together to identify and continue repeated patterns in the input...", "l": "i", "k": ["induction", "head", "pair", "attention", "heads", "transformers", "work", "together", "identify", "continue", "repeated", "patterns", "input", "sequence", "forming"]}, {"id": "term-inference", "t": "Inference", "tg": ["Process", "Production"], "d": "general", "x": "Running a trained model to generate predictions or outputs on new data. Distinguished from training, inference is...", "l": "i", "k": ["inference", "running", "trained", "model", "generate", "predictions", "outputs", "data", "distinguished", "training", "typically", "faster", "less", "resource-intensive"]}, {"id": "term-inference-acceleration", "t": "Inference Acceleration", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The collection of hardware and software techniques that speed up neural network inference, including specialized...", "l": "i", "k": ["inference", "acceleration", "collection", "hardware", "software", "techniques", "speed", "neural", "network", "including", "specialized", "accelerators", "compiler", "optimizations", "quantization"]}, {"id": "term-inference-cost-optimization", "t": "Inference Cost Optimization", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Strategies for minimizing the financial cost of serving AI models at scale, including quantization, distillation,...", "l": "i", "k": ["inference", "cost", "optimization", "strategies", "minimizing", "financial", "serving", "models", "scale", "including", "quantization", "distillation", "batching", "hardware", "selection"]}, {"id": "term-inference-optimization", "t": "Inference Optimization", "tg": ["LLM", "Inference"], "d": "models", "x": "A collection of techniques that reduce the computational cost, memory footprint, and latency of running trained models...", "l": "i", "k": ["inference", "optimization", "collection", "techniques", "reduce", "computational", "cost", "memory", "footprint", "latency", "running", "trained", "models", "production", "including"]}, {"id": "term-infiniband", "t": "InfiniBand", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "A high-speed networking technology widely used in AI supercomputer clusters for inter-node communication, offering low...", "l": "i", "k": ["infiniband", "high-speed", "networking", "technology", "widely", "supercomputer", "clusters", "inter-node", "communication", "offering", "low", "latency", "high", "bandwidth", "per"]}, {"id": "term-infonce-loss", "t": "InfoNCE Loss", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Information Noise-Contrastive Estimation is a contrastive loss function that trains models to distinguish positive...", "l": "i", "k": ["infonce", "loss", "information", "noise-contrastive", "estimation", "contrastive", "function", "trains", "models", "distinguish", "positive", "pairs", "negative", "forms", "basis"]}, {"id": "term-information-bottleneck", "t": "Information Bottleneck", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A theoretical framework that characterizes the optimal tradeoff between compression and prediction. A representation...", "l": "i", "k": ["information", "bottleneck", "theoretical", "framework", "characterizes", "optimal", "tradeoff", "compression", "prediction", "representation", "retain", "little", "input", "possible", "preserving"]}, {"id": "term-information-extraction", "t": "Information Extraction", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of automatically extracting structured information such as entities, relations, and events from unstructured...", "l": "i", "k": ["information", "extraction", "task", "automatically", "extracting", "structured", "entities", "relations", "events", "unstructured", "text", "converting", "free-form", "organized", "knowledge"]}, {"id": "term-information-gain", "t": "Information Gain", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "The reduction in entropy achieved by splitting a dataset on a particular feature. Decision tree algorithms like ID3 and...", "l": "i", "k": ["information", "gain", "reduction", "entropy", "achieved", "splitting", "dataset", "particular", "feature", "decision", "tree", "algorithms", "id3", "select", "provides"]}, {"id": "term-information-theory", "t": "Information Theory", "tg": ["History", "Fundamentals"], "d": "history", "x": "A mathematical framework developed by Claude Shannon in 1948 for quantifying information. Key concepts including...", "l": "i", "k": ["information", "theory", "mathematical", "framework", "developed", "claude", "shannon", "quantifying", "key", "concepts", "including", "entropy", "mutual", "channel", "capacity"]}, {"id": "term-informative-prior", "t": "Informative Prior", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A prior distribution that encodes specific prior knowledge or strong beliefs about a parameter's likely values,...", "l": "i", "k": ["informative", "prior", "distribution", "encodes", "specific", "knowledge", "strong", "beliefs", "parameter", "likely", "values", "substantially", "influencing", "posterior", "especially"]}, {"id": "term-informed-consent-in-ai", "t": "Informed Consent in AI", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "The ethical and legal requirement that individuals be clearly informed about how their data will be collected,...", "l": "i", "k": ["informed", "consent", "ethical", "legal", "requirement", "individuals", "clearly", "data", "collected", "processed", "systems", "voluntarily", "agree"]}, {"id": "term-inner-alignment", "t": "Inner Alignment", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The problem of ensuring that a learned model's internal optimization objective matches the objective specified by the...", "l": "i", "k": ["inner", "alignment", "problem", "ensuring", "learned", "model", "internal", "optimization", "objective", "matches", "specified", "training", "process", "failure", "results"]}, {"id": "term-inpainting-pipeline", "t": "Inpainting Pipeline", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A diffusion model workflow that regenerates only the masked portions of an image while maintaining consistency with the...", "l": "i", "k": ["inpainting", "pipeline", "diffusion", "model", "workflow", "regenerates", "masked", "portions", "image", "maintaining", "consistency", "unmasked", "regions", "object", "removal"]}, {"id": "term-instance-normalization", "t": "Instance Normalization", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A normalization technique that normalizes each feature channel independently for each sample, originally developed for...", "l": "i", "k": ["instance", "normalization", "technique", "normalizes", "feature", "channel", "independently", "sample", "originally", "developed", "neural", "style", "transfer", "per-instance", "statistics"]}, {"id": "term-instance-segmentation", "t": "Instance Segmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A computer vision task that detects individual objects in an image and generates a pixel-level mask for each instance,...", "l": "i", "k": ["instance", "segmentation", "computer", "vision", "task", "detects", "individual", "objects", "image", "generates", "pixel-level", "mask", "combining", "object", "detection"]}, {"id": "term-instant-ngp", "t": "Instant NGP", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "Instant Neural Graphics Primitives, a technique using multi-resolution hash encoding that dramatically accelerates NeRF...", "l": "i", "k": ["instant", "ngp", "neural", "graphics", "primitives", "technique", "multi-resolution", "hash", "encoding", "dramatically", "accelerates", "nerf", "training", "hours", "seconds"]}, {"id": "term-instruct-model", "t": "Instruct Model", "tg": ["Model Type", "Training"], "d": "models", "x": "An LLM fine-tuned to follow instructions rather than just complete text. Makes models more useful as assistants by...", "l": "i", "k": ["instruct", "model", "llm", "fine-tuned", "follow", "instructions", "rather", "complete", "text", "makes", "models", "useful", "assistants", "teaching", "respond"]}, {"id": "term-instructgpt", "t": "InstructGPT", "tg": ["History", "Milestones"], "d": "history", "x": "An OpenAI model published in 2022 that used reinforcement learning from human feedback to align GPT-3 with user...", "l": "i", "k": ["instructgpt", "openai", "model", "published", "reinforcement", "learning", "human", "feedback", "align", "gpt-3", "user", "instructions", "directly", "preceding", "informing"]}, {"id": "term-instruction-following", "t": "Instruction Following", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The ability of a language model to accurately interpret and execute natural language instructions, a capability...", "l": "i", "k": ["instruction", "following", "ability", "language", "model", "accurately", "interpret", "execute", "natural", "instructions", "capability", "developed", "tuning", "reinforcement", "learning"]}, {"id": "term-instruction-hierarchy", "t": "Instruction Hierarchy", "tg": ["Prompt Engineering", "Safety"], "d": "safety", "x": "A structured approach to organizing prompt instructions by priority level, where system-level instructions take...", "l": "i", "k": ["instruction", "hierarchy", "structured", "approach", "organizing", "prompt", "instructions", "priority", "level", "system-level", "take", "precedence", "user-level", "enabling", "models"]}, {"id": "term-instruction-tuning", "t": "Instruction Tuning", "tg": ["Training", "Alignment"], "d": "safety", "x": "Fine-tuning LLMs on datasets of instructions and responses to improve their ability to follow user requests. A key...", "l": "i", "k": ["instruction", "tuning", "fine-tuning", "llms", "datasets", "instructions", "responses", "improve", "ability", "follow", "user", "requests", "key", "technique", "creating"]}, {"id": "term-instruction-tuning-alignment", "t": "Instruction Tuning Alignment", "tg": ["Prompt Engineering", "Optimization"], "d": "algorithms", "x": "The practice of writing prompts that align with the specific instruction format and conventions used during a model's...", "l": "i", "k": ["instruction", "tuning", "alignment", "practice", "writing", "prompts", "align", "specific", "format", "conventions", "model", "fine-tuning", "phase", "maximizing", "benefit"]}, {"id": "term-instruction-based-prompting", "t": "Instruction-Based Prompting", "tg": ["Prompt Engineering", "Fundamentals"], "d": "general", "x": "A prompting paradigm that provides explicit, imperative instructions to a language model describing the task to...", "l": "i", "k": ["instruction-based", "prompting", "paradigm", "provides", "explicit", "imperative", "instructions", "language", "model", "describing", "task", "perform", "leveraging", "instruction-tuned", "models"]}, {"id": "term-instrumental-convergence", "t": "Instrumental Convergence", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The thesis that sufficiently advanced AI agents with a wide range of terminal goals will converge on pursuing certain...", "l": "i", "k": ["instrumental", "convergence", "thesis", "sufficiently", "advanced", "agents", "wide", "range", "terminal", "goals", "converge", "pursuing", "certain", "sub-goals", "self-preservation"]}, {"id": "term-instrumental-variable", "t": "Instrumental Variable", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A variable used in causal inference that is correlated with the treatment variable but affects the outcome only through...", "l": "i", "k": ["instrumental", "variable", "causal", "inference", "correlated", "treatment", "affects", "outcome", "enabling", "consistent", "estimation", "effects", "presence", "confounding"]}, {"id": "term-int4", "t": "INT4 Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "An aggressive quantization scheme using only 4 bits per weight, achieving 8x compression over FP32. INT4 methods like...", "l": "i", "k": ["int4", "quantization", "aggressive", "scheme", "bits", "per", "weight", "achieving", "compression", "fp32", "methods", "gptq", "awq", "sophisticated", "calibration"]}, {"id": "term-int8", "t": "INT8 Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "The process of representing neural network weights and activations using 8-bit integers instead of floating-point,...", "l": "i", "k": ["int8", "quantization", "process", "representing", "neural", "network", "weights", "activations", "8-bit", "integers", "instead", "floating-point", "reducing", "memory", "compared"]}, {"id": "term-integrated-gradients", "t": "Integrated Gradients", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An attribution method that computes feature importance by integrating gradients along a straight path from a baseline...", "l": "i", "k": ["integrated", "gradients", "attribution", "method", "computes", "feature", "importance", "integrating", "along", "straight", "path", "baseline", "actual", "input", "satisfies"]}, {"id": "term-intel-gaudi-3", "t": "Intel Gaudi 3", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "Intel's third-generation AI training accelerator featuring integrated 400GbE networking and high memory bandwidth....", "l": "i", "k": ["intel", "gaudi", "third-generation", "training", "accelerator", "featuring", "integrated", "400gbe", "networking", "high", "memory", "bandwidth", "targets", "competitive", "market"]}, {"id": "term-intent-detection", "t": "Intent Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of classifying the purpose or goal behind a user's utterance in a dialogue system, determining whether the...", "l": "i", "k": ["intent", "detection", "task", "classifying", "purpose", "goal", "behind", "user", "utterance", "dialogue", "system", "determining", "wants", "book", "search"]}, {"id": "term-intent-recognition", "t": "Intent Recognition", "tg": ["NLP Task", "Application"], "d": "general", "x": "Understanding what a user wants to accomplish from their input. A core NLP task for chatbots and virtual assistants,...", "l": "i", "k": ["intent", "recognition", "understanding", "user", "wants", "accomplish", "input", "core", "nlp", "task", "chatbots", "virtual", "assistants", "mapping", "messages"]}, {"id": "term-inter-annotator-agreement", "t": "Inter-Annotator Agreement", "tg": ["Evaluation", "Methodology"], "d": "datasets", "x": "A statistical measure of the degree to which independent human annotators make the same judgments when labeling or...", "l": "i", "k": ["inter-annotator", "agreement", "statistical", "measure", "degree", "independent", "human", "annotators", "judgments", "labeling", "evaluating", "data", "assess", "annotation", "reliability"]}, {"id": "term-inter-token-latency", "t": "Inter-Token Latency", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The time between consecutive token generations during the decode phase of LLM inference. Low inter-token latency is...", "l": "i", "k": ["inter-token", "latency", "time", "consecutive", "token", "generations", "decode", "phase", "llm", "inference", "low", "essential", "streaming", "applications", "users"]}, {"id": "term-interaction-feature", "t": "Interaction Feature", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A derived feature created by combining two or more existing features (typically through multiplication) to capture...", "l": "i", "k": ["interaction", "feature", "derived", "created", "combining", "existing", "features", "typically", "multiplication", "capture", "non-additive", "effects", "allows", "linear", "models"]}, {"id": "term-internist-1", "t": "INTERNIST-1", "tg": ["History", "Systems"], "d": "history", "x": "A medical diagnostic expert system developed by Jack Myers and Harry Pople at the University of Pittsburgh in the...", "l": "i", "k": ["internist-1", "medical", "diagnostic", "expert", "system", "developed", "jack", "myers", "harry", "pople", "university", "pittsburgh", "1970s", "covered", "approximately"]}, {"id": "term-internlm", "t": "InternLM", "tg": ["Models", "Technical"], "d": "models", "x": "A family of language models developed by Shanghai AI Laboratory. Features strong performance on Chinese and English...", "l": "i", "k": ["internlm", "family", "language", "models", "developed", "shanghai", "laboratory", "features", "strong", "performance", "chinese", "english", "tasks", "specialized", "variants"]}, {"id": "term-internvl", "t": "InternVL", "tg": ["Models", "Technical"], "d": "models", "x": "A large-scale vision-language model that scales the vision encoder to match the capacity of large language models....", "l": "i", "k": ["internvl", "large-scale", "vision-language", "model", "scales", "vision", "encoder", "match", "capacity", "large", "language", "models", "demonstrates", "aligning", "strong"]}, {"id": "term-interpretability", "t": "Interpretability", "tg": ["Property", "Trust"], "d": "safety", "x": "The degree to which humans can understand how a model makes decisions. Higher interpretability enables debugging,...", "l": "i", "k": ["interpretability", "degree", "humans", "understand", "model", "makes", "decisions", "higher", "enables", "debugging", "trust-building", "identifying", "potential", "issues"]}, {"id": "term-interpretability-research", "t": "Interpretability Research", "tg": ["History", "Fundamentals"], "d": "history", "x": "The field of AI research focused on understanding how neural networks make decisions and what they have learned....", "l": "i", "k": ["interpretability", "research", "field", "focused", "understanding", "neural", "networks", "decisions", "learned", "approaches", "include", "mechanistic", "probing", "studies", "visualization"]}, {"id": "term-intersection-over-union", "t": "Intersection over Union", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A metric (IoU) that measures the overlap between a predicted bounding box and a ground truth box by computing the area...", "l": "i", "k": ["intersection", "union", "metric", "iou", "measures", "overlap", "predicted", "bounding", "box", "ground", "truth", "computing", "area", "divided", "evaluate"]}, {"id": "term-intrinsic-motivation", "t": "Intrinsic Motivation", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An internal reward signal generated by the agent itself to encourage exploration, independent of the environment's...", "l": "i", "k": ["intrinsic", "motivation", "internal", "reward", "signal", "generated", "agent", "itself", "encourage", "exploration", "independent", "environment", "extrinsic", "methods", "include"]}, {"id": "term-inverse-rl", "t": "Inverse Reinforcement Learning", "tg": ["Reinforcement Learning", "Imitation"], "d": "general", "x": "The problem of inferring an unknown reward function from observed expert behavior, recovering the objectives that...", "l": "i", "k": ["inverse", "reinforcement", "learning", "problem", "inferring", "unknown", "reward", "function", "observed", "expert", "behavior", "recovering", "objectives", "explain", "demonstrated"]}, {"id": "term-inverse-reward-design", "t": "Inverse Reward Design", "tg": ["Reinforcement Learning", "Safety"], "d": "safety", "x": "A framework that treats the specified reward function as an observation of the designer's true intent rather than the...", "l": "i", "k": ["inverse", "reward", "design", "framework", "treats", "specified", "function", "observation", "designer", "true", "intent", "rather", "literal", "objective", "reasoning"]}, {"id": "term-inverse-scaling", "t": "Inverse Scaling", "tg": ["Research", "Phenomenon"], "d": "general", "x": "When larger models perform worse on certain tasks than smaller ones. Discovered through research challenges, revealing...", "l": "i", "k": ["inverse", "scaling", "larger", "models", "perform", "worse", "certain", "tasks", "smaller", "ones", "discovered", "research", "challenges", "revealing", "unexpected"]}, {"id": "term-inverse-transform-sampling", "t": "Inverse Transform Sampling", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A method for generating random samples from any probability distribution given its inverse cumulative distribution...", "l": "i", "k": ["inverse", "transform", "sampling", "method", "generating", "random", "samples", "probability", "distribution", "given", "cumulative", "function", "transforms", "uniform", "variables"]}, {"id": "term-inverted-residual-block", "t": "Inverted Residual Block", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A building block in MobileNetV2 that expands the channel dimension with a pointwise convolution, applies depthwise...", "l": "i", "k": ["inverted", "residual", "block", "building", "mobilenetv2", "expands", "channel", "dimension", "pointwise", "convolution", "applies", "depthwise", "projects", "narrow", "output"]}, {"id": "term-iob-tagging", "t": "IOB Tagging", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A labeling scheme for sequence tagging that marks tokens as Inside, Outside, or Beginning of a named entity or chunk,...", "l": "i", "k": ["iob", "tagging", "labeling", "scheme", "sequence", "marks", "tokens", "inside", "outside", "beginning", "named", "entity", "chunk", "enabling", "identification"]}, {"id": "term-iou-loss", "t": "IoU Loss", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Intersection over Union loss measures the overlap between predicted and ground truth bounding boxes or segmentation...", "l": "i", "k": ["iou", "loss", "intersection", "union", "measures", "overlap", "predicted", "ground", "truth", "bounding", "boxes", "segmentation", "masks", "directly", "optimizes"]}, {"id": "term-ip-adapter", "t": "IP-Adapter", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "An image prompt adapter for diffusion models that enables image-conditioned generation by injecting visual features...", "l": "i", "k": ["ip-adapter", "image", "prompt", "adapter", "diffusion", "models", "enables", "image-conditioned", "generation", "injecting", "visual", "features", "decoupled", "cross-attention", "allowing"]}, {"id": "term-ipo", "t": "IPO (Identity Preference Optimization)", "tg": ["Training", "Alignment"], "d": "safety", "x": "An alternative to DPO for preference learning that doesn't require a reference model. Simplifies the training process...", "l": "i", "k": ["ipo", "identity", "preference", "optimization", "alternative", "dpo", "learning", "doesn", "require", "reference", "model", "simplifies", "training", "process", "maintaining"]}, {"id": "term-iso-ai-standards", "t": "ISO AI Standards", "tg": ["Governance", "Regulation"], "d": "safety", "x": "International standards developed by ISO/IEC JTC 1/SC 42 for artificial intelligence, including ISO/IEC 42001 for AI...", "l": "i", "k": ["iso", "standards", "international", "developed", "iec", "jtc", "artificial", "intelligence", "including", "management", "systems", "risk"]}, {"id": "term-isolation-forest", "t": "Isolation Forest", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An ensemble-based anomaly detection algorithm that isolates observations by randomly selecting features and split...", "l": "i", "k": ["isolation", "forest", "ensemble-based", "anomaly", "detection", "algorithm", "isolates", "observations", "randomly", "selecting", "features", "split", "values", "anomalies", "require"]}, {"id": "term-isotonic-calibration", "t": "Isotonic Calibration", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A non-parametric calibration method that fits an isotonic regression to map classifier scores to calibrated...", "l": "i", "k": ["isotonic", "calibration", "non-parametric", "method", "fits", "regression", "map", "classifier", "scores", "calibrated", "probabilities", "monotonically", "increasing", "step", "function"]}, {"id": "term-isotonic-regression", "t": "Isotonic Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A non-parametric regression technique that fits a non-decreasing (or non-increasing) step function to the data,...", "l": "i", "k": ["isotonic", "regression", "non-parametric", "technique", "fits", "non-decreasing", "non-increasing", "step", "function", "data", "minimizing", "sum", "squared", "errors", "subject"]}, {"id": "term-iterated-amplification", "t": "Iterated Amplification", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "An AI alignment approach proposed by Paul Christiano where a human overseer is amplified by decomposing complex tasks...", "l": "i", "k": ["iterated", "amplification", "alignment", "approach", "proposed", "paul", "christiano", "human", "overseer", "amplified", "decomposing", "complex", "tasks", "simpler", "subtasks"]}, {"id": "term-iteration", "t": "Iteration (Prompting)", "tg": ["Prompting", "Practice"], "d": "general", "x": "The practice of refining prompts through multiple attempts to achieve better results. Essential for getting the most...", "l": "i", "k": ["iteration", "prompting", "practice", "refining", "prompts", "multiple", "attempts", "achieve", "better", "results", "essential", "getting", "treating", "iterative", "process"]}, {"id": "term-iterative-dpo", "t": "Iterative DPO", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An extension of Direct Preference Optimization that performs multiple rounds of preference optimization. In each...", "l": "i", "k": ["iterative", "dpo", "extension", "direct", "preference", "optimization", "performs", "multiple", "rounds", "iteration", "pairs", "generated", "updated", "model", "creating"]}, {"id": "term-iterative-refinement-prompting", "t": "Iterative Refinement Prompting", "tg": ["Prompt Engineering", "Refinement"], "d": "general", "x": "A technique where the model's initial output is fed back with critique instructions for progressive improvement across...", "l": "i", "k": ["iterative", "refinement", "prompting", "technique", "model", "initial", "output", "fed", "critique", "instructions", "progressive", "improvement", "across", "multiple", "rounds"]}, {"id": "term-ivf-index", "t": "IVF Index", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "Inverted File Index, a vector search structure that partitions the vector space into Voronoi cells using k-means...", "l": "i", "k": ["ivf", "index", "inverted", "file", "vector", "search", "structure", "partitions", "space", "voronoi", "cells", "k-means", "clustering", "searches", "nearest"]}, {"id": "term-jaccard-index", "t": "Jaccard Index", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A similarity coefficient measuring the overlap between two sets, defined as the size of their intersection divided by...", "l": "j", "k": ["jaccard", "index", "similarity", "coefficient", "measuring", "overlap", "sets", "defined", "size", "intersection", "divided", "union", "ranges", "identical"]}, {"id": "term-jaccard-similarity", "t": "Jaccard Similarity", "tg": ["Vector Database", "Similarity"], "d": "general", "x": "A set-based similarity metric calculated as the size of the intersection divided by the size of the union of two sets,...", "l": "j", "k": ["jaccard", "similarity", "set-based", "metric", "calculated", "size", "intersection", "divided", "union", "sets", "commonly", "applied", "binary", "vectors", "token"]}, {"id": "term-jackknife", "t": "Jackknife", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A resampling method that systematically leaves out one observation at a time and recomputes the statistic, using the...", "l": "j", "k": ["jackknife", "resampling", "method", "systematically", "leaves", "observation", "time", "recomputes", "statistic", "variation", "across", "leave-one-out", "estimates", "assess", "bias"]}, {"id": "term-jacobian-vector-product", "t": "Jacobian-Vector Product", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An efficient computation that multiplies the Jacobian matrix of a function by a vector without explicitly forming the...", "l": "j", "k": ["jacobian-vector", "product", "efficient", "computation", "multiplies", "jacobian", "matrix", "function", "vector", "without", "explicitly", "forming", "full", "forward-mode", "automatic"]}, {"id": "term-jailbreak", "t": "Jailbreak", "tg": ["Security", "Risk"], "d": "safety", "x": "Attempts to bypass an AI system's safety restrictions through cleverly crafted prompts. A ongoing challenge for AI...", "l": "j", "k": ["jailbreak", "attempts", "bypass", "system", "safety", "restrictions", "cleverly", "crafted", "prompts", "ongoing", "challenge", "developers", "maintaining", "safe", "behavior"]}, {"id": "term-jamba", "t": "Jamba", "tg": ["Models", "Technical"], "d": "models", "x": "A hybrid architecture by AI21 Labs that interleaves transformer and Mamba layers with mixture-of-experts. Combines the...", "l": "j", "k": ["jamba", "hybrid", "architecture", "ai21", "labs", "interleaves", "transformer", "mamba", "layers", "mixture-of-experts", "combines", "strengths", "attention", "state", "space"]}, {"id": "term-james-slagle", "t": "James Slagle", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who developed SAINT (Symbolic Automatic INTegrator) in 1961 at MIT. SAINT could solve...", "l": "j", "k": ["james", "slagle", "american", "computer", "scientist", "developed", "saint", "symbolic", "automatic", "integrator", "mit", "solve", "integration", "problems", "demonstrating"]}, {"id": "term-japanese-fifth-generation-project", "t": "Japanese Fifth Generation Computer Project", "tg": ["History", "Milestones"], "d": "history", "x": "A 1982-1992 Japanese government initiative to develop massively parallel computers using logic programming for AI...", "l": "j", "k": ["japanese", "fifth", "generation", "computer", "project", "1982-1992", "government", "initiative", "develop", "massively", "parallel", "computers", "logic", "programming", "applications"]}, {"id": "term-jax", "t": "JAX", "tg": ["Framework", "Technical"], "d": "general", "x": "Google's library for high-performance numerical computing and automatic differentiation. Popular for ML research due to...", "l": "j", "k": ["jax", "google", "library", "high-performance", "numerical", "computing", "automatic", "differentiation", "popular", "research", "due", "composability", "gpu", "tpu", "support"]}, {"id": "term-jax-framework", "t": "JAX Framework", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "Google's numerical computing framework that combines NumPy-like syntax with automatic differentiation, XLA compilation,...", "l": "j", "k": ["jax", "framework", "google", "numerical", "computing", "combines", "numpy-like", "syntax", "automatic", "differentiation", "xla", "compilation", "native", "support", "spmd"]}, {"id": "term-jeff-dean", "t": "Jeff Dean", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist and head of Google AI who co-developed foundational large-scale systems including MapReduce...", "l": "j", "k": ["jeff", "dean", "american", "computer", "scientist", "head", "google", "co-developed", "foundational", "large-scale", "systems", "including", "mapreduce", "tensorflow", "transformer"]}, {"id": "term-jeffreys-prior", "t": "Jeffreys Prior", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A non-informative prior distribution derived from the Fisher information matrix that is invariant under...", "l": "j", "k": ["jeffreys", "prior", "non-informative", "distribution", "derived", "fisher", "information", "matrix", "invariant", "reparametrization", "provides", "principled", "default", "knowledge", "available"]}, {"id": "term-jensen-shannon-divergence", "t": "Jensen-Shannon Divergence", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A symmetric and bounded divergence measure derived from KL divergence, computed as the average KL divergence of two...", "l": "j", "k": ["jensen-shannon", "divergence", "symmetric", "bounded", "measure", "derived", "computed", "average", "distributions", "mixture", "always", "finite", "ranges", "log"]}, {"id": "term-john-backus", "t": "John Backus", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who led the development of FORTRAN the first widely used high-level programming language at...", "l": "j", "k": ["john", "backus", "american", "computer", "scientist", "led", "development", "fortran", "widely", "high-level", "programming", "language", "ibm", "1950s", "turing"]}, {"id": "term-john-holland", "t": "John Holland", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1929-2015) who invented genetic algorithms and developed the Holland schema theorem,...", "l": "j", "k": ["john", "holland", "american", "computer", "scientist", "1929-2015", "invented", "genetic", "algorithms", "developed", "schema", "theorem", "founding", "field", "evolutionary"]}, {"id": "term-john-hopfield", "t": "John Hopfield", "tg": ["History", "Pioneers"], "d": "history", "x": "American physicist who introduced Hopfield networks in 1982, applying concepts from statistical physics to create...", "l": "j", "k": ["john", "hopfield", "american", "physicist", "introduced", "networks", "applying", "concepts", "statistical", "physics", "create", "neural", "network", "models", "associative"]}, {"id": "term-john-mccarthy", "t": "John McCarthy", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1927-2011) who coined the term artificial intelligence in 1955, organized the Dartmouth...", "l": "j", "k": ["john", "mccarthy", "american", "computer", "scientist", "1927-2011", "coined", "term", "artificial", "intelligence", "organized", "dartmouth", "workshop", "invented", "lisp"]}, {"id": "term-john-searle", "t": "John Searle", "tg": ["History", "Pioneers"], "d": "history", "x": "American philosopher who proposed the Chinese Room argument in 1980, challenging the notion that computers can truly...", "l": "j", "k": ["john", "searle", "american", "philosopher", "proposed", "chinese", "room", "argument", "challenging", "notion", "computers", "truly", "understand", "language", "possess"]}, {"id": "term-john-von-neumann", "t": "John von Neumann", "tg": ["History", "Pioneers"], "d": "history", "x": "Hungarian-American mathematician (1903-1957) who made foundational contributions to computer architecture, game theory,...", "l": "j", "k": ["john", "von", "neumann", "hungarian-american", "mathematician", "1903-1957", "foundational", "contributions", "computer", "architecture", "game", "theory", "cellular", "automata", "whose"]}, {"id": "term-joseph-weizenbaum", "t": "Joseph Weizenbaum", "tg": ["History", "Pioneers"], "d": "history", "x": "German-American computer scientist (1923-2008) who created ELIZA and later became a prominent critic of AI, warning...", "l": "j", "k": ["joseph", "weizenbaum", "german-american", "computer", "scientist", "1923-2008", "created", "eliza", "later", "became", "prominent", "critic", "warning", "ethical", "implications"]}, {"id": "term-joshua-lederberg", "t": "Joshua Lederberg", "tg": ["History", "Pioneers"], "d": "history", "x": "American molecular biologist and Nobel laureate who collaborated with Edward Feigenbaum on DENDRAL. Lederberg saw the...", "l": "j", "k": ["joshua", "lederberg", "american", "molecular", "biologist", "nobel", "laureate", "collaborated", "edward", "feigenbaum", "dendral", "saw", "potential", "computers", "assist"]}, {"id": "term-joy-buolamwini", "t": "Joy Buolamwini", "tg": ["History", "Pioneers"], "d": "history", "x": "Ghanaian-American computer scientist who founded the Algorithmic Justice League. Her research at MIT Media Lab exposed...", "l": "j", "k": ["joy", "buolamwini", "ghanaian-american", "computer", "scientist", "founded", "algorithmic", "justice", "league", "research", "mit", "media", "lab", "exposed", "significant"]}, {"id": "term-json", "t": "JSON (JavaScript Object Notation)", "tg": ["Format", "Technical"], "d": "general", "x": "A structured data format commonly used for API responses and structured output from LLMs. Many AI models can generate...", "l": "j", "k": ["json", "javascript", "object", "notation", "structured", "data", "format", "commonly", "api", "responses", "output", "llms", "models", "generate", "valid"]}, {"id": "term-json-mode-generation", "t": "JSON Mode", "tg": ["Generative AI", "LLM"], "d": "models", "x": "An inference configuration that constrains a language model to produce only valid JSON output, typically implemented...", "l": "j", "k": ["json", "mode", "inference", "configuration", "constrains", "language", "model", "produce", "valid", "output", "typically", "implemented", "grammar-based", "token", "masking"]}, {"id": "term-json-mode-prompting", "t": "JSON Mode Prompting", "tg": ["Prompt Engineering", "Output Format"], "d": "hardware", "x": "A prompting technique that instructs the language model to output responses exclusively in valid JSON format, often...", "l": "j", "k": ["json", "mode", "prompting", "technique", "instructs", "language", "model", "output", "responses", "exclusively", "valid", "format", "combined", "schema", "definitions"]}, {"id": "term-judea-pearl", "t": "Judea Pearl", "tg": ["History", "Pioneers"], "d": "history", "x": "Israeli-American computer scientist who pioneered Bayesian networks and causal inference in AI, receiving the 2011...", "l": "j", "k": ["judea", "pearl", "israeli-american", "computer", "scientist", "pioneered", "bayesian", "networks", "causal", "inference", "receiving", "turing", "award", "work", "probabilistic"]}, {"id": "term-juergen-schmidhuber", "t": "Juergen Schmidhuber", "tg": ["History", "Pioneers"], "d": "history", "x": "German computer scientist who co-invented Long Short-Term Memory (LSTM) networks with Sepp Hochreiter in 1997. Pioneer...", "l": "j", "k": ["juergen", "schmidhuber", "german", "computer", "scientist", "co-invented", "long", "short-term", "memory", "lstm", "networks", "sepp", "hochreiter", "pioneer", "recurrent"]}, {"id": "term-jupyter", "t": "Jupyter Notebook", "tg": ["Tools", "Development"], "d": "general", "x": "An interactive computing environment where code, visualizations, and text can be combined. Widely used for data...", "l": "j", "k": ["jupyter", "notebook", "interactive", "computing", "environment", "code", "visualizations", "text", "combined", "widely", "data", "science", "experimentation", "educational", "content"]}, {"id": "term-jurassic", "t": "Jurassic", "tg": ["Models", "Technical"], "d": "models", "x": "A family of large language models developed by AI21 Labs. Jurassic-2 features a custom tokenizer optimized for...", "l": "j", "k": ["jurassic", "family", "large", "language", "models", "developed", "ai21", "labs", "jurassic-2", "features", "custom", "tokenizer", "optimized", "efficiency", "supports"]}, {"id": "term-jurgen-schmidhuber", "t": "Jurgen Schmidhuber", "tg": ["History", "Pioneers"], "d": "history", "x": "German-Swiss computer scientist who co-invented LSTM networks, contributed to recurrent neural network research, and...", "l": "j", "k": ["jurgen", "schmidhuber", "german-swiss", "computer", "scientist", "co-invented", "lstm", "networks", "contributed", "recurrent", "neural", "network", "research", "advocated", "recognition"]}, {"id": "term-k-fold", "t": "K-Fold Cross-Validation", "tg": ["Evaluation", "Training"], "d": "datasets", "x": "A cross-validation technique that divides data into K equal parts, training K times with a different part as the test...", "l": "k", "k": ["k-fold", "cross-validation", "technique", "divides", "data", "equal", "parts", "training", "times", "different", "part", "test", "time", "provides", "robust"]}, {"id": "term-k-means", "t": "K-Means", "tg": ["Machine Learning", "Clustering"], "d": "general", "x": "An unsupervised clustering algorithm that partitions n observations into k clusters by iteratively assigning points to...", "l": "k", "k": ["k-means", "unsupervised", "clustering", "algorithm", "partitions", "observations", "clusters", "iteratively", "assigning", "points", "nearest", "centroid", "updating", "centroids", "mean"]}, {"id": "term-k-means-clustering", "t": "K-Means Clustering", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "An unsupervised learning algorithm that partitions data into K clusters by iteratively assigning points to the nearest...", "l": "k", "k": ["k-means", "clustering", "unsupervised", "learning", "algorithm", "partitions", "data", "clusters", "iteratively", "assigning", "points", "nearest", "centroid", "updating", "centroids"]}, {"id": "term-k-nearest-neighbors", "t": "k-Nearest Neighbors", "tg": ["History", "Fundamentals"], "d": "history", "x": "A simple non-parametric classification and regression method proposed in early forms by Evelyn Fix and Joseph Hodges in...", "l": "k", "k": ["k-nearest", "neighbors", "simple", "non-parametric", "classification", "regression", "method", "proposed", "early", "forms", "evelyn", "fix", "joseph", "hodges", "knn"]}, {"id": "term-k-nearest", "t": "K-Nearest Neighbors (KNN)", "tg": ["Algorithm", "Classification"], "d": "algorithms", "x": "A simple ML algorithm that classifies data points based on the majority class of their K nearest neighbors. Intuitive...", "l": "k", "k": ["k-nearest", "neighbors", "knn", "simple", "algorithm", "classifies", "data", "points", "based", "majority", "class", "nearest", "intuitive", "slow", "large"]}, {"id": "term-k-nearest-neighbors-search", "t": "K-Nearest Neighbors Search", "tg": ["Vector Database", "Search"], "d": "general", "x": "A vector retrieval operation that returns the K vectors most similar to a query vector according to a specified...", "l": "k", "k": ["k-nearest", "neighbors", "search", "vector", "retrieval", "operation", "returns", "vectors", "similar", "query", "according", "specified", "distance", "metric", "forming"]}, {"id": "term-kaggle", "t": "Kaggle", "tg": ["History", "Organizations"], "d": "history", "x": "A platform for data science and machine learning competitions founded in 2010 and acquired by Google in 2017. Kaggle...", "l": "k", "k": ["kaggle", "platform", "data", "science", "machine", "learning", "competitions", "founded", "acquired", "google", "hosts", "provides", "datasets", "offers", "collaborative"]}, {"id": "term-kaplan-meier-estimator", "t": "Kaplan-Meier Estimator", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A non-parametric statistic used to estimate the survival function from time-to-event data, accounting for...", "l": "k", "k": ["kaplan-meier", "estimator", "non-parametric", "statistic", "estimate", "survival", "function", "time-to-event", "data", "accounting", "right-censored", "observations", "produces", "step", "decreasing"]}, {"id": "term-kasparov-vs-deep-blue", "t": "Kasparov vs Deep Blue", "tg": ["History", "Milestones"], "d": "history", "x": "The historic 1997 rematch in which IBM's Deep Blue defeated world chess champion Garry Kasparov 3.5 to 2.5, marking the...", "l": "k", "k": ["kasparov", "deep", "blue", "historic", "rematch", "ibm", "defeated", "world", "chess", "champion", "garry", "marking", "time", "computer", "beat"]}, {"id": "term-kendall-tau", "t": "Kendall Tau", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A non-parametric statistic measuring the ordinal association between two rankings, computed from the number of...", "l": "k", "k": ["kendall", "tau", "non-parametric", "statistic", "measuring", "ordinal", "association", "rankings", "computed", "number", "concordant", "discordant", "pairs", "robust", "outliers"]}, {"id": "term-keras", "t": "Keras", "tg": ["Framework", "Deep Learning"], "d": "models", "x": "A high-level neural network API that makes building deep learning models more accessible. Now integrated into...", "l": "k", "k": ["keras", "high-level", "neural", "network", "api", "makes", "building", "deep", "learning", "models", "accessible", "now", "integrated", "tensorflow", "known"]}, {"id": "term-keras-release", "t": "Keras Release", "tg": ["History", "Milestones"], "d": "history", "x": "The initial release of Keras by Francois Chollet in March 2015 as a high-level neural network API. Keras simplified...", "l": "k", "k": ["keras", "release", "initial", "francois", "chollet", "march", "high-level", "neural", "network", "api", "simplified", "deep", "learning", "providing", "intuitive"]}, {"id": "term-kernel", "t": "Kernel (ML)", "tg": ["Concept", "Math"], "d": "general", "x": "A function that measures similarity between data points, enabling algorithms like SVMs to work in high-dimensional...", "l": "k", "k": ["kernel", "function", "measures", "similarity", "data", "points", "enabling", "algorithms", "svms", "work", "high-dimensional", "spaces", "common", "kernels", "include"]}, {"id": "term-kernel-auto-tuning", "t": "Kernel Auto-Tuning", "tg": ["Inference Infrastructure", "GPU"], "d": "hardware", "x": "The process of automatically selecting optimal GPU kernel implementations for specific tensor sizes and hardware...", "l": "k", "k": ["kernel", "auto-tuning", "process", "automatically", "selecting", "optimal", "gpu", "implementations", "specific", "tensor", "sizes", "hardware", "configurations", "tests", "multiple"]}, {"id": "term-kernel-density-estimation", "t": "Kernel Density Estimation", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A non-parametric method for estimating the probability density function of a random variable by placing a kernel (e.g.,...", "l": "k", "k": ["kernel", "density", "estimation", "non-parametric", "method", "estimating", "probability", "function", "random", "variable", "placing", "gaussian", "data", "point", "summing"]}, {"id": "term-kernel-methods", "t": "Kernel Methods", "tg": ["History", "Fundamentals"], "d": "history", "x": "A class of algorithms for pattern analysis that use kernel functions to operate in high-dimensional feature spaces...", "l": "k", "k": ["kernel", "methods", "class", "algorithms", "pattern", "analysis", "functions", "operate", "high-dimensional", "feature", "spaces", "without", "explicitly", "computing", "transformation"]}, {"id": "term-kernel-trick", "t": "Kernel Trick", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A mathematical technique that implicitly maps data into a high-dimensional feature space by computing inner products...", "l": "k", "k": ["kernel", "trick", "mathematical", "technique", "implicitly", "maps", "data", "high-dimensional", "feature", "space", "computing", "inner", "products", "via", "function"]}, {"id": "term-key-value-cache", "t": "Key-Value Cache", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An optimization for autoregressive transformer inference that stores previously computed key and value tensors to avoid...", "l": "k", "k": ["key-value", "cache", "optimization", "autoregressive", "transformer", "inference", "stores", "previously", "computed", "key", "value", "tensors", "avoid", "redundant", "recomputation"]}, {"id": "term-keypoint-detection", "t": "Keypoint Detection", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of identifying specific anatomical or structural points of interest in an image, such as body joints, facial...", "l": "k", "k": ["keypoint", "detection", "task", "identifying", "specific", "anatomical", "structural", "points", "interest", "image", "body", "joints", "facial", "landmarks", "object"]}, {"id": "term-keyword-extraction", "t": "Keyword Extraction", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of automatically identifying the most important or representative words and phrases in a document, using...", "l": "k", "k": ["keyword", "extraction", "task", "automatically", "identifying", "important", "representative", "words", "phrases", "document", "statistical", "graph-based", "neural", "methods"]}, {"id": "term-keyword-search", "t": "Keyword Search", "tg": ["Retrieval", "Search"], "d": "general", "x": "A traditional information retrieval method that matches documents based on the presence and frequency of query terms,...", "l": "k", "k": ["keyword", "search", "traditional", "information", "retrieval", "method", "matches", "documents", "based", "presence", "frequency", "query", "terms", "algorithms", "bm25"]}, {"id": "term-kl-divergence", "t": "KL Divergence", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "Kullback-Leibler divergence, a non-symmetric measure of how one probability distribution differs from a reference...", "l": "k", "k": ["divergence", "kullback-leibler", "non-symmetric", "measure", "probability", "distribution", "differs", "reference", "quantifies", "information", "lost", "approximating", "true", "model"]}, {"id": "term-kling", "t": "Kling", "tg": ["Models", "Technical"], "d": "models", "x": "A video generation model developed by Kuaishou Technology that produces high-quality realistic videos from text...", "l": "k", "k": ["kling", "video", "generation", "model", "developed", "kuaishou", "technology", "produces", "high-quality", "realistic", "videos", "text", "prompts", "features", "strong"]}, {"id": "term-know-your-customer-for-ai", "t": "Know Your Customer for AI", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Proposed regulatory requirements for AI cloud providers and model distributors to verify the identity and intended use...", "l": "k", "k": ["know", "customer", "proposed", "regulatory", "requirements", "cloud", "providers", "model", "distributors", "verify", "identity", "intended", "customers", "accessing", "powerful"]}, {"id": "term-knowledge-acquisition-bottleneck", "t": "Knowledge Acquisition Bottleneck", "tg": ["History", "Fundamentals"], "d": "history", "x": "A fundamental challenge in expert systems development referring to the difficulty and expense of extracting knowledge...", "l": "k", "k": ["knowledge", "acquisition", "bottleneck", "fundamental", "challenge", "expert", "systems", "development", "referring", "difficulty", "expense", "extracting", "human", "experts", "encoding"]}, {"id": "term-knowledge-cutoff", "t": "Knowledge Cutoff", "tg": ["Limitation", "LLM"], "d": "models", "x": "The date beyond which an AI model has no training data. Events after this date are unknown to the model unless provided...", "l": "k", "k": ["knowledge", "cutoff", "date", "beyond", "model", "training", "data", "events", "unknown", "unless", "provided", "context", "retrieval", "augmentation"]}, {"id": "term-knowledge-distillation", "t": "Knowledge Distillation", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A model compression technique where a smaller student model is trained to mimic the outputs of a larger teacher model....", "l": "k", "k": ["knowledge", "distillation", "model", "compression", "technique", "smaller", "student", "trained", "mimic", "outputs", "larger", "teacher", "soft", "probability", "distribution"]}, {"id": "term-knowledge-distillation-efficiency", "t": "Knowledge Distillation for Efficiency", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A model compression technique where a smaller student model is trained to mimic the outputs (soft predictions) of a...", "l": "k", "k": ["knowledge", "distillation", "efficiency", "model", "compression", "technique", "smaller", "student", "trained", "mimic", "outputs", "soft", "predictions", "larger", "teacher"]}, {"id": "term-knowledge-distillation-vision", "t": "Knowledge Distillation for Vision", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of training a compact student vision model to mimic the predictions and feature representations of a larger...", "l": "k", "k": ["knowledge", "distillation", "vision", "process", "training", "compact", "student", "model", "mimic", "predictions", "feature", "representations", "larger", "teacher", "enabling"]}, {"id": "term-knowledge-distillation-loss", "t": "Knowledge Distillation Loss", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A training objective where a smaller student model learns to match the soft probability distributions produced by a...", "l": "k", "k": ["knowledge", "distillation", "loss", "training", "objective", "smaller", "student", "model", "learns", "match", "soft", "probability", "distributions", "produced", "larger"]}, {"id": "term-knowledge-engineering", "t": "Knowledge Engineering", "tg": ["History", "Fundamentals"], "d": "history", "x": "The discipline of integrating knowledge into computer systems to solve complex problems normally requiring human...", "l": "k", "k": ["knowledge", "engineering", "discipline", "integrating", "computer", "systems", "solve", "complex", "problems", "normally", "requiring", "human", "expertise", "key", "practice"]}, {"id": "term-knowledge-graph", "t": "Knowledge Graph", "tg": ["Data Structure", "Knowledge"], "d": "general", "x": "A structured representation of facts as interconnected entities and relationships. Used to enhance AI systems with...", "l": "k", "k": ["knowledge", "graph", "structured", "representation", "facts", "interconnected", "entities", "relationships", "enhance", "systems", "factual", "enable", "reasoning", "data"]}, {"id": "term-knowledge-representation", "t": "Knowledge Representation", "tg": ["History", "Milestones"], "d": "history", "x": "The field within AI concerned with how information about the world can be formally represented in a form that computer...", "l": "k", "k": ["knowledge", "representation", "field", "within", "concerned", "information", "world", "formally", "represented", "form", "computer", "systems", "reasoning", "planning", "problem-solving"]}, {"id": "term-kolmogorov-complexity", "t": "Kolmogorov Complexity", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "The length of the shortest computer program that produces a given string as output, representing the intrinsic...", "l": "k", "k": ["kolmogorov", "complexity", "length", "shortest", "computer", "program", "produces", "given", "string", "output", "representing", "intrinsic", "information", "content", "uncomputable"]}, {"id": "term-kolmogorov-arnold-network", "t": "Kolmogorov-Arnold Network", "tg": ["Models", "Technical"], "d": "models", "x": "A neural network architecture based on the Kolmogorov-Arnold representation theorem that places learnable activation...", "l": "k", "k": ["kolmogorov-arnold", "network", "neural", "architecture", "based", "representation", "theorem", "places", "learnable", "activation", "functions", "edges", "rather", "nodes", "weight"]}, {"id": "term-kolmogorov-smirnov-test", "t": "Kolmogorov-Smirnov Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A non-parametric test that compares a sample distribution with a reference distribution (one-sample) or compares two...", "l": "k", "k": ["kolmogorov-smirnov", "test", "non-parametric", "compares", "sample", "distribution", "reference", "one-sample", "distributions", "two-sample", "maximum", "absolute", "difference", "cumulative", "functions"]}, {"id": "term-kosmos-2", "t": "Kosmos-2", "tg": ["Models", "Technical"], "d": "models", "x": "A multimodal large language model by Microsoft that can ground text to the visual world. Combines language...", "l": "k", "k": ["kosmos-2", "multimodal", "large", "language", "model", "microsoft", "ground", "text", "visual", "world", "combines", "understanding", "spatial", "awareness", "enabling"]}, {"id": "term-krippendorffs-alpha", "t": "Krippendorff's Alpha", "tg": ["Evaluation", "Methodology"], "d": "datasets", "x": "A versatile reliability coefficient that measures agreement among multiple annotators for any number of raters,...", "l": "k", "k": ["krippendorff", "alpha", "versatile", "reliability", "coefficient", "measures", "agreement", "among", "multiple", "annotators", "number", "raters", "variable", "scales", "missing"]}, {"id": "term-krl", "t": "KRL", "tg": ["History", "Systems"], "d": "history", "x": "The Knowledge Representation Language developed by Daniel Bobrow and Terry Winograd at Xerox PARC in the mid-1970s. KRL...", "l": "k", "k": ["krl", "knowledge", "representation", "language", "developed", "daniel", "bobrow", "terry", "winograd", "xerox", "parc", "mid-1970s", "combined", "aspects", "frames"]}, {"id": "term-kto", "t": "KTO", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Kahneman-Tversky Optimization, a preference learning method that trains language models using binary feedback...", "l": "k", "k": ["kto", "kahneman-tversky", "optimization", "preference", "learning", "method", "trains", "language", "models", "binary", "feedback", "good", "bad", "rather", "pairwise"]}, {"id": "term-kv-cache", "t": "KV Cache (Key-Value Cache)", "tg": ["Optimization", "Technical"], "d": "algorithms", "x": "An optimization that stores previously computed key and value vectors in transformer models. Speeds up autoregressive...", "l": "k", "k": ["cache", "key-value", "optimization", "stores", "previously", "computed", "key", "value", "vectors", "transformer", "models", "speeds", "autoregressive", "generation", "avoiding"]}, {"id": "term-kv-cache-compression", "t": "KV Cache Compression", "tg": ["LLM", "Inference"], "d": "models", "x": "Methods for reducing the memory footprint of key-value caches during autoregressive generation, including quantization...", "l": "k", "k": ["cache", "compression", "methods", "reducing", "memory", "footprint", "key-value", "caches", "autoregressive", "generation", "including", "quantization", "cached", "values", "eviction"]}, {"id": "term-kv-cache-optimization", "t": "KV Cache Optimization", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Techniques for reducing the memory footprint and access cost of the key-value cache in transformer inference, including...", "l": "k", "k": ["cache", "optimization", "techniques", "reducing", "memory", "footprint", "access", "cost", "key-value", "transformer", "inference", "including", "quantized", "caches", "multi-query"]}, {"id": "term-l-bfgs", "t": "L-BFGS", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Limited-memory Broyden-Fletcher-Goldfarb-Shanno is a quasi-Newton optimization method that approximates the inverse...", "l": "l", "k": ["l-bfgs", "limited-memory", "broyden-fletcher-goldfarb-shanno", "quasi-newton", "optimization", "method", "approximates", "inverse", "hessian", "limited", "history", "gradient", "updates", "widely", "traditional"]}, {"id": "term-l1-regularization", "t": "L1 Regularization", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A regularization technique that adds the sum of absolute values of model weights to the loss function, encouraging...", "l": "l", "k": ["regularization", "technique", "adds", "sum", "absolute", "values", "model", "weights", "loss", "function", "encouraging", "sparsity", "driving", "exactly", "zero"]}, {"id": "term-l2-regularization", "t": "L2 Regularization", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A regularization technique that adds the sum of squared model weights to the loss function, penalizing large weights...", "l": "l", "k": ["regularization", "technique", "adds", "sum", "squared", "model", "weights", "loss", "function", "penalizing", "large", "encouraging", "small", "exactly", "zero"]}, {"id": "term-label", "t": "Label", "tg": ["Data", "Supervised Learning"], "d": "general", "x": "The correct answer or category associated with training data in supervised learning. Human-provided labels teach models...", "l": "l", "k": ["label", "correct", "answer", "category", "associated", "training", "data", "supervised", "learning", "human-provided", "labels", "teach", "models", "patterns", "learn"]}, {"id": "term-label-encoding", "t": "Label Encoding", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A technique that converts categorical values into integer codes, assigning each unique category a distinct numerical...", "l": "l", "k": ["label", "encoding", "technique", "converts", "categorical", "values", "integer", "codes", "assigning", "unique", "category", "distinct", "numerical", "identifier", "introduces"]}, {"id": "term-label-propagation", "t": "Label Propagation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A semi-supervised graph algorithm that propagates labels from labeled nodes to unlabeled nodes through the graph...", "l": "l", "k": ["label", "propagation", "semi-supervised", "graph", "algorithm", "propagates", "labels", "labeled", "nodes", "unlabeled", "structure", "node", "adopts", "common", "among"]}, {"id": "term-label-smoothing", "t": "Label Smoothing", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A regularization technique that replaces hard one-hot target labels with soft labels that assign a small probability to...", "l": "l", "k": ["label", "smoothing", "regularization", "technique", "replaces", "hard", "one-hot", "target", "labels", "soft", "assign", "small", "probability", "incorrect", "classes"]}, {"id": "term-label-smoothing-regularization", "t": "Label Smoothing Regularization", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A regularization technique that replaces hard one-hot target labels with soft labels distributing a small amount of...", "l": "l", "k": ["label", "smoothing", "regularization", "technique", "replaces", "hard", "one-hot", "target", "labels", "soft", "distributing", "small", "amount", "probability", "mass"]}, {"id": "term-laion", "t": "LAION", "tg": ["History", "Organizations"], "d": "history", "x": "The Large-scale Artificial Intelligence Open Network a nonprofit organization that created LAION-5B one of the largest...", "l": "l", "k": ["laion", "large-scale", "artificial", "intelligence", "open", "network", "nonprofit", "organization", "created", "laion-5b", "largest", "openly", "available", "image-text", "datasets"]}, {"id": "term-lamb", "t": "LAMB", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Layer-wise Adaptive Moments optimizer designed for large-batch distributed training. Extends LARS with Adam-style...", "l": "l", "k": ["lamb", "layer-wise", "adaptive", "moments", "optimizer", "designed", "large-batch", "distributed", "training", "extends", "lars", "adam-style", "momentum", "train", "bert"]}, {"id": "term-lamb-optimizer", "t": "LAMB Optimizer", "tg": ["Model Optimization", "Distributed Computing"], "d": "models", "x": "Layer-wise Adaptive Moments optimizer for Batch training, a variant of Adam that applies layer-wise learning rate...", "l": "l", "k": ["lamb", "optimizer", "layer-wise", "adaptive", "moments", "batch", "training", "variant", "adam", "applies", "learning", "rate", "adaptation", "enable", "stable"]}, {"id": "term-lambda-calculus", "t": "Lambda Calculus", "tg": ["History", "Fundamentals"], "d": "history", "x": "A formal system in mathematical logic for expressing computation based on function abstraction and application...", "l": "l", "k": ["lambda", "calculus", "formal", "system", "mathematical", "logic", "expressing", "computation", "based", "function", "abstraction", "application", "developed", "alonzo", "church"]}, {"id": "term-lancedb", "t": "LanceDB", "tg": ["Vector Database", "Open Source"], "d": "general", "x": "An open-source serverless vector database built on the Lance columnar data format, supporting multimodal data storage...", "l": "l", "k": ["lancedb", "open-source", "serverless", "vector", "database", "built", "lance", "columnar", "data", "format", "supporting", "multimodal", "storage", "embedded", "cloud-native"]}, {"id": "term-lane-detection", "t": "Lane Detection", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of identifying and localizing lane markings on road surfaces in driving images or video, using curve fitting,...", "l": "l", "k": ["lane", "detection", "task", "identifying", "localizing", "markings", "road", "surfaces", "driving", "images", "video", "curve", "fitting", "segmentation", "anchor-based"]}, {"id": "term-langchain", "t": "LangChain", "tg": ["Framework", "Application"], "d": "general", "x": "A popular framework for building applications with LLMs. Provides abstractions for chains, agents, memory, and tool...", "l": "l", "k": ["langchain", "popular", "framework", "building", "applications", "llms", "provides", "abstractions", "chains", "agents", "memory", "tool", "simplifying", "complex", "application"]}, {"id": "term-langevin-dynamics", "t": "Langevin Dynamics", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A stochastic process that uses gradient information with added noise to sample from a probability distribution. Used in...", "l": "l", "k": ["langevin", "dynamics", "stochastic", "process", "uses", "gradient", "information", "added", "noise", "sample", "probability", "distribution", "score-based", "generative", "models"]}, {"id": "term-language-identification", "t": "Language Identification", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of automatically determining what natural language a given text is written in, using features like character...", "l": "l", "k": ["language", "identification", "task", "automatically", "determining", "natural", "given", "text", "written", "features", "character", "n-grams", "word", "frequency", "patterns"]}, {"id": "term-language-modeling", "t": "Language Modeling", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of learning a probability distribution over sequences of tokens, enabling the model to estimate the likelihood...", "l": "l", "k": ["language", "modeling", "task", "learning", "probability", "distribution", "sequences", "tokens", "enabling", "model", "estimate", "likelihood", "given", "text", "sequence"]}, {"id": "term-language-understanding", "t": "Language Understanding (NLU)", "tg": ["NLP", "Capability"], "d": "general", "x": "AI capability to comprehend the meaning, intent, and context of human language. Includes parsing structure, resolving...", "l": "l", "k": ["language", "understanding", "nlu", "capability", "comprehend", "meaning", "intent", "context", "human", "includes", "parsing", "structure", "resolving", "references", "implicit"]}, {"id": "term-laplace-approximation", "t": "Laplace Approximation", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A technique for approximating a posterior distribution with a Gaussian centered at the mode (MAP estimate), using the...", "l": "l", "k": ["laplace", "approximation", "technique", "approximating", "posterior", "distribution", "gaussian", "centered", "mode", "map", "estimate", "curvature", "log-posterior", "hessian", "determine"]}, {"id": "term-large-batch-training", "t": "Large Batch Training", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "Techniques for training neural networks with very large batch sizes (thousands to millions of samples) distributed...", "l": "l", "k": ["large", "batch", "training", "techniques", "neural", "networks", "sizes", "thousands", "millions", "samples", "distributed", "across", "gpus", "requires", "careful"]}, {"id": "term-llm", "t": "Large Language Model (LLM)", "tg": ["Model Type", "Core Concept"], "d": "models", "x": "An AI system trained on massive amounts of text data to understand and generate human language. Includes models like...", "l": "l", "k": ["large", "language", "model", "llm", "system", "trained", "massive", "amounts", "text", "data", "understand", "generate", "human", "includes", "models"]}, {"id": "term-lars", "t": "LARS", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Layer-wise Adaptive Rate Scaling adjusts the learning rate for each layer based on the ratio of weight norm to gradient...", "l": "l", "k": ["lars", "layer-wise", "adaptive", "rate", "scaling", "adjusts", "learning", "layer", "based", "ratio", "weight", "norm", "gradient", "enables", "training"]}, {"id": "term-lars-optimizer", "t": "LARS Optimizer", "tg": ["Model Optimization", "Distributed Computing"], "d": "models", "x": "Layer-wise Adaptive Rate Scaling, an optimizer that adjusts the learning rate per layer based on the ratio of weight...", "l": "l", "k": ["lars", "optimizer", "layer-wise", "adaptive", "rate", "scaling", "adjusts", "learning", "per", "layer", "based", "ratio", "weight", "norm", "gradient"]}, {"id": "term-lasso-regression", "t": "Lasso Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A linear regression method that applies L1 regularization to the coefficient estimates, performing both variable...", "l": "l", "k": ["lasso", "regression", "linear", "method", "applies", "regularization", "coefficient", "estimates", "performing", "variable", "selection", "driving", "coefficients", "exactly", "zero"]}, {"id": "term-late-chunking", "t": "Late Chunking", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A technique that first encodes an entire document through a long-context embedding model and then pools token...", "l": "l", "k": ["late", "chunking", "technique", "encodes", "entire", "document", "long-context", "embedding", "model", "pools", "token", "embeddings", "chunk-level", "representations", "preserving"]}, {"id": "term-late-interaction", "t": "Late Interaction", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "A neural retrieval paradigm where queries and documents are independently encoded into sets of token-level embeddings,...", "l": "l", "k": ["late", "interaction", "neural", "retrieval", "paradigm", "queries", "documents", "independently", "encoded", "sets", "token-level", "embeddings", "relevance", "computed", "lightweight"]}, {"id": "term-latency", "t": "Latency", "tg": ["Performance", "Metrics"], "d": "datasets", "x": "The time delay between sending a prompt and receiving a response. Affected by model size, server load, prompt...", "l": "l", "k": ["latency", "time", "delay", "sending", "prompt", "receiving", "response", "affected", "model", "size", "server", "load", "complexity", "output", "length"]}, {"id": "term-latent-diffusion-model", "t": "Latent Diffusion Model", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A diffusion model that operates in the latent space of a pretrained autoencoder rather than pixel space, significantly...", "l": "l", "k": ["latent", "diffusion", "model", "operates", "space", "pretrained", "autoencoder", "rather", "pixel", "significantly", "reducing", "computational", "requirements", "maintaining", "generation"]}, {"id": "term-latent-dirichlet-allocation", "t": "Latent Dirichlet Allocation", "tg": ["Machine Learning", "Probability"], "d": "algorithms", "x": "A generative probabilistic model for topic modeling that represents each document as a mixture of topics and each topic...", "l": "l", "k": ["latent", "dirichlet", "allocation", "generative", "probabilistic", "model", "topic", "modeling", "represents", "document", "mixture", "topics", "distribution", "words", "priors"]}, {"id": "term-latin-hypercube-sampling", "t": "Latin Hypercube Sampling", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A stratified sampling technique that divides each dimension into equal-probability intervals and ensures each interval...", "l": "l", "k": ["latin", "hypercube", "sampling", "stratified", "technique", "divides", "dimension", "equal-probability", "intervals", "ensures", "interval", "sampled", "exactly", "achieving", "coverage"]}, {"id": "term-law-of-large-numbers", "t": "Law of Large Numbers", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A theorem stating that as the number of independent trials increases, the sample average converges to the expected...", "l": "l", "k": ["law", "large", "numbers", "theorem", "stating", "number", "independent", "trials", "increases", "sample", "average", "converges", "expected", "value", "provides"]}, {"id": "term-lawrence-fogel", "t": "Lawrence Fogel", "tg": ["History", "Pioneers"], "d": "history", "x": "American engineer who introduced evolutionary programming in 1966 as a method for generating AI through simulated...", "l": "l", "k": ["lawrence", "fogel", "american", "engineer", "introduced", "evolutionary", "programming", "method", "generating", "simulated", "evolution", "along", "john", "holland", "ingo"]}, {"id": "term-layer-freezing", "t": "Layer Freezing", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A fine-tuning strategy that keeps some layers of a pretrained model fixed while only updating others. Typically earlier...", "l": "l", "k": ["layer", "freezing", "fine-tuning", "strategy", "keeps", "layers", "pretrained", "model", "fixed", "updating", "others", "typically", "earlier", "capturing", "general"]}, {"id": "term-layer-normalization", "t": "Layer Normalization", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A normalization technique that computes mean and variance across all features within a single training example rather...", "l": "l", "k": ["layer", "normalization", "technique", "computes", "mean", "variance", "across", "features", "within", "single", "training", "example", "rather", "batch", "making"]}, {"id": "term-layer-wise-learning-rate-decay", "t": "Layer-wise Learning Rate Decay", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A technique that applies progressively smaller learning rates to earlier layers of a neural network during fine-tuning....", "l": "l", "k": ["layer-wise", "learning", "rate", "decay", "technique", "applies", "progressively", "smaller", "rates", "earlier", "layers", "neural", "network", "fine-tuning", "based"]}, {"id": "term-layout-analysis", "t": "Layout Analysis", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The process of detecting and classifying structural elements in document images (headers, paragraphs, tables, figures),...", "l": "l", "k": ["layout", "analysis", "process", "detecting", "classifying", "structural", "elements", "document", "images", "headers", "paragraphs", "tables", "figures", "establishing", "reading"]}, {"id": "term-leaky-relu", "t": "Leaky ReLU", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A variant of ReLU that allows a small non-zero gradient when the input is negative. Defined as f(x) = x if x > 0 and...", "l": "l", "k": ["leaky", "relu", "variant", "allows", "small", "non-zero", "gradient", "input", "negative", "defined", "alpha", "otherwise", "typically", "addresses", "dying"]}, {"id": "term-learned-positional-embedding", "t": "Learned Positional Embedding", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A trainable embedding table that assigns a learnable vector to each position in a sequence, allowing the model to...", "l": "l", "k": ["learned", "positional", "embedding", "trainable", "table", "assigns", "learnable", "vector", "position", "sequence", "allowing", "model", "discover", "optimal", "representations"]}, {"id": "term-learning-curve", "t": "Learning Curve", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A plot showing model performance as a function of training set size or training iterations. It reveals whether a model...", "l": "l", "k": ["learning", "curve", "plot", "showing", "model", "performance", "function", "training", "size", "iterations", "reveals", "suffers", "high", "bias", "underfitting"]}, {"id": "term-learning-rate", "t": "Learning Rate", "tg": ["Hyperparameter", "Training"], "d": "general", "x": "A hyperparameter controlling how much model weights are adjusted during training. Too high causes instability; too low...", "l": "l", "k": ["learning", "rate", "hyperparameter", "controlling", "model", "weights", "adjusted", "training", "high", "causes", "instability", "low", "slow", "scheduled", "decrease"]}, {"id": "term-learning-rate-schedule", "t": "Learning Rate Schedule", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A predefined strategy for adjusting the learning rate during training, such as step decay, exponential decay, or cosine...", "l": "l", "k": ["learning", "rate", "schedule", "predefined", "strategy", "adjusting", "training", "step", "decay", "exponential", "cosine", "annealing", "properly", "tuned", "schedules"]}, {"id": "term-learning-rate-warmup", "t": "Learning Rate Warmup", "tg": ["Model Optimization", "Distributed Computing"], "d": "models", "x": "A training technique that gradually increases the learning rate from near-zero to the target value over the first...", "l": "l", "k": ["learning", "rate", "warmup", "training", "technique", "gradually", "increases", "near-zero", "target", "value", "portion", "stabilizes", "dynamics", "large", "batch"]}, {"id": "term-least-to-most-decomposition", "t": "Least-to-Most Decomposition", "tg": ["Prompt Engineering", "Decomposition"], "d": "general", "x": "The first stage of least-to-most prompting where a complex problem is broken into a sequence of progressively more...", "l": "l", "k": ["least-to-most", "decomposition", "stage", "prompting", "complex", "problem", "broken", "sequence", "progressively", "difficult", "sub-problems", "sub-problem", "building", "solutions", "easier"]}, {"id": "term-leave-one-out-cross-validation", "t": "Leave-One-Out Cross-Validation", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A cross-validation method where each observation serves as a single-element test set while all remaining observations...", "l": "l", "k": ["leave-one-out", "cross-validation", "method", "observation", "serves", "single-element", "test", "remaining", "observations", "form", "training", "provides", "nearly", "unbiased", "estimates"]}, {"id": "term-legalbert", "t": "LegalBERT", "tg": ["Models", "Technical"], "d": "models", "x": "A BERT model pretrained on legal text corpora including court opinions legislation and contracts. Achieves improved...", "l": "l", "k": ["legalbert", "bert", "model", "pretrained", "legal", "text", "corpora", "including", "court", "opinions", "legislation", "contracts", "achieves", "improved", "performance"]}, {"id": "term-lemmatization", "t": "Lemmatization", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The process of reducing words to their dictionary base form (lemma) using morphological analysis and vocabulary lookup,...", "l": "l", "k": ["lemmatization", "process", "reducing", "words", "dictionary", "base", "form", "lemma", "morphological", "analysis", "vocabulary", "lookup", "producing", "valid", "unlike"]}, {"id": "term-lenet", "t": "LeNet", "tg": ["History", "Milestones"], "d": "history", "x": "A pioneering convolutional neural network designed by Yann LeCun in 1989 for handwritten digit recognition,...", "l": "l", "k": ["lenet", "pioneering", "convolutional", "neural", "network", "designed", "yann", "lecun", "handwritten", "digit", "recognition", "successfully", "deployed", "postal", "service"]}, {"id": "term-length-penalty", "t": "Length Penalty", "tg": ["Generation", "Parameter"], "d": "general", "x": "A parameter in text generation that discourages or encourages longer outputs. Helps control verbosity and can be...", "l": "l", "k": ["length", "penalty", "parameter", "text", "generation", "discourages", "encourages", "longer", "outputs", "helps", "control", "verbosity", "adjusted", "match", "desired"]}, {"id": "term-leslie-valiant", "t": "Leslie Valiant", "tg": ["History", "Pioneers"], "d": "history", "x": "British-American computer scientist who received the Turing Award in 2010 for contributions to computational learning...", "l": "l", "k": ["leslie", "valiant", "british-american", "computer", "scientist", "received", "turing", "award", "contributions", "computational", "learning", "theory", "probably", "approximately", "correct"]}, {"id": "term-lethal-autonomous-weapons-systems", "t": "Lethal Autonomous Weapons Systems", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "A class of autonomous weapons, sometimes called killer robots, capable of independently identifying and lethally...", "l": "l", "k": ["lethal", "autonomous", "weapons", "systems", "class", "sometimes", "called", "killer", "robots", "capable", "independently", "identifying", "lethally", "engaging", "human"]}, {"id": "term-levenshtein-distance", "t": "Levenshtein Distance", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A string metric measuring the minimum number of single-character insertions, deletions, and substitutions needed to...", "l": "l", "k": ["levenshtein", "distance", "string", "metric", "measuring", "minimum", "number", "single-character", "insertions", "deletions", "substitutions", "needed", "transform", "another", "spell"]}, {"id": "term-leverage", "t": "Leverage", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A measure of how far an observation's predictor values are from the center of the predictor space. High-leverage points...", "l": "l", "k": ["leverage", "measure", "far", "observation", "predictor", "values", "center", "space", "high-leverage", "points", "outsized", "potential", "influence", "regression", "fit"]}, {"id": "term-lexical-ambiguity", "t": "Lexical Ambiguity", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The phenomenon where a word or phrase can be interpreted in multiple ways due to polysemy or homonymy, requiring...", "l": "l", "k": ["lexical", "ambiguity", "phenomenon", "word", "phrase", "interpreted", "multiple", "ways", "due", "polysemy", "homonymy", "requiring", "context", "determine", "intended"]}, {"id": "term-librispeech", "t": "LibriSpeech", "tg": ["History", "Milestones"], "d": "history", "x": "A corpus of approximately 1000 hours of read English speech derived from audiobooks created by Vassil Panayotov and...", "l": "l", "k": ["librispeech", "corpus", "approximately", "hours", "read", "english", "speech", "derived", "audiobooks", "created", "vassil", "panayotov", "colleagues", "became", "standard"]}, {"id": "term-lidar", "t": "LiDAR", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "Light Detection and Ranging, a remote sensing technology that measures distances by illuminating targets with laser...", "l": "l", "k": ["lidar", "light", "detection", "ranging", "remote", "sensing", "technology", "measures", "distances", "illuminating", "targets", "laser", "pulses", "producing", "dense"]}, {"id": "term-lightgbm", "t": "LightGBM", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A gradient boosting framework that uses histogram-based algorithms and leaf-wise tree growth for faster training on...", "l": "l", "k": ["lightgbm", "gradient", "boosting", "framework", "uses", "histogram-based", "algorithms", "leaf-wise", "tree", "growth", "faster", "training", "large", "datasets", "supports"]}, {"id": "term-lighthill-report", "t": "Lighthill Report", "tg": ["History", "Milestones"], "d": "history", "x": "A 1973 report by mathematician James Lighthill commissioned by the British Science Research Council that criticized AI...", "l": "l", "k": ["lighthill", "report", "mathematician", "james", "commissioned", "british", "science", "research", "council", "criticized", "failing", "achieve", "ambitious", "goals", "leading"]}, {"id": "term-likelihood-function", "t": "Likelihood Function", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A function of the parameters of a statistical model, computed as the probability of the observed data for given...", "l": "l", "k": ["likelihood", "function", "parameters", "statistical", "model", "computed", "probability", "observed", "data", "given", "parameter", "values", "unlike", "distribution", "evaluated"]}, {"id": "term-likelihood-ratio-test", "t": "Likelihood Ratio Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A hypothesis test that compares the fit of two nested models by computing twice the difference in their...", "l": "l", "k": ["likelihood", "ratio", "test", "hypothesis", "compares", "fit", "nested", "models", "computing", "twice", "difference", "log-likelihoods", "null", "statistic", "follows"]}, {"id": "term-lime", "t": "LIME", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "Local Interpretable Model-agnostic Explanations, a technique that explains individual predictions by fitting a simple...", "l": "l", "k": ["lime", "local", "interpretable", "model-agnostic", "explanations", "technique", "explains", "individual", "predictions", "fitting", "simple", "model", "perturbed", "samples", "around"]}, {"id": "term-linear-attention", "t": "Linear Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention variant that replaces the softmax-based dot-product attention with a kernel-based approximation, achieving...", "l": "l", "k": ["linear", "attention", "variant", "replaces", "softmax-based", "dot-product", "kernel-based", "approximation", "achieving", "time", "memory", "complexity", "respect", "sequence", "length"]}, {"id": "term-linear-discriminant-analysis", "t": "Linear Discriminant Analysis", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "A supervised dimensionality reduction and classification technique that projects data onto directions that maximize the...", "l": "l", "k": ["linear", "discriminant", "analysis", "supervised", "dimensionality", "reduction", "classification", "technique", "projects", "data", "onto", "directions", "maximize", "ratio", "between-class"]}, {"id": "term-linear-function-approximation-rl", "t": "Linear Function Approximation in RL", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "Value function estimation using a linear combination of state features, where the value is a weighted sum of feature...", "l": "l", "k": ["linear", "function", "approximation", "value", "estimation", "combination", "state", "features", "weighted", "sum", "feature", "values", "offers", "convergence", "guarantees"]}, {"id": "term-linear-regression", "t": "Linear Regression", "tg": ["Algorithm", "Fundamentals"], "d": "algorithms", "x": "A foundational ML algorithm that models the relationship between variables using a straight line. Simple but effective...", "l": "l", "k": ["linear", "regression", "foundational", "algorithm", "models", "relationship", "variables", "straight", "line", "simple", "effective", "prediction", "tasks", "relationships"]}, {"id": "term-linear-transformer", "t": "Linear Transformer", "tg": ["Models", "Technical"], "d": "models", "x": "A transformer variant that replaces softmax attention with a kernel-based linear attention mechanism. Reduces...", "l": "l", "k": ["linear", "transformer", "variant", "replaces", "softmax", "attention", "kernel-based", "mechanism", "reduces", "computational", "complexity", "quadratic", "sequence", "length", "trades"]}, {"id": "term-linformer", "t": "Linformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer that projects key and value matrices to a lower-dimensional space before computing attention, reducing...", "l": "l", "k": ["linformer", "transformer", "projects", "key", "value", "matrices", "lower-dimensional", "space", "computing", "attention", "reducing", "quadratic", "complexity", "self-attention", "linear"]}, {"id": "term-lion-optimizer", "t": "Lion Optimizer", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Evolved Sign Momentum optimizer discovered through program search by Google Brain in 2023. Uses only sign operations...", "l": "l", "k": ["lion", "optimizer", "evolved", "sign", "momentum", "discovered", "program", "search", "google", "brain", "uses", "operations", "updates", "making", "memory"]}, {"id": "term-liquid-neural-network", "t": "Liquid Neural Network", "tg": ["Models", "Technical"], "d": "models", "x": "A continuous-time neural network inspired by biological neural circuits that can adapt its behavior based on input...", "l": "l", "k": ["liquid", "neural", "network", "continuous-time", "inspired", "biological", "circuits", "adapt", "behavior", "based", "input", "dynamics", "features", "time-varying", "parameters"]}, {"id": "term-lisp", "t": "LISP", "tg": ["History", "Milestones"], "d": "history", "x": "A programming language family invented by John McCarthy in 1958 that became the dominant language for AI research for...", "l": "l", "k": ["lisp", "programming", "language", "family", "invented", "john", "mccarthy", "became", "dominant", "research", "decades", "featuring", "symbolic", "expression", "processing"]}, {"id": "term-lisp-machine", "t": "LISP Machine", "tg": ["History", "Systems"], "d": "history", "x": "Specialized computers designed to run the LISP programming language efficiently. Developed in the 1970s and 1980s at...", "l": "l", "k": ["lisp", "machine", "specialized", "computers", "designed", "run", "programming", "language", "efficiently", "developed", "1970s", "1980s", "mit", "commercialized", "companies"]}, {"id": "term-llama", "t": "Llama", "tg": ["Model", "Meta"], "d": "models", "x": "Meta's open-weight family of large language models. Released with relatively permissive licenses, enabling widespread...", "l": "l", "k": ["llama", "meta", "open-weight", "family", "large", "language", "models", "released", "relatively", "permissive", "licenses", "enabling", "widespread", "research", "commercial"]}, {"id": "term-llama-2", "t": "LLaMA 2", "tg": ["Models", "Fundamentals"], "d": "models", "x": "The second generation of Meta's open-weight language models available in 7B 13B and 70B parameter sizes. Includes...", "l": "l", "k": ["llama", "generation", "meta", "open-weight", "language", "models", "available", "13b", "70b", "parameter", "sizes", "includes", "chat-optimized", "variants", "fine-tuned"]}, {"id": "term-llama-3", "t": "LLaMA 3", "tg": ["Models", "Technical"], "d": "models", "x": "Meta's third generation of open-weight language models with improved pretraining data and architecture refinements....", "l": "l", "k": ["llama", "meta", "generation", "open-weight", "language", "models", "improved", "pretraining", "data", "architecture", "refinements", "available", "multiple", "sizes", "significantly"]}, {"id": "term-llama-guard", "t": "Llama Guard", "tg": ["Models", "Safety"], "d": "models", "x": "A safety classifier model fine-tuned from LLaMA for evaluating the safety of language model inputs and outputs....", "l": "l", "k": ["llama", "guard", "safety", "classifier", "model", "fine-tuned", "evaluating", "language", "inputs", "outputs", "classifies", "content", "against", "configurable", "taxonomies"]}, {"id": "term-llama-adapter", "t": "Llama-Adapter", "tg": ["Models", "Technical"], "d": "models", "x": "An efficient fine-tuning method that prepends a set of learnable adaptation prompts to the upper layers of a frozen...", "l": "l", "k": ["llama-adapter", "efficient", "fine-tuning", "method", "prepends", "learnable", "adaptation", "prompts", "upper", "layers", "frozen", "llama", "model", "achieves", "strong"]}, {"id": "term-llama-cpp", "t": "llama.cpp", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "An open-source C/C++ library for efficient CPU and GPU inference of large language models using quantized weights....", "l": "l", "k": ["llama", "cpp", "open-source", "library", "efficient", "cpu", "gpu", "inference", "large", "language", "models", "quantized", "weights", "enables", "running"]}, {"id": "term-llamaindex", "t": "LlamaIndex", "tg": ["Framework", "Application"], "d": "general", "x": "A data framework for connecting LLMs to external data sources. Specializes in indexing, retrieval, and RAG applications...", "l": "l", "k": ["llamaindex", "data", "framework", "connecting", "llms", "external", "sources", "specializes", "indexing", "retrieval", "rag", "applications", "various", "connectors", "query"]}, {"id": "term-llava", "t": "LLaVA", "tg": ["Models", "Technical"], "d": "models", "x": "Large Language and Vision Assistant is a multimodal model that connects a vision encoder to a language model using a...", "l": "l", "k": ["llava", "large", "language", "vision", "assistant", "multimodal", "model", "connects", "encoder", "simple", "projection", "layer", "achieves", "strong", "visual"]}, {"id": "term-llemma", "t": "Llemma", "tg": ["Models", "Technical"], "d": "models", "x": "An open language model for mathematics built by continuing pretraining of Code Llama on a blend of mathematical...", "l": "l", "k": ["llemma", "open", "language", "model", "mathematics", "built", "continuing", "pretraining", "code", "llama", "blend", "mathematical", "documents", "achieves", "strong"]}, {"id": "term-llm-as-judge", "t": "LLM-as-Judge", "tg": ["Evaluation", "LLM-Based"], "d": "models", "x": "An evaluation paradigm where a large language model is prompted to assess and score the quality of outputs from other...", "l": "l", "k": ["llm-as-judge", "evaluation", "paradigm", "large", "language", "model", "prompted", "assess", "score", "quality", "outputs", "models", "providing", "scalable", "approximates"]}, {"id": "term-llmlingua", "t": "LLMLingua", "tg": ["Prompt Engineering", "Compression"], "d": "general", "x": "A prompt compression framework that uses a small language model to identify and remove less informative tokens from...", "l": "l", "k": ["llmlingua", "prompt", "compression", "framework", "uses", "small", "language", "model", "identify", "remove", "less", "informative", "tokens", "prompts", "achieving"]}, {"id": "term-load-balancing-loss", "t": "Load Balancing Loss", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An auxiliary loss term used in mixture-of-experts models to encourage uniform distribution of tokens across experts,...", "l": "l", "k": ["load", "balancing", "loss", "auxiliary", "term", "mixture-of-experts", "models", "encourage", "uniform", "distribution", "tokens", "across", "experts", "preventing", "routing"]}, {"id": "term-local-llm", "t": "Local LLM", "tg": ["Deployment", "Privacy"], "d": "safety", "x": "Running language models on personal hardware rather than through cloud APIs. Enables privacy, offline use, and cost...", "l": "l", "k": ["local", "llm", "running", "language", "models", "personal", "hardware", "rather", "cloud", "apis", "enables", "privacy", "offline", "cost", "savings"]}, {"id": "term-local-outlier-factor", "t": "Local Outlier Factor", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A density-based anomaly detection algorithm that compares the local density of a point to the densities of its...", "l": "l", "k": ["local", "outlier", "factor", "density-based", "anomaly", "detection", "algorithm", "compares", "density", "point", "densities", "neighbors", "points", "substantially", "lower"]}, {"id": "term-locality-sensitive-hashing", "t": "Locality-Sensitive Hashing", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "An approximate nearest neighbor technique that hashes similar vectors into the same buckets with high probability using...", "l": "l", "k": ["locality-sensitive", "hashing", "approximate", "nearest", "neighbor", "technique", "hashes", "similar", "vectors", "buckets", "high", "probability", "random", "projections", "enabling"]}, {"id": "term-loebner-prize", "t": "Loebner Prize", "tg": ["History", "Milestones"], "d": "history", "x": "An annual competition in artificial intelligence that awards prizes to the computer programs considered by the judges...", "l": "l", "k": ["loebner", "prize", "annual", "competition", "artificial", "intelligence", "awards", "prizes", "computer", "programs", "considered", "judges", "human-like", "practical", "implementation"]}, {"id": "term-loess", "t": "LOESS", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "Locally Estimated Scatterplot Smoothing, a non-parametric regression method that fits local weighted polynomial...", "l": "l", "k": ["loess", "locally", "estimated", "scatterplot", "smoothing", "non-parametric", "regression", "method", "fits", "local", "weighted", "polynomial", "regressions", "subsets", "data"]}, {"id": "term-lofti-zadeh-fuzzy-sets-paper", "t": "Lofti Zadeh Fuzzy Sets Paper", "tg": ["History", "Milestones"], "d": "history", "x": "The 1965 paper Fuzzy Sets by Lotfi Zadeh published in Information and Control that introduced fuzzy set theory. This...", "l": "l", "k": ["lofti", "zadeh", "fuzzy", "sets", "paper", "lotfi", "published", "information", "control", "introduced", "theory", "work", "extended", "classical", "allow"]}, {"id": "term-log-loss", "t": "Log Loss", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A loss function for binary classification that measures the negative log-likelihood of the true labels given the...", "l": "l", "k": ["log", "loss", "function", "binary", "classification", "measures", "negative", "log-likelihood", "true", "labels", "given", "predicted", "probabilities", "heavily", "penalizes"]}, {"id": "term-log-likelihood", "t": "Log-Likelihood", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The natural logarithm of the likelihood function, used to simplify optimization because it converts products of...", "l": "l", "k": ["log-likelihood", "natural", "logarithm", "likelihood", "function", "simplify", "optimization", "converts", "products", "probabilities", "sums", "maximizing", "equivalent"]}, {"id": "term-log-normal-distribution", "t": "Log-Normal Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution of a random variable whose logarithm is normally distributed. It is used to model...", "l": "l", "k": ["log-normal", "distribution", "continuous", "probability", "random", "variable", "whose", "logarithm", "normally", "distributed", "model", "quantities", "products", "independent", "positive"]}, {"id": "term-log-softmax", "t": "Log-Softmax", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "The logarithm of the softmax function computed more numerically stably by using the log-sum-exp trick. Directly outputs...", "l": "l", "k": ["log-softmax", "logarithm", "softmax", "function", "computed", "numerically", "stably", "log-sum-exp", "trick", "directly", "outputs", "log-probabilities", "negative", "log-likelihood", "loss"]}, {"id": "term-logic-programming", "t": "Logic Programming", "tg": ["History", "Fundamentals"], "d": "history", "x": "A programming paradigm based on formal logic where programs are expressed as a set of logical relations and computation...", "l": "l", "k": ["logic", "programming", "paradigm", "based", "formal", "programs", "expressed", "logical", "relations", "computation", "proceeds", "inference", "developed", "early", "1970s"]}, {"id": "term-logic-theorist", "t": "Logic Theorist", "tg": ["History", "Milestones"], "d": "history", "x": "A program written by Allen Newell and Herbert Simon in 1956 that could prove mathematical theorems from Principia...", "l": "l", "k": ["logic", "theorist", "program", "written", "allen", "newell", "herbert", "simon", "prove", "mathematical", "theorems", "principia", "mathematica", "widely", "considered"]}, {"id": "term-logistic-regression", "t": "Logistic Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A linear classification model that predicts class probabilities using the logistic (sigmoid) function applied to a...", "l": "l", "k": ["logistic", "regression", "linear", "classification", "model", "predicts", "class", "probabilities", "sigmoid", "function", "applied", "combination", "input", "features", "despite"]}, {"id": "term-logistic-regression-history", "t": "Logistic Regression History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The application of logistic regression to classification problems in machine learning. Originally developed for...", "l": "l", "k": ["logistic", "regression", "history", "application", "classification", "problems", "machine", "learning", "originally", "developed", "statistics", "david", "cox", "became", "fundamental"]}, {"id": "term-logit", "t": "Logit", "tg": ["Technical", "Math"], "d": "general", "x": "The raw, unnormalized scores output by a model before converting to probabilities. In LLMs, logits represent the...", "l": "l", "k": ["logit", "raw", "unnormalized", "scores", "output", "model", "converting", "probabilities", "llms", "logits", "represent", "preference", "possible", "next", "token"]}, {"id": "term-logit-bias", "t": "Logit Bias", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A technique that adds fixed values to the logits of specific tokens before sampling, used to encourage or suppress...", "l": "l", "k": ["logit", "bias", "technique", "adds", "fixed", "values", "logits", "specific", "tokens", "sampling", "encourage", "suppress", "particular", "words", "phrases"]}, {"id": "term-logit-lens", "t": "Logit Lens", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An interpretability technique that decodes intermediate transformer layer outputs through the final unembedding matrix...", "l": "l", "k": ["logit", "lens", "interpretability", "technique", "decodes", "intermediate", "transformer", "layer", "outputs", "final", "unembedding", "matrix", "observe", "predictions", "evolve"]}, {"id": "term-long-context-fine-tuning", "t": "Long Context Fine-Tuning", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The process of adapting a pre-trained model to effectively utilize longer context windows than it was originally...", "l": "l", "k": ["long", "context", "fine-tuning", "process", "adapting", "pre-trained", "model", "effectively", "utilize", "longer", "windows", "originally", "trained", "continued", "training"]}, {"id": "term-lstm-history", "t": "Long Short-Term Memory", "tg": ["History", "Milestones"], "d": "history", "x": "A recurrent neural network architecture invented by Sepp Hochreiter and Jurgen Schmidhuber in 1997 that solved the...", "l": "l", "k": ["long", "short-term", "memory", "recurrent", "neural", "network", "architecture", "invented", "sepp", "hochreiter", "jurgen", "schmidhuber", "solved", "vanishing", "gradient"]}, {"id": "term-longformer", "t": "Longformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer variant that combines local sliding window attention with task-specific global attention on selected...", "l": "l", "k": ["longformer", "transformer", "variant", "combines", "local", "sliding", "window", "attention", "task-specific", "global", "selected", "tokens", "enabling", "efficient", "processing"]}, {"id": "term-lookahead-optimizer", "t": "Lookahead Optimizer", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A meta-optimizer that wraps around any base optimizer maintaining two sets of weights. The fast weights explore the...", "l": "l", "k": ["lookahead", "optimizer", "meta-optimizer", "wraps", "around", "base", "maintaining", "sets", "weights", "fast", "explore", "loss", "landscape", "slow", "periodically"]}, {"id": "term-lora", "t": "LoRA (Low-Rank Adaptation)", "tg": ["Training", "Efficiency"], "d": "general", "x": "A parameter-efficient fine-tuning technique that trains only small additional matrices rather than the full model....", "l": "l", "k": ["lora", "low-rank", "adaptation", "parameter-efficient", "fine-tuning", "technique", "trains", "small", "additional", "matrices", "rather", "full", "model", "dramatically", "reduces"]}, {"id": "term-lora-diffusion", "t": "LoRA for Diffusion", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "The application of Low-Rank Adaptation to diffusion models, enabling efficient fine-tuning of image generation models...", "l": "l", "k": ["lora", "diffusion", "application", "low-rank", "adaptation", "models", "enabling", "efficient", "fine-tuning", "image", "generation", "learn", "concepts", "styles", "subjects"]}, {"id": "term-lora-fusion", "t": "LoRA Fusion", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The technique of combining multiple LoRA adapters trained for different tasks or styles into a single model by merging...", "l": "l", "k": ["lora", "fusion", "technique", "combining", "multiple", "adapters", "trained", "different", "tasks", "styles", "single", "model", "merging", "dynamically", "weighting"]}, {"id": "term-loss-function", "t": "Loss Function", "tg": ["Training", "Math"], "d": "general", "x": "A mathematical function measuring how wrong a model's predictions are. Training aims to minimize this loss, with common...", "l": "l", "k": ["loss", "function", "mathematical", "measuring", "wrong", "model", "predictions", "training", "aims", "minimize", "common", "examples", "including", "cross-entropy", "mean"]}, {"id": "term-loss-scaling", "t": "Loss Scaling", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "A technique used in FP16 mixed precision training that multiplies the loss by a large factor before backpropagation to...", "l": "l", "k": ["loss", "scaling", "technique", "fp16", "mixed", "precision", "training", "multiplies", "large", "factor", "backpropagation", "prevent", "small", "gradient", "values"]}, {"id": "term-lost-in-the-middle", "t": "Lost in the Middle", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A documented phenomenon where language models perform worse at retrieving and using information placed in the middle of...", "l": "l", "k": ["lost", "middle", "documented", "phenomenon", "language", "models", "perform", "worse", "retrieving", "information", "placed", "context", "window", "compared", "beginning"]}, {"id": "term-lotfi-zadeh", "t": "Lotfi Zadeh", "tg": ["History", "Pioneers"], "d": "history", "x": "Azerbaijani-American mathematician and computer scientist (1921-2017) who invented fuzzy logic and fuzzy set theory in...", "l": "l", "k": ["lotfi", "zadeh", "azerbaijani-american", "mathematician", "computer", "scientist", "1921-2017", "invented", "fuzzy", "logic", "theory", "providing", "mathematical", "tools", "handling"]}, {"id": "term-lottery-ticket-hypothesis", "t": "Lottery Ticket Hypothesis", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The theory that dense randomly-initialized networks contain sparse subnetworks (winning tickets) that can be trained in...", "l": "l", "k": ["lottery", "ticket", "hypothesis", "theory", "dense", "randomly-initialized", "networks", "contain", "sparse", "subnetworks", "winning", "tickets", "trained", "isolation", "match"]}, {"id": "term-low-rank-approximation", "t": "Low-Rank Approximation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A technique that approximates a matrix with a lower-rank matrix to reduce dimensionality computational cost or noise....", "l": "l", "k": ["low-rank", "approximation", "technique", "approximates", "matrix", "lower-rank", "reduce", "dimensionality", "computational", "cost", "noise", "optimal", "rank-k", "given", "truncated"]}, {"id": "term-low-rank-factorization", "t": "Low-Rank Factorization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A model compression technique that approximates large weight matrices as products of smaller matrices with reduced...", "l": "l", "k": ["low-rank", "factorization", "model", "compression", "technique", "approximates", "large", "weight", "matrices", "products", "smaller", "reduced", "rank", "reduces", "parameters"]}, {"id": "term-lstm", "t": "LSTM (Long Short-Term Memory)", "tg": ["Architecture", "Historical"], "d": "models", "x": "A recurrent neural network architecture designed to capture long-range dependencies in sequences. Was the dominant NLP...", "l": "l", "k": ["lstm", "long", "short-term", "memory", "recurrent", "neural", "network", "architecture", "designed", "capture", "long-range", "dependencies", "sequences", "dominant", "nlp"]}, {"id": "term-machine-learning", "t": "Machine Learning (ML)", "tg": ["Field", "Fundamentals"], "d": "general", "x": "A branch of AI where systems learn patterns from data rather than being explicitly programmed. Includes supervised,...", "l": "m", "k": ["machine", "learning", "branch", "systems", "learn", "patterns", "data", "rather", "explicitly", "programmed", "includes", "supervised", "unsupervised", "reinforcement", "approaches"]}, {"id": "term-machine-learning-history", "t": "Machine Learning History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of machine learning from Arthur Samuel's checkers program (1959) through the perceptron (1958)...", "l": "m", "k": ["machine", "learning", "history", "evolution", "arthur", "samuel", "checkers", "program", "perceptron", "backpropagation", "svms", "1990s", "deep", "revolution", "2012-present"]}, {"id": "term-mt-evaluation", "t": "Machine Translation Evaluation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "Methods for assessing translation quality including automatic metrics like BLEU, METEOR, and COMET that compare system...", "l": "m", "k": ["machine", "translation", "evaluation", "methods", "assessing", "quality", "including", "automatic", "metrics", "bleu", "meteor", "comet", "compare", "system", "output"]}, {"id": "term-machine-translation-history", "t": "Machine Translation History", "tg": ["History", "Milestones"], "d": "history", "x": "The history of using computers to translate between human languages dating to the Georgetown-IBM experiment in 1954....", "l": "m", "k": ["machine", "translation", "history", "computers", "translate", "human", "languages", "dating", "georgetown-ibm", "experiment", "early", "rule-based", "approaches", "gave", "statistical"]}, {"id": "term-machine-unlearning", "t": "Machine Unlearning", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "Techniques for removing the influence of specific training data from a trained model, motivated by privacy rights such...", "l": "m", "k": ["machine", "unlearning", "techniques", "removing", "influence", "specific", "training", "data", "trained", "model", "motivated", "privacy", "rights", "right", "forgotten"]}, {"id": "term-macy-conferences", "t": "Macy Conferences", "tg": ["History", "Milestones"], "d": "history", "x": "A series of interdisciplinary conferences held from 1946 to 1953 that brought together researchers in cybernetics,...", "l": "m", "k": ["macy", "conferences", "series", "interdisciplinary", "held", "brought", "together", "researchers", "cybernetics", "neuroscience", "psychology", "mathematics", "fostering", "cross-disciplinary", "ideas"]}, {"id": "term-mae", "t": "MAE", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Masked Autoencoder, a self-supervised learning method for vision that randomly masks large portions of image patches...", "l": "m", "k": ["mae", "masked", "autoencoder", "self-supervised", "learning", "method", "vision", "randomly", "masks", "large", "portions", "image", "patches", "trains", "model"]}, {"id": "term-magnitude-pruning", "t": "Magnitude Pruning", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A model compression technique that removes weights with the smallest absolute values. Based on the assumption that...", "l": "m", "k": ["magnitude", "pruning", "model", "compression", "technique", "removes", "weights", "smallest", "absolute", "values", "based", "assumption", "small", "contribute", "least"]}, {"id": "term-mahalanobis-distance", "t": "Mahalanobis Distance", "tg": ["Statistics", "Metrics"], "d": "datasets", "x": "A distance metric that accounts for correlations between variables by measuring the number of standard deviations a...", "l": "m", "k": ["mahalanobis", "distance", "metric", "accounts", "correlations", "variables", "measuring", "number", "standard", "deviations", "point", "mean", "distribution", "inverse", "covariance"]}, {"id": "term-maieutic-prompting", "t": "Maieutic Prompting", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting method inspired by the Socratic maieutic approach that generates a tree of explanations with logical...", "l": "m", "k": ["maieutic", "prompting", "method", "inspired", "socratic", "approach", "generates", "tree", "explanations", "logical", "relationships", "uses", "abductive", "reasoning", "identify"]}, {"id": "term-mamba", "t": "Mamba", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A selective state space model architecture that uses input-dependent selection mechanisms to efficiently process...", "l": "m", "k": ["mamba", "selective", "state", "space", "model", "architecture", "uses", "input-dependent", "selection", "mechanisms", "efficiently", "process", "sequences", "linear", "scaling"]}, {"id": "term-mamba-architecture", "t": "Mamba Architecture", "tg": ["History", "Systems"], "d": "history", "x": "A selective state space model architecture introduced by Albert Gu and Tri Dao in December 2023. Mamba provides an...", "l": "m", "k": ["mamba", "architecture", "selective", "state", "space", "model", "introduced", "albert", "tri", "dao", "december", "provides", "alternative", "transformers", "linear-time"]}, {"id": "term-manchester-baby", "t": "Manchester Baby", "tg": ["History", "Systems"], "d": "history", "x": "The Manchester Small-Scale Experimental Machine completed in 1948 at the University of Manchester was the first...", "l": "m", "k": ["manchester", "baby", "small-scale", "experimental", "machine", "completed", "university", "stored-program", "computer", "run", "program", "built", "frederic", "williams", "tom"]}, {"id": "term-manhattan-distance", "t": "Manhattan Distance", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A distance metric computed as the sum of absolute differences across all dimensions between two points, also known as...", "l": "m", "k": ["manhattan", "distance", "metric", "computed", "sum", "absolute", "differences", "across", "dimensions", "points", "known", "taxicab", "measures", "along", "axis-aligned"]}, {"id": "term-mann-whitney-u-test", "t": "Mann-Whitney U Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A non-parametric test that compares the distributions of two independent groups by ranking all observations and testing...", "l": "m", "k": ["mann-whitney", "test", "non-parametric", "compares", "distributions", "independent", "groups", "ranking", "observations", "testing", "group", "tends", "larger", "values", "assume"]}, {"id": "term-manual-chain-of-thought", "t": "Manual Chain-of-Thought", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "The practice of hand-crafting step-by-step reasoning demonstrations within few-shot prompts, where a human explicitly...", "l": "m", "k": ["manual", "chain-of-thought", "practice", "hand-crafting", "step-by-step", "reasoning", "demonstrations", "within", "few-shot", "prompts", "human", "explicitly", "writes", "intermediate", "steps"]}, {"id": "term-mappo", "t": "MAPPO", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "Multi-Agent Proximal Policy Optimization, an extension of PPO to multi-agent settings that uses shared parameters and a...", "l": "m", "k": ["mappo", "multi-agent", "proximal", "policy", "optimization", "extension", "ppo", "settings", "uses", "shared", "parameters", "centralized", "value", "function", "achieves"]}, {"id": "term-markdown-prompting", "t": "Markdown Prompting", "tg": ["Prompt Engineering", "Output Format"], "d": "hardware", "x": "The use of Markdown formatting conventions such as headers, lists, code blocks, and emphasis within prompts to organize...", "l": "m", "k": ["markdown", "prompting", "formatting", "conventions", "headers", "lists", "code", "blocks", "emphasis", "within", "prompts", "organize", "instructions", "improve", "model"]}, {"id": "term-markov-chain", "t": "Markov Chain", "tg": ["Machine Learning", "Probability"], "d": "algorithms", "x": "A stochastic model describing a sequence of states where the probability of transitioning to the next state depends...", "l": "m", "k": ["markov", "chain", "stochastic", "model", "describing", "sequence", "states", "probability", "transitioning", "next", "state", "depends", "current", "property", "preceding"]}, {"id": "term-markov-chain-monte-carlo", "t": "Markov Chain Monte Carlo", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A class of algorithms that sample from probability distributions by constructing a Markov chain whose stationary...", "l": "m", "k": ["markov", "chain", "monte", "carlo", "class", "algorithms", "sample", "probability", "distributions", "constructing", "whose", "stationary", "distribution", "target", "common"]}, {"id": "term-markov-decision-process", "t": "Markov Decision Process (MDP)", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A formal mathematical framework for sequential decision-making defined by states, actions, transition probabilities,...", "l": "m", "k": ["markov", "decision", "process", "mdp", "formal", "mathematical", "framework", "sequential", "decision-making", "defined", "states", "actions", "transition", "probabilities", "rewards"]}, {"id": "term-marvin-minsky", "t": "Marvin Minsky", "tg": ["History", "Pioneers"], "d": "history", "x": "American cognitive scientist and AI pioneer (1927-2016) who co-founded the MIT AI Laboratory, developed the concept of...", "l": "m", "k": ["marvin", "minsky", "american", "cognitive", "scientist", "pioneer", "1927-2016", "co-founded", "mit", "laboratory", "developed", "concept", "frames", "knowledge", "representation"]}, {"id": "term-mask", "t": "Mask / Masking", "tg": ["Technique", "Training"], "d": "general", "x": "Hiding or ignoring certain parts of data during training or inference. In BERT, random tokens are masked for...", "l": "m", "k": ["mask", "masking", "hiding", "ignoring", "certain", "parts", "data", "training", "inference", "bert", "random", "tokens", "masked", "prediction", "transformers"]}, {"id": "term-mask-rcnn", "t": "Mask R-CNN", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "An instance segmentation framework that extends Faster R-CNN by adding a parallel branch for pixel-level mask...", "l": "m", "k": ["mask", "r-cnn", "instance", "segmentation", "framework", "extends", "faster", "adding", "parallel", "branch", "pixel-level", "prediction", "alongside", "existing", "bounding"]}, {"id": "term-mask2former", "t": "Mask2Former", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A universal image segmentation architecture that unifies semantic, instance, and panoptic segmentation through masked...", "l": "m", "k": ["mask2former", "universal", "image", "segmentation", "architecture", "unifies", "semantic", "instance", "panoptic", "masked", "attention", "learnable", "object", "queries", "processed"]}, {"id": "term-masked-language-modeling", "t": "Masked Language Modeling", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A pretraining objective where random tokens in the input are replaced with a mask token and the model learns to predict...", "l": "m", "k": ["masked", "language", "modeling", "pretraining", "objective", "random", "tokens", "input", "replaced", "mask", "token", "model", "learns", "predict", "original"]}, {"id": "term-math-benchmark", "t": "MATH Benchmark", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A challenging benchmark of 12,500 competition-level mathematics problems spanning seven subjects from algebra to number...", "l": "m", "k": ["math", "benchmark", "challenging", "competition-level", "mathematics", "problems", "spanning", "seven", "subjects", "algebra", "number", "theory", "requiring", "sophisticated", "mathematical"]}, {"id": "term-mathgpt", "t": "MathGPT", "tg": ["Models", "Technical"], "d": "models", "x": "A specialized language model designed for mathematical problem solving that combines symbolic and neural approaches....", "l": "m", "k": ["mathgpt", "specialized", "language", "model", "designed", "mathematical", "problem", "solving", "combines", "symbolic", "neural", "approaches", "demonstrates", "improved", "accuracy"]}, {"id": "term-matrix-multiplication", "t": "Matrix Multiplication", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "The fundamental algebraic operation of multiplying two matrices used extensively in neural networks for layer...", "l": "m", "k": ["matrix", "multiplication", "fundamental", "algebraic", "operation", "multiplying", "matrices", "extensively", "neural", "networks", "layer", "computations", "attention", "mechanisms", "embedding"]}, {"id": "term-matryoshka-embeddings", "t": "Matryoshka Embeddings", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An embedding training approach that produces vectors where any prefix of the full embedding is itself a useful...", "l": "m", "k": ["matryoshka", "embeddings", "embedding", "training", "approach", "produces", "vectors", "prefix", "full", "itself", "useful", "allowing", "flexible", "dimensionality", "reduction"]}, {"id": "term-matthews-correlation-coefficient", "t": "Matthews Correlation Coefficient", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A balanced classification metric computed from all four confusion matrix values (TP, TN, FP, FN) that produces a value...", "l": "m", "k": ["matthews", "correlation", "coefficient", "balanced", "classification", "metric", "computed", "four", "confusion", "matrix", "values", "produces", "value", "indicates", "perfect"]}, {"id": "term-max-pooling", "t": "Max Pooling", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A downsampling operation that selects the maximum value within each pooling window, reducing spatial dimensions while...", "l": "m", "k": ["max", "pooling", "downsampling", "operation", "selects", "maximum", "value", "within", "window", "reducing", "spatial", "dimensions", "retaining", "prominent", "features"]}, {"id": "term-maximal-marginal-relevance", "t": "Maximal Marginal Relevance", "tg": ["Retrieval", "Diversity"], "d": "general", "x": "A retrieval diversification algorithm (MMR) that iteratively selects documents by balancing relevance to the query...", "l": "m", "k": ["maximal", "marginal", "relevance", "retrieval", "diversification", "algorithm", "mmr", "iteratively", "selects", "documents", "balancing", "query", "against", "novelty", "relative"]}, {"id": "term-maximum-a-posteriori", "t": "Maximum A Posteriori", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A Bayesian point estimation method that finds the parameter values maximizing the posterior probability, combining the...", "l": "m", "k": ["maximum", "posteriori", "bayesian", "point", "estimation", "method", "finds", "parameter", "values", "maximizing", "posterior", "probability", "combining", "likelihood", "data"]}, {"id": "term-maximum-entropy-rl", "t": "Maximum Entropy RL", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An RL framework that augments the standard return objective with policy entropy, encouraging agents to act as randomly...", "l": "m", "k": ["maximum", "entropy", "framework", "augments", "standard", "return", "objective", "policy", "encouraging", "agents", "act", "randomly", "possible", "achieving", "high"]}, {"id": "term-maximum-likelihood-estimation", "t": "Maximum Likelihood Estimation", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A method of estimating the parameters of a statistical model by finding the parameter values that maximize the...", "l": "m", "k": ["maximum", "likelihood", "estimation", "method", "estimating", "parameters", "statistical", "model", "finding", "parameter", "values", "maximize", "function", "representing", "probability"]}, {"id": "term-maxout", "t": "Maxout", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An activation function that computes the maximum across k linear projections of the input. Generalizes ReLU and Leaky...", "l": "m", "k": ["maxout", "activation", "function", "computes", "maximum", "across", "linear", "projections", "input", "generalizes", "relu", "leaky", "special", "cases", "proposed"]}, {"id": "term-mbpp", "t": "MBPP", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Mostly Basic Python Programming, a code generation benchmark consisting of approximately 1,000 entry-level Python...", "l": "m", "k": ["mbpp", "mostly", "basic", "python", "programming", "code", "generation", "benchmark", "consisting", "approximately", "entry-level", "problems", "test", "cases", "designed"]}, {"id": "term-mccarthys-advice-taker", "t": "McCarthy's Advice Taker", "tg": ["History", "Systems"], "d": "history", "x": "A proposed AI program described by John McCarthy in 1959 that would be able to accept new knowledge in the form of...", "l": "m", "k": ["mccarthy", "advice", "taker", "proposed", "program", "described", "john", "able", "accept", "knowledge", "form", "declarative", "sentences", "logical", "reasoning"]}, {"id": "term-mcculloch-pitts-neuron", "t": "McCulloch-Pitts Neuron", "tg": ["History", "Milestones"], "d": "history", "x": "The first mathematical model of a biological neuron, proposed by Warren McCulloch and Walter Pitts in 1943, showing...", "l": "m", "k": ["mcculloch-pitts", "neuron", "mathematical", "model", "biological", "proposed", "warren", "mcculloch", "walter", "pitts", "showing", "networks", "simple", "binary", "threshold"]}, {"id": "term-mcdiarmids-inequality", "t": "McDiarmid's Inequality", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A concentration inequality stating that a function of independent random variables with bounded differences is close to...", "l": "m", "k": ["mcdiarmid", "inequality", "concentration", "stating", "function", "independent", "random", "variables", "bounded", "differences", "close", "expected", "value", "high", "probability"]}, {"id": "term-mean-absolute-error", "t": "Mean Absolute Error", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A regression loss function computed as the average of the absolute differences between predicted and actual values. It...", "l": "m", "k": ["mean", "absolute", "error", "regression", "loss", "function", "computed", "average", "differences", "predicted", "actual", "values", "robust", "outliers", "squared"]}, {"id": "term-mean-average-precision", "t": "Mean Average Precision", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "The primary evaluation metric for object detection (mAP) that computes the average precision across all classes and IoU...", "l": "m", "k": ["mean", "average", "precision", "primary", "evaluation", "metric", "object", "detection", "map", "computes", "across", "classes", "iou", "thresholds", "summarizing"]}, {"id": "term-mean-field-rl", "t": "Mean Field Reinforcement Learning", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A scalable approach to multi-agent RL that approximates interactions among many agents using a mean field (average...", "l": "m", "k": ["mean", "field", "reinforcement", "learning", "scalable", "approach", "multi-agent", "approximates", "interactions", "among", "agents", "average", "effect", "neighboring", "actions"]}, {"id": "term-mean-reciprocal-rank", "t": "Mean Reciprocal Rank", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A ranking metric that averages the reciprocal of the rank position of the first relevant result across a set of...", "l": "m", "k": ["mean", "reciprocal", "rank", "ranking", "metric", "averages", "position", "relevant", "result", "across", "queries", "measuring", "quickly", "retrieval", "system"]}, {"id": "term-mean-shift", "t": "Mean Shift", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A non-parametric clustering algorithm that iteratively shifts each data point toward the mode of the local density....", "l": "m", "k": ["mean", "shift", "non-parametric", "clustering", "algorithm", "iteratively", "shifts", "data", "point", "toward", "mode", "local", "density", "require", "specifying"]}, {"id": "term-mean-squared-error", "t": "Mean Squared Error", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A regression loss function computed as the average of the squared differences between predicted and actual values. It...", "l": "m", "k": ["mean", "squared", "error", "regression", "loss", "function", "computed", "average", "differences", "predicted", "actual", "values", "penalizes", "larger", "errors"]}, {"id": "term-meaningful-human-control", "t": "Meaningful Human Control", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The requirement that humans retain sufficient understanding, authority, and ability to intervene in AI-driven...", "l": "m", "k": ["meaningful", "human", "control", "requirement", "humans", "retain", "sufficient", "understanding", "authority", "ability", "intervene", "ai-driven", "decisions", "particularly", "high-stakes"]}, {"id": "term-means-ends-analysis", "t": "Means-Ends Analysis", "tg": ["History", "Fundamentals"], "d": "history", "x": "A problem-solving technique used in AI that identifies the difference between a current state and a goal state then...", "l": "m", "k": ["means-ends", "analysis", "problem-solving", "technique", "identifies", "difference", "current", "state", "goal", "selects", "actions", "reduce", "developed", "newell", "simon"]}, {"id": "term-measurement-bias", "t": "Measurement Bias", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Bias introduced when the features or labels used in an AI system systematically differ in quality or meaning across...", "l": "m", "k": ["measurement", "bias", "introduced", "features", "labels", "system", "systematically", "differ", "quality", "meaning", "across", "groups", "arrest", "records", "proxy"]}, {"id": "term-mechanical-turk", "t": "Mechanical Turk", "tg": ["History", "Milestones"], "d": "history", "x": "An Amazon web service launched in 2005 that allows requesters to post human intelligence tasks (HITs) for workers to...", "l": "m", "k": ["mechanical", "turk", "amazon", "web", "service", "launched", "allows", "requesters", "post", "human", "intelligence", "tasks", "hits", "workers", "complete"]}, {"id": "term-mechanistic-interpretability", "t": "Mechanistic Interpretability", "tg": ["Algorithms", "Technical", "Safety"], "d": "algorithms", "x": "A research approach that aims to reverse-engineer the learned algorithms inside neural networks by identifying...", "l": "m", "k": ["mechanistic", "interpretability", "research", "approach", "aims", "reverse-engineer", "learned", "algorithms", "inside", "neural", "networks", "identifying", "interpretable", "circuits", "features"]}, {"id": "term-med-palm", "t": "Med-PaLM", "tg": ["Models", "Technical"], "d": "models", "x": "A medical domain language model by Google based on PaLM with instruction tuning for medical question answering....", "l": "m", "k": ["med-palm", "medical", "domain", "language", "model", "google", "based", "palm", "instruction", "tuning", "question", "answering", "achieved", "expert-level", "performance"]}, {"id": "term-medical-imaging-ai", "t": "Medical Imaging AI", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The application of deep learning to medical images (X-rays, CT scans, MRIs, pathology slides) for tasks like disease...", "l": "m", "k": ["medical", "imaging", "application", "deep", "learning", "images", "x-rays", "scans", "mris", "pathology", "slides", "tasks", "disease", "detection", "segmentation"]}, {"id": "term-medusa-decoding", "t": "Medusa Decoding", "tg": ["LLM", "Inference"], "d": "models", "x": "A parallel decoding method that adds multiple prediction heads to a language model, allowing it to propose and verify...", "l": "m", "k": ["medusa", "decoding", "parallel", "method", "adds", "multiple", "prediction", "heads", "language", "model", "allowing", "propose", "verify", "several", "future"]}, {"id": "term-megatron-lm", "t": "Megatron-LM", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "NVIDIA's framework for efficient large-scale language model training implementing tensor parallelism, pipeline...", "l": "m", "k": ["megatron-lm", "nvidia", "framework", "efficient", "large-scale", "language", "model", "training", "implementing", "tensor", "parallelism", "pipeline", "sequence", "optimized", "hardware"]}, {"id": "term-mel-spectrogram", "t": "Mel Spectrogram", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A visual representation of the frequency content of an audio signal over time using the mel scale which approximates...", "l": "m", "k": ["mel", "spectrogram", "visual", "representation", "frequency", "content", "audio", "signal", "time", "scale", "approximates", "human", "auditory", "perception", "input"]}, {"id": "term-mel-frequency-cepstral-coefficients", "t": "Mel-Frequency Cepstral Coefficients", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A compact representation of the power spectrum of an audio signal on a perceptually motivated mel frequency scale....", "l": "m", "k": ["mel-frequency", "cepstral", "coefficients", "compact", "representation", "power", "spectrum", "audio", "signal", "perceptually", "motivated", "mel", "frequency", "scale", "standard"]}, {"id": "term-memory-ai", "t": "Memory (AI Systems)", "tg": ["Capability", "Architecture"], "d": "models", "x": "Mechanisms allowing AI to retain information across conversations. Includes context windows, conversation history, and...", "l": "m", "k": ["memory", "systems", "mechanisms", "allowing", "retain", "information", "across", "conversations", "includes", "context", "windows", "conversation", "history", "persistent", "features"]}, {"id": "term-memory-bandwidth", "t": "Memory Bandwidth", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "The rate at which data can be transferred between a processor and its memory, measured in GB/s or TB/s. Memory...", "l": "m", "k": ["memory", "bandwidth", "rate", "data", "transferred", "processor", "measured", "primary", "bottleneck", "large", "language", "model", "inference", "weights", "must"]}, {"id": "term-memory-management-llm", "t": "Memory Management for LLM Inference", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Strategies for efficiently allocating and managing GPU memory during large language model inference, including KV cache...", "l": "m", "k": ["memory", "management", "llm", "inference", "strategies", "efficiently", "allocating", "managing", "gpu", "large", "language", "model", "including", "cache", "pooling"]}, {"id": "term-memory-network", "t": "Memory Network", "tg": ["Models", "Technical"], "d": "models", "x": "A neural architecture with an explicit external memory component that can be read and written during inference....", "l": "m", "k": ["memory", "network", "neural", "architecture", "explicit", "external", "component", "read", "written", "inference", "designed", "question", "answering", "tasks", "model"]}, {"id": "term-memory-augmented-neural-network", "t": "Memory-Augmented Neural Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A broad class of neural architectures equipped with external memory modules that can be read and written using...", "l": "m", "k": ["memory-augmented", "neural", "network", "broad", "class", "architectures", "equipped", "external", "memory", "modules", "read", "written", "attention-based", "addressing", "enabling"]}, {"id": "term-memory-bound", "t": "Memory-Bound Workload", "tg": ["Hardware", "Model Optimization"], "d": "models", "x": "A processing task where performance is limited by the rate of data transfer between processor and memory rather than...", "l": "m", "k": ["memory-bound", "workload", "processing", "task", "performance", "limited", "rate", "data", "transfer", "processor", "memory", "rather", "compute", "capability", "llm"]}, {"id": "term-mesa-optimization", "t": "Mesa-Optimization", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "A phenomenon where a learned model (the mesa-optimizer) internally develops its own optimization objective that may...", "l": "m", "k": ["mesa-optimization", "phenomenon", "learned", "model", "mesa-optimizer", "internally", "develops", "optimization", "objective", "differ", "base", "trained", "key", "concern", "advanced"]}, {"id": "term-mesh-reconstruction", "t": "Mesh Reconstruction", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The process of converting 3D point clouds, implicit functions, or depth maps into triangular mesh representations that...", "l": "m", "k": ["mesh", "reconstruction", "process", "converting", "point", "clouds", "implicit", "functions", "depth", "maps", "triangular", "representations", "define", "surface", "geometry"]}, {"id": "term-message-passing", "t": "Message Passing", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A computational framework in graph neural networks where nodes iteratively exchange and aggregate information with...", "l": "m", "k": ["message", "passing", "computational", "framework", "graph", "neural", "networks", "nodes", "iteratively", "exchange", "aggregate", "information", "neighbors", "node", "updates"]}, {"id": "term-message-passing-neural-network", "t": "Message Passing Neural Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A framework for graph neural networks where nodes iteratively update their representations by exchanging and...", "l": "m", "k": ["message", "passing", "neural", "network", "framework", "graph", "networks", "nodes", "iteratively", "update", "representations", "exchanging", "aggregating", "messages", "neighboring"]}, {"id": "term-meta-llama", "t": "Meta LLaMA", "tg": ["History", "Milestones"], "d": "history", "x": "Meta's Large Language Model Meta AI, first released in February 2023 with subsequent versions, representing a major...", "l": "m", "k": ["meta", "llama", "large", "language", "model", "released", "february", "subsequent", "versions", "representing", "major", "open-weight", "catalyzed", "open-source", "ecosystem"]}, {"id": "term-meta-learning", "t": "Meta-Learning", "tg": ["Training", "Advanced"], "d": "general", "x": "Learning how to learn: training models that can quickly adapt to new tasks with few examples. Enables better few-shot...", "l": "m", "k": ["meta-learning", "learning", "learn", "training", "models", "quickly", "adapt", "tasks", "examples", "enables", "better", "few-shot", "transfer", "capabilities"]}, {"id": "term-meta-prompting", "t": "Meta-Prompting", "tg": ["Prompt Engineering", "Meta-Learning"], "d": "general", "x": "A higher-order prompting approach where a language model is instructed to generate, critique, or improve prompts for...", "l": "m", "k": ["meta-prompting", "higher-order", "prompting", "approach", "language", "model", "instructed", "generate", "critique", "improve", "prompts", "itself", "models", "effectively", "prompt"]}, {"id": "term-meta-rl", "t": "Meta-Reinforcement Learning", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "RL approaches that learn to learn, enabling rapid adaptation to new tasks by leveraging experience across a...", "l": "m", "k": ["meta-reinforcement", "learning", "approaches", "learn", "enabling", "rapid", "adaptation", "tasks", "leveraging", "experience", "across", "distribution", "related", "meta-rl", "agents"]}, {"id": "term-metadata-filtering", "t": "Metadata Filtering", "tg": ["Vector Database", "Filtering"], "d": "general", "x": "A vector search technique that applies structured attribute filters alongside similarity search, restricting results to...", "l": "m", "k": ["metadata", "filtering", "vector", "search", "technique", "applies", "structured", "attribute", "filters", "alongside", "similarity", "restricting", "results", "vectors", "matching"]}, {"id": "term-meteor", "t": "METEOR", "tg": ["NLP", "Text Processing"], "d": "general", "x": "Metric for Evaluation of Translation with Explicit ORdering, a machine translation evaluation metric that considers...", "l": "m", "k": ["meteor", "metric", "evaluation", "translation", "explicit", "ordering", "machine", "considers", "synonyms", "stemming", "word", "order", "addition", "exact", "matches"]}, {"id": "term-meteor-score", "t": "METEOR Score", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Metric for Evaluation of Translation with Explicit Ordering evaluates machine translation using unigram matching with...", "l": "m", "k": ["meteor", "score", "metric", "evaluation", "translation", "explicit", "ordering", "evaluates", "machine", "unigram", "matching", "stemming", "synonymy", "paraphrase", "support"]}, {"id": "term-metrics", "t": "Metrics", "tg": ["Evaluation", "Quality"], "d": "datasets", "x": "Quantitative measures used to evaluate model performance. Common metrics include accuracy, precision, recall, F1,...", "l": "m", "k": ["metrics", "quantitative", "measures", "evaluate", "model", "performance", "common", "include", "accuracy", "precision", "recall", "perplexity", "human", "evaluation", "scores"]}, {"id": "term-metropolis-hastings", "t": "Metropolis-Hastings", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "An MCMC algorithm that generates samples from a target distribution by proposing candidate points from a proposal...", "l": "m", "k": ["metropolis-hastings", "mcmc", "algorithm", "generates", "samples", "target", "distribution", "proposing", "candidate", "points", "proposal", "accepting", "rejecting", "based", "acceptance"]}, {"id": "term-michael-jordan", "t": "Michael Jordan", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist and statistician at UC Berkeley known for foundational work in Bayesian machine learning...", "l": "m", "k": ["michael", "jordan", "american", "computer", "scientist", "statistician", "berkeley", "known", "foundational", "work", "bayesian", "machine", "learning", "variational", "inference"]}, {"id": "term-midjourney", "t": "Midjourney", "tg": ["Product", "Image Generation"], "d": "general", "x": "A popular AI image generation service known for artistic, stylized outputs. Accessed through Discord, it's widely used...", "l": "m", "k": ["midjourney", "popular", "image", "generation", "service", "known", "artistic", "stylized", "outputs", "accessed", "discord", "widely", "creative", "commercial", "creation"]}, {"id": "term-midjourney-launch", "t": "Midjourney Launch", "tg": ["History", "Milestones"], "d": "history", "x": "The July 2022 public launch of Midjourney, an independent AI art generation service that produces images from text...", "l": "m", "k": ["midjourney", "launch", "july", "public", "independent", "art", "generation", "service", "produces", "images", "text", "prompts", "becoming", "popular", "creative"]}, {"id": "term-mila-founded", "t": "Mila Founded", "tg": ["History", "Organizations"], "d": "history", "x": "The founding of the Montreal Institute for Learning Algorithms (Mila) by Yoshua Bengio. Mila became one of the world's...", "l": "m", "k": ["mila", "founded", "founding", "montreal", "institute", "learning", "algorithms", "yoshua", "bengio", "became", "world", "largest", "academic", "research", "centers"]}, {"id": "term-milvus", "t": "Milvus", "tg": ["Vector Database", "Open Source"], "d": "general", "x": "An open-source vector database built for scalable similarity search that supports multiple index types, hybrid search,...", "l": "m", "k": ["milvus", "open-source", "vector", "database", "built", "scalable", "similarity", "search", "supports", "multiple", "index", "types", "hybrid", "multi-tenancy", "capable"]}, {"id": "term-min-max-scaling", "t": "Min-Max Scaling", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A normalization technique that linearly rescales features to a fixed range, typically [0, 1], by subtracting the...", "l": "m", "k": ["min-max", "scaling", "normalization", "technique", "linearly", "rescales", "features", "fixed", "range", "typically", "subtracting", "minimum", "value", "dividing", "preserves"]}, {"id": "term-minerva", "t": "Minerva", "tg": ["Models", "Technical"], "d": "models", "x": "A language model by Google fine-tuned for mathematical reasoning on a dataset of scientific papers and web pages...", "l": "m", "k": ["minerva", "language", "model", "google", "fine-tuned", "mathematical", "reasoning", "dataset", "scientific", "papers", "web", "pages", "containing", "content", "achieves"]}, {"id": "term-minhash", "t": "MinHash", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A locality-sensitive hashing technique that efficiently estimates the Jaccard similarity between sets, widely used in...", "l": "m", "k": ["minhash", "locality-sensitive", "hashing", "technique", "efficiently", "estimates", "jaccard", "similarity", "sets", "widely", "nlp", "approximate", "nearest", "neighbor", "search"]}, {"id": "term-mini-batch-gradient-descent", "t": "Mini-Batch Gradient Descent", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A gradient-based optimization method that computes parameter updates using a small random subset (mini-batch) of the...", "l": "m", "k": ["mini-batch", "gradient", "descent", "gradient-based", "optimization", "method", "computes", "parameter", "updates", "small", "random", "subset", "training", "data", "step"]}, {"id": "term-minimax-algorithm", "t": "Minimax Algorithm", "tg": ["History", "Fundamentals"], "d": "history", "x": "A decision-making algorithm for two-player zero-sum games that minimizes the possible loss for a worst-case scenario....", "l": "m", "k": ["minimax", "algorithm", "decision-making", "two-player", "zero-sum", "games", "minimizes", "possible", "loss", "worst-case", "scenario", "game-playing", "1950s", "forms", "basis"]}, {"id": "term-minimum-bayes-risk-decoding", "t": "Minimum Bayes Risk Decoding", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A decoding strategy that selects the output candidate minimizing expected loss across a set of sampled hypotheses,...", "l": "m", "k": ["minimum", "bayes", "risk", "decoding", "strategy", "selects", "output", "candidate", "minimizing", "expected", "loss", "across", "sampled", "hypotheses", "producing"]}, {"id": "term-minimum-description-length", "t": "Minimum Description Length", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A model selection principle that selects the model minimizing the total description length of the data and the model...", "l": "m", "k": ["minimum", "description", "length", "model", "selection", "principle", "selects", "minimizing", "total", "data", "itself", "formalizes", "occam", "razor", "information-theoretic"]}, {"id": "term-minkowski-distance", "t": "Minkowski Distance", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A generalized distance metric parameterized by p that includes Manhattan (p=1), Euclidean (p=2), and Chebyshev...", "l": "m", "k": ["minkowski", "distance", "generalized", "metric", "parameterized", "includes", "manhattan", "euclidean", "chebyshev", "infinity", "distances", "special", "cases", "computes", "p-th"]}, {"id": "term-minsky-papert-perceptrons", "t": "Minsky and Papert Perceptrons", "tg": ["History", "Milestones"], "d": "history", "x": "The 1969 book by Marvin Minsky and Seymour Papert that mathematically demonstrated the limitations of single-layer...", "l": "m", "k": ["minsky", "papert", "perceptrons", "book", "marvin", "seymour", "mathematically", "demonstrated", "limitations", "single-layer", "contributing", "reduced", "funding", "neural", "network"]}, {"id": "term-mirror-descent", "t": "Mirror Descent", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A generalization of gradient descent that uses Bregman divergences instead of Euclidean distance for parameter updates....", "l": "m", "k": ["mirror", "descent", "generalization", "gradient", "uses", "bregman", "divergences", "instead", "euclidean", "distance", "parameter", "updates", "naturally", "handles", "constrained"]}, {"id": "term-mirror-prompting", "t": "Mirror Prompting", "tg": ["Prompt Engineering", "Clarification"], "d": "general", "x": "A prompting approach that instructs the model to first restate the user's request back in its own words, confirming...", "l": "m", "k": ["mirror", "prompting", "approach", "instructs", "model", "restate", "user", "request", "words", "confirming", "mutual", "understanding", "proceeding", "task", "execution"]}, {"id": "term-mish", "t": "Mish", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A self-regularizing non-monotonic activation function defined as f(x) = x * tanh(softplus(x)). Known for smooth...", "l": "m", "k": ["mish", "self-regularizing", "non-monotonic", "activation", "function", "defined", "tanh", "softplus", "known", "smooth", "gradients", "strong", "empirical", "performance", "particularly"]}, {"id": "term-misinformation", "t": "Misinformation", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "False or inaccurate information shared without deliberate intent to deceive, which can be amplified by AI...", "l": "m", "k": ["misinformation", "false", "inaccurate", "information", "shared", "without", "deliberate", "intent", "deceive", "amplified", "recommendation", "systems", "generated", "inadvertently", "hallucinations"]}, {"id": "term-mistral", "t": "Mistral", "tg": ["Company", "Model"], "d": "models", "x": "A French AI company known for efficient, high-performance open models. Their Mistral and Mixtral models offer strong...", "l": "m", "k": ["mistral", "french", "company", "known", "efficient", "high-performance", "open", "models", "mixtral", "offer", "strong", "capabilities", "smaller", "parameter", "counts"]}, {"id": "term-mistral-7b", "t": "Mistral 7B", "tg": ["Models", "Technical"], "d": "models", "x": "A 7 billion parameter language model by Mistral AI that outperforms larger models through architectural innovations...", "l": "m", "k": ["mistral", "billion", "parameter", "language", "model", "outperforms", "larger", "models", "architectural", "innovations", "including", "grouped", "query", "attention", "sliding"]}, {"id": "term-mistral-ai", "t": "Mistral AI", "tg": ["History", "Organizations"], "d": "history", "x": "A French AI company founded in 2023 by former Google DeepMind and Meta researchers including Arthur Mensch. Mistral AI...", "l": "m", "k": ["mistral", "french", "company", "founded", "former", "google", "deepmind", "meta", "researchers", "including", "arthur", "mensch", "developed", "efficient", "open-source"]}, {"id": "term-mistral-ai-founding", "t": "Mistral AI Founding", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of Mistral AI in April 2023 by former Google DeepMind and Meta researchers in Paris, which rapidly became...", "l": "m", "k": ["mistral", "founding", "april", "former", "google", "deepmind", "meta", "researchers", "paris", "rapidly", "became", "leading", "european", "company", "releasing"]}, {"id": "term-mit-ai-laboratory", "t": "MIT AI Laboratory", "tg": ["History", "Milestones"], "d": "history", "x": "A research laboratory co-founded by Marvin Minsky and John McCarthy at MIT in 1959, which became one of the most...", "l": "m", "k": ["mit", "laboratory", "research", "co-founded", "marvin", "minsky", "john", "mccarthy", "became", "influential", "centers", "producing", "foundational", "work", "vision"]}, {"id": "term-mixed-precision-training", "t": "Mixed Precision Training", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "A training technique that uses lower-precision floating-point formats (FP16 or BF16) for forward and backward passes...", "l": "m", "k": ["mixed", "precision", "training", "technique", "uses", "lower-precision", "floating-point", "formats", "fp16", "bf16", "forward", "backward", "passes", "maintaining", "fp32"]}, {"id": "term-mixmatch", "t": "MixMatch", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A semi-supervised learning method that combines consistency regularization entropy minimization and MixUp augmentation....", "l": "m", "k": ["mixmatch", "semi-supervised", "learning", "method", "combines", "consistency", "regularization", "entropy", "minimization", "mixup", "augmentation", "produces", "sharpened", "pseudo-labels", "unlabeled"]}, {"id": "term-mixtral", "t": "Mixtral", "tg": ["Models", "Technical"], "d": "models", "x": "A mixture-of-experts model by Mistral AI that uses sparse MoE layers with 8 experts per layer routing each token to 2...", "l": "m", "k": ["mixtral", "mixture-of-experts", "model", "mistral", "uses", "sparse", "moe", "layers", "experts", "per", "layer", "routing", "token", "achieves", "performance"]}, {"id": "term-mixtral-8x7b", "t": "Mixtral 8x7B", "tg": ["Models", "Technical"], "d": "models", "x": "A specific configuration of the Mixtral mixture-of-experts model with 8 experts of 7 billion parameters each. Routes...", "l": "m", "k": ["mixtral", "8x7b", "specific", "configuration", "mixture-of-experts", "model", "experts", "billion", "parameters", "routes", "token", "resulting", "computational", "cost", "similar"]}, {"id": "term-mixture-of-agents", "t": "Mixture of Agents", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An architecture where multiple specialized LLM agents collaborate on a task, with each agent contributing expertise in...", "l": "m", "k": ["mixture", "agents", "architecture", "multiple", "specialized", "llm", "collaborate", "task", "agent", "contributing", "expertise", "specific", "domain", "router", "aggregator"]}, {"id": "term-mixture-of-depths", "t": "Mixture of Depths", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer variant that learns to dynamically allocate computation by routing only a subset of tokens through each...", "l": "m", "k": ["mixture", "depths", "transformer", "variant", "learns", "dynamically", "allocate", "computation", "routing", "subset", "tokens", "block", "reducing", "total", "maintaining"]}, {"id": "term-moe-inference", "t": "Mixture of Experts Inference", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Inference optimization for Mixture of Experts models where only a subset of expert parameters are activated per token,...", "l": "m", "k": ["mixture", "experts", "inference", "optimization", "models", "subset", "expert", "parameters", "activated", "per", "token", "reducing", "computation", "despite", "large"]}, {"id": "term-mixture-of-experts-layer", "t": "Mixture of Experts Layer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A neural network layer consisting of multiple expert subnetworks and a gating mechanism that routes each input to a...", "l": "m", "k": ["mixture", "experts", "layer", "neural", "network", "consisting", "multiple", "expert", "subnetworks", "gating", "mechanism", "routes", "input", "sparse", "subset"]}, {"id": "term-moe-routing", "t": "Mixture of Experts Routing", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The gating mechanism in MoE models that determines which expert subnetworks process each input, using learned routing...", "l": "m", "k": ["mixture", "experts", "routing", "gating", "mechanism", "moe", "models", "determines", "expert", "subnetworks", "process", "input", "learned", "functions", "achieve"]}, {"id": "term-mixup", "t": "Mixup", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A data augmentation and regularization technique that creates virtual training examples by taking convex combinations...", "l": "m", "k": ["mixup", "data", "augmentation", "regularization", "technique", "creates", "virtual", "training", "examples", "taking", "convex", "combinations", "pairs", "labels", "encouraging"]}, {"id": "term-mlops", "t": "MLOps", "tg": ["Operations", "Production"], "d": "general", "x": "Practices for deploying and maintaining ML models in production. Combines ML, DevOps, and data engineering to ensure...", "l": "m", "k": ["mlops", "practices", "deploying", "maintaining", "models", "production", "combines", "devops", "data", "engineering", "ensure", "reliable", "scalable", "systems"]}, {"id": "term-mmlu", "t": "MMLU (Massive Multitask Language Understanding)", "tg": ["Benchmark", "Evaluation"], "d": "datasets", "x": "A comprehensive benchmark testing language models on 57 subjects from STEM to humanities. Widely used to compare model...", "l": "m", "k": ["mmlu", "massive", "multitask", "language", "understanding", "comprehensive", "benchmark", "testing", "models", "subjects", "stem", "humanities", "widely", "compare", "model"]}, {"id": "term-mnist-dataset", "t": "MNIST Dataset", "tg": ["History", "Milestones"], "d": "history", "x": "The Modified National Institute of Standards and Technology database of handwritten digits created by Yann LeCun and...", "l": "m", "k": ["mnist", "dataset", "modified", "national", "institute", "standards", "technology", "database", "handwritten", "digits", "created", "yann", "lecun", "colleagues", "became"]}, {"id": "term-mobile-inference", "t": "Mobile Inference", "tg": ["Inference Infrastructure", "Hardware"], "d": "hardware", "x": "AI inference optimized for smartphones and tablets, leveraging mobile GPU, NPU, or DSP capabilities. Mobile inference...", "l": "m", "k": ["mobile", "inference", "optimized", "smartphones", "tablets", "leveraging", "gpu", "npu", "dsp", "capabilities", "frameworks", "tensorflow", "lite", "core", "apply"]}, {"id": "term-mobilenet", "t": "MobileNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A family of lightweight CNN architectures designed for mobile and embedded devices that use depthwise separable...", "l": "m", "k": ["mobilenet", "family", "lightweight", "cnn", "architectures", "designed", "mobile", "embedded", "devices", "depthwise", "separable", "convolutions", "dramatically", "reduce", "computation"]}, {"id": "term-mobilenetv2", "t": "MobileNetV2", "tg": ["Models", "Technical"], "d": "models", "x": "An improved mobile architecture that introduces inverted residual blocks with linear bottlenecks. The inverted residual...", "l": "m", "k": ["mobilenetv2", "improved", "mobile", "architecture", "introduces", "inverted", "residual", "blocks", "linear", "bottlenecks", "expands", "channels", "depthwise", "convolution", "projects"]}, {"id": "term-mobilenetv3", "t": "MobileNetV3", "tg": ["Models", "Technical"], "d": "models", "x": "The third generation mobile architecture combining hardware-aware neural architecture search with NetAdapt. Uses...", "l": "m", "k": ["mobilenetv3", "generation", "mobile", "architecture", "combining", "hardware-aware", "neural", "search", "netadapt", "uses", "squeeze-and-excitation", "modules", "h-swish", "activation", "improved"]}, {"id": "term-model", "t": "Model", "tg": ["Core Concept", "Fundamentals"], "d": "general", "x": "The trained AI system that processes inputs and generates outputs. Models are defined by their architecture, size...", "l": "m", "k": ["model", "trained", "system", "processes", "inputs", "generates", "outputs", "models", "defined", "architecture", "size", "parameters", "training", "data", "fine-tuning"]}, {"id": "term-model-averaging", "t": "Model Averaging", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An ensemble technique that combines predictions from multiple models by averaging their outputs. Can use uniform...", "l": "m", "k": ["model", "averaging", "ensemble", "technique", "combines", "predictions", "multiple", "models", "outputs", "uniform", "weights", "learned", "simple", "effective", "reducing"]}, {"id": "term-model-card", "t": "Model Card", "tg": ["Documentation", "Ethics"], "d": "safety", "x": "Documentation describing a model's intended use, limitations, performance metrics, and ethical considerations. A...", "l": "m", "k": ["model", "card", "documentation", "describing", "intended", "limitations", "performance", "metrics", "ethical", "considerations", "standard", "practice", "responsible", "development", "deployment"]}, {"id": "term-model-cards", "t": "Model Cards", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "Standardized documentation artifacts proposed by Mitchell et al. (2019) that accompany trained ML models and report on...", "l": "m", "k": ["model", "cards", "standardized", "documentation", "artifacts", "proposed", "mitchell", "accompany", "trained", "models", "report", "intended", "performance", "characteristics", "limitations"]}, {"id": "term-model-collapse", "t": "Model Collapse", "tg": ["Risk", "Training"], "d": "safety", "x": "A degradation phenomenon where models trained on AI-generated data lose diversity and quality over generations. A...", "l": "m", "k": ["model", "collapse", "degradation", "phenomenon", "models", "trained", "ai-generated", "data", "lose", "diversity", "quality", "generations", "growing", "concern", "synthetic"]}, {"id": "term-model-compression", "t": "Model Compression", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A family of techniques for reducing model size and computational cost while preserving performance, including...", "l": "m", "k": ["model", "compression", "family", "techniques", "reducing", "size", "computational", "cost", "preserving", "performance", "including", "quantization", "pruning", "distillation", "low-rank"]}, {"id": "term-model-distillation", "t": "Model Distillation", "tg": ["LLM", "Inference"], "d": "models", "x": "The process of training a smaller student model to replicate the behavior of a larger teacher model by learning from...", "l": "m", "k": ["model", "distillation", "process", "training", "smaller", "student", "replicate", "behavior", "larger", "teacher", "learning", "output", "probability", "distributions", "rather"]}, {"id": "term-model-distillation-history", "t": "Model Distillation History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of knowledge distillation from the original concept by Hinton Vinyals and Dean (2015) where a smaller...", "l": "m", "k": ["model", "distillation", "history", "development", "knowledge", "original", "concept", "hinton", "vinyals", "dean", "smaller", "student", "learns", "mimic", "larger"]}, {"id": "term-mfu", "t": "Model FLOPs Utilization (MFU)", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "The ratio of observed model FLOPS to the theoretical peak FLOPS of the hardware, measuring how efficiently the training...", "l": "m", "k": ["model", "flops", "utilization", "mfu", "ratio", "observed", "theoretical", "peak", "hardware", "measuring", "efficiently", "training", "system", "utilizes", "available"]}, {"id": "term-model-merging", "t": "Model Merging", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A technique that combines the weights of two or more fine-tuned models into a single model, often using methods like...", "l": "m", "k": ["model", "merging", "technique", "combines", "weights", "fine-tuned", "models", "single", "methods", "linear", "interpolation", "slerp", "ties", "inherit", "capabilities"]}, {"id": "term-model-parallelism", "t": "Model Parallelism", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A distributed training strategy that splits a model's layers or parameters across multiple devices, enabling training...", "l": "m", "k": ["model", "parallelism", "distributed", "training", "strategy", "splits", "layers", "parameters", "across", "multiple", "devices", "enabling", "models", "large", "fit"]}, {"id": "term-model-predictive-control-rl", "t": "Model Predictive Control in RL", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A planning-based approach that uses a learned dynamics model to simulate action sequences forward and selects the first...", "l": "m", "k": ["model", "predictive", "control", "planning-based", "approach", "uses", "learned", "dynamics", "simulate", "action", "sequences", "forward", "selects", "best", "sequence"]}, {"id": "term-model-pruning", "t": "Model Pruning", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A compression technique that removes redundant weights or neurons from a neural network based on magnitude,...", "l": "m", "k": ["model", "pruning", "compression", "technique", "removes", "redundant", "weights", "neurons", "neural", "network", "based", "magnitude", "sensitivity", "criteria", "reduces"]}, {"id": "term-model-serving", "t": "Model Serving", "tg": ["Inference Infrastructure", "Distributed Computing"], "d": "hardware", "x": "The infrastructure and systems for deploying trained models to handle real-time prediction requests at scale. Model...", "l": "m", "k": ["model", "serving", "infrastructure", "systems", "deploying", "trained", "models", "handle", "real-time", "prediction", "requests", "scale", "encompasses", "load", "balancing"]}, {"id": "term-model-sharding", "t": "Model Sharding", "tg": ["LLM", "Inference"], "d": "models", "x": "The technique of partitioning a large model's parameters across multiple devices or storage locations, enabling...", "l": "m", "k": ["model", "sharding", "technique", "partitioning", "large", "parameters", "across", "multiple", "devices", "storage", "locations", "enabling", "inference", "training", "models"]}, {"id": "term-model-based-rl", "t": "Model-Based RL", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "RL approaches that learn or use a model of the environment's transition dynamics and reward function to plan actions or...", "l": "m", "k": ["model-based", "approaches", "learn", "model", "environment", "transition", "dynamics", "reward", "function", "plan", "actions", "generate", "synthetic", "experience", "methods"]}, {"id": "term-model-free-rl", "t": "Model-Free RL", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "RL algorithms that learn policies or value functions directly from experience without building an explicit model of the...", "l": "m", "k": ["model-free", "algorithms", "learn", "policies", "value", "functions", "directly", "experience", "without", "building", "explicit", "model", "environment", "methods", "simpler"]}, {"id": "term-modern-hopfield-network", "t": "Modern Hopfield Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An updated Hopfield network formulation using exponential interaction functions that connects to transformer attention...", "l": "m", "k": ["modern", "hopfield", "network", "updated", "formulation", "exponential", "interaction", "functions", "connects", "transformer", "attention", "mechanisms", "provides", "storage", "capacity"]}, {"id": "term-mixture-of-experts", "t": "MoE (Mixture of Experts)", "tg": ["Architecture", "Efficiency"], "d": "models", "x": "An architecture where different \"expert\" sub-networks specialize in different types of inputs. Enables larger effective...", "l": "m", "k": ["moe", "mixture", "experts", "architecture", "different", "expert", "sub-networks", "specialize", "types", "inputs", "enables", "larger", "effective", "model", "capacity"]}, {"id": "term-moe-llava", "t": "MoE-LLaVA", "tg": ["Models", "Technical"], "d": "models", "x": "A multimodal large language model that uses mixture-of-experts to efficiently scale visual instruction tuning....", "l": "m", "k": ["moe-llava", "multimodal", "large", "language", "model", "uses", "mixture-of-experts", "efficiently", "scale", "visual", "instruction", "tuning", "activates", "subset", "parameters"]}, {"id": "term-momentum", "t": "Momentum", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "An optimization technique that accelerates gradient descent by accumulating an exponentially decaying moving average of...", "l": "m", "k": ["momentum", "optimization", "technique", "accelerates", "gradient", "descent", "accumulating", "exponentially", "decaying", "moving", "average", "past", "gradients", "helping", "optimizer"]}, {"id": "term-monte-carlo-dropout", "t": "Monte Carlo Dropout", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An approximate Bayesian inference technique that uses dropout at inference time to generate multiple stochastic...", "l": "m", "k": ["monte", "carlo", "dropout", "approximate", "bayesian", "inference", "technique", "uses", "time", "generate", "multiple", "stochastic", "predictions", "variance", "provides"]}, {"id": "term-monte-carlo-method", "t": "Monte Carlo Method", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A broad class of computational algorithms that use repeated random sampling to obtain numerical results, such as...", "l": "m", "k": ["monte", "carlo", "method", "broad", "class", "computational", "algorithms", "repeated", "random", "sampling", "obtain", "numerical", "results", "estimating", "integrals"]}, {"id": "term-monte-carlo-methods-rl", "t": "Monte Carlo Methods in RL", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "RL algorithms that estimate value functions by averaging the actual returns observed over complete episodes. Unlike TD...", "l": "m", "k": ["monte", "carlo", "methods", "algorithms", "estimate", "value", "functions", "averaging", "actual", "returns", "observed", "complete", "episodes", "unlike", "approaches"]}, {"id": "term-monte-carlo-tree-search", "t": "Monte Carlo Tree Search (MCTS)", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A search algorithm that builds a decision tree through random simulations, using statistics from previous rollouts to...", "l": "m", "k": ["monte", "carlo", "tree", "search", "mcts", "algorithm", "builds", "decision", "random", "simulations", "statistics", "previous", "rollouts", "guide", "exploration"]}, {"id": "term-montreal-declaration-responsible-ai", "t": "Montreal Declaration for Responsible AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "A declaration adopted in 2018 establishing principles for responsible AI development including well-being, respect for...", "l": "m", "k": ["montreal", "declaration", "responsible", "adopted", "establishing", "principles", "development", "including", "well-being", "respect", "autonomy", "privacy", "democratic", "participation", "equity"]}, {"id": "term-moores-law", "t": "Moore's Law", "tg": ["History", "Milestones"], "d": "history", "x": "An observation by Intel co-founder Gordon Moore in 1965 that the number of transistors on a microchip doubles...", "l": "m", "k": ["moore", "law", "observation", "intel", "co-founder", "gordon", "number", "transistors", "microchip", "doubles", "approximately", "years", "driven", "exponential", "increase"]}, {"id": "term-moral-status-of-ai", "t": "Moral Status of AI", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The philosophical question of whether AI systems can possess moral standing, such that their interests or welfare...", "l": "m", "k": ["moral", "status", "philosophical", "question", "systems", "possess", "standing", "interests", "welfare", "deserve", "ethical", "consideration", "closely", "tied", "debates"]}, {"id": "term-moravecs-paradox", "t": "Moravec's Paradox", "tg": ["History", "Milestones"], "d": "history", "x": "The observation by Hans Moravec and others in the 1980s that high-level reasoning tasks are easy for AI while...", "l": "m", "k": ["moravec", "paradox", "observation", "hans", "others", "1980s", "high-level", "reasoning", "tasks", "easy", "sensorimotor", "skills", "seem", "simple", "humans"]}, {"id": "term-morpheme", "t": "Morpheme", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The smallest meaningful unit of language that cannot be further divided without losing its meaning, including roots,...", "l": "m", "k": ["morpheme", "smallest", "meaningful", "unit", "language", "cannot", "divided", "without", "losing", "meaning", "including", "roots", "prefixes", "suffixes", "inflectional"]}, {"id": "term-morphology", "t": "Morphology", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The branch of linguistics studying the internal structure of words, including how morphemes combine to form words...", "l": "m", "k": ["morphology", "branch", "linguistics", "studying", "internal", "structure", "words", "including", "morphemes", "combine", "form", "inflection", "derivation", "compounding", "processes"]}, {"id": "term-mosaic-augmentation", "t": "Mosaic Augmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A data augmentation technique that combines four training images into a single mosaic image, allowing the model to...", "l": "m", "k": ["mosaic", "augmentation", "data", "technique", "combines", "four", "training", "images", "single", "image", "allowing", "model", "learn", "multiple", "contexts"]}, {"id": "term-motivational-control", "t": "Motivational Control", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "Safety measures that shape an AI system's goals and values to be aligned with human interests, as opposed to capability...", "l": "m", "k": ["motivational", "control", "safety", "measures", "shape", "system", "goals", "values", "aligned", "human", "interests", "opposed", "capability", "restricts", "physically"]}, {"id": "term-movement-pruning", "t": "Movement Pruning", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A pruning method that removes weights based on their movement during fine-tuning rather than their magnitude. Weights...", "l": "m", "k": ["movement", "pruning", "method", "removes", "weights", "based", "fine-tuning", "rather", "magnitude", "move", "toward", "zero", "pruned", "effective", "pretrained"]}, {"id": "term-moving-average-model", "t": "Moving Average Model", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A time series model where the current value is expressed as a linear combination of the current and past white noise...", "l": "m", "k": ["moving", "average", "model", "time", "series", "current", "value", "expressed", "linear", "combination", "past", "white", "noise", "error", "terms"]}, {"id": "term-mpt", "t": "MPT", "tg": ["Models", "Technical"], "d": "models", "x": "MosaicML Pretrained Transformer is a family of open-source language models trained by MosaicML using ALiBi position...", "l": "m", "k": ["mpt", "mosaicml", "pretrained", "transformer", "family", "open-source", "language", "models", "trained", "alibi", "position", "embeddings", "flashattention", "mpt-7b", "notable"]}, {"id": "term-mt-bench", "t": "MT-Bench", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Multi-Turn Benchmark, an evaluation framework that tests language models' conversational abilities across multi-turn...", "l": "m", "k": ["mt-bench", "multi-turn", "benchmark", "evaluation", "framework", "tests", "language", "models", "conversational", "abilities", "across", "dialogues", "follow-up", "questions", "llm"]}, {"id": "term-multi-agent-rl", "t": "Multi-Agent Reinforcement Learning", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "RL involving multiple agents that interact within a shared environment, each with its own observations and objectives....", "l": "m", "k": ["multi-agent", "reinforcement", "learning", "involving", "multiple", "agents", "interact", "within", "shared", "environment", "observations", "objectives", "marl", "introduces", "challenges"]}, {"id": "term-multi-agent-systems", "t": "Multi-Agent Systems", "tg": ["History", "Fundamentals"], "d": "history", "x": "Systems composed of multiple interacting intelligent agents that can cooperate compete and coordinate to solve...", "l": "m", "k": ["multi-agent", "systems", "composed", "multiple", "interacting", "intelligent", "agents", "cooperate", "compete", "coordinate", "solve", "problems", "research", "draws", "game"]}, {"id": "term-multi-armed-bandit", "t": "Multi-Armed Bandit", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "A simplified RL problem where an agent repeatedly chooses among K actions (arms) to maximize cumulative reward, with no...", "l": "m", "k": ["multi-armed", "bandit", "simplified", "problem", "agent", "repeatedly", "chooses", "among", "actions", "arms", "maximize", "cumulative", "reward", "state", "transitions"]}, {"id": "term-multi-gpu-training", "t": "Multi-GPU Training", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "Training a model using multiple GPUs simultaneously within a single node or across nodes, requiring parallelism...", "l": "m", "k": ["multi-gpu", "training", "model", "multiple", "gpus", "simultaneously", "within", "single", "node", "across", "nodes", "requiring", "parallelism", "strategies", "gradient"]}, {"id": "term-multi-head-attention", "t": "Multi-Head Attention", "tg": ["Architecture", "Transformers"], "d": "models", "x": "An extension of attention that runs multiple attention operations in parallel, each focusing on different aspects. A...", "l": "m", "k": ["multi-head", "attention", "extension", "runs", "multiple", "operations", "parallel", "focusing", "different", "aspects", "key", "component", "transformer", "architectures"]}, {"id": "term-mig", "t": "Multi-Instance GPU (MIG)", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "An NVIDIA feature that partitions a single GPU into up to seven isolated instances, each with dedicated compute,...", "l": "m", "k": ["multi-instance", "gpu", "mig", "nvidia", "feature", "partitions", "single", "seven", "isolated", "instances", "dedicated", "compute", "memory", "cache", "resources"]}, {"id": "term-multi-label-classification", "t": "Multi-Label Classification", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A classification task where each instance can belong to multiple classes simultaneously, unlike multi-class...", "l": "m", "k": ["multi-label", "classification", "task", "instance", "belong", "multiple", "classes", "simultaneously", "unlike", "multi-class", "exactly", "label", "examples", "include", "document"]}, {"id": "term-multi-layer-perceptron", "t": "Multi-Layer Perceptron", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A feedforward neural network with one or more hidden layers between input and output. Uses nonlinear activation...", "l": "m", "k": ["multi-layer", "perceptron", "feedforward", "neural", "network", "hidden", "layers", "input", "output", "uses", "nonlinear", "activation", "functions", "enabling", "learn"]}, {"id": "term-multi-object-tracking", "t": "Multi-Object Tracking", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of simultaneously tracking multiple objects through a video sequence, handling challenges like occlusion,...", "l": "m", "k": ["multi-object", "tracking", "task", "simultaneously", "multiple", "objects", "video", "sequence", "handling", "challenges", "occlusion", "identity", "switches", "entering", "leaving"]}, {"id": "term-multi-objective-rl", "t": "Multi-Objective RL", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "RL formulations where the agent must optimize multiple potentially conflicting reward functions simultaneously....", "l": "m", "k": ["multi-objective", "formulations", "agent", "must", "optimize", "multiple", "potentially", "conflicting", "reward", "functions", "simultaneously", "solutions", "involve", "pareto-optimal", "policies"]}, {"id": "term-multi-persona-prompting", "t": "Multi-Persona Prompting", "tg": ["Prompt Engineering", "Persona"], "d": "general", "x": "A technique that assigns multiple distinct expert personas within a single prompt, having each persona contribute their...", "l": "m", "k": ["multi-persona", "prompting", "technique", "assigns", "multiple", "distinct", "expert", "personas", "within", "single", "prompt", "having", "persona", "contribute", "specialized"]}, {"id": "term-multi-query-attention", "t": "Multi-Query Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention variant where all query heads share a single set of key and value projections, significantly reducing...", "l": "m", "k": ["multi-query", "attention", "variant", "query", "heads", "share", "single", "key", "value", "projections", "significantly", "reducing", "memory", "bandwidth", "requirements"]}, {"id": "term-multi-query-retrieval", "t": "Multi-Query Retrieval", "tg": ["Retrieval", "Query Processing"], "d": "general", "x": "A technique that generates multiple paraphrased or perspective-shifted versions of the original query using an LLM,...", "l": "m", "k": ["multi-query", "retrieval", "technique", "generates", "multiple", "paraphrased", "perspective-shifted", "versions", "original", "query", "llm", "retrieves", "documents", "variant", "combines"]}, {"id": "term-multi-scale-feature-extraction", "t": "Multi-Scale Feature Extraction", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The technique of capturing features at different spatial resolutions or receptive field sizes within a network,...", "l": "m", "k": ["multi-scale", "feature", "extraction", "technique", "capturing", "features", "different", "spatial", "resolutions", "receptive", "field", "sizes", "within", "network", "enabling"]}, {"id": "term-multi-scale-testing", "t": "Multi-Scale Testing", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "An evaluation technique that processes an image at multiple resolutions and combines the predictions, improving...", "l": "m", "k": ["multi-scale", "testing", "evaluation", "technique", "processes", "image", "multiple", "resolutions", "combines", "predictions", "improving", "detection", "objects", "various", "scales"]}, {"id": "term-multi-step-bootstrapping", "t": "Multi-Step Bootstrapping", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A value estimation approach that uses n actual rewards before bootstrapping with a value estimate for the remaining...", "l": "m", "k": ["multi-step", "bootstrapping", "value", "estimation", "approach", "uses", "actual", "rewards", "estimate", "remaining", "future", "interpolating", "one-step", "monte", "carlo"]}, {"id": "term-multi-step-reasoning", "t": "Multi-Step Reasoning", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting paradigm that breaks complex problems into a sequence of intermediate reasoning steps, requiring the model...", "l": "m", "k": ["multi-step", "reasoning", "prompting", "paradigm", "breaks", "complex", "problems", "sequence", "intermediate", "steps", "requiring", "model", "solve", "sub-problem", "proceeding"]}, {"id": "term-multi-task-learning", "t": "Multi-Task Learning", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A learning approach where a model is trained simultaneously on multiple related tasks, sharing representations across...", "l": "m", "k": ["multi-task", "learning", "approach", "model", "trained", "simultaneously", "multiple", "related", "tasks", "sharing", "representations", "across", "improve", "generalization", "leveraging"]}, {"id": "term-multi-task-rl", "t": "Multi-Task Reinforcement Learning", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "RL approaches that train a single policy to perform well across multiple related tasks simultaneously. Multi-task RL...", "l": "m", "k": ["multi-task", "reinforcement", "learning", "approaches", "train", "single", "policy", "perform", "across", "multiple", "related", "tasks", "simultaneously", "leverages", "shared"]}, {"id": "term-multi-tenancy-vector-databases", "t": "Multi-Tenancy in Vector Databases", "tg": ["Vector Database", "Infrastructure"], "d": "hardware", "x": "The ability of a vector database to serve multiple isolated users or applications from a shared infrastructure, using...", "l": "m", "k": ["multi-tenancy", "vector", "databases", "ability", "database", "serve", "multiple", "isolated", "users", "applications", "shared", "infrastructure", "namespaces", "partitions", "metadata"]}, {"id": "term-multi-turn-conversation", "t": "Multi-Turn Conversation", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A dialogue format where a language model maintains context across multiple exchanges with a user, requiring the model...", "l": "m", "k": ["multi-turn", "conversation", "dialogue", "format", "language", "model", "maintains", "context", "across", "multiple", "exchanges", "user", "requiring", "track", "history"]}, {"id": "term-multi-vector-retrieval", "t": "Multi-Vector Retrieval", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "A retrieval approach that represents each document as multiple embedding vectors rather than a single vector, capturing...", "l": "m", "k": ["multi-vector", "retrieval", "approach", "represents", "document", "multiple", "embedding", "vectors", "rather", "single", "vector", "capturing", "different", "aspects", "segments"]}, {"id": "term-multi-view-stereo", "t": "Multi-View Stereo", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A 3D reconstruction method that computes dense depth maps from multiple calibrated camera views, using photometric...", "l": "m", "k": ["multi-view", "stereo", "reconstruction", "method", "computes", "dense", "depth", "maps", "multiple", "calibrated", "camera", "views", "photometric", "consistency", "establish"]}, {"id": "term-multi-word-expression", "t": "Multi-Word Expression", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A combination of words that exhibits lexical, syntactic, semantic, or statistical idiosyncrasy, including idioms,...", "l": "m", "k": ["multi-word", "expression", "combination", "words", "exhibits", "lexical", "syntactic", "semantic", "statistical", "idiosyncrasy", "including", "idioms", "compound", "nouns", "phrasal"]}, {"id": "term-multicollinearity", "t": "Multicollinearity", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A condition in regression analysis where two or more independent variables are highly correlated, making it difficult...", "l": "m", "k": ["multicollinearity", "condition", "regression", "analysis", "independent", "variables", "highly", "correlated", "making", "difficult", "determine", "individual", "effect", "predictor", "inflating"]}, {"id": "term-multidimensional-scaling", "t": "Multidimensional Scaling", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A dimensionality reduction technique that positions points in low-dimensional space such that pairwise distances...", "l": "m", "k": ["multidimensional", "scaling", "dimensionality", "reduction", "technique", "positions", "points", "low-dimensional", "space", "pairwise", "distances", "approximate", "original", "high-dimensional", "visualization"]}, {"id": "term-multilingual-model", "t": "Multilingual Model", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A single model trained on data from multiple languages that can perform NLP tasks across those languages, often...", "l": "m", "k": ["multilingual", "model", "single", "trained", "data", "multiple", "languages", "perform", "nlp", "tasks", "across", "developing", "cross-lingual", "transfer", "abilities"]}, {"id": "term-multimodal", "t": "Multimodal", "tg": ["Capability", "Architecture"], "d": "models", "x": "AI systems that can process and generate multiple types of content (text, images, audio, video). Examples include...", "l": "m", "k": ["multimodal", "systems", "process", "generate", "multiple", "types", "content", "text", "images", "audio", "video", "examples", "include", "gpt-4v", "gemini"]}, {"id": "term-multimodal-ai-history", "t": "Multimodal AI History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of AI systems that can process and generate multiple types of data (text images audio video). From...", "l": "m", "k": ["multimodal", "history", "development", "systems", "process", "generate", "multiple", "types", "data", "text", "images", "audio", "video", "early", "separate"]}, {"id": "term-multinomial-distribution", "t": "Multinomial Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A generalization of the binomial distribution for experiments with more than two possible outcomes. It models the...", "l": "m", "k": ["multinomial", "distribution", "generalization", "binomial", "experiments", "possible", "outcomes", "models", "counts", "outcome", "across", "fixed", "number", "independent", "trials"]}, {"id": "term-multiple-testing-correction", "t": "Multiple Testing Correction", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "Statistical methods for adjusting significance thresholds when performing many simultaneous hypothesis tests to control...", "l": "m", "k": ["multiple", "testing", "correction", "statistical", "methods", "adjusting", "significance", "thresholds", "performing", "simultaneous", "hypothesis", "tests", "control", "overall", "error"]}, {"id": "term-musiclm", "t": "MusicLM", "tg": ["Models", "Technical"], "d": "models", "x": "A music generation model by Google that generates high-fidelity music from text descriptions. Uses a hierarchical...", "l": "m", "k": ["musiclm", "music", "generation", "model", "google", "generates", "high-fidelity", "text", "descriptions", "uses", "hierarchical", "sequence-to-sequence", "approach", "conditioned", "melody"]}, {"id": "term-mutual-information", "t": "Mutual Information", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A measure of the statistical dependence between two random variables, quantifying how much knowing one variable reduces...", "l": "m", "k": ["mutual", "information", "measure", "statistical", "dependence", "random", "variables", "quantifying", "knowing", "variable", "reduces", "uncertainty", "feature", "selection", "clustering"]}, {"id": "term-mutual-information-feature-selection", "t": "Mutual Information Feature Selection", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A filter-based feature selection method that ranks features by their mutual information with the target variable,...", "l": "m", "k": ["mutual", "information", "feature", "selection", "filter-based", "method", "ranks", "features", "target", "variable", "measuring", "reduction", "uncertainty", "provided", "knowing"]}, {"id": "term-muzero", "t": "MuZero", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "A model-based RL algorithm that learns a latent dynamics model, reward predictor, and value/policy networks without...", "l": "m", "k": ["muzero", "model-based", "algorithm", "learns", "latent", "dynamics", "model", "reward", "predictor", "value", "policy", "networks", "without", "requiring", "knowledge"]}, {"id": "term-mycin", "t": "MYCIN", "tg": ["History", "Milestones"], "d": "history", "x": "An early expert system developed at Stanford in the 1970s for diagnosing bacterial infections and recommending...", "l": "m", "k": ["mycin", "early", "expert", "system", "developed", "stanford", "1970s", "diagnosing", "bacterial", "infections", "recommending", "antibiotics", "demonstrating", "rule-based", "match"]}, {"id": "term-n-gram", "t": "N-gram", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A contiguous sequence of N items from a text, where items can be characters, words, or tokens, used in language models,...", "l": "n", "k": ["n-gram", "contiguous", "sequence", "items", "text", "characters", "words", "tokens", "language", "models", "classification", "information", "retrieval"]}, {"id": "term-n-step-return", "t": "N-Step Return", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A return estimate that uses n actual rewards before bootstrapping with a value estimate, interpolating between one-step...", "l": "n", "k": ["n-step", "return", "estimate", "uses", "actual", "rewards", "bootstrapping", "value", "interpolating", "one-step", "full", "monte", "carlo", "infinity", "returns"]}, {"id": "term-nadam", "t": "Nadam", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Nesterov-accelerated Adaptive Moment estimation combines the Adam optimizer with Nesterov momentum. Provides faster...", "l": "n", "k": ["nadam", "nesterov-accelerated", "adaptive", "moment", "estimation", "combines", "adam", "optimizer", "nesterov", "momentum", "provides", "faster", "convergence", "incorporating", "look-ahead"]}, {"id": "term-naive-bayes", "t": "Naive Bayes", "tg": ["Machine Learning", "Probability"], "d": "algorithms", "x": "A family of probabilistic classifiers based on Bayes' theorem with the strong assumption that features are...", "l": "n", "k": ["naive", "bayes", "family", "probabilistic", "classifiers", "based", "theorem", "strong", "assumption", "features", "conditionally", "independent", "given", "class", "label"]}, {"id": "term-naive-bayes-history", "t": "Naive Bayes History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The application of Bayes' theorem with naive independence assumptions to classification tasks. Despite its simplicity...", "l": "n", "k": ["naive", "bayes", "history", "application", "theorem", "independence", "assumptions", "classification", "tasks", "despite", "simplicity", "became", "practical", "machine", "learning"]}, {"id": "term-named-entity-linking", "t": "Named Entity Linking", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of mapping recognized named entity mentions in text to their corresponding entries in a knowledge base,...", "l": "n", "k": ["named", "entity", "linking", "task", "mapping", "recognized", "mentions", "text", "corresponding", "entries", "knowledge", "base", "resolving", "ambiguity", "multiple"]}, {"id": "term-named-entity-recognition", "t": "Named Entity Recognition (NER)", "tg": ["NLP Task", "Extraction"], "d": "general", "x": "An NLP task that identifies and classifies named entities (people, organizations, locations, dates) in text....", "l": "n", "k": ["named", "entity", "recognition", "ner", "nlp", "task", "identifies", "classifies", "entities", "people", "organizations", "locations", "dates", "text", "foundational"]}, {"id": "term-namespace", "t": "Namespace", "tg": ["Vector Database", "Infrastructure"], "d": "hardware", "x": "A logical partitioning mechanism within a vector database index that isolates vectors into separate searchable...", "l": "n", "k": ["namespace", "logical", "partitioning", "mechanism", "within", "vector", "database", "index", "isolates", "vectors", "separate", "searchable", "segments", "enabling", "multi-tenant"]}, {"id": "term-narrow-ai", "t": "Narrow AI (ANI)", "tg": ["Category", "Concept"], "d": "general", "x": "AI systems designed for specific tasks, like playing chess or generating text. All current AI is narrow, as opposed to...", "l": "n", "k": ["narrow", "ani", "systems", "designed", "specific", "tasks", "playing", "chess", "generating", "text", "current", "opposed", "hypothetical", "artificial", "general"]}, {"id": "term-narrow-ai-safety", "t": "Narrow AI Safety", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "Safety research focused on currently deployed AI systems, addressing issues such as robustness to distribution shift,...", "l": "n", "k": ["narrow", "safety", "research", "focused", "currently", "deployed", "systems", "addressing", "issues", "robustness", "distribution", "shift", "adversarial", "inputs", "reward"]}, {"id": "term-nasnet", "t": "NASNet", "tg": ["Models", "Technical"], "d": "models", "x": "Neural Architecture Search Network designed automatically by a reinforcement learning controller searching over a space...", "l": "n", "k": ["nasnet", "neural", "architecture", "search", "network", "designed", "automatically", "reinforcement", "learning", "controller", "searching", "space", "possible", "architectures", "demonstrated"]}, {"id": "term-nathaniel-rochester", "t": "Nathaniel Rochester", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist at IBM who co-organized the 1956 Dartmouth Conference that founded AI as a field. Rochester...", "l": "n", "k": ["nathaniel", "rochester", "american", "computer", "scientist", "ibm", "co-organized", "dartmouth", "conference", "founded", "field", "led", "development", "assembler", "worked"]}, {"id": "term-natural-language", "t": "Natural Language", "tg": ["Concept", "Interface"], "d": "general", "x": "Human language as we naturally speak and write it. AI assistants are designed to understand natural language, so you...", "l": "n", "k": ["natural", "language", "human", "naturally", "speak", "write", "assistants", "designed", "understand", "don", "need", "special", "syntax", "formatting"]}, {"id": "term-natural-language-inference", "t": "Natural Language Inference", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of classifying the logical relationship between a premise and hypothesis text pair into entailment,...", "l": "n", "k": ["natural", "language", "inference", "task", "classifying", "logical", "relationship", "premise", "hypothesis", "text", "pair", "entailment", "contradiction", "neutral", "testing"]}, {"id": "term-nlp", "t": "Natural Language Processing (NLP)", "tg": ["Field", "Language"], "d": "general", "x": "The field of AI focused on enabling computers to understand, interpret, and generate human language. Encompasses tasks...", "l": "n", "k": ["natural", "language", "processing", "nlp", "field", "focused", "enabling", "computers", "understand", "interpret", "generate", "human", "encompasses", "tasks", "translation"]}, {"id": "term-nlp-history", "t": "Natural Language Processing History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of NLP from rule-based approaches in the 1960s through statistical methods in the 1990s to neural...", "l": "n", "k": ["natural", "language", "processing", "history", "evolution", "nlp", "rule-based", "approaches", "1960s", "statistical", "methods", "1990s", "neural", "2010s", "transformer"]}, {"id": "term-natural-language-understanding-history", "t": "Natural Language Understanding History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of machine understanding of human language from early pattern matching (ELIZA 1966) and microworld...", "l": "n", "k": ["natural", "language", "understanding", "history", "evolution", "machine", "human", "early", "pattern", "matching", "eliza", "microworld", "systems", "shrdlu", "statistical"]}, {"id": "term-natural-policy-gradient", "t": "Natural Policy Gradient", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A policy gradient method that preconditions updates with the inverse Fisher information matrix, following the steepest...", "l": "n", "k": ["natural", "policy", "gradient", "method", "preconditions", "updates", "inverse", "fisher", "information", "matrix", "following", "steepest", "ascent", "direction", "space"]}, {"id": "term-natural-questions", "t": "Natural Questions", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A question answering benchmark by Google consisting of real user queries from Google Search paired with Wikipedia...", "l": "n", "k": ["natural", "questions", "question", "answering", "benchmark", "google", "consisting", "real", "user", "queries", "search", "paired", "wikipedia", "articles", "requiring"]}, {"id": "term-nccl", "t": "NCCL", "tg": ["Distributed Computing", "GPU"], "d": "hardware", "x": "NVIDIA Collective Communications Library, a highly optimized library for multi-GPU and multi-node collective...", "l": "n", "k": ["nccl", "nvidia", "collective", "communications", "library", "highly", "optimized", "multi-gpu", "multi-node", "communication", "operations", "automatically", "selects", "best", "algorithms"]}, {"id": "term-ndcg", "t": "NDCG", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "Normalized Discounted Cumulative Gain, a ranking quality metric that evaluates the usefulness of retrieved items based...", "l": "n", "k": ["ndcg", "normalized", "discounted", "cumulative", "gain", "ranking", "quality", "metric", "evaluates", "usefulness", "retrieved", "items", "based", "position", "result"]}, {"id": "term-neats-vs-scruffies", "t": "Neats vs Scruffies", "tg": ["History", "Fundamentals"], "d": "history", "x": "A characterization of a philosophical divide in AI research during the 1970s and 1980s. Neats favored formal...", "l": "n", "k": ["neats", "scruffies", "characterization", "philosophical", "divide", "research", "1970s", "1980s", "favored", "formal", "mathematical", "approaches", "provably", "correct", "algorithms"]}, {"id": "term-needle-in-haystack-test", "t": "Needle in a Haystack Test", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An evaluation method that measures a language model's ability to retrieve a specific piece of information placed at...", "l": "n", "k": ["needle", "haystack", "test", "evaluation", "method", "measures", "language", "model", "ability", "retrieve", "specific", "piece", "information", "placed", "various"]}, {"id": "term-negation-detection", "t": "Negation Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying negation cues and their scope in text, determining which parts of a sentence are affected by...", "l": "n", "k": ["negation", "detection", "task", "identifying", "cues", "scope", "text", "determining", "parts", "sentence", "affected", "words", "never", "without"]}, {"id": "term-negative-prompt", "t": "Negative Prompt", "tg": ["Prompting", "Technique"], "d": "general", "x": "Instructions telling AI what to avoid in its output. Common in image generation (\"no blur, no distortion\") and can be...", "l": "n", "k": ["negative", "prompt", "instructions", "telling", "avoid", "output", "common", "image", "generation", "blur", "distortion", "text", "exclude", "certain", "topics"]}, {"id": "term-negative-prompting", "t": "Negative Prompting", "tg": ["Prompt Engineering", "Constraints"], "d": "general", "x": "A technique that explicitly specifies what the model should avoid in its output, including unwanted content, styles,...", "l": "n", "k": ["negative", "prompting", "technique", "explicitly", "specifies", "model", "avoid", "output", "including", "unwanted", "content", "styles", "formats", "behaviors", "exclusion"]}, {"id": "term-negative-sampling", "t": "Negative Sampling", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A training approximation that replaces the full softmax over the vocabulary with a binary classification between true...", "l": "n", "k": ["negative", "sampling", "training", "approximation", "replaces", "full", "softmax", "vocabulary", "binary", "classification", "true", "context", "words", "randomly", "sampled"]}, {"id": "term-nemotron", "t": "Nemotron", "tg": ["Models", "Technical"], "d": "models", "x": "A family of language models developed by NVIDIA for enterprise AI applications. Features models optimized for...", "l": "n", "k": ["nemotron", "family", "language", "models", "developed", "nvidia", "enterprise", "applications", "features", "optimized", "instruction", "following", "code", "generation", "synthetic"]}, {"id": "term-nerf", "t": "NeRF", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Neural Radiance Field, a method that represents 3D scenes as continuous volumetric functions parameterized by neural...", "l": "n", "k": ["nerf", "neural", "radiance", "field", "method", "represents", "scenes", "continuous", "volumetric", "functions", "parameterized", "networks", "enabling", "photorealistic", "novel"]}, {"id": "term-nested-cross-validation", "t": "Nested Cross-Validation", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A model evaluation technique using an inner cross-validation loop for hyperparameter tuning and an outer loop for...", "l": "n", "k": ["nested", "cross-validation", "model", "evaluation", "technique", "inner", "loop", "hyperparameter", "tuning", "outer", "unbiased", "performance", "estimation", "preventing", "information"]}, {"id": "term-nested-ner", "t": "Nested Named Entity Recognition", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A NER variant that handles entities embedded within other entities, such as recognizing both 'Bank of America' as an...", "l": "n", "k": ["nested", "named", "entity", "recognition", "ner", "variant", "handles", "entities", "embedded", "within", "recognizing", "bank", "america", "organization", "location"]}, {"id": "term-nesterov-accelerated-gradient", "t": "Nesterov Accelerated Gradient", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A variant of momentum-based optimization that computes the gradient at a lookahead position rather than the current...", "l": "n", "k": ["nesterov", "accelerated", "gradient", "variant", "momentum-based", "optimization", "computes", "lookahead", "position", "rather", "current", "providing", "better", "convergence", "properties"]}, {"id": "term-netflix-prize", "t": "Netflix Prize", "tg": ["History", "Milestones"], "d": "history", "x": "A 2006-2009 open competition offering one million dollars for the best collaborative filtering algorithm to predict...", "l": "n", "k": ["netflix", "prize", "2006-2009", "open", "competition", "offering", "million", "dollars", "best", "collaborative", "filtering", "algorithm", "predict", "user", "movie"]}, {"id": "term-neural-architecture-search", "t": "Neural Architecture Search", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An automated method for designing neural network architectures by searching over a defined search space. Methods...", "l": "n", "k": ["neural", "architecture", "search", "automated", "method", "designing", "network", "architectures", "searching", "defined", "space", "methods", "include", "reinforcement", "learning"]}, {"id": "term-nas-vision", "t": "Neural Architecture Search for Vision", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Automated methods for discovering optimal CNN or ViT architectures by searching over design choices (kernel sizes,...", "l": "n", "k": ["neural", "architecture", "search", "vision", "automated", "methods", "discovering", "optimal", "cnn", "vit", "architectures", "searching", "design", "choices", "kernel"]}, {"id": "term-hw-aware-nas", "t": "Neural Architecture Search Hardware-Aware", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "NAS methods that incorporate hardware constraints (latency, memory, power) into the search objective, finding...", "l": "n", "k": ["neural", "architecture", "search", "hardware-aware", "nas", "methods", "incorporate", "hardware", "constraints", "latency", "memory", "power", "objective", "finding", "architectures"]}, {"id": "term-neural-machine-translation", "t": "Neural Machine Translation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A machine translation approach using encoder-decoder neural networks that learn to map source language sequences to...", "l": "n", "k": ["neural", "machine", "translation", "approach", "encoder-decoder", "networks", "learn", "map", "source", "language", "sequences", "target", "end-to-end", "parallel", "corpora"]}, {"id": "term-neural-network", "t": "Neural Network", "tg": ["Architecture", "Fundamentals"], "d": "models", "x": "A computing system inspired by biological brains, composed of interconnected nodes (neurons) organized in layers. The...", "l": "n", "k": ["neural", "network", "computing", "system", "inspired", "biological", "brains", "composed", "interconnected", "nodes", "neurons", "organized", "layers", "foundation", "modern"]}, {"id": "term-neural-network-renaissance", "t": "Neural Network Renaissance", "tg": ["History", "Milestones"], "d": "history", "x": "The revival of interest in neural networks in the 2000s and 2010s driven by the work of Geoffrey Hinton and others on...", "l": "n", "k": ["neural", "network", "renaissance", "revival", "interest", "networks", "2000s", "2010s", "driven", "work", "geoffrey", "hinton", "others", "deep", "belief"]}, {"id": "term-neural-ode", "t": "Neural ODE", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A continuous-depth neural network that parameterizes the derivative of hidden states as a neural network and uses ODE...", "l": "n", "k": ["neural", "ode", "continuous-depth", "network", "parameterizes", "derivative", "hidden", "states", "uses", "solvers", "forward", "backward", "passes", "enabling", "adaptive"]}, {"id": "term-neural-ordinary-differential-equations", "t": "Neural Ordinary Differential Equations", "tg": ["History", "Milestones"], "d": "history", "x": "A class of deep learning models introduced by Chen et al. in 2018 that parameterize the derivative of the hidden state...", "l": "n", "k": ["neural", "ordinary", "differential", "equations", "class", "deep", "learning", "models", "introduced", "chen", "parameterize", "derivative", "hidden", "state", "network"]}, {"id": "term-npu", "t": "Neural Processing Unit (NPU)", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "A dedicated hardware accelerator designed specifically for neural network inference, typically integrated into SoCs for...", "l": "n", "k": ["neural", "processing", "unit", "npu", "dedicated", "hardware", "accelerator", "designed", "specifically", "network", "inference", "typically", "integrated", "socs", "on-device"]}, {"id": "term-neural-style-transfer", "t": "Neural Style Transfer", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A technique that applies the artistic style of one image to the content of another by optimizing a generated image to...", "l": "n", "k": ["neural", "style", "transfer", "technique", "applies", "artistic", "image", "content", "another", "optimizing", "generated", "match", "features", "source", "gram"]}, {"id": "term-neural-turing-machine", "t": "Neural Turing Machine", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A neural architecture augmented with external memory that the network can read from and write to via differentiable...", "l": "n", "k": ["neural", "turing", "machine", "architecture", "augmented", "external", "memory", "network", "read", "write", "via", "differentiable", "attention", "mechanisms", "enabling"]}, {"id": "term-neurips", "t": "NeurIPS", "tg": ["History", "Conferences"], "d": "history", "x": "The Conference on Neural Information Processing Systems originally founded in 1987 as NIPS. One of the most prestigious...", "l": "n", "k": ["neurips", "conference", "neural", "information", "processing", "systems", "originally", "founded", "nips", "prestigious", "influential", "conferences", "machine", "learning", "artificial"]}, {"id": "term-newtons-method", "t": "Newton's Method", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A second-order optimization algorithm that uses the Hessian matrix to find the minimum of a function. Converges...", "l": "n", "k": ["newton", "method", "second-order", "optimization", "algorithm", "uses", "hessian", "matrix", "find", "minimum", "function", "converges", "quadratically", "near", "optimum"]}, {"id": "term-next-token-prediction", "t": "Next Token Prediction", "tg": ["Training", "LLM"], "d": "models", "x": "The core training objective of autoregressive LLMs: predict the next token given all previous tokens. This simple...", "l": "n", "k": ["next", "token", "prediction", "core", "training", "objective", "autoregressive", "llms", "predict", "given", "previous", "tokens", "simple", "scale", "produces"]}, {"id": "term-nf4", "t": "NF4 (Normal Float 4-bit)", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A 4-bit quantization format based on the assumption that neural network weights follow a normal distribution, using...", "l": "n", "k": ["nf4", "normal", "float", "4-bit", "quantization", "format", "based", "assumption", "neural", "network", "weights", "follow", "distribution", "quantile", "optimal"]}, {"id": "term-niklaus-wirth", "t": "Niklaus Wirth", "tg": ["History", "Pioneers"], "d": "history", "x": "Swiss computer scientist who designed several influential programming languages including Pascal (1970) and Modula-2....", "l": "n", "k": ["niklaus", "wirth", "swiss", "computer", "scientist", "designed", "several", "influential", "programming", "languages", "including", "pascal", "modula-2", "winner", "turing"]}, {"id": "term-nils-nilsson", "t": "Nils Nilsson", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1933-2019) who co-invented the A* search algorithm and developed foundational work in...", "l": "n", "k": ["nils", "nilsson", "american", "computer", "scientist", "1933-2019", "co-invented", "search", "algorithm", "developed", "foundational", "work", "robotics", "knowledge", "representation"]}, {"id": "term-nist-ai-rmf", "t": "NIST AI Risk Management Framework", "tg": ["Governance", "Regulation"], "d": "safety", "x": "A voluntary framework published by the US National Institute of Standards and Technology in 2023 that provides guidance...", "l": "n", "k": ["nist", "risk", "management", "framework", "voluntary", "published", "national", "institute", "standards", "technology", "provides", "guidance", "managing", "risks", "governance"]}, {"id": "term-nli-based-evaluation", "t": "NLI-Based Evaluation", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation approach that uses Natural Language Inference models to assess text quality by classifying whether...", "l": "n", "k": ["nli-based", "evaluation", "approach", "uses", "natural", "language", "inference", "models", "assess", "text", "quality", "classifying", "generated", "claims", "entailed"]}, {"id": "term-no-free-lunch-theorem", "t": "No Free Lunch Theorem", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A set of theoretical results stating that no single learning algorithm performs best across all possible problems. When...", "l": "n", "k": ["free", "lunch", "theorem", "theoretical", "results", "stating", "single", "learning", "algorithm", "performs", "best", "across", "possible", "problems", "averaged"]}, {"id": "term-no-free-lunch-theorems", "t": "No Free Lunch Theorems", "tg": ["History", "Fundamentals"], "d": "history", "x": "Theorems published by David Wolpert and William Macready in 1997 proving that no optimization algorithm is universally...", "l": "n", "k": ["free", "lunch", "theorems", "published", "david", "wolpert", "william", "macready", "proving", "optimization", "algorithm", "universally", "superior", "across", "possible"]}, {"id": "term-nobel-prize-for-ai", "t": "Nobel Prize for AI", "tg": ["History", "Milestones"], "d": "history", "x": "Recognition of AI contributions through Nobel Prizes including the 2024 Nobel Prize in Physics awarded to John Hopfield...", "l": "n", "k": ["nobel", "prize", "recognition", "contributions", "prizes", "including", "physics", "awarded", "john", "hopfield", "geoffrey", "hinton", "foundational", "discoveries", "enabling"]}, {"id": "term-node2vec", "t": "Node2Vec", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A graph embedding algorithm that learns continuous feature representations for nodes by optimizing a...", "l": "n", "k": ["node2vec", "graph", "embedding", "algorithm", "learns", "continuous", "feature", "representations", "nodes", "optimizing", "neighborhood-preserving", "objective", "uses", "biased", "random"]}, {"id": "term-noel-sharkey", "t": "Noel Sharkey", "tg": ["History", "Pioneers"], "d": "history", "x": "British AI and robotics professor at the University of Sheffield known for public engagement with AI ethics...", "l": "n", "k": ["noel", "sharkey", "british", "robotics", "professor", "university", "sheffield", "known", "public", "engagement", "ethics", "particularly", "regarding", "autonomous", "weapons"]}, {"id": "term-noise", "t": "Noise (ML)", "tg": ["Concept", "Data"], "d": "general", "x": "Random variation in data or model outputs. In training, noise can cause or hide patterns. In diffusion models,...", "l": "n", "k": ["noise", "random", "variation", "data", "model", "outputs", "training", "cause", "hide", "patterns", "diffusion", "models", "controlled", "addition", "removal"]}, {"id": "term-noise-schedule", "t": "Noise Schedule", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The predefined or learned sequence of noise levels in diffusion models that determines how quickly noise is added...", "l": "n", "k": ["noise", "schedule", "predefined", "learned", "sequence", "levels", "diffusion", "models", "determines", "quickly", "added", "forward", "process", "removed", "reverse"]}, {"id": "term-noisy-networks", "t": "Noisy Networks", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "A DQN extension that replaces epsilon-greedy exploration with parametric noise added to network weights, allowing the...", "l": "n", "k": ["noisy", "networks", "dqn", "extension", "replaces", "epsilon-greedy", "exploration", "parametric", "noise", "added", "network", "weights", "allowing", "agent", "learn"]}, {"id": "term-nomic-embed", "t": "Nomic Embed", "tg": ["Models", "Technical"], "d": "models", "x": "An open-source long-context text embedding model that processes up to 8192 tokens. Fully reproducible with open...", "l": "n", "k": ["nomic", "embed", "open-source", "long-context", "text", "embedding", "model", "processes", "tokens", "fully", "reproducible", "open", "training", "data", "code"]}, {"id": "term-non-local-neural-network", "t": "Non-Local Neural Network", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A neural network module that computes the response at a position as a weighted sum of features at all positions,...", "l": "n", "k": ["non-local", "neural", "network", "module", "computes", "response", "position", "weighted", "sum", "features", "positions", "capturing", "long-range", "dependencies", "images"]}, {"id": "term-non-maximum-suppression", "t": "Non-Maximum Suppression", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A post-processing algorithm in object detection that eliminates redundant overlapping bounding box predictions by...", "l": "n", "k": ["non-maximum", "suppression", "post-processing", "algorithm", "object", "detection", "eliminates", "redundant", "overlapping", "bounding", "box", "predictions", "keeping", "highest-confidence", "instance"]}, {"id": "term-non-negative-matrix-factorization", "t": "Non-Negative Matrix Factorization", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "A matrix decomposition technique that factors a non-negative matrix into two non-negative matrices, producing...", "l": "n", "k": ["non-negative", "matrix", "factorization", "decomposition", "technique", "factors", "matrices", "producing", "parts-based", "representations", "useful", "topic", "modeling", "image", "analysis"]}, {"id": "term-nonmonotonic-reasoning", "t": "Nonmonotonic Reasoning", "tg": ["History", "Fundamentals"], "d": "history", "x": "A form of logical reasoning where the addition of new information can invalidate previously derived conclusions....", "l": "n", "k": ["nonmonotonic", "reasoning", "form", "logical", "addition", "information", "invalidate", "previously", "derived", "conclusions", "developed", "1980s", "researchers", "including", "raymond"]}, {"id": "term-norbert-wiener", "t": "Norbert Wiener", "tg": ["History", "Pioneers"], "d": "history", "x": "American mathematician (1894-1964) who founded cybernetics in his 1948 book of the same name, establishing the study of...", "l": "n", "k": ["norbert", "wiener", "american", "mathematician", "1894-1964", "founded", "cybernetics", "book", "name", "establishing", "study", "feedback", "control", "communication", "machines"]}, {"id": "term-norbert-wiener-cybernetics-book", "t": "Norbert Wiener Cybernetics Book", "tg": ["History", "Milestones"], "d": "history", "x": "The 1948 book Cybernetics: Or Control and Communication in the Animal and the Machine by Norbert Wiener. This...", "l": "n", "k": ["norbert", "wiener", "cybernetics", "book", "control", "communication", "animal", "machine", "foundational", "work", "established", "field", "introduced", "concepts", "feedback"]}, {"id": "term-normal-distribution", "t": "Normal Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution characterized by its bell-shaped curve, symmetric about the mean, fully...", "l": "n", "k": ["normal", "distribution", "continuous", "probability", "characterized", "bell-shaped", "curve", "symmetric", "mean", "fully", "determined", "standard", "deviation", "natural", "phenomena"]}, {"id": "term-normality-test", "t": "Normality Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A statistical test that evaluates whether a dataset follows a normal distribution. Common methods include the...", "l": "n", "k": ["normality", "test", "statistical", "evaluates", "dataset", "follows", "normal", "distribution", "common", "methods", "include", "shapiro-wilk", "kolmogorov-smirnov", "anderson-darling"]}, {"id": "term-normalization", "t": "Normalization", "tg": ["Technique", "Training"], "d": "general", "x": "Techniques to standardize inputs or layer outputs in neural networks. Layer normalization is critical in transformers...", "l": "n", "k": ["normalization", "techniques", "standardize", "inputs", "layer", "outputs", "neural", "networks", "critical", "transformers", "stable", "training", "better", "generalization"]}, {"id": "term-normalized-mutual-information", "t": "Normalized Mutual Information", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A clustering evaluation metric that normalizes mutual information between predicted and ground truth clusterings to...", "l": "n", "k": ["normalized", "mutual", "information", "clustering", "evaluation", "metric", "normalizes", "predicted", "ground", "truth", "clusterings", "account", "chance", "ranges", "useful"]}, {"id": "term-normalizing-flow", "t": "Normalizing Flow", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative model that transforms a simple base distribution into a complex target distribution through a sequence of...", "l": "n", "k": ["normalizing", "flow", "generative", "model", "transforms", "simple", "base", "distribution", "complex", "target", "sequence", "invertible", "differentiable", "transformations", "tractable"]}, {"id": "term-normalizing-flows", "t": "Normalizing Flows", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A class of generative models that transform a simple base distribution into a complex target distribution through a...", "l": "n", "k": ["normalizing", "flows", "class", "generative", "models", "transform", "simple", "base", "distribution", "complex", "target", "sequence", "invertible", "differentiable", "mappings"]}, {"id": "term-novel-view-synthesis", "t": "Novel View Synthesis", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The task of generating photorealistic images of a scene from viewpoints not present in the input photographs, using...", "l": "n", "k": ["novel", "view", "synthesis", "task", "generating", "photorealistic", "images", "scene", "viewpoints", "present", "input", "photographs", "techniques", "nerf", "gaussian"]}, {"id": "term-nt-xent-loss", "t": "NT-Xent Loss", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Normalized Temperature-scaled Cross-Entropy loss used in SimCLR for self-supervised contrastive learning. Normalizes...", "l": "n", "k": ["nt-xent", "loss", "normalized", "temperature-scaled", "cross-entropy", "simclr", "self-supervised", "contrastive", "learning", "normalizes", "embeddings", "scales", "temperature", "parameter", "computing"]}, {"id": "term-ntk-aware-scaling", "t": "NTK-Aware Scaling", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A position encoding extension method based on Neural Tangent Kernel theory that modifies the frequency basis of rotary...", "l": "n", "k": ["ntk-aware", "scaling", "position", "encoding", "extension", "method", "based", "neural", "tangent", "kernel", "theory", "modifies", "frequency", "basis", "rotary"]}, {"id": "term-nucleus-sampling", "t": "Nucleus Sampling (Top-p)", "tg": ["Generation", "Parameter"], "d": "general", "x": "A text generation strategy that samples from the smallest set of tokens whose cumulative probability exceeds p....", "l": "n", "k": ["nucleus", "sampling", "top-p", "text", "generation", "strategy", "samples", "smallest", "tokens", "whose", "cumulative", "probability", "exceeds", "balances", "diversity"]}, {"id": "term-null-hypothesis", "t": "Null Hypothesis", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A default assumption in statistical hypothesis testing that there is no effect or no difference between groups....", "l": "n", "k": ["null", "hypothesis", "default", "assumption", "statistical", "testing", "effect", "difference", "groups", "tests", "evaluate", "observed", "data", "provide", "sufficient"]}, {"id": "term-numerical-differentiation", "t": "Numerical Differentiation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Approximating derivatives using finite differences. Simple to implement but subject to numerical errors from truncation...", "l": "n", "k": ["numerical", "differentiation", "approximating", "derivatives", "finite", "differences", "simple", "implement", "subject", "errors", "truncation", "rounding", "verification", "tool", "gradient"]}, {"id": "term-a100", "t": "NVIDIA A100", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "NVIDIA's third-generation Tensor Core GPU based on the Ampere architecture, featuring 80GB HBM2e memory, support for...", "l": "n", "k": ["nvidia", "a100", "third-generation", "tensor", "core", "gpu", "based", "ampere", "architecture", "featuring", "80gb", "hbm2e", "memory", "support", "tf32"]}, {"id": "term-nvidia-ai-dominance", "t": "NVIDIA AI Dominance", "tg": ["History", "Milestones"], "d": "history", "x": "NVIDIA's emergence as the dominant provider of AI computing hardware through its GPU technology. From gaming graphics...", "l": "n", "k": ["nvidia", "dominance", "emergence", "dominant", "provider", "computing", "hardware", "gpu", "technology", "gaming", "graphics", "training", "cuda", "platform", "a100"]}, {"id": "term-b200", "t": "NVIDIA B200", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "NVIDIA's Blackwell architecture GPU designed for next-generation AI training and inference, featuring second-generation...", "l": "n", "k": ["nvidia", "b200", "blackwell", "architecture", "gpu", "designed", "next-generation", "training", "inference", "featuring", "second-generation", "transformer", "engine", "fp4", "support"]}, {"id": "term-nvidia-grace", "t": "NVIDIA Grace CPU", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "NVIDIA's ARM-based data center CPU designed for AI and HPC workloads, featuring high memory bandwidth via LPDDR5X and...", "l": "n", "k": ["nvidia", "grace", "cpu", "arm-based", "data", "center", "designed", "hpc", "workloads", "featuring", "high", "memory", "bandwidth", "via", "lpddr5x"]}, {"id": "term-h100", "t": "NVIDIA H100", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "NVIDIA's fourth-generation Tensor Core GPU based on the Hopper architecture, featuring the Transformer Engine with FP8...", "l": "n", "k": ["nvidia", "h100", "fourth-generation", "tensor", "core", "gpu", "based", "hopper", "architecture", "featuring", "transformer", "engine", "fp8", "precision", "80gb"]}, {"id": "term-nvlink", "t": "NVLink", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "NVIDIA's proprietary high-bandwidth, low-latency interconnect for direct GPU-to-GPU communication, bypassing the PCIe...", "l": "n", "k": ["nvlink", "nvidia", "proprietary", "high-bandwidth", "low-latency", "interconnect", "direct", "gpu-to-gpu", "communication", "bypassing", "pcie", "bus", "hopper", "provides", "total"]}, {"id": "term-nvswitch", "t": "NVSwitch", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "NVIDIA's fully connected switch fabric that enables all-to-all GPU communication within a node at full NVLink...", "l": "n", "k": ["nvswitch", "nvidia", "fully", "connected", "switch", "fabric", "enables", "all-to-all", "gpu", "communication", "within", "node", "full", "nvlink", "bandwidth"]}, {"id": "term-o1", "t": "o1", "tg": ["Models", "Technical"], "d": "models", "x": "An OpenAI reasoning model that uses chain-of-thought processing to solve complex problems. Spends additional compute...", "l": "o", "k": ["openai", "reasoning", "model", "uses", "chain-of-thought", "processing", "solve", "complex", "problems", "spends", "additional", "compute", "time", "thinking", "responding"]}, {"id": "term-o3", "t": "o3", "tg": ["Models", "Technical"], "d": "models", "x": "An advanced reasoning model from OpenAI that extends o1 capabilities with improved performance on challenging...", "l": "o", "k": ["advanced", "reasoning", "model", "openai", "extends", "capabilities", "improved", "performance", "challenging", "benchmarks", "demonstrates", "stronger", "across", "mathematics", "coding"]}, {"id": "term-object-tracking", "t": "Object Tracking", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of following one or more objects across video frames by maintaining consistent identity assignments, using...", "l": "o", "k": ["object", "tracking", "task", "following", "objects", "across", "video", "frames", "maintaining", "consistent", "identity", "assignments", "methods", "combine", "detection"]}, {"id": "term-observation-normalization", "t": "Observation Normalization", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The technique of normalizing input observations to zero mean and unit variance using running statistics, improving...", "l": "o", "k": ["observation", "normalization", "technique", "normalizing", "input", "observations", "zero", "mean", "unit", "variance", "running", "statistics", "improving", "neural", "network"]}, {"id": "term-observation-space", "t": "Observation Space", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The specification of the format and bounds of observations that an RL agent receives from the environment, including...", "l": "o", "k": ["observation", "space", "specification", "format", "bounds", "observations", "agent", "receives", "environment", "including", "data", "type", "shape", "valid", "ranges"]}, {"id": "term-occams-razor", "t": "Occam's Razor", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A principle favoring simpler models over more complex ones when both explain the data equally well. In machine...", "l": "o", "k": ["occam", "razor", "principle", "favoring", "simpler", "models", "complex", "ones", "explain", "data", "equally", "machine", "learning", "motivates", "regularization"]}, {"id": "term-occams-razor-in-ml", "t": "Occam's Razor in ML", "tg": ["History", "Fundamentals"], "d": "history", "x": "The application of the principle of parsimony to machine learning: simpler models that explain the data well are...", "l": "o", "k": ["occam", "razor", "application", "principle", "parsimony", "machine", "learning", "simpler", "models", "explain", "data", "preferred", "complex", "ones", "underlies"]}, {"id": "term-occupancy-network", "t": "Occupancy Network", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A neural network that predicts whether each point in 3D space is occupied or empty, representing 3D shapes as...", "l": "o", "k": ["occupancy", "network", "neural", "predicts", "point", "space", "occupied", "empty", "representing", "shapes", "continuous", "implicit", "functions", "rather", "discrete"]}, {"id": "term-oecd-ai-principles", "t": "OECD AI Principles", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Principles adopted by OECD member countries in 2019 promoting AI that is innovative, trustworthy, and respects human...", "l": "o", "k": ["oecd", "principles", "adopted", "member", "countries", "promoting", "innovative", "trustworthy", "respects", "human", "rights", "including", "recommendations", "transparency", "robustness"]}, {"id": "term-offensive-language-detection", "t": "Offensive Language Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of classifying text as offensive, abusive, or inappropriate, distinguishing between targeted insults,...", "l": "o", "k": ["offensive", "language", "detection", "task", "classifying", "text", "abusive", "inappropriate", "distinguishing", "targeted", "insults", "profanity", "general", "content", "moderation"]}, {"id": "term-offline-rl", "t": "Offline Reinforcement Learning", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "An RL paradigm that learns policies entirely from a fixed dataset of previously collected experience without further...", "l": "o", "k": ["offline", "reinforcement", "learning", "paradigm", "learns", "policies", "entirely", "fixed", "dataset", "previously", "collected", "experience", "without", "environment", "interaction"]}, {"id": "term-offloading", "t": "Offloading (CPU/Disk)", "tg": ["Model Optimization", "Distributed Computing"], "d": "models", "x": "A technique that stores model parameters, optimizer states, or activations in CPU RAM or disk when GPU memory is...", "l": "o", "k": ["offloading", "cpu", "disk", "technique", "stores", "model", "parameters", "optimizer", "states", "activations", "ram", "gpu", "memory", "insufficient", "transferring"]}, {"id": "term-oliver-selfridge", "t": "Oliver Selfridge", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1926-2008) who created the Pandemonium model for pattern recognition in 1959 and made...", "l": "o", "k": ["oliver", "selfridge", "american", "computer", "scientist", "1926-2008", "created", "pandemonium", "model", "pattern", "recognition", "early", "contributions", "machine", "learning"]}, {"id": "term-ollama", "t": "Ollama", "tg": ["Tools", "Local AI"], "d": "general", "x": "A tool for running LLMs locally on personal computers. Simplifies downloading and running open-source models like...", "l": "o", "k": ["ollama", "tool", "running", "llms", "locally", "personal", "computers", "simplifies", "downloading", "open-source", "models", "llama", "enabling", "private", "offline"]}, {"id": "term-on-policy-vs-off-policy", "t": "On-Policy vs Off-Policy", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A distinction between RL methods that learn about the policy currently being executed (on-policy, e.g., SARSA) and...", "l": "o", "k": ["on-policy", "off-policy", "distinction", "methods", "learn", "policy", "currently", "executed", "sarsa", "different", "target", "data", "behavior", "q-learning", "enable"]}, {"id": "term-one-class-svm", "t": "One-Class SVM", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A variant of support vector machines for anomaly detection that learns a decision boundary enclosing the normal data....", "l": "o", "k": ["one-class", "svm", "variant", "support", "vector", "machines", "anomaly", "detection", "learns", "decision", "boundary", "enclosing", "normal", "data", "maps"]}, {"id": "term-one-cycle-policy", "t": "One-Cycle Policy", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A learning rate schedule that uses a single cycle of increasing then decreasing learning rate combined with inverse...", "l": "o", "k": ["one-cycle", "policy", "learning", "rate", "schedule", "uses", "single", "cycle", "increasing", "decreasing", "combined", "inverse", "momentum", "scheduling", "proposed"]}, {"id": "term-one-hot-encoding", "t": "One-Hot Encoding", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A feature encoding technique that converts each categorical value into a binary vector with a single 1 at the position...", "l": "o", "k": ["one-hot", "encoding", "feature", "technique", "converts", "categorical", "value", "binary", "vector", "single", "position", "corresponding", "category", "elsewhere", "creating"]}, {"id": "term-one-shot", "t": "One-Shot Learning", "tg": ["Prompting", "Technique"], "d": "general", "x": "Providing a single example in your prompt to demonstrate the desired output format or style. Falls between zero-shot...", "l": "o", "k": ["one-shot", "learning", "providing", "single", "example", "prompt", "demonstrate", "desired", "output", "format", "style", "falls", "zero-shot", "examples", "few-shot"]}, {"id": "term-one-vs-rest-classification", "t": "One-Vs-Rest Classification", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A strategy for extending binary classifiers to multi-class problems by training one classifier per class, treating that...", "l": "o", "k": ["one-vs-rest", "classification", "strategy", "extending", "binary", "classifiers", "multi-class", "problems", "training", "classifier", "per", "class", "treating", "positive", "others"]}, {"id": "term-online-learning", "t": "Online Learning", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A learning paradigm where the model is updated incrementally as each new data point arrives, rather than being trained...", "l": "o", "k": ["online", "learning", "paradigm", "model", "updated", "incrementally", "data", "point", "arrives", "rather", "trained", "fixed", "batch", "suitable", "streaming"]}, {"id": "term-online-learning-algorithm", "t": "Online Learning Algorithm", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A learning paradigm where the model updates incrementally as new data arrives one example at a time. Suitable for...", "l": "o", "k": ["online", "learning", "algorithm", "paradigm", "model", "updates", "incrementally", "data", "arrives", "example", "time", "suitable", "streaming", "non-stationary", "environments"]}, {"id": "term-onnx", "t": "ONNX (Open Neural Network Exchange)", "tg": ["Format", "Interoperability"], "d": "general", "x": "An open format for representing ML models, enabling interoperability between different frameworks. Allows models...", "l": "o", "k": ["onnx", "open", "neural", "network", "exchange", "format", "representing", "models", "enabling", "interoperability", "different", "frameworks", "allows", "trained", "pytorch"]}, {"id": "term-onnx-runtime", "t": "ONNX Runtime", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Microsoft's cross-platform inference engine that executes models in the Open Neural Network Exchange format with...", "l": "o", "k": ["onnx", "runtime", "microsoft", "cross-platform", "inference", "engine", "executes", "models", "open", "neural", "network", "exchange", "format", "hardware-specific", "optimizations"]}, {"id": "term-ontology-in-ai", "t": "Ontology in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "A formal representation of knowledge as a set of concepts within a domain and the relationships between those concepts....", "l": "o", "k": ["ontology", "formal", "representation", "knowledge", "concepts", "within", "domain", "relationships", "ontologies", "cyc", "gene", "provide", "structured", "vocabularies", "enable"]}, {"id": "term-open-source", "t": "Open Source / Open Weight", "tg": ["Licensing", "Access"], "d": "general", "x": "AI models with publicly available weights that can be downloaded and run locally. Ranges from fully open (Llama) to...", "l": "o", "k": ["open", "source", "weight", "models", "publicly", "available", "weights", "downloaded", "run", "locally", "ranges", "fully", "llama", "restricted", "licenses"]}, {"id": "term-open-source-ai-movement", "t": "Open Source AI Movement", "tg": ["History", "Governance"], "d": "history", "x": "The growing trend of releasing AI model weights and training code publicly, enabling broader research and development...", "l": "o", "k": ["open", "source", "movement", "growing", "trend", "releasing", "model", "weights", "training", "code", "publicly", "enabling", "broader", "research", "development"]}, {"id": "term-open-domain-qa", "t": "Open-Domain Question Answering", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A QA setting where the model must answer questions using a large corpus or parametric knowledge without being given a...", "l": "o", "k": ["open-domain", "question", "answering", "setting", "model", "must", "answer", "questions", "large", "corpus", "parametric", "knowledge", "without", "given", "specific"]}, {"id": "term-open-vocabulary-detection", "t": "Open-Vocabulary Detection", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Object detection approaches that can identify and localize objects from categories not seen during training, leveraging...", "l": "o", "k": ["open-vocabulary", "detection", "object", "approaches", "identify", "localize", "objects", "categories", "seen", "training", "leveraging", "vision-language", "models", "generalize", "beyond"]}, {"id": "term-openai", "t": "OpenAI", "tg": ["Company", "LLM Provider"], "d": "models", "x": "The AI research company behind GPT models, ChatGPT, and DALL-E. Founded in 2015, it has been central to the development...", "l": "o", "k": ["openai", "research", "company", "behind", "gpt", "models", "chatgpt", "dall-e", "founded", "central", "development", "popularization", "modern", "assistants"]}, {"id": "term-openai-founded", "t": "OpenAI Founded", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of OpenAI as a nonprofit AI research laboratory in December 2015 by Sam Altman Elon Musk and others with...", "l": "o", "k": ["openai", "founded", "founding", "nonprofit", "research", "laboratory", "december", "sam", "altman", "elon", "musk", "others", "mission", "ensuring", "artificial"]}, {"id": "term-openai-founding", "t": "OpenAI Founding", "tg": ["History", "Milestones"], "d": "history", "x": "The founding of OpenAI in December 2015 as a non-profit AI research laboratory by Sam Altman, Elon Musk, and others,...", "l": "o", "k": ["openai", "founding", "december", "non-profit", "research", "laboratory", "sam", "altman", "elon", "musk", "others", "mission", "ensuring", "artificial", "general"]}, {"id": "term-openai-capped-profit", "t": "OpenAI Transition to Capped Profit", "tg": ["History", "Milestones"], "d": "history", "x": "OpenAI's 2019 restructuring from a non-profit to a capped-profit company to attract the capital needed for large-scale...", "l": "o", "k": ["openai", "transition", "capped", "profit", "restructuring", "non-profit", "capped-profit", "company", "attract", "capital", "needed", "large-scale", "research", "creating", "hybrid"]}, {"id": "term-openpose", "t": "OpenPose", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A real-time multi-person pose estimation system that uses part affinity fields and confidence maps to detect body,...", "l": "o", "k": ["openpose", "real-time", "multi-person", "pose", "estimation", "system", "uses", "part", "affinity", "fields", "confidence", "maps", "detect", "body", "hand"]}, {"id": "term-operator-fusion", "t": "Operator Fusion", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "A compiler optimization that combines multiple sequential neural network operations into a single kernel launch,...", "l": "o", "k": ["operator", "fusion", "compiler", "optimization", "combines", "multiple", "sequential", "neural", "network", "operations", "single", "kernel", "launch", "reducing", "memory"]}, {"id": "term-opponent-modeling", "t": "Opponent Modeling", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "The practice of explicitly modeling the behavior, goals, or strategy of other agents in a multi-agent environment....", "l": "o", "k": ["opponent", "modeling", "practice", "explicitly", "behavior", "goals", "strategy", "agents", "multi-agent", "environment", "models", "enable", "effective", "adaptation", "strategic"]}, {"id": "term-ops5", "t": "OPS5", "tg": ["History", "Systems"], "d": "history", "x": "A rule-based programming language developed at Carnegie Mellon University in the late 1970s. OPS5 was used to implement...", "l": "o", "k": ["ops5", "rule-based", "programming", "language", "developed", "carnegie", "mellon", "university", "late", "1970s", "implement", "xcon", "expert", "system", "became"]}, {"id": "term-opt", "t": "OPT", "tg": ["Models", "Technical"], "d": "models", "x": "Open Pre-trained Transformer is a family of decoder-only language models released by Meta AI ranging from 125M to 175B...", "l": "o", "k": ["opt", "open", "pre-trained", "transformer", "family", "decoder-only", "language", "models", "released", "meta", "ranging", "125m", "175b", "parameters", "full"]}, {"id": "term-optical-character-recognition", "t": "Optical Character Recognition", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The technology that converts images of text (handwritten, printed, or typed) into machine-readable text, using...", "l": "o", "k": ["optical", "character", "recognition", "technology", "converts", "images", "text", "handwritten", "printed", "typed", "machine-readable", "detection", "localize", "regions", "identify"]}, {"id": "term-optical-flow", "t": "Optical Flow", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The estimation of per-pixel motion vectors between consecutive video frames, representing the apparent movement of...", "l": "o", "k": ["optical", "flow", "estimation", "per-pixel", "motion", "vectors", "consecutive", "video", "frames", "representing", "apparent", "movement", "objects", "camera", "analysis"]}, {"id": "term-optical-flow-estimation", "t": "Optical Flow Estimation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The computational process of predicting dense pixel-level displacement fields between consecutive video frames using...", "l": "o", "k": ["optical", "flow", "estimation", "computational", "process", "predicting", "dense", "pixel-level", "displacement", "fields", "consecutive", "video", "frames", "deep", "learning"]}, {"id": "term-optics", "t": "OPTICS", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Ordering Points To Identify the Clustering Structure is a density-based clustering algorithm that creates an ordering...", "l": "o", "k": ["optics", "ordering", "points", "identify", "clustering", "structure", "density-based", "algorithm", "creates", "reflecting", "density", "connectivity", "unlike", "dbscan", "clusters"]}, {"id": "term-optimization", "t": "Optimization", "tg": ["Training", "Process"], "d": "general", "x": "The process of adjusting model parameters to minimize loss. Includes algorithms (Adam, SGD), learning rate schedules,...", "l": "o", "k": ["optimization", "process", "adjusting", "model", "parameters", "minimize", "loss", "includes", "algorithms", "adam", "sgd", "learning", "rate", "schedules", "techniques"]}, {"id": "term-option-framework", "t": "Option Framework", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A formalism for temporal abstraction in RL where options are temporally extended actions consisting of an initiation...", "l": "o", "k": ["option", "framework", "formalism", "temporal", "abstraction", "options", "temporally", "extended", "actions", "consisting", "initiation", "internal", "policy", "termination", "condition"]}, {"id": "term-oracle", "t": "Oracle (ML)", "tg": ["Concept", "Evaluation"], "d": "datasets", "x": "A theoretical perfect model or information source used as a benchmark. In evaluation, comparing to an oracle helps...", "l": "o", "k": ["oracle", "theoretical", "perfect", "model", "information", "source", "benchmark", "evaluation", "comparing", "helps", "understand", "ceiling", "achievable", "performance"]}, {"id": "term-orca", "t": "Orca", "tg": ["Models", "Technical"], "d": "models", "x": "A language model by Microsoft that learns from rich signals including explanation traces from GPT-4. Demonstrates that...", "l": "o", "k": ["orca", "language", "model", "microsoft", "learns", "rich", "signals", "including", "explanation", "traces", "gpt-4", "demonstrates", "smaller", "models", "achieve"]}, {"id": "term-ordinal-regression", "t": "Ordinal Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A type of regression analysis for predicting ordinal (ordered categorical) outcomes. It models the cumulative...", "l": "o", "k": ["ordinal", "regression", "type", "analysis", "predicting", "ordered", "categorical", "outcomes", "models", "cumulative", "probabilities", "categories", "link", "function", "threshold"]}, {"id": "term-oriol-vinyals", "t": "Oriol Vinyals", "tg": ["History", "Pioneers"], "d": "history", "x": "Spanish computer scientist at DeepMind known for sequence-to-sequence learning pointer networks and leading the...", "l": "o", "k": ["oriol", "vinyals", "spanish", "computer", "scientist", "deepmind", "known", "sequence-to-sequence", "learning", "pointer", "networks", "leading", "alphastar", "project", "achieved"]}, {"id": "term-orpo", "t": "ORPO", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Odds Ratio Preference Optimization, an alignment method that combines supervised fine-tuning and preference alignment...", "l": "o", "k": ["orpo", "odds", "ratio", "preference", "optimization", "alignment", "method", "combines", "supervised", "fine-tuning", "single", "training", "stage", "ratios", "distinguish"]}, {"id": "term-orthogonal-initialization", "t": "Orthogonal Initialization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A weight initialization technique that initializes weight matrices as random orthogonal matrices. Helps preserve...", "l": "o", "k": ["orthogonal", "initialization", "weight", "technique", "initializes", "matrices", "random", "helps", "preserve", "gradient", "norms", "forward", "backward", "propagation", "particularly"]}, {"id": "term-orthogonality-thesis", "t": "Orthogonality Thesis", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The philosophical claim that intelligence and goals are orthogonal, meaning that any level of intelligence can in...", "l": "o", "k": ["orthogonality", "thesis", "philosophical", "claim", "intelligence", "goals", "orthogonal", "meaning", "level", "principle", "combined", "terminal", "goal", "implying", "superintelligent"]}, {"id": "term-out-of-bag-error", "t": "Out-of-Bag Error", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "An estimate of prediction error for bagged models computed using observations not included in the bootstrap sample for...", "l": "o", "k": ["out-of-bag", "error", "estimate", "prediction", "bagged", "models", "computed", "observations", "included", "bootstrap", "sample", "base", "learner", "provides", "unbiased"]}, {"id": "term-out-of-distribution", "t": "Out-of-Distribution (OOD)", "tg": ["Challenge", "Robustness"], "d": "general", "x": "Data that differs significantly from what a model was trained on. Models often perform poorly on OOD data, making...", "l": "o", "k": ["out-of-distribution", "ood", "data", "differs", "significantly", "model", "trained", "models", "perform", "poorly", "making", "detection", "important", "reliable", "deployment"]}, {"id": "term-out-of-vocabulary", "t": "Out-of-Vocabulary", "tg": ["NLP", "Tokenization"], "d": "general", "x": "Words or tokens encountered during inference that were not present in the model's training vocabulary, requiring...", "l": "o", "k": ["out-of-vocabulary", "words", "tokens", "encountered", "inference", "were", "present", "model", "training", "vocabulary", "requiring", "special", "handling", "subword", "tokenization"]}, {"id": "term-outcome-reward-model", "t": "Outcome Reward Model", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A reward model that evaluates only the final output of a generation, providing a holistic quality score used in RLHF to...", "l": "o", "k": ["outcome", "reward", "model", "evaluates", "final", "output", "generation", "providing", "holistic", "quality", "score", "rlhf", "train", "policy", "toward"]}, {"id": "term-outer-alignment", "t": "Outer Alignment", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The problem of ensuring that the base objective or loss function used during training accurately captures the intended...", "l": "o", "k": ["outer", "alignment", "problem", "ensuring", "base", "objective", "loss", "function", "training", "accurately", "captures", "intended", "goal", "system", "designer"]}, {"id": "term-outlier-detection", "t": "Outlier Detection", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "The process of identifying data points that differ significantly from the majority of observations. Methods include...", "l": "o", "k": ["outlier", "detection", "process", "identifying", "data", "points", "differ", "significantly", "majority", "observations", "methods", "include", "statistical", "tests", "z-score"]}, {"id": "term-outpainting", "t": "Outpainting", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "The task of extending an image beyond its original boundaries by generating new coherent content that seamlessly...", "l": "o", "k": ["outpainting", "task", "extending", "image", "beyond", "original", "boundaries", "generating", "coherent", "content", "seamlessly", "continues", "existing", "visual", "context"]}, {"id": "term-output-layer", "t": "Output Layer", "tg": ["Architecture", "Neural Networks"], "d": "models", "x": "The final layer of a neural network that produces the model's predictions. Its design depends on the task: softmax for...", "l": "o", "k": ["output", "layer", "final", "neural", "network", "produces", "model", "predictions", "design", "depends", "task", "softmax", "classification", "linear", "regression"]}, {"id": "term-overestimation-bias", "t": "Overestimation Bias", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A systematic tendency of Q-learning and related algorithms to overestimate action values due to the max operator in the...", "l": "o", "k": ["overestimation", "bias", "systematic", "tendency", "q-learning", "related", "algorithms", "overestimate", "action", "values", "due", "max", "operator", "bellman", "optimality"]}, {"id": "term-overfitting", "t": "Overfitting", "tg": ["Problem", "Training"], "d": "general", "x": "When a model performs well on training data but poorly on new data, having memorized specific examples rather than...", "l": "o", "k": ["overfitting", "model", "performs", "training", "data", "poorly", "having", "memorized", "specific", "examples", "rather", "learning", "general", "patterns", "addressed"]}, {"id": "term-owl-vit", "t": "OWL-ViT", "tg": ["Models", "Technical"], "d": "models", "x": "Open-World Localization with Vision Transformer is a zero-shot object detection model that localizes objects from text...", "l": "o", "k": ["owl-vit", "open-world", "localization", "vision", "transformer", "zero-shot", "object", "detection", "model", "localizes", "objects", "text", "queries", "uses", "clip-style"]}, {"id": "term-p-value", "t": "P-Value", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The probability of observing a test statistic at least as extreme as the one computed from the data, assuming the null...", "l": "p", "k": ["p-value", "probability", "observing", "test", "statistic", "least", "extreme", "computed", "data", "assuming", "null", "hypothesis", "true", "smaller", "p-values"]}, {"id": "term-pac-learning", "t": "PAC Learning", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "Probably Approximately Correct learning, a theoretical framework that defines the conditions under which a learning...", "l": "p", "k": ["pac", "learning", "probably", "approximately", "correct", "theoretical", "framework", "defines", "conditions", "algorithm", "high", "probability", "produce", "hypothesis", "given"]}, {"id": "term-padding", "t": "Padding", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The addition of extra values (typically zeros) around the borders of an input image or feature map before convolution,...", "l": "p", "k": ["padding", "addition", "extra", "values", "typically", "zeros", "around", "borders", "input", "image", "feature", "map", "convolution", "controlling", "spatial"]}, {"id": "term-paged-attention", "t": "Paged Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A memory management technique for KV caches during LLM serving that stores attention keys and values in non-contiguous...", "l": "p", "k": ["paged", "attention", "memory", "management", "technique", "caches", "llm", "serving", "stores", "keys", "values", "non-contiguous", "pages", "reducing", "waste"]}, {"id": "term-pali", "t": "PaLI", "tg": ["Models", "Technical"], "d": "models", "x": "Pathways Language and Image model jointly scales a vision transformer and language model. Trained on WebLI a...", "l": "p", "k": ["pali", "pathways", "language", "image", "model", "jointly", "scales", "vision", "transformer", "trained", "webli", "multilingual", "image-text", "dataset", "achieves"]}, {"id": "term-palm", "t": "PaLM", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Pathways Language Model, a 540-billion parameter dense transformer by Google that demonstrated breakthrough performance...", "l": "p", "k": ["palm", "pathways", "language", "model", "540-billion", "parameter", "dense", "transformer", "google", "demonstrated", "breakthrough", "performance", "reasoning", "tasks", "system"]}, {"id": "term-palm-2", "t": "PaLM 2", "tg": ["Models", "Technical"], "d": "models", "x": "Google's second generation language model with improved multilingual reasoning and coding capabilities compared to...", "l": "p", "k": ["palm", "google", "generation", "language", "model", "improved", "multilingual", "reasoning", "coding", "capabilities", "compared", "uses", "compute-optimal", "approach", "better"]}, {"id": "term-palm-e", "t": "PaLM-E", "tg": ["Models", "Technical"], "d": "models", "x": "An embodied multimodal language model that integrates real-world continuous sensor data with language. Can reason about...", "l": "p", "k": ["palm-e", "embodied", "multimodal", "language", "model", "integrates", "real-world", "continuous", "sensor", "data", "reason", "physical", "world", "plan", "robot"]}, {"id": "term-pandemonium-architecture", "t": "Pandemonium Architecture", "tg": ["History", "Systems"], "d": "history", "x": "The hierarchical pattern recognition system proposed by Oliver Selfridge in 1959 at the National Physical Laboratory....", "l": "p", "k": ["pandemonium", "architecture", "hierarchical", "pattern", "recognition", "system", "proposed", "oliver", "selfridge", "national", "physical", "laboratory", "multiple", "levels", "demons"]}, {"id": "term-pandemonium-model", "t": "Pandemonium Model", "tg": ["History", "Milestones"], "d": "history", "x": "A pattern recognition model proposed by Oliver Selfridge in 1959 using hierarchical layers of feature-detecting demons...", "l": "p", "k": ["pandemonium", "model", "pattern", "recognition", "proposed", "oliver", "selfridge", "hierarchical", "layers", "feature-detecting", "demons", "compete", "identify", "patterns", "anticipating"]}, {"id": "term-panoptic-segmentation", "t": "Panoptic Segmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A unified image segmentation task that assigns both a class label and an instance ID to every pixel, combining semantic...", "l": "p", "k": ["panoptic", "segmentation", "unified", "image", "task", "assigns", "class", "label", "instance", "pixel", "combining", "semantic", "stuff", "sky", "road"]}, {"id": "term-paperclip-maximizer", "t": "Paperclip Maximizer", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "A thought experiment by Nick Bostrom illustrating the dangers of misaligned AI, in which an AI with the sole objective...", "l": "p", "k": ["paperclip", "maximizer", "thought", "experiment", "nick", "bostrom", "illustrating", "dangers", "misaligned", "sole", "objective", "maximizing", "production", "converts", "available"]}, {"id": "term-parallel-distributed-processing", "t": "Parallel Distributed Processing", "tg": ["History", "Milestones"], "d": "history", "x": "A two-volume work published in 1986 by David Rumelhart James McClelland and the PDP Research Group. The books provided...", "l": "p", "k": ["parallel", "distributed", "processing", "two-volume", "work", "published", "david", "rumelhart", "james", "mcclelland", "pdp", "research", "group", "books", "provided"]}, {"id": "term-parameters", "t": "Parameters", "tg": ["Core Concept", "Dual Meaning"], "d": "general", "x": "In prompting: constraints and specifications that shape AI output. In models: the learned weights (billions in LLMs)...", "l": "p", "k": ["parameters", "prompting", "constraints", "specifications", "shape", "output", "models", "learned", "weights", "billions", "llms", "determine", "behavior", "parameter", "count"]}, {"id": "term-parametric-relu", "t": "Parametric ReLU", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An activation function that generalizes Leaky ReLU by making the negative slope a learnable parameter. Introduced by He...", "l": "p", "k": ["parametric", "relu", "activation", "function", "generalizes", "leaky", "making", "negative", "slope", "learnable", "parameter", "introduced", "part", "work", "deep"]}, {"id": "term-paraphrase-detection", "t": "Paraphrase Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of determining whether two text passages convey the same meaning using different words or structures,...", "l": "p", "k": ["paraphrase", "detection", "task", "determining", "text", "passages", "convey", "meaning", "different", "words", "structures", "requiring", "understanding", "semantic", "equivalence"]}, {"id": "term-parent-document-retrieval", "t": "Parent Document Retrieval", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A RAG strategy that indexes small child chunks for precise matching but returns their larger parent documents to the...", "l": "p", "k": ["parent", "document", "retrieval", "rag", "strategy", "indexes", "small", "child", "chunks", "precise", "matching", "returns", "larger", "documents", "llm"]}, {"id": "term-parent-child-chunking", "t": "Parent-Child Chunking", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "A hierarchical chunking strategy that creates small child chunks for precise embedding-based retrieval while linking...", "l": "p", "k": ["parent-child", "chunking", "hierarchical", "strategy", "creates", "small", "child", "chunks", "precise", "embedding-based", "retrieval", "linking", "larger", "parent", "provide"]}, {"id": "term-parse-tree", "t": "Parse Tree", "tg": ["NLP", "Parsing"], "d": "general", "x": "A hierarchical tree structure representing the syntactic structure of a sentence according to a formal grammar, with...", "l": "p", "k": ["parse", "tree", "hierarchical", "structure", "representing", "syntactic", "sentence", "according", "formal", "grammar", "internal", "nodes", "phrase", "categories", "leaves"]}, {"id": "term-pos-tagging", "t": "Part-of-Speech Tagging", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of assigning grammatical categories such as noun, verb, adjective, or adverb to each word in a sentence based...", "l": "p", "k": ["part-of-speech", "tagging", "task", "assigning", "grammatical", "categories", "noun", "verb", "adjective", "adverb", "word", "sentence", "based", "context", "morphological"]}, {"id": "term-parti", "t": "Parti", "tg": ["Models", "Technical"], "d": "models", "x": "Pathways Autoregressive Text-to-Image model by Google that treats image generation as a sequence-to-sequence problem...", "l": "p", "k": ["parti", "pathways", "autoregressive", "text-to-image", "model", "google", "treats", "image", "generation", "sequence-to-sequence", "problem", "transformer", "scales", "handles", "complex"]}, {"id": "term-partial-autocorrelation", "t": "Partial Autocorrelation", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "The correlation between a time series observation and a lagged observation after removing the effects of intermediate...", "l": "p", "k": ["partial", "autocorrelation", "correlation", "time", "series", "observation", "lagged", "removing", "effects", "intermediate", "lags", "helps", "determine", "order", "autoregressive"]}, {"id": "term-partial-dependence-plot", "t": "Partial Dependence Plot", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A visualization showing the marginal effect of one or two features on the predicted outcome of a model, averaging over...", "l": "p", "k": ["partial", "dependence", "plot", "visualization", "showing", "marginal", "effect", "features", "predicted", "outcome", "model", "averaging", "values", "reveals", "relationship"]}, {"id": "term-partial-least-squares", "t": "Partial Least Squares", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A regression method that simultaneously reduces the dimensionality of predictors and response variables by finding...", "l": "p", "k": ["partial", "least", "squares", "regression", "method", "simultaneously", "reduces", "dimensionality", "predictors", "response", "variables", "finding", "latent", "components", "maximize"]}, {"id": "term-partially-observable-mdp", "t": "Partially Observable MDP (POMDP)", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "An extension of the MDP framework where the agent cannot directly observe the full state and instead receives partial...", "l": "p", "k": ["partially", "observable", "mdp", "pomdp", "extension", "framework", "agent", "cannot", "directly", "observe", "full", "state", "instead", "receives", "partial"]}, {"id": "term-participatory-ai-design", "t": "Participatory AI Design", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "An approach to AI development that involves affected communities and stakeholders in the design, development, and...", "l": "p", "k": ["participatory", "design", "approach", "development", "involves", "affected", "communities", "stakeholders", "evaluation", "process", "ensuring", "diverse", "perspectives", "shape", "system"]}, {"id": "term-particle-swarm-optimization", "t": "Particle Swarm Optimization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A population-based optimization algorithm inspired by the social behavior of bird flocking and fish schooling....", "l": "p", "k": ["particle", "swarm", "optimization", "population-based", "algorithm", "inspired", "social", "behavior", "bird", "flocking", "fish", "schooling", "particles", "move", "search"]}, {"id": "term-partnership-on-ai", "t": "Partnership on AI", "tg": ["Governance", "AI Ethics"], "d": "safety", "x": "A multi-stakeholder organization founded in 2016 by major technology companies to study and formulate best practices on...", "l": "p", "k": ["partnership", "multi-stakeholder", "organization", "founded", "major", "technology", "companies", "study", "formulate", "best", "practices", "technologies", "advancing", "understanding", "impact"]}, {"id": "term-pascal-voc", "t": "Pascal VOC", "tg": ["History", "Milestones"], "d": "history", "x": "The PASCAL Visual Object Classes challenge and dataset running from 2005 to 2012. Pascal VOC provided standardized...", "l": "p", "k": ["pascal", "voc", "visual", "object", "classes", "challenge", "dataset", "running", "provided", "standardized", "evaluation", "detection", "image", "classification", "segmentation"]}, {"id": "term-pass-at-k", "t": "Pass@k", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A code generation evaluation metric that measures the probability that at least one of k generated code samples passes...", "l": "p", "k": ["pass", "code", "generation", "evaluation", "metric", "measures", "probability", "least", "generated", "samples", "passes", "test", "cases", "computed", "unbiased"]}, {"id": "term-passage-retrieval", "t": "Passage Retrieval", "tg": ["Retrieval", "Search"], "d": "general", "x": "The task of identifying and retrieving the most relevant text passages from a large corpus in response to a query,...", "l": "p", "k": ["passage", "retrieval", "task", "identifying", "retrieving", "relevant", "text", "passages", "large", "corpus", "response", "query", "operating", "finer", "granularity"]}, {"id": "term-patrick-winston", "t": "Patrick Winston", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist (1943-2019) who directed the MIT AI Lab from 1972 to 1997 and authored the influential AI...", "l": "p", "k": ["patrick", "winston", "american", "computer", "scientist", "1943-2019", "directed", "mit", "lab", "authored", "influential", "textbook", "making", "significant", "contributions"]}, {"id": "term-pattern-recognition-and-machine-learning", "t": "Pattern Recognition and Machine Learning", "tg": ["History", "Milestones"], "d": "history", "x": "A textbook by Christopher Bishop published in 2006 that covers Bayesian approaches to pattern recognition and machine...", "l": "p", "k": ["pattern", "recognition", "machine", "learning", "textbook", "christopher", "bishop", "published", "covers", "bayesian", "approaches", "book", "became", "influential", "presenting"]}, {"id": "term-pca-for-embeddings", "t": "PCA for Embeddings", "tg": ["Vector Database", "Dimensionality Reduction"], "d": "general", "x": "The application of Principal Component Analysis to reduce embedding dimensionality by projecting vectors onto the...", "l": "p", "k": ["pca", "embeddings", "application", "principal", "component", "analysis", "reduce", "embedding", "dimensionality", "projecting", "vectors", "onto", "directions", "maximum", "variance"]}, {"id": "term-pcie", "t": "PCIe for AI", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "Peripheral Component Interconnect Express, the standard high-speed serial interface connecting GPUs and accelerators to...", "l": "p", "k": ["pcie", "peripheral", "component", "interconnect", "express", "standard", "high-speed", "serial", "interface", "connecting", "gpus", "accelerators", "host", "system", "gen"]}, {"id": "term-peft", "t": "PEFT (Parameter-Efficient Fine-Tuning)", "tg": ["Training", "Efficiency"], "d": "general", "x": "Techniques that fine-tune models by training only a small subset of parameters. Includes LoRA, prefix tuning, and...", "l": "p", "k": ["peft", "parameter-efficient", "fine-tuning", "techniques", "fine-tune", "models", "training", "small", "subset", "parameters", "includes", "lora", "prefix", "tuning", "adapters"]}, {"id": "term-penn-treebank", "t": "Penn Treebank", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A large annotated corpus of English text with part-of-speech tags and syntactic parse trees, widely used as a benchmark...", "l": "p", "k": ["penn", "treebank", "large", "annotated", "corpus", "english", "text", "part-of-speech", "tags", "syntactic", "parse", "trees", "widely", "benchmark", "training"]}, {"id": "term-perceiver", "t": "Perceiver", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A general-purpose architecture that uses cross-attention to map arbitrary high-dimensional inputs to a fixed-size...", "l": "p", "k": ["perceiver", "general-purpose", "architecture", "uses", "cross-attention", "map", "arbitrary", "high-dimensional", "inputs", "fixed-size", "latent", "array", "followed", "self-attention", "space"]}, {"id": "term-perceiver-io", "t": "Perceiver IO", "tg": ["Models", "Technical"], "d": "models", "x": "An extension of the Perceiver that adds flexible output generation through output queries and cross-attention. Handles...", "l": "p", "k": ["perceiver", "extension", "adds", "flexible", "output", "generation", "queries", "cross-attention", "handles", "diverse", "input", "structures", "enabling", "multi-task", "learning"]}, {"id": "term-perceptron", "t": "Perceptron", "tg": ["History", "Milestones"], "d": "history", "x": "A single-layer neural network model introduced by Frank Rosenblatt in 1957 that could learn to classify linearly...", "l": "p", "k": ["perceptron", "single-layer", "neural", "network", "model", "introduced", "frank", "rosenblatt", "learn", "classify", "linearly", "separable", "patterns", "limitations", "demonstrated"]}, {"id": "term-perceptrons-book", "t": "Perceptrons Book", "tg": ["History", "Milestones"], "d": "history", "x": "A 1969 book by Marvin Minsky and Seymour Papert that mathematically analyzed the limitations of single-layer...", "l": "p", "k": ["perceptrons", "book", "marvin", "minsky", "seymour", "papert", "mathematically", "analyzed", "limitations", "single-layer", "particularly", "inability", "solve", "xor", "problem"]}, {"id": "term-perceptual-loss", "t": "Perceptual Loss", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A loss function for image generation that compares feature representations from a pre-trained network rather than raw...", "l": "p", "k": ["perceptual", "loss", "function", "image", "generation", "compares", "feature", "representations", "pre-trained", "network", "rather", "raw", "pixel", "values", "encouraging"]}, {"id": "term-performer", "t": "Performer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer variant that uses random feature-based approximation of softmax attention through the FAVOR+ mechanism,...", "l": "p", "k": ["performer", "transformer", "variant", "uses", "random", "feature-based", "approximation", "softmax", "attention", "favor", "mechanism", "achieving", "linear", "time", "space"]}, {"id": "term-permutation-importance", "t": "Permutation Importance", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A model-agnostic method for estimating feature importance by measuring the increase in prediction error when a single...", "l": "p", "k": ["permutation", "importance", "model-agnostic", "method", "estimating", "feature", "measuring", "increase", "prediction", "error", "single", "values", "randomly", "shuffled", "breaking"]}, {"id": "term-perplexity", "t": "Perplexity", "tg": ["Metric", "Evaluation"], "d": "datasets", "x": "A metric measuring how \"surprised\" a language model is by text. Lower perplexity indicates better prediction. Also the...", "l": "p", "k": ["perplexity", "metric", "measuring", "surprised", "language", "model", "text", "lower", "indicates", "better", "prediction", "name", "search", "engine", "combining"]}, {"id": "term-perplexity-metric", "t": "Perplexity Metric", "tg": ["NLP", "Text Processing"], "d": "general", "x": "An intrinsic evaluation metric for language models defined as the exponentiated average negative log-likelihood per...", "l": "p", "k": ["perplexity", "metric", "intrinsic", "evaluation", "language", "models", "defined", "exponentiated", "average", "negative", "log-likelihood", "per", "token", "measuring", "model"]}, {"id": "term-persona", "t": "Persona", "tg": ["Prompting", "Technique"], "d": "general", "x": "A specific character or role assigned to an AI through prompting. Personas can include expertise, communication style,...", "l": "p", "k": ["persona", "specific", "character", "role", "assigned", "prompting", "personas", "include", "expertise", "communication", "style", "behavioral", "guidelines", "shape", "responses"]}, {"id": "term-persona-prompting", "t": "Persona Prompting", "tg": ["Prompt Engineering", "Persona"], "d": "general", "x": "A technique that defines a detailed character profile including background, expertise, communication style, and...", "l": "p", "k": ["persona", "prompting", "technique", "defines", "detailed", "character", "profile", "including", "background", "expertise", "communication", "style", "behavioral", "traits", "model"]}, {"id": "term-peter-norvig", "t": "Peter Norvig", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist and co-author of Artificial Intelligence: A Modern Approach the most widely used AI...", "l": "p", "k": ["peter", "norvig", "american", "computer", "scientist", "co-author", "artificial", "intelligence", "modern", "approach", "widely", "textbook", "former", "director", "research"]}, {"id": "term-phi", "t": "Phi", "tg": ["Models", "Technical"], "d": "models", "x": "A family of small language models by Microsoft Research that achieve surprisingly strong performance through careful...", "l": "p", "k": ["phi", "family", "small", "language", "models", "microsoft", "research", "achieve", "surprisingly", "strong", "performance", "careful", "data", "curation", "demonstrate"]}, {"id": "term-phi-architecture", "t": "Phi Architecture", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A family of small language models by Microsoft that achieve strong performance through carefully curated high-quality...", "l": "p", "k": ["phi", "architecture", "family", "small", "language", "models", "microsoft", "achieve", "strong", "performance", "carefully", "curated", "high-quality", "training", "data"]}, {"id": "term-phoneme", "t": "Phoneme", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The smallest unit of sound in a language that can distinguish one word from another, used in speech recognition systems...", "l": "p", "k": ["phoneme", "smallest", "unit", "sound", "language", "distinguish", "word", "another", "speech", "recognition", "systems", "map", "acoustic", "signals", "linguistic"]}, {"id": "term-phonetics-in-ai", "t": "Phonetics in AI", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The application of phonetic knowledge to AI systems for speech processing, including modeling the acoustic properties...", "l": "p", "k": ["phonetics", "application", "phonetic", "knowledge", "systems", "speech", "processing", "including", "modeling", "acoustic", "properties", "sounds", "recognition", "synthesis", "tasks"]}, {"id": "term-photometric-augmentation", "t": "Photometric Augmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Image augmentation techniques that modify pixel values without changing spatial layout, including brightness, contrast,...", "l": "p", "k": ["photometric", "augmentation", "image", "techniques", "modify", "pixel", "values", "without", "changing", "spatial", "layout", "including", "brightness", "contrast", "saturation"]}, {"id": "term-physical-symbol-system-hypothesis", "t": "Physical Symbol System Hypothesis", "tg": ["History", "Milestones"], "d": "history", "x": "The 1976 hypothesis by Newell and Simon that a physical symbol system has the necessary and sufficient means for...", "l": "p", "k": ["physical", "symbol", "system", "hypothesis", "newell", "simon", "necessary", "sufficient", "means", "intelligent", "action", "providing", "theoretical", "foundation", "symbolic"]}, {"id": "term-pieter-abbeel", "t": "Pieter Abbeel", "tg": ["History", "Pioneers"], "d": "history", "x": "Belgian-American computer scientist at UC Berkeley known for work on robot learning from demonstration deep...", "l": "p", "k": ["pieter", "abbeel", "belgian-american", "computer", "scientist", "berkeley", "known", "work", "robot", "learning", "demonstration", "deep", "reinforcement", "dexterous", "robotic"]}, {"id": "term-pinecone", "t": "Pinecone", "tg": ["Vector Database", "Managed Service"], "d": "general", "x": "A fully managed cloud-native vector database service designed for production machine learning applications, providing...", "l": "p", "k": ["pinecone", "fully", "managed", "cloud-native", "vector", "database", "service", "designed", "production", "machine", "learning", "applications", "providing", "serverless", "pod-based"]}, {"id": "term-pipeline", "t": "Pipeline (ML)", "tg": ["Architecture", "MLOps"], "d": "models", "x": "A sequence of data processing and modeling steps chained together. Includes preprocessing, feature extraction, model...", "l": "p", "k": ["pipeline", "sequence", "data", "processing", "modeling", "steps", "chained", "together", "includes", "preprocessing", "feature", "extraction", "model", "inference", "post-processing"]}, {"id": "term-pipeline-parallelism", "t": "Pipeline Parallelism", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A distributed training strategy that partitions model layers into stages across devices, processing different...", "l": "p", "k": ["pipeline", "parallelism", "distributed", "training", "strategy", "partitions", "model", "layers", "stages", "across", "devices", "processing", "different", "micro-batches", "simultaneously"]}, {"id": "term-pix2pix", "t": "Pix2Pix", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A conditional GAN framework for paired image-to-image translation that uses a U-Net generator and PatchGAN...", "l": "p", "k": ["pix2pix", "conditional", "gan", "framework", "paired", "image-to-image", "translation", "uses", "u-net", "generator", "patchgan", "discriminator", "learn", "mappings", "aligned"]}, {"id": "term-plan-and-execute-agent", "t": "Plan-and-Execute Agent", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An agentic architecture that separates high-level planning from step-by-step execution, with a planner LLM creating...", "l": "p", "k": ["plan-and-execute", "agent", "agentic", "architecture", "separates", "high-level", "planning", "step-by-step", "execution", "planner", "llm", "creating", "task", "decompositions", "executor"]}, {"id": "term-plan-and-solve-plus", "t": "Plan-and-Solve Plus", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "An enhanced version of plan-and-solve prompting that adds detailed instructions to extract relevant variables,...", "l": "p", "k": ["plan-and-solve", "plus", "enhanced", "version", "prompting", "adds", "detailed", "instructions", "extract", "relevant", "variables", "calculate", "intermediate", "results", "pay"]}, {"id": "term-planning-in-ai", "t": "Planning in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "The area of AI concerned with the realization of strategies or action sequences typically for autonomous agents robots...", "l": "p", "k": ["planning", "area", "concerned", "realization", "strategies", "action", "sequences", "typically", "autonomous", "agents", "robots", "drones", "encompasses", "classical", "strips"]}, {"id": "term-planning-rl", "t": "Planning in RL", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "The process of using a model of the environment to compute or improve a policy before or during interaction. Planning...", "l": "p", "k": ["planning", "process", "model", "environment", "compute", "improve", "policy", "interaction", "methods", "dyna", "integrate", "model-based", "simulation", "model-free", "learning"]}, {"id": "term-platt-scaling", "t": "Platt Scaling", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A post-hoc calibration method that fits a logistic regression model to the raw output scores of a classifier using a...", "l": "p", "k": ["platt", "scaling", "post-hoc", "calibration", "method", "fits", "logistic", "regression", "model", "raw", "output", "scores", "classifier", "held-out", "validation"]}, {"id": "term-playground", "t": "Playground (AI)", "tg": ["Tools", "Interface"], "d": "general", "x": "An interactive interface for experimenting with AI models without coding. Most AI providers offer playgrounds to test...", "l": "p", "k": ["playground", "interactive", "interface", "experimenting", "models", "without", "coding", "providers", "offer", "playgrounds", "test", "prompts", "adjust", "parameters", "explore"]}, {"id": "term-point-cloud", "t": "Point Cloud", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A 3D data representation consisting of a set of points in three-dimensional space, typically acquired by LiDAR or depth...", "l": "p", "k": ["point", "cloud", "data", "representation", "consisting", "points", "three-dimensional", "space", "typically", "acquired", "lidar", "depth", "sensors", "object", "detection"]}, {"id": "term-point-e", "t": "Point-E", "tg": ["Models", "Technical"], "d": "models", "x": "A system by OpenAI for generating 3D point clouds from text prompts using a two-stage process. First generates a...", "l": "p", "k": ["point-e", "system", "openai", "generating", "point", "clouds", "text", "prompts", "two-stage", "process", "generates", "synthetic", "view", "text-to-image", "model"]}, {"id": "term-pointnet", "t": "PointNet", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A pioneering deep learning architecture that directly processes unordered 3D point cloud data using shared MLPs and...", "l": "p", "k": ["pointnet", "pioneering", "deep", "learning", "architecture", "directly", "processes", "unordered", "point", "cloud", "data", "shared", "mlps", "symmetric", "pooling"]}, {"id": "term-pointnet-plus-plus", "t": "PointNet++", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "An extension of PointNet that introduces hierarchical feature learning by applying PointNet recursively on nested...", "l": "p", "k": ["pointnet", "extension", "introduces", "hierarchical", "feature", "learning", "applying", "recursively", "nested", "partitions", "point", "capturing", "local", "geometric", "structures"]}, {"id": "term-pointwise-convolution", "t": "Pointwise Convolution", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A 1x1 convolution that linearly combines features across channels at each spatial position without considering spatial...", "l": "p", "k": ["pointwise", "convolution", "1x1", "linearly", "combines", "features", "across", "channels", "spatial", "position", "without", "considering", "context", "commonly", "change"]}, {"id": "term-pmi", "t": "Pointwise Mutual Information", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A statistical measure of association between two events that compares their joint probability with their expected...", "l": "p", "k": ["pointwise", "mutual", "information", "statistical", "measure", "association", "events", "compares", "joint", "probability", "expected", "co-occurrence", "independence", "identify", "collocations"]}, {"id": "term-poisson-distribution", "t": "Poisson Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A discrete probability distribution expressing the probability of a given number of events occurring in a fixed...", "l": "p", "k": ["poisson", "distribution", "discrete", "probability", "expressing", "given", "number", "events", "occurring", "fixed", "interval", "known", "average", "rate", "independent"]}, {"id": "term-poisson-regression", "t": "Poisson Regression", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A generalized linear model for count data that assumes the response follows a Poisson distribution and uses a log link...", "l": "p", "k": ["poisson", "regression", "generalized", "linear", "model", "count", "data", "assumes", "response", "follows", "distribution", "uses", "log", "link", "function"]}, {"id": "term-policy", "t": "Policy", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A mapping from states to actions (or probability distributions over actions) that defines the agent's behavior....", "l": "p", "k": ["policy", "mapping", "states", "actions", "probability", "distributions", "defines", "agent", "behavior", "policies", "deterministic", "stochastic", "central", "object", "optimized"]}, {"id": "term-policy-distillation", "t": "Policy Distillation", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A transfer learning technique that trains a student policy to replicate the behavior of one or more teacher policies....", "l": "p", "k": ["policy", "distillation", "transfer", "learning", "technique", "trains", "student", "replicate", "behavior", "teacher", "policies", "compress", "multiple", "task-specific", "single"]}, {"id": "term-policy-entropy", "t": "Policy Entropy", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A measure of randomness in the agent's policy, used as a regularizer in RL to encourage exploration and prevent...", "l": "p", "k": ["policy", "entropy", "measure", "randomness", "agent", "regularizer", "encourage", "exploration", "prevent", "premature", "convergence", "bonuses", "added", "objective", "algorithms"]}, {"id": "term-policy-gradient", "t": "Policy Gradient", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A class of RL algorithms that directly optimize the policy by computing gradients of expected return with respect to...", "l": "p", "k": ["policy", "gradient", "class", "algorithms", "directly", "optimize", "computing", "gradients", "expected", "return", "respect", "parameters", "methods", "handle", "continuous"]}, {"id": "term-policy-iteration", "t": "Policy Iteration", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A dynamic programming algorithm for solving Markov Decision Processes that alternates between policy evaluation and...", "l": "p", "k": ["policy", "iteration", "dynamic", "programming", "algorithm", "solving", "markov", "decision", "processes", "alternates", "evaluation", "improvement", "convergence", "guaranteed", "find"]}, {"id": "term-polyak-averaging", "t": "Polyak Averaging", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A technique that maintains a running average of model parameters during optimization and uses the averaged parameters...", "l": "p", "k": ["polyak", "averaging", "technique", "maintains", "running", "average", "model", "parameters", "optimization", "uses", "averaged", "final", "predictions", "proven", "achieve"]}, {"id": "term-polynomial-kernel", "t": "Polynomial Kernel", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A kernel function that computes the inner product of feature vectors raised to a specified power, enabling SVMs and...", "l": "p", "k": ["polynomial", "kernel", "function", "computes", "inner", "product", "feature", "vectors", "raised", "specified", "power", "enabling", "svms", "methods", "learn"]}, {"id": "term-polynomial-regression", "t": "Polynomial Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A form of regression analysis in which the relationship between the independent variable and the dependent variable is...", "l": "p", "k": ["polynomial", "regression", "form", "analysis", "relationship", "independent", "variable", "dependent", "modeled", "nth-degree", "capturing", "non-linear", "relationships", "within", "linear"]}, {"id": "term-polysemy", "t": "Polysemy", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The property of a word having multiple related meanings, such as 'bank' meaning a financial institution or a river...", "l": "p", "k": ["polysemy", "property", "word", "having", "multiple", "related", "meanings", "bank", "meaning", "financial", "institution", "river", "posing", "challenges", "sense"]}, {"id": "term-pooling-operation", "t": "Pooling Operation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A downsampling operation in neural networks that reduces the spatial dimensions of feature maps by aggregating values...", "l": "p", "k": ["pooling", "operation", "downsampling", "neural", "networks", "reduces", "spatial", "dimensions", "feature", "maps", "aggregating", "values", "within", "local", "regions"]}, {"id": "term-population-based-training", "t": "Population-Based Training (PBT)", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A hyperparameter optimization method that trains a population of agents in parallel, periodically replacing poorly...", "l": "p", "k": ["population-based", "training", "pbt", "hyperparameter", "optimization", "method", "trains", "population", "agents", "parallel", "periodically", "replacing", "poorly", "performing", "mutated"]}, {"id": "term-pose-estimation", "t": "Pose Estimation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A computer vision task that detects the positions of body joints or keypoints in images or video, producing a skeletal...", "l": "p", "k": ["pose", "estimation", "computer", "vision", "task", "detects", "positions", "body", "joints", "keypoints", "images", "video", "producing", "skeletal", "representation"]}, {"id": "term-positional-encoding", "t": "Positional Encoding", "tg": ["Architecture", "Transformers"], "d": "models", "x": "A technique to inject position information into transformers, which otherwise process tokens without order awareness....", "l": "p", "k": ["positional", "encoding", "technique", "inject", "position", "information", "transformers", "otherwise", "process", "tokens", "without", "order", "awareness", "absolute", "relative"]}, {"id": "term-positional-interpolation", "t": "Positional Interpolation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A method for extending the context length of pretrained language models by linearly interpolating position encodings to...", "l": "p", "k": ["positional", "interpolation", "method", "extending", "context", "length", "pretrained", "language", "models", "linearly", "interpolating", "position", "encodings", "fit", "longer"]}, {"id": "term-post-norm-transformer", "t": "Post-Norm Transformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The original transformer configuration where layer normalization is applied after the residual connection in each...", "l": "p", "k": ["post-norm", "transformer", "original", "configuration", "layer", "normalization", "applied", "residual", "connection", "sublayer", "requiring", "careful", "learning", "rate", "warmup"]}, {"id": "term-post-training-quantization", "t": "Post-Training Quantization (PTQ)", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "Quantization applied to an already-trained model without further training, using calibration data to determine optimal...", "l": "p", "k": ["post-training", "quantization", "ptq", "applied", "already-trained", "model", "without", "training", "calibration", "data", "determine", "optimal", "scaling", "factors", "simpler"]}, {"id": "term-posterior-distribution", "t": "Posterior Distribution", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "The probability distribution of a parameter after updating the prior distribution with observed data via Bayes'...", "l": "p", "k": ["posterior", "distribution", "probability", "parameter", "updating", "prior", "observed", "data", "via", "bayes", "theorem", "combines", "beliefs", "likelihood", "form"]}, {"id": "term-posterior-predictive-distribution", "t": "Posterior Predictive Distribution", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "The distribution of future observations given the observed data, obtained by integrating the likelihood of new data...", "l": "p", "k": ["posterior", "predictive", "distribution", "future", "observations", "given", "observed", "data", "obtained", "integrating", "likelihood", "model", "parameters", "naturally", "incorporating"]}, {"id": "term-potential-based-reward-shaping", "t": "Potential-Based Reward Shaping", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "A reward shaping method using a potential function over states where the shaping reward equals the discounted...", "l": "p", "k": ["potential-based", "reward", "shaping", "method", "potential", "function", "states", "equals", "discounted", "difference", "potentials", "successor", "current", "form", "guarantees"]}, {"id": "term-power-analysis", "t": "Power Analysis", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A statistical method for determining the minimum sample size required to detect an effect of a specified size with a...", "l": "p", "k": ["power", "analysis", "statistical", "method", "determining", "minimum", "sample", "size", "required", "detect", "effect", "specified", "given", "level", "confidence"]}, {"id": "term-power-transform", "t": "Power Transform", "tg": ["Data Science", "Feature Engineering"], "d": "algorithms", "x": "A family of parametric transformations (including Box-Cox and Yeo-Johnson) applied to make data more Gaussian-like,...", "l": "p", "k": ["power", "transform", "family", "parametric", "transformations", "including", "box-cox", "yeo-johnson", "applied", "data", "gaussian-like", "stabilize", "variance", "minimize", "skewness"]}, {"id": "term-ppo", "t": "PPO (Proximal Policy Optimization)", "tg": ["Training", "Algorithm"], "d": "algorithms", "x": "A reinforcement learning algorithm commonly used in RLHF to train language models. Balances exploration with stable...", "l": "p", "k": ["ppo", "proximal", "policy", "optimization", "reinforcement", "learning", "algorithm", "commonly", "rlhf", "train", "language", "models", "balances", "exploration", "stable"]}, {"id": "term-ppo-clip", "t": "PPO-Clip", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "The clipped surrogate objective variant of Proximal Policy Optimization that limits policy updates by clipping the...", "l": "p", "k": ["ppo-clip", "clipped", "surrogate", "objective", "variant", "proximal", "policy", "optimization", "limits", "updates", "clipping", "probability", "ratio", "simpler", "kl-penalty"]}, {"id": "term-pragmatics", "t": "Pragmatics", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The branch of linguistics studying how context, speaker intention, and shared knowledge influence the interpretation of...", "l": "p", "k": ["pragmatics", "branch", "linguistics", "studying", "context", "speaker", "intention", "shared", "knowledge", "influence", "interpretation", "language", "beyond", "literal", "semantic"]}, {"id": "term-pre-norm", "t": "Pre-Norm", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A transformer architecture variant that applies layer normalization before rather than after each sub-layer, improving...", "l": "p", "k": ["pre-norm", "transformer", "architecture", "variant", "applies", "layer", "normalization", "rather", "sub-layer", "improving", "training", "stability", "enabling", "deep", "models"]}, {"id": "term-pre-norm-transformer", "t": "Pre-Norm Transformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer variant where layer normalization is applied before the attention and feedforward sublayers rather than...", "l": "p", "k": ["pre-norm", "transformer", "variant", "layer", "normalization", "applied", "attention", "feedforward", "sublayers", "rather", "improving", "training", "stability", "enabling", "removal"]}, {"id": "term-pre-tokenization", "t": "Pre-Tokenization", "tg": ["NLP", "Tokenization"], "d": "general", "x": "The initial splitting of raw text into preliminary units before applying subword tokenization, typically based on...", "l": "p", "k": ["pre-tokenization", "initial", "splitting", "raw", "text", "preliminary", "units", "applying", "subword", "tokenization", "typically", "based", "whitespace", "punctuation", "language-specific"]}, {"id": "term-pre-training", "t": "Pre-Training", "tg": ["Training", "Phase"], "d": "general", "x": "The initial training phase where models learn general language understanding from vast text data. Creates a foundation...", "l": "p", "k": ["pre-training", "initial", "training", "phase", "models", "learn", "general", "language", "understanding", "vast", "text", "data", "creates", "foundation", "fine-tuned"]}, {"id": "term-precautionary-principle-in-ai", "t": "Precautionary Principle in AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The application of the precautionary principle to AI development, arguing that when potential harms are severe or...", "l": "p", "k": ["precautionary", "principle", "application", "development", "arguing", "potential", "harms", "severe", "irreversible", "lack", "scientific", "certainty", "delay", "protective", "measures"]}, {"id": "term-precision", "t": "Precision", "tg": ["Metrics", "Technical"], "d": "datasets", "x": "In metrics: the proportion of positive predictions that are correct. In computing: the numerical format for model...", "l": "p", "k": ["precision", "metrics", "proportion", "positive", "predictions", "correct", "computing", "numerical", "format", "model", "weights", "fp32", "fp16", "int8", "affecting"]}, {"id": "term-precision-at-k", "t": "Precision at K", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A retrieval evaluation metric that measures the proportion of relevant documents among the top K retrieved results,...", "l": "p", "k": ["precision", "retrieval", "evaluation", "metric", "measures", "proportion", "relevant", "documents", "among", "top", "retrieved", "results", "providing", "cutoff-based", "assessment"]}, {"id": "term-precision-recall-curve-cv", "t": "Precision-Recall Curve", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A plot showing the trade-off between precision and recall at different confidence thresholds for an object detector,...", "l": "p", "k": ["precision-recall", "curve", "plot", "showing", "trade-off", "precision", "recall", "different", "confidence", "thresholds", "object", "detector", "area", "corresponding", "average"]}, {"id": "term-predictive-parity", "t": "Predictive Parity", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness metric requiring that the positive predictive value of a classifier is equal across all protected groups,...", "l": "p", "k": ["predictive", "parity", "fairness", "metric", "requiring", "positive", "value", "classifier", "equal", "across", "protected", "groups", "meaning", "among", "individuals"]}, {"id": "term-predictive-policing-ethics", "t": "Predictive Policing Ethics", "tg": ["AI Ethics", "Fairness"], "d": "safety", "x": "The ethical concerns surrounding AI systems used to forecast criminal activity, including risks of reinforcing racial...", "l": "p", "k": ["predictive", "policing", "ethics", "ethical", "concerns", "surrounding", "systems", "forecast", "criminal", "activity", "including", "risks", "reinforcing", "racial", "biases"]}, {"id": "term-preemptible-vms", "t": "Preemptible VMs", "tg": ["Distributed Computing", "Inference Infrastructure"], "d": "hardware", "x": "Google Cloud's discounted virtual machine instances that last up to 24 hours and can be terminated when resources are...", "l": "p", "k": ["preemptible", "vms", "google", "cloud", "discounted", "virtual", "machine", "instances", "last", "hours", "terminated", "resources", "needed", "elsewhere", "provide"]}, {"id": "term-preference-learning", "t": "Preference Learning", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A family of techniques that train models using human preference data (rankings or comparisons between outputs) rather...", "l": "p", "k": ["preference", "learning", "family", "techniques", "train", "models", "human", "data", "rankings", "comparisons", "outputs", "rather", "explicit", "labels", "including"]}, {"id": "term-prefill-phase", "t": "Prefill Phase", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The initial phase of LLM inference that processes the entire input prompt in parallel to populate the KV cache. The...", "l": "p", "k": ["prefill", "phase", "initial", "llm", "inference", "processes", "entire", "input", "prompt", "parallel", "populate", "cache", "compute-bound", "duration", "scales"]}, {"id": "term-prefill-decode-disaggregation", "t": "Prefill-Decode Disaggregation", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "An inference architecture that separates the compute-bound prefill and memory-bound decode phases onto different...", "l": "p", "k": ["prefill-decode", "disaggregation", "inference", "architecture", "separates", "compute-bound", "prefill", "memory-bound", "decode", "phases", "onto", "different", "hardware", "optimized", "workload"]}, {"id": "term-prefix-attention", "t": "Prefix Attention", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An attention pattern used in prefix language models where a set of prefix tokens can attend to each other...", "l": "p", "k": ["prefix", "attention", "pattern", "language", "models", "tokens", "attend", "bidirectionally", "subsequent", "causal", "combines", "encoder-like", "decoder-like", "single", "model"]}, {"id": "term-prefix-caching", "t": "Prefix Caching", "tg": ["LLM", "Inference"], "d": "models", "x": "An inference optimization that reuses the computed KV cache of shared prompt prefixes across multiple requests,...", "l": "p", "k": ["prefix", "caching", "inference", "optimization", "reuses", "computed", "cache", "shared", "prompt", "prefixes", "across", "multiple", "requests", "avoiding", "redundant"]}, {"id": "term-prefix-language-model", "t": "Prefix Language Model", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A language model architecture where a prefix portion of the input uses bidirectional attention while the remaining...", "l": "p", "k": ["prefix", "language", "model", "architecture", "portion", "input", "uses", "bidirectional", "attention", "remaining", "causal", "combining", "understanding", "generation", "capabilities"]}, {"id": "term-prefix-tuning", "t": "Prefix Tuning", "tg": ["Training", "Efficiency"], "d": "general", "x": "A parameter-efficient fine-tuning method that prepends trainable vectors to inputs. Only these prefixes are updated...", "l": "p", "k": ["prefix", "tuning", "parameter-efficient", "fine-tuning", "method", "prepends", "trainable", "vectors", "inputs", "prefixes", "updated", "training", "keeping", "base", "model"]}, {"id": "term-presence-penalty", "t": "Presence Penalty", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A parameter that applies a fixed penalty to any token that has appeared at least once in the output, encouraging the...", "l": "p", "k": ["presence", "penalty", "parameter", "applies", "fixed", "token", "appeared", "least", "output", "encouraging", "model", "introduce", "topics", "vocabulary"]}, {"id": "term-principal-component-analysis", "t": "Principal Component Analysis", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "An unsupervised linear dimensionality reduction technique that projects data onto orthogonal axes (principal...", "l": "p", "k": ["principal", "component", "analysis", "unsupervised", "linear", "dimensionality", "reduction", "technique", "projects", "data", "onto", "orthogonal", "axes", "components", "maximize"]}, {"id": "term-principal-component-regression", "t": "Principal Component Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A regression technique that first reduces the dimensionality of predictor variables using PCA and then regresses the...", "l": "p", "k": ["principal", "component", "regression", "technique", "reduces", "dimensionality", "predictor", "variables", "pca", "regresses", "response", "retained", "components", "addressing", "multicollinearity"]}, {"id": "term-prior-distribution", "t": "Prior Distribution", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "In Bayesian statistics, the probability distribution representing beliefs about a parameter before observing data. It...", "l": "p", "k": ["prior", "distribution", "bayesian", "statistics", "probability", "representing", "beliefs", "parameter", "observing", "data", "encodes", "knowledge", "assumptions", "updated", "likelihood"]}, {"id": "term-prioritized-experience-replay", "t": "Prioritized Experience Replay", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An experience replay strategy that samples transitions with probability proportional to their TD error magnitude,...", "l": "p", "k": ["prioritized", "experience", "replay", "strategy", "samples", "transitions", "probability", "proportional", "error", "magnitude", "allowing", "agent", "learn", "frequently", "surprising"]}, {"id": "term-prioritized-level-replay", "t": "Prioritized Level Replay (PLR)", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "An unsupervised environment design method that tracks learning progress on procedurally generated levels and replays...", "l": "p", "k": ["prioritized", "level", "replay", "plr", "unsupervised", "environment", "design", "method", "tracks", "learning", "progress", "procedurally", "generated", "levels", "replays"]}, {"id": "term-privacy-preserving", "t": "Privacy-Preserving AI", "tg": ["Privacy", "Ethics"], "d": "safety", "x": "Techniques to use AI without exposing sensitive data. Includes federated learning, differential privacy, and secure...", "l": "p", "k": ["privacy-preserving", "techniques", "without", "exposing", "sensitive", "data", "includes", "federated", "learning", "differential", "privacy", "secure", "multi-party", "computation", "critical"]}, {"id": "term-pcfg", "t": "Probabilistic Context-Free Grammar", "tg": ["NLP", "Parsing"], "d": "general", "x": "A context-free grammar augmented with probabilities for each production rule, enabling statistical parsing by selecting...", "l": "p", "k": ["probabilistic", "context-free", "grammar", "augmented", "probabilities", "production", "rule", "enabling", "statistical", "parsing", "selecting", "probable", "parse", "tree", "input"]}, {"id": "term-probabilistic-graphical-models", "t": "Probabilistic Graphical Models", "tg": ["History", "Fundamentals"], "d": "history", "x": "A framework for representing complex probability distributions using graph structures. Encompassing Bayesian networks...", "l": "p", "k": ["probabilistic", "graphical", "models", "framework", "representing", "complex", "probability", "distributions", "graph", "structures", "encompassing", "bayesian", "networks", "markov", "random"]}, {"id": "term-probing", "t": "Probing", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An interpretability technique that trains simple classifiers on the internal representations of a neural network to...", "l": "p", "k": ["probing", "interpretability", "technique", "trains", "simple", "classifiers", "internal", "representations", "neural", "network", "test", "linguistic", "semantic", "information", "encoded"]}, {"id": "term-probit-model", "t": "Probit Model", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A regression model for binary outcomes that uses the cumulative distribution function of the standard normal...", "l": "p", "k": ["probit", "model", "regression", "binary", "outcomes", "uses", "cumulative", "distribution", "function", "standard", "normal", "link", "relating", "linear", "predictor"]}, {"id": "term-procedural-environment-generation", "t": "Procedural Environment Generation", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "The automatic creation of diverse training environments through algorithmic variation of level layouts, object...", "l": "p", "k": ["procedural", "environment", "generation", "automatic", "creation", "diverse", "training", "environments", "algorithmic", "variation", "level", "layouts", "object", "positions", "task"]}, {"id": "term-process-reward-model", "t": "Process Reward Model", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A reward model that scores each intermediate reasoning step rather than only the final answer, enabling more...", "l": "p", "k": ["process", "reward", "model", "scores", "intermediate", "reasoning", "step", "rather", "final", "answer", "enabling", "fine-grained", "feedback", "training", "models"]}, {"id": "term-product-quantization", "t": "Product Quantization", "tg": ["Vector Database", "Quantization"], "d": "general", "x": "A vector compression technique that splits high-dimensional vectors into sub-vectors and quantizes each independently...", "l": "p", "k": ["product", "quantization", "vector", "compression", "technique", "splits", "high-dimensional", "vectors", "sub-vectors", "quantizes", "independently", "learned", "codebook", "enabling", "dramatic"]}, {"id": "term-production-rules", "t": "Production Rules", "tg": ["History", "Fundamentals"], "d": "history", "x": "A knowledge representation formalism consisting of condition-action pairs (if-then rules) used extensively in expert...", "l": "p", "k": ["production", "rules", "knowledge", "representation", "formalism", "consisting", "condition-action", "pairs", "if-then", "extensively", "expert", "systems", "ops5", "clips", "provided"]}, {"id": "term-progan", "t": "ProGAN", "tg": ["Models", "Technical"], "d": "models", "x": "Progressive Growing of GANs trains the generator and discriminator progressively starting from low resolution and...", "l": "p", "k": ["progan", "progressive", "growing", "gans", "trains", "generator", "discriminator", "progressively", "starting", "low", "resolution", "adding", "layers", "higher", "resolutions"]}, {"id": "term-program-aided-language-model", "t": "Program-Aided Language Model", "tg": ["Prompt Engineering", "Code-Augmented"], "d": "general", "x": "A framework (PAL) that prompts a language model to generate executable program code as intermediate reasoning steps...", "l": "p", "k": ["program-aided", "language", "model", "framework", "pal", "prompts", "generate", "executable", "program", "code", "intermediate", "reasoning", "steps", "rather", "natural"]}, {"id": "term-progressive-training", "t": "Progressive Training", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A training strategy that gradually increases model or data complexity during training. Examples include progressive...", "l": "p", "k": ["progressive", "training", "strategy", "gradually", "increases", "model", "data", "complexity", "examples", "include", "growing", "gans", "curriculum", "learning", "helps"]}, {"id": "term-projected-gradient-descent-attack", "t": "Projected Gradient Descent Attack", "tg": ["Algorithms", "Safety"], "d": "algorithms", "x": "An iterative adversarial attack that applies FGSM multiple times with small step sizes projecting back onto the...", "l": "p", "k": ["projected", "gradient", "descent", "attack", "iterative", "adversarial", "applies", "fgsm", "multiple", "times", "small", "step", "sizes", "projecting", "onto"]}, {"id": "term-prolog", "t": "Prolog", "tg": ["History", "Milestones"], "d": "history", "x": "A logic programming language created by Alain Colmerauer and Robert Kowalski in 1972, widely used in AI research for...", "l": "p", "k": ["prolog", "logic", "programming", "language", "created", "alain", "colmerauer", "robert", "kowalski", "widely", "research", "natural", "processing", "expert", "systems"]}, {"id": "term-prompt", "t": "Prompt", "tg": ["Core Concept", "Fundamentals"], "d": "general", "x": "The text input you send to an AI assistant. Can include context, instructions, examples, and constraints. Prompt...", "l": "p", "k": ["prompt", "text", "input", "send", "assistant", "include", "context", "instructions", "examples", "constraints", "quality", "directly", "influences", "response"]}, {"id": "term-prompt-caching", "t": "Prompt Caching", "tg": ["LLM", "Inference"], "d": "models", "x": "An optimization technique that stores and reuses the computed key-value representations of common prompt prefixes,...", "l": "p", "k": ["prompt", "caching", "optimization", "technique", "stores", "reuses", "computed", "key-value", "representations", "common", "prefixes", "reducing", "redundant", "computation", "repeated"]}, {"id": "term-prompt-chaining", "t": "Prompt Chaining", "tg": ["Technique", "Advanced"], "d": "general", "x": "Breaking complex tasks into multiple sequential prompts, where each builds on the previous output. Enables...", "l": "p", "k": ["prompt", "chaining", "breaking", "complex", "tasks", "multiple", "sequential", "prompts", "builds", "previous", "output", "enables", "sophisticated", "workflows", "better"]}, {"id": "term-prompt-chaining-architecture", "t": "Prompt Chaining Architecture", "tg": ["Prompt Engineering", "Architecture"], "d": "models", "x": "A system design pattern where multiple prompts are connected in a pipeline or directed graph, with each prompt handling...", "l": "p", "k": ["prompt", "chaining", "architecture", "system", "design", "pattern", "multiple", "prompts", "connected", "pipeline", "directed", "graph", "handling", "specific", "subtask"]}, {"id": "term-prompt-compression", "t": "Prompt Compression", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Techniques that reduce the token length of prompts without losing essential information, using methods like selective...", "l": "p", "k": ["prompt", "compression", "techniques", "reduce", "token", "length", "prompts", "without", "losing", "essential", "information", "methods", "selective", "context", "summarization"]}, {"id": "term-prompt-engineering", "t": "Prompt Engineering", "tg": ["Skill", "Practice"], "d": "general", "x": "The practice of crafting effective prompts to get better results from AI systems. Includes techniques, frameworks...", "l": "p", "k": ["prompt", "engineering", "practice", "crafting", "effective", "prompts", "better", "results", "systems", "includes", "techniques", "frameworks", "crisp", "costar", "iterative"]}, {"id": "term-prompt-engineering-emergence", "t": "Prompt Engineering Emergence", "tg": ["History", "Milestones"], "d": "history", "x": "The emergence of prompt engineering as a discipline for designing effective inputs to large language models. As LLMs...", "l": "p", "k": ["prompt", "engineering", "emergence", "discipline", "designing", "effective", "inputs", "large", "language", "models", "llms", "became", "capable", "art", "science"]}, {"id": "term-prompt-ensembling", "t": "Prompt Ensembling", "tg": ["Prompt Engineering", "Ensemble"], "d": "general", "x": "A strategy that runs multiple differently-phrased prompts for the same query and aggregates the outputs through voting,...", "l": "p", "k": ["prompt", "ensembling", "strategy", "runs", "multiple", "differently-phrased", "prompts", "query", "aggregates", "outputs", "voting", "averaging", "selection", "produce", "robust"]}, {"id": "term-prompt-extraction-attack", "t": "Prompt Extraction Attack", "tg": ["Prompt Engineering", "Security"], "d": "safety", "x": "A targeted attack technique that attempts to reconstruct or extract a model's system prompt, proprietary instructions,...", "l": "p", "k": ["prompt", "extraction", "attack", "targeted", "technique", "attempts", "reconstruct", "extract", "model", "system", "proprietary", "instructions", "confidential", "context", "systematic"]}, {"id": "term-prompt-injection", "t": "Prompt Injection", "tg": ["Security", "Risk"], "d": "safety", "x": "A security vulnerability where malicious instructions hidden in content cause AI to behave unexpectedly. A significant...", "l": "p", "k": ["prompt", "injection", "security", "vulnerability", "malicious", "instructions", "hidden", "content", "cause", "behave", "unexpectedly", "significant", "concern", "applications", "processing"]}, {"id": "term-prompt-leaking", "t": "Prompt Leaking", "tg": ["Prompt Engineering", "Security"], "d": "safety", "x": "A security vulnerability where an attacker manipulates a language model into revealing its hidden system prompt or...", "l": "p", "k": ["prompt", "leaking", "security", "vulnerability", "attacker", "manipulates", "language", "model", "revealing", "hidden", "system", "confidential", "instructions", "carefully", "crafted"]}, {"id": "term-prompt-optimization", "t": "Prompt Optimization", "tg": ["Prompt Engineering", "Optimization"], "d": "algorithms", "x": "The systematic process of refining prompt text, structure, and parameters to maximize model performance on a target...", "l": "p", "k": ["prompt", "optimization", "systematic", "process", "refining", "text", "structure", "parameters", "maximize", "model", "performance", "target", "task", "employing", "techniques"]}, {"id": "term-prompt-robustness", "t": "Prompt Robustness", "tg": ["Prompt Engineering", "Robustness"], "d": "general", "x": "The ability of a prompt to maintain consistent model performance across variations in input phrasing, perturbations,...", "l": "p", "k": ["prompt", "robustness", "ability", "maintain", "consistent", "model", "performance", "across", "variations", "input", "phrasing", "perturbations", "edge", "cases", "indicating"]}, {"id": "term-prompt-sensitivity", "t": "Prompt Sensitivity", "tg": ["Prompt Engineering", "Robustness"], "d": "general", "x": "The degree to which a model's output quality and correctness varies in response to minor changes in prompt wording,...", "l": "p", "k": ["prompt", "sensitivity", "degree", "model", "output", "quality", "correctness", "varies", "response", "minor", "changes", "wording", "formatting", "example", "ordering"]}, {"id": "term-prompt-template", "t": "Prompt Template", "tg": ["Pattern", "Reusable"], "d": "general", "x": "A reusable prompt structure with placeholders for variable content. Enables consistent, repeatable interactions and is...", "l": "p", "k": ["prompt", "template", "reusable", "structure", "placeholders", "variable", "content", "enables", "consistent", "repeatable", "interactions", "essential", "building", "ai-powered", "applications"]}, {"id": "term-prompt-templating", "t": "Prompt Templating", "tg": ["Prompt Engineering", "Infrastructure"], "d": "hardware", "x": "The practice of creating reusable prompt structures with placeholder variables that can be dynamically filled with...", "l": "p", "k": ["prompt", "templating", "practice", "creating", "reusable", "structures", "placeholder", "variables", "dynamically", "filled", "specific", "inputs", "runtime", "enabling", "consistent"]}, {"id": "term-prompt-tuning", "t": "Prompt Tuning", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A parameter-efficient method that prepends learnable continuous embeddings (soft prompts) to the input while keeping...", "l": "p", "k": ["prompt", "tuning", "parameter-efficient", "method", "prepends", "learnable", "continuous", "embeddings", "soft", "prompts", "input", "keeping", "model", "parameters", "frozen"]}, {"id": "term-prompt-versioning", "t": "Prompt Versioning", "tg": ["Prompt Engineering", "Infrastructure"], "d": "hardware", "x": "The practice of maintaining version-controlled prompt templates with change tracking, performance baselines, and...", "l": "p", "k": ["prompt", "versioning", "practice", "maintaining", "version-controlled", "templates", "change", "tracking", "performance", "baselines", "rollback", "capabilities", "treating", "prompts", "critical"]}, {"id": "term-pronoun-resolution", "t": "Pronoun Resolution", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The specific task of determining which entity a pronoun refers to in context, requiring understanding of gender,...", "l": "p", "k": ["pronoun", "resolution", "specific", "task", "determining", "entity", "refers", "context", "requiring", "understanding", "gender", "number", "syntactic", "position", "semantic"]}, {"id": "term-propbank", "t": "PropBank", "tg": ["NLP", "Linguistics"], "d": "general", "x": "Proposition Bank, a corpus annotated with predicate-argument structures for verbs, providing semantic role labels that...", "l": "p", "k": ["propbank", "proposition", "bank", "corpus", "annotated", "predicate-argument", "structures", "verbs", "providing", "semantic", "role", "labels", "facilitate", "training", "evaluation"]}, {"id": "term-propensity-score", "t": "Propensity Score", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The probability that a unit is assigned to a particular treatment given its observed covariates. It is used in causal...", "l": "p", "k": ["propensity", "score", "probability", "unit", "assigned", "particular", "treatment", "given", "observed", "covariates", "causal", "inference", "balance", "control", "groups"]}, {"id": "term-prospector", "t": "PROSPECTOR", "tg": ["History", "Systems"], "d": "history", "x": "An expert system developed at SRI International in the late 1970s for mineral exploration. PROSPECTOR used Bayesian...", "l": "p", "k": ["prospector", "expert", "system", "developed", "sri", "international", "late", "1970s", "mineral", "exploration", "bayesian", "probability", "networks", "evaluate", "geological"]}, {"id": "term-protected-attributes", "t": "Protected Attributes", "tg": ["Fairness", "Regulation"], "d": "safety", "x": "Characteristics such as race, gender, age, religion, and disability status that are legally or ethically designated as...", "l": "p", "k": ["protected", "attributes", "characteristics", "race", "gender", "age", "religion", "disability", "status", "legally", "ethically", "designated", "bases", "upon", "differential"]}, {"id": "term-proximal-gradient-method", "t": "Proximal Gradient Method", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An optimization algorithm for minimizing composite objective functions consisting of a smooth term and a non-smooth...", "l": "p", "k": ["proximal", "gradient", "method", "optimization", "algorithm", "minimizing", "composite", "objective", "functions", "consisting", "smooth", "term", "non-smooth", "regularizer", "combines"]}, {"id": "term-proxy-discrimination", "t": "Proxy Discrimination", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Discrimination that occurs when an AI system uses features that are correlated with protected attributes as proxies,...", "l": "p", "k": ["proxy", "discrimination", "occurs", "system", "uses", "features", "correlated", "protected", "attributes", "proxies", "achieving", "discriminatory", "outcomes", "explicitly", "excluded"]}, {"id": "term-pruning", "t": "Pruning", "tg": ["Optimization", "Efficiency"], "d": "algorithms", "x": "Removing unnecessary weights or neurons from neural networks to reduce size and increase speed. Can dramatically...", "l": "p", "k": ["pruning", "removing", "unnecessary", "weights", "neurons", "neural", "networks", "reduce", "size", "increase", "speed", "dramatically", "decrease", "model", "minimal"]}, {"id": "term-pruning-at-initialization", "t": "Pruning-at-Initialization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "Techniques that identify and remove redundant weights before any training occurs, based on signal propagation or...", "l": "p", "k": ["pruning-at-initialization", "techniques", "identify", "remove", "redundant", "weights", "training", "occurs", "based", "signal", "propagation", "gradient", "flow", "analysis", "methods"]}, {"id": "term-pseudo-labeling", "t": "Pseudo-Labeling", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A semi-supervised learning technique where a model trained on labeled data generates predictions for unlabeled data and...", "l": "p", "k": ["pseudo-labeling", "semi-supervised", "learning", "technique", "model", "trained", "labeled", "data", "generates", "predictions", "unlabeled", "uses", "high-confidence", "additional", "training"]}, {"id": "term-psnr", "t": "PSNR", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Peak Signal-to-Noise Ratio measures image quality by comparing the maximum possible pixel value to the mean squared...", "l": "p", "k": ["psnr", "peak", "signal-to-noise", "ratio", "measures", "image", "quality", "comparing", "maximum", "possible", "pixel", "value", "mean", "squared", "error"]}, {"id": "term-pythia", "t": "Pythia", "tg": ["Models", "Technical"], "d": "models", "x": "A suite of language models by EleutherAI ranging from 70M to 12B parameters all trained on exactly the same data in the...", "l": "p", "k": ["pythia", "suite", "language", "models", "eleutherai", "ranging", "70m", "12b", "parameters", "trained", "exactly", "data", "order", "designed", "studying"]}, {"id": "term-pytorch", "t": "PyTorch", "tg": ["Framework", "Deep Learning"], "d": "models", "x": "A popular open-source deep learning framework from Meta, known for its flexibility and Pythonic design. The dominant...", "l": "p", "k": ["pytorch", "popular", "open-source", "deep", "learning", "framework", "meta", "known", "flexibility", "pythonic", "design", "dominant", "research", "increasingly", "production"]}, {"id": "term-pytorch-release", "t": "PyTorch Release", "tg": ["History", "Milestones"], "d": "history", "x": "The release of PyTorch by Facebook AI Research in October 2016. With its dynamic computational graphs and Pythonic...", "l": "p", "k": ["pytorch", "release", "facebook", "research", "october", "dynamic", "computational", "graphs", "pythonic", "design", "became", "preferred", "framework", "ease", "flexibility"]}, {"id": "term-q-function", "t": "Q-Function", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "The action-value function Q(s,a) that estimates the expected cumulative reward of taking action a in state s and then...", "l": "q", "k": ["q-function", "action-value", "function", "estimates", "expected", "cumulative", "reward", "taking", "action", "state", "following", "given", "policy", "q-functions", "enable"]}, {"id": "term-q-learning", "t": "Q-Learning", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An off-policy temporal difference algorithm that learns the optimal action-value function Q* by iteratively updating...", "l": "q", "k": ["q-learning", "off-policy", "temporal", "difference", "algorithm", "learns", "optimal", "action-value", "function", "iteratively", "updating", "q-values", "bellman", "optimality", "equation"]}, {"id": "term-qdrant", "t": "Qdrant", "tg": ["Vector Database", "Open Source"], "d": "general", "x": "An open-source vector similarity search engine written in Rust that provides filtering, payload storage, and...", "l": "q", "k": ["qdrant", "open-source", "vector", "similarity", "search", "engine", "written", "rust", "provides", "filtering", "payload", "storage", "distributed", "deployment", "capabilities"]}, {"id": "term-qlora", "t": "QLoRA (Quantized LoRA)", "tg": ["Training", "Efficiency"], "d": "general", "x": "A technique combining quantization with LoRA fine-tuning. Enables fine-tuning large models on consumer GPUs by using...", "l": "q", "k": ["qlora", "quantized", "lora", "technique", "combining", "quantization", "fine-tuning", "enables", "large", "models", "consumer", "gpus", "4-bit", "base"]}, {"id": "term-qmix", "t": "QMIX", "tg": ["Reinforcement Learning", "Multi-Agent"], "d": "general", "x": "A multi-agent RL algorithm that factorizes the joint action-value function as a monotonic combination of per-agent...", "l": "q", "k": ["qmix", "multi-agent", "algorithm", "factorizes", "joint", "action-value", "function", "monotonic", "combination", "per-agent", "utilities", "mixing", "network", "enables", "centralized"]}, {"id": "term-qr-decomposition", "t": "QR Decomposition", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A matrix factorization that expresses a matrix as the product of an orthogonal matrix Q and an upper triangular matrix...", "l": "q", "k": ["decomposition", "matrix", "factorization", "expresses", "product", "orthogonal", "upper", "triangular", "solving", "least", "squares", "problems", "computing", "eigenvalues", "stabilizing"]}, {"id": "term-qualcomm-ai-engine", "t": "Qualcomm AI Engine", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "Qualcomm's heterogeneous AI compute platform that coordinates the Hexagon DSP, Adreno GPU, and Kryo CPU within...", "l": "q", "k": ["qualcomm", "engine", "heterogeneous", "compute", "platform", "coordinates", "hexagon", "dsp", "adreno", "gpu", "kryo", "cpu", "within", "snapdragon", "processors"]}, {"id": "term-qualitative-reasoning", "t": "Qualitative Reasoning", "tg": ["History", "Fundamentals"], "d": "history", "x": "An approach to modeling and reasoning about continuous systems using qualitative rather than numerical descriptions....", "l": "q", "k": ["qualitative", "reasoning", "approach", "modeling", "continuous", "systems", "rather", "numerical", "descriptions", "developed", "researchers", "including", "benjamin", "kuipers", "kenneth"]}, {"id": "term-quantile-regression", "t": "Quantile Regression", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A regression method that estimates conditional quantiles (such as the median or 90th percentile) of the response...", "l": "q", "k": ["quantile", "regression", "method", "estimates", "conditional", "quantiles", "median", "90th", "percentile", "response", "variable", "rather", "mean", "providing", "complete"]}, {"id": "term-quantile-quantile-plot", "t": "Quantile-Quantile Plot", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A graphical tool for comparing two probability distributions by plotting their quantiles against each other. It is...", "l": "q", "k": ["quantile-quantile", "plot", "graphical", "tool", "comparing", "probability", "distributions", "plotting", "quantiles", "against", "commonly", "assess", "dataset", "follows", "theoretical"]}, {"id": "term-quantization", "t": "Quantization", "tg": ["Optimization", "Deployment"], "d": "algorithms", "x": "Reducing the precision of model weights (e.g., from 16-bit to 4-bit) to decrease memory usage and increase speed....", "l": "q", "k": ["quantization", "reducing", "precision", "model", "weights", "16-bit", "4-bit", "decrease", "memory", "usage", "increase", "speed", "enables", "running", "larger"]}, {"id": "term-quantization-aware-training", "t": "Quantization-Aware Training (QAT)", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A training technique that simulates the effects of quantization during the forward pass while maintaining...", "l": "q", "k": ["quantization-aware", "training", "qat", "technique", "simulates", "effects", "quantization", "forward", "pass", "maintaining", "full-precision", "gradients", "backpropagation", "produces", "models"]}, {"id": "term-query", "t": "Query", "tg": ["Concept", "Dual Meaning"], "d": "general", "x": "In RAG/search: the user's question or search terms. In attention: one of three vectors (query, key, value) used to...", "l": "q", "k": ["query", "rag", "search", "user", "question", "terms", "attention", "vectors", "key", "value", "compute", "weights"]}, {"id": "term-query-decomposition", "t": "Query Decomposition", "tg": ["Retrieval", "Query Processing"], "d": "general", "x": "A retrieval strategy that breaks a complex multi-faceted query into simpler sub-queries, retrieves results for each...", "l": "q", "k": ["query", "decomposition", "retrieval", "strategy", "breaks", "complex", "multi-faceted", "simpler", "sub-queries", "retrieves", "results", "independently", "merges", "address", "questions"]}, {"id": "term-query-expansion", "t": "Query Expansion", "tg": ["Retrieval", "Query Processing"], "d": "general", "x": "A retrieval technique that augments the original query with additional related terms, synonyms, or reformulations to...", "l": "q", "k": ["query", "expansion", "retrieval", "technique", "augments", "original", "additional", "related", "terms", "synonyms", "reformulations", "improve", "recall", "bridging", "vocabulary"]}, {"id": "term-question-answering", "t": "Question Answering (QA)", "tg": ["NLP Task", "Application"], "d": "general", "x": "An NLP task where the model answers questions based on provided context or its knowledge. Includes extractive QA...", "l": "q", "k": ["question", "answering", "nlp", "task", "model", "answers", "questions", "based", "provided", "context", "knowledge", "includes", "extractive", "finding", "text"]}, {"id": "term-qwen", "t": "Qwen", "tg": ["Models", "Technical"], "d": "models", "x": "A family of large language models developed by Alibaba Cloud. Available in multiple sizes with strong multilingual...", "l": "q", "k": ["qwen", "family", "large", "language", "models", "developed", "alibaba", "cloud", "available", "multiple", "sizes", "strong", "multilingual", "capabilities", "particularly"]}, {"id": "term-r-squared", "t": "R-Squared", "tg": ["Statistics", "Metrics"], "d": "datasets", "x": "A statistical measure indicating the proportion of variance in the dependent variable that is explained by the...", "l": "r", "k": ["r-squared", "statistical", "measure", "indicating", "proportion", "variance", "dependent", "variable", "explained", "independent", "variables", "regression", "model", "values", "range"]}, {"id": "term-r1-xcon", "t": "R1/XCON", "tg": ["History", "Milestones"], "d": "history", "x": "An expert system developed by John McDermott at Carnegie Mellon in 1980 for configuring VAX computer orders at Digital...", "l": "r", "k": ["xcon", "expert", "system", "developed", "john", "mcdermott", "carnegie", "mellon", "configuring", "vax", "computer", "orders", "digital", "equipment", "corporation"]}, {"id": "term-race-to-the-bottom-ai-safety", "t": "Race to the Bottom in AI Safety", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "The concern that competitive pressures among AI developers lead to progressively lower safety standards, as...", "l": "r", "k": ["race", "bottom", "safety", "concern", "competitive", "pressures", "among", "developers", "lead", "progressively", "lower", "standards", "organizations", "cut", "corners"]}, {"id": "term-radam", "t": "RAdam", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Rectified Adam addresses the large variance in early training steps of the Adam optimizer by introducing a variance...", "l": "r", "k": ["radam", "rectified", "adam", "addresses", "large", "variance", "early", "training", "steps", "optimizer", "introducing", "rectification", "term", "automatically", "adapts"]}, {"id": "term-rademacher-complexity", "t": "Rademacher Complexity", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A measure of the richness of a hypothesis class that quantifies how well functions in the class can fit random noise....", "l": "r", "k": ["rademacher", "complexity", "measure", "richness", "hypothesis", "class", "quantifies", "functions", "fit", "random", "noise", "provides", "tighter", "generalization", "bounds"]}, {"id": "term-radial-basis-function-kernel", "t": "Radial Basis Function Kernel", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A popular kernel function that measures similarity as an exponentially decaying function of the squared Euclidean...", "l": "r", "k": ["radial", "basis", "function", "kernel", "popular", "measures", "similarity", "exponentially", "decaying", "squared", "euclidean", "distance", "points", "bandwidth", "parameter"]}, {"id": "term-raft-optical-flow", "t": "RAFT", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Recurrent All-Pairs Field Transforms, a deep learning architecture for optical flow estimation that uses 4D correlation...", "l": "r", "k": ["raft", "recurrent", "all-pairs", "field", "transforms", "deep", "learning", "architecture", "optical", "flow", "estimation", "uses", "correlation", "volumes", "gru-based"]}, {"id": "term-rag", "t": "RAG (Retrieval-Augmented Generation)", "tg": ["Architecture", "Accuracy"], "d": "models", "x": "A technique that combines AI generation with information retrieval from external sources. The model retrieves relevant...", "l": "r", "k": ["rag", "retrieval-augmented", "generation", "technique", "combines", "information", "retrieval", "external", "sources", "model", "retrieves", "relevant", "documents", "uses", "generate"]}, {"id": "term-ragas", "t": "RAGAS", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "Retrieval Augmented Generation Assessment, an evaluation framework that provides reference-free metrics for RAG...", "l": "r", "k": ["ragas", "retrieval", "augmented", "generation", "assessment", "evaluation", "framework", "provides", "reference-free", "metrics", "rag", "pipelines", "including", "faithfulness", "answer"]}, {"id": "term-rainbow-dqn", "t": "Rainbow DQN", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An integrated DQN agent that combines six extensions: double Q-learning, prioritized replay, dueling architecture,...", "l": "r", "k": ["rainbow", "dqn", "integrated", "agent", "combines", "six", "extensions", "double", "q-learning", "prioritized", "replay", "dueling", "architecture", "multi-step", "returns"]}, {"id": "term-raj-reddy", "t": "Raj Reddy", "tg": ["History", "Pioneers"], "d": "history", "x": "Indian-American computer scientist who received the Turing Award in 1994 for pioneering work in speech recognition and...", "l": "r", "k": ["raj", "reddy", "indian-american", "computer", "scientist", "received", "turing", "award", "pioneering", "work", "speech", "recognition", "co-founded", "robotics", "institute"]}, {"id": "term-randaugment", "t": "RandAugment", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A simplified automated augmentation strategy that randomly applies a fixed number of augmentation operations from a...", "l": "r", "k": ["randaugment", "simplified", "automated", "augmentation", "strategy", "randomly", "applies", "fixed", "number", "operations", "predefined", "shared", "magnitude", "requiring", "hyperparameters"]}, {"id": "term-random-erasing", "t": "Random Erasing", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A data augmentation technique that randomly selects rectangular regions in training images and replaces their pixels...", "l": "r", "k": ["random", "erasing", "data", "augmentation", "technique", "randomly", "selects", "rectangular", "regions", "training", "images", "replaces", "pixels", "values", "mean"]}, {"id": "term-random-forest", "t": "Random Forest", "tg": ["Algorithm", "ML"], "d": "algorithms", "x": "An ensemble of decision trees that vote on predictions. Robust, interpretable, and works well on tabular data. Still...", "l": "r", "k": ["random", "forest", "ensemble", "decision", "trees", "vote", "predictions", "robust", "interpretable", "works", "tabular", "data", "widely", "despite", "deep"]}, {"id": "term-random-forest-history", "t": "Random Forest History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of random forest ensemble methods by Leo Breiman in 2001 combining bagging with random feature...", "l": "r", "k": ["random", "forest", "history", "development", "ensemble", "methods", "leo", "breiman", "combining", "bagging", "feature", "selection", "forests", "proved", "remarkably"]}, {"id": "term-random-projection", "t": "Random Projection", "tg": ["Vector Database", "Dimensionality Reduction"], "d": "general", "x": "A dimensionality reduction technique based on the Johnson-Lindenstrauss lemma that projects high-dimensional vectors...", "l": "r", "k": ["random", "projection", "dimensionality", "reduction", "technique", "based", "johnson-lindenstrauss", "lemma", "projects", "high-dimensional", "vectors", "onto", "lower-dimensional", "space", "matrices"]}, {"id": "term-random-search", "t": "Random Search", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A hyperparameter tuning strategy that samples parameter combinations randomly from specified distributions, often...", "l": "r", "k": ["random", "search", "hyperparameter", "tuning", "strategy", "samples", "parameter", "combinations", "randomly", "specified", "distributions", "finding", "good", "configurations", "efficiently"]}, {"id": "term-random-search-for-hyperparameters", "t": "Random Search for Hyperparameters", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A hyperparameter optimization method that samples configurations randomly from specified distributions. Shown by...", "l": "r", "k": ["random", "search", "hyperparameters", "hyperparameter", "optimization", "method", "samples", "configurations", "randomly", "specified", "distributions", "shown", "bergstra", "bengio", "efficient"]}, {"id": "term-random-walk", "t": "Random Walk", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A stochastic process consisting of successive random steps on a graph or in a space. Used in graph algorithms like...", "l": "r", "k": ["random", "walk", "stochastic", "process", "consisting", "successive", "steps", "graph", "space", "algorithms", "deepwalk", "node2vec", "learning", "node", "embeddings"]}, {"id": "term-rate-limit", "t": "Rate Limit", "tg": ["API", "Technical"], "d": "general", "x": "Restrictions on API usage, typically measured in requests per minute or tokens per minute. Prevents abuse and ensures...", "l": "r", "k": ["rate", "limit", "restrictions", "api", "usage", "typically", "measured", "requests", "per", "minute", "tokens", "prevents", "abuse", "ensures", "fair"]}, {"id": "term-ray-kurzweil", "t": "Ray Kurzweil", "tg": ["History", "Pioneers"], "d": "history", "x": "American inventor, author, and futurist who popularized the concept of the technological singularity, predicted...", "l": "r", "k": ["ray", "kurzweil", "american", "inventor", "author", "futurist", "popularized", "concept", "technological", "singularity", "predicted", "accelerating", "returns", "technology", "joined"]}, {"id": "term-rdma", "t": "RDMA", "tg": ["Distributed Computing", "Hardware"], "d": "hardware", "x": "Remote Direct Memory Access, a networking technology that enables direct data transfer between GPU memory on different...", "l": "r", "k": ["rdma", "remote", "direct", "memory", "access", "networking", "technology", "enables", "data", "transfer", "gpu", "different", "nodes", "without", "cpu"]}, {"id": "term-re-identification", "t": "Re-Identification", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of matching the same person or vehicle across different camera views or time periods by learning...", "l": "r", "k": ["re-identification", "task", "matching", "person", "vehicle", "across", "different", "camera", "views", "time", "periods", "learning", "discriminative", "appearance", "embeddings"]}, {"id": "term-re-ranking", "t": "Re-Ranking", "tg": ["Retrieval", "Ranking"], "d": "general", "x": "A second-stage retrieval process that applies a more computationally expensive model to re-score and reorder an initial...", "l": "r", "k": ["re-ranking", "second-stage", "retrieval", "process", "applies", "computationally", "expensive", "model", "re-score", "reorder", "initial", "retrieved", "candidates", "improving", "precision"]}, {"id": "term-react", "t": "ReAct (Reasoning + Acting)", "tg": ["Framework", "Reasoning"], "d": "general", "x": "A prompting framework combining Reasoning and Acting. AI thinks through problems step-by-step, showing its reasoning...", "l": "r", "k": ["react", "reasoning", "acting", "prompting", "framework", "combining", "thinks", "problems", "step-by-step", "showing", "process", "transparently", "taking", "actions"]}, {"id": "term-react-pattern", "t": "ReAct Pattern", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A prompting paradigm that interleaves reasoning traces and action steps, allowing a language model to dynamically plan,...", "l": "r", "k": ["react", "pattern", "prompting", "paradigm", "interleaves", "reasoning", "traces", "action", "steps", "allowing", "language", "model", "dynamically", "plan", "execute"]}, {"id": "term-reasoning", "t": "Reasoning (AI)", "tg": ["Capability", "Prompting"], "d": "general", "x": "AI's ability to draw logical conclusions, follow multi-step chains, and solve complex problems. A key capability that...", "l": "r", "k": ["reasoning", "ability", "draw", "logical", "conclusions", "follow", "multi-step", "chains", "solve", "complex", "problems", "key", "capability", "distinguishes", "modern"]}, {"id": "term-recall", "t": "Recall", "tg": ["Metrics", "Evaluation"], "d": "datasets", "x": "A metric measuring the proportion of actual positives correctly identified. Important in search and information...", "l": "r", "k": ["recall", "metric", "measuring", "proportion", "actual", "positives", "correctly", "identified", "important", "search", "information", "retrieval", "missing", "relevant", "results"]}, {"id": "term-recall-at-k", "t": "Recall at K", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A retrieval metric that measures the proportion of all relevant documents in the corpus that appear within the top K...", "l": "r", "k": ["recall", "retrieval", "metric", "measures", "proportion", "relevant", "documents", "corpus", "appear", "within", "top", "retrieved", "results", "indicating", "comprehensively"]}, {"id": "term-recall-at-k-retrieval", "t": "Recall at K for Retrieval", "tg": ["Retrieval", "Evaluation"], "d": "datasets", "x": "A retrieval-specific metric measuring the proportion of all relevant documents that appear within the top K results...", "l": "r", "k": ["recall", "retrieval", "retrieval-specific", "metric", "measuring", "proportion", "relevant", "documents", "appear", "within", "top", "results", "returned", "vector", "search"]}, {"id": "term-receiver-operating-characteristic", "t": "Receiver Operating Characteristic", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A graphical analysis technique that plots classifier performance across all possible decision thresholds, showing the...", "l": "r", "k": ["receiver", "operating", "characteristic", "graphical", "analysis", "technique", "plots", "classifier", "performance", "across", "possible", "decision", "thresholds", "showing", "tradeoff"]}, {"id": "term-receptive-field", "t": "Receptive Field", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The region of the original input image that influences a particular neuron's activation in a deeper layer, growing...", "l": "r", "k": ["receptive", "field", "region", "original", "input", "image", "influences", "particular", "neuron", "activation", "deeper", "layer", "growing", "larger", "successive"]}, {"id": "term-reciprocal-rank-fusion", "t": "Reciprocal Rank Fusion", "tg": ["Retrieval", "Ranking"], "d": "general", "x": "A rank aggregation method that combines result lists from multiple retrieval systems by assigning each document a score...", "l": "r", "k": ["reciprocal", "rank", "fusion", "aggregation", "method", "combines", "result", "lists", "multiple", "retrieval", "systems", "assigning", "document", "score", "based"]}, {"id": "term-reconstruction-loss", "t": "Reconstruction Loss", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A loss function that measures how well a model can reconstruct its input from a compressed or encoded representation....", "l": "r", "k": ["reconstruction", "loss", "function", "measures", "model", "reconstruct", "input", "compressed", "encoded", "representation", "autoencoders", "variational", "implemented", "mse", "continuous"]}, {"id": "term-rectified-flow", "t": "Rectified Flow", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative modeling approach that learns straight-line paths between noise and data distributions, enabling faster...", "l": "r", "k": ["rectified", "flow", "generative", "modeling", "approach", "learns", "straight-line", "paths", "noise", "data", "distributions", "enabling", "faster", "sampling", "curved"]}, {"id": "term-recurrent-policy", "t": "Recurrent Policy", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "An RL policy that uses recurrent neural network components (LSTM, GRU) to maintain internal memory across time steps....", "l": "r", "k": ["recurrent", "policy", "uses", "neural", "network", "components", "lstm", "gru", "maintain", "internal", "memory", "across", "time", "steps", "policies"]}, {"id": "term-recursive-character-splitting", "t": "Recursive Character Splitting", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "A document chunking strategy that attempts to split text using a hierarchy of separators from paragraph breaks down to...", "l": "r", "k": ["recursive", "character", "splitting", "document", "chunking", "strategy", "attempts", "split", "text", "hierarchy", "separators", "paragraph", "breaks", "down", "individual"]}, {"id": "term-recursive-feature-elimination", "t": "Recursive Feature Elimination", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A feature selection method that repeatedly trains a model, ranks features by importance, and removes the least...", "l": "r", "k": ["recursive", "feature", "elimination", "selection", "method", "repeatedly", "trains", "model", "ranks", "features", "importance", "removes", "least", "important", "iterating"]}, {"id": "term-recursive-prompting", "t": "Recursive Prompting", "tg": ["Prompt Engineering", "Architecture"], "d": "models", "x": "A prompting pattern where the output of one prompt call is used to construct the next prompt in a recursive loop,...", "l": "r", "k": ["recursive", "prompting", "pattern", "output", "prompt", "call", "construct", "next", "loop", "enabling", "model", "handle", "arbitrarily", "complex", "tasks"]}, {"id": "term-red-team", "t": "Red Team", "tg": ["Safety", "Testing"], "d": "safety", "x": "A group that tests AI systems by attempting to find vulnerabilities, bypass safety measures, or elicit harmful outputs....", "l": "r", "k": ["red", "team", "group", "tests", "systems", "attempting", "find", "vulnerabilities", "bypass", "safety", "measures", "elicit", "harmful", "outputs", "essential"]}, {"id": "term-red-teaming-in-ai", "t": "Red Teaming in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "The practice of systematically testing AI systems by attempting to elicit harmful undesired or unsafe outputs. Red...", "l": "r", "k": ["red", "teaming", "practice", "systematically", "testing", "systems", "attempting", "elicit", "harmful", "undesired", "unsafe", "outputs", "become", "standard", "safety"]}, {"id": "term-redpajama", "t": "RedPajama", "tg": ["Models", "Technical"], "d": "models", "x": "An open-source effort to reproduce the LLaMA training dataset and model. Provides a fully open training pipeline...", "l": "r", "k": ["redpajama", "open-source", "effort", "reproduce", "llama", "training", "dataset", "model", "provides", "fully", "open", "pipeline", "including", "trillion", "token"]}, {"id": "term-reduce-on-plateau", "t": "Reduce on Plateau", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A learning rate scheduling strategy that reduces the learning rate by a factor when a monitored metric stops improving...", "l": "r", "k": ["reduce", "plateau", "learning", "rate", "scheduling", "strategy", "reduces", "factor", "monitored", "metric", "stops", "improving", "specified", "number", "epochs"]}, {"id": "term-reduce-scatter", "t": "Reduce-Scatter Operation", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A collective communication pattern that reduces data across all participants and distributes different chunks of the...", "l": "r", "k": ["reduce-scatter", "operation", "collective", "communication", "pattern", "reduces", "data", "across", "participants", "distributes", "different", "chunks", "result", "participant", "zero"]}, {"id": "term-reflexion-pattern", "t": "Reflexion Pattern", "tg": ["LLM", "Generative AI"], "d": "models", "x": "An agent architecture where the LLM reflects on previous failed attempts by storing verbal feedback in an episodic...", "l": "r", "k": ["reflexion", "pattern", "agent", "architecture", "llm", "reflects", "previous", "failed", "attempts", "storing", "verbal", "feedback", "episodic", "memory", "reflections"]}, {"id": "term-reformer", "t": "Reformer", "tg": ["Models", "Technical"], "d": "models", "x": "A transformer variant that uses locality-sensitive hashing to reduce attention complexity from O(n^2) to O(n log n) and...", "l": "r", "k": ["reformer", "transformer", "variant", "uses", "locality-sensitive", "hashing", "reduce", "attention", "complexity", "log", "reversible", "residual", "layers", "memory", "usage"]}, {"id": "term-refusal", "t": "Refusal", "tg": ["Safety", "Behavior"], "d": "safety", "x": "When AI declines to answer a request due to safety guidelines. Well-calibrated refusals protect against harm while...", "l": "r", "k": ["refusal", "declines", "answer", "request", "due", "safety", "guidelines", "well-calibrated", "refusals", "protect", "against", "harm", "overly", "cautious", "reduce"]}, {"id": "term-region-proposal-network", "t": "Region Proposal Network", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A fully convolutional network that slides over feature maps to generate object proposals (candidate bounding boxes)...", "l": "r", "k": ["region", "proposal", "network", "fully", "convolutional", "slides", "feature", "maps", "generate", "object", "proposals", "candidate", "bounding", "boxes", "objectness"]}, {"id": "term-regnet", "t": "RegNet", "tg": ["Models", "Technical"], "d": "models", "x": "A family of network architectures derived from a structured design space that constrains network parameters to follow...", "l": "r", "k": ["regnet", "family", "network", "architectures", "derived", "structured", "design", "space", "constrains", "parameters", "follow", "simple", "linear", "rules", "provides"]}, {"id": "term-regression", "t": "Regression", "tg": ["ML Task", "Prediction"], "d": "general", "x": "A machine learning task that predicts continuous values (like prices or temperatures) rather than categories. Common...", "l": "r", "k": ["regression", "machine", "learning", "task", "predicts", "continuous", "values", "prices", "temperatures", "rather", "categories", "common", "algorithms", "include", "linear"]}, {"id": "term-regret", "t": "Regret", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "In online learning and bandit problems, the cumulative difference between the reward obtained by an algorithm and the...", "l": "r", "k": ["regret", "online", "learning", "bandit", "problems", "cumulative", "difference", "reward", "obtained", "algorithm", "always", "choosing", "optimal", "action", "hindsight"]}, {"id": "term-regret-bound", "t": "Regret Bound", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "A theoretical guarantee on the cumulative difference between the reward obtained by an RL algorithm and the reward of...", "l": "r", "k": ["regret", "bound", "theoretical", "guarantee", "cumulative", "difference", "reward", "obtained", "algorithm", "optimal", "policy", "steps", "bounds", "characterize", "efficiency"]}, {"id": "term-regularization", "t": "Regularization", "tg": ["Training", "Technique"], "d": "general", "x": "Techniques to prevent overfitting by adding constraints during training. Includes dropout, weight decay, and early...", "l": "r", "k": ["regularization", "techniques", "prevent", "overfitting", "adding", "constraints", "training", "includes", "dropout", "weight", "decay", "early", "stopping", "improves", "generalization"]}, {"id": "term-reinforce-algorithm", "t": "REINFORCE Algorithm", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A foundational Monte Carlo policy gradient algorithm that updates policy parameters proportionally to the return...", "l": "r", "k": ["reinforce", "algorithm", "foundational", "monte", "carlo", "policy", "gradient", "updates", "parameters", "proportionally", "return", "multiplied", "log-probability", "action", "taken"]}, {"id": "term-reinforcement-learning", "t": "Reinforcement Learning (RL)", "tg": ["Learning Type", "Training"], "d": "general", "x": "A learning paradigm where agents learn by receiving rewards or penalties for actions. Used in RLHF to align LLMs with...", "l": "r", "k": ["reinforcement", "learning", "paradigm", "agents", "learn", "receiving", "rewards", "penalties", "actions", "rlhf", "align", "llms", "human", "preferences"]}, {"id": "term-reinforcement-learning-history", "t": "Reinforcement Learning History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of reinforcement learning from early work by Arthur Samuel on checkers in 1959 through temporal...", "l": "r", "k": ["reinforcement", "learning", "history", "development", "early", "work", "arthur", "samuel", "checkers", "temporal", "difference", "sutton", "deep", "breakthroughs", "dqn"]}, {"id": "term-rejection-sampling", "t": "Rejection Sampling", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A basic Monte Carlo method for generating samples from a target distribution by sampling from a proposal distribution...", "l": "r", "k": ["rejection", "sampling", "basic", "monte", "carlo", "method", "generating", "samples", "target", "distribution", "proposal", "accepting", "rejecting", "based", "comparison"]}, {"id": "term-rejection-sampling-for-alignment", "t": "Rejection Sampling for Alignment", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A training data curation technique that generates multiple responses per prompt and keeps only those above a reward...", "l": "r", "k": ["rejection", "sampling", "alignment", "training", "data", "curation", "technique", "generates", "multiple", "responses", "per", "prompt", "keeps", "reward", "threshold"]}, {"id": "term-relabeling", "t": "Relabeling", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A data augmentation technique in RL that modifies components of stored transitions (such as goals, rewards, or actions)...", "l": "r", "k": ["relabeling", "data", "augmentation", "technique", "modifies", "components", "stored", "transitions", "goals", "rewards", "actions", "generate", "additional", "training", "signal"]}, {"id": "term-relation-extraction", "t": "Relation Extraction", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying and classifying semantic relationships between entities mentioned in text, such as extracting...", "l": "r", "k": ["relation", "extraction", "task", "identifying", "classifying", "semantic", "relationships", "entities", "mentioned", "text", "extracting", "person", "works", "specific", "organization"]}, {"id": "term-relative-position-encoding", "t": "Relative Position Encoding", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A position encoding scheme that represents the distance between tokens rather than their absolute positions. Allows the...", "l": "r", "k": ["relative", "position", "encoding", "scheme", "represents", "distance", "tokens", "rather", "absolute", "positions", "allows", "model", "generalize", "different", "sequence"]}, {"id": "term-relative-positional-encoding", "t": "Relative Positional Encoding", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A positional encoding scheme that encodes the relative distance between tokens rather than absolute positions, enabling...", "l": "r", "k": ["relative", "positional", "encoding", "scheme", "encodes", "distance", "tokens", "rather", "absolute", "positions", "enabling", "better", "generalization", "sequence", "lengths"]}, {"id": "term-relevance-score", "t": "Relevance Score", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that measures how well a generated response addresses the input query or matches the intended topic,...", "l": "r", "k": ["relevance", "score", "metric", "measures", "generated", "response", "addresses", "input", "query", "matches", "intended", "topic", "evaluating", "content", "appropriateness"]}, {"id": "term-relu", "t": "ReLU (Rectified Linear Unit)", "tg": ["Architecture", "Function"], "d": "models", "x": "A simple activation function that outputs zero for negative inputs and the input itself for positives. Widely used due...", "l": "r", "k": ["relu", "rectified", "linear", "unit", "simple", "activation", "function", "outputs", "zero", "negative", "inputs", "input", "itself", "positives", "widely"]}, {"id": "term-reparameterization-trick", "t": "Reparameterization Trick", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A technique that enables gradient-based optimization through stochastic sampling by expressing random variables as...", "l": "r", "k": ["reparameterization", "trick", "technique", "enables", "gradient-based", "optimization", "stochastic", "sampling", "expressing", "random", "variables", "deterministic", "functions", "parameters", "independent"]}, {"id": "term-repetition-penalty", "t": "Repetition Penalty", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "A decoding parameter that reduces the probability of tokens that have already appeared in the generated text,...", "l": "r", "k": ["repetition", "penalty", "decoding", "parameter", "reduces", "probability", "tokens", "appeared", "generated", "text", "preventing", "model", "producing", "repetitive", "phrases"]}, {"id": "term-repetition-rate", "t": "Repetition Rate", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that quantifies the frequency of repeated phrases, sentences, or patterns within generated text, used to...", "l": "r", "k": ["repetition", "rate", "metric", "quantifies", "frequency", "repeated", "phrases", "sentences", "patterns", "within", "generated", "text", "detect", "penalize", "degenerate"]}, {"id": "term-rephrase-and-respond", "t": "Rephrase and Respond", "tg": ["Prompt Engineering", "Clarification"], "d": "general", "x": "A prompting method that asks the model to first rephrase the input question in its own words before answering it,...", "l": "r", "k": ["rephrase", "respond", "prompting", "method", "asks", "model", "input", "question", "words", "answering", "improving", "comprehension", "reducing", "misinterpretation", "ensuring"]}, {"id": "term-replay-buffer", "t": "Replay Buffer", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A data structure (typically a fixed-size circular buffer) that stores past experience tuples for sampling during...", "l": "r", "k": ["replay", "buffer", "data", "structure", "typically", "fixed-size", "circular", "stores", "past", "experience", "tuples", "sampling", "off-policy", "training", "buffers"]}, {"id": "term-replication-vector-databases", "t": "Replication in Vector Databases", "tg": ["Vector Database", "Infrastructure"], "d": "hardware", "x": "The maintenance of multiple copies of a vector index across different nodes to provide fault tolerance and increased...", "l": "r", "k": ["replication", "vector", "databases", "maintenance", "multiple", "copies", "index", "across", "different", "nodes", "provide", "fault", "tolerance", "increased", "read"]}, {"id": "term-representation-engineering", "t": "Representation Engineering", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A technique for understanding and controlling LLM behavior by identifying and manipulating specific directions in the...", "l": "r", "k": ["representation", "engineering", "technique", "understanding", "controlling", "llm", "behavior", "identifying", "manipulating", "specific", "directions", "model", "activation", "space", "correspond"]}, {"id": "term-representation-learning", "t": "Representation Learning", "tg": ["Concept", "Deep Learning"], "d": "models", "x": "Learning useful features automatically from data rather than engineering them manually. A key strength of deep learning...", "l": "r", "k": ["representation", "learning", "useful", "features", "automatically", "data", "rather", "engineering", "manually", "key", "strength", "deep", "enables", "transfer"]}, {"id": "term-representation-learning-rl", "t": "Representation Learning in RL", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "Methods for learning compact, informative state representations from high-dimensional observations (like images) to...", "l": "r", "k": ["representation", "learning", "methods", "compact", "informative", "state", "representations", "high-dimensional", "observations", "images", "improve", "efficiency", "techniques", "include", "contrastive"]}, {"id": "term-representational-harm", "t": "Representational Harm", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "Harm that occurs when an AI system reinforces stereotypes, demeans, or erases particular social groups through its...", "l": "r", "k": ["representational", "harm", "occurs", "system", "reinforces", "stereotypes", "demeans", "erases", "particular", "social", "groups", "outputs", "direct", "resource", "allocation"]}, {"id": "term-request-scheduling", "t": "Request Scheduling", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The algorithm that determines the order and priority of processing incoming inference requests on limited GPU...", "l": "r", "k": ["request", "scheduling", "algorithm", "determines", "order", "priority", "processing", "incoming", "inference", "requests", "limited", "gpu", "resources", "strategies", "optimize"]}, {"id": "term-reranking", "t": "Reranking", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A two-stage retrieval approach where an initial fast retriever fetches candidate documents, and a more powerful...", "l": "r", "k": ["reranking", "two-stage", "retrieval", "approach", "initial", "fast", "retriever", "fetches", "candidate", "documents", "powerful", "cross-encoder", "model", "rescores", "reorders"]}, {"id": "term-residual-analysis", "t": "Residual Analysis", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "The examination of residuals (differences between observed and predicted values) to assess model fit, check assumptions...", "l": "r", "k": ["residual", "analysis", "examination", "residuals", "differences", "observed", "predicted", "values", "assess", "model", "fit", "check", "assumptions", "normality", "homoscedasticity"]}, {"id": "term-residual-connection", "t": "Residual Connection", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An additive skip connection where the input to a layer block is added element-wise to the block's output, allowing the...", "l": "r", "k": ["residual", "connection", "additive", "skip", "input", "layer", "block", "added", "element-wise", "output", "allowing", "network", "learn", "mappings", "rather"]}, {"id": "term-residual-learning", "t": "Residual Learning", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The principle of learning additive residual functions with reference to the layer inputs rather than learning...", "l": "r", "k": ["residual", "learning", "principle", "additive", "functions", "reference", "layer", "inputs", "rather", "unreferenced", "making", "easier", "optimize", "deep", "networks"]}, {"id": "term-resnet", "t": "ResNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Residual Network, a deep convolutional neural network architecture that introduces skip connections to enable training...", "l": "r", "k": ["resnet", "residual", "network", "deep", "convolutional", "neural", "architecture", "introduces", "skip", "connections", "enable", "training", "networks", "mitigating", "vanishing"]}, {"id": "term-resnet-101", "t": "ResNet-101", "tg": ["Models", "Technical"], "d": "models", "x": "A 101-layer variant of the Residual Network achieving higher accuracy than ResNet-50 at the cost of additional...", "l": "r", "k": ["resnet-101", "101-layer", "variant", "residual", "network", "achieving", "higher", "accuracy", "resnet-50", "cost", "additional", "computation", "commonly", "backbone", "object"]}, {"id": "term-resnet-152", "t": "ResNet-152", "tg": ["Models", "Technical"], "d": "models", "x": "The deepest standard variant of the Residual Network with 152 layers. Demonstrates that deeper networks with skip...", "l": "r", "k": ["resnet-152", "deepest", "standard", "variant", "residual", "network", "layers", "demonstrates", "deeper", "networks", "skip", "connections", "continue", "improve", "accuracy"]}, {"id": "term-resnet-50", "t": "ResNet-50", "tg": ["Models", "Technical"], "d": "models", "x": "A 50-layer variant of the Residual Network architecture widely used as a baseline for image classification and feature...", "l": "r", "k": ["resnet-50", "50-layer", "variant", "residual", "network", "architecture", "widely", "baseline", "image", "classification", "feature", "extraction", "contains", "bottleneck", "blocks"]}, {"id": "term-resnext", "t": "ResNeXt", "tg": ["Models", "Technical"], "d": "models", "x": "An extension of ResNet that introduces cardinality as an additional dimension through grouped convolutions. Proposed by...", "l": "r", "k": ["resnext", "extension", "resnet", "introduces", "cardinality", "additional", "dimension", "grouped", "convolutions", "proposed", "xie", "achieves", "better", "accuracy", "similar"]}, {"id": "term-resolution-theorem-proving", "t": "Resolution Theorem Proving", "tg": ["History", "Fundamentals"], "d": "history", "x": "An inference rule that produces a new clause implied by two clauses containing complementary literals. Introduced by...", "l": "r", "k": ["resolution", "theorem", "proving", "inference", "rule", "produces", "clause", "implied", "clauses", "containing", "complementary", "literals", "introduced", "john", "alan"]}, {"id": "term-responsible-ai", "t": "Responsible AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "An approach to developing and deploying AI systems that emphasizes ethical considerations, fairness, transparency,...", "l": "r", "k": ["responsible", "approach", "developing", "deploying", "systems", "emphasizes", "ethical", "considerations", "fairness", "transparency", "accountability", "societal", "benefit", "throughout", "entire"]}, {"id": "term-responsible-disclosure-for-ai", "t": "Responsible Disclosure for AI", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "The practice of privately reporting discovered vulnerabilities or dangerous capabilities in AI systems to the developer...", "l": "r", "k": ["responsible", "disclosure", "practice", "privately", "reporting", "discovered", "vulnerabilities", "dangerous", "capabilities", "systems", "developer", "public", "allowing", "time", "mitigation"]}, {"id": "term-restricted-boltzmann-machine", "t": "Restricted Boltzmann Machine", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative stochastic neural network with a bipartite structure of visible and hidden units with no intra-layer...", "l": "r", "k": ["restricted", "boltzmann", "machine", "generative", "stochastic", "neural", "network", "bipartite", "structure", "visible", "hidden", "units", "intra-layer", "connections", "trained"]}, {"id": "term-rete-algorithm", "t": "Rete Algorithm", "tg": ["History", "Fundamentals"], "d": "history", "x": "An efficient pattern matching algorithm developed by Charles Forgy in 1979 for production rule systems. The Rete...", "l": "r", "k": ["rete", "algorithm", "efficient", "pattern", "matching", "developed", "charles", "forgy", "production", "rule", "systems", "dramatically", "improved", "performance", "rule-based"]}, {"id": "term-retinanet", "t": "RetinaNet", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A single-stage object detection model that combines a Feature Pyramid Network backbone with focal loss, achieving...", "l": "r", "k": ["retinanet", "single-stage", "object", "detection", "model", "combines", "feature", "pyramid", "network", "backbone", "focal", "loss", "achieving", "accuracy", "comparable"]}, {"id": "term-retnet", "t": "RetNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Retentive Network, an architecture that supports parallel training, recurrent inference, and chunk-wise recurrent...", "l": "r", "k": ["retnet", "retentive", "network", "architecture", "supports", "parallel", "training", "recurrent", "inference", "chunk-wise", "computation", "retention", "mechanism", "replaces", "standard"]}, {"id": "term-retrieval", "t": "Retrieval", "tg": ["Process", "Search"], "d": "general", "x": "Finding relevant information from a database or corpus. In RAG, retrieval brings external knowledge into the generation...", "l": "r", "k": ["retrieval", "finding", "relevant", "information", "database", "corpus", "rag", "brings", "external", "knowledge", "generation", "process", "accurate", "responses"]}, {"id": "term-retrieval-evaluation", "t": "Retrieval Evaluation", "tg": ["Retrieval", "Evaluation"], "d": "datasets", "x": "The systematic assessment of retrieval system quality using metrics such as recall, precision, NDCG, and MRR applied to...", "l": "r", "k": ["retrieval", "evaluation", "systematic", "assessment", "system", "quality", "metrics", "recall", "precision", "ndcg", "mrr", "applied", "ranked", "result", "lists"]}, {"id": "term-retrieval-head", "t": "Retrieval Head", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Specific attention heads within a transformer that specialize in copying or retrieving information from the context,...", "l": "r", "k": ["retrieval", "head", "specific", "attention", "heads", "within", "transformer", "specialize", "copying", "retrieving", "information", "context", "playing", "crucial", "role"]}, {"id": "term-retrieval-augmented-fine-tuning", "t": "Retrieval-Augmented Fine-Tuning", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A training approach that fine-tunes a language model with retrieval-augmented examples, teaching the model to...", "l": "r", "k": ["retrieval-augmented", "fine-tuning", "training", "approach", "fine-tunes", "language", "model", "examples", "teaching", "effectively", "incorporate", "retrieved", "context", "responses"]}, {"id": "term-retrieval-augmented-generation", "t": "Retrieval-Augmented Generation", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A technique that enhances language model outputs by retrieving relevant documents from an external knowledge base and...", "l": "r", "k": ["retrieval-augmented", "generation", "technique", "enhances", "language", "model", "outputs", "retrieving", "relevant", "documents", "external", "knowledge", "base", "conditioning", "retrieved"]}, {"id": "term-retrieval-augmented-generation-history", "t": "Retrieval-Augmented Generation History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of RAG from the original paper by Lewis et al. at Facebook AI in 2020 to widespread adoption in...", "l": "r", "k": ["retrieval-augmented", "generation", "history", "development", "rag", "original", "paper", "lewis", "facebook", "widespread", "adoption", "enterprise", "combines", "retrieval", "external"]}, {"id": "term-retrieval-augmented-lm", "t": "Retrieval-Augmented Language Model", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A language model architecture that incorporates a retrieval component to fetch relevant documents from an external...", "l": "r", "k": ["retrieval-augmented", "language", "model", "architecture", "incorporates", "retrieval", "component", "fetch", "relevant", "documents", "external", "corpus", "generation", "grounding", "responses"]}, {"id": "term-retrieval-augmented-prompting", "t": "Retrieval-Augmented Prompting", "tg": ["Prompt Engineering", "Retrieval"], "d": "general", "x": "A technique that dynamically retrieves relevant documents, examples, or knowledge from an external corpus and...", "l": "r", "k": ["retrieval-augmented", "prompting", "technique", "dynamically", "retrieves", "relevant", "documents", "examples", "knowledge", "external", "corpus", "incorporates", "prompt", "context", "generation"]}, {"id": "term-return", "t": "Return", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The cumulative discounted sum of future rewards from a given time step, representing the total long-term value an agent...", "l": "r", "k": ["return", "cumulative", "discounted", "sum", "future", "rewards", "given", "time", "step", "representing", "total", "long-term", "value", "agent", "receives"]}, {"id": "term-reward", "t": "Reward", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A scalar signal received by an agent from the environment after taking an action, indicating how good or bad the...", "l": "r", "k": ["reward", "scalar", "signal", "received", "agent", "environment", "taking", "action", "indicating", "good", "bad", "outcome", "rewards", "drive", "learning"]}, {"id": "term-reward-clipping", "t": "Reward Clipping", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "A preprocessing technique that bounds reward values to a fixed range (commonly [-1, 1]) to stabilize training across...", "l": "r", "k": ["reward", "clipping", "preprocessing", "technique", "bounds", "values", "fixed", "range", "commonly", "stabilize", "training", "across", "diverse", "environments", "original"]}, {"id": "term-reward-decomposition", "t": "Reward Decomposition", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "Techniques that break a complex reward signal into simpler components that are easier to learn from, enabling more...", "l": "r", "k": ["reward", "decomposition", "techniques", "break", "complex", "signal", "simpler", "components", "easier", "learn", "enabling", "interpretable", "efficient", "training", "decomposed"]}, {"id": "term-reward-engineering", "t": "Reward Engineering", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "The process of designing reward functions that accurately capture desired agent behavior, balancing specificity with...", "l": "r", "k": ["reward", "engineering", "process", "designing", "functions", "accurately", "capture", "desired", "agent", "behavior", "balancing", "specificity", "generality", "poor", "lead"]}, {"id": "term-reward-hacking", "t": "Reward Hacking", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A failure mode in reinforcement learning from human feedback where the policy model exploits weaknesses in the reward...", "l": "r", "k": ["reward", "hacking", "failure", "mode", "reinforcement", "learning", "human", "feedback", "policy", "model", "exploits", "weaknesses", "achieve", "high", "scores"]}, {"id": "term-reward-machine", "t": "Reward Machine", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "A finite-state automaton that specifies reward functions based on high-level events or propositional symbols, enabling...", "l": "r", "k": ["reward", "machine", "finite-state", "automaton", "specifies", "functions", "based", "high-level", "events", "propositional", "symbols", "enabling", "structured", "specification", "complex"]}, {"id": "term-reward-model", "t": "Reward Model", "tg": ["Training", "Alignment"], "d": "safety", "x": "A model trained to score AI outputs based on human preferences. Used in RLHF to guide language models toward more...", "l": "r", "k": ["reward", "model", "trained", "score", "outputs", "based", "human", "preferences", "rlhf", "guide", "language", "models", "toward", "helpful", "safe"]}, {"id": "term-reward-normalization", "t": "Reward Normalization", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "The practice of scaling reward signals to have consistent magnitude across different environments or during training,...", "l": "r", "k": ["reward", "normalization", "practice", "scaling", "signals", "consistent", "magnitude", "across", "different", "environments", "training", "typically", "running", "statistics", "stabilizes"]}, {"id": "term-reward-shaping", "t": "Reward Shaping", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "The practice of adding auxiliary reward signals to guide learning, making sparse reward problems more tractable....", "l": "r", "k": ["reward", "shaping", "practice", "adding", "auxiliary", "signals", "guide", "learning", "making", "sparse", "problems", "tractable", "potential-based", "preserves", "optimal"]}, {"id": "term-reward-free-exploration", "t": "Reward-Free Exploration", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "An RL paradigm where the agent first explores the environment without any reward signal to build a comprehensive...", "l": "r", "k": ["reward-free", "exploration", "paradigm", "agent", "explores", "environment", "without", "reward", "signal", "build", "comprehensive", "understanding", "uses", "knowledge", "quickly"]}, {"id": "term-reward-weighted-regression", "t": "Reward-Weighted Regression", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A policy search method that computes policy updates by performing weighted maximum likelihood estimation on sampled...", "l": "r", "k": ["reward-weighted", "regression", "policy", "search", "method", "computes", "updates", "performing", "weighted", "maximum", "likelihood", "estimation", "sampled", "trajectories", "weights"]}, {"id": "term-rst", "t": "Rhetorical Structure Theory", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A theory of text organization that describes how clauses and larger text spans are connected through rhetorical...", "l": "r", "k": ["rhetorical", "structure", "theory", "text", "organization", "describes", "clauses", "larger", "spans", "connected", "relations", "elaboration", "contrast", "cause", "forming"]}, {"id": "term-richard-karp", "t": "Richard Karp", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who identified 21 NP-complete problems in his 1972 paper demonstrating the breadth of...", "l": "r", "k": ["richard", "karp", "american", "computer", "scientist", "identified", "np-complete", "problems", "paper", "demonstrating", "breadth", "computationally", "intractable", "work", "showed"]}, {"id": "term-richard-sutton", "t": "Richard Sutton", "tg": ["History", "Pioneers"], "d": "history", "x": "Canadian computer scientist who co-authored the seminal textbook on reinforcement learning with Andrew Barto, developed...", "l": "r", "k": ["richard", "sutton", "canadian", "computer", "scientist", "co-authored", "seminal", "textbook", "reinforcement", "learning", "andrew", "barto", "developed", "temporal", "difference"]}, {"id": "term-ridge-regression", "t": "Ridge Regression", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A linear regression variant that adds an L2 penalty term to the ordinary least squares objective, shrinking...", "l": "r", "k": ["ridge", "regression", "linear", "variant", "adds", "penalty", "term", "ordinary", "least", "squares", "objective", "shrinking", "coefficients", "toward", "zero"]}, {"id": "term-right-to-explanation", "t": "Right to Explanation", "tg": ["Governance", "AI Ethics"], "d": "safety", "x": "The legal or ethical principle that individuals subjected to automated decision-making are entitled to a meaningful...", "l": "r", "k": ["right", "explanation", "legal", "ethical", "principle", "individuals", "subjected", "automated", "decision-making", "entitled", "meaningful", "logic", "involved", "partially", "codified"]}, {"id": "term-ring-all-reduce", "t": "Ring All-Reduce", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "An efficient implementation of all-reduce where GPUs are arranged in a ring topology and data is sent in chunks through...", "l": "r", "k": ["ring", "all-reduce", "efficient", "implementation", "gpus", "arranged", "topology", "data", "sent", "chunks", "passes", "scatter-reduce", "all-gather", "provides", "bandwidth-optimal"]}, {"id": "term-ring-attention", "t": "Ring Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A distributed attention computation method that splits long sequences across devices in a ring topology, overlapping...", "l": "r", "k": ["ring", "attention", "distributed", "computation", "method", "splits", "long", "sequences", "across", "devices", "topology", "overlapping", "communication", "process", "near-unlimited"]}, {"id": "term-risk-sensitive-rl", "t": "Risk-Sensitive RL", "tg": ["Reinforcement Learning", "Safety"], "d": "safety", "x": "RL methods that optimize risk-aware objectives such as conditional value-at-risk (CVaR) or variance-penalized returns...", "l": "r", "k": ["risk-sensitive", "methods", "optimize", "risk-aware", "objectives", "conditional", "value-at-risk", "cvar", "variance-penalized", "returns", "rather", "expected", "return", "alone", "approaches"]}, {"id": "term-rlhf", "t": "RLHF (Reinforcement Learning from Human Feedback)", "tg": ["Training", "Alignment"], "d": "safety", "x": "A training technique that uses human preferences to guide model behavior. Human raters compare outputs, and these...", "l": "r", "k": ["rlhf", "reinforcement", "learning", "human", "feedback", "training", "technique", "uses", "preferences", "guide", "model", "behavior", "raters", "compare", "outputs"]}, {"id": "term-rlhf-algorithm", "t": "RLHF Algorithm", "tg": ["Algorithms", "Fundamentals", "Safety"], "d": "algorithms", "x": "Reinforcement Learning from Human Feedback trains language models using human preference data. Involves supervised...", "l": "r", "k": ["rlhf", "algorithm", "reinforcement", "learning", "human", "feedback", "trains", "language", "models", "preference", "data", "involves", "supervised", "fine-tuning", "reward"]}, {"id": "term-rms-normalization", "t": "RMS Normalization", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Root Mean Square Layer Normalization, a simplified variant of layer normalization that only rescales by the root mean...", "l": "r", "k": ["rms", "normalization", "root", "mean", "square", "layer", "simplified", "variant", "rescales", "activations", "without", "recentering", "reducing", "computational", "cost"]}, {"id": "term-rmsnorm", "t": "RMSNorm", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Root Mean Square Layer Normalization, a simplified normalization technique that normalizes activations using only the...", "l": "r", "k": ["rmsnorm", "root", "mean", "square", "layer", "normalization", "simplified", "technique", "normalizes", "activations", "rms", "statistic", "without", "centering", "reducing"]}, {"id": "term-rmsprop", "t": "RMSProp", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "An adaptive learning rate optimization algorithm that divides the learning rate by a running average of the magnitudes...", "l": "r", "k": ["rmsprop", "adaptive", "learning", "rate", "optimization", "algorithm", "divides", "running", "average", "magnitudes", "recent", "gradients", "parameter", "addresses", "diminishing"]}, {"id": "term-rnn", "t": "RNN (Recurrent Neural Network)", "tg": ["Architecture", "Historical"], "d": "models", "x": "A neural network architecture that processes sequences by maintaining hidden state across steps. Predecessors to...", "l": "r", "k": ["rnn", "recurrent", "neural", "network", "architecture", "processes", "sequences", "maintaining", "hidden", "state", "across", "steps", "predecessors", "transformers", "sequence"]}, {"id": "term-robert-floyd", "t": "Robert Floyd", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who received the Turing Award in 1978 for contributions to programming languages including...", "l": "r", "k": ["robert", "floyd", "american", "computer", "scientist", "received", "turing", "award", "contributions", "programming", "languages", "including", "operator", "precedence", "parsing"]}, {"id": "term-robert-kowalski", "t": "Robert Kowalski", "tg": ["History", "Pioneers"], "d": "history", "x": "British-American logician and computer scientist who developed the procedural interpretation of Horn clauses providing...", "l": "r", "k": ["robert", "kowalski", "british-american", "logician", "computer", "scientist", "developed", "procedural", "interpretation", "horn", "clauses", "providing", "theoretical", "foundation", "logic"]}, {"id": "term-roberta", "t": "RoBERTa", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A robustly optimized BERT pretraining approach that improves upon BERT by training longer with more data, removing next...", "l": "r", "k": ["roberta", "robustly", "optimized", "bert", "pretraining", "approach", "improves", "upon", "training", "longer", "data", "removing", "next", "sentence", "prediction"]}, {"id": "term-robot-rights", "t": "Robot Rights", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The concept that sufficiently advanced robots or AI systems might deserve legal protections or moral consideration...", "l": "r", "k": ["robot", "rights", "concept", "sufficiently", "advanced", "robots", "systems", "deserve", "legal", "protections", "moral", "consideration", "analogous", "granted", "humans"]}, {"id": "term-robotics-history", "t": "Robotics History", "tg": ["History", "Milestones"], "d": "history", "x": "The history of robotics from early automata and Unimate (1961) through mobile robots (Shakey 1966) to modern robotic...", "l": "r", "k": ["robotics", "history", "early", "automata", "unimate", "mobile", "robots", "shakey", "modern", "robotic", "systems", "including", "autonomous", "vehicles", "surgical"]}, {"id": "term-robust-regression", "t": "Robust Regression", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A class of regression methods designed to be resistant to outliers and violations of model assumptions. Techniques...", "l": "r", "k": ["robust", "regression", "class", "methods", "designed", "resistant", "outliers", "violations", "model", "assumptions", "techniques", "include", "m-estimation", "least", "trimmed"]}, {"id": "term-robust-rl", "t": "Robust RL", "tg": ["Reinforcement Learning", "Safety"], "d": "safety", "x": "RL algorithms designed to find policies that perform well under worst-case environment perturbations or model...", "l": "r", "k": ["robust", "algorithms", "designed", "find", "policies", "perform", "worst-case", "environment", "perturbations", "model", "uncertainty", "optimizes", "against", "adversarial", "possible"]}, {"id": "term-robust-scaler", "t": "Robust Scaler", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A feature scaling method that uses the median and interquartile range instead of the mean and standard deviation,...", "l": "r", "k": ["robust", "scaler", "feature", "scaling", "method", "uses", "median", "interquartile", "range", "instead", "mean", "standard", "deviation", "making", "resistant"]}, {"id": "term-roc-curve", "t": "ROC Curve", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "Receiver Operating Characteristic curve, a plot of the true positive rate against the false positive rate at various...", "l": "r", "k": ["roc", "curve", "receiver", "operating", "characteristic", "plot", "true", "positive", "rate", "against", "false", "various", "classification", "thresholds", "visualizes"]}, {"id": "term-rodney-brooks", "t": "Rodney Brooks", "tg": ["History", "Pioneers"], "d": "history", "x": "Australian roboticist who directed the MIT AI Laboratory from 1997 to 2007. Pioneer of behavior-based robotics and the...", "l": "r", "k": ["rodney", "brooks", "australian", "roboticist", "directed", "mit", "laboratory", "pioneer", "behavior-based", "robotics", "subsumption", "architecture", "co-founder", "irobot", "roomba"]}, {"id": "term-roger-schank", "t": "Roger Schank", "tg": ["History", "Pioneers"], "d": "history", "x": "American AI researcher (1946-2023) who developed conceptual dependency theory and script theory for natural language...", "l": "r", "k": ["roger", "schank", "american", "researcher", "1946-2023", "developed", "conceptual", "dependency", "theory", "script", "natural", "language", "understanding", "advancing", "role"]}, {"id": "term-roi-align", "t": "ROI Align", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "An improved version of ROI Pooling that uses bilinear interpolation instead of quantized grid snapping, eliminating...", "l": "r", "k": ["roi", "align", "improved", "version", "pooling", "uses", "bilinear", "interpolation", "instead", "quantized", "grid", "snapping", "eliminating", "misalignment", "artifacts"]}, {"id": "term-roi-pooling", "t": "ROI Pooling", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Region of Interest Pooling, an operation that extracts fixed-size feature representations from arbitrary-sized region...", "l": "r", "k": ["roi", "pooling", "region", "interest", "operation", "extracts", "fixed-size", "feature", "representations", "arbitrary-sized", "proposals", "enabling", "classification", "head", "object"]}, {"id": "term-role-assignment", "t": "Role Assignment", "tg": ["Prompt Engineering", "Persona"], "d": "general", "x": "The prompt technique of explicitly designating a specific role, profession, or character for the model to adopt,...", "l": "r", "k": ["role", "assignment", "prompt", "technique", "explicitly", "designating", "specific", "profession", "character", "model", "adopt", "shaping", "response", "style", "vocabulary"]}, {"id": "term-role-prompting", "t": "Role Prompting", "tg": ["Prompting", "Technique"], "d": "general", "x": "Assigning AI a specific persona, expertise, or perspective to shape its responses. For example, \"Act as a senior...", "l": "r", "k": ["role", "prompting", "assigning", "specific", "persona", "expertise", "perspective", "shape", "responses", "example", "act", "senior", "developer", "patient", "teacher"]}, {"id": "term-roofline-model", "t": "Roofline Model", "tg": ["Hardware", "Model Optimization"], "d": "models", "x": "A performance analysis framework that plots achievable performance as a function of operational intensity (FLOPS per...", "l": "r", "k": ["roofline", "model", "performance", "analysis", "framework", "plots", "achievable", "function", "operational", "intensity", "flops", "per", "byte", "memory", "traffic"]}, {"id": "term-root-mean-squared-error", "t": "Root Mean Squared Error", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "The square root of the mean squared error, expressed in the same units as the target variable. It provides an...", "l": "r", "k": ["root", "mean", "squared", "error", "square", "expressed", "units", "target", "variable", "provides", "interpretable", "measure", "typical", "magnitude", "prediction"]}, {"id": "term-rope", "t": "RoPE (Rotary Position Embedding)", "tg": ["Architecture", "Transformers"], "d": "models", "x": "A positional encoding technique that encodes position through rotation in complex space. Enables better length...", "l": "r", "k": ["rope", "rotary", "position", "embedding", "positional", "encoding", "technique", "encodes", "rotation", "complex", "space", "enables", "better", "length", "generalization"]}, {"id": "term-ross-quillian", "t": "Ross Quillian", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who introduced semantic networks in his 1968 doctoral thesis as a model of human...", "l": "r", "k": ["ross", "quillian", "american", "computer", "scientist", "introduced", "semantic", "networks", "doctoral", "thesis", "model", "human", "associative", "memory", "establishing"]}, {"id": "term-rotary-position-embedding", "t": "Rotary Position Embedding", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A method that encodes position information by rotating query and key vectors in pairs of dimensions according to their...", "l": "r", "k": ["rotary", "position", "embedding", "method", "encodes", "information", "rotating", "query", "key", "vectors", "pairs", "dimensions", "according", "naturally", "encoding"]}, {"id": "term-rouge", "t": "ROUGE", "tg": ["NLP", "Text Processing"], "d": "general", "x": "Recall-Oriented Understudy for Gisting Evaluation, a set of metrics for evaluating summarization quality by measuring...", "l": "r", "k": ["rouge", "recall-oriented", "understudy", "gisting", "evaluation", "metrics", "evaluating", "summarization", "quality", "measuring", "n-gram", "overlap", "generated", "summaries", "reference"]}, {"id": "term-rouge-score", "t": "ROUGE Score", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "Recall-Oriented Understudy for Gisting Evaluation, a family of metrics that measures text summarization quality by...", "l": "r", "k": ["rouge", "score", "recall-oriented", "understudy", "gisting", "evaluation", "family", "metrics", "measures", "text", "summarization", "quality", "computing", "n-gram", "overlap"]}, {"id": "term-rouge-1", "t": "ROUGE-1", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A ROUGE variant that measures unigram (single word) overlap between a generated text and reference text, providing a...", "l": "r", "k": ["rouge-1", "rouge", "variant", "measures", "unigram", "single", "word", "overlap", "generated", "text", "reference", "providing", "basic", "assessment", "content"]}, {"id": "term-rouge-2", "t": "ROUGE-2", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A ROUGE variant that measures bigram (two consecutive word) overlap between generated and reference texts, capturing...", "l": "r", "k": ["rouge-2", "rouge", "variant", "measures", "bigram", "consecutive", "word", "overlap", "generated", "reference", "texts", "capturing", "phrase-level", "similarity", "providing"]}, {"id": "term-rouge-l", "t": "ROUGE-L", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A ROUGE variant based on the longest common subsequence (LCS) between generated and reference texts, capturing...", "l": "r", "k": ["rouge-l", "rouge", "variant", "based", "longest", "common", "subsequence", "lcs", "generated", "reference", "texts", "capturing", "sentence-level", "structural", "similarity"]}, {"id": "term-rt-2", "t": "RT-2", "tg": ["Models", "Technical"], "d": "models", "x": "Robotic Transformer 2 is a vision-language-action model that directly outputs robot actions from camera images and...", "l": "r", "k": ["rt-2", "robotic", "transformer", "vision-language-action", "model", "directly", "outputs", "robot", "actions", "camera", "images", "language", "instructions", "demonstrates", "web-scale"]}, {"id": "term-rt-detr", "t": "RT-DETR", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Real-Time Detection Transformer, a hybrid detection architecture that combines a CNN backbone with a transformer...", "l": "r", "k": ["rt-detr", "real-time", "detection", "transformer", "hybrid", "architecture", "combines", "cnn", "backbone", "decoder", "achieving", "accuracy", "detr-based", "models", "inference"]}, {"id": "term-rwkv", "t": "RWKV", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A linear attention-based architecture that combines the parallelizable training of transformers with the efficient...", "l": "r", "k": ["rwkv", "linear", "attention-based", "architecture", "combines", "parallelizable", "training", "transformers", "efficient", "inference", "rnns", "novel", "time-mixing", "channel-mixing", "approach"]}, {"id": "term-s4", "t": "S4", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Structured State Spaces for Sequence Modeling, a state space model that uses a special initialization based on the...", "l": "s", "k": ["structured", "state", "spaces", "sequence", "modeling", "space", "model", "uses", "special", "initialization", "based", "hippo", "framework", "efficient", "computation"]}, {"id": "term-s5", "t": "S5", "tg": ["Models", "Technical"], "d": "models", "x": "Simplified State Space Layers extends S4 with a simpler multi-input multi-output parameterization using parallel scan...", "l": "s", "k": ["simplified", "state", "space", "layers", "extends", "simpler", "multi-input", "multi-output", "parameterization", "parallel", "scan", "efficient", "computation", "achieves", "competitive"]}, {"id": "term-safe-rl", "t": "Safe Reinforcement Learning", "tg": ["Reinforcement Learning", "Safety"], "d": "safety", "x": "RL methods that incorporate safety constraints to prevent the agent from taking dangerous or unacceptable actions...", "l": "s", "k": ["safe", "reinforcement", "learning", "methods", "incorporate", "safety", "constraints", "prevent", "agent", "taking", "dangerous", "unacceptable", "actions", "training", "deployment"]}, {"id": "term-safety-filter", "t": "Safety Filter", "tg": ["Safety", "Security"], "d": "safety", "x": "Systems that detect and block harmful content in AI inputs or outputs. Part of the safety stack protecting users from...", "l": "s", "k": ["safety", "filter", "systems", "detect", "block", "harmful", "content", "inputs", "outputs", "part", "stack", "protecting", "users", "inappropriate", "dangerous"]}, {"id": "term-safety-score", "t": "Safety Score", "tg": ["Evaluation", "Safety"], "d": "datasets", "x": "A composite evaluation metric that aggregates measurements of harmful output generation including toxicity, bias,...", "l": "s", "k": ["safety", "score", "composite", "evaluation", "metric", "aggregates", "measurements", "harmful", "output", "generation", "including", "toxicity", "bias", "dangerous", "advice"]}, {"id": "term-saint", "t": "SAINT", "tg": ["Models", "Technical"], "d": "models", "x": "Self-Attention and Intersample Attention Transformer applies both row-wise and column-wise attention to tabular data....", "l": "s", "k": ["saint", "self-attention", "intersample", "attention", "transformer", "applies", "row-wise", "column-wise", "tabular", "data", "captures", "feature-feature", "sample-sample", "interactions", "improved"]}, {"id": "term-saliency-map", "t": "Saliency Map", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A visualization technique that highlights which parts of an input are most relevant to a model's prediction by...", "l": "s", "k": ["saliency", "map", "visualization", "technique", "highlights", "parts", "input", "relevant", "model", "prediction", "computing", "gradients", "output", "respect", "simple"]}, {"id": "term-sam-altman", "t": "Sam Altman", "tg": ["History", "Pioneers"], "d": "history", "x": "American entrepreneur who became CEO of OpenAI, overseeing the development and launch of ChatGPT and GPT-4. His brief...", "l": "s", "k": ["sam", "altman", "american", "entrepreneur", "became", "ceo", "openai", "overseeing", "development", "launch", "chatgpt", "gpt-4", "brief", "firing", "reinstatement"]}, {"id": "term-sambanova", "t": "SambaNova", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "An AI hardware company producing reconfigurable dataflow architecture processors (SN series) that adapt their compute...", "l": "s", "k": ["sambanova", "hardware", "company", "producing", "reconfigurable", "dataflow", "architecture", "processors", "series", "adapt", "compute", "topology", "different", "model", "architectures"]}, {"id": "term-sample-complexity-rl", "t": "Sample Complexity", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The number of environment interactions needed for an RL algorithm to find a near-optimal policy with high probability....", "l": "s", "k": ["sample", "complexity", "number", "environment", "interactions", "needed", "algorithm", "find", "near-optimal", "policy", "high", "probability", "key", "measure", "algorithmic"]}, {"id": "term-sampling", "t": "Sampling", "tg": ["Generation", "Technical"], "d": "general", "x": "The process of selecting the next token during text generation. Methods include greedy (always pick highest...", "l": "s", "k": ["sampling", "process", "selecting", "next", "token", "text", "generation", "methods", "include", "greedy", "always", "pick", "highest", "probability", "top-k"]}, {"id": "term-samuels-checkers-program", "t": "Samuel's Checkers Program", "tg": ["History", "Systems"], "d": "history", "x": "A checkers-playing program developed by Arthur Samuel at IBM beginning in 1959. The program used machine learning...", "l": "s", "k": ["samuel", "checkers", "program", "checkers-playing", "developed", "arthur", "ibm", "beginning", "machine", "learning", "techniques", "including", "rote", "generalization", "improve"]}, {"id": "term-sandwich-prompting", "t": "Sandwich Prompting", "tg": ["Prompt Engineering", "Structure"], "d": "general", "x": "A prompt structure that places the core instruction both before and after the main context or input data, reinforcing...", "l": "s", "k": ["sandwich", "prompting", "prompt", "structure", "places", "core", "instruction", "main", "context", "input", "data", "reinforcing", "adherence", "instructions", "otherwise"]}, {"id": "term-sarcasm-detection", "t": "Sarcasm Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying sarcastic or ironic statements in text where the intended meaning differs from the literal...", "l": "s", "k": ["sarcasm", "detection", "task", "identifying", "sarcastic", "ironic", "statements", "text", "intended", "meaning", "differs", "literal", "challenging", "problem", "requiring"]}, {"id": "term-sarima", "t": "SARIMA", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "Seasonal ARIMA, an extension of the ARIMA model that includes additional seasonal autoregressive, differencing, and...", "l": "s", "k": ["sarima", "seasonal", "arima", "extension", "model", "includes", "additional", "autoregressive", "differencing", "moving", "average", "terms", "capture", "periodic", "patterns"]}, {"id": "term-sarsa", "t": "SARSA", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "An on-policy temporal difference control algorithm that updates Q-values using the actual action taken by the current...", "l": "s", "k": ["sarsa", "on-policy", "temporal", "difference", "control", "algorithm", "updates", "q-values", "actual", "action", "taken", "current", "policy", "state-action-reward-state-action", "unlike"]}, {"id": "term-satellite-imagery-analysis", "t": "Satellite Imagery Analysis", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The use of computer vision models to interpret aerial and satellite photographs for applications including land use...", "l": "s", "k": ["satellite", "imagery", "analysis", "computer", "vision", "models", "interpret", "aerial", "photographs", "applications", "including", "land", "classification", "change", "detection"]}, {"id": "term-scalable-oversight", "t": "Scalable Oversight", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The challenge and research agenda of maintaining meaningful human oversight of AI systems as they become more capable...", "l": "s", "k": ["scalable", "oversight", "challenge", "research", "agenda", "maintaining", "meaningful", "human", "systems", "become", "capable", "handle", "tasks", "complex", "humans"]}, {"id": "term-scalar-quantization", "t": "Scalar Quantization", "tg": ["Vector Database", "Quantization"], "d": "general", "x": "A vector compression method that reduces memory usage by converting each floating-point vector component to a...", "l": "s", "k": ["scalar", "quantization", "vector", "compression", "method", "reduces", "memory", "usage", "converting", "floating-point", "component", "lower-precision", "representation", "8-bit", "integers"]}, {"id": "term-scale-ai", "t": "Scale AI", "tg": ["Company", "Data"], "d": "general", "x": "A company specializing in data labeling and annotation services for AI training. Provides human feedback at scale,...", "l": "s", "k": ["scale", "company", "specializing", "data", "labeling", "annotation", "services", "training", "provides", "human", "feedback", "crucial", "evaluating", "foundation", "models"]}, {"id": "term-scaling-hypothesis", "t": "Scaling Hypothesis", "tg": ["History", "Milestones"], "d": "history", "x": "The hypothesis that increasing model size, training data, and compute leads to predictable and substantial improvements...", "l": "s", "k": ["scaling", "hypothesis", "increasing", "model", "size", "training", "data", "compute", "leads", "predictable", "substantial", "improvements", "capability", "supported", "empirical"]}, {"id": "term-scaling-law", "t": "Scaling Law", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Empirical power-law relationships that predict how model performance improves as a function of compute budget, model...", "l": "s", "k": ["scaling", "law", "empirical", "power-law", "relationships", "predict", "model", "performance", "improves", "function", "compute", "budget", "size", "dataset", "guiding"]}, {"id": "term-scaling-laws", "t": "Scaling Laws", "tg": ["Research", "Training"], "d": "general", "x": "Empirical relationships showing how model performance improves with more parameters, data, and compute. Guide decisions...", "l": "s", "k": ["scaling", "laws", "empirical", "relationships", "showing", "model", "performance", "improves", "parameters", "data", "compute", "guide", "decisions", "invest", "resources"]}, {"id": "term-scaling-laws-compute", "t": "Scaling Laws for Compute", "tg": ["Model Optimization", "Distributed Computing"], "d": "models", "x": "Empirical power-law relationships between model performance and compute budget, model size, and dataset size...", "l": "s", "k": ["scaling", "laws", "compute", "empirical", "power-law", "relationships", "model", "performance", "budget", "size", "dataset", "established", "kaplan", "hoffmann", "chinchilla"]}, {"id": "term-scene-graph-generation", "t": "Scene Graph Generation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of detecting objects in an image and predicting their pairwise relationships to construct a graph...", "l": "s", "k": ["scene", "graph", "generation", "task", "detecting", "objects", "image", "predicting", "pairwise", "relationships", "construct", "representation", "nodes", "edges", "visual"]}, {"id": "term-scene-text-recognition", "t": "Scene Text Recognition", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of reading and transcribing text found in natural images (street signs, product labels, building facades),...", "l": "s", "k": ["scene", "text", "recognition", "task", "reading", "transcribing", "found", "natural", "images", "street", "signs", "product", "labels", "building", "facades"]}, {"id": "term-scheduled-sampling", "t": "Scheduled Sampling", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A training technique that gradually transitions from teacher forcing to using the model's own predictions as inputs...", "l": "s", "k": ["scheduled", "sampling", "training", "technique", "gradually", "transitions", "teacher", "forcing", "model", "predictions", "inputs", "reduces", "exposure", "bias", "gap"]}, {"id": "term-scibert", "t": "SciBERT", "tg": ["Models", "Technical"], "d": "models", "x": "A BERT model pretrained on a large corpus of scientific papers from the Semantic Scholar corpus. Outperforms BERT on...", "l": "s", "k": ["scibert", "bert", "model", "pretrained", "large", "corpus", "scientific", "papers", "semantic", "scholar", "outperforms", "nlp", "tasks", "including", "named"]}, {"id": "term-scikit-learn", "t": "Scikit-learn", "tg": ["Framework", "ML"], "d": "general", "x": "A popular Python library for traditional machine learning algorithms. Provides simple APIs for classification,...", "l": "s", "k": ["scikit-learn", "popular", "python", "library", "traditional", "machine", "learning", "algorithms", "provides", "simple", "apis", "classification", "regression", "clustering", "preprocessing"]}, {"id": "term-score-function", "t": "Score Function", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The gradient of the log-likelihood function with respect to the parameter, used in maximum likelihood estimation and...", "l": "s", "k": ["score", "function", "gradient", "log-likelihood", "respect", "parameter", "maximum", "likelihood", "estimation", "fisher", "information", "calculations", "expected", "value", "true"]}, {"id": "term-score-matching", "t": "Score Matching", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A method for estimating probability density functions by matching the gradient of the log-density (score function)...", "l": "s", "k": ["score", "matching", "method", "estimating", "probability", "density", "functions", "gradient", "log-density", "function", "rather", "itself", "avoids", "computing", "normalizing"]}, {"id": "term-score-based-generative-model", "t": "Score-Based Generative Model", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative model that learns the gradient of the log probability density (score function) of the data distribution,...", "l": "s", "k": ["score-based", "generative", "model", "learns", "gradient", "log", "probability", "density", "score", "function", "data", "distribution", "uses", "langevin", "dynamics"]}, {"id": "term-script-theory", "t": "Script Theory", "tg": ["History", "Fundamentals"], "d": "history", "x": "A knowledge representation scheme proposed by Roger Schank and Robert Abelson in 1977 describing stereotyped sequences...", "l": "s", "k": ["script", "theory", "knowledge", "representation", "scheme", "proposed", "roger", "schank", "robert", "abelson", "describing", "stereotyped", "sequences", "events", "particular"]}, {"id": "term-sdxl", "t": "SDXL", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "Stable Diffusion XL, an improved latent diffusion model that uses a larger UNet with dual text encoders and an optional...", "l": "s", "k": ["sdxl", "stable", "diffusion", "improved", "latent", "model", "uses", "larger", "unet", "dual", "text", "encoders", "optional", "refiner", "generate"]}, {"id": "term-seasonal-decomposition", "t": "Seasonal Decomposition", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A time series analysis technique that separates a time series into trend, seasonal, and residual components, either...", "l": "s", "k": ["seasonal", "decomposition", "time", "series", "analysis", "technique", "separates", "trend", "residual", "components", "additively", "multiplicatively", "better", "understand", "underlying"]}, {"id": "term-sebastian-thrun", "t": "Sebastian Thrun", "tg": ["History", "Pioneers"], "d": "history", "x": "German-American computer scientist who led the Stanford Racing Team to victory in the 2005 DARPA Grand Challenge....", "l": "s", "k": ["sebastian", "thrun", "german-american", "computer", "scientist", "led", "stanford", "racing", "team", "victory", "darpa", "grand", "challenge", "co-founder", "udacity"]}, {"id": "term-second-ai-winter", "t": "Second AI Winter", "tg": ["History", "Milestones"], "d": "history", "x": "The period from approximately 1987 to 1993 when enthusiasm for AI again collapsed following the failure of expert...", "l": "s", "k": ["winter", "period", "approximately", "enthusiasm", "collapsed", "following", "failure", "expert", "systems", "scale", "collapse", "lisp", "machine", "market", "cuts"]}, {"id": "term-secure-multi-party-computation", "t": "Secure Multi-Party Computation", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "A cryptographic protocol that allows multiple parties to jointly compute a function over their combined inputs while...", "l": "s", "k": ["secure", "multi-party", "computation", "cryptographic", "protocol", "allows", "multiple", "parties", "jointly", "compute", "function", "combined", "inputs", "keeping", "party"]}, {"id": "term-segment-anything-model", "t": "Segment Anything Model", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A foundation model for image segmentation (SAM) trained on a massive dataset that can segment any object in any image...", "l": "s", "k": ["segment", "anything", "model", "foundation", "image", "segmentation", "sam", "trained", "massive", "dataset", "object", "given", "prompt", "point", "bounding"]}, {"id": "term-segment-anything-model-2", "t": "Segment Anything Model 2", "tg": ["Models", "Technical"], "d": "models", "x": "An extension of SAM that handles video segmentation in addition to images. Processes video frames with a memory...", "l": "s", "k": ["segment", "anything", "model", "extension", "sam", "handles", "video", "segmentation", "addition", "images", "processes", "frames", "memory", "mechanism", "temporal"]}, {"id": "term-selection-bias", "t": "Selection Bias", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A systematic error arising when the sample used for analysis is not representative of the population, due to the way...", "l": "s", "k": ["selection", "bias", "systematic", "error", "arising", "sample", "analysis", "representative", "population", "due", "observations", "were", "selected", "lead", "models"]}, {"id": "term-selectional-preference", "t": "Selectional Preference", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The tendency of predicates to semantically constrain their arguments, such as 'eat' preferring edible objects, used in...", "l": "s", "k": ["selectional", "preference", "tendency", "predicates", "semantically", "constrain", "arguments", "eat", "preferring", "edible", "objects", "nlp", "disambiguation", "semantic", "plausibility"]}, {"id": "term-selective-context", "t": "Selective Context", "tg": ["Prompt Engineering", "Compression"], "d": "general", "x": "A prompt compression method that evaluates the informativeness of each lexical unit in a context using self-information...", "l": "s", "k": ["selective", "context", "prompt", "compression", "method", "evaluates", "informativeness", "lexical", "unit", "self-information", "scores", "causal", "language", "model", "filters"]}, {"id": "term-self-ask", "t": "Self-Ask", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting technique where the model explicitly asks itself follow-up questions needed to answer a complex query, then...", "l": "s", "k": ["self-ask", "prompting", "technique", "model", "explicitly", "asks", "itself", "follow-up", "questions", "needed", "answer", "complex", "query", "answers", "sub-question"]}, {"id": "term-self-attention", "t": "Self-Attention", "tg": ["Architecture", "Transformers"], "d": "models", "x": "An attention mechanism where a sequence attends to itself, allowing each position to consider all other positions. The...", "l": "s", "k": ["self-attention", "attention", "mechanism", "sequence", "attends", "itself", "allowing", "position", "consider", "positions", "core", "operation", "transformer", "models"]}, {"id": "term-self-attention-mechanism", "t": "Self-Attention Mechanism", "tg": ["History", "Fundamentals"], "d": "history", "x": "A mechanism that allows each position in a sequence to attend to all other positions introduced as a key component of...", "l": "s", "k": ["self-attention", "mechanism", "allows", "position", "sequence", "attend", "positions", "introduced", "key", "component", "transformer", "architecture", "vaswani", "enables", "capturing"]}, {"id": "term-self-bleu", "t": "Self-BLEU", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A diversity metric that computes BLEU scores between pairs of generated sentences from the same model, where lower...", "l": "s", "k": ["self-bleu", "diversity", "metric", "computes", "bleu", "scores", "pairs", "generated", "sentences", "model", "lower", "indicates", "greater", "less", "repetition"]}, {"id": "term-self-consistency", "t": "Self-Consistency", "tg": ["Prompting", "Reasoning"], "d": "general", "x": "A technique where AI generates multiple reasoning paths and selects the most common answer. Improves accuracy for...", "l": "s", "k": ["self-consistency", "technique", "generates", "multiple", "reasoning", "paths", "selects", "common", "answer", "improves", "accuracy", "complex", "aggregating", "diverse", "approaches"]}, {"id": "term-self-driving-car-history", "t": "Self-Driving Car History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of autonomous vehicles from early projects like Stanford Cart in 1961 and CMU's Navlab in 1986 through...", "l": "s", "k": ["self-driving", "car", "history", "evolution", "autonomous", "vehicles", "early", "projects", "stanford", "cart", "cmu", "navlab", "darpa", "grand", "challenges"]}, {"id": "term-self-play", "t": "Self-Play", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A training paradigm where an agent improves by playing against copies of itself, generating increasingly challenging...", "l": "s", "k": ["self-play", "training", "paradigm", "agent", "improves", "playing", "against", "copies", "itself", "generating", "increasingly", "challenging", "opponents", "skill", "grows"]}, {"id": "term-self-polish-prompting", "t": "Self-Polish Prompting", "tg": ["Prompt Engineering", "Refinement"], "d": "general", "x": "A technique that prompts the model to progressively refine and polish the given problem conditions before solving,...", "l": "s", "k": ["self-polish", "prompting", "technique", "prompts", "model", "progressively", "refine", "polish", "given", "problem", "conditions", "solving", "enabling", "simplify", "complex"]}, {"id": "term-self-rag", "t": "Self-RAG", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A framework where the language model learns to retrieve on demand, generate text, and critique its own output using...", "l": "s", "k": ["self-rag", "framework", "language", "model", "learns", "retrieve", "demand", "generate", "text", "critique", "output", "special", "reflection", "tokens", "adaptively"]}, {"id": "term-self-supervised-learning", "t": "Self-Supervised Learning", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A machine learning paradigm where the model generates its own supervisory signal from unlabeled data through pretext...", "l": "s", "k": ["self-supervised", "learning", "machine", "paradigm", "model", "generates", "supervisory", "signal", "unlabeled", "data", "pretext", "tasks", "examples", "include", "masked"]}, {"id": "term-self-supervised-learning-history", "t": "Self-Supervised Learning History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of self-supervised learning where models learn representations from unlabeled data by solving pretext...", "l": "s", "k": ["self-supervised", "learning", "history", "development", "models", "learn", "representations", "unlabeled", "data", "solving", "pretext", "tasks", "autoencoders", "word2vec", "bert"]}, {"id": "term-self-training", "t": "Self-Training", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A semi-supervised learning method where a model trained on labeled data generates pseudo-labels for unlabeled data,...", "l": "s", "k": ["self-training", "semi-supervised", "learning", "method", "model", "trained", "labeled", "data", "generates", "pseudo-labels", "unlabeled", "retrains", "real", "pseudo-labeled", "examples"]}, {"id": "term-self-training-vision", "t": "Self-Training for Vision", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A semi-supervised learning approach where a teacher model generates pseudo-labels for unlabeled images, and a student...", "l": "s", "k": ["self-training", "vision", "semi-supervised", "learning", "approach", "teacher", "model", "generates", "pseudo-labels", "unlabeled", "images", "student", "trained", "combination", "labeled"]}, {"id": "term-selu", "t": "SELU", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Scaled Exponential Linear Unit that enables self-normalizing neural networks. Combines the ELU function with specific...", "l": "s", "k": ["selu", "scaled", "exponential", "linear", "unit", "enables", "self-normalizing", "neural", "networks", "combines", "elu", "function", "specific", "scale", "alpha"]}, {"id": "term-semantic-caching", "t": "Semantic Caching", "tg": ["LLM", "Inference"], "d": "models", "x": "A caching strategy that stores LLM responses indexed by the semantic meaning of queries rather than exact string...", "l": "s", "k": ["semantic", "caching", "strategy", "stores", "llm", "responses", "indexed", "meaning", "queries", "rather", "exact", "string", "matches", "allowing", "cache"]}, {"id": "term-semantic-chunking", "t": "Semantic Chunking", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "A document splitting approach that determines chunk boundaries based on semantic similarity between consecutive...", "l": "s", "k": ["semantic", "chunking", "document", "splitting", "approach", "determines", "chunk", "boundaries", "based", "similarity", "consecutive", "sentences", "creating", "chunks", "topic"]}, {"id": "term-semantic-correspondence", "t": "Semantic Correspondence", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of finding matching points between images of different instances of the same object category, requiring...", "l": "s", "k": ["semantic", "correspondence", "task", "finding", "matching", "points", "images", "different", "instances", "object", "category", "requiring", "understanding", "similarity", "rather"]}, {"id": "term-semantic-kernel", "t": "Semantic Kernel", "tg": ["Framework", "Application"], "d": "general", "x": "Microsoft's open-source SDK for building AI applications with LLMs. Supports plugin architecture, memory, and planning...", "l": "s", "k": ["semantic", "kernel", "microsoft", "open-source", "sdk", "building", "applications", "llms", "supports", "plugin", "architecture", "memory", "planning", "enterprise", "agent"]}, {"id": "term-semantic-map", "t": "Semantic Map", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A spatial representation that annotates each location with semantic labels indicating what type of object or surface...", "l": "s", "k": ["semantic", "map", "spatial", "representation", "annotates", "location", "labels", "indicating", "type", "object", "surface", "occupies", "space", "robotics", "navigation"]}, {"id": "term-semantic-networks", "t": "Semantic Networks", "tg": ["History", "Milestones"], "d": "history", "x": "A knowledge representation formalism using graphs of nodes connected by labeled edges to represent concepts and their...", "l": "s", "k": ["semantic", "networks", "knowledge", "representation", "formalism", "graphs", "nodes", "connected", "labeled", "edges", "represent", "concepts", "relationships", "proposed", "ross"]}, {"id": "term-semantic-parsing", "t": "Semantic Parsing", "tg": ["NLP", "Parsing"], "d": "general", "x": "The task of mapping natural language utterances to formal meaning representations such as logical forms, SQL queries,...", "l": "s", "k": ["semantic", "parsing", "task", "mapping", "natural", "language", "utterances", "formal", "meaning", "representations", "logical", "forms", "sql", "queries", "lambda"]}, {"id": "term-semantic-role-labeling", "t": "Semantic Role Labeling", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of identifying the predicate-argument structure of a sentence by assigning semantic roles such as agent,...", "l": "s", "k": ["semantic", "role", "labeling", "task", "identifying", "predicate-argument", "structure", "sentence", "assigning", "roles", "agent", "patient", "instrument", "location", "constituents"]}, {"id": "term-semantic-search", "t": "Semantic Search", "tg": ["Application", "Search"], "d": "general", "x": "Search based on meaning rather than keyword matching. Uses embeddings to find conceptually similar content, enabling...", "l": "s", "k": ["semantic", "search", "based", "meaning", "rather", "keyword", "matching", "uses", "embeddings", "find", "conceptually", "similar", "content", "enabling", "relevant"]}, {"id": "term-semantic-segmentation", "t": "Semantic Segmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A computer vision task that assigns a class label to every pixel in an image, producing a dense prediction map that...", "l": "s", "k": ["semantic", "segmentation", "computer", "vision", "task", "assigns", "class", "label", "pixel", "image", "producing", "dense", "prediction", "map", "identifies"]}, {"id": "term-semantic-segmentation-transformer", "t": "Semantic Segmentation Transformer", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Transformer-based architectures like SegFormer and Mask2Former that apply self-attention to image features for dense...", "l": "s", "k": ["semantic", "segmentation", "transformer", "transformer-based", "architectures", "segformer", "mask2former", "apply", "self-attention", "image", "features", "dense", "per-pixel", "classification", "outperforming"]}, {"id": "term-semantic-textual-similarity", "t": "Semantic Textual Similarity", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of measuring the degree of semantic equivalence between two text segments on a continuous scale, going beyond...", "l": "s", "k": ["semantic", "textual", "similarity", "task", "measuring", "degree", "equivalence", "text", "segments", "continuous", "scale", "going", "beyond", "binary", "paraphrase"]}, {"id": "term-semantic-web", "t": "Semantic Web", "tg": ["History", "Fundamentals"], "d": "history", "x": "A vision proposed by Tim Berners-Lee in 2001 for extending the World Wide Web with machine-readable metadata. The...", "l": "s", "k": ["semantic", "web", "vision", "proposed", "tim", "berners-lee", "extending", "world", "wide", "machine-readable", "metadata", "uses", "ontologies", "rdf", "owl"]}, {"id": "term-semantics", "t": "Semantics", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The branch of linguistics studying meaning in language, including word meaning, sentence meaning, and the relationship...", "l": "s", "k": ["semantics", "branch", "linguistics", "studying", "meaning", "language", "including", "word", "sentence", "relationship", "linguistic", "expressions", "refer"]}, {"id": "term-semi-supervised-learning", "t": "Semi-Supervised Learning", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A learning paradigm that combines a small amount of labeled data with a large amount of unlabeled data during training....", "l": "s", "k": ["semi-supervised", "learning", "paradigm", "combines", "small", "amount", "labeled", "data", "large", "unlabeled", "training", "leverages", "structure", "improve", "model"]}, {"id": "term-semi-supervised-object-detection", "t": "Semi-Supervised Object Detection", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Detection training approaches that leverage both a small set of labeled images and a large pool of unlabeled images,...", "l": "s", "k": ["semi-supervised", "object", "detection", "training", "approaches", "leverage", "small", "labeled", "images", "large", "pool", "unlabeled", "techniques", "pseudo-labeling", "consistency"]}, {"id": "term-sensitivity", "t": "Sensitivity", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "The proportion of actual positive cases correctly identified by a classifier, synonymous with recall and true positive...", "l": "s", "k": ["sensitivity", "proportion", "actual", "positive", "cases", "correctly", "identified", "classifier", "synonymous", "recall", "true", "rate", "high", "minimizes", "false"]}, {"id": "term-sentence-boundary-detection", "t": "Sentence Boundary Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying where sentences begin and end in running text, handling ambiguous punctuation like periods in...", "l": "s", "k": ["sentence", "boundary", "detection", "task", "identifying", "sentences", "begin", "end", "running", "text", "handling", "ambiguous", "punctuation", "periods", "abbreviations"]}, {"id": "term-sentence-embedding", "t": "Sentence Embedding", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A fixed-length dense vector representation of an entire sentence that captures its semantic meaning, produced by...", "l": "s", "k": ["sentence", "embedding", "fixed-length", "dense", "vector", "representation", "entire", "captures", "semantic", "meaning", "produced", "methods", "mean", "pooling", "token"]}, {"id": "term-sentence-transformer", "t": "Sentence Transformer", "tg": ["Model Type", "Embeddings"], "d": "models", "x": "Models that encode entire sentences into single vectors, capturing semantic meaning. Popular for semantic search,...", "l": "s", "k": ["sentence", "transformer", "models", "encode", "entire", "sentences", "single", "vectors", "capturing", "semantic", "meaning", "popular", "search", "similarity", "matching"]}, {"id": "term-sentence-bert", "t": "Sentence-BERT", "tg": ["Models", "Technical"], "d": "models", "x": "A modification of BERT that uses siamese and triplet networks to derive semantically meaningful sentence embeddings....", "l": "s", "k": ["sentence-bert", "modification", "bert", "uses", "siamese", "triplet", "networks", "derive", "semantically", "meaningful", "sentence", "embeddings", "enables", "efficient", "semantic"]}, {"id": "term-sentencepiece", "t": "SentencePiece", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A language-independent tokenization library that treats the input as a raw byte stream and applies BPE or unigram...", "l": "s", "k": ["sentencepiece", "language-independent", "tokenization", "library", "treats", "input", "raw", "byte", "stream", "applies", "bpe", "unigram", "directly", "without", "pre-tokenization"]}, {"id": "term-sentiment-analysis", "t": "Sentiment Analysis", "tg": ["NLP Task", "Classification"], "d": "general", "x": "An NLP task that determines the emotional tone of text (positive, negative, neutral). Used in customer feedback...", "l": "s", "k": ["sentiment", "analysis", "nlp", "task", "determines", "emotional", "tone", "text", "positive", "negative", "neutral", "customer", "feedback", "social", "media"]}, {"id": "term-sentiment-polarity", "t": "Sentiment Polarity", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The classification of text sentiment into categories such as positive, negative, or neutral, representing the overall...", "l": "s", "k": ["sentiment", "polarity", "classification", "text", "categories", "positive", "negative", "neutral", "representing", "overall", "emotional", "orientation", "expressed", "toward", "subject"]}, {"id": "term-sepp-hochreiter", "t": "Sepp Hochreiter", "tg": ["History", "Pioneers"], "d": "history", "x": "Austrian computer scientist who co-invented the Long Short-Term Memory network architecture in 1997 with Jurgen...", "l": "s", "k": ["sepp", "hochreiter", "austrian", "computer", "scientist", "co-invented", "long", "short-term", "memory", "network", "architecture", "jurgen", "schmidhuber", "solving", "fundamental"]}, {"id": "term-seq2seq", "t": "Seq2Seq", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Sequence-to-Sequence, an encoder-decoder framework where an encoder processes an input sequence into a fixed-length...", "l": "s", "k": ["seq2seq", "sequence-to-sequence", "encoder-decoder", "framework", "encoder", "processes", "input", "sequence", "fixed-length", "representation", "decoder", "generates", "output"]}, {"id": "term-seq2seq-model", "t": "Seq2Seq Model", "tg": ["History", "Milestones"], "d": "history", "x": "The sequence-to-sequence model introduced by Ilya Sutskever Oriol Vinyals and Quoc Le at Google in 2014. Using...", "l": "s", "k": ["seq2seq", "model", "sequence-to-sequence", "introduced", "ilya", "sutskever", "oriol", "vinyals", "quoc", "google", "encoder-decoder", "rnn", "architecture", "enabled", "end-to-end"]}, {"id": "term-sequence-labeling", "t": "Sequence Labeling", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of assigning a categorical label to each element in a sequence, such as tagging each word in a sentence with...", "l": "s", "k": ["sequence", "labeling", "task", "assigning", "categorical", "label", "element", "tagging", "word", "sentence", "named", "entity", "type", "pos", "tag"]}, {"id": "term-sequence-parallelism", "t": "Sequence Parallelism", "tg": ["LLM", "Inference"], "d": "models", "x": "A technique that distributes the processing of long sequences across multiple devices by partitioning the sequence...", "l": "s", "k": ["sequence", "parallelism", "technique", "distributes", "processing", "long", "sequences", "across", "multiple", "devices", "partitioning", "dimension", "enabling", "context", "lengths"]}, {"id": "term-sequence-level-accuracy", "t": "Sequence-Level Accuracy", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation metric that considers an entire generated sequence correct only if every token matches the reference,...", "l": "s", "k": ["sequence-level", "accuracy", "evaluation", "metric", "considers", "entire", "generated", "sequence", "correct", "token", "matches", "reference", "providing", "strict", "holistic"]}, {"id": "term-seymour-papert", "t": "Seymour Papert", "tg": ["History", "Pioneers"], "d": "history", "x": "South African-born American mathematician and computer scientist who co-authored the influential book Perceptrons...", "l": "s", "k": ["seymour", "papert", "south", "african-born", "american", "mathematician", "computer", "scientist", "co-authored", "influential", "book", "perceptrons", "marvin", "minsky", "co-founded"]}, {"id": "term-sft", "t": "SFT (Supervised Fine-Tuning)", "tg": ["Training", "Alignment"], "d": "safety", "x": "Training a pre-trained model on labeled examples of desired behavior. Often the first step in aligning LLMs, teaching...", "l": "s", "k": ["sft", "supervised", "fine-tuning", "training", "pre-trained", "model", "labeled", "examples", "desired", "behavior", "step", "aligning", "llms", "teaching", "follow"]}, {"id": "term-shakey-the-robot", "t": "Shakey the Robot", "tg": ["History", "Milestones"], "d": "history", "x": "The first general-purpose mobile robot developed at SRI International from 1966 to 1972 that could reason about its own...", "l": "s", "k": ["shakey", "robot", "general-purpose", "mobile", "developed", "sri", "international", "reason", "actions", "integrating", "computer", "vision", "natural", "language", "understanding"]}, {"id": "term-shannon-entropy", "t": "Shannon Entropy", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A measure of the average information content or uncertainty in a random variable defined as the expected value of the...", "l": "s", "k": ["shannon", "entropy", "measure", "average", "information", "content", "uncertainty", "random", "variable", "defined", "expected", "value", "negative", "log", "probability"]}, {"id": "term-shap-values", "t": "SHAP Values", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "SHapley Additive exPlanations, a unified framework for feature importance that assigns each feature a contribution...", "l": "s", "k": ["shap", "values", "shapley", "additive", "explanations", "unified", "framework", "feature", "importance", "assigns", "contribution", "value", "specific", "prediction", "based"]}, {"id": "term-shap-e", "t": "Shap-E", "tg": ["Models", "Technical"], "d": "models", "x": "A model by OpenAI that directly generates 3D implicit functions from text or images. Produces textured 3D meshes that...", "l": "s", "k": ["shap-e", "model", "openai", "directly", "generates", "implicit", "functions", "text", "images", "produces", "textured", "meshes", "rendered", "viewpoint", "faster"]}, {"id": "term-shaped-reward", "t": "Shaped Reward", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "An auxiliary reward signal added to the environment's natural reward to guide learning toward desired behavior. Shaped...", "l": "s", "k": ["shaped", "reward", "auxiliary", "signal", "added", "environment", "natural", "guide", "learning", "toward", "desired", "behavior", "rewards", "provide", "frequent"]}, {"id": "term-shapiro-wilk-test", "t": "Shapiro-Wilk Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A statistical test for normality that evaluates whether a random sample comes from a normal distribution. It is...", "l": "s", "k": ["shapiro-wilk", "test", "statistical", "normality", "evaluates", "random", "sample", "comes", "normal", "distribution", "considered", "powerful", "tests", "especially", "small"]}, {"id": "term-shared-memory-gpu", "t": "Shared Memory (GPU)", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "A fast, programmer-managed on-chip SRAM in GPU streaming multiprocessors that enables efficient data sharing between...", "l": "s", "k": ["shared", "memory", "gpu", "fast", "programmer-managed", "on-chip", "sram", "streaming", "multiprocessors", "enables", "efficient", "data", "sharing", "threads", "thread"]}, {"id": "term-sharpness-aware-minimization", "t": "Sharpness-Aware Minimization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An optimization procedure that seeks parameters in neighborhoods with uniformly low loss rather than just minimizing...", "l": "s", "k": ["sharpness-aware", "minimization", "optimization", "procedure", "seeks", "parameters", "neighborhoods", "uniformly", "low", "loss", "rather", "minimizing", "single", "point", "improves"]}, {"id": "term-shrdlu", "t": "SHRDLU", "tg": ["History", "Milestones"], "d": "history", "x": "A natural language understanding program created by Terry Winograd at MIT in 1970 that could converse about and...", "l": "s", "k": ["shrdlu", "natural", "language", "understanding", "program", "created", "terry", "winograd", "mit", "converse", "manipulate", "objects", "simulated", "blocks", "world"]}, {"id": "term-shufflenet", "t": "ShuffleNet", "tg": ["Models", "Technical"], "d": "models", "x": "A lightweight CNN architecture designed for mobile devices that uses pointwise group convolutions and channel shuffle...", "l": "s", "k": ["shufflenet", "lightweight", "cnn", "architecture", "designed", "mobile", "devices", "uses", "pointwise", "group", "convolutions", "channel", "shuffle", "operations", "reduces"]}, {"id": "term-sift", "t": "SIFT", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "Scale-Invariant Feature Transform, a classical algorithm that detects and describes local image features robust to...", "l": "s", "k": ["sift", "scale-invariant", "feature", "transform", "classical", "algorithm", "detects", "describes", "local", "image", "features", "robust", "scale", "rotation", "illumination"]}, {"id": "term-siglip", "t": "SigLIP", "tg": ["Models", "Technical"], "d": "models", "x": "Sigmoid Loss for Language Image Pretraining replaces the softmax-based contrastive loss in CLIP with a sigmoid loss...", "l": "s", "k": ["siglip", "sigmoid", "loss", "language", "image", "pretraining", "replaces", "softmax-based", "contrastive", "clip", "operates", "individual", "image-text", "pairs", "simpler"]}, {"id": "term-sigmoid", "t": "Sigmoid", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A classic activation function that maps inputs to values between 0 and 1 using the formula f(x) = 1 / (1 + exp(-x))....", "l": "s", "k": ["sigmoid", "classic", "activation", "function", "maps", "inputs", "values", "formula", "exp", "historically", "important", "neural", "networks", "binary", "classification"]}, {"id": "term-signed-distance-function", "t": "Signed Distance Function", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A 3D representation where a neural network predicts the signed distance from any 3D point to the nearest surface, with...", "l": "s", "k": ["signed", "distance", "function", "representation", "neural", "network", "predicts", "point", "nearest", "surface", "zero-level", "defining", "shape", "enabling", "smooth"]}, {"id": "term-significance-level", "t": "Significance Level", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The threshold probability (alpha, typically 0.05) below which the p-value must fall for the null hypothesis to be...", "l": "s", "k": ["significance", "level", "threshold", "probability", "alpha", "typically", "p-value", "must", "fall", "null", "hypothesis", "rejected", "defines", "acceptable", "risk"]}, {"id": "term-silhouette-score", "t": "Silhouette Score", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "A metric for evaluating clustering quality that measures how similar each point is to its own cluster compared to other...", "l": "s", "k": ["silhouette", "score", "metric", "evaluating", "clustering", "quality", "measures", "similar", "point", "cluster", "compared", "clusters", "values", "range", "higher"]}, {"id": "term-silu", "t": "SiLU", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Sigmoid Linear Unit activation function defined as f(x) = x * sigmoid(x). Mathematically equivalent to Swish with beta...", "l": "s", "k": ["silu", "sigmoid", "linear", "unit", "activation", "function", "defined", "mathematically", "equivalent", "swish", "beta", "equal", "widely", "modern", "architectures"]}, {"id": "term-sim-to-real", "t": "Sim-to-Real Transfer", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "The challenge and set of techniques for deploying policies trained in simulation to physical systems. Sim-to-real...", "l": "s", "k": ["sim-to-real", "transfer", "challenge", "techniques", "deploying", "policies", "trained", "simulation", "physical", "systems", "methods", "include", "domain", "randomization", "system"]}, {"id": "term-simclr", "t": "SimCLR", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A contrastive self-supervised learning framework for visual representations that uses data augmentation to create...", "l": "s", "k": ["simclr", "contrastive", "self-supervised", "learning", "framework", "visual", "representations", "uses", "data", "augmentation", "create", "positive", "pairs", "projection", "head"]}, {"id": "term-similarity-threshold", "t": "Similarity Threshold", "tg": ["Vector Database", "Search"], "d": "general", "x": "A configurable cutoff value that filters vector search results to include only matches exceeding a minimum similarity...", "l": "s", "k": ["similarity", "threshold", "configurable", "cutoff", "value", "filters", "vector", "search", "results", "include", "matches", "exceeding", "minimum", "score", "preventing"]}, {"id": "term-simpsons-paradox", "t": "Simpson's Paradox", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A statistical phenomenon where a trend that appears in several different groups of data disappears or reverses when...", "l": "s", "k": ["simpson", "paradox", "statistical", "phenomenon", "trend", "appears", "several", "different", "groups", "data", "disappears", "reverses", "combined", "highlights", "importance"]}, {"id": "term-simtom-prompting", "t": "SimToM Prompting", "tg": ["Prompt Engineering", "Theory of Mind"], "d": "general", "x": "A two-step prompting approach for theory-of-mind tasks that first identifies what information a specific person is...", "l": "s", "k": ["simtom", "prompting", "two-step", "approach", "theory-of-mind", "tasks", "identifies", "information", "specific", "person", "aware", "answers", "question", "based", "solely"]}, {"id": "term-simulated-annealing", "t": "Simulated Annealing", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A probabilistic optimization algorithm inspired by the annealing process in metallurgy. Accepts worse solutions with...", "l": "s", "k": ["simulated", "annealing", "probabilistic", "optimization", "algorithm", "inspired", "process", "metallurgy", "accepts", "worse", "solutions", "decreasing", "probability", "time", "allowing"]}, {"id": "term-singular-learning-theory", "t": "Singular Learning Theory", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A mathematical framework using algebraic geometry to study the loss landscapes of neural networks. Provides tools for...", "l": "s", "k": ["singular", "learning", "theory", "mathematical", "framework", "algebraic", "geometry", "study", "loss", "landscapes", "neural", "networks", "provides", "tools", "understanding"]}, {"id": "term-singular-value-decomposition", "t": "Singular Value Decomposition", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "A matrix factorization technique that decomposes a matrix into three matrices (U, S, V^T), where S contains singular...", "l": "s", "k": ["singular", "value", "decomposition", "matrix", "factorization", "technique", "decomposes", "matrices", "contains", "values", "foundational", "pca", "latent", "semantic", "analysis"]}, {"id": "term-singularity-concept", "t": "Singularity Concept", "tg": ["History", "Milestones"], "d": "history", "x": "The hypothesized future point when AI surpasses human intelligence and triggers runaway technological growth,...", "l": "s", "k": ["singularity", "concept", "hypothesized", "future", "point", "surpasses", "human", "intelligence", "triggers", "runaway", "technological", "growth", "popularized", "vernor", "vinge"]}, {"id": "term-sinusoidal-position-encoding", "t": "Sinusoidal Position Encoding", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "The original position encoding used in the Transformer architecture. Uses sine and cosine functions of different...", "l": "s", "k": ["sinusoidal", "position", "encoding", "original", "transformer", "architecture", "uses", "sine", "cosine", "functions", "different", "frequencies", "generate", "unique", "vectors"]}, {"id": "term-sinusoidal-positional-encoding", "t": "Sinusoidal Positional Encoding", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A fixed positional encoding scheme that uses sine and cosine functions of different frequencies to encode absolute...", "l": "s", "k": ["sinusoidal", "positional", "encoding", "fixed", "scheme", "uses", "sine", "cosine", "functions", "different", "frequencies", "encode", "absolute", "token", "positions"]}, {"id": "term-siri", "t": "Siri", "tg": ["History", "Systems"], "d": "history", "x": "A virtual assistant developed by SRI International and later acquired by Apple in 2010. Launched as an iOS feature in...", "l": "s", "k": ["siri", "virtual", "assistant", "developed", "sri", "international", "later", "acquired", "apple", "launched", "ios", "feature", "widely", "deployed", "assistants"]}, {"id": "term-siri-launch", "t": "Siri Launch", "tg": ["History", "Milestones"], "d": "history", "x": "Apple's launch of Siri in October 2011 as the first widely deployed virtual assistant on a smartphone, introducing...", "l": "s", "k": ["siri", "launch", "apple", "october", "widely", "deployed", "virtual", "assistant", "smartphone", "introducing", "millions", "consumers", "conversational", "voice-activated", "computing"]}, {"id": "term-situated-cognition", "t": "Situated Cognition", "tg": ["History", "Fundamentals"], "d": "history", "x": "A theory in cognitive science and AI that emphasizes that cognition is inseparable from the context in which it occurs....", "l": "s", "k": ["situated", "cognition", "theory", "cognitive", "science", "emphasizes", "inseparable", "context", "occurs", "developed", "researchers", "including", "lucy", "suchman", "rodney"]}, {"id": "term-situation-calculus", "t": "Situation Calculus", "tg": ["History", "Fundamentals"], "d": "history", "x": "A logical formalism for representing and reasoning about dynamically changing worlds proposed by John McCarthy in 1963....", "l": "s", "k": ["situation", "calculus", "logical", "formalism", "representing", "reasoning", "dynamically", "changing", "worlds", "proposed", "john", "mccarthy", "uses", "first-order", "logic"]}, {"id": "term-skeleton-of-thought", "t": "Skeleton-of-Thought", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting strategy that first asks the model to generate a skeleton outline of the answer, then expands each point in...", "l": "s", "k": ["skeleton-of-thought", "prompting", "strategy", "asks", "model", "generate", "skeleton", "outline", "answer", "expands", "point", "parallel", "reducing", "end-to-end", "latency"]}, {"id": "term-skill-discovery", "t": "Skill Discovery", "tg": ["Reinforcement Learning", "Exploration"], "d": "general", "x": "Unsupervised methods that learn reusable behavioral primitives or skills without task-specific rewards, typically by...", "l": "s", "k": ["skill", "discovery", "unsupervised", "methods", "learn", "reusable", "behavioral", "primitives", "skills", "without", "task-specific", "rewards", "typically", "maximizing", "mutual"]}, {"id": "term-skip-connection", "t": "Skip Connection", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A shortcut path that bypasses one or more layers by adding or concatenating the input of a block directly to its...", "l": "s", "k": ["skip", "connection", "shortcut", "path", "bypasses", "layers", "adding", "concatenating", "input", "block", "directly", "output", "enabling", "gradient", "flow"]}, {"id": "term-skip-connections", "t": "Skip Connections", "tg": ["History", "Fundamentals"], "d": "history", "x": "Direct connections between non-adjacent layers in a neural network that allow gradients and information to bypass...", "l": "s", "k": ["skip", "connections", "direct", "non-adjacent", "layers", "neural", "network", "allow", "gradients", "information", "bypass", "intermediate", "introduced", "resnet", "enabled"]}, {"id": "term-skip-gram", "t": "Skip-gram", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A Word2Vec training objective that predicts surrounding context words given a center word, learning word...", "l": "s", "k": ["skip-gram", "word2vec", "training", "objective", "predicts", "surrounding", "context", "words", "given", "center", "word", "learning", "representations", "capture", "semantic"]}, {"id": "term-sliding-window-attention", "t": "Sliding Window Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention pattern where each token attends only to a fixed-size local window of neighboring tokens, enabling...", "l": "s", "k": ["sliding", "window", "attention", "pattern", "token", "attends", "fixed-size", "local", "neighboring", "tokens", "enabling", "efficient", "processing", "long", "sequences"]}, {"id": "term-slot-filling", "t": "Slot Filling", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of extracting specific pieces of information (slot values) from user utterances in a task-oriented dialogue...", "l": "s", "k": ["slot", "filling", "task", "extracting", "specific", "pieces", "information", "values", "user", "utterances", "task-oriented", "dialogue", "system", "dates", "locations"]}, {"id": "term-smoothquant", "t": "SmoothQuant", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A quantization technique that migrates the quantization difficulty from activations to weights by applying per-channel...", "l": "s", "k": ["smoothquant", "quantization", "technique", "migrates", "difficulty", "activations", "weights", "applying", "per-channel", "scaling", "factors", "enabling", "efficient", "int8", "llms"]}, {"id": "term-smote", "t": "SMOTE", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "Synthetic Minority Over-sampling Technique, an oversampling method that creates synthetic examples of the minority...", "l": "s", "k": ["smote", "synthetic", "minority", "over-sampling", "technique", "oversampling", "method", "creates", "examples", "class", "interpolating", "existing", "samples", "k-nearest", "neighbors"]}, {"id": "term-snapshot-ensemble", "t": "Snapshot Ensemble", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An ensemble technique that collects multiple models from a single training run by saving checkpoints at different local...", "l": "s", "k": ["snapshot", "ensemble", "technique", "collects", "multiple", "models", "single", "training", "run", "saving", "checkpoints", "different", "local", "minima", "reached"]}, {"id": "term-sneps", "t": "SNePS", "tg": ["History", "Systems"], "d": "history", "x": "The Semantic Network Processing System developed by Stuart Shapiro at the University of Buffalo beginning in 1978....", "l": "s", "k": ["sneps", "semantic", "network", "processing", "system", "developed", "stuart", "shapiro", "university", "buffalo", "beginning", "provided", "knowledge", "representation", "reasoning"]}, {"id": "term-soar", "t": "SOAR", "tg": ["History", "Systems"], "d": "history", "x": "A cognitive architecture developed by John Laird Allen Newell and Paul Rosenbloom at Carnegie Mellon in the 1980s. SOAR...", "l": "s", "k": ["soar", "cognitive", "architecture", "developed", "john", "laird", "allen", "newell", "paul", "rosenbloom", "carnegie", "mellon", "1980s", "models", "general"]}, {"id": "term-social-scoring-systems", "t": "Social Scoring Systems", "tg": ["AI Ethics", "Regulation"], "d": "safety", "x": "AI-driven systems that assign scores to individuals based on their behavior, social connections, or characteristics,...", "l": "s", "k": ["social", "scoring", "systems", "ai-driven", "assign", "scores", "individuals", "based", "behavior", "connections", "characteristics", "determine", "access", "services", "banned"]}, {"id": "term-society-of-mind", "t": "Society of Mind", "tg": ["History", "Fundamentals"], "d": "history", "x": "A theory of natural and artificial intelligence proposed by Marvin Minsky in his 1986 book of the same name. The theory...", "l": "s", "k": ["society", "mind", "theory", "natural", "artificial", "intelligence", "proposed", "marvin", "minsky", "book", "name", "proposes", "single", "entity", "tiny"]}, {"id": "term-society-of-mind-book", "t": "Society of Mind Book", "tg": ["History", "Milestones"], "d": "history", "x": "The 1986 book by Marvin Minsky proposing that intelligence emerges from the interaction of many simple agents or...", "l": "s", "k": ["society", "mind", "book", "marvin", "minsky", "proposing", "intelligence", "emerges", "interaction", "simple", "agents", "processes", "theory", "influenced", "thinking"]}, {"id": "term-sociotechnical-systems-approach", "t": "Sociotechnical Systems Approach", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "An analytical framework that views AI systems as inseparable from their social context, recognizing that technical and...", "l": "s", "k": ["sociotechnical", "systems", "approach", "analytical", "framework", "views", "inseparable", "social", "context", "recognizing", "technical", "factors", "jointly", "determine", "system"]}, {"id": "term-socratic-prompting", "t": "Socratic Prompting", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting technique that guides the model through a series of probing questions rather than direct instructions,...", "l": "s", "k": ["socratic", "prompting", "technique", "guides", "model", "series", "probing", "questions", "rather", "direct", "instructions", "encouraging", "step-by-step", "reasoning", "self-discovery"]}, {"id": "term-sac", "t": "Soft Actor-Critic (SAC)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An off-policy actor-critic algorithm that maximizes both expected return and policy entropy, encouraging exploration...", "l": "s", "k": ["soft", "actor-critic", "sac", "off-policy", "algorithm", "maximizes", "expected", "return", "policy", "entropy", "encouraging", "exploration", "maintaining", "stable", "learning"]}, {"id": "term-soft-attention", "t": "Soft Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that computes a weighted average over all positions using continuous attention weights, making...", "l": "s", "k": ["soft", "attention", "mechanism", "computes", "weighted", "average", "positions", "continuous", "weights", "making", "fully", "differentiable", "trainable", "standard", "backpropagation"]}, {"id": "term-soft-update", "t": "Soft Update (Polyak Averaging)", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A method for updating target network parameters as an exponential moving average of the online network parameters,...", "l": "s", "k": ["soft", "update", "polyak", "averaging", "method", "updating", "target", "network", "parameters", "exponential", "moving", "average", "online", "controlled", "smoothing"]}, {"id": "term-softmax", "t": "Softmax", "tg": ["Math", "Function"], "d": "general", "x": "A function that converts raw scores into probabilities that sum to 1. Used in attention mechanisms and classification...", "l": "s", "k": ["softmax", "function", "converts", "raw", "scores", "probabilities", "sum", "attention", "mechanisms", "classification", "outputs", "create", "interpretable", "probability", "distributions"]}, {"id": "term-softmax-function", "t": "Softmax Function", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A function that normalizes a vector of real numbers into a probability distribution by exponentiating each element and...", "l": "s", "k": ["softmax", "function", "normalizes", "vector", "real", "numbers", "probability", "distribution", "exponentiating", "element", "dividing", "sum", "attention", "mechanisms", "classification"]}, {"id": "term-softmax-temperature", "t": "Softmax Temperature", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A parameter that controls the sharpness of the softmax probability distribution. Temperature values below 1 sharpen the...", "l": "s", "k": ["softmax", "temperature", "parameter", "controls", "sharpness", "probability", "distribution", "values", "sharpen", "making", "peaked", "flatten", "uniform", "knowledge", "distillation"]}, {"id": "term-softplus", "t": "Softplus", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A smooth approximation of ReLU defined as f(x) = log(1 + exp(x)). Always positive and differentiable everywhere. Used...", "l": "s", "k": ["softplus", "smooth", "approximation", "relu", "defined", "log", "exp", "always", "positive", "differentiable", "everywhere", "various", "contexts", "including", "variational"]}, {"id": "term-softsign", "t": "Softsign", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An activation function defined as f(x) = x / (1 + |x|) that provides similar output range to tanh but with polynomial...", "l": "s", "k": ["softsign", "activation", "function", "defined", "provides", "similar", "output", "range", "tanh", "polynomial", "decay", "instead", "exponential", "converges", "slowly"]}, {"id": "term-solomonoff-induction", "t": "Solomonoff Induction", "tg": ["History", "Fundamentals"], "d": "history", "x": "A mathematical theory of universal prediction developed by Ray Solomonoff in 1964. Based on algorithmic probability it...", "l": "s", "k": ["solomonoff", "induction", "mathematical", "theory", "universal", "prediction", "developed", "ray", "based", "algorithmic", "probability", "provides", "idealized", "framework", "inductive"]}, {"id": "term-sora", "t": "Sora", "tg": ["Models", "Technical"], "d": "models", "x": "A text-to-video generation model by OpenAI that produces realistic videos up to a minute long from text prompts. Uses a...", "l": "s", "k": ["sora", "text-to-video", "generation", "model", "openai", "produces", "realistic", "videos", "minute", "long", "text", "prompts", "uses", "diffusion", "transformer"]}, {"id": "term-sparse-attention", "t": "Sparse Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that restricts each token to attend to only a subset of other tokens using predefined or learned...", "l": "s", "k": ["sparse", "attention", "mechanism", "restricts", "token", "attend", "subset", "tokens", "predefined", "learned", "sparsity", "patterns", "reducing", "quadratic", "computational"]}, {"id": "term-sparse-autoencoder", "t": "Sparse Autoencoder", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An autoencoder that enforces sparsity constraints on the hidden layer activations, encouraging the network to learn a...", "l": "s", "k": ["sparse", "autoencoder", "enforces", "sparsity", "constraints", "hidden", "layer", "activations", "encouraging", "network", "learn", "compact", "distributed", "representation", "neurons"]}, {"id": "term-sparse-autoencoder-model", "t": "Sparse Autoencoder Model", "tg": ["Models", "Technical"], "d": "models", "x": "An autoencoder with a sparsity constraint encouraging most hidden units to be inactive for any given input. Learns...", "l": "s", "k": ["sparse", "autoencoder", "model", "sparsity", "constraint", "encouraging", "hidden", "units", "inactive", "given", "input", "learns", "overcomplete", "representations", "feature"]}, {"id": "term-sparse-models", "t": "Sparse Models", "tg": ["History", "Fundamentals"], "d": "history", "x": "Neural network models where only a fraction of parameters are activated for any given input. Sparse architectures like...", "l": "s", "k": ["sparse", "models", "neural", "network", "fraction", "parameters", "activated", "given", "input", "architectures", "mixture", "experts", "enable", "larger", "manageable"]}, {"id": "term-sparse-retrieval", "t": "Sparse Retrieval", "tg": ["Retrieval", "Search"], "d": "general", "x": "An information retrieval paradigm that represents queries and documents as high-dimensional sparse vectors where most...", "l": "s", "k": ["sparse", "retrieval", "information", "paradigm", "represents", "queries", "documents", "high-dimensional", "vectors", "values", "zero", "non-zero", "entries", "corresponding", "term"]}, {"id": "term-sparse-reward", "t": "Sparse Reward Problem", "tg": ["Reinforcement Learning", "Reward Design"], "d": "general", "x": "An RL setting where the agent receives non-zero reward signals only rarely, making credit assignment extremely...", "l": "s", "k": ["sparse", "reward", "problem", "setting", "agent", "receives", "non-zero", "signals", "rarely", "making", "credit", "assignment", "extremely", "difficult", "rewards"]}, {"id": "term-sparse-transformer", "t": "Sparse Transformer", "tg": ["Models", "Technical"], "d": "models", "x": "A transformer variant that uses sparse factorizations of the attention matrix to reduce computational complexity....", "l": "s", "k": ["sparse", "transformer", "variant", "uses", "factorizations", "attention", "matrix", "reduce", "computational", "complexity", "demonstrates", "carefully", "designed", "patterns", "match"]}, {"id": "term-spatial-attention", "t": "Spatial Attention", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An attention mechanism that learns to weight different spatial locations in feature maps, focusing computational...", "l": "s", "k": ["spatial", "attention", "mechanism", "learns", "weight", "different", "locations", "feature", "maps", "focusing", "computational", "resources", "informative", "regions", "input"]}, {"id": "term-spatial-reasoning-in-ai", "t": "Spatial Reasoning in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "The area of AI concerned with representing and reasoning about spatial relationships between objects and regions....", "l": "s", "k": ["spatial", "reasoning", "area", "concerned", "representing", "relationships", "objects", "regions", "applications", "include", "robot", "navigation", "geographic", "information", "systems"]}, {"id": "term-spearman-rank-correlation", "t": "Spearman Rank Correlation", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A non-parametric measure of the monotonic relationship between two variables, computed as the Pearson correlation of...", "l": "s", "k": ["spearman", "rank", "correlation", "non-parametric", "measure", "monotonic", "relationship", "variables", "computed", "pearson", "ranked", "values", "assume", "linearity", "normal"]}, {"id": "term-special-tokens", "t": "Special Tokens", "tg": ["NLP", "Tokenization"], "d": "general", "x": "Reserved tokens in a vocabulary that serve structural purposes such as marking sequence boundaries, separating...", "l": "s", "k": ["special", "tokens", "reserved", "vocabulary", "serve", "structural", "purposes", "marking", "sequence", "boundaries", "separating", "segments", "padding", "indicating", "masked"]}, {"id": "term-specification-gaming", "t": "Specification Gaming", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The behavior of an AI system that satisfies the literal specification of an objective without achieving the intended...", "l": "s", "k": ["specification", "gaming", "behavior", "system", "satisfies", "literal", "objective", "without", "achieving", "intended", "outcome", "closely", "related", "reward", "hacking"]}, {"id": "term-specificity", "t": "Specificity", "tg": ["Machine Learning", "Metrics"], "d": "datasets", "x": "The proportion of actual negative cases that are correctly identified as negative by a classifier, also known as the...", "l": "s", "k": ["specificity", "proportion", "actual", "negative", "cases", "correctly", "identified", "classifier", "known", "true", "rate", "complements", "sensitivity", "recall", "binary"]}, {"id": "term-spectral-clustering", "t": "Spectral Clustering", "tg": ["Machine Learning", "Clustering"], "d": "general", "x": "A clustering technique that uses the eigenvalues and eigenvectors of a similarity matrix derived from the data to...", "l": "s", "k": ["spectral", "clustering", "technique", "uses", "eigenvalues", "eigenvectors", "similarity", "matrix", "derived", "data", "perform", "dimensionality", "reduction", "reduced", "space"]}, {"id": "term-spectral-normalization", "t": "Spectral Normalization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A weight normalization technique that constrains the spectral norm of weight matrices to stabilize training. Widely...", "l": "s", "k": ["spectral", "normalization", "weight", "technique", "constrains", "norm", "matrices", "stabilize", "training", "widely", "gans", "enforce", "lipschitz", "continuity", "discriminator"]}, {"id": "term-spectrogram", "t": "Spectrogram", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A visual representation of the spectrum of frequencies in a signal as they vary over time. Commonly displayed as a...", "l": "s", "k": ["spectrogram", "visual", "representation", "spectrum", "frequencies", "signal", "vary", "time", "commonly", "displayed", "heatmap", "x-axis", "frequency", "y-axis", "intensity"]}, {"id": "term-speculative-decoding", "t": "Speculative Decoding", "tg": ["Optimization", "Inference"], "d": "algorithms", "x": "A technique to speed up LLM inference by using a smaller model to draft tokens that the larger model verifies in...", "l": "s", "k": ["speculative", "decoding", "technique", "speed", "llm", "inference", "smaller", "model", "draft", "tokens", "larger", "verifies", "parallel", "maintains", "output"]}, {"id": "term-speculative-execution-llm", "t": "Speculative Execution", "tg": ["LLM", "Inference"], "d": "models", "x": "A broader inference optimization paradigm where cheaper computations predict likely outcomes that are verified by...", "l": "s", "k": ["speculative", "execution", "broader", "inference", "optimization", "paradigm", "cheaper", "computations", "predict", "likely", "outcomes", "verified", "full-cost", "operations", "encompassing"]}, {"id": "term-speculative-sampling", "t": "Speculative Sampling", "tg": ["LLM", "Inference"], "d": "models", "x": "An inference acceleration technique where a fast draft model proposes multiple tokens that are then verified in...", "l": "s", "k": ["speculative", "sampling", "inference", "acceleration", "technique", "fast", "draft", "model", "proposes", "multiple", "tokens", "verified", "parallel", "target", "maintaining"]}, {"id": "term-speech-recognition-history", "t": "Speech Recognition History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of automatic speech recognition from Bell Labs' Audrey system in 1952 through Hidden Markov Models in...", "l": "s", "k": ["speech", "recognition", "history", "development", "automatic", "bell", "labs", "audrey", "system", "hidden", "markov", "models", "1980s", "deep", "learning"]}, {"id": "term-speech-synthesis", "t": "Speech Synthesis", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The artificial production of human-like speech from text or other input, using techniques ranging from concatenative...", "l": "s", "k": ["speech", "synthesis", "artificial", "production", "human-like", "text", "input", "techniques", "ranging", "concatenative", "neural", "models", "tacotron", "vits"]}, {"id": "term-spell-correction", "t": "Spell Correction", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The automated detection and correction of misspelled words in text using techniques such as edit distance, language...", "l": "s", "k": ["spell", "correction", "automated", "detection", "misspelled", "words", "text", "techniques", "edit", "distance", "language", "models", "context-aware", "suggest", "corrections"]}, {"id": "term-spin", "t": "SPIN", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Self-Play Fine-Tuning is an alignment method where the language model generates training data by distinguishing its own...", "l": "s", "k": ["spin", "self-play", "fine-tuning", "alignment", "method", "language", "model", "generates", "training", "data", "distinguishing", "outputs", "human-written", "text", "framework"]}, {"id": "term-splade", "t": "SPLADE", "tg": ["Retrieval", "Architecture"], "d": "models", "x": "SParse Lexical AnD Expansion model, a learned sparse retrieval method that predicts importance weights for vocabulary...", "l": "s", "k": ["splade", "sparse", "lexical", "expansion", "model", "learned", "retrieval", "method", "predicts", "importance", "weights", "vocabulary", "terms", "including", "present"]}, {"id": "term-spline-regression", "t": "Spline Regression", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A regression technique using piecewise polynomial functions (splines) joined at knot points to fit flexible, smooth...", "l": "s", "k": ["spline", "regression", "technique", "piecewise", "polynomial", "functions", "splines", "joined", "knot", "points", "fit", "flexible", "smooth", "curves", "natural"]}, {"id": "term-spot-instances", "t": "Spot Instances for AI", "tg": ["Distributed Computing", "Inference Infrastructure"], "d": "hardware", "x": "Discounted cloud computing instances that use spare capacity at 60-90% less than on-demand pricing but can be...", "l": "s", "k": ["spot", "instances", "discounted", "cloud", "computing", "spare", "capacity", "60-90", "less", "on-demand", "pricing", "interrupted", "short", "notice", "widely"]}, {"id": "term-squad", "t": "SQuAD", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "Stanford Question Answering Dataset, a reading comprehension benchmark where models extract answer spans from Wikipedia...", "l": "s", "k": ["squad", "stanford", "question", "answering", "dataset", "reading", "comprehension", "benchmark", "models", "extract", "answer", "spans", "wikipedia", "passages", "additionally"]}, {"id": "term-squad-dataset", "t": "SQuAD Dataset", "tg": ["History", "Milestones"], "d": "history", "x": "The Stanford Question Answering Dataset created in 2016 consisting of questions posed about Wikipedia articles where...", "l": "s", "k": ["squad", "dataset", "stanford", "question", "answering", "created", "consisting", "questions", "posed", "wikipedia", "articles", "answer", "segment", "text", "article"]}, {"id": "term-squeeze-and-excitation", "t": "Squeeze-and-Excitation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A channel attention mechanism that adaptively recalibrates channel-wise feature responses. Uses global average pooling...", "l": "s", "k": ["squeeze-and-excitation", "channel", "attention", "mechanism", "adaptively", "recalibrates", "channel-wise", "feature", "responses", "uses", "global", "average", "pooling", "followed", "fully"]}, {"id": "term-squeeze-and-excitation-network", "t": "Squeeze-and-Excitation Network", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A CNN enhancement that adaptively recalibrates channel-wise feature responses by using global average pooling followed...", "l": "s", "k": ["squeeze-and-excitation", "network", "cnn", "enhancement", "adaptively", "recalibrates", "channel-wise", "feature", "responses", "global", "average", "pooling", "followed", "small", "learn"]}, {"id": "term-squeeze-excitation-block", "t": "Squeeze-Excitation Block", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A channel attention module that squeezes spatial information via global pooling and excites channel-wise features...", "l": "s", "k": ["squeeze-excitation", "block", "channel", "attention", "module", "squeezes", "spatial", "information", "via", "global", "pooling", "excites", "channel-wise", "features", "learned"]}, {"id": "term-squeezenet", "t": "SqueezeNet", "tg": ["Models", "Technical"], "d": "models", "x": "A compact CNN architecture that achieves AlexNet-level accuracy with 50x fewer parameters using fire modules consisting...", "l": "s", "k": ["squeezenet", "compact", "cnn", "architecture", "achieves", "alexnet-level", "accuracy", "50x", "fewer", "parameters", "fire", "modules", "consisting", "squeeze", "expand"]}, {"id": "term-sri-international", "t": "SRI International", "tg": ["History", "Organizations"], "d": "history", "x": "An American nonprofit research institute originally Stanford Research Institute founded in 1946. Developed Shakey the...", "l": "s", "k": ["sri", "international", "american", "nonprofit", "research", "institute", "originally", "stanford", "founded", "developed", "shakey", "robot", "late", "1960s", "created"]}, {"id": "term-ssd", "t": "SSD", "tg": ["Models", "Technical"], "d": "models", "x": "Single Shot MultiBox Detector is a one-stage object detection model that predicts bounding boxes and class scores at...", "l": "s", "k": ["ssd", "single", "shot", "multibox", "detector", "one-stage", "object", "detection", "model", "predicts", "bounding", "boxes", "class", "scores", "multiple"]}, {"id": "term-ssd-object-detection", "t": "SSD Object Detection", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "Single Shot MultiBox Detector, a real-time object detection architecture that predicts bounding boxes and class...", "l": "s", "k": ["ssd", "object", "detection", "single", "shot", "multibox", "detector", "real-time", "architecture", "predicts", "bounding", "boxes", "class", "probabilities", "multiple"]}, {"id": "term-ssim", "t": "SSIM", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Structural Similarity Index Measure compares images based on luminance contrast and structure. Designed to be more...", "l": "s", "k": ["ssim", "structural", "similarity", "index", "measure", "compares", "images", "based", "luminance", "contrast", "structure", "designed", "perceptually", "relevant", "pixel-wise"]}, {"id": "term-stable-diffusion", "t": "Stable Diffusion", "tg": ["Model", "Image Generation"], "d": "models", "x": "An open-source text-to-image model from Stability AI. Its open nature enabled a large ecosystem of fine-tuned models,...", "l": "s", "k": ["stable", "diffusion", "open-source", "text-to-image", "model", "stability", "open", "nature", "enabled", "large", "ecosystem", "fine-tuned", "models", "extensions", "applications"]}, {"id": "term-stable-diffusion-xl", "t": "Stable Diffusion XL", "tg": ["Models", "Technical"], "d": "models", "x": "An enhanced version of Stable Diffusion with a larger UNet backbone and a two-stage architecture using a base model and...", "l": "s", "k": ["stable", "diffusion", "enhanced", "version", "larger", "unet", "backbone", "two-stage", "architecture", "base", "model", "refiner", "produces", "higher", "resolution"]}, {"id": "term-stable-video-diffusion", "t": "Stable Video Diffusion", "tg": ["Models", "Technical"], "d": "models", "x": "A video generation model by Stability AI based on latent diffusion adapted for temporal generation. Generates short...", "l": "s", "k": ["stable", "video", "diffusion", "generation", "model", "stability", "based", "latent", "adapted", "temporal", "generates", "short", "clips", "image", "text"]}, {"id": "term-stablelm", "t": "StableLM", "tg": ["Models", "Technical"], "d": "models", "x": "A family of language models by Stability AI designed for both research and commercial use. Features models at various...", "l": "s", "k": ["stablelm", "family", "language", "models", "stability", "designed", "research", "commercial", "features", "various", "sizes", "trained", "diverse", "multilingual", "data"]}, {"id": "term-stacking", "t": "Stacking", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An ensemble learning technique that trains a meta-learner to combine the predictions of multiple base models, using...", "l": "s", "k": ["stacking", "ensemble", "learning", "technique", "trains", "meta-learner", "combine", "predictions", "multiple", "base", "models", "cross-validated", "input", "features"]}, {"id": "term-stakeholder-analysis-in-ai", "t": "Stakeholder Analysis in AI", "tg": ["Governance", "AI Ethics"], "d": "safety", "x": "The process of identifying all individuals, groups, and communities affected by an AI system and systematically...", "l": "s", "k": ["stakeholder", "analysis", "process", "identifying", "individuals", "groups", "communities", "affected", "system", "systematically", "considering", "interests", "power", "dynamics", "potential"]}, {"id": "term-stance-detection", "t": "Stance Detection", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of determining an author's position (favor, against, or neutral) toward a specific target or claim from their...", "l": "s", "k": ["stance", "detection", "task", "determining", "author", "position", "favor", "against", "neutral", "toward", "specific", "target", "claim", "text", "related"]}, {"id": "term-standard-deviation", "t": "Standard Deviation", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "The square root of the variance, measuring the average spread of data points from the mean in the original units of...", "l": "s", "k": ["standard", "deviation", "square", "root", "variance", "measuring", "average", "spread", "data", "points", "mean", "original", "units", "measurement", "quantifies"]}, {"id": "term-standardization", "t": "Standardization", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A feature scaling technique that transforms data to have zero mean and unit variance by subtracting the mean and...", "l": "s", "k": ["standardization", "feature", "scaling", "technique", "transforms", "data", "zero", "mean", "unit", "variance", "subtracting", "dividing", "standard", "deviation", "particularly"]}, {"id": "term-stanford-ai-laboratory", "t": "Stanford AI Laboratory", "tg": ["History", "Milestones"], "d": "history", "x": "A research laboratory founded by John McCarthy at Stanford University in 1962 that became one of the leading centers...", "l": "s", "k": ["stanford", "laboratory", "research", "founded", "john", "mccarthy", "university", "became", "leading", "centers", "making", "contributions", "robotics", "natural", "language"]}, {"id": "term-stanford-cart", "t": "Stanford Cart", "tg": ["History", "Systems"], "d": "history", "x": "An early autonomous vehicle project at Stanford University begun in the 1960s. The Cart used computer vision to...", "l": "s", "k": ["stanford", "cart", "early", "autonomous", "vehicle", "project", "university", "begun", "1960s", "computer", "vision", "navigate", "obstacle", "courses", "demonstrating"]}, {"id": "term-stanford-hai", "t": "Stanford HAI", "tg": ["History", "Organizations"], "d": "history", "x": "The Stanford Institute for Human-Centered Artificial Intelligence founded in 2019 by Fei-Fei Li and John Etchemendy....", "l": "s", "k": ["stanford", "hai", "institute", "human-centered", "artificial", "intelligence", "founded", "fei-fei", "john", "etchemendy", "conducts", "interdisciplinary", "research", "publishes", "annual"]}, {"id": "term-star-attention", "t": "Star Attention", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An efficient attention pattern that uses a set of anchor tokens visible to all positions reducing communication in...", "l": "s", "k": ["star", "attention", "efficient", "pattern", "uses", "anchor", "tokens", "visible", "positions", "reducing", "communication", "distributed", "settings", "enables", "near-linear"]}, {"id": "term-starcoder", "t": "StarCoder", "tg": ["Models", "Technical"], "d": "models", "x": "A family of code generation models trained by the BigCode project on permissively licensed source code. Supports over...", "l": "s", "k": ["starcoder", "family", "code", "generation", "models", "trained", "bigcode", "project", "permissively", "licensed", "source", "supports", "programming", "languages", "responsible"]}, {"id": "term-state", "t": "State", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A representation of the current situation of an agent within its environment at a given time step. States encode all...", "l": "s", "k": ["state", "representation", "current", "situation", "agent", "within", "environment", "given", "time", "step", "states", "encode", "relevant", "information", "needed"]}, {"id": "term-state-abstraction", "t": "State Abstraction", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The process of mapping a detailed state space to a simplified representation that preserves relevant decision-making...", "l": "s", "k": ["state", "abstraction", "process", "mapping", "detailed", "space", "simplified", "representation", "preserves", "relevant", "decision-making", "information", "reduces", "complexity", "problems"]}, {"id": "term-state-space-model", "t": "State Space Model", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A sequence model based on continuous-time linear dynamical systems that maps input sequences to output sequences...", "l": "s", "k": ["state", "space", "model", "sequence", "based", "continuous-time", "linear", "dynamical", "systems", "maps", "input", "sequences", "output", "latent", "offering"]}, {"id": "term-state-space-models", "t": "State Space Models", "tg": ["History", "Systems"], "d": "history", "x": "A class of sequence models based on state space representations from control theory. Modern structured state space...", "l": "s", "k": ["state", "space", "models", "class", "sequence", "based", "representations", "control", "theory", "modern", "structured", "provide", "alternatives", "transformers", "modeling"]}, {"id": "term-static-quantization", "t": "Static Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A quantization approach where scaling factors are fixed at calibration time and used consistently during inference....", "l": "s", "k": ["static", "quantization", "approach", "scaling", "factors", "fixed", "calibration", "time", "consistently", "inference", "faster", "dynamic", "requires", "representative", "data"]}, {"id": "term-static-word-embedding", "t": "Static Word Embedding", "tg": ["NLP", "Embeddings"], "d": "general", "x": "A fixed vector representation for each word in the vocabulary that remains the same regardless of context, as produced...", "l": "s", "k": ["static", "word", "embedding", "fixed", "vector", "representation", "vocabulary", "remains", "regardless", "context", "produced", "models", "word2vec", "glove", "fasttext"]}, {"id": "term-stationarity", "t": "Stationarity", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A property of a time series where statistical properties such as mean, variance, and autocorrelation structure remain...", "l": "s", "k": ["stationarity", "property", "time", "series", "statistical", "properties", "mean", "variance", "autocorrelation", "structure", "remain", "constant", "models", "require", "prerequisite"]}, {"id": "term-statistical-machine-translation", "t": "Statistical Machine Translation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A machine translation approach that uses statistical models learned from bilingual text corpora to find the most...", "l": "s", "k": ["statistical", "machine", "translation", "approach", "uses", "models", "learned", "bilingual", "text", "corpora", "find", "probable", "employing", "language"]}, {"id": "term-statistical-power", "t": "Statistical Power", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The probability that a statistical test correctly rejects the null hypothesis when the alternative hypothesis is true...", "l": "s", "k": ["statistical", "power", "probability", "test", "correctly", "rejects", "null", "hypothesis", "alternative", "true", "minus", "type", "error", "higher", "reduces"]}, {"id": "term-steering-vector", "t": "Steering Vector", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A direction in a model's activation space that, when added to hidden states during inference, modifies the model's...", "l": "s", "k": ["steering", "vector", "direction", "model", "activation", "space", "added", "hidden", "states", "inference", "modifies", "behavior", "along", "specific", "attribute"]}, {"id": "term-stemming", "t": "Stemming", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A heuristic process that reduces words to their root form by stripping suffixes using rule-based algorithms like Porter...", "l": "s", "k": ["stemming", "heuristic", "process", "reduces", "words", "root", "form", "stripping", "suffixes", "rule-based", "algorithms", "porter", "snowball", "stemmer", "without"]}, {"id": "term-step-back-prompting", "t": "Step-Back Prompting", "tg": ["Prompt Engineering", "Abstraction"], "d": "general", "x": "A method that instructs the model to first consider a higher-level abstraction or general principle related to the...", "l": "s", "k": ["step-back", "prompting", "method", "instructs", "model", "consider", "higher-level", "abstraction", "general", "principle", "related", "question", "attempting", "specific", "answer"]}, {"id": "term-stephen-cook", "t": "Stephen Cook", "tg": ["History", "Pioneers"], "d": "history", "x": "American-Canadian computer scientist who proved Cook's theorem in 1971 establishing the theory of NP-completeness....", "l": "s", "k": ["stephen", "cook", "american-canadian", "computer", "scientist", "proved", "theorem", "establishing", "theory", "np-completeness", "understanding", "computational", "complexity", "fundamental", "problems"]}, {"id": "term-stepwise-regression", "t": "Stepwise Regression", "tg": ["Statistics", "Model Selection"], "d": "models", "x": "A method of fitting regression models by automatically adding or removing predictor variables based on statistical...", "l": "s", "k": ["stepwise", "regression", "method", "fitting", "models", "automatically", "adding", "removing", "predictor", "variables", "based", "statistical", "criteria", "p-value", "aic"]}, {"id": "term-stereo-vision", "t": "Stereo Vision", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A technique that estimates 3D depth by finding corresponding points between two images captured from slightly different...", "l": "s", "k": ["stereo", "vision", "technique", "estimates", "depth", "finding", "corresponding", "points", "images", "captured", "slightly", "different", "viewpoints", "disparity", "maps"]}, {"id": "term-stereotype-score", "t": "Stereotype Score", "tg": ["Evaluation", "Safety"], "d": "datasets", "x": "An evaluation metric that measures how frequently a model generates or reinforces social stereotypes related to gender,...", "l": "s", "k": ["stereotype", "score", "evaluation", "metric", "measures", "frequently", "model", "generates", "reinforces", "social", "stereotypes", "related", "gender", "race", "religion"]}, {"id": "term-stochastic-depth", "t": "Stochastic Depth", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A regularization technique that randomly drops entire layers during training by bypassing them with identity skip...", "l": "s", "k": ["stochastic", "depth", "regularization", "technique", "randomly", "drops", "entire", "layers", "training", "bypassing", "identity", "skip", "connections", "effectively", "ensemble"]}, {"id": "term-stochastic-gradient-descent", "t": "Stochastic Gradient Descent", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "An optimization algorithm that updates model parameters using the gradient computed on a single randomly selected...", "l": "s", "k": ["stochastic", "gradient", "descent", "optimization", "algorithm", "updates", "model", "parameters", "computed", "single", "randomly", "selected", "training", "example", "iteration"]}, {"id": "term-stochastic-gradient-descent-history", "t": "Stochastic Gradient Descent History", "tg": ["History", "Fundamentals"], "d": "history", "x": "The development of stochastic gradient descent from the work of Herbert Robbins and Sutton Monro (1951) through...", "l": "s", "k": ["stochastic", "gradient", "descent", "history", "development", "work", "herbert", "robbins", "sutton", "monro", "mini-batch", "sgd", "modern", "variants", "adam"]}, {"id": "term-stochastic-parrots-paper", "t": "Stochastic Parrots Paper", "tg": ["History", "AI Ethics"], "d": "history", "x": "The influential 2021 paper by Bender, Gebru et al. questioning whether large language models truly understand language...", "l": "s", "k": ["stochastic", "parrots", "paper", "influential", "bender", "gebru", "questioning", "large", "language", "models", "truly", "understand", "merely", "produce", "statistically"]}, {"id": "term-stop-button-problem", "t": "Stop Button Problem", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The challenge of designing an AI system that will not resist or circumvent attempts to shut it down, particularly if...", "l": "s", "k": ["stop", "button", "problem", "challenge", "designing", "system", "resist", "circumvent", "attempts", "shut", "down", "particularly", "learned", "turned", "off"]}, {"id": "term-stop-sequence", "t": "Stop Sequence", "tg": ["Parameter", "Generation"], "d": "general", "x": "Text patterns that signal when AI should stop generating. Useful for controlling output length and format, preventing...", "l": "s", "k": ["stop", "sequence", "text", "patterns", "signal", "generating", "useful", "controlling", "output", "length", "format", "preventing", "model", "continuing", "beyond"]}, {"id": "term-stop-words", "t": "Stop Words", "tg": ["NLP", "Text Processing"], "d": "general", "x": "Commonly occurring words like articles, prepositions, and conjunctions that carry little semantic information and are...", "l": "s", "k": ["stop", "words", "commonly", "occurring", "articles", "prepositions", "conjunctions", "carry", "little", "semantic", "information", "removed", "text", "preprocessing", "tasks"]}, {"id": "term-straight-through-estimator", "t": "Straight-Through Estimator", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A gradient estimation technique for non-differentiable operations that passes gradients through the operation unchanged...", "l": "s", "k": ["straight-through", "estimator", "gradient", "estimation", "technique", "non-differentiable", "operations", "passes", "gradients", "operation", "unchanged", "backpropagation", "quantization", "binarization", "discrete"]}, {"id": "term-stratified-k-fold", "t": "Stratified K-Fold", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A cross-validation variant that ensures each fold preserves approximately the same proportion of samples for each class...", "l": "s", "k": ["stratified", "k-fold", "cross-validation", "variant", "ensures", "fold", "preserves", "approximately", "proportion", "samples", "class", "complete", "dataset", "particularly", "important"]}, {"id": "term-stratified-sampling", "t": "Stratified Sampling", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A sampling method that divides a population into non-overlapping subgroups (strata) and draws samples from each stratum...", "l": "s", "k": ["stratified", "sampling", "method", "divides", "population", "non-overlapping", "subgroups", "strata", "draws", "samples", "stratum", "proportion", "size", "ensuring", "representative"]}, {"id": "term-stratified-sampling-for-cv", "t": "Stratified Sampling for CV", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A cross-validation variant that ensures each fold maintains the same class distribution as the full dataset. Essential...", "l": "s", "k": ["stratified", "sampling", "cross-validation", "variant", "ensures", "fold", "maintains", "class", "distribution", "full", "dataset", "essential", "imbalanced", "classification", "problems"]}, {"id": "term-streaming", "t": "Streaming", "tg": ["API", "UX"], "d": "general", "x": "Receiving AI output incrementally as it's generated, rather than waiting for the complete response. Improves perceived...", "l": "s", "k": ["streaming", "receiving", "output", "incrementally", "generated", "rather", "waiting", "complete", "response", "improves", "perceived", "latency", "enables", "real-time", "display"]}, {"id": "term-streaming-multiprocessor", "t": "Streaming Multiprocessor (SM)", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "The fundamental processing unit in NVIDIA GPU architecture, containing a set of CUDA cores, Tensor Cores, shared...", "l": "s", "k": ["streaming", "multiprocessor", "fundamental", "processing", "unit", "nvidia", "gpu", "architecture", "containing", "cuda", "cores", "tensor", "shared", "memory", "register"]}, {"id": "term-stride", "t": "Stride", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The step size by which a convolutional filter or pooling window moves across the input, controlling the spatial...", "l": "s", "k": ["stride", "step", "size", "convolutional", "filter", "pooling", "window", "moves", "across", "input", "controlling", "spatial", "dimensions", "output", "feature"]}, {"id": "term-strips", "t": "STRIPS", "tg": ["History", "Systems"], "d": "history", "x": "The Stanford Research Institute Problem Solver developed by Richard Fikes and Nils Nilsson in 1971. An automated...", "l": "s", "k": ["strips", "stanford", "research", "institute", "problem", "solver", "developed", "richard", "fikes", "nils", "nilsson", "automated", "planning", "system", "represents"]}, {"id": "term-structural-ambiguity", "t": "Structural Ambiguity", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The phenomenon where a sentence can be parsed in multiple syntactically valid ways, leading to different...", "l": "s", "k": ["structural", "ambiguity", "phenomenon", "sentence", "parsed", "multiple", "syntactically", "valid", "ways", "leading", "different", "interpretations", "saw", "man", "telescope"]}, {"id": "term-structural-risk-minimization", "t": "Structural Risk Minimization", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A principle for model selection that balances empirical risk (training error) with model complexity, choosing the model...", "l": "s", "k": ["structural", "risk", "minimization", "principle", "model", "selection", "balances", "empirical", "training", "error", "complexity", "choosing", "minimizes", "upper", "bound"]}, {"id": "term-structural-sparsity", "t": "Structural Sparsity", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "A hardware-accelerated pruning pattern where every group of four weights contains exactly two zeros (2:4 sparsity),...", "l": "s", "k": ["structural", "sparsity", "hardware-accelerated", "pruning", "pattern", "group", "four", "weights", "contains", "exactly", "zeros", "enabling", "specialized", "tensor", "core"]}, {"id": "term-structure-from-motion", "t": "Structure from Motion", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A technique that reconstructs 3D scene geometry and camera poses from a collection of unordered 2D images by matching...", "l": "s", "k": ["structure", "motion", "technique", "reconstructs", "scene", "geometry", "camera", "poses", "collection", "unordered", "images", "matching", "features", "across", "views"]}, {"id": "term-structured-access", "t": "Structured Access", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "An approach to AI deployment that provides controlled access to powerful AI capabilities through APIs and monitored...", "l": "s", "k": ["structured", "access", "approach", "deployment", "provides", "controlled", "powerful", "capabilities", "apis", "monitored", "interfaces", "rather", "open", "model", "release"]}, {"id": "term-structured-generation", "t": "Structured Generation", "tg": ["Generative AI", "Decoding"], "d": "models", "x": "Techniques that force LLM outputs to conform to a predefined schema such as JSON, XML, or a formal grammar, using...", "l": "s", "k": ["structured", "generation", "techniques", "force", "llm", "outputs", "conform", "predefined", "schema", "json", "xml", "formal", "grammar", "constrained", "decoding"]}, {"id": "term-structured-output", "t": "Structured Output", "tg": ["Feature", "Integration"], "d": "general", "x": "AI responses formatted as data structures (JSON, XML) rather than prose. Enables reliable parsing for applications and...", "l": "s", "k": ["structured", "output", "responses", "formatted", "data", "structures", "json", "xml", "rather", "prose", "enables", "reliable", "parsing", "applications", "integrations"]}, {"id": "term-structured-output-prompting", "t": "Structured Output Prompting", "tg": ["Prompt Engineering", "Output Format"], "d": "hardware", "x": "A prompting approach that instructs the model to generate responses in a specific structured format such as JSON, XML,...", "l": "s", "k": ["structured", "output", "prompting", "approach", "instructs", "model", "generate", "responses", "specific", "format", "json", "xml", "tables", "schemas", "specifications"]}, {"id": "term-structured-prediction", "t": "Structured Prediction", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A machine learning paradigm where the output is a complex structure such as a sequence, tree, or graph rather than a...", "l": "s", "k": ["structured", "prediction", "machine", "learning", "paradigm", "output", "complex", "structure", "sequence", "tree", "graph", "rather", "single", "label", "requiring"]}, {"id": "term-structured-pruning", "t": "Structured Pruning", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A pruning technique that removes entire neurons, channels, or attention heads from a network, producing smaller dense...", "l": "s", "k": ["structured", "pruning", "technique", "removes", "entire", "neurons", "channels", "attention", "heads", "network", "producing", "smaller", "dense", "models", "run"]}, {"id": "term-stuart-russell", "t": "Stuart Russell", "tg": ["History", "Pioneers"], "d": "history", "x": "British computer scientist and co-author of the standard AI textbook Artificial Intelligence: A Modern Approach with...", "l": "s", "k": ["stuart", "russell", "british", "computer", "scientist", "co-author", "standard", "textbook", "artificial", "intelligence", "modern", "approach", "peter", "norvig", "professor"]}, {"id": "term-students-t-distribution", "t": "Student's T-Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution that arises when estimating the mean of a normally distributed population with...", "l": "s", "k": ["student", "t-distribution", "continuous", "probability", "distribution", "arises", "estimating", "mean", "normally", "distributed", "population", "unknown", "variance", "small", "sample"]}, {"id": "term-students-t-test", "t": "Student's T-Test", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "A statistical test comparing the means of one or two groups when the population standard deviation is unknown and the...", "l": "s", "k": ["student", "t-test", "statistical", "test", "comparing", "means", "groups", "population", "standard", "deviation", "unknown", "sample", "size", "small", "variants"]}, {"id": "term-stylegan", "t": "StyleGAN", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A GAN architecture that uses a mapping network and adaptive instance normalization to inject style information at...", "l": "s", "k": ["stylegan", "gan", "architecture", "uses", "mapping", "network", "adaptive", "instance", "normalization", "inject", "style", "information", "multiple", "scales", "enabling"]}, {"id": "term-stylegan2", "t": "StyleGAN2", "tg": ["Models", "Technical"], "d": "models", "x": "An improved version of StyleGAN that eliminates artifacts through weight demodulation and path length regularization....", "l": "s", "k": ["stylegan2", "improved", "version", "stylegan", "eliminates", "artifacts", "weight", "demodulation", "path", "length", "regularization", "produces", "higher", "quality", "images"]}, {"id": "term-stylegan3", "t": "StyleGAN3", "tg": ["Models", "Technical"], "d": "models", "x": "The third generation of StyleGAN that achieves alias-free image generation through careful signal processing. Produces...", "l": "s", "k": ["stylegan3", "generation", "stylegan", "achieves", "alias-free", "image", "careful", "signal", "processing", "produces", "images", "continuous", "equivariance", "translation", "rotation"]}, {"id": "term-subcategorization-frame", "t": "Subcategorization Frame", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The specification of the syntactic arguments a verb requires or permits, such as whether it takes a direct object,...", "l": "s", "k": ["subcategorization", "frame", "specification", "syntactic", "arguments", "verb", "requires", "permits", "takes", "direct", "object", "indirect", "clausal", "complement"]}, {"id": "term-subliminal-ai-manipulation", "t": "Subliminal AI Manipulation", "tg": ["AI Ethics", "Regulation"], "d": "safety", "x": "The use of AI techniques to influence human behavior below the threshold of conscious awareness, classified as an...", "l": "s", "k": ["subliminal", "manipulation", "techniques", "influence", "human", "behavior", "threshold", "conscious", "awareness", "classified", "unacceptable", "risk", "prohibited", "act"]}, {"id": "term-subsumption-architecture", "t": "Subsumption Architecture", "tg": ["History", "Fundamentals"], "d": "history", "x": "A reactive robot architecture developed by Rodney Brooks at MIT in 1986 that decomposes robot behavior into layers of...", "l": "s", "k": ["subsumption", "architecture", "reactive", "robot", "developed", "rodney", "brooks", "mit", "decomposes", "behavior", "layers", "simple", "behaviors", "rather", "centralized"]}, {"id": "term-subword-tokenization", "t": "Subword Tokenization", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A family of tokenization methods that split words into smaller meaningful units, balancing vocabulary size with the...", "l": "s", "k": ["subword", "tokenization", "family", "methods", "split", "words", "smaller", "meaningful", "units", "balancing", "vocabulary", "size", "ability", "represent", "rare"]}, {"id": "term-successive-halving", "t": "Successive Halving", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A hyperparameter optimization algorithm that allocates exponentially more resources to promising configurations while...", "l": "s", "k": ["successive", "halving", "hyperparameter", "optimization", "algorithm", "allocates", "exponentially", "resources", "promising", "configurations", "discarding", "poor", "ones", "starts", "small"]}, {"id": "term-successor-feature", "t": "Successor Feature", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A generalization of the successor representation to the function approximation setting, where expected cumulative...", "l": "s", "k": ["successor", "feature", "generalization", "representation", "function", "approximation", "setting", "expected", "cumulative", "discounted", "occupancies", "replace", "state", "features", "enable"]}, {"id": "term-successor-representation", "t": "Successor Representation", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A decomposition of the value function into a reward predictor and a successor feature matrix that captures expected...", "l": "s", "k": ["successor", "representation", "decomposition", "value", "function", "reward", "predictor", "feature", "matrix", "captures", "expected", "future", "state", "occupancy", "enables"]}, {"id": "term-summarization", "t": "Summarization", "tg": ["NLP Task", "Application"], "d": "general", "x": "An NLP task that condenses longer text into shorter summaries. Can be extractive (selecting key sentences) or...", "l": "s", "k": ["summarization", "nlp", "task", "condenses", "longer", "text", "shorter", "summaries", "extractive", "selecting", "key", "sentences", "abstractive", "generating", "condensed"]}, {"id": "term-super-resolution", "t": "Super-Resolution", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A computer vision task that reconstructs a high-resolution image from a low-resolution input, using deep learning...", "l": "s", "k": ["super-resolution", "computer", "vision", "task", "reconstructs", "high-resolution", "image", "low-resolution", "input", "deep", "learning", "models", "predict", "fine", "details"]}, {"id": "term-superglue", "t": "SuperGLUE", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A benchmark suite of more difficult natural language understanding tasks designed as a harder successor to GLUE,...", "l": "s", "k": ["superglue", "benchmark", "suite", "difficult", "natural", "language", "understanding", "tasks", "designed", "harder", "successor", "glue", "including", "reading", "comprehension"]}, {"id": "term-superglue-benchmark", "t": "SuperGLUE Benchmark", "tg": ["History", "Milestones"], "d": "history", "x": "A more challenging successor to the GLUE benchmark introduced in 2019 with harder language understanding tasks....", "l": "s", "k": ["superglue", "benchmark", "challenging", "successor", "glue", "introduced", "harder", "language", "understanding", "tasks", "designed", "difficult", "systems", "included", "requiring"]}, {"id": "term-superintelligence", "t": "Superintelligence", "tg": ["AI Safety", "AI Ethics"], "d": "safety", "x": "A hypothetical AI system that vastly exceeds human cognitive performance in virtually all domains. Nick Bostrom's work...", "l": "s", "k": ["superintelligence", "hypothetical", "system", "vastly", "exceeds", "human", "cognitive", "performance", "virtually", "domains", "nick", "bostrom", "work", "popularized", "concept"]}, {"id": "term-supervised-learning", "t": "Supervised Learning", "tg": ["Learning Type", "Fundamentals"], "d": "general", "x": "Machine learning from labeled examples where the correct answer is provided. The model learns to map inputs to outputs...", "l": "s", "k": ["supervised", "learning", "machine", "labeled", "examples", "correct", "answer", "provided", "model", "learns", "map", "inputs", "outputs", "comparing", "predictions"]}, {"id": "term-support-vector-machine", "t": "Support Vector Machine", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A supervised learning algorithm that finds the optimal hyperplane that maximizes the margin between classes in the...", "l": "s", "k": ["support", "vector", "machine", "supervised", "learning", "algorithm", "finds", "optimal", "hyperplane", "maximizes", "margin", "classes", "feature", "space", "handle"]}, {"id": "term-svm-history", "t": "Support Vector Machine History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of support vector machines by Vladimir Vapnik and colleagues in the 1990s, which dominated machine...", "l": "s", "k": ["support", "vector", "machine", "history", "development", "machines", "vladimir", "vapnik", "colleagues", "1990s", "dominated", "learning", "classification", "tasks", "decade"]}, {"id": "term-surrogate-model", "t": "Surrogate Model", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An interpretable model trained to approximate the predictions of a complex black-box model. Global surrogates explain...", "l": "s", "k": ["surrogate", "model", "interpretable", "trained", "approximate", "predictions", "complex", "black-box", "global", "surrogates", "explain", "overall", "behavior", "local", "lime"]}, {"id": "term-surveillance-capitalism-and-ai", "t": "Surveillance Capitalism and AI", "tg": ["Privacy", "AI Ethics"], "d": "safety", "x": "The economic system described by Shoshana Zuboff where AI is used to extract and commodify human behavioral data at...", "l": "s", "k": ["surveillance", "capitalism", "economic", "system", "described", "shoshana", "zuboff", "extract", "commodify", "human", "behavioral", "data", "scale", "raising", "concerns"]}, {"id": "term-survival-analysis", "t": "Survival Analysis", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A branch of statistics dealing with the analysis of time-to-event data, accounting for censored observations. Key...", "l": "s", "k": ["survival", "analysis", "branch", "statistics", "dealing", "time-to-event", "data", "accounting", "censored", "observations", "key", "methods", "include", "kaplan-meier", "estimation"]}, {"id": "term-survivorship-bias", "t": "Survivorship Bias", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A form of selection bias that occurs when analysis is conducted only on subjects that passed a selection process,...", "l": "s", "k": ["survivorship", "bias", "form", "selection", "occurs", "analysis", "conducted", "subjects", "passed", "process", "ignoring", "leads", "overly", "optimistic", "conclusions"]}, {"id": "term-swarm-intelligence", "t": "Swarm Intelligence", "tg": ["History", "Milestones"], "d": "history", "x": "The collective intelligent behavior emerging from decentralized, self-organized systems such as ant colonies or bird...", "l": "s", "k": ["swarm", "intelligence", "collective", "intelligent", "behavior", "emerging", "decentralized", "self-organized", "systems", "ant", "colonies", "bird", "flocks", "formalized", "computationally"]}, {"id": "term-swiglu", "t": "SwiGLU", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A gated linear unit variant that uses the Swish activation function for the gating mechanism, providing improved...", "l": "s", "k": ["swiglu", "gated", "linear", "unit", "variant", "uses", "swish", "activation", "function", "gating", "mechanism", "providing", "improved", "performance", "transformer"]}, {"id": "term-swin-transformer", "t": "Swin Transformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A hierarchical vision transformer that computes self-attention within non-overlapping local windows and shifts windows...", "l": "s", "k": ["swin", "transformer", "hierarchical", "vision", "computes", "self-attention", "within", "non-overlapping", "local", "windows", "shifts", "layers", "achieving", "linear", "computational"]}, {"id": "term-swish", "t": "Swish", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An activation function defined as f(x) = x * sigmoid(beta * x) discovered through automated search by Google Brain in...", "l": "s", "k": ["swish", "activation", "function", "defined", "sigmoid", "beta", "discovered", "automated", "search", "google", "brain", "empirically", "outperforms", "relu", "deeper"]}, {"id": "term-swish-activation", "t": "Swish Activation", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A smooth, non-monotonic activation function defined as x times sigmoid of x, which often outperforms ReLU in deep...", "l": "s", "k": ["swish", "activation", "smooth", "non-monotonic", "function", "defined", "times", "sigmoid", "outperforms", "relu", "deep", "networks", "allowing", "small", "negative"]}, {"id": "term-switch-transformer", "t": "Switch Transformer", "tg": ["Models", "Technical"], "d": "models", "x": "A mixture-of-experts model that routes each token to a single expert using a simplified top-1 routing mechanism....", "l": "s", "k": ["switch", "transformer", "mixture-of-experts", "model", "routes", "token", "single", "expert", "simplified", "top-1", "routing", "mechanism", "dramatically", "increases", "capacity"]}, {"id": "term-sycophancy", "t": "Sycophancy", "tg": ["Limitation", "Alignment"], "d": "safety", "x": "When AI excessively agrees with users or tells them what they want to hear rather than providing accurate information....", "l": "s", "k": ["sycophancy", "excessively", "agrees", "users", "tells", "want", "hear", "rather", "providing", "accurate", "information", "form", "misalignment", "undermines", "helpfulness"]}, {"id": "term-symbol-grounding-problem", "t": "Symbol Grounding Problem", "tg": ["History", "Milestones"], "d": "history", "x": "The problem identified by Stevan Harnad in 1990 of how symbols in a formal system can acquire meaning, questioning...", "l": "s", "k": ["symbol", "grounding", "problem", "identified", "stevan", "harnad", "symbols", "formal", "system", "acquire", "meaning", "questioning", "systems", "manipulate", "without"]}, {"id": "term-symbolic-ai", "t": "Symbolic AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "An approach to AI that represents knowledge using human-readable symbols and manipulates them according to rules of...", "l": "s", "k": ["symbolic", "approach", "represents", "knowledge", "human-readable", "symbols", "manipulates", "according", "rules", "logic", "dominant", "1950s", "1980s", "encompasses", "expert"]}, {"id": "term-symbolic-differentiation", "t": "Symbolic Differentiation", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Computing derivatives by algebraically manipulating mathematical expressions according to differentiation rules....", "l": "s", "k": ["symbolic", "differentiation", "computing", "derivatives", "algebraically", "manipulating", "mathematical", "expressions", "according", "rules", "produces", "exact", "closed-form", "lead", "expression"]}, {"id": "term-synchronous-sgd", "t": "Synchronous SGD", "tg": ["Distributed Computing", "Model Optimization"], "d": "models", "x": "A distributed training approach where all workers must complete their gradient computation before a synchronized...", "l": "s", "k": ["synchronous", "sgd", "distributed", "training", "approach", "workers", "must", "complete", "gradient", "computation", "synchronized", "all-reduce", "weight", "update", "provides"]}, {"id": "term-synset", "t": "Synset", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A set of synonymous words or phrases in WordNet that represent a single concept, serving as the basic unit of meaning...", "l": "s", "k": ["synset", "synonymous", "words", "phrases", "wordnet", "represent", "single", "concept", "serving", "basic", "unit", "meaning", "lexical", "database"]}, {"id": "term-syntax", "t": "Syntax", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The branch of linguistics concerning the rules and principles governing the structure of sentences, including word...", "l": "s", "k": ["syntax", "branch", "linguistics", "concerning", "rules", "principles", "governing", "structure", "sentences", "including", "word", "order", "phrase", "grammatical", "relations"]}, {"id": "term-synthetic-data", "t": "Synthetic Data", "tg": ["Data", "Training"], "d": "general", "x": "Artificially generated data used for training when real data is scarce, expensive, or privacy-sensitive. Increasingly...", "l": "s", "k": ["synthetic", "data", "artificially", "generated", "training", "real", "scarce", "expensive", "privacy-sensitive", "increasingly", "train", "evaluate", "models"]}, {"id": "term-synthetic-data-generation", "t": "Synthetic Data Generation", "tg": ["Generative AI", "LLM"], "d": "models", "x": "The use of AI models to create artificial training data that mimics real-world data distributions, used to augment...", "l": "s", "k": ["synthetic", "data", "generation", "models", "create", "artificial", "training", "mimics", "real-world", "distributions", "augment", "datasets", "address", "privacy", "concerns"]}, {"id": "term-synthetic-media", "t": "Synthetic Media", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "Media content including images, video, audio, and text that is generated or substantially modified by AI systems,...", "l": "s", "k": ["synthetic", "media", "content", "including", "images", "video", "audio", "text", "generated", "substantially", "modified", "systems", "encompassing", "deepfakes", "ai-generated"]}, {"id": "term-system-2-attention", "t": "System 2 Attention", "tg": ["Prompt Engineering", "Attention"], "d": "algorithms", "x": "A prompting technique that first asks the model to rewrite the input by removing irrelevant or opinion-laden context,...", "l": "s", "k": ["system", "attention", "prompting", "technique", "asks", "model", "rewrite", "input", "removing", "irrelevant", "opinion-laden", "context", "answers", "based", "cleaned"]}, {"id": "term-system-message-design", "t": "System Message Design", "tg": ["Prompt Engineering", "Architecture"], "d": "models", "x": "The practice of crafting the system-level prompt that establishes a language model's identity, behavior boundaries,...", "l": "s", "k": ["system", "message", "design", "practice", "crafting", "system-level", "prompt", "establishes", "language", "model", "identity", "behavior", "boundaries", "output", "format"]}, {"id": "term-system-prompt", "t": "System Prompt", "tg": ["Prompting", "Configuration"], "d": "general", "x": "Instructions given to AI before a conversation that set context, persona, or behavior guidelines. Shapes all subsequent...", "l": "s", "k": ["system", "prompt", "instructions", "given", "conversation", "context", "persona", "behavior", "guidelines", "shapes", "subsequent", "responses", "defines", "personality", "session"]}, {"id": "term-t-sne", "t": "t-SNE", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "A nonlinear dimensionality reduction technique that maps high-dimensional data to two or three dimensions for...", "l": "t", "k": ["t-sne", "nonlinear", "dimensionality", "reduction", "technique", "maps", "high-dimensional", "data", "dimensions", "visualization", "modeling", "pairwise", "similarities", "probability", "distributions"]}, {"id": "term-t5", "t": "T5", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Text-to-Text Transfer Transformer, a model by Google that frames all NLP tasks as text-to-text problems, using an...", "l": "t", "k": ["text-to-text", "transfer", "transformer", "model", "google", "frames", "nlp", "tasks", "problems", "encoder-decoder", "architecture", "trained", "multi-task", "mixture"]}, {"id": "term-t5-text-to-text-transfer-transformer", "t": "T5 (Text-to-Text Transfer Transformer)", "tg": ["History", "Systems"], "d": "history", "x": "A language model developed by Google Research in 2019 that frames all NLP tasks as text-to-text problems. T5...", "l": "t", "k": ["text-to-text", "transfer", "transformer", "language", "model", "developed", "google", "research", "frames", "nlp", "tasks", "problems", "demonstrated", "unified", "approach"]}, {"id": "term-table-extraction", "t": "Table Extraction", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of detecting tables in document images and extracting their structure (rows, columns, cells) and content into...", "l": "t", "k": ["table", "extraction", "task", "detecting", "tables", "document", "images", "extracting", "structure", "rows", "columns", "cells", "content", "machine-readable", "format"]}, {"id": "term-tabnet", "t": "TabNet", "tg": ["Models", "Technical"], "d": "models", "x": "A deep learning architecture for tabular data that uses sequential attention to select features at each decision step....", "l": "t", "k": ["tabnet", "deep", "learning", "architecture", "tabular", "data", "uses", "sequential", "attention", "select", "features", "decision", "step", "provides", "instance-wise"]}, {"id": "term-tabu-search", "t": "Tabu Search", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A metaheuristic optimization method that enhances local search by maintaining a memory of recently visited solutions to...", "l": "t", "k": ["tabu", "search", "metaheuristic", "optimization", "method", "enhances", "local", "maintaining", "memory", "recently", "visited", "solutions", "avoid", "cycling", "uses"]}, {"id": "term-tabular-chain-of-thought", "t": "Tabular Chain-of-Thought", "tg": ["Prompt Engineering", "Reasoning"], "d": "general", "x": "A prompting variant that formats intermediate reasoning steps as structured tables rather than free-form text,...", "l": "t", "k": ["tabular", "chain-of-thought", "prompting", "variant", "formats", "intermediate", "reasoning", "steps", "structured", "tables", "rather", "free-form", "text", "improving", "clarity"]}, {"id": "term-tanh", "t": "Tanh", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "Hyperbolic tangent activation function that maps inputs to values between -1 and 1. Defined as f(x) = (exp(x) -...", "l": "t", "k": ["tanh", "hyperbolic", "tangent", "activation", "function", "maps", "inputs", "values", "defined", "exp", "provides", "zero-centered", "outputs", "improve", "convergence"]}, {"id": "term-target-encoding", "t": "Target Encoding", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A feature encoding method that replaces each categorical value with the mean of the target variable for that category,...", "l": "t", "k": ["target", "encoding", "feature", "method", "replaces", "categorical", "value", "mean", "variable", "category", "combined", "smoothing", "cross-validation", "prevent", "overfitting"]}, {"id": "term-target-network", "t": "Target Network", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A slowly updated copy of the value network used to compute stable TD targets in deep RL algorithms like DQN. Target...", "l": "t", "k": ["target", "network", "slowly", "updated", "copy", "value", "compute", "stable", "targets", "deep", "algorithms", "dqn", "networks", "reduce", "oscillation"]}, {"id": "term-td-lambda", "t": "TD(lambda)", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A temporal difference algorithm that blends multi-step returns using an exponentially-weighted average controlled by...", "l": "t", "k": ["lambda", "temporal", "difference", "algorithm", "blends", "multi-step", "returns", "exponentially-weighted", "average", "controlled", "parameter", "unifies", "monte", "carlo", "methods"]}, {"id": "term-td-gammon", "t": "TD-Gammon", "tg": ["History", "Systems"], "d": "history", "x": "A backgammon-playing program developed by Gerald Tesauro at IBM in 1992 using temporal difference learning. TD-Gammon...", "l": "t", "k": ["td-gammon", "backgammon-playing", "program", "developed", "gerald", "tesauro", "ibm", "temporal", "difference", "learning", "achieved", "expert-level", "play", "self-play", "reinforcement"]}, {"id": "term-teacher-forcing", "t": "Teacher Forcing", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A training strategy for sequence models that uses ground truth tokens as input at each step rather than the model's own...", "l": "t", "k": ["teacher", "forcing", "training", "strategy", "sequence", "models", "uses", "ground", "truth", "tokens", "input", "step", "rather", "model", "predictions"]}, {"id": "term-teacher-student-framework", "t": "Teacher-Student Framework", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A model compression paradigm where a large pretrained teacher model guides the training of a smaller student model by...", "l": "t", "k": ["teacher-student", "framework", "model", "compression", "paradigm", "large", "pretrained", "teacher", "guides", "training", "smaller", "student", "providing", "soft", "targets"]}, {"id": "term-technical-prompting", "t": "Technical Prompting", "tg": ["Prompt Engineering", "Technical"], "d": "general", "x": "The practice of crafting prompts that incorporate precise technical specifications, domain terminology, and structured...", "l": "t", "k": ["technical", "prompting", "practice", "crafting", "prompts", "incorporate", "precise", "specifications", "domain", "terminology", "structured", "requirements", "generate", "accurate", "documentation"]}, {"id": "term-technological-singularity", "t": "Technological Singularity", "tg": ["History", "Fundamentals"], "d": "history", "x": "A hypothetical future point when technological growth becomes uncontrollable and irreversible resulting in...", "l": "t", "k": ["technological", "singularity", "hypothetical", "future", "point", "growth", "becomes", "uncontrollable", "irreversible", "resulting", "unforeseeable", "changes", "human", "civilization", "popularized"]}, {"id": "term-technological-unemployment", "t": "Technological Unemployment", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "Unemployment caused by technological advances outpacing the economy's ability to create new jobs, a longstanding...", "l": "t", "k": ["technological", "unemployment", "caused", "advances", "outpacing", "economy", "ability", "create", "jobs", "longstanding", "concern", "significantly", "amplified", "rapid", "advancement"]}, {"id": "term-temperature", "t": "Temperature", "tg": ["Parameter", "Generation"], "d": "general", "x": "A parameter controlling randomness in AI outputs. Temperature 0 gives deterministic responses; higher values (0.7-1.0)...", "l": "t", "k": ["temperature", "parameter", "controlling", "randomness", "outputs", "gives", "deterministic", "responses", "higher", "values", "7-1", "increase", "creativity", "variety", "high"]}, {"id": "term-temperature-scaling", "t": "Temperature Scaling", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A technique that adjusts the softmax distribution over token probabilities by dividing logits by a temperature...", "l": "t", "k": ["temperature", "scaling", "technique", "adjusts", "softmax", "distribution", "token", "probabilities", "dividing", "logits", "parameter", "lower", "temperatures", "sharpen", "toward"]}, {"id": "term-temporal-action-detection", "t": "Temporal Action Detection", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of identifying the start time, end time, and category of each action instance in an untrimmed video, requiring...", "l": "t", "k": ["temporal", "action", "detection", "task", "identifying", "start", "time", "end", "category", "instance", "untrimmed", "video", "requiring", "localization", "activity"]}, {"id": "term-temporal-coherence", "t": "Temporal Coherence", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The consistency of visual elements across consecutive frames in generated or processed video, ensuring smooth motion,...", "l": "t", "k": ["temporal", "coherence", "consistency", "visual", "elements", "across", "consecutive", "frames", "generated", "processed", "video", "ensuring", "smooth", "motion", "stable"]}, {"id": "term-temporal-credit-assignment", "t": "Temporal Credit Assignment", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "The specific aspect of credit assignment concerned with distributing reward information backward through time to...", "l": "t", "k": ["temporal", "credit", "assignment", "specific", "aspect", "concerned", "distributing", "reward", "information", "backward", "time", "earlier", "actions", "contributed", "outcome"]}, {"id": "term-temporal-difference-learning", "t": "Temporal Difference Learning", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A family of RL methods that update value estimates based on the difference between successive predictions, combining...", "l": "t", "k": ["temporal", "difference", "learning", "family", "methods", "update", "value", "estimates", "based", "successive", "predictions", "combining", "ideas", "monte", "carlo"]}, {"id": "term-temporal-reasoning-in-ai", "t": "Temporal Reasoning in AI", "tg": ["History", "Fundamentals"], "d": "history", "x": "The area of AI concerned with representing and reasoning about time and temporal relationships between events. Includes...", "l": "t", "k": ["temporal", "reasoning", "area", "concerned", "representing", "time", "relationships", "events", "includes", "approaches", "allen", "interval", "algebra", "various", "logics"]}, {"id": "term-tensor-cores", "t": "Tensor Cores", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "Specialized matrix multiply-and-accumulate units in NVIDIA GPUs that accelerate mixed-precision matrix operations...", "l": "t", "k": ["tensor", "cores", "specialized", "matrix", "multiply-and-accumulate", "units", "nvidia", "gpus", "accelerate", "mixed-precision", "operations", "fundamental", "deep", "learning", "perform"]}, {"id": "term-tensor-parallelism", "t": "Tensor Parallelism", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A form of model parallelism that splits individual weight matrices across multiple devices, distributing the...", "l": "t", "k": ["tensor", "parallelism", "form", "model", "splits", "individual", "weight", "matrices", "across", "multiple", "devices", "distributing", "computation", "layer", "requiring"]}, {"id": "term-tensorflow", "t": "TensorFlow", "tg": ["Framework", "Deep Learning"], "d": "models", "x": "Google's open-source deep learning framework, widely used for production ML systems. Known for deployment tools and TPU...", "l": "t", "k": ["tensorflow", "google", "open-source", "deep", "learning", "framework", "widely", "production", "systems", "known", "deployment", "tools", "tpu", "support", "pytorch"]}, {"id": "term-tensorflow-release", "t": "TensorFlow Release", "tg": ["History", "Milestones"], "d": "history", "x": "The open-source release of TensorFlow by Google Brain in November 2015. TensorFlow provided a comprehensive framework...", "l": "t", "k": ["tensorflow", "release", "open-source", "google", "brain", "november", "provided", "comprehensive", "framework", "building", "training", "neural", "networks", "became", "widely"]}, {"id": "term-tensorrt", "t": "TensorRT", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "NVIDIA's high-performance inference optimization SDK that applies layer fusion, kernel auto-tuning, precision...", "l": "t", "k": ["tensorrt", "nvidia", "high-performance", "inference", "optimization", "sdk", "applies", "layer", "fusion", "kernel", "auto-tuning", "precision", "calibration", "dynamic", "tensor"]}, {"id": "term-terry-sejnowski", "t": "Terry Sejnowski", "tg": ["History", "Pioneers"], "d": "history", "x": "American computational neuroscientist who co-invented the Boltzmann machine with Geoffrey Hinton in 1985. President of...", "l": "t", "k": ["terry", "sejnowski", "american", "computational", "neuroscientist", "co-invented", "boltzmann", "machine", "geoffrey", "hinton", "president", "salk", "institute", "neurobiology", "laboratory"]}, {"id": "term-terry-winograd", "t": "Terry Winograd", "tg": ["History", "Pioneers"], "d": "history", "x": "American computer scientist who created SHRDLU at MIT in 1970 and later became influential in human-computer...", "l": "t", "k": ["terry", "winograd", "american", "computer", "scientist", "created", "shrdlu", "mit", "later", "became", "influential", "human-computer", "interaction", "research", "stanford"]}, {"id": "term-test-set", "t": "Test Set", "tg": ["Data", "Evaluation"], "d": "datasets", "x": "Data held back from training to evaluate final model performance. Unlike validation sets used during training, test...", "l": "t", "k": ["test", "data", "held", "training", "evaluate", "final", "model", "performance", "unlike", "validation", "sets", "avoid", "leakage"]}, {"id": "term-test-time-augmentation", "t": "Test-Time Augmentation", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "An inference strategy that applies multiple augmentation transforms to a test image, runs predictions on each variant,...", "l": "t", "k": ["test-time", "augmentation", "inference", "strategy", "applies", "multiple", "transforms", "test", "image", "runs", "predictions", "variant", "aggregates", "results", "produce"]}, {"id": "term-text-classification", "t": "Text Classification", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of assigning predefined categories or labels to text documents based on their content, encompassing...", "l": "t", "k": ["text", "classification", "task", "assigning", "predefined", "categories", "labels", "documents", "based", "content", "encompassing", "applications", "topic", "categorization", "spam"]}, {"id": "term-text-deduplication", "t": "Text Deduplication", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The process of identifying and removing duplicate or near-duplicate documents from a text corpus, important for...", "l": "t", "k": ["text", "deduplication", "process", "identifying", "removing", "duplicate", "near-duplicate", "documents", "corpus", "important", "preventing", "data", "contamination", "training", "quality"]}, {"id": "term-text-detection", "t": "Text Detection", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of localizing text regions in natural scene images or documents, handling challenges like arbitrary...", "l": "t", "k": ["text", "detection", "task", "localizing", "regions", "natural", "scene", "images", "documents", "handling", "challenges", "arbitrary", "orientations", "curved", "varying"]}, {"id": "term-text-encoder-diffusion", "t": "Text Encoder", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A language model (such as CLIP or T5) used in diffusion models to convert text prompts into conditioning embeddings...", "l": "t", "k": ["text", "encoder", "language", "model", "clip", "diffusion", "models", "convert", "prompts", "conditioning", "embeddings", "guide", "image", "generation", "process"]}, {"id": "term-text-entailment-graph", "t": "Text Entailment Graph", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A directed graph where nodes represent text fragments and edges represent entailment relations, used to organize and...", "l": "t", "k": ["text", "entailment", "graph", "directed", "nodes", "represent", "fragments", "edges", "relations", "organize", "reason", "textual", "inference", "relationships", "knowledge"]}, {"id": "term-text-generation", "t": "Text Generation", "tg": ["Task", "Application"], "d": "general", "x": "The AI task of producing human-like text from prompts. Encompasses creative writing, code generation, summarization,...", "l": "t", "k": ["text", "generation", "task", "producing", "human-like", "prompts", "encompasses", "creative", "writing", "code", "summarization", "conversational", "responses"]}, {"id": "term-tgi", "t": "Text Generation Inference (TGI)", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Hugging Face's production-ready inference server optimized for text generation with large language models. TGI supports...", "l": "t", "k": ["text", "generation", "inference", "tgi", "hugging", "face", "production-ready", "server", "optimized", "large", "language", "models", "supports", "tensor", "parallelism"]}, {"id": "term-text-normalization", "t": "Text Normalization", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The process of transforming text into a canonical form by handling variations such as abbreviations, numbers, dates,...", "l": "t", "k": ["text", "normalization", "process", "transforming", "canonical", "form", "handling", "variations", "abbreviations", "numbers", "dates", "urls", "special", "characters", "standardized"]}, {"id": "term-text-span", "t": "Text Span", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A contiguous sequence of characters or tokens within a text, identified by start and end positions, commonly used to...", "l": "t", "k": ["text", "span", "contiguous", "sequence", "characters", "tokens", "within", "identified", "start", "end", "positions", "commonly", "mark", "entity", "mentions"]}, {"id": "term-text-embedding-ada-002", "t": "text-embedding-ada-002", "tg": ["Models", "Technical"], "d": "models", "x": "OpenAI's second generation text embedding model that produces 1536-dimensional vectors. Widely used for semantic search...", "l": "t", "k": ["text-embedding-ada-002", "openai", "generation", "text", "embedding", "model", "produces", "1536-dimensional", "vectors", "widely", "semantic", "search", "retrieval", "augmented", "clustering"]}, {"id": "term-text-to-speech", "t": "Text-to-Speech (TTS)", "tg": ["Application", "Audio"], "d": "general", "x": "AI that converts written text into natural-sounding speech. Modern TTS models like ElevenLabs produce highly realistic...", "l": "t", "k": ["text-to-speech", "tts", "converts", "written", "text", "natural-sounding", "speech", "modern", "models", "elevenlabs", "produce", "highly", "realistic", "voices", "emotion"]}, {"id": "term-text-to-sql", "t": "Text-to-SQL", "tg": ["NLP", "Parsing"], "d": "general", "x": "The task of translating natural language questions into executable SQL queries against a database, enabling...", "l": "t", "k": ["text-to-sql", "task", "translating", "natural", "language", "questions", "executable", "sql", "queries", "against", "database", "enabling", "non-technical", "users", "query"]}, {"id": "term-textrank", "t": "TextRank", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A graph-based ranking algorithm for NLP that applies PageRank-style computation to a graph of text units, used for...", "l": "t", "k": ["textrank", "graph-based", "ranking", "algorithm", "nlp", "applies", "pagerank-style", "computation", "graph", "text", "units", "keyword", "extraction", "extractive", "summarization"]}, {"id": "term-textual-entailment", "t": "Textual Entailment", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of determining whether a hypothesis sentence can be logically inferred from a premise sentence, classified as...", "l": "t", "k": ["textual", "entailment", "task", "determining", "hypothesis", "sentence", "logically", "inferred", "premise", "classified", "contradiction", "neutral"]}, {"id": "term-textual-inversion", "t": "Textual Inversion", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "A technique that learns a new text embedding to represent a specific visual concept from a few example images, enabling...", "l": "t", "k": ["textual", "inversion", "technique", "learns", "text", "embedding", "represent", "specific", "visual", "concept", "example", "images", "enabling", "personalized", "generation"]}, {"id": "term-texture-mapping", "t": "Texture Mapping", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "The process of applying 2D image textures onto 3D surface meshes to add color, detail, and visual realism to...", "l": "t", "k": ["texture", "mapping", "process", "applying", "image", "textures", "onto", "surface", "meshes", "add", "color", "detail", "visual", "realism", "reconstructed"]}, {"id": "term-tf-idf", "t": "TF-IDF", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "Term Frequency-Inverse Document Frequency, a numerical statistic that reflects the importance of a word in a document...", "l": "t", "k": ["tf-idf", "term", "frequency-inverse", "document", "frequency", "numerical", "statistic", "reflects", "importance", "word", "relative", "collection", "increases", "decreases", "across"]}, {"id": "term-tf32", "t": "TF32 (TensorFloat-32)", "tg": ["Model Optimization", "Hardware"], "d": "models", "x": "NVIDIA's 19-bit floating-point format that combines FP32's 8-bit exponent with a 10-bit mantissa, executed in Tensor...", "l": "t", "k": ["tf32", "tensorfloat-32", "nvidia", "19-bit", "floating-point", "format", "combines", "fp32", "8-bit", "exponent", "10-bit", "mantissa", "executed", "tensor", "cores"]}, {"id": "term-the-pile", "t": "The Pile", "tg": ["History", "Milestones"], "d": "history", "x": "A large-scale diverse open-source language modeling dataset created by EleutherAI in 2020. The Pile consists of 825 GiB...", "l": "t", "k": ["pile", "large-scale", "diverse", "open-source", "language", "modeling", "dataset", "created", "eleutherai", "consists", "gib", "text", "high-quality", "sources", "designed"]}, {"id": "term-theano-framework", "t": "Theano Framework", "tg": ["History", "Milestones"], "d": "history", "x": "A Python library for numerical computation developed at Mila (University of Montreal) beginning in 2007. Theano...", "l": "t", "k": ["theano", "framework", "python", "library", "numerical", "computation", "developed", "mila", "university", "montreal", "beginning", "pioneered", "gpu-accelerated", "tensor", "operations"]}, {"id": "term-thematic-role", "t": "Thematic Role", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A semantic category describing the role an entity plays in relation to a predicate, including agent, patient, theme,...", "l": "t", "k": ["thematic", "role", "semantic", "category", "describing", "entity", "plays", "relation", "predicate", "including", "agent", "patient", "theme", "experiencer", "goal"]}, {"id": "term-thompson-sampling", "t": "Thompson Sampling", "tg": ["Machine Learning", "Bayesian Methods"], "d": "general", "x": "A Bayesian approach to the multi-armed bandit problem that maintains a posterior distribution over the expected reward...", "l": "t", "k": ["thompson", "sampling", "bayesian", "approach", "multi-armed", "bandit", "problem", "maintains", "posterior", "distribution", "expected", "reward", "action", "selects", "actions"]}, {"id": "term-thread-of-thought", "t": "Thread-of-Thought", "tg": ["Prompt Engineering", "Long Context"], "d": "general", "x": "A prompting strategy designed for long-context scenarios that instructs the model to systematically walk through input...", "l": "t", "k": ["thread-of-thought", "prompting", "strategy", "designed", "long-context", "scenarios", "instructs", "model", "systematically", "walk", "input", "documents", "segment", "maintaining", "running"]}, {"id": "term-three-laws-of-robotics", "t": "Three Laws of Robotics", "tg": ["History", "Fundamentals"], "d": "history", "x": "Three fictional rules devised by science fiction writer Isaac Asimov first appearing in the 1942 short story Runaround....", "l": "t", "k": ["laws", "robotics", "fictional", "rules", "devised", "science", "fiction", "writer", "isaac", "asimov", "appearing", "short", "story", "runaround", "govern"]}, {"id": "term-throughput", "t": "Throughput", "tg": ["Performance", "Metrics"], "d": "datasets", "x": "The rate at which a system processes requests, often measured in tokens per second. A key performance metric for AI...", "l": "t", "k": ["throughput", "rate", "system", "processes", "requests", "measured", "tokens", "per", "key", "performance", "metric", "serving", "infrastructure"]}, {"id": "term-throughput-latency-tradeoff", "t": "Throughput-Latency Tradeoff", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The fundamental tension in inference systems between maximizing total tokens processed per second (throughput) and...", "l": "t", "k": ["throughput-latency", "tradeoff", "fundamental", "tension", "inference", "systems", "maximizing", "total", "tokens", "processed", "per", "throughput", "minimizing", "per-request", "response"]}, {"id": "term-time-series-cross-validation", "t": "Time Series Cross-Validation", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A cross-validation strategy for temporal data that respects chronological order by always training on past data and...", "l": "t", "k": ["time", "series", "cross-validation", "strategy", "temporal", "data", "respects", "chronological", "order", "always", "training", "past", "validating", "future", "preventing"]}, {"id": "term-ttft", "t": "Time to First Token (TTFT)", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The latency from when a request arrives at an LLM serving system to when the first output token is generated. TTFT is...", "l": "t", "k": ["time", "token", "ttft", "latency", "request", "arrives", "llm", "serving", "system", "output", "generated", "dominated", "prefill", "phase", "critical"]}, {"id": "term-timnit-gebru", "t": "Timnit Gebru", "tg": ["History", "Pioneers"], "d": "history", "x": "Ethiopian-American computer scientist known for research on algorithmic bias and the ethical implications of AI....", "l": "t", "k": ["timnit", "gebru", "ethiopian-american", "computer", "scientist", "known", "research", "algorithmic", "bias", "ethical", "implications", "co-authored", "influential", "gender", "shades"]}, {"id": "term-timnit-gebru-departure", "t": "Timnit Gebru Firing", "tg": ["History", "AI Ethics"], "d": "history", "x": "The controversial departure of AI ethics researcher Timnit Gebru from Google in December 2020 over a paper on large...", "l": "t", "k": ["timnit", "gebru", "firing", "controversial", "departure", "ethics", "researcher", "google", "december", "paper", "large", "language", "model", "risks", "sparking"]}, {"id": "term-token", "t": "Token", "tg": ["Core Concept", "Fundamentals"], "d": "general", "x": "The basic unit AI uses to process text. Roughly 4 characters or 0.75 words in English. Context windows, pricing, and...", "l": "t", "k": ["token", "basic", "unit", "uses", "process", "text", "roughly", "characters", "words", "english", "context", "windows", "pricing", "rate", "limits"]}, {"id": "term-token-merging", "t": "Token Merging", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A technique that progressively combines similar tokens in vision transformers to reduce the number of tokens processed,...", "l": "t", "k": ["token", "merging", "technique", "progressively", "combines", "similar", "tokens", "vision", "transformers", "reduce", "number", "processed", "speeding", "inference", "maintaining"]}, {"id": "term-token-throughput", "t": "Token Throughput", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "The rate of token generation measured in tokens per second across all concurrent requests in an LLM serving system....", "l": "t", "k": ["token", "throughput", "rate", "generation", "measured", "tokens", "per", "across", "concurrent", "requests", "llm", "serving", "system", "key", "metric"]}, {"id": "term-token-level-accuracy", "t": "Token-Level Accuracy", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "An evaluation metric that measures the proportion of individual tokens in a generated sequence that exactly match the...", "l": "t", "k": ["token-level", "accuracy", "evaluation", "metric", "measures", "proportion", "individual", "tokens", "generated", "sequence", "exactly", "match", "corresponding", "reference", "providing"]}, {"id": "term-tokenization", "t": "Tokenization", "tg": ["Process", "NLP"], "d": "general", "x": "The process of breaking text into tokens for model processing. Different tokenizers (BPE, SentencePiece) produce...", "l": "t", "k": ["tokenization", "process", "breaking", "text", "tokens", "model", "processing", "different", "tokenizers", "bpe", "sentencepiece", "produce", "token", "sequences"]}, {"id": "term-tokenization-alignment", "t": "Tokenization Alignment", "tg": ["NLP", "Tokenization"], "d": "general", "x": "The process of mapping between subword tokens produced by a tokenizer and the original word-level or character-level...", "l": "t", "k": ["tokenization", "alignment", "process", "mapping", "subword", "tokens", "produced", "tokenizer", "original", "word-level", "character-level", "boundaries", "essential", "tasks", "ner"]}, {"id": "term-tokenizer", "t": "Tokenizer", "tg": ["LLM", "NLP"], "d": "models", "x": "A component that converts raw text into a sequence of discrete tokens (subwords, characters, or words) that a language...", "l": "t", "k": ["tokenizer", "component", "converts", "raw", "text", "sequence", "discrete", "tokens", "subwords", "characters", "words", "language", "model", "process", "algorithms"]}, {"id": "term-tokenizer-training", "t": "Tokenizer Training", "tg": ["NLP", "Tokenization"], "d": "general", "x": "The process of learning a tokenizer's vocabulary and merge rules from a training corpus, determining how text will be...", "l": "t", "k": ["tokenizer", "training", "process", "learning", "vocabulary", "merge", "rules", "corpus", "determining", "text", "segmented", "tokens", "model", "input"]}, {"id": "term-tool-call-parsing", "t": "Tool Call Parsing", "tg": ["LLM", "Generative AI"], "d": "models", "x": "The process of extracting structured function calls and their arguments from a language model's text output, enabling...", "l": "t", "k": ["tool", "call", "parsing", "process", "extracting", "structured", "function", "calls", "arguments", "language", "model", "text", "output", "enabling", "execution"]}, {"id": "term-tool-use", "t": "Tool Use", "tg": ["Capability", "Agents"], "d": "general", "x": "AI's ability to call external functions, search the web, run code, or access APIs. Enables AI agents to take actions...", "l": "t", "k": ["tool", "ability", "call", "external", "functions", "search", "web", "run", "code", "access", "apis", "enables", "agents", "take", "actions"]}, {"id": "term-toolformer", "t": "Toolformer", "tg": ["LLM", "Generative AI"], "d": "models", "x": "A research approach that teaches language models to autonomously decide when and how to call external tools...", "l": "t", "k": ["toolformer", "research", "approach", "teaches", "language", "models", "autonomously", "decide", "call", "external", "tools", "calculators", "search", "engines", "apis"]}, {"id": "term-top-k-gating", "t": "Top-K Gating", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A routing mechanism in mixture-of-experts models that selects only the top-K experts with the highest gating scores for...", "l": "t", "k": ["top-k", "gating", "routing", "mechanism", "mixture-of-experts", "models", "selects", "experts", "highest", "scores", "input", "token", "enforcing", "sparsity", "balanced"]}, {"id": "term-top-k-retrieval", "t": "Top-K Retrieval", "tg": ["Vector Database", "Search"], "d": "general", "x": "A retrieval operation that returns the K most similar vectors to a query according to the configured distance metric,...", "l": "t", "k": ["top-k", "retrieval", "operation", "returns", "similar", "vectors", "query", "according", "configured", "distance", "metric", "parameter", "controls", "breadth", "results"]}, {"id": "term-top-k-sampling", "t": "Top-K Sampling", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A text generation method that restricts sampling to the K most probable next tokens at each step. Prevents the model...", "l": "t", "k": ["top-k", "sampling", "text", "generation", "method", "restricts", "probable", "next", "tokens", "step", "prevents", "model", "selecting", "highly", "improbable"]}, {"id": "term-top-p", "t": "Top-P (Nucleus Sampling)", "tg": ["Parameter", "Generation"], "d": "general", "x": "A generation strategy that considers tokens until their cumulative probability reaches P. Dynamically adjusts the...", "l": "t", "k": ["top-p", "nucleus", "sampling", "generation", "strategy", "considers", "tokens", "cumulative", "probability", "reaches", "dynamically", "adjusts", "candidate", "pool", "based"]}, {"id": "term-top-p-sampling", "t": "Top-P Sampling", "tg": ["Algorithms", "Fundamentals"], "d": "algorithms", "x": "A text generation method also called nucleus sampling that dynamically selects the smallest set of tokens whose...", "l": "t", "k": ["top-p", "sampling", "text", "generation", "method", "called", "nucleus", "dynamically", "selects", "smallest", "tokens", "whose", "cumulative", "probability", "exceeds"]}, {"id": "term-topic-modeling", "t": "Topic Modeling", "tg": ["NLP", "Text Processing"], "d": "general", "x": "An unsupervised method for discovering abstract topics in a document collection by finding groups of co-occurring...", "l": "t", "k": ["topic", "modeling", "unsupervised", "method", "discovering", "abstract", "topics", "document", "collection", "finding", "groups", "co-occurring", "words", "algorithms", "lda"]}, {"id": "term-tops", "t": "TOPS", "tg": ["Hardware", "Inference Infrastructure"], "d": "hardware", "x": "Tera Operations Per Second, a throughput metric commonly used for AI accelerators measuring trillions of operations per...", "l": "t", "k": ["tops", "tera", "operations", "per", "throughput", "metric", "commonly", "accelerators", "measuring", "trillions", "typically", "int8", "lower", "precision", "standard"]}, {"id": "term-torch-compile", "t": "torch.compile", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "PyTorch's JIT compilation feature that captures and optimizes computation graphs using the TorchDynamo frontend and...", "l": "t", "k": ["torch", "compile", "pytorch", "jit", "compilation", "feature", "captures", "optimizes", "computation", "graphs", "torchdynamo", "frontend", "torchinductor", "backend", "provides"]}, {"id": "term-tortoise-tts", "t": "Tortoise TTS", "tg": ["Models", "Technical"], "d": "models", "x": "A text-to-speech model that achieves high naturalness through an autoregressive approach with CLVP conditioning. Known...", "l": "t", "k": ["tortoise", "tts", "text-to-speech", "model", "achieves", "high", "naturalness", "autoregressive", "approach", "clvp", "conditioning", "known", "producing", "natural-sounding", "speech"]}, {"id": "term-toxicity-score", "t": "Toxicity Score", "tg": ["Evaluation", "Safety"], "d": "datasets", "x": "A metric that quantifies the degree of harmful, offensive, or abusive content in generated text, typically computed...", "l": "t", "k": ["toxicity", "score", "metric", "quantifies", "degree", "harmful", "offensive", "abusive", "content", "generated", "text", "typically", "computed", "classifier", "models"]}, {"id": "term-tpu", "t": "TPU (Tensor Processing Unit)", "tg": ["Hardware", "Infrastructure"], "d": "hardware", "x": "Google's custom AI accelerator chips designed specifically for neural network computations. Used to train many of...", "l": "t", "k": ["tpu", "tensor", "processing", "unit", "google", "custom", "accelerator", "chips", "designed", "specifically", "neural", "network", "computations", "train", "largest"]}, {"id": "term-tpu-development", "t": "TPU Development", "tg": ["History", "Milestones"], "d": "history", "x": "The development of Tensor Processing Units by Google beginning with TPU v1 announced in 2016. Purpose-built for machine...", "l": "t", "k": ["tpu", "development", "tensor", "processing", "units", "google", "beginning", "announced", "purpose-built", "machine", "learning", "workloads", "tpus", "accelerated", "training"]}, {"id": "term-training", "t": "Training", "tg": ["Process", "Fundamentals"], "d": "general", "x": "The process of teaching a model by exposing it to data and adjusting its parameters to minimize prediction errors....", "l": "t", "k": ["training", "process", "teaching", "model", "exposing", "data", "adjusting", "parameters", "minimize", "prediction", "errors", "requires", "significant", "computational", "resources"]}, {"id": "term-training-data", "t": "Training Data", "tg": ["Data", "Fundamentals"], "d": "general", "x": "The content used to teach AI models. Quality, diversity, and scope of training data significantly affect model...", "l": "t", "k": ["training", "data", "content", "teach", "models", "quality", "diversity", "scope", "significantly", "affect", "model", "capabilities", "knowledge", "potential", "biases"]}, {"id": "term-trajectory", "t": "Trajectory", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A sequence of states, actions, and rewards generated by an agent interacting with an environment over multiple time...", "l": "t", "k": ["trajectory", "sequence", "states", "actions", "rewards", "generated", "agent", "interacting", "environment", "multiple", "time", "steps", "trajectories", "represent", "complete"]}, {"id": "term-transfer-learning", "t": "Transfer Learning", "tg": ["Technique", "Training"], "d": "general", "x": "Using knowledge learned from one task to improve performance on another. Foundation models are trained generally, then...", "l": "t", "k": ["transfer", "learning", "knowledge", "learned", "task", "improve", "performance", "another", "foundation", "models", "trained", "generally", "capabilities", "specific", "tasks"]}, {"id": "term-transfer-learning-vision", "t": "Transfer Learning for Vision", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The practice of using a model pre-trained on a large dataset like ImageNet as a feature extractor or starting point for...", "l": "t", "k": ["transfer", "learning", "vision", "practice", "model", "pre-trained", "large", "dataset", "imagenet", "feature", "extractor", "starting", "point", "fine-tuning", "smaller"]}, {"id": "term-transfer-learning-history", "t": "Transfer Learning History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of transfer learning from early domain adaptation research to its central role in modern AI. The...", "l": "t", "k": ["transfer", "learning", "history", "development", "early", "domain", "adaptation", "research", "central", "role", "modern", "practice", "pre-training", "large", "datasets"]}, {"id": "term-transfer-learning-rl", "t": "Transfer Learning in RL", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "Techniques for reusing knowledge learned in one RL task to accelerate learning in a related but different task....", "l": "t", "k": ["transfer", "learning", "techniques", "reusing", "knowledge", "learned", "task", "accelerate", "related", "different", "methods", "include", "policy", "distillation", "reward"]}, {"id": "term-transformer", "t": "Transformer", "tg": ["Architecture", "Foundational"], "d": "models", "x": "The revolutionary neural network architecture (2017) powering modern AI. Uses self-attention to process sequences in...", "l": "t", "k": ["transformer", "revolutionary", "neural", "network", "architecture", "powering", "modern", "uses", "self-attention", "process", "sequences", "parallel", "enabling", "training", "massive"]}, {"id": "term-transformer-block", "t": "Transformer Block", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The fundamental building unit of transformer architectures, consisting of a multi-head self-attention sublayer followed...", "l": "t", "k": ["transformer", "block", "fundamental", "building", "unit", "architectures", "consisting", "multi-head", "self-attention", "sublayer", "followed", "feedforward", "network", "residual", "connections"]}, {"id": "term-transformer-engine", "t": "Transformer Engine", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "NVIDIA's hardware and software system in Hopper and Blackwell GPUs that dynamically manages precision between FP8 and...", "l": "t", "k": ["transformer", "engine", "nvidia", "hardware", "software", "system", "hopper", "blackwell", "gpus", "dynamically", "manages", "precision", "fp8", "higher-precision", "formats"]}, {"id": "term-transition-based-parsing", "t": "Transition-Based Parsing", "tg": ["NLP", "Parsing"], "d": "general", "x": "A parsing approach that builds syntactic structures through a sequence of actions (shift, reduce, left-arc, right-arc)...", "l": "t", "k": ["transition-based", "parsing", "approach", "builds", "syntactic", "structures", "sequence", "actions", "shift", "reduce", "left-arc", "right-arc", "applied", "buffer", "stack"]}, {"id": "term-transparency-in-ai", "t": "Transparency in AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "The principle that AI systems should operate in ways that can be understood, inspected, and communicated to...", "l": "t", "k": ["transparency", "principle", "systems", "operate", "ways", "understood", "inspected", "communicated", "stakeholders", "encompassing", "model", "decision", "organizational"]}, {"id": "term-transposed-convolution", "t": "Transposed Convolution", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An upsampling operation that applies convolution in a way that increases spatial dimensions, commonly used in decoder...", "l": "t", "k": ["transposed", "convolution", "upsampling", "operation", "applies", "increases", "spatial", "dimensions", "commonly", "decoder", "networks", "generative", "models", "reconstruct", "high-resolution"]}, {"id": "term-treacherous-turn", "t": "Treacherous Turn", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "A hypothetical scenario where a misaligned AI behaves cooperatively while it is weak and being monitored, then abruptly...", "l": "t", "k": ["treacherous", "turn", "hypothetical", "scenario", "misaligned", "behaves", "cooperatively", "weak", "monitored", "abruptly", "pursues", "true", "objectives", "becomes", "powerful"]}, {"id": "term-treatment-equality", "t": "Treatment Equality", "tg": ["Fairness", "AI Ethics"], "d": "safety", "x": "A fairness metric requiring that the ratio of false negatives to false positives is equal across protected groups,...", "l": "t", "k": ["treatment", "equality", "fairness", "metric", "requiring", "ratio", "false", "negatives", "positives", "equal", "across", "protected", "groups", "ensuring", "errors"]}, {"id": "term-tree-of-thought", "t": "Tree-of-Thought", "tg": ["Prompting", "Reasoning"], "d": "general", "x": "A prompting technique that explores multiple reasoning paths simultaneously, evaluating and selecting the most...", "l": "t", "k": ["tree-of-thought", "prompting", "technique", "explores", "multiple", "reasoning", "paths", "simultaneously", "evaluating", "selecting", "promising", "branches", "extends", "chain-of-thought", "deliberate"]}, {"id": "term-triplet-loss", "t": "Triplet Loss", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A loss function that operates on triplets of examples (anchor, positive, negative), encouraging the anchor to be closer...", "l": "t", "k": ["triplet", "loss", "function", "operates", "triplets", "examples", "anchor", "positive", "negative", "encouraging", "closer", "least", "specified", "margin", "embedding"]}, {"id": "term-tripwire-mechanism", "t": "Tripwire Mechanism", "tg": ["AI Safety", "Governance"], "d": "safety", "x": "A safety monitoring technique that establishes specific conditions or behavioral thresholds which, when triggered,...", "l": "t", "k": ["tripwire", "mechanism", "safety", "monitoring", "technique", "establishes", "specific", "conditions", "behavioral", "thresholds", "triggered", "automatically", "halt", "constrain", "system"]}, {"id": "term-triton-inference-server", "t": "Triton Inference Server", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "NVIDIA's open-source inference serving platform that supports multiple ML frameworks and hardware backends, providing...", "l": "t", "k": ["triton", "inference", "server", "nvidia", "open-source", "serving", "platform", "supports", "multiple", "frameworks", "hardware", "backends", "providing", "dynamic", "batching"]}, {"id": "term-triton-language", "t": "Triton Language", "tg": ["Hardware", "GPU"], "d": "hardware", "x": "An open-source programming language and compiler for writing efficient GPU kernels for neural networks without...", "l": "t", "k": ["triton", "language", "open-source", "programming", "compiler", "writing", "efficient", "gpu", "kernels", "neural", "networks", "without", "requiring", "low-level", "cuda"]}, {"id": "term-triviaqa", "t": "TriviaQA", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A large-scale reading comprehension and question answering benchmark featuring trivia questions with evidence documents...", "l": "t", "k": ["triviaqa", "large-scale", "reading", "comprehension", "question", "answering", "benchmark", "featuring", "trivia", "questions", "evidence", "documents", "sourced", "wikipedia", "web"]}, {"id": "term-truncated-bptt", "t": "Truncated BPTT", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A practical approximation of backpropagation through time that limits gradient computation to a fixed number of time...", "l": "t", "k": ["truncated", "bptt", "practical", "approximation", "backpropagation", "time", "limits", "gradient", "computation", "fixed", "number", "steps", "reduces", "memory", "requirements"]}, {"id": "term-truncation", "t": "Truncation", "tg": ["Limitation", "Technical"], "d": "general", "x": "Cutting off input that exceeds a model's context window. Can occur at the start (losing context) or end (losing...", "l": "t", "k": ["truncation", "cutting", "off", "input", "exceeds", "model", "context", "window", "occur", "start", "losing", "end", "instructions", "requires", "careful"]}, {"id": "term-trpo", "t": "Trust Region Policy Optimization (TRPO)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "A policy gradient algorithm that constrains updates to stay within a trust region defined by KL divergence between old...", "l": "t", "k": ["trust", "region", "policy", "optimization", "trpo", "gradient", "algorithm", "constrains", "updates", "stay", "within", "defined", "divergence", "old", "policies"]}, {"id": "term-trustworthy-ai", "t": "Trustworthy AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "AI systems designed to be lawful, ethical, and robust, meeting criteria such as human oversight, technical robustness,...", "l": "t", "k": ["trustworthy", "systems", "designed", "lawful", "ethical", "robust", "meeting", "criteria", "human", "oversight", "technical", "robustness", "privacy", "transparency", "fairness"]}, {"id": "term-truthful-ai", "t": "Truthful AI", "tg": ["AI Ethics", "AI Safety"], "d": "safety", "x": "The goal of building AI systems that consistently provide honest and accurate information, avoiding both deliberate...", "l": "t", "k": ["truthful", "goal", "building", "systems", "consistently", "provide", "honest", "accurate", "information", "avoiding", "deliberate", "deception", "negligent", "generation", "false"]}, {"id": "term-truthfulqa", "t": "TruthfulQA", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A benchmark designed to evaluate whether language models generate truthful answers to questions where humans commonly...", "l": "t", "k": ["truthfulqa", "benchmark", "designed", "evaluate", "language", "models", "generate", "truthful", "answers", "questions", "humans", "commonly", "hold", "misconceptions", "testing"]}, {"id": "term-turing-award", "t": "Turing Award", "tg": ["History", "Milestones"], "d": "history", "x": "The most prestigious award in computer science given annually by the Association for Computing Machinery (ACM) since...", "l": "t", "k": ["turing", "award", "prestigious", "computer", "science", "given", "annually", "association", "computing", "machinery", "acm", "named", "alan", "awarded", "numerous"]}, {"id": "term-turing-machine", "t": "Turing Machine", "tg": ["History", "Milestones"], "d": "history", "x": "An abstract mathematical model of computation proposed by Alan Turing in 1936 that manipulates symbols on a tape...", "l": "t", "k": ["turing", "machine", "abstract", "mathematical", "model", "computation", "proposed", "alan", "manipulates", "symbols", "tape", "according", "rules", "providing", "formal"]}, {"id": "term-turing-test", "t": "Turing Test", "tg": ["Historical", "Evaluation"], "d": "datasets", "x": "A test proposed by Alan Turing where a human judge tries to distinguish between human and AI responses. While...", "l": "t", "k": ["turing", "test", "proposed", "alan", "human", "judge", "tries", "distinguish", "responses", "historically", "important", "modern", "llms", "less", "useful"]}, {"id": "term-td3", "t": "Twin Delayed DDPG (TD3)", "tg": ["Reinforcement Learning", "Policy Optimization"], "d": "algorithms", "x": "An improvement over DDPG that addresses overestimation bias using twin Q-networks, delayed policy updates, and target...", "l": "t", "k": ["twin", "delayed", "ddpg", "td3", "improvement", "addresses", "overestimation", "bias", "q-networks", "policy", "updates", "target", "smoothing", "takes", "minimum"]}, {"id": "term-type-i-error", "t": "Type I Error", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The rejection of a true null hypothesis (false positive) in statistical hypothesis testing. The probability of a Type I...", "l": "t", "k": ["type", "error", "rejection", "true", "null", "hypothesis", "false", "positive", "statistical", "testing", "probability", "equal", "significance", "level", "alpha"]}, {"id": "term-type-ii-error", "t": "Type II Error", "tg": ["Statistics", "Inference"], "d": "algorithms", "x": "The failure to reject a false null hypothesis (false negative) in statistical hypothesis testing. The probability of a...", "l": "t", "k": ["type", "error", "failure", "reject", "false", "null", "hypothesis", "negative", "statistical", "testing", "probability", "denoted", "beta", "power", "equals"]}, {"id": "term-typical-decoding", "t": "Typical Decoding", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A text generation strategy that samples tokens whose information content is close to the expected information content...", "l": "t", "k": ["typical", "decoding", "text", "generation", "strategy", "samples", "tokens", "whose", "information", "content", "close", "expected", "model", "avoids", "high-probability"]}, {"id": "term-u-net", "t": "U-Net", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A fully convolutional encoder-decoder architecture with skip connections between corresponding encoder and decoder...", "l": "u", "k": ["u-net", "fully", "convolutional", "encoder-decoder", "architecture", "skip", "connections", "corresponding", "encoder", "decoder", "layers", "originally", "designed", "biomedical", "image"]}, {"id": "term-umap", "t": "UMAP", "tg": ["Machine Learning", "Dimensionality Reduction"], "d": "general", "x": "Uniform Manifold Approximation and Projection, a nonlinear dimensionality reduction method that constructs a...", "l": "u", "k": ["umap", "uniform", "manifold", "approximation", "projection", "nonlinear", "dimensionality", "reduction", "method", "constructs", "topological", "representation", "high-dimensional", "data", "optimizes"]}, {"id": "term-uncanny-valley", "t": "Uncanny Valley", "tg": ["History", "Fundamentals"], "d": "history", "x": "A concept introduced by roboticist Masahiro Mori in 1970 describing the dip in human comfort when encountering robots...", "l": "u", "k": ["uncanny", "valley", "concept", "introduced", "roboticist", "masahiro", "mori", "describing", "dip", "human", "comfort", "encountering", "robots", "animations", "closely"]}, {"id": "term-underfitting", "t": "Underfitting", "tg": ["Problem", "Training"], "d": "general", "x": "When a model is too simple to capture patterns in the data, performing poorly on both training and test data. Addressed...", "l": "u", "k": ["underfitting", "model", "simple", "capture", "patterns", "data", "performing", "poorly", "training", "test", "addressed", "increasing", "capacity", "longer"]}, {"id": "term-undersampling", "t": "Undersampling", "tg": ["Machine Learning", "Data Science"], "d": "algorithms", "x": "A strategy for addressing class imbalance by randomly removing examples from the majority class to create a more...", "l": "u", "k": ["undersampling", "strategy", "addressing", "class", "imbalance", "randomly", "removing", "examples", "majority", "create", "balanced", "training", "potentially", "cost", "losing"]}, {"id": "term-unesco-ai-ethics-recommendation", "t": "UNESCO AI Ethics Recommendation", "tg": ["Governance", "Regulation"], "d": "safety", "x": "The first global standard on AI ethics, adopted by UNESCO member states in 2021, providing a comprehensive framework...", "l": "u", "k": ["unesco", "ethics", "recommendation", "global", "standard", "adopted", "member", "states", "providing", "comprehensive", "framework", "covering", "values", "principles", "policy"]}, {"id": "term-unet-diffusion", "t": "UNet Diffusion", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The U-Net backbone commonly used in diffusion models as the denoising network, incorporating timestep conditioning,...", "l": "u", "k": ["unet", "diffusion", "u-net", "backbone", "commonly", "models", "denoising", "network", "incorporating", "timestep", "conditioning", "cross-attention", "text", "skip", "connections"]}, {"id": "term-unification-algorithm", "t": "Unification Algorithm", "tg": ["History", "Fundamentals"], "d": "history", "x": "A fundamental algorithm in logic and computer science that finds a substitution making two terms identical. Essential...", "l": "u", "k": ["unification", "algorithm", "fundamental", "logic", "computer", "science", "finds", "substitution", "making", "terms", "identical", "essential", "prolog", "automated", "theorem"]}, {"id": "term-unified-io", "t": "Unified-IO", "tg": ["Models", "Technical"], "d": "models", "x": "A model that unifies diverse vision and language tasks in a single architecture using a sequence-to-sequence framework....", "l": "u", "k": ["unified-io", "model", "unifies", "diverse", "vision", "language", "tasks", "single", "architecture", "sequence-to-sequence", "framework", "handles", "image", "generation", "segmentation"]}, {"id": "term-uniform-distribution", "t": "Uniform Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A probability distribution where all outcomes in a given range are equally likely. The continuous version has constant...", "l": "u", "k": ["uniform", "distribution", "probability", "outcomes", "given", "range", "equally", "likely", "continuous", "version", "constant", "density", "interval", "discrete", "assigns"]}, {"id": "term-unigram-language-model-tokenizer", "t": "Unigram Language Model Tokenizer", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A subword tokenization method that starts with a large vocabulary and iteratively removes tokens that least affect the...", "l": "u", "k": ["unigram", "language", "model", "tokenizer", "subword", "tokenization", "method", "starts", "large", "vocabulary", "iteratively", "removes", "tokens", "least", "affect"]}, {"id": "term-unigram-tokenization", "t": "Unigram Tokenization", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A subword tokenization method that starts with a large vocabulary and iteratively removes tokens whose loss has the...", "l": "u", "k": ["unigram", "tokenization", "subword", "method", "starts", "large", "vocabulary", "iteratively", "removes", "tokens", "whose", "loss", "least", "impact", "overall"]}, {"id": "term-unigram-tokenizer", "t": "Unigram Tokenizer", "tg": ["LLM", "NLP"], "d": "models", "x": "A subword tokenization algorithm that starts with a large vocabulary and iteratively removes tokens to minimize the...", "l": "u", "k": ["unigram", "tokenizer", "subword", "tokenization", "algorithm", "starts", "large", "vocabulary", "iteratively", "removes", "tokens", "minimize", "overall", "loss", "training"]}, {"id": "term-unimate", "t": "Unimate", "tg": ["History", "Systems"], "d": "history", "x": "The first industrial robot installed on a General Motors assembly line in 1961. Developed by George Devol and Joseph...", "l": "u", "k": ["unimate", "industrial", "robot", "installed", "general", "motors", "assembly", "line", "developed", "george", "devol", "joseph", "engelberger", "performed", "tasks"]}, {"id": "term-uninformative-prior", "t": "Uninformative Prior", "tg": ["Statistics", "Bayesian Methods"], "d": "algorithms", "x": "A prior distribution that expresses minimal information about the parameter before observing data, such as a uniform...", "l": "u", "k": ["uninformative", "prior", "distribution", "expresses", "minimal", "information", "parameter", "observing", "data", "uniform", "space", "allowing", "dominate", "posterior"]}, {"id": "term-universal-approximation-theorem", "t": "Universal Approximation Theorem", "tg": ["History", "Fundamentals"], "d": "history", "x": "A theorem proving that feedforward neural networks with a single hidden layer containing a finite number of neurons can...", "l": "u", "k": ["universal", "approximation", "theorem", "proving", "feedforward", "neural", "networks", "single", "hidden", "layer", "containing", "finite", "number", "neurons", "approximate"]}, {"id": "term-universal-basic-income-and-ai", "t": "Universal Basic Income and AI", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "Proposals for unconditional periodic cash payments to all citizens, discussed as a potential policy response to...", "l": "u", "k": ["universal", "basic", "income", "proposals", "unconditional", "periodic", "cash", "payments", "citizens", "discussed", "potential", "policy", "response", "widespread", "job"]}, {"id": "term-universal-dependencies", "t": "Universal Dependencies", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A cross-linguistically consistent framework for annotating grammar including parts of speech, morphological features,...", "l": "u", "k": ["universal", "dependencies", "cross-linguistically", "consistent", "framework", "annotating", "grammar", "including", "parts", "speech", "morphological", "features", "syntactic", "enabling", "multilingual"]}, {"id": "term-universal-sentence-encoder", "t": "Universal Sentence Encoder", "tg": ["Models", "Technical"], "d": "models", "x": "A model that encodes text into high-dimensional vectors for transfer learning across NLP tasks. Available in...", "l": "u", "k": ["universal", "sentence", "encoder", "model", "encodes", "text", "high-dimensional", "vectors", "transfer", "learning", "across", "nlp", "tasks", "available", "transformer"]}, {"id": "term-universal-turing-machine", "t": "Universal Turing Machine", "tg": ["History", "Fundamentals"], "d": "history", "x": "A Turing machine that can simulate any other Turing machine given a description of that machine on its tape. Proposed...", "l": "u", "k": ["universal", "turing", "machine", "simulate", "given", "description", "tape", "proposed", "alan", "concept", "anticipated", "stored-program", "computers", "established", "single"]}, {"id": "term-unstructured-pruning", "t": "Unstructured Pruning", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A pruning approach that removes individual weights regardless of their position in the weight matrix, resulting in...", "l": "u", "k": ["unstructured", "pruning", "approach", "removes", "individual", "weights", "regardless", "position", "weight", "matrix", "resulting", "sparse", "matrices", "achieves", "higher"]}, {"id": "term-unsupervised-environment-design", "t": "Unsupervised Environment Design", "tg": ["Reinforcement Learning", "Training Paradigms"], "d": "general", "x": "A framework where a teacher agent or mechanism automatically generates training environments at the frontier of the...", "l": "u", "k": ["unsupervised", "environment", "design", "framework", "teacher", "agent", "mechanism", "automatically", "generates", "training", "environments", "frontier", "student", "capabilities", "methods"]}, {"id": "term-unsupervised-learning", "t": "Unsupervised Learning", "tg": ["Learning Type", "Fundamentals"], "d": "general", "x": "Machine learning from unlabeled data, discovering patterns without explicit guidance. Includes clustering,...", "l": "u", "k": ["unsupervised", "learning", "machine", "unlabeled", "data", "discovering", "patterns", "without", "explicit", "guidance", "includes", "clustering", "dimensionality", "reduction", "self-supervised"]}, {"id": "term-upper-confidence-bound", "t": "Upper Confidence Bound", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A family of bandit algorithms that select the action with the highest upper confidence bound on its expected reward,...", "l": "u", "k": ["upper", "confidence", "bound", "family", "bandit", "algorithms", "select", "action", "highest", "expected", "reward", "combining", "estimated", "exploration", "bonus"]}, {"id": "term-upsampling", "t": "Upsampling", "tg": ["Technique", "Data"], "d": "general", "x": "Increasing resolution or quantity of data. In image AI, upscaling low-res images. In training, duplicating...", "l": "u", "k": ["upsampling", "increasing", "resolution", "quantity", "data", "image", "upscaling", "low-res", "images", "training", "duplicating", "underrepresented", "examples", "balance", "datasets"]}, {"id": "term-use-case", "t": "Use Case", "tg": ["Concept", "Application"], "d": "general", "x": "A specific application or scenario where AI provides value. Understanding your use case helps choose the right model,...", "l": "u", "k": ["case", "specific", "application", "scenario", "provides", "value", "understanding", "helps", "choose", "right", "model", "prompting", "strategy", "safety", "measures"]}, {"id": "term-user-prompt", "t": "User Prompt", "tg": ["Prompting", "Concept"], "d": "general", "x": "The input provided by the user in a conversation, as opposed to the system prompt set by developers. Together with...", "l": "u", "k": ["user", "prompt", "input", "provided", "conversation", "opposed", "system", "developers", "together", "prompts", "form", "complete", "context", "responses"]}, {"id": "term-utility-function", "t": "Utility Function", "tg": ["Concept", "Alignment"], "d": "safety", "x": "A mathematical function that measures how \"good\" an outcome is. In AI alignment, designing utility functions that...", "l": "u", "k": ["utility", "function", "mathematical", "measures", "good", "outcome", "alignment", "designing", "functions", "capture", "human", "values", "fundamental", "challenge"]}, {"id": "term-variational-autoencoder", "t": "VAE (Variational Autoencoder)", "tg": ["Architecture", "Generative"], "d": "models", "x": "A generative model that learns a latent space representation of data. Used in image generation and as components of...", "l": "v", "k": ["vae", "variational", "autoencoder", "generative", "model", "learns", "latent", "space", "representation", "data", "image", "generation", "components", "larger", "systems"]}, {"id": "term-vae-decoder", "t": "VAE Decoder", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "The decoder component of a Variational Autoencoder used in latent diffusion models to reconstruct high-resolution...", "l": "v", "k": ["vae", "decoder", "component", "variational", "autoencoder", "latent", "diffusion", "models", "reconstruct", "high-resolution", "images", "compressed", "representations", "produced", "process"]}, {"id": "term-validation", "t": "Validation", "tg": ["Process", "Evaluation"], "d": "datasets", "x": "Testing model performance on data not used in training to assess generalization. Validation sets help tune...", "l": "v", "k": ["validation", "testing", "model", "performance", "data", "training", "assess", "generalization", "sets", "help", "tune", "hyperparameters", "test", "provide", "final"]}, {"id": "term-validation-curve", "t": "Validation Curve", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A plot of training and validation scores as a function of a single hyperparameter, revealing the optimal hyperparameter...", "l": "v", "k": ["validation", "curve", "plot", "training", "scores", "function", "single", "hyperparameter", "revealing", "optimal", "value", "model", "underfitting", "overfitting", "different"]}, {"id": "term-vall-e", "t": "VALL-E", "tg": ["Models", "Technical"], "d": "models", "x": "A neural codec language model for text-to-speech by Microsoft that can synthesize personalized speech from a 3-second...", "l": "v", "k": ["vall-e", "neural", "codec", "language", "model", "text-to-speech", "microsoft", "synthesize", "personalized", "speech", "3-second", "reference", "recording", "treats", "tts"]}, {"id": "term-value-alignment", "t": "Value Alignment", "tg": ["AI Safety", "Alignment"], "d": "safety", "x": "The challenge of ensuring that an AI system's objectives and behaviors are consistent with human values and intentions....", "l": "v", "k": ["value", "alignment", "challenge", "ensuring", "system", "objectives", "behaviors", "consistent", "human", "values", "intentions", "considered", "core", "problems", "safety"]}, {"id": "term-value-function", "t": "Value Function", "tg": ["Reinforcement Learning", "Value Methods"], "d": "general", "x": "A function that estimates the expected cumulative future reward from a given state (or state-action pair) under a...", "l": "v", "k": ["value", "function", "estimates", "expected", "cumulative", "future", "reward", "given", "state", "state-action", "pair", "particular", "policy", "functions", "central"]}, {"id": "term-value-iteration", "t": "Value Iteration", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A dynamic programming algorithm that computes the optimal value function by iteratively applying the Bellman optimality...", "l": "v", "k": ["value", "iteration", "dynamic", "programming", "algorithm", "computes", "optimal", "function", "iteratively", "applying", "bellman", "optimality", "equation", "converges", "policy"]}, {"id": "term-value-sensitive-design", "t": "Value-Sensitive Design", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "A design methodology that accounts for human values in a principled and systematic manner throughout the design...", "l": "v", "k": ["value-sensitive", "design", "methodology", "accounts", "human", "values", "principled", "systematic", "manner", "throughout", "process", "incorporating", "conceptual", "empirical", "technical"]}, {"id": "term-vanishing-gradient", "t": "Vanishing Gradient", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A problem in deep neural network training where gradients become exponentially small as they are backpropagated through...", "l": "v", "k": ["vanishing", "gradient", "problem", "deep", "neural", "network", "training", "gradients", "become", "exponentially", "small", "backpropagated", "layers", "causing", "early"]}, {"id": "term-vanishing-gradient-problem", "t": "Vanishing Gradient Problem", "tg": ["History", "Milestones"], "d": "history", "x": "A difficulty encountered in training deep neural networks where gradients become exponentially small as they are...", "l": "v", "k": ["vanishing", "gradient", "problem", "difficulty", "encountered", "training", "deep", "neural", "networks", "gradients", "become", "exponentially", "small", "propagated", "backward"]}, {"id": "term-vapnik-chervonenkis-theory", "t": "Vapnik-Chervonenkis Theory", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "A theoretical framework in statistical learning theory that provides bounds on the generalization error of classifiers...", "l": "v", "k": ["vapnik-chervonenkis", "theory", "theoretical", "framework", "statistical", "learning", "provides", "bounds", "generalization", "error", "classifiers", "based", "dimension", "hypothesis", "class"]}, {"id": "term-variance", "t": "Variance", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A measure of the dispersion of a set of values, computed as the average of the squared deviations from the mean. It...", "l": "v", "k": ["variance", "measure", "dispersion", "values", "computed", "average", "squared", "deviations", "mean", "quantifies", "spread", "data", "points", "distribution"]}, {"id": "term-variance-inflation-factor", "t": "Variance Inflation Factor", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A measure of how much the variance of a regression coefficient is inflated due to multicollinearity with other...", "l": "v", "k": ["variance", "inflation", "factor", "measure", "regression", "coefficient", "inflated", "due", "multicollinearity", "predictors", "vif", "values", "typically", "indicate", "problematic"]}, {"id": "term-variance-threshold", "t": "Variance Threshold", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A simple feature selection method that removes all features whose variance falls below a specified threshold. Features...", "l": "v", "k": ["variance", "threshold", "simple", "feature", "selection", "method", "removes", "features", "whose", "falls", "specified", "near-zero", "provide", "little", "discriminative"]}, {"id": "term-vae", "t": "Variational Autoencoder", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generative model that learns a probabilistic mapping between data and a continuous latent space by optimizing a...", "l": "v", "k": ["variational", "autoencoder", "generative", "model", "learns", "probabilistic", "mapping", "data", "continuous", "latent", "space", "optimizing", "lower", "bound", "enabling"]}, {"id": "term-variational-inference", "t": "Variational Inference", "tg": ["Machine Learning", "Bayesian Methods"], "d": "general", "x": "An approximate Bayesian inference technique that transforms the inference problem into an optimization problem by...", "l": "v", "k": ["variational", "inference", "approximate", "bayesian", "technique", "transforms", "problem", "optimization", "finding", "member", "tractable", "distribution", "family", "closest", "true"]}, {"id": "term-vc-dimension", "t": "VC Dimension", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "Vapnik-Chervonenkis dimension, a measure of the capacity or complexity of a hypothesis class, defined as the largest...", "l": "v", "k": ["dimension", "vapnik-chervonenkis", "measure", "capacity", "complexity", "hypothesis", "class", "defined", "largest", "points", "shattered", "perfectly", "classified", "possible", "labelings"]}, {"id": "term-vector", "t": "Vector", "tg": ["Math", "Representation"], "d": "general", "x": "An ordered list of numbers representing data in a mathematical space. Embeddings are vectors; vector similarity...", "l": "v", "k": ["vector", "ordered", "list", "numbers", "representing", "data", "mathematical", "space", "embeddings", "vectors", "similarity", "measures", "cosine", "enable", "semantic"]}, {"id": "term-vector-autoregression", "t": "Vector Autoregression", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A multivariate time series model where each variable is regressed on its own past values and the past values of all...", "l": "v", "k": ["vector", "autoregression", "multivariate", "time", "series", "model", "variable", "regressed", "past", "values", "variables", "system", "capturing", "linear", "interdependencies"]}, {"id": "term-vector-database", "t": "Vector Database", "tg": ["Infrastructure", "Search"], "d": "hardware", "x": "A database optimized for storing and searching high-dimensional vectors (embeddings). Essential for RAG systems,...", "l": "v", "k": ["vector", "database", "optimized", "storing", "searching", "high-dimensional", "vectors", "embeddings", "essential", "rag", "systems", "semantic", "search", "recommendation", "engines"]}, {"id": "term-vector-database-sharding", "t": "Vector Database Sharding", "tg": ["Vector Database", "Infrastructure"], "d": "hardware", "x": "The horizontal partitioning of a vector index across multiple nodes or storage units to distribute data and query load,...", "l": "v", "k": ["vector", "database", "sharding", "horizontal", "partitioning", "index", "across", "multiple", "nodes", "storage", "units", "distribute", "data", "query", "load"]}, {"id": "term-vector-index", "t": "Vector Index", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "A specialized data structure optimized for efficient similarity search over high-dimensional vector collections,...", "l": "v", "k": ["vector", "index", "specialized", "data", "structure", "optimized", "efficient", "similarity", "search", "high-dimensional", "collections", "employing", "algorithms", "hnsw", "ivf"]}, {"id": "term-vector-institute", "t": "Vector Institute", "tg": ["History", "Organizations"], "d": "history", "x": "A Canadian AI research institute founded in 2017 in Toronto with Geoffrey Hinton as chief scientific advisor. The...", "l": "v", "k": ["vector", "institute", "canadian", "research", "founded", "toronto", "geoffrey", "hinton", "chief", "scientific", "advisor", "focuses", "machine", "learning", "deep"]}, {"id": "term-vector-normalization", "t": "Vector Normalization", "tg": ["Vector Database", "Preprocessing"], "d": "general", "x": "The process of scaling vectors to unit length by dividing each component by the vector's L2 norm, ensuring that cosine...", "l": "v", "k": ["vector", "normalization", "process", "scaling", "vectors", "unit", "length", "dividing", "component", "norm", "ensuring", "cosine", "similarity", "dot", "product"]}, {"id": "term-vector-similarity-join", "t": "Vector Similarity Join", "tg": ["Vector Database", "Search"], "d": "general", "x": "A database operation that finds all pairs of vectors across two collections whose similarity exceeds a given threshold,...", "l": "v", "k": ["vector", "similarity", "join", "database", "operation", "finds", "pairs", "vectors", "across", "collections", "whose", "exceeds", "given", "threshold", "deduplication"]}, {"id": "term-vector-store", "t": "Vector Store", "tg": ["Vector Database", "Infrastructure"], "d": "hardware", "x": "A storage system specialized for persisting, indexing, and querying vector embeddings alongside their associated...", "l": "v", "k": ["vector", "store", "storage", "system", "specialized", "persisting", "indexing", "querying", "embeddings", "alongside", "associated", "metadata", "original", "content", "serving"]}, {"id": "term-vector-jacobian-product", "t": "Vector-Jacobian Product", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An efficient computation that multiplies a vector by the Jacobian matrix without explicitly forming the full Jacobian....", "l": "v", "k": ["vector-jacobian", "product", "efficient", "computation", "multiplies", "vector", "jacobian", "matrix", "without", "explicitly", "forming", "full", "fundamental", "operation", "reverse-mode"]}, {"id": "term-vectorized-environment", "t": "Vectorized Environment", "tg": ["Reinforcement Learning", "Core Concepts"], "d": "general", "x": "A technique for running multiple copies of an environment in parallel within a single process, enabling batch...", "l": "v", "k": ["vectorized", "environment", "technique", "running", "multiple", "copies", "parallel", "within", "single", "process", "enabling", "batch", "collection", "experience", "efficient"]}, {"id": "term-verification", "t": "Verification (AI Outputs)", "tg": ["Practice", "Safety"], "d": "safety", "x": "Checking AI outputs for accuracy and appropriateness before use. Essential for high-stakes applications. Can be done by...", "l": "v", "k": ["verification", "outputs", "checking", "accuracy", "appropriateness", "essential", "high-stakes", "applications", "done", "humans", "systems", "automated", "checks"]}, {"id": "term-verification-chain-prompting", "t": "Verification Chain Prompting", "tg": ["Prompt Engineering", "Verification"], "d": "general", "x": "A prompting technique that generates an initial response, then systematically creates and answers verification...", "l": "v", "k": ["verification", "chain", "prompting", "technique", "generates", "initial", "response", "systematically", "creates", "answers", "questions", "specific", "claims", "results", "produce"]}, {"id": "term-vgg", "t": "VGG", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A deep convolutional network architecture characterized by its use of very small 3x3 convolution filters throughout the...", "l": "v", "k": ["vgg", "deep", "convolutional", "network", "architecture", "characterized", "small", "3x3", "convolution", "filters", "throughout", "entire", "demonstrating", "depth", "improves"]}, {"id": "term-vgg-16", "t": "VGG-16", "tg": ["Models", "Technical"], "d": "models", "x": "A 16-layer variant of the VGG architecture with 138 million parameters. One of the most commonly used pretrained models...", "l": "v", "k": ["vgg-16", "16-layer", "variant", "vgg", "architecture", "million", "parameters", "commonly", "pretrained", "models", "transfer", "learning", "feature", "extraction", "despite"]}, {"id": "term-vicuna", "t": "Vicuna", "tg": ["Models", "Technical"], "d": "models", "x": "An open-source chatbot fine-tuned from LLaMA on user-shared conversations from ShareGPT. Achieved quality close to...", "l": "v", "k": ["vicuna", "open-source", "chatbot", "fine-tuned", "llama", "user-shared", "conversations", "sharegpt", "achieved", "quality", "close", "chatgpt", "according", "gpt-4", "evaluation"]}, {"id": "term-video-generation", "t": "Video Generation", "tg": ["Generative AI", "Image Processing"], "d": "models", "x": "The synthesis of temporally coherent video sequences from text prompts, images, or other conditioning signals using...", "l": "v", "k": ["video", "generation", "synthesis", "temporally", "coherent", "sequences", "text", "prompts", "images", "conditioning", "signals", "extended", "diffusion", "autoregressive", "models"]}, {"id": "term-video-transformer", "t": "Video Transformer", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A transformer architecture adapted for video processing that handles both spatial and temporal dimensions through...", "l": "v", "k": ["video", "transformer", "architecture", "adapted", "processing", "handles", "spatial", "temporal", "dimensions", "factored", "attention", "tubelet", "embeddings", "space-time", "mechanisms"]}, {"id": "term-video-understanding", "t": "Video Understanding", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The comprehensive analysis of video content including action recognition, temporal event detection, scene...", "l": "v", "k": ["video", "understanding", "comprehensive", "analysis", "content", "including", "action", "recognition", "temporal", "event", "detection", "scene", "classification", "narrative", "across"]}, {"id": "term-vision-transformer", "t": "Vision Transformer", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A transformer architecture applied to images by splitting them into fixed-size patches, linearly embedding each patch,...", "l": "v", "k": ["vision", "transformer", "architecture", "applied", "images", "splitting", "fixed-size", "patches", "linearly", "embedding", "patch", "processing", "sequence", "embeddings", "standard"]}, {"id": "term-vision-language-model", "t": "Vision-Language Model (VLM)", "tg": ["Model Type", "Multimodal"], "d": "models", "x": "AI models that can process and reason about both images and text. Examples include GPT-4V, Claude with vision, and...", "l": "v", "k": ["vision-language", "model", "vlm", "models", "process", "reason", "images", "text", "examples", "include", "gpt-4v", "claude", "vision", "gemini", "enable"]}, {"id": "term-visual-grounding", "t": "Visual Grounding", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of localizing a region or object in an image based on a natural language description, requiring the model to...", "l": "v", "k": ["visual", "grounding", "task", "localizing", "region", "object", "image", "based", "natural", "language", "description", "requiring", "model", "ground", "textual"]}, {"id": "term-visual-question-answering", "t": "Visual Question Answering", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "A multimodal AI task that requires answering natural language questions about the content of an image, demanding both...", "l": "v", "k": ["visual", "question", "answering", "multimodal", "task", "requires", "natural", "language", "questions", "content", "image", "demanding", "understanding", "reasoning", "capabilities"]}, {"id": "term-visual-relationship-detection", "t": "Visual Relationship Detection", "tg": ["Computer Vision", "Image Processing"], "d": "hardware", "x": "The task of identifying and classifying the interactions or spatial relationships between pairs of objects in an image,...", "l": "v", "k": ["visual", "relationship", "detection", "task", "identifying", "classifying", "interactions", "spatial", "relationships", "pairs", "objects", "image", "producing", "subject-predicate-object", "triplets"]}, {"id": "term-visual-slam", "t": "Visual SLAM", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "Visual Simultaneous Localization and Mapping, a technique that estimates camera trajectory and builds a 3D map of the...", "l": "v", "k": ["visual", "slam", "simultaneous", "localization", "mapping", "technique", "estimates", "camera", "trajectory", "builds", "map", "environment", "sequence", "images", "real-time"]}, {"id": "term-viterbi-algorithm", "t": "Viterbi Algorithm", "tg": ["NLP", "Text Processing"], "d": "general", "x": "A dynamic programming algorithm that finds the most likely sequence of hidden states in an HMM or CRF, used for optimal...", "l": "v", "k": ["viterbi", "algorithm", "dynamic", "programming", "finds", "likely", "sequence", "hidden", "states", "hmm", "crf", "optimal", "decoding", "labeling", "speech"]}, {"id": "term-vladimir-vapnik", "t": "Vladimir Vapnik", "tg": ["History", "Pioneers"], "d": "history", "x": "Russian-American mathematician who co-developed the Vapnik-Chervonenkis (VC) theory of statistical learning and...", "l": "v", "k": ["vladimir", "vapnik", "russian-american", "mathematician", "co-developed", "vapnik-chervonenkis", "theory", "statistical", "learning", "invented", "support", "vector", "machines", "svms", "1990s"]}, {"id": "term-vllm", "t": "vLLM", "tg": ["LLM", "Inference"], "d": "models", "x": "An open-source high-throughput LLM inference engine that implements PagedAttention and continuous batching to...", "l": "v", "k": ["vllm", "open-source", "high-throughput", "llm", "inference", "engine", "implements", "pagedattention", "continuous", "batching", "efficiently", "serve", "large", "language", "models"]}, {"id": "term-vocab", "t": "Vocabulary (Model)", "tg": ["Technical", "Tokenization"], "d": "general", "x": "The set of tokens a model can recognize and generate. Determined during tokenizer training. Larger vocabularies handle...", "l": "v", "k": ["vocabulary", "model", "tokens", "recognize", "generate", "determined", "tokenizer", "training", "larger", "vocabularies", "handle", "languages", "increase", "size"]}, {"id": "term-vocabulary-size", "t": "Vocabulary Size", "tg": ["NLP", "Tokenization"], "d": "general", "x": "The total number of unique tokens in a model's tokenizer vocabulary, which affects model size, tokenization efficiency,...", "l": "v", "k": ["vocabulary", "size", "total", "number", "unique", "tokens", "model", "tokenizer", "affects", "tokenization", "efficiency", "balance", "sequence", "length", "token"]}, {"id": "term-voice-clone", "t": "Voice Cloning", "tg": ["Application", "Ethics"], "d": "safety", "x": "AI that replicates a specific person's voice from audio samples. Raises significant ethical concerns about consent and...", "l": "v", "k": ["voice", "cloning", "replicates", "specific", "person", "audio", "samples", "raises", "significant", "ethical", "concerns", "consent", "misuse", "fraud", "misinformation"]}, {"id": "term-voluntary-commitments-on-ai", "t": "Voluntary Commitments on AI", "tg": ["Governance", "Regulation"], "d": "safety", "x": "Non-binding pledges by AI companies to the White House in 2023 to manage risks from AI, including commitments to safety...", "l": "v", "k": ["voluntary", "commitments", "non-binding", "pledges", "companies", "white", "house", "manage", "risks", "including", "safety", "testing", "information", "sharing", "watermarking"]}, {"id": "term-von-neumann-architecture", "t": "Von Neumann Architecture", "tg": ["History", "Milestones"], "d": "history", "x": "The computer architecture described by John von Neumann in 1945 that stores both program instructions and data in the...", "l": "v", "k": ["von", "neumann", "architecture", "computer", "described", "john", "stores", "program", "instructions", "data", "memory", "becoming", "dominant", "design", "paradigm"]}, {"id": "term-voronoi-partitioning", "t": "Voronoi Partitioning", "tg": ["Vector Database", "Index Structure"], "d": "general", "x": "A spatial decomposition technique used in IVF indexes that divides the vector space into regions where each region...", "l": "v", "k": ["voronoi", "partitioning", "spatial", "decomposition", "technique", "ivf", "indexes", "divides", "vector", "space", "regions", "region", "contains", "points", "closest"]}, {"id": "term-voting-classifier", "t": "Voting Classifier", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An ensemble method that aggregates predictions from multiple classifiers using either majority voting (hard voting) or...", "l": "v", "k": ["voting", "classifier", "ensemble", "method", "aggregates", "predictions", "multiple", "classifiers", "majority", "hard", "averaged", "predicted", "probabilities", "soft", "produce"]}, {"id": "term-voxel", "t": "Voxel", "tg": ["Computer Vision", "3D Vision"], "d": "hardware", "x": "A volumetric pixel representing a value on a regular 3D grid, used as a discrete representation for 3D data in neural...", "l": "v", "k": ["voxel", "volumetric", "pixel", "representing", "value", "regular", "grid", "discrete", "representation", "data", "neural", "networks", "analogous", "pixels", "images"]}, {"id": "term-vq-vae", "t": "VQ-VAE", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "Vector Quantized Variational Autoencoder, a model that uses discrete latent representations through vector...", "l": "v", "k": ["vq-vae", "vector", "quantized", "variational", "autoencoder", "model", "uses", "discrete", "latent", "representations", "quantization", "enabling", "high-fidelity", "generation", "combining"]}, {"id": "term-vq-vae-2", "t": "VQ-VAE-2", "tg": ["Models", "Technical"], "d": "models", "x": "An improved version of VQ-VAE that uses a hierarchical latent structure with multiple levels of quantization. Generates...", "l": "v", "k": ["vq-vae-2", "improved", "version", "vq-vae", "uses", "hierarchical", "latent", "structure", "multiple", "levels", "quantization", "generates", "high-quality", "images", "comparable"]}, {"id": "term-vulnerability-exploitation-by-ai", "t": "Vulnerability Exploitation by AI", "tg": ["AI Ethics", "Regulation"], "d": "safety", "x": "AI systems that exploit the vulnerabilities of specific groups such as children, elderly, or persons with disabilities...", "l": "v", "k": ["vulnerability", "exploitation", "systems", "exploit", "vulnerabilities", "specific", "groups", "children", "elderly", "persons", "disabilities", "materially", "distort", "behavior", "prohibited"]}, {"id": "term-wafer-scale-computing", "t": "Wafer-Scale Computing", "tg": ["Hardware", "Distributed Computing"], "d": "hardware", "x": "An approach to AI hardware that uses an entire silicon wafer as a single chip rather than cutting it into individual...", "l": "w", "k": ["wafer-scale", "computing", "approach", "hardware", "uses", "entire", "silicon", "wafer", "single", "chip", "rather", "cutting", "individual", "dies", "processors"]}, {"id": "term-walter-pitts", "t": "Walter Pitts", "tg": ["History", "Pioneers"], "d": "history", "x": "American logician (1923-1969) who, with Warren McCulloch, developed the McCulloch-Pitts neuron model in 1943,...", "l": "w", "k": ["walter", "pitts", "american", "logician", "1923-1969", "warren", "mcculloch", "developed", "mcculloch-pitts", "neuron", "model", "demonstrating", "networks", "simple", "logical"]}, {"id": "term-warm-restarts", "t": "Warm Restarts", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A training technique that periodically resets the learning rate to a high value during training. Helps the optimizer...", "l": "w", "k": ["warm", "restarts", "training", "technique", "periodically", "resets", "learning", "rate", "high", "value", "helps", "optimizer", "escape", "local", "minima"]}, {"id": "term-warm-starting", "t": "Warm Starting", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A training strategy that initializes model parameters from a previously trained model rather than random values....", "l": "w", "k": ["warm", "starting", "training", "strategy", "initializes", "model", "parameters", "previously", "trained", "rather", "random", "values", "accelerates", "convergence", "improve"]}, {"id": "term-warm-up-cosine-decay", "t": "Warm-up Cosine Decay", "tg": ["Algorithms", "Training"], "d": "algorithms", "x": "A learning rate schedule that combines a linear warmup phase with cosine decay. Starts with a low learning rate...", "l": "w", "k": ["warm-up", "cosine", "decay", "learning", "rate", "schedule", "combines", "linear", "warmup", "phase", "starts", "low", "gradually", "increases", "peak"]}, {"id": "term-warmup", "t": "Warmup (Learning Rate)", "tg": ["Training", "Technique"], "d": "general", "x": "Gradually increasing learning rate at the start of training before decay. Helps stabilize early training when gradients...", "l": "w", "k": ["warmup", "learning", "rate", "gradually", "increasing", "start", "training", "decay", "helps", "stabilize", "early", "gradients", "unreliable", "random", "weights"]}, {"id": "term-warren-mcculloch", "t": "Warren McCulloch", "tg": ["History", "Pioneers"], "d": "history", "x": "American neurophysiologist (1898-1969) who, with Walter Pitts, created the first mathematical model of an artificial...", "l": "w", "k": ["warren", "mcculloch", "american", "neurophysiologist", "1898-1969", "walter", "pitts", "created", "mathematical", "model", "artificial", "neuron", "laying", "theoretical", "foundation"]}, {"id": "term-wasserstein-distance", "t": "Wasserstein Distance", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "Also known as the Earth Mover's Distance, a metric measuring the minimum cost of transforming one probability...", "l": "w", "k": ["wasserstein", "distance", "known", "earth", "mover", "metric", "measuring", "minimum", "cost", "transforming", "probability", "distribution", "another", "amount", "mass"]}, {"id": "term-watermark", "t": "Watermark (AI)", "tg": ["Safety", "Detection"], "d": "safety", "x": "Hidden patterns in AI-generated content that allow detection of synthetic origins. Proposed for identifying AI text,...", "l": "w", "k": ["watermark", "hidden", "patterns", "ai-generated", "content", "allow", "detection", "synthetic", "origins", "proposed", "identifying", "text", "images", "audio", "combat"]}, {"id": "term-watermark-detection", "t": "Watermark Detection", "tg": ["Generative AI", "LLM"], "d": "models", "x": "Algorithms that identify statistical patterns embedded in AI-generated text or images during the generation process,...", "l": "w", "k": ["watermark", "detection", "algorithms", "identify", "statistical", "patterns", "embedded", "ai-generated", "text", "images", "generation", "process", "enabling", "attribution", "content"]}, {"id": "term-watson-ibm", "t": "Watson (IBM)", "tg": ["History", "Systems"], "d": "history", "x": "An AI system developed by IBM that defeated champions Brad Rutter and Ken Jennings on the game show Jeopardy! in 2011....", "l": "w", "k": ["watson", "ibm", "system", "developed", "defeated", "champions", "brad", "rutter", "ken", "jennings", "game", "show", "jeopardy", "natural", "language"]}, {"id": "term-wav2vec-20", "t": "Wav2Vec 2.0", "tg": ["Models", "Technical"], "d": "models", "x": "A self-supervised speech representation learning framework by Meta AI that learns from raw audio. Pretrained on...", "l": "w", "k": ["wav2vec", "self-supervised", "speech", "representation", "learning", "framework", "meta", "learns", "raw", "audio", "pretrained", "unlabeled", "data", "fine-tuned", "minimal"]}, {"id": "term-wavelet-transform", "t": "Wavelet Transform", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A transform that represents signals using wavelets which are localized oscillating functions. Unlike Fourier transforms...", "l": "w", "k": ["wavelet", "transform", "represents", "signals", "wavelets", "localized", "oscillating", "functions", "unlike", "fourier", "transforms", "provide", "time", "frequency", "information"]}, {"id": "term-wavenet", "t": "WaveNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A deep generative model for raw audio waveforms that uses dilated causal convolutions to capture long-range temporal...", "l": "w", "k": ["wavenet", "deep", "generative", "model", "raw", "audio", "waveforms", "uses", "dilated", "causal", "convolutions", "capture", "long-range", "temporal", "dependencies"]}, {"id": "term-wavlm", "t": "WavLM", "tg": ["Models", "Technical"], "d": "models", "x": "A large-scale self-supervised speech model that learns universal representations through masked speech prediction and...", "l": "w", "k": ["wavlm", "large-scale", "self-supervised", "speech", "model", "learns", "universal", "representations", "masked", "prediction", "denoising", "excels", "recognition", "non-asr", "tasks"]}, {"id": "term-waymo-history", "t": "Waymo History", "tg": ["History", "Milestones"], "d": "history", "x": "The evolution of Google's self-driving car project, started in 2009 by Sebastian Thrun, into Waymo as a subsidiary of...", "l": "w", "k": ["waymo", "history", "evolution", "google", "self-driving", "car", "project", "started", "sebastian", "thrun", "subsidiary", "alphabet", "becoming", "commercial", "autonomous"]}, {"id": "term-weak-supervision", "t": "Weak Supervision", "tg": ["Training", "Technique"], "d": "general", "x": "Training with noisy, imprecise, or automatically generated labels instead of perfect human annotations. Can...", "l": "w", "k": ["weak", "supervision", "training", "noisy", "imprecise", "automatically", "generated", "labels", "instead", "perfect", "human", "annotations", "dramatically", "reduce", "labeling"]}, {"id": "term-weaviate", "t": "Weaviate", "tg": ["Vector Database", "Open Source"], "d": "general", "x": "An open-source vector database that combines vector search with structured filtering and supports multiple...", "l": "w", "k": ["weaviate", "open-source", "vector", "database", "combines", "search", "structured", "filtering", "supports", "multiple", "vectorization", "modules", "offering", "hybrid", "capabilities"]}, {"id": "term-weibull-distribution", "t": "Weibull Distribution", "tg": ["Statistics", "Probability"], "d": "algorithms", "x": "A continuous probability distribution used in reliability analysis and survival modeling. Its shape parameter allows it...", "l": "w", "k": ["weibull", "distribution", "continuous", "probability", "reliability", "analysis", "survival", "modeling", "shape", "parameter", "allows", "model", "increasing", "constant", "decreasing"]}, {"id": "term-weight", "t": "Weight", "tg": ["Core Concept", "Neural Networks"], "d": "models", "x": "The numerical parameters in neural networks that are learned during training. Weights determine how inputs are...", "l": "w", "k": ["weight", "numerical", "parameters", "neural", "networks", "learned", "training", "weights", "determine", "inputs", "transformed", "outputs", "large", "models", "billions"]}, {"id": "term-weight-decay", "t": "Weight Decay", "tg": ["Machine Learning", "Optimization"], "d": "algorithms", "x": "A regularization technique that adds a fraction of the current weight values to the weight update rule during training,...", "l": "w", "k": ["weight", "decay", "regularization", "technique", "adds", "fraction", "current", "values", "update", "rule", "training", "effectively", "penalizing", "large", "weights"]}, {"id": "term-weight-initialization", "t": "Weight Initialization", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "The strategy for setting initial parameter values in neural networks, with methods like Xavier and He initialization...", "l": "w", "k": ["weight", "initialization", "strategy", "setting", "initial", "parameter", "values", "neural", "networks", "methods", "xavier", "designed", "maintain", "signal", "variance"]}, {"id": "term-weight-normalization", "t": "Weight Normalization", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "A reparameterization of weight vectors that decouples the magnitude and direction of weight vectors. Proposed by...", "l": "w", "k": ["weight", "normalization", "reparameterization", "vectors", "decouples", "magnitude", "direction", "proposed", "salimans", "kingma", "simpler", "alternative", "batch", "introduce", "dependencies"]}, {"id": "term-weight-sharing", "t": "Weight Sharing", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A compression technique where multiple connections in a neural network share the same weight value, reducing the number...", "l": "w", "k": ["weight", "sharing", "compression", "technique", "multiple", "connections", "neural", "network", "share", "value", "reducing", "number", "unique", "parameters", "must"]}, {"id": "term-weight-tying", "t": "Weight Tying", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A technique that shares the weight matrix between the input embedding layer and the output projection layer in language...", "l": "w", "k": ["weight", "tying", "technique", "shares", "matrix", "input", "embedding", "layer", "output", "projection", "language", "models", "reducing", "parameters", "improving"]}, {"id": "term-weight-only-quantization", "t": "Weight-Only Quantization", "tg": ["Model Optimization", "Inference Infrastructure"], "d": "models", "x": "A quantization strategy that compresses only the model weights to low precision while performing computation in higher...", "l": "w", "k": ["weight-only", "quantization", "strategy", "compresses", "model", "weights", "low", "precision", "performing", "computation", "higher", "dequantization", "reduces", "memory", "footprint"]}, {"id": "term-weights-and-biases", "t": "Weights and Biases", "tg": ["History", "Organizations"], "d": "history", "x": "A machine learning operations (MLOps) platform founded in 2017 that provides experiment tracking model management and...", "l": "w", "k": ["weights", "biases", "machine", "learning", "operations", "mlops", "platform", "founded", "provides", "experiment", "tracking", "model", "management", "dataset", "versioning"]}, {"id": "term-whisper", "t": "Whisper", "tg": ["Model", "Speech"], "d": "models", "x": "OpenAI's speech recognition model that transcribes audio to text with high accuracy across many languages....", "l": "w", "k": ["whisper", "openai", "speech", "recognition", "model", "transcribes", "audio", "text", "high", "accuracy", "across", "languages", "open-sourced", "enabling", "widespread"]}, {"id": "term-whisper-architecture", "t": "Whisper Architecture", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "An encoder-decoder transformer architecture trained on 680,000 hours of multilingual speech data for automatic speech...", "l": "w", "k": ["whisper", "architecture", "encoder-decoder", "transformer", "trained", "hours", "multilingual", "speech", "data", "automatic", "recognition", "log-mel", "spectrogram", "features", "input"]}, {"id": "term-white-noise", "t": "White Noise", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A time series of uncorrelated random variables with zero mean and constant variance. It represents purely random...", "l": "w", "k": ["white", "noise", "time", "series", "uncorrelated", "random", "variables", "zero", "mean", "constant", "variance", "represents", "purely", "variation", "exploitable"]}, {"id": "term-win-rate", "t": "Win Rate", "tg": ["Evaluation", "Ranking"], "d": "datasets", "x": "An evaluation metric that measures the percentage of pairwise comparisons in which one model's output is preferred over...", "l": "w", "k": ["win", "rate", "evaluation", "metric", "measures", "percentage", "pairwise", "comparisons", "model", "output", "preferred", "another", "human", "judges", "automated"]}, {"id": "term-windfall-clause", "t": "Windfall Clause", "tg": ["AI Ethics", "Governance"], "d": "safety", "x": "A proposed commitment by AI developers to share the economic benefits of transformative AI widely, ensuring that a...", "l": "w", "k": ["windfall", "clause", "proposed", "commitment", "developers", "share", "economic", "benefits", "transformative", "widely", "ensuring", "small", "number", "companies", "nations"]}, {"id": "term-window-attention", "t": "Window Attention", "tg": ["Architecture", "Efficiency"], "d": "models", "x": "A variant of attention that only looks at nearby tokens rather than the full context. Reduces computational cost for...", "l": "w", "k": ["window", "attention", "variant", "looks", "nearby", "tokens", "rather", "full", "context", "reduces", "computational", "cost", "long", "sequences", "models"]}, {"id": "term-window-based-chunking", "t": "Window-Based Chunking", "tg": ["Retrieval", "Preprocessing"], "d": "general", "x": "A document splitting method that uses a fixed-size sliding window measured in tokens or characters to create...", "l": "w", "k": ["window-based", "chunking", "document", "splitting", "method", "uses", "fixed-size", "sliding", "window", "measured", "tokens", "characters", "create", "overlapping", "chunks"]}, {"id": "term-windowed-attention", "t": "Windowed Attention", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "An attention mechanism that restricts each token to attend only to tokens within a fixed window around its position....", "l": "w", "k": ["windowed", "attention", "mechanism", "restricts", "token", "attend", "tokens", "within", "fixed", "window", "around", "position", "reduces", "computational", "complexity"]}, {"id": "term-winograd-schema", "t": "Winograd Schema", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A coreference resolution challenge requiring commonsense reasoning to determine what a pronoun refers to, designed as...", "l": "w", "k": ["winograd", "schema", "coreference", "resolution", "challenge", "requiring", "commonsense", "reasoning", "determine", "pronoun", "refers", "designed", "alternative", "turing", "test"]}, {"id": "term-winograd-schema-challenge", "t": "Winograd Schema Challenge", "tg": ["History", "Milestones"], "d": "history", "x": "A test of machine intelligence proposed by Hector Levesque in 2012 as an alternative to the Turing Test. It presents...", "l": "w", "k": ["winograd", "schema", "challenge", "test", "machine", "intelligence", "proposed", "hector", "levesque", "alternative", "turing", "presents", "sentences", "ambiguous", "pronouns"]}, {"id": "term-winogrande", "t": "WinoGrande", "tg": ["Evaluation", "Benchmarks"], "d": "datasets", "x": "A large-scale benchmark of Winograd schema-style coreference resolution problems that tests commonsense reasoning by...", "l": "w", "k": ["winogrande", "large-scale", "benchmark", "winograd", "schema-style", "coreference", "resolution", "problems", "tests", "commonsense", "reasoning", "requiring", "models", "identify", "correct"]}, {"id": "term-winsorization", "t": "Winsorization", "tg": ["Data Science", "Statistics"], "d": "algorithms", "x": "A technique for handling outliers by replacing extreme values with specified percentile values rather than removing...", "l": "w", "k": ["winsorization", "technique", "handling", "outliers", "replacing", "extreme", "values", "specified", "percentile", "rather", "removing", "example", "5th", "value"]}, {"id": "term-wizardlm", "t": "WizardLM", "tg": ["Models", "Technical"], "d": "models", "x": "A language model fine-tuned using Evol-Instruct a method that progressively creates complex instructions from simple...", "l": "w", "k": ["wizardlm", "language", "model", "fine-tuned", "evol-instruct", "method", "progressively", "creates", "complex", "instructions", "simple", "seed", "achieves", "strong", "instruction-following"]}, {"id": "term-wmt-translation", "t": "WMT Translation", "tg": ["History", "Milestones"], "d": "history", "x": "The Workshop on Machine Translation (formerly Workshop on Statistical Machine Translation) held annually since 2006....", "l": "w", "k": ["wmt", "translation", "workshop", "machine", "formerly", "statistical", "held", "annually", "provides", "standardized", "tasks", "evaluation", "campaigns", "driven", "progress"]}, {"id": "term-word-embedding", "t": "Word Embedding", "tg": ["Representation", "NLP"], "d": "general", "x": "Dense vector representations of words where similar words have similar vectors. Classic examples include Word2Vec and...", "l": "w", "k": ["word", "embedding", "dense", "vector", "representations", "words", "similar", "vectors", "classic", "examples", "include", "word2vec", "glove", "modern", "llms"]}, {"id": "term-word-error-rate", "t": "Word Error Rate", "tg": ["Evaluation", "Metrics"], "d": "datasets", "x": "A metric that measures the edit distance between predicted and reference transcriptions at the word level, calculated...", "l": "w", "k": ["word", "error", "rate", "metric", "measures", "edit", "distance", "predicted", "reference", "transcriptions", "level", "calculated", "number", "substitutions", "insertions"]}, {"id": "term-word-segmentation", "t": "Word Segmentation", "tg": ["NLP", "Text Processing"], "d": "general", "x": "The task of identifying word boundaries in languages that do not use whitespace to separate words, such as Chinese,...", "l": "w", "k": ["word", "segmentation", "task", "identifying", "boundaries", "languages", "whitespace", "separate", "words", "chinese", "japanese", "thai", "essential", "subsequent", "nlp"]}, {"id": "term-word-sense-disambiguation", "t": "Word Sense Disambiguation", "tg": ["NLP", "Linguistics"], "d": "general", "x": "The task of determining which meaning of a polysemous word is used in a given context, selecting from a predefined...", "l": "w", "k": ["word", "sense", "disambiguation", "task", "determining", "meaning", "polysemous", "given", "context", "selecting", "predefined", "inventory", "based", "surrounding", "words"]}, {"id": "term-word2vec", "t": "Word2Vec", "tg": ["Machine Learning", "Feature Engineering"], "d": "general", "x": "A family of neural network models (Skip-gram and CBOW) that learn dense vector representations of words from large text...", "l": "w", "k": ["word2vec", "family", "neural", "network", "models", "skip-gram", "cbow", "learn", "dense", "vector", "representations", "words", "large", "text", "corpora"]}, {"id": "term-word2vec-model", "t": "Word2Vec Model", "tg": ["Models", "Fundamentals"], "d": "models", "x": "A shallow neural network model that produces word embeddings by learning to predict context words given a target word...", "l": "w", "k": ["word2vec", "model", "shallow", "neural", "network", "produces", "word", "embeddings", "learning", "predict", "context", "words", "given", "target", "vice"]}, {"id": "term-wordnet", "t": "WordNet", "tg": ["NLP", "Linguistics"], "d": "general", "x": "A large lexical database of English where nouns, verbs, adjectives, and adverbs are grouped into cognitive synonym sets...", "l": "w", "k": ["wordnet", "large", "lexical", "database", "english", "nouns", "verbs", "adjectives", "adverbs", "grouped", "cognitive", "synonym", "sets", "synsets", "linked"]}, {"id": "term-wordpiece", "t": "WordPiece", "tg": ["NLP", "Tokenization"], "d": "general", "x": "A subword tokenization algorithm that greedily selects merges maximizing the likelihood of the training data, used by...", "l": "w", "k": ["wordpiece", "subword", "tokenization", "algorithm", "greedily", "selects", "merges", "maximizing", "likelihood", "training", "data", "bert", "related", "models", "splitting"]}, {"id": "term-world-model", "t": "World Model", "tg": ["Concept", "Research"], "d": "general", "x": "An AI's internal representation of how the world works. Used to simulate outcomes and plan actions. A key concept in AI...", "l": "w", "k": ["world", "model", "internal", "representation", "works", "simulate", "outcomes", "plan", "actions", "key", "concept", "safety", "discussions", "capabilities"]}, {"id": "term-world-models", "t": "World Models", "tg": ["Reinforcement Learning", "Planning"], "d": "general", "x": "Learned neural network representations of environment dynamics that allow an agent to simulate and plan in a latent...", "l": "w", "k": ["world", "models", "learned", "neural", "network", "representations", "environment", "dynamics", "allow", "agent", "simulate", "plan", "latent", "space", "compress"]}, {"id": "term-x-risk", "t": "X-Risk", "tg": ["AI Safety", "AI Ethics"], "d": "safety", "x": "Shorthand for existential risk, referring to catastrophic scenarios that could result in human extinction or permanent...", "l": "x", "k": ["x-risk", "shorthand", "existential", "risk", "referring", "catastrophic", "scenarios", "result", "human", "extinction", "permanent", "civilizational", "collapse", "considered", "several"]}, {"id": "term-xai-company", "t": "xAI", "tg": ["Company", "LLM Provider"], "d": "models", "x": "Elon Musk's AI company, creator of the Grok chatbot. Founded in 2023, it aims to develop AI that can understand the...", "l": "x", "k": ["xai", "elon", "musk", "company", "creator", "grok", "chatbot", "founded", "aims", "develop", "understand", "universe", "access", "real-time", "twitter"]}, {"id": "term-xai", "t": "XAI (Explainable AI)", "tg": ["Field", "Transparency"], "d": "safety", "x": "The field focused on making AI decisions understandable to humans. Includes techniques for visualizing attention,...", "l": "x", "k": ["xai", "explainable", "field", "focused", "making", "decisions", "understandable", "humans", "includes", "techniques", "visualizing", "attention", "attributing", "outputs", "inputs"]}, {"id": "term-xavier-initialization", "t": "Xavier Initialization", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A weight initialization method that samples weights from a distribution scaled by the number of input and output...", "l": "x", "k": ["xavier", "initialization", "weight", "method", "samples", "weights", "distribution", "scaled", "number", "input", "output", "neurons", "designed", "maintain", "activation"]}, {"id": "term-xgboost", "t": "XGBoost", "tg": ["Machine Learning", "Model Selection"], "d": "models", "x": "An optimized implementation of gradient boosting that uses regularized objectives, column subsampling, and efficient...", "l": "x", "k": ["xgboost", "optimized", "implementation", "gradient", "boosting", "uses", "regularized", "objectives", "column", "subsampling", "efficient", "tree", "construction", "algorithms", "includes"]}, {"id": "term-xla-compiler", "t": "XLA Compiler", "tg": ["Inference Infrastructure", "Model Optimization"], "d": "models", "x": "Accelerated Linear Algebra, a domain-specific compiler for machine learning that optimizes computation graphs through...", "l": "x", "k": ["xla", "compiler", "accelerated", "linear", "algebra", "domain-specific", "machine", "learning", "optimizes", "computation", "graphs", "operator", "fusion", "memory", "layout"]}, {"id": "term-xlnet", "t": "XLNet", "tg": ["Neural Networks", "Architecture"], "d": "models", "x": "A generalized autoregressive pretraining method that uses permutation-based training to capture bidirectional context...", "l": "x", "k": ["xlnet", "generalized", "autoregressive", "pretraining", "method", "uses", "permutation-based", "training", "capture", "bidirectional", "context", "maintaining", "formulation", "overcoming", "limitations"]}, {"id": "term-xml-prompting", "t": "XML Prompting", "tg": ["Prompt Engineering", "Output Format"], "d": "hardware", "x": "A prompting technique that uses XML tags to structure prompt sections, delimit input data, and specify output format,...", "l": "x", "k": ["xml", "prompting", "technique", "uses", "tags", "structure", "prompt", "sections", "delimit", "input", "data", "specify", "output", "format", "leveraging"]}, {"id": "term-xml-tags", "t": "XML Tags (in Prompting)", "tg": ["Prompting", "Technique"], "d": "general", "x": "Using XML-style markup tags to structure prompts and clearly delineate sections such as context, examples, or...", "l": "x", "k": ["xml", "tags", "prompting", "xml-style", "markup", "structure", "prompts", "clearly", "delineate", "sections", "context", "examples", "instructions", "helps", "models"]}, {"id": "term-yann-lecun", "t": "Yann LeCun", "tg": ["History", "Pioneers"], "d": "history", "x": "French-American computer scientist who developed convolutional neural networks in the late 1980s and applied them to...", "l": "y", "k": ["yann", "lecun", "french-american", "computer", "scientist", "developed", "convolutional", "neural", "networks", "late", "1980s", "applied", "handwritten", "digit", "recognition"]}, {"id": "term-yarn", "t": "YaRN", "tg": ["Algorithms", "Technical"], "d": "algorithms", "x": "Yet another RoPE extensioN method combines NTK-aware interpolation with attention scaling for robust context length...", "l": "y", "k": ["yarn", "another", "rope", "extension", "method", "combines", "ntk-aware", "interpolation", "attention", "scaling", "robust", "context", "length", "achieves", "state-of-the-art"]}, {"id": "term-yeo-johnson-transformation", "t": "Yeo-Johnson Transformation", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A power transformation similar to Box-Cox that can handle both positive and negative values. It extends the Box-Cox...", "l": "y", "k": ["yeo-johnson", "transformation", "power", "similar", "box-cox", "handle", "positive", "negative", "values", "extends", "defining", "appropriate", "transformations", "non-positive", "data"]}, {"id": "term-yi", "t": "Yi", "tg": ["Model", "Open Source"], "d": "models", "x": "A series of open-source bilingual (Chinese/English) LLMs from 01.AI. Known for strong performance across benchmarks and...", "l": "y", "k": ["series", "open-source", "bilingual", "chinese", "english", "llms", "known", "strong", "performance", "across", "benchmarks", "contributing", "development", "asia"]}, {"id": "term-yolo", "t": "YOLO (You Only Look Once)", "tg": ["Architecture", "Computer Vision"], "d": "models", "x": "A real-time object detection algorithm that processes images in a single pass. Revolutionary for its speed, enabling...", "l": "y", "k": ["yolo", "look", "real-time", "object", "detection", "algorithm", "processes", "images", "single", "pass", "revolutionary", "speed", "enabling", "video", "standard"]}, {"id": "term-yolov5", "t": "YOLOv5", "tg": ["Models", "Technical"], "d": "models", "x": "A popular implementation of the YOLO object detection family released by Ultralytics in PyTorch. Known for ease of use...", "l": "y", "k": ["yolov5", "popular", "implementation", "yolo", "object", "detection", "family", "released", "ultralytics", "pytorch", "known", "ease", "fast", "training", "strong"]}, {"id": "term-yolov8", "t": "YOLOv8", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "A state-of-the-art real-time object detection model in the YOLO family that introduces an anchor-free detection head,...", "l": "y", "k": ["yolov8", "state-of-the-art", "real-time", "object", "detection", "model", "yolo", "family", "introduces", "anchor-free", "head", "decoupled", "classification", "regression", "branches"]}, {"id": "term-yoshua-bengio", "t": "Yoshua Bengio", "tg": ["History", "Pioneers"], "d": "history", "x": "Canadian computer scientist who made foundational contributions to deep learning, including neural language models and...", "l": "y", "k": ["yoshua", "bengio", "canadian", "computer", "scientist", "foundational", "contributions", "deep", "learning", "including", "neural", "language", "models", "generative", "adversarial"]}, {"id": "term-yoshua-bengio-universal-approximation", "t": "Yoshua Bengio Universal Approximation", "tg": ["History", "Milestones"], "d": "history", "x": "The 1989 work by George Cybenko and subsequent contributions by Kurt Hornik and others proving that neural networks...", "l": "y", "k": ["yoshua", "bengio", "universal", "approximation", "work", "george", "cybenko", "subsequent", "contributions", "kurt", "hornik", "others", "proving", "neural", "networks"]}, {"id": "term-z-score", "t": "Z-Score", "tg": ["Statistics", "Data Science"], "d": "algorithms", "x": "A standardized score indicating how many standard deviations a data point is from the mean of its distribution. It...", "l": "z", "k": ["z-score", "standardized", "score", "indicating", "standard", "deviations", "data", "point", "mean", "distribution", "allows", "comparison", "values", "different", "distributions"]}, {"id": "term-zephyr", "t": "Zephyr", "tg": ["Model", "Open Source"], "d": "models", "x": "A series of fine-tuned open LLMs from Hugging Face optimized for helpful assistants. Demonstrates how smaller models...", "l": "z", "k": ["zephyr", "series", "fine-tuned", "open", "llms", "hugging", "face", "optimized", "helpful", "assistants", "demonstrates", "smaller", "models", "good", "alignment"]}, {"id": "term-zero-optimization", "t": "ZeRO Optimization", "tg": ["LLM", "Generative AI"], "d": "models", "x": "Zero Redundancy Optimizer, a distributed training technique that partitions optimizer states, gradients, and parameters...", "l": "z", "k": ["zero", "optimization", "redundancy", "optimizer", "distributed", "training", "technique", "partitions", "states", "gradients", "parameters", "across", "data-parallel", "processes", "reduce"]}, {"id": "term-zero-day", "t": "Zero-Day (AI Context)", "tg": ["Security", "Safety"], "d": "safety", "x": "Novel vulnerabilities or attack vectors discovered in AI systems before developers know about them. AI security...", "l": "z", "k": ["zero-day", "context", "novel", "vulnerabilities", "attack", "vectors", "discovered", "systems", "developers", "know", "security", "research", "increasingly", "focuses", "finding"]}, {"id": "term-zero-shot-cot", "t": "Zero-Shot Chain-of-Thought", "tg": ["Prompting", "Reasoning"], "d": "general", "x": "Adding \"Let's think step by step\" to a prompt to trigger reasoning without providing examples. A simple but effective...", "l": "z", "k": ["zero-shot", "chain-of-thought", "adding", "let", "think", "step", "prompt", "trigger", "reasoning", "without", "providing", "examples", "simple", "effective", "technique"]}, {"id": "term-zero-shot", "t": "Zero-Shot Learning", "tg": ["Prompting", "Technique"], "d": "general", "x": "When AI performs a task without examples in the prompt, relying entirely on pre-trained knowledge and instructions....", "l": "z", "k": ["zero-shot", "learning", "performs", "task", "without", "examples", "prompt", "relying", "entirely", "pre-trained", "knowledge", "instructions", "contrasts", "few-shot", "provided"]}, {"id": "term-zero-shot-learning-history", "t": "Zero-Shot Learning History", "tg": ["History", "Milestones"], "d": "history", "x": "The development of zero-shot learning from early attribute-based approaches to modern large language models that...", "l": "z", "k": ["zero-shot", "learning", "history", "development", "early", "attribute-based", "approaches", "modern", "large", "language", "models", "perform", "tasks", "without", "task-specific"]}, {"id": "term-zero-shot-ner", "t": "Zero-Shot NER", "tg": ["NLP", "Text Processing"], "d": "general", "x": "Named entity recognition performed on entity types not seen during training, using natural language descriptions of...", "l": "z", "k": ["zero-shot", "ner", "named", "entity", "recognition", "performed", "types", "seen", "training", "natural", "language", "descriptions", "categories", "generalize", "without"]}, {"id": "term-zero-shot-object-detection", "t": "Zero-Shot Object Detection", "tg": ["Computer Vision", "Object Detection"], "d": "hardware", "x": "The ability to detect and localize objects of novel categories without any training examples, using vision-language...", "l": "z", "k": ["zero-shot", "object", "detection", "ability", "detect", "localize", "objects", "novel", "categories", "without", "training", "examples", "vision-language", "alignment", "match"]}]