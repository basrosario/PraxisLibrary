{
  "letter": "n",
  "count": 69,
  "terms": [
    {
      "id": "term-n-gram",
      "term": "N-gram",
      "definition": "A contiguous sequence of N items from a text, where items can be characters, words, or tokens, used in language models, text classification, and information retrieval.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-n-step-return",
      "term": "N-Step Return",
      "definition": "A return estimate that uses n actual rewards before bootstrapping with a value estimate, interpolating between one-step TD (n=1) and full Monte Carlo (n=infinity). N-step returns offer a bias-variance tradeoff controlled by the step parameter.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-nadam",
      "term": "Nadam",
      "definition": "Nesterov-accelerated Adaptive Moment estimation combines the Adam optimizer with Nesterov momentum. Provides faster convergence by incorporating the look-ahead gradient computation from Nesterov accelerated gradient into Adam.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-naive-bayes",
      "term": "Naive Bayes",
      "definition": "A family of probabilistic classifiers based on Bayes' theorem with the strong assumption that features are conditionally independent given the class label. Despite this simplification, it often performs well on text classification tasks.",
      "tags": [
        "Machine Learning",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-named-entity-linking",
      "term": "Named Entity Linking",
      "definition": "The task of mapping recognized named entity mentions in text to their corresponding entries in a knowledge base, resolving ambiguity when multiple entities share the same name.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-named-entity-recognition",
      "term": "Named Entity Recognition (NER)",
      "definition": "An NLP task that identifies and classifies named entities (people, organizations, locations, dates) in text. Foundational for information extraction and knowledge graph construction.",
      "tags": [
        "NLP Task",
        "Extraction"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-namespace",
      "term": "Namespace",
      "definition": "A logical partitioning mechanism within a vector database index that isolates vectors into separate searchable segments, enabling multi-tenant applications and scoped queries without maintaining separate physical indexes.",
      "tags": [
        "Vector Database",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-narrow-ai",
      "term": "Narrow AI (ANI)",
      "definition": "AI systems designed for specific tasks, like playing chess or generating text. All current AI is narrow, as opposed to hypothetical artificial general intelligence (AGI).",
      "tags": [
        "Category",
        "Concept"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-narrow-ai-safety",
      "term": "Narrow AI Safety",
      "definition": "Safety research focused on currently deployed AI systems, addressing issues such as robustness to distribution shift, adversarial inputs, reward misspecification, and safe exploration in constrained environments.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-natural-language",
      "term": "Natural Language",
      "definition": "Human language as we naturally speak and write it. AI assistants are designed to understand natural language, so you don't need special syntax or formatting.",
      "tags": [
        "Concept",
        "Interface"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-natural-language-inference",
      "term": "Natural Language Inference",
      "definition": "The task of classifying the logical relationship between a premise and hypothesis text pair into entailment, contradiction, or neutral, testing a model's ability to reason about meaning.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-nlp",
      "term": "Natural Language Processing (NLP)",
      "definition": "The field of AI focused on enabling computers to understand, interpret, and generate human language. Encompasses tasks from translation to summarization to dialogue.",
      "tags": [
        "Field",
        "Language"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-nlp-history",
      "term": "Natural Language Processing History",
      "definition": "The evolution of NLP from rule-based approaches in the 1960s through statistical methods in the 1990s to neural approaches in the 2010s and the transformer revolution, culminating in modern large language models.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-natural-policy-gradient",
      "term": "Natural Policy Gradient",
      "definition": "A policy gradient method that preconditions updates with the inverse Fisher information matrix, following the steepest ascent direction in the space of policy distributions rather than parameter space. Natural gradients provide more efficient optimization.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-natural-questions",
      "term": "Natural Questions",
      "definition": "A question answering benchmark by Google consisting of real user queries from Google Search paired with Wikipedia articles, requiring models to identify both short answers and long answer passages to satisfy genuine information needs.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-nccl",
      "term": "NCCL",
      "definition": "NVIDIA Collective Communications Library, a highly optimized library for multi-GPU and multi-node collective communication operations. NCCL automatically selects the best communication algorithms and topologies for the available hardware interconnects.",
      "tags": [
        "Distributed Computing",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-ndcg",
      "term": "NDCG",
      "definition": "Normalized Discounted Cumulative Gain, a ranking quality metric that evaluates the usefulness of retrieved items based on their position in the result list, assigning higher weights to relevant items appearing earlier and normalizing against the ideal ranking.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-needle-in-haystack-test",
      "term": "Needle in a Haystack Test",
      "definition": "An evaluation method that measures a language model's ability to retrieve a specific piece of information placed at various positions within a long context, revealing attention degradation patterns.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-negation-detection",
      "term": "Negation Detection",
      "definition": "The task of identifying negation cues and their scope in text, determining which parts of a sentence are affected by negation words like 'not,' 'never,' or 'without.'",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-negative-prompt",
      "term": "Negative Prompt",
      "definition": "Instructions telling AI what to avoid in its output. Common in image generation (\"no blur, no distortion\") and can be used in text to exclude certain topics or styles.",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-negative-prompting",
      "term": "Negative Prompting",
      "definition": "A technique that explicitly specifies what the model should avoid in its output, including unwanted content, styles, formats, or behaviors, using exclusion instructions to constrain the generation space toward desired outputs.",
      "tags": [
        "Prompt Engineering",
        "Constraints"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-negative-sampling",
      "term": "Negative Sampling",
      "definition": "A training approximation that replaces the full softmax over the vocabulary with a binary classification between true context words and randomly sampled negative examples, making embedding training tractable.",
      "tags": [
        "NLP",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-nerf",
      "term": "NeRF",
      "definition": "Neural Radiance Field, a method that represents 3D scenes as continuous volumetric functions parameterized by neural networks, enabling photorealistic novel view synthesis from sparse input images.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-nested-cross-validation",
      "term": "Nested Cross-Validation",
      "definition": "A model evaluation technique using an inner cross-validation loop for hyperparameter tuning and an outer loop for unbiased performance estimation, preventing information leakage from the tuning process.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-nested-ner",
      "term": "Nested Named Entity Recognition",
      "definition": "A NER variant that handles entities embedded within other entities, such as recognizing both 'Bank of America' as an organization and 'America' as a location within the same span.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-nesterov-accelerated-gradient",
      "term": "Nesterov Accelerated Gradient",
      "definition": "A variant of momentum-based optimization that computes the gradient at a lookahead position rather than the current position, providing better convergence properties by correcting the momentum direction before taking a step.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-netflix-prize",
      "term": "Netflix Prize",
      "definition": "A 2006-2009 open competition offering one million dollars for the best collaborative filtering algorithm to predict user movie ratings, which accelerated recommender systems research and popularized machine learning competitions.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-neural-architecture-search",
      "term": "Neural Architecture Search",
      "definition": "An automated method for designing neural network architectures by searching over a defined search space. Methods include reinforcement learning evolutionary algorithms and differentiable approaches. Discovered architectures like EfficientNet and NASNet.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-nas-vision",
      "term": "Neural Architecture Search for Vision",
      "definition": "Automated methods for discovering optimal CNN or ViT architectures by searching over design choices (kernel sizes, channel widths, layer connections) using reinforcement learning or evolutionary algorithms.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-hw-aware-nas",
      "term": "Neural Architecture Search Hardware-Aware",
      "definition": "NAS methods that incorporate hardware constraints (latency, memory, power) into the search objective, finding architectures optimized for specific target devices. Hardware-aware NAS produces models that achieve optimal accuracy-efficiency tradeoffs on deployment hardware.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-neural-machine-translation",
      "term": "Neural Machine Translation",
      "definition": "A machine translation approach using encoder-decoder neural networks that learn to map source language sequences to target language sequences end-to-end from parallel corpora.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-neural-network",
      "term": "Neural Network",
      "definition": "A computing system inspired by biological brains, composed of interconnected nodes (neurons) organized in layers. The foundation of modern deep learning and AI.",
      "tags": [
        "Architecture",
        "Fundamentals"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-neural-ode",
      "term": "Neural ODE",
      "definition": "A continuous-depth neural network that parameterizes the derivative of hidden states as a neural network and uses ODE solvers for forward and backward passes, enabling adaptive computation and continuous dynamics.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-npu",
      "term": "Neural Processing Unit (NPU)",
      "definition": "A dedicated hardware accelerator designed specifically for neural network inference, typically integrated into SoCs for on-device AI. NPUs optimize matrix operations and activation functions with minimal power consumption for edge deployment.",
      "tags": [
        "Hardware",
        "Inference Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-neural-style-transfer",
      "term": "Neural Style Transfer",
      "definition": "A technique that applies the artistic style of one image to the content of another by optimizing a generated image to match content features from one source and style features (Gram matrices) from another.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-neural-turing-machine",
      "term": "Neural Turing Machine",
      "definition": "A neural architecture augmented with external memory that the network can read from and write to via differentiable attention mechanisms, enabling learning of algorithmic procedures.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-newtons-method",
      "term": "Newton's Method",
      "definition": "A second-order optimization algorithm that uses the Hessian matrix to find the minimum of a function. Converges quadratically near the optimum but requires computing and inverting the full Hessian which is expensive for high-dimensional problems.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-next-token-prediction",
      "term": "Next Token Prediction",
      "definition": "The core training objective of autoregressive LLMs: predict the next token given all previous tokens. This simple objective, at scale, produces sophisticated language understanding.",
      "tags": [
        "Training",
        "LLM"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-nf4",
      "term": "NF4 (Normal Float 4-bit)",
      "definition": "A 4-bit quantization format based on the assumption that neural network weights follow a normal distribution, using quantile quantization for optimal information-theoretic representation. NF4 is used in QLoRA for memory-efficient fine-tuning.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-nils-nilsson",
      "term": "Nils Nilsson",
      "definition": "American computer scientist (1933-2019) who co-invented the A* search algorithm and developed foundational work in robotics and knowledge representation at SRI International, later directing the Stanford AI Lab.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-nist-ai-rmf",
      "term": "NIST AI Risk Management Framework",
      "definition": "A voluntary framework published by the US National Institute of Standards and Technology in 2023 that provides guidance for managing AI risks through governance, mapping, measuring, and managing functions.",
      "tags": [
        "Governance",
        "Regulation"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-nli-based-evaluation",
      "term": "NLI-Based Evaluation",
      "definition": "An evaluation approach that uses Natural Language Inference models to assess text quality by classifying whether generated claims are entailed by, contradicted by, or neutral with respect to reference text or source documents.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-no-free-lunch-theorem",
      "term": "No Free Lunch Theorem",
      "definition": "A set of theoretical results stating that no single learning algorithm performs best across all possible problems. When averaged over all possible data distributions, all algorithms perform equally.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-node2vec",
      "term": "Node2Vec",
      "definition": "A graph embedding algorithm that learns continuous feature representations for nodes by optimizing a neighborhood-preserving objective. Uses biased random walks with parameters controlling the balance between BFS-like and DFS-like exploration.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-noise",
      "term": "Noise (ML)",
      "definition": "Random variation in data or model outputs. In training, noise can cause or hide patterns. In diffusion models, controlled noise addition and removal is how images are generated.",
      "tags": [
        "Concept",
        "Data"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-noise-schedule",
      "term": "Noise Schedule",
      "definition": "The predefined or learned sequence of noise levels in diffusion models that determines how quickly noise is added during the forward process and removed during the reverse generation process.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-noisy-networks",
      "term": "Noisy Networks",
      "definition": "A DQN extension that replaces epsilon-greedy exploration with parametric noise added to network weights, allowing the agent to learn the optimal level of exploration. Noisy networks achieve state-dependent exploration that adapts during training.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-non-local-neural-network",
      "term": "Non-Local Neural Network",
      "definition": "A neural network module that computes the response at a position as a weighted sum of features at all positions, capturing long-range dependencies in images and video beyond local receptive fields.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-non-maximum-suppression",
      "term": "Non-Maximum Suppression",
      "definition": "A post-processing algorithm in object detection that eliminates redundant overlapping bounding box predictions by keeping only the highest-confidence detection for each object instance.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-non-negative-matrix-factorization",
      "term": "Non-Negative Matrix Factorization",
      "definition": "A matrix decomposition technique that factors a non-negative matrix into two non-negative matrices, producing parts-based representations. It is useful for topic modeling, image analysis, and signal processing.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-norbert-wiener",
      "term": "Norbert Wiener",
      "definition": "American mathematician (1894-1964) who founded cybernetics in his 1948 book of the same name, establishing the study of feedback, control, and communication in machines and living organisms as a precursor to AI.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-normal-distribution",
      "term": "Normal Distribution",
      "definition": "A continuous probability distribution characterized by its bell-shaped curve, symmetric about the mean, fully determined by its mean and standard deviation. Many natural phenomena and statistical methods assume normality.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-normality-test",
      "term": "Normality Test",
      "definition": "A statistical test that evaluates whether a dataset follows a normal distribution. Common methods include the Shapiro-Wilk test, Kolmogorov-Smirnov test, and Anderson-Darling test.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-normalization",
      "term": "Normalization",
      "definition": "Techniques to standardize inputs or layer outputs in neural networks. Layer normalization is critical in transformers for stable training and better generalization.",
      "tags": [
        "Technique",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-normalized-mutual-information",
      "term": "Normalized Mutual Information",
      "definition": "A clustering evaluation metric that normalizes mutual information between predicted and ground truth clusterings to account for chance. Ranges from 0 to 1 and is useful for comparing clusterings with different numbers of clusters.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-normalizing-flow",
      "term": "Normalizing Flow",
      "definition": "A generative model that transforms a simple base distribution into a complex target distribution through a sequence of invertible and differentiable transformations with tractable Jacobian determinants.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-normalizing-flows",
      "term": "Normalizing Flows",
      "definition": "A class of generative models that transform a simple base distribution into a complex target distribution through a sequence of invertible and differentiable mappings. Allows exact likelihood computation and efficient sampling.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-novel-view-synthesis",
      "term": "Novel View Synthesis",
      "definition": "The task of generating photorealistic images of a scene from viewpoints not present in the input photographs, using techniques like NeRF, Gaussian splatting, or light field interpolation.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-nt-xent-loss",
      "term": "NT-Xent Loss",
      "definition": "Normalized Temperature-scaled Cross-Entropy loss used in SimCLR for self-supervised contrastive learning. Normalizes embeddings and scales by a temperature parameter before computing cross-entropy over positive and negative pairs within a mini-batch.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-ntk-aware-scaling",
      "term": "NTK-Aware Scaling",
      "definition": "A position encoding extension method based on Neural Tangent Kernel theory that modifies the frequency basis of rotary embeddings. Enables context length extension without fine-tuning by adjusting the base frequency of the rotation.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-nucleus-sampling",
      "term": "Nucleus Sampling (Top-p)",
      "definition": "A text generation strategy that samples from the smallest set of tokens whose cumulative probability exceeds p. Balances diversity and quality better than pure random sampling.",
      "tags": [
        "Generation",
        "Parameter"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-null-hypothesis",
      "term": "Null Hypothesis",
      "definition": "A default assumption in statistical hypothesis testing that there is no effect or no difference between groups. Statistical tests evaluate whether observed data provide sufficient evidence to reject this assumption.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-numerical-differentiation",
      "term": "Numerical Differentiation",
      "definition": "Approximating derivatives using finite differences. Simple to implement but subject to numerical errors from truncation and rounding. Used as a verification tool for gradient implementations but too inaccurate for optimization.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-a100",
      "term": "NVIDIA A100",
      "definition": "NVIDIA's third-generation Tensor Core GPU based on the Ampere architecture, featuring 80GB HBM2e memory, support for TF32 and structural sparsity, and multi-instance GPU (MIG) capability. The A100 was the dominant GPU for AI training and inference from 2020-2022.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-b200",
      "term": "NVIDIA B200",
      "definition": "NVIDIA's Blackwell architecture GPU designed for next-generation AI training and inference, featuring second-generation Transformer Engine with FP4 support and significantly increased memory bandwidth. The B200 targets trillion-parameter model training.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-nvidia-grace",
      "term": "NVIDIA Grace CPU",
      "definition": "NVIDIA's ARM-based data center CPU designed for AI and HPC workloads, featuring high memory bandwidth via LPDDR5X and direct NVLink connectivity to NVIDIA GPUs. Grace eliminates PCIe bottlenecks in CPU-GPU communication for AI training.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-h100",
      "term": "NVIDIA H100",
      "definition": "NVIDIA's fourth-generation Tensor Core GPU based on the Hopper architecture, featuring the Transformer Engine with FP8 precision, 80GB HBM3 memory, and fourth-generation NVLink. The H100 delivers roughly 3x the AI training performance of the A100.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-nvlink",
      "term": "NVLink",
      "definition": "NVIDIA's proprietary high-bandwidth, low-latency interconnect for direct GPU-to-GPU communication, bypassing the PCIe bus. NVLink 4.0 (Hopper) provides 900 GB/s total bandwidth per GPU, enabling efficient multi-GPU training and inference.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-nvswitch",
      "term": "NVSwitch",
      "definition": "NVIDIA's fully connected switch fabric that enables all-to-all GPU communication within a node at full NVLink bandwidth. NVSwitch creates a unified memory space across multiple GPUs, critical for large model training requiring high inter-GPU bandwidth.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    }
  ]
}