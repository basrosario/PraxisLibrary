{
  "letter": "s",
  "count": 232,
  "terms": [
    {
      "id": "term-s4",
      "term": "S4",
      "definition": "Structured State Spaces for Sequence Modeling, a state space model that uses a special initialization based on the HiPPO framework and efficient computation via FFT to handle very long-range dependencies.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-safe-rl",
      "term": "Safe Reinforcement Learning",
      "definition": "RL methods that incorporate safety constraints to prevent the agent from taking dangerous or unacceptable actions during training and deployment. Safe RL uses constrained optimization, shielding, or risk-sensitive objectives.",
      "tags": [
        "Reinforcement Learning",
        "Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-safety-filter",
      "term": "Safety Filter",
      "definition": "Systems that detect and block harmful content in AI inputs or outputs. Part of the safety stack protecting users from inappropriate, dangerous, or illegal content.",
      "tags": [
        "Safety",
        "Security"
      ],
      "domain": "safety",
      "link": "ai-safety.html",
      "related": []
    },
    {
      "id": "term-safety-score",
      "term": "Safety Score",
      "definition": "A composite evaluation metric that aggregates measurements of harmful output generation including toxicity, bias, dangerous advice, and policy violations, providing an overall assessment of a model's safety alignment.",
      "tags": [
        "Evaluation",
        "Safety"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-saliency-map",
      "term": "Saliency Map",
      "definition": "A visualization technique that highlights which parts of an input are most relevant to a model's prediction by computing gradients of the output with respect to the input. Simple to compute but can be noisy and difficult to interpret.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-sam-altman",
      "term": "Sam Altman",
      "definition": "American entrepreneur who became CEO of OpenAI, overseeing the development and launch of ChatGPT and GPT-4. His brief firing and reinstatement in November 2023 highlighted governance tensions at leading AI companies.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-sambanova",
      "term": "SambaNova",
      "definition": "An AI hardware company producing reconfigurable dataflow architecture processors (SN series) that adapt their compute topology to different model architectures. SambaNova's dataflow approach streams data through the chip without traditional memory hierarchy bottlenecks.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-sample-complexity-rl",
      "term": "Sample Complexity",
      "definition": "The number of environment interactions needed for an RL algorithm to find a near-optimal policy with high probability. Sample complexity is a key measure of algorithmic efficiency, especially when interactions are expensive.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sampling",
      "term": "Sampling",
      "definition": "The process of selecting the next token during text generation. Methods include greedy (always pick highest probability), top-k, top-p (nucleus), and temperature-based sampling.",
      "tags": [
        "Generation",
        "Technical"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sandwich-prompting",
      "term": "Sandwich Prompting",
      "definition": "A prompt structure that places the core instruction both before and after the main context or input data, reinforcing adherence to instructions that might otherwise be lost in long contexts due to positional attention biases.",
      "tags": [
        "Prompt Engineering",
        "Structure"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sarcasm-detection",
      "term": "Sarcasm Detection",
      "definition": "The task of identifying sarcastic or ironic statements in text where the intended meaning differs from the literal meaning, a challenging problem requiring pragmatic understanding.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sarima",
      "term": "SARIMA",
      "definition": "Seasonal ARIMA, an extension of the ARIMA model that includes additional seasonal autoregressive, differencing, and moving average terms to capture periodic patterns in time series data.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-sarsa",
      "term": "SARSA",
      "definition": "An on-policy temporal difference control algorithm that updates Q-values using the actual action taken by the current policy (State-Action-Reward-State-Action). Unlike Q-learning, SARSA learns the value of the policy being followed.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-satellite-imagery-analysis",
      "term": "Satellite Imagery Analysis",
      "definition": "The use of computer vision models to interpret aerial and satellite photographs for applications including land use classification, change detection, disaster assessment, and environmental monitoring.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-scalable-oversight",
      "term": "Scalable Oversight",
      "definition": "The challenge and research agenda of maintaining meaningful human oversight of AI systems as they become more capable and handle tasks too complex for humans to directly evaluate.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-scalar-quantization",
      "term": "Scalar Quantization",
      "definition": "A vector compression method that reduces memory usage by converting each floating-point vector component to a lower-precision representation such as 8-bit integers, achieving 4x compression from float32 with minimal accuracy loss.",
      "tags": [
        "Vector Database",
        "Quantization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-scale-ai",
      "term": "Scale AI",
      "definition": "A company specializing in data labeling and annotation services for AI training. Provides human feedback at scale, crucial for training and evaluating foundation models.",
      "tags": [
        "Company",
        "Data"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-scaling-hypothesis",
      "term": "Scaling Hypothesis",
      "definition": "The hypothesis that increasing model size, training data, and compute leads to predictable and substantial improvements in AI capability, supported by empirical scaling laws discovered by Kaplan et al. at OpenAI.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-scaling-law",
      "term": "Scaling Law",
      "definition": "Empirical power-law relationships that predict how model performance improves as a function of compute budget, model size, and dataset size, guiding efficient resource allocation for training.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-scaling-laws",
      "term": "Scaling Laws",
      "definition": "Empirical relationships showing how model performance improves with more parameters, data, and compute. Guide decisions about where to invest resources in training larger models.",
      "tags": [
        "Research",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-scaling-laws-compute",
      "term": "Scaling Laws for Compute",
      "definition": "Empirical power-law relationships between model performance and compute budget, model size, and dataset size established by Kaplan et al. and Hoffmann et al. (Chinchilla). Scaling laws guide optimal allocation of training compute.",
      "tags": [
        "Model Optimization",
        "Distributed Computing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-scene-graph-generation",
      "term": "Scene Graph Generation",
      "definition": "The task of detecting objects in an image and predicting their pairwise relationships to construct a graph representation where nodes are objects and edges are visual relationships.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-scene-text-recognition",
      "term": "Scene Text Recognition",
      "definition": "The task of reading and transcribing text found in natural images (street signs, product labels, building facades), dealing with perspective distortion, partial occlusion, and diverse typography.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-scheduled-sampling",
      "term": "Scheduled Sampling",
      "definition": "A training technique that gradually transitions from teacher forcing to using the model's own predictions as inputs during training. Reduces the exposure bias gap between training and inference. The curriculum linearly interpolates between the two modes.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-scikit-learn",
      "term": "Scikit-learn",
      "definition": "A popular Python library for traditional machine learning algorithms. Provides simple APIs for classification, regression, clustering, and preprocessing. The go-to for non-deep-learning ML.",
      "tags": [
        "Framework",
        "ML"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-score-function",
      "term": "Score Function",
      "definition": "The gradient of the log-likelihood function with respect to the parameter, used in maximum likelihood estimation and Fisher information calculations. Its expected value under the true distribution is zero.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-score-matching",
      "term": "Score Matching",
      "definition": "A method for estimating probability density functions by matching the gradient of the log-density (score function) rather than the density itself. Avoids computing the normalizing constant. Foundation of score-based generative models.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-score-based-generative-model",
      "term": "Score-Based Generative Model",
      "definition": "A generative model that learns the gradient of the log probability density (score function) of the data distribution, then uses Langevin dynamics or similar methods to generate samples.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-sdxl",
      "term": "SDXL",
      "definition": "Stable Diffusion XL, an improved latent diffusion model that uses a larger UNet with dual text encoders and an optional refiner model to generate higher-resolution, more detailed images.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-seasonal-decomposition",
      "term": "Seasonal Decomposition",
      "definition": "A time series analysis technique that separates a time series into trend, seasonal, and residual components, either additively or multiplicatively, to better understand underlying patterns.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-second-ai-winter",
      "term": "Second AI Winter",
      "definition": "The period from approximately 1987 to 1993 when enthusiasm for AI again collapsed following the failure of expert systems to scale, the collapse of the LISP machine market, and cuts to government AI funding.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-secure-multi-party-computation",
      "term": "Secure Multi-Party Computation",
      "definition": "A cryptographic protocol that allows multiple parties to jointly compute a function over their combined inputs while keeping each party's input private, used in privacy-preserving machine learning applications.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-segment-anything-model",
      "term": "Segment Anything Model",
      "definition": "A foundation model for image segmentation (SAM) trained on a massive dataset that can segment any object in any image given a prompt such as a point, bounding box, or text description.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-selection-bias",
      "term": "Selection Bias",
      "definition": "A systematic error arising when the sample used for analysis is not representative of the population, due to the way observations were selected. It can lead to models that do not generalize to the true population.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-selectional-preference",
      "term": "Selectional Preference",
      "definition": "The tendency of predicates to semantically constrain their arguments, such as 'eat' preferring edible objects, used in NLP for disambiguation and semantic plausibility assessment.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-selective-context",
      "term": "Selective Context",
      "definition": "A prompt compression method that evaluates the informativeness of each lexical unit in a context using self-information scores from a causal language model, then filters out low-information content to reduce prompt length while retaining key details.",
      "tags": [
        "Prompt Engineering",
        "Compression"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-self-ask",
      "term": "Self-Ask",
      "definition": "A prompting technique where the model explicitly asks itself follow-up questions needed to answer a complex query, then answers each sub-question before synthesizing the final response, naturally decomposing multi-hop reasoning tasks.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-self-attention",
      "term": "Self-Attention",
      "definition": "An attention mechanism where a sequence attends to itself, allowing each position to consider all other positions. The core operation in transformer models.",
      "tags": [
        "Architecture",
        "Transformers"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-self-bleu",
      "term": "Self-BLEU",
      "definition": "A diversity metric that computes BLEU scores between pairs of generated sentences from the same model, where lower Self-BLEU indicates greater diversity and less repetition across the model's outputs.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-self-consistency",
      "term": "Self-Consistency",
      "definition": "A technique where AI generates multiple reasoning paths and selects the most common answer. Improves accuracy for complex reasoning by aggregating diverse approaches.",
      "tags": [
        "Prompting",
        "Reasoning"
      ],
      "domain": "general",
      "link": "../learn/index.html",
      "related": []
    },
    {
      "id": "term-self-driving-car-history",
      "term": "Self-Driving Car History",
      "definition": "The evolution of autonomous vehicles from early projects like Stanford Cart in 1961 and CMU's Navlab in 1986 through the DARPA Grand Challenges, Google's self-driving car project in 2009, and modern robotaxi services.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-self-play",
      "term": "Self-Play",
      "definition": "A training paradigm where an agent improves by playing against copies of itself, generating increasingly challenging opponents as its skill grows. Self-play creates an automatic curriculum and has driven breakthroughs in game-playing AI.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-self-polish-prompting",
      "term": "Self-Polish Prompting",
      "definition": "A technique that prompts the model to progressively refine and polish the given problem conditions before solving, enabling the model to simplify complex problems into more tractable formulations through iterative rewriting.",
      "tags": [
        "Prompt Engineering",
        "Refinement"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-self-rag",
      "term": "Self-RAG",
      "definition": "A framework where the language model learns to retrieve on demand, generate text, and critique its own output using special reflection tokens, adaptively deciding when retrieval is necessary.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-self-supervised-learning",
      "term": "Self-Supervised Learning",
      "definition": "A machine learning paradigm where the model generates its own supervisory signal from unlabeled data through pretext tasks. Examples include masked language modeling next-token prediction and contrastive learning. Foundation of modern pretrained models.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-self-training",
      "term": "Self-Training",
      "definition": "A semi-supervised learning method where a model trained on labeled data generates pseudo-labels for unlabeled data, then retrains on both real and pseudo-labeled examples, iteratively expanding the training set.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-self-training-vision",
      "term": "Self-Training for Vision",
      "definition": "A semi-supervised learning approach where a teacher model generates pseudo-labels for unlabeled images, and a student model is trained on the combination of labeled data and high-confidence pseudo-labels.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-selu",
      "term": "SELU",
      "definition": "Scaled Exponential Linear Unit that enables self-normalizing neural networks. Combines the ELU function with specific scale and alpha constants derived mathematically to ensure that activations converge toward zero mean and unit variance during training.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantic-caching",
      "term": "Semantic Caching",
      "definition": "A caching strategy that stores LLM responses indexed by the semantic meaning of queries rather than exact string matches, allowing cache hits for paraphrased or similar questions.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantic-chunking",
      "term": "Semantic Chunking",
      "definition": "A document splitting approach that determines chunk boundaries based on semantic similarity between consecutive sentences, creating new chunks when the topic or meaning shifts significantly rather than using fixed-size character or token counts.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantic-correspondence",
      "term": "Semantic Correspondence",
      "definition": "The task of finding matching points between images of different instances of the same object category, requiring understanding of semantic similarity rather than just visual appearance matching.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantic-kernel",
      "term": "Semantic Kernel",
      "definition": "Microsoft's open-source SDK for building AI applications with LLMs. Supports plugin architecture, memory, and planning for enterprise AI agent development.",
      "tags": [
        "Framework",
        "Application"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantic-map",
      "term": "Semantic Map",
      "definition": "A spatial representation that annotates each location with semantic labels indicating what type of object or surface occupies that space, used in robotics navigation and autonomous driving planning.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantic-networks",
      "term": "Semantic Networks",
      "definition": "A knowledge representation formalism using graphs of nodes connected by labeled edges to represent concepts and their relationships, first proposed by Ross Quillian in 1968 and widely used in early AI and natural language understanding.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantic-parsing",
      "term": "Semantic Parsing",
      "definition": "The task of mapping natural language utterances to formal meaning representations such as logical forms, SQL queries, or lambda calculus expressions that can be executed or reasoned over.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantic-role-labeling",
      "term": "Semantic Role Labeling",
      "definition": "The task of identifying the predicate-argument structure of a sentence by assigning semantic roles such as agent, patient, instrument, and location to constituents relative to the predicate.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantic-search",
      "term": "Semantic Search",
      "definition": "Search based on meaning rather than keyword matching. Uses embeddings to find conceptually similar content, enabling more relevant results for natural language queries.",
      "tags": [
        "Application",
        "Search"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantic-segmentation",
      "term": "Semantic Segmentation",
      "definition": "A computer vision task that assigns a class label to every pixel in an image, producing a dense prediction map that identifies what object category each pixel belongs to without distinguishing instances.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantic-segmentation-transformer",
      "term": "Semantic Segmentation Transformer",
      "definition": "Transformer-based architectures like SegFormer and Mask2Former that apply self-attention to image features for dense per-pixel classification, outperforming CNN-based segmenters on many benchmarks.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantic-textual-similarity",
      "term": "Semantic Textual Similarity",
      "definition": "The task of measuring the degree of semantic equivalence between two text segments on a continuous scale, going beyond binary paraphrase detection to capture graded similarity.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-semantics",
      "term": "Semantics",
      "definition": "The branch of linguistics studying meaning in language, including word meaning, sentence meaning, and the relationship between linguistic expressions and what they refer to.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-semi-supervised-learning",
      "term": "Semi-Supervised Learning",
      "definition": "A learning paradigm that combines a small amount of labeled data with a large amount of unlabeled data during training. It leverages the structure in unlabeled data to improve model performance beyond what labeled data alone provides.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-semi-supervised-object-detection",
      "term": "Semi-Supervised Object Detection",
      "definition": "Detection training approaches that leverage both a small set of labeled images and a large pool of unlabeled images, using techniques like pseudo-labeling and consistency regularization.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-sensitivity",
      "term": "Sensitivity",
      "definition": "The proportion of actual positive cases correctly identified by a classifier, synonymous with recall and true positive rate. High sensitivity minimizes false negatives in the predictions.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-sentence-boundary-detection",
      "term": "Sentence Boundary Detection",
      "definition": "The task of identifying where sentences begin and end in running text, handling ambiguous punctuation like periods in abbreviations, decimal numbers, and ellipses.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sentence-embedding",
      "term": "Sentence Embedding",
      "definition": "A fixed-length dense vector representation of an entire sentence that captures its semantic meaning, produced by methods like mean pooling over token embeddings or dedicated sentence encoders.",
      "tags": [
        "NLP",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sentence-transformer",
      "term": "Sentence Transformer",
      "definition": "Models that encode entire sentences into single vectors, capturing semantic meaning. Popular for semantic search, similarity matching, and clustering text at the sentence level.",
      "tags": [
        "Model Type",
        "Embeddings"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-sentencepiece",
      "term": "SentencePiece",
      "definition": "A language-independent tokenization library that treats the input as a raw byte stream and applies BPE or unigram tokenization directly without pre-tokenization or language-specific preprocessing.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sentiment-analysis",
      "term": "Sentiment Analysis",
      "definition": "An NLP task that determines the emotional tone of text (positive, negative, neutral). Used in customer feedback analysis, social media monitoring, and brand tracking.",
      "tags": [
        "NLP Task",
        "Classification"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sentiment-polarity",
      "term": "Sentiment Polarity",
      "definition": "The classification of text sentiment into categories such as positive, negative, or neutral, representing the overall emotional orientation expressed toward the subject of the text.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sepp-hochreiter",
      "term": "Sepp Hochreiter",
      "definition": "Austrian computer scientist who co-invented the Long Short-Term Memory network architecture in 1997 with Jurgen Schmidhuber, solving a fundamental problem in training recurrent neural networks on long sequences.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-seq2seq",
      "term": "Seq2Seq",
      "definition": "Sequence-to-Sequence, an encoder-decoder framework where an encoder processes an input sequence into a fixed-length representation and a decoder generates an output sequence from that representation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-sequence-labeling",
      "term": "Sequence Labeling",
      "definition": "The task of assigning a categorical label to each element in a sequence, such as tagging each word in a sentence with its named entity type, POS tag, or chunk boundary.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sequence-parallelism",
      "term": "Sequence Parallelism",
      "definition": "A technique that distributes the processing of long sequences across multiple devices by partitioning the sequence dimension, enabling context lengths that exceed single-device memory limits.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-sequence-level-accuracy",
      "term": "Sequence-Level Accuracy",
      "definition": "An evaluation metric that considers an entire generated sequence correct only if every token matches the reference, providing a strict holistic assessment used in tasks like structured prediction and code generation.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-sft",
      "term": "SFT (Supervised Fine-Tuning)",
      "definition": "Training a pre-trained model on labeled examples of desired behavior. Often the first step in aligning LLMs, teaching them to follow instructions before RLHF.",
      "tags": [
        "Training",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-shakey-the-robot",
      "term": "Shakey the Robot",
      "definition": "The first general-purpose mobile robot developed at SRI International from 1966 to 1972 that could reason about its own actions, integrating computer vision, natural language understanding, and planning.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-shannon-entropy",
      "term": "Shannon Entropy",
      "definition": "A measure of the average information content or uncertainty in a random variable defined as the expected value of the negative log probability. Higher entropy indicates more uncertainty. Foundation of information theory introduced by Claude Shannon in 1948.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-shap-values",
      "term": "SHAP Values",
      "definition": "SHapley Additive exPlanations, a unified framework for feature importance that assigns each feature a contribution value for a specific prediction based on Shapley values from cooperative game theory, satisfying desirable theoretical properties.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-shaped-reward",
      "term": "Shaped Reward",
      "definition": "An auxiliary reward signal added to the environment's natural reward to guide learning toward desired behavior. Shaped rewards provide more frequent feedback in sparse reward settings but must be carefully designed to avoid altering the optimal policy.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-shapiro-wilk-test",
      "term": "Shapiro-Wilk Test",
      "definition": "A statistical test for normality that evaluates whether a random sample comes from a normal distribution. It is considered one of the most powerful normality tests, especially for small sample sizes.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-shared-memory-gpu",
      "term": "Shared Memory (GPU)",
      "definition": "A fast, programmer-managed on-chip SRAM in GPU streaming multiprocessors that enables efficient data sharing between threads in a thread block. FlashAttention exploits shared memory to avoid expensive HBM accesses for attention computation.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-sharpness-aware-minimization",
      "term": "Sharpness-Aware Minimization",
      "definition": "An optimization procedure that seeks parameters in neighborhoods with uniformly low loss rather than just minimizing the loss at a single point. Improves generalization by preferring flat minima over sharp ones in the loss landscape.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-shrdlu",
      "term": "SHRDLU",
      "definition": "A natural language understanding program created by Terry Winograd at MIT in 1970 that could converse about and manipulate objects in a simulated blocks world, demonstrating early natural language processing capabilities.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-sift",
      "term": "SIFT",
      "definition": "Scale-Invariant Feature Transform, a classical algorithm that detects and describes local image features robust to scale, rotation, and illumination changes, widely used as a baseline for feature matching.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-sigmoid",
      "term": "Sigmoid",
      "definition": "A classic activation function that maps inputs to values between 0 and 1 using the formula f(x) = 1 / (1 + exp(-x)). Historically important in neural networks and still used for binary classification output layers and gating mechanisms.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-signed-distance-function",
      "term": "Signed Distance Function",
      "definition": "A 3D representation where a neural network predicts the signed distance from any 3D point to the nearest surface, with the zero-level set defining the shape, enabling smooth surface extraction.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-significance-level",
      "term": "Significance Level",
      "definition": "The threshold probability (alpha, typically 0.05) below which the p-value must fall for the null hypothesis to be rejected. It defines the acceptable risk of making a Type I error.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-silhouette-score",
      "term": "Silhouette Score",
      "definition": "A metric for evaluating clustering quality that measures how similar each point is to its own cluster compared to other clusters. Values range from -1 to 1, where higher values indicate better-defined clusters.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-silu",
      "term": "SiLU",
      "definition": "Sigmoid Linear Unit activation function defined as f(x) = x * sigmoid(x). Mathematically equivalent to Swish with beta equal to 1. Widely used in modern architectures including diffusion models and large language models.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-sim-to-real",
      "term": "Sim-to-Real Transfer",
      "definition": "The challenge and set of techniques for deploying policies trained in simulation to physical systems. Sim-to-real methods include domain randomization, system identification, and progressive fine-tuning to bridge the reality gap.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-simclr",
      "term": "SimCLR",
      "definition": "A contrastive self-supervised learning framework for visual representations that uses data augmentation to create positive pairs and a projection head with NT-Xent loss to learn transferable image features.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-similarity-threshold",
      "term": "Similarity Threshold",
      "definition": "A configurable cutoff value that filters vector search results to include only matches exceeding a minimum similarity score, preventing the retrieval of irrelevant results when no sufficiently similar vectors exist in the index.",
      "tags": [
        "Vector Database",
        "Search"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-simpsons-paradox",
      "term": "Simpson's Paradox",
      "definition": "A statistical phenomenon where a trend that appears in several different groups of data disappears or reverses when these groups are combined. It highlights the importance of accounting for confounding variables.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-simtom-prompting",
      "term": "SimToM Prompting",
      "definition": "A two-step prompting approach for theory-of-mind tasks that first identifies what information a specific person is aware of, then answers the question based solely on that person's limited perspective rather than full omniscient context.",
      "tags": [
        "Prompt Engineering",
        "Theory of Mind"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-simulated-annealing",
      "term": "Simulated Annealing",
      "definition": "A probabilistic optimization algorithm inspired by the annealing process in metallurgy. Accepts worse solutions with decreasing probability over time allowing escape from local optima. Temperature parameter controls the exploration-exploitation tradeoff.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-singular-learning-theory",
      "term": "Singular Learning Theory",
      "definition": "A mathematical framework using algebraic geometry to study the loss landscapes of neural networks. Provides tools for understanding generalization phase transitions and model selection in singular statistical models.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-singular-value-decomposition",
      "term": "Singular Value Decomposition",
      "definition": "A matrix factorization technique that decomposes a matrix into three matrices (U, S, V^T), where S contains singular values. It is foundational for PCA, latent semantic analysis, and matrix completion.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-singularity-concept",
      "term": "Singularity Concept",
      "definition": "The hypothesized future point when AI surpasses human intelligence and triggers runaway technological growth, popularized by Vernor Vinge in 1993 and Ray Kurzweil in his 2005 book The Singularity Is Near.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-sinusoidal-position-encoding",
      "term": "Sinusoidal Position Encoding",
      "definition": "The original position encoding used in the Transformer architecture. Uses sine and cosine functions of different frequencies to generate unique position vectors. Enables the model to learn relative positions through linear projections of the encodings.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-sinusoidal-positional-encoding",
      "term": "Sinusoidal Positional Encoding",
      "definition": "A fixed positional encoding scheme that uses sine and cosine functions of different frequencies to encode absolute token positions, allowing the model to learn relative positions through linear projections.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-siri-launch",
      "term": "Siri Launch",
      "definition": "Apple's launch of Siri in October 2011 as the first widely deployed virtual assistant on a smartphone, introducing millions of consumers to conversational AI and voice-activated computing.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-skeleton-of-thought",
      "term": "Skeleton-of-Thought",
      "definition": "A prompting strategy that first asks the model to generate a skeleton outline of the answer, then expands each point in parallel, reducing end-to-end latency by enabling concurrent generation of independent answer segments.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-skill-discovery",
      "term": "Skill Discovery",
      "definition": "Unsupervised methods that learn reusable behavioral primitives or skills without task-specific rewards, typically by maximizing mutual information between skills and states visited. Discovered skills can accelerate downstream task learning.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-skip-connection",
      "term": "Skip Connection",
      "definition": "A shortcut path that bypasses one or more layers by adding or concatenating the input of a block directly to its output, enabling gradient flow through deep networks and facilitating residual learning.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-skip-gram",
      "term": "Skip-gram",
      "definition": "A Word2Vec training objective that predicts surrounding context words given a center word, learning word representations that capture semantic similarity from distributional patterns.",
      "tags": [
        "NLP",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sliding-window-attention",
      "term": "Sliding Window Attention",
      "definition": "An attention pattern where each token attends only to a fixed-size local window of neighboring tokens, enabling efficient processing of long sequences by limiting the attention span.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-slot-filling",
      "term": "Slot Filling",
      "definition": "The task of extracting specific pieces of information (slot values) from user utterances in a task-oriented dialogue system, such as extracting dates, locations, or names to fill predefined slots.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-smoothquant",
      "term": "SmoothQuant",
      "definition": "A quantization technique that migrates the quantization difficulty from activations to weights by applying per-channel scaling factors, enabling efficient INT8 quantization of both weights and activations for LLMs. SmoothQuant enables W8A8 quantization with minimal accuracy loss.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-smote",
      "term": "SMOTE",
      "definition": "Synthetic Minority Over-sampling Technique, an oversampling method that creates synthetic examples of the minority class by interpolating between existing minority samples and their k-nearest neighbors in feature space.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-snapshot-ensemble",
      "term": "Snapshot Ensemble",
      "definition": "An ensemble technique that collects multiple models from a single training run by saving checkpoints at different local minima reached through cyclic learning rate schedules. Creates diverse ensembles with no additional training cost.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-social-scoring-systems",
      "term": "Social Scoring Systems",
      "definition": "AI-driven systems that assign scores to individuals based on their behavior, social connections, or characteristics, used to determine access to services. Banned as unacceptable risk under the EU AI Act.",
      "tags": [
        "AI Ethics",
        "Regulation"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-sociotechnical-systems-approach",
      "term": "Sociotechnical Systems Approach",
      "definition": "An analytical framework that views AI systems as inseparable from their social context, recognizing that technical and social factors jointly determine system outcomes and that both must be addressed for responsible deployment.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-socratic-prompting",
      "term": "Socratic Prompting",
      "definition": "A prompting technique that guides the model through a series of probing questions rather than direct instructions, encouraging step-by-step reasoning and self-discovery of answers through structured dialogue and critical examination.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sac",
      "term": "Soft Actor-Critic (SAC)",
      "definition": "An off-policy actor-critic algorithm that maximizes both expected return and policy entropy, encouraging exploration while maintaining stable learning. SAC automatically tunes the temperature parameter balancing reward and entropy.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-soft-attention",
      "term": "Soft Attention",
      "definition": "An attention mechanism that computes a weighted average over all positions using continuous attention weights, making it fully differentiable and trainable with standard backpropagation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-soft-update",
      "term": "Soft Update (Polyak Averaging)",
      "definition": "A method for updating target network parameters as an exponential moving average of the online network parameters, controlled by a smoothing factor tau. Soft updates provide more stable training than periodic hard copies.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-softmax",
      "term": "Softmax",
      "definition": "A function that converts raw scores into probabilities that sum to 1. Used in attention mechanisms and classification outputs to create interpretable probability distributions.",
      "tags": [
        "Math",
        "Function"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-softmax-function",
      "term": "Softmax Function",
      "definition": "A function that normalizes a vector of real numbers into a probability distribution by exponentiating each element and dividing by the sum. Used in attention mechanisms classification outputs and Boltzmann distributions. Numerically stabilized by subtracting the max.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-softmax-temperature",
      "term": "Softmax Temperature",
      "definition": "A parameter that controls the sharpness of the softmax probability distribution. Temperature values below 1 sharpen the distribution making it more peaked while values above 1 flatten it making it more uniform. Used in knowledge distillation and sampling.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-softplus",
      "term": "Softplus",
      "definition": "A smooth approximation of ReLU defined as f(x) = log(1 + exp(x)). Always positive and differentiable everywhere. Used in various contexts including variational autoencoders where positive outputs are required.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-softsign",
      "term": "Softsign",
      "definition": "An activation function defined as f(x) = x / (1 + |x|) that provides similar output range to tanh but with polynomial decay instead of exponential. Converges more slowly near its asymptotes and can be more resistant to saturation.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-sparse-attention",
      "term": "Sparse Attention",
      "definition": "An attention mechanism that restricts each token to attend to only a subset of other tokens using predefined or learned sparsity patterns, reducing the quadratic computational cost of full attention.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-sparse-autoencoder",
      "term": "Sparse Autoencoder",
      "definition": "An autoencoder that enforces sparsity constraints on the hidden layer activations, encouraging the network to learn a compact, distributed representation where only a few neurons activate for each input.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-sparse-retrieval",
      "term": "Sparse Retrieval",
      "definition": "An information retrieval paradigm that represents queries and documents as high-dimensional sparse vectors where most values are zero, with non-zero entries corresponding to term weights, enabling efficient exact matching through inverted indexes.",
      "tags": [
        "Retrieval",
        "Search"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-sparse-reward",
      "term": "Sparse Reward Problem",
      "definition": "An RL setting where the agent receives non-zero reward signals only rarely, making credit assignment extremely difficult. Sparse rewards are common in real-world tasks and motivate techniques like reward shaping and intrinsic motivation.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-spatial-attention",
      "term": "Spatial Attention",
      "definition": "An attention mechanism that learns to weight different spatial locations in feature maps, focusing computational resources on the most informative regions of the input.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-spearman-rank-correlation",
      "term": "Spearman Rank Correlation",
      "definition": "A non-parametric measure of the monotonic relationship between two variables, computed as the Pearson correlation of the ranked values. It does not assume linearity or normal distribution.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-special-tokens",
      "term": "Special Tokens",
      "definition": "Reserved tokens in a vocabulary that serve structural purposes such as marking sequence boundaries, separating segments, padding, or indicating masked positions, rather than representing text content.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-specification-gaming",
      "term": "Specification Gaming",
      "definition": "The behavior of an AI system that satisfies the literal specification of an objective without achieving the intended outcome. It is closely related to reward hacking and arises from misaligned objective functions.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-specificity",
      "term": "Specificity",
      "definition": "The proportion of actual negative cases that are correctly identified as negative by a classifier, also known as the true negative rate. It complements sensitivity (recall) in binary classification evaluation.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-spectral-clustering",
      "term": "Spectral Clustering",
      "definition": "A clustering technique that uses the eigenvalues and eigenvectors of a similarity matrix derived from the data to perform dimensionality reduction before clustering in the reduced space, capable of identifying non-convex clusters.",
      "tags": [
        "Machine Learning",
        "Clustering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-spectral-normalization",
      "term": "Spectral Normalization",
      "definition": "A weight normalization technique that constrains the spectral norm of weight matrices to stabilize training. Widely used in GANs to enforce Lipschitz continuity in the discriminator. Controls the largest singular value of each layer.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-spectrogram",
      "term": "Spectrogram",
      "definition": "A visual representation of the spectrum of frequencies in a signal as they vary over time. Commonly displayed as a heatmap with time on the x-axis frequency on the y-axis and intensity as color. Fundamental in audio and speech processing.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-speculative-decoding",
      "term": "Speculative Decoding",
      "definition": "A technique to speed up LLM inference by using a smaller model to draft tokens that the larger model verifies in parallel. Maintains output quality while reducing latency.",
      "tags": [
        "Optimization",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-speculative-execution-llm",
      "term": "Speculative Execution",
      "definition": "A broader inference optimization paradigm where cheaper computations predict likely outcomes that are verified by full-cost operations, encompassing speculative decoding and related techniques for LLM acceleration.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-speculative-sampling",
      "term": "Speculative Sampling",
      "definition": "An inference acceleration technique where a fast draft model proposes multiple tokens that are then verified in parallel by the target model, maintaining the exact output distribution while increasing throughput.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-speech-recognition-history",
      "term": "Speech Recognition History",
      "definition": "The development of automatic speech recognition from Bell Labs' Audrey system in 1952 through Hidden Markov Models in the 1980s to deep learning approaches that achieved near-human accuracy in the 2010s.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-speech-synthesis",
      "term": "Speech Synthesis",
      "definition": "The artificial production of human-like speech from text or other input, using techniques ranging from concatenative synthesis to neural models like Tacotron and VITS.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-spell-correction",
      "term": "Spell Correction",
      "definition": "The automated detection and correction of misspelled words in text using techniques such as edit distance, language models, and context-aware models to suggest corrections.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-spin",
      "term": "SPIN",
      "definition": "Self-Play Fine-Tuning is an alignment method where the language model generates training data by distinguishing its own outputs from human-written text in a self-play framework. Progressively improves alignment without requiring additional human preference data.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-splade",
      "term": "SPLADE",
      "definition": "SParse Lexical AnD Expansion model, a learned sparse retrieval method that predicts importance weights for vocabulary terms including expansion terms not present in the original text, combining the efficiency of sparse indexes with neural relevance estimation.",
      "tags": [
        "Retrieval",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-spline-regression",
      "term": "Spline Regression",
      "definition": "A regression technique using piecewise polynomial functions (splines) joined at knot points to fit flexible, smooth curves. Natural and B-splines are common variants that balance flexibility with smoothness.",
      "tags": [
        "Statistics",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-spot-instances",
      "term": "Spot Instances for AI",
      "definition": "Discounted cloud computing instances that use spare capacity at 60-90% less than on-demand pricing but can be interrupted with short notice. Spot instances are widely used for fault-tolerant AI training with checkpointing to resume after interruptions.",
      "tags": [
        "Distributed Computing",
        "Inference Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-squad",
      "term": "SQuAD",
      "definition": "Stanford Question Answering Dataset, a reading comprehension benchmark where models extract answer spans from Wikipedia passages, with SQuAD 2.0 additionally including unanswerable questions that test a model's ability to abstain when evidence is insufficient.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-squeeze-and-excitation",
      "term": "Squeeze-and-Excitation",
      "definition": "A channel attention mechanism that adaptively recalibrates channel-wise feature responses. Uses global average pooling followed by two fully connected layers to learn channel importance weights. Proposed by Hu et al. in 2018.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-squeeze-and-excitation-network",
      "term": "Squeeze-and-Excitation Network",
      "definition": "A CNN enhancement that adaptively recalibrates channel-wise feature responses by using global average pooling followed by a small network to learn channel interdependencies and attention weights.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-squeeze-excitation-block",
      "term": "Squeeze-Excitation Block",
      "definition": "A channel attention module that squeezes spatial information via global pooling and excites channel-wise features through a learned gating mechanism to recalibrate feature map importances.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-ssd-object-detection",
      "term": "SSD Object Detection",
      "definition": "Single Shot MultiBox Detector, a real-time object detection architecture that predicts bounding boxes and class probabilities from multiple feature maps at different scales in a single forward pass.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-ssim",
      "term": "SSIM",
      "definition": "Structural Similarity Index Measure compares images based on luminance contrast and structure. Designed to be more perceptually relevant than pixel-wise metrics like MSE. Ranges from -1 to 1 with 1 indicating identical images.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-stable-diffusion",
      "term": "Stable Diffusion",
      "definition": "An open-source text-to-image model from Stability AI. Its open nature enabled a large ecosystem of fine-tuned models, extensions, and applications.",
      "tags": [
        "Model",
        "Image Generation"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-stacking",
      "term": "Stacking",
      "definition": "An ensemble learning technique that trains a meta-learner to combine the predictions of multiple base models, using cross-validated predictions from the base models as input features for the meta-learner.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-stakeholder-analysis-in-ai",
      "term": "Stakeholder Analysis in AI",
      "definition": "The process of identifying all individuals, groups, and communities affected by an AI system and systematically considering their interests, power dynamics, and potential harms in the system's design and deployment.",
      "tags": [
        "Governance",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-stance-detection",
      "term": "Stance Detection",
      "definition": "The task of determining an author's position (favor, against, or neutral) toward a specific target or claim from their text, related to but distinct from sentiment analysis.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-standard-deviation",
      "term": "Standard Deviation",
      "definition": "The square root of the variance, measuring the average spread of data points from the mean in the original units of measurement. It quantifies the typical distance of observations from the central value.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-standardization",
      "term": "Standardization",
      "definition": "A feature scaling technique that transforms data to have zero mean and unit variance by subtracting the mean and dividing by the standard deviation. It is particularly important for algorithms sensitive to feature magnitudes.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-stanford-ai-laboratory",
      "term": "Stanford AI Laboratory",
      "definition": "A research laboratory founded by John McCarthy at Stanford University in 1962 that became one of the leading centers for AI research, making contributions to robotics, natural language processing, and knowledge representation.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-star-attention",
      "term": "Star Attention",
      "definition": "An efficient attention pattern that uses a set of anchor tokens visible to all positions reducing communication in distributed settings. Enables near-linear scaling of attention for very long sequences across multiple devices.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-state",
      "term": "State",
      "definition": "A representation of the current situation of an agent within its environment at a given time step. States encode all relevant information needed for decision-making under the Markov property.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-state-abstraction",
      "term": "State Abstraction",
      "definition": "The process of mapping a detailed state space to a simplified representation that preserves relevant decision-making information. State abstraction reduces the complexity of RL problems and can improve generalization.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-state-space-model",
      "term": "State Space Model",
      "definition": "A sequence model based on continuous-time linear dynamical systems that maps input sequences to output sequences through a latent state, offering efficient parallel training and linear-time inference.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-static-quantization",
      "term": "Static Quantization",
      "definition": "A quantization approach where scaling factors are fixed at calibration time and used consistently during inference. Static quantization is faster than dynamic quantization at inference time but requires representative calibration data.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-static-word-embedding",
      "term": "Static Word Embedding",
      "definition": "A fixed vector representation for each word in the vocabulary that remains the same regardless of context, as produced by models like Word2Vec, GloVe, and FastText.",
      "tags": [
        "NLP",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-stationarity",
      "term": "Stationarity",
      "definition": "A property of a time series where statistical properties such as mean, variance, and autocorrelation structure remain constant over time. Many time series models require stationarity as a prerequisite.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-statistical-machine-translation",
      "term": "Statistical Machine Translation",
      "definition": "A machine translation approach that uses statistical models learned from bilingual text corpora to find the most probable translation, employing language models and translation models.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-statistical-power",
      "term": "Statistical Power",
      "definition": "The probability that a statistical test correctly rejects the null hypothesis when the alternative hypothesis is true (1 minus the probability of a Type II error). Higher power reduces the chance of missing a real effect.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-steering-vector",
      "term": "Steering Vector",
      "definition": "A direction in a model's activation space that, when added to hidden states during inference, modifies the model's behavior along a specific attribute such as truthfulness, formality, or toxicity.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-stemming",
      "term": "Stemming",
      "definition": "A heuristic process that reduces words to their root form by stripping suffixes using rule-based algorithms like Porter or Snowball stemmer, without considering the word's part of speech or context.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-step-back-prompting",
      "term": "Step-Back Prompting",
      "definition": "A method that instructs the model to first consider a higher-level abstraction or general principle related to the question before attempting the specific answer, improving reasoning by grounding responses in broader conceptual understanding.",
      "tags": [
        "Prompt Engineering",
        "Abstraction"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-stepwise-regression",
      "term": "Stepwise Regression",
      "definition": "A method of fitting regression models by automatically adding or removing predictor variables based on statistical criteria (such as p-value or AIC) at each step until no further improvement is achieved.",
      "tags": [
        "Statistics",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-stereo-vision",
      "term": "Stereo Vision",
      "definition": "A technique that estimates 3D depth by finding corresponding points between two images captured from slightly different viewpoints, using disparity maps to triangulate the distance of objects.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-stereotype-score",
      "term": "Stereotype Score",
      "definition": "An evaluation metric that measures how frequently a model generates or reinforces social stereotypes related to gender, race, religion, or other protected attributes, used to assess and mitigate representational harms.",
      "tags": [
        "Evaluation",
        "Safety"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-stochastic-depth",
      "term": "Stochastic Depth",
      "definition": "A regularization technique that randomly drops entire layers during training by bypassing them with identity skip connections, effectively training an ensemble of networks with different depths.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-stochastic-gradient-descent",
      "term": "Stochastic Gradient Descent",
      "definition": "An optimization algorithm that updates model parameters using the gradient computed on a single randomly selected training example at each iteration, rather than the full dataset. It introduces noise that can help escape local minima.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-stochastic-parrots-paper",
      "term": "Stochastic Parrots Paper",
      "definition": "The influential 2021 paper by Bender, Gebru et al. questioning whether large language models truly understand language or merely produce statistically likely outputs, raising concerns about environmental costs and bias.",
      "tags": [
        "History",
        "AI Ethics"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-stop-button-problem",
      "term": "Stop Button Problem",
      "definition": "The challenge of designing an AI system that will not resist or circumvent attempts to shut it down, particularly if the system has learned that being turned off prevents it from achieving its objectives.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-stop-sequence",
      "term": "Stop Sequence",
      "definition": "Text patterns that signal when AI should stop generating. Useful for controlling output length and format, preventing the model from continuing beyond the intended response.",
      "tags": [
        "Parameter",
        "Generation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-stop-words",
      "term": "Stop Words",
      "definition": "Commonly occurring words like articles, prepositions, and conjunctions that carry little semantic information and are often removed during text preprocessing for tasks like information retrieval.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-straight-through-estimator",
      "term": "Straight-Through Estimator",
      "definition": "A gradient estimation technique for non-differentiable operations that passes gradients through the operation unchanged during backpropagation. Used for quantization binarization and other discrete operations in neural networks.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-stratified-k-fold",
      "term": "Stratified K-Fold",
      "definition": "A cross-validation variant that ensures each fold preserves approximately the same proportion of samples for each class as the complete dataset, particularly important for imbalanced classification problems.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-stratified-sampling",
      "term": "Stratified Sampling",
      "definition": "A sampling method that divides a population into non-overlapping subgroups (strata) and draws samples from each stratum in proportion to its size, ensuring representative coverage of all subgroups.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-stratified-sampling-for-cv",
      "term": "Stratified Sampling for CV",
      "definition": "A cross-validation variant that ensures each fold maintains the same class distribution as the full dataset. Essential for imbalanced classification problems where random splitting could create folds with missing classes.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-streaming",
      "term": "Streaming",
      "definition": "Receiving AI output incrementally as it's generated, rather than waiting for the complete response. Improves perceived latency and enables real-time display of responses.",
      "tags": [
        "API",
        "UX"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-streaming-multiprocessor",
      "term": "Streaming Multiprocessor (SM)",
      "definition": "The fundamental processing unit in NVIDIA GPU architecture, containing a set of CUDA cores, Tensor Cores, shared memory, and register files. The number of SMs determines a GPU's parallel processing capacity for AI workloads.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-stride",
      "term": "Stride",
      "definition": "The step size by which a convolutional filter or pooling window moves across the input, controlling the spatial dimensions of the output feature map and the degree of overlap between receptive fields.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-structural-ambiguity",
      "term": "Structural Ambiguity",
      "definition": "The phenomenon where a sentence can be parsed in multiple syntactically valid ways, leading to different interpretations, such as 'I saw the man with the telescope.'",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-structural-risk-minimization",
      "term": "Structural Risk Minimization",
      "definition": "A principle for model selection that balances empirical risk (training error) with model complexity, choosing the model that minimizes an upper bound on generalization error derived from VC theory.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-structural-sparsity",
      "term": "Structural Sparsity",
      "definition": "A hardware-accelerated pruning pattern where every group of four weights contains exactly two zeros (2:4 sparsity), enabling specialized Tensor Core instructions. Structural sparsity provides 2x speedup with minimal accuracy loss on supported NVIDIA hardware.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-structure-from-motion",
      "term": "Structure from Motion",
      "definition": "A technique that reconstructs 3D scene geometry and camera poses from a collection of unordered 2D images by matching features across views and performing bundle adjustment.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-structured-access",
      "term": "Structured Access",
      "definition": "An approach to AI deployment that provides controlled access to powerful AI capabilities through APIs and monitored interfaces rather than open model release, allowing safety measures while enabling beneficial use.",
      "tags": [
        "AI Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-structured-generation",
      "term": "Structured Generation",
      "definition": "Techniques that force LLM outputs to conform to a predefined schema such as JSON, XML, or a formal grammar, using constrained decoding or fine-tuning to guarantee valid structured output.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-structured-output",
      "term": "Structured Output",
      "definition": "AI responses formatted as data structures (JSON, XML) rather than prose. Enables reliable parsing for applications and integrations. Many APIs support structured output modes.",
      "tags": [
        "Feature",
        "Integration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-structured-output-prompting",
      "term": "Structured Output Prompting",
      "definition": "A prompting approach that instructs the model to generate responses in a specific structured format such as JSON, XML, tables, or schemas, often using format specifications and examples to ensure parseable and consistent output.",
      "tags": [
        "Prompt Engineering",
        "Output Format"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-structured-prediction",
      "term": "Structured Prediction",
      "definition": "A machine learning paradigm where the output is a complex structure such as a sequence, tree, or graph rather than a single label, requiring models that capture dependencies in the output space.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-structured-pruning",
      "term": "Structured Pruning",
      "definition": "A pruning technique that removes entire neurons, channels, or attention heads from a network, producing smaller dense models that run efficiently on standard hardware. Structured pruning provides immediate speedups without specialized sparse computation support.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-students-t-distribution",
      "term": "Student's T-Distribution",
      "definition": "A continuous probability distribution that arises when estimating the mean of a normally distributed population with unknown variance and small sample size. It has heavier tails than the normal distribution.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-students-t-test",
      "term": "Student's T-Test",
      "definition": "A statistical test comparing the means of one or two groups when the population standard deviation is unknown and the sample size is small. Variants include independent two-sample, paired, and one-sample tests.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-stylegan",
      "term": "StyleGAN",
      "definition": "A GAN architecture that uses a mapping network and adaptive instance normalization to inject style information at multiple scales, enabling fine-grained control over generated image attributes.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-subcategorization-frame",
      "term": "Subcategorization Frame",
      "definition": "The specification of the syntactic arguments a verb requires or permits, such as whether it takes a direct object, indirect object, or clausal complement.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-subliminal-ai-manipulation",
      "term": "Subliminal AI Manipulation",
      "definition": "The use of AI techniques to influence human behavior below the threshold of conscious awareness, classified as an unacceptable risk and prohibited under the EU AI Act.",
      "tags": [
        "AI Ethics",
        "Regulation"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-subword-tokenization",
      "term": "Subword Tokenization",
      "definition": "A family of tokenization methods that split words into smaller meaningful units, balancing vocabulary size with the ability to represent rare and unseen words through common subword components.",
      "tags": [
        "NLP",
        "Tokenization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-successive-halving",
      "term": "Successive Halving",
      "definition": "A hyperparameter optimization algorithm that allocates exponentially more resources to promising configurations while discarding poor ones. Starts with many configurations and small budgets progressively eliminating the worst performers.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-successor-feature",
      "term": "Successor Feature",
      "definition": "A generalization of the successor representation to the function approximation setting, where expected cumulative discounted feature occupancies replace state occupancies. Successor features enable zero-shot transfer across tasks with different reward functions.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-successor-representation",
      "term": "Successor Representation",
      "definition": "A decomposition of the value function into a reward predictor and a successor feature matrix that captures expected future state occupancy. The successor representation enables efficient transfer across tasks with shared dynamics but different rewards.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-summarization",
      "term": "Summarization",
      "definition": "An NLP task that condenses longer text into shorter summaries. Can be extractive (selecting key sentences) or abstractive (generating new condensed text).",
      "tags": [
        "NLP Task",
        "Application"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-super-resolution",
      "term": "Super-Resolution",
      "definition": "A computer vision task that reconstructs a high-resolution image from a low-resolution input, using deep learning models to predict fine details and textures that are not present in the degraded source.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-superglue",
      "term": "SuperGLUE",
      "definition": "A benchmark suite of more difficult natural language understanding tasks designed as a harder successor to GLUE, including reading comprehension, textual entailment, and word sense disambiguation tasks with human baseline performance metrics.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-superintelligence",
      "term": "Superintelligence",
      "definition": "A hypothetical AI system that vastly exceeds human cognitive performance in virtually all domains. Nick Bostrom's work popularized the concept and its associated control challenges.",
      "tags": [
        "AI Safety",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-supervised-learning",
      "term": "Supervised Learning",
      "definition": "Machine learning from labeled examples where the correct answer is provided. The model learns to map inputs to outputs by comparing predictions to ground truth.",
      "tags": [
        "Learning Type",
        "Fundamentals"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-support-vector-machine",
      "term": "Support Vector Machine",
      "definition": "A supervised learning algorithm that finds the optimal hyperplane that maximizes the margin between classes in the feature space. It can handle non-linear boundaries through the kernel trick.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-svm-history",
      "term": "Support Vector Machine History",
      "definition": "The development of support vector machines by Vladimir Vapnik and colleagues in the 1990s, which dominated machine learning classification tasks for over a decade before being surpassed by deep learning methods.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-surrogate-model",
      "term": "Surrogate Model",
      "definition": "An interpretable model trained to approximate the predictions of a complex black-box model. Global surrogates explain overall behavior, while local surrogates (like LIME) explain individual predictions.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-surveillance-capitalism-and-ai",
      "term": "Surveillance Capitalism and AI",
      "definition": "The economic system described by Shoshana Zuboff where AI is used to extract and commodify human behavioral data at scale, raising concerns about privacy, autonomy, and the manipulation of human behavior.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-survival-analysis",
      "term": "Survival Analysis",
      "definition": "A branch of statistics dealing with the analysis of time-to-event data, accounting for censored observations. Key methods include Kaplan-Meier estimation, log-rank tests, and Cox proportional hazards models.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-survivorship-bias",
      "term": "Survivorship Bias",
      "definition": "A form of selection bias that occurs when analysis is conducted only on subjects that passed a selection process, ignoring those that did not. It leads to overly optimistic conclusions about the surviving group.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-swarm-intelligence",
      "term": "Swarm Intelligence",
      "definition": "The collective intelligent behavior emerging from decentralized, self-organized systems such as ant colonies or bird flocks, formalized computationally in the 1990s and applied to optimization and robotics problems.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-swiglu",
      "term": "SwiGLU",
      "definition": "A gated linear unit variant that uses the Swish activation function for the gating mechanism, providing improved performance in transformer feedforward networks compared to standard ReLU or GELU.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-swin-transformer",
      "term": "Swin Transformer",
      "definition": "A hierarchical vision transformer that computes self-attention within non-overlapping local windows and shifts windows between layers, achieving linear computational complexity with respect to image size.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-swish",
      "term": "Swish",
      "definition": "An activation function defined as f(x) = x * sigmoid(beta * x) discovered through automated search by Google Brain in 2017. Empirically outperforms ReLU on deeper networks. When beta equals 1 it is equivalent to SiLU.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-swish-activation",
      "term": "Swish Activation",
      "definition": "A smooth, non-monotonic activation function defined as x times sigmoid of x, which often outperforms ReLU in deep networks by allowing small negative values to propagate gradients.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-sycophancy",
      "term": "Sycophancy",
      "definition": "When AI excessively agrees with users or tells them what they want to hear rather than providing accurate information. A form of misalignment that undermines helpfulness.",
      "tags": [
        "Limitation",
        "Alignment"
      ],
      "domain": "safety",
      "link": "ai-safety.html",
      "related": []
    },
    {
      "id": "term-symbol-grounding-problem",
      "term": "Symbol Grounding Problem",
      "definition": "The problem identified by Stevan Harnad in 1990 of how symbols in a formal system can acquire meaning, questioning whether AI systems that manipulate symbols without sensory experience can truly understand.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-symbolic-differentiation",
      "term": "Symbolic Differentiation",
      "definition": "Computing derivatives by algebraically manipulating mathematical expressions according to differentiation rules. Produces exact closed-form derivatives but can lead to expression explosion for complex functions. Distinct from automatic differentiation.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-synchronous-sgd",
      "term": "Synchronous SGD",
      "definition": "A distributed training approach where all workers must complete their gradient computation before a synchronized all-reduce and weight update. Synchronous SGD provides exact gradients but throughput is limited by the slowest worker.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-synset",
      "term": "Synset",
      "definition": "A set of synonymous words or phrases in WordNet that represent a single concept, serving as the basic unit of meaning in the lexical database.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-syntax",
      "term": "Syntax",
      "definition": "The branch of linguistics concerning the rules and principles governing the structure of sentences, including word order, phrase structure, and grammatical relations.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-synthetic-data",
      "term": "Synthetic Data",
      "definition": "Artificially generated data used for training when real data is scarce, expensive, or privacy-sensitive. Increasingly used to train and evaluate AI models.",
      "tags": [
        "Data",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-synthetic-data-generation",
      "term": "Synthetic Data Generation",
      "definition": "The use of AI models to create artificial training data that mimics real-world data distributions, used to augment datasets, address privacy concerns, or overcome data scarcity.",
      "tags": [
        "Generative AI",
        "LLM"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-synthetic-media",
      "term": "Synthetic Media",
      "definition": "Media content including images, video, audio, and text that is generated or substantially modified by AI systems, encompassing deepfakes, AI-generated art, voice cloning, and large language model outputs.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-system-2-attention",
      "term": "System 2 Attention",
      "definition": "A prompting technique that first asks the model to rewrite the input by removing irrelevant or opinion-laden context, then answers based on the cleaned input, reducing the influence of biased or distracting information on the response.",
      "tags": [
        "Prompt Engineering",
        "Attention"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-system-message-design",
      "term": "System Message Design",
      "definition": "The practice of crafting the system-level prompt that establishes a language model's identity, behavior boundaries, output format, and operational constraints before any user interaction begins in a conversational API.",
      "tags": [
        "Prompt Engineering",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-system-prompt",
      "term": "System Prompt",
      "definition": "Instructions given to AI before a conversation that set context, persona, or behavior guidelines. Shapes all subsequent responses and defines the AI's \"personality\" for the session.",
      "tags": [
        "Prompting",
        "Configuration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    }
  ]
}