{
  "letter": "c",
  "count": 296,
  "terms": [
    {
      "id": "term-c2pa",
      "term": "C2PA",
      "definition": "The Coalition for Content Provenance and Authenticity, a joint development foundation creating technical standards for certifying the source and history of media content through cryptographic provenance metadata.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-caffe-framework",
      "term": "Caffe Framework",
      "definition": "A deep learning framework developed by Yangqing Jia at UC Berkeley in 2013. Caffe (Convolutional Architecture for Fast Feature Embedding) was widely used for computer vision research and contributed to the rapid adoption of CNNs in the early deep learning era.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-calibration",
      "term": "Calibration",
      "definition": "The degree to which a model's predicted probabilities match the true frequencies of outcomes. A well-calibrated model predicting 80% probability should be correct approximately 80% of the time for such predictions.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-calibration-data",
      "term": "Calibration Data for Quantization",
      "definition": "A representative subset of input data used to determine optimal quantization parameters such as scaling factors and zero points. Calibration data quality directly affects the accuracy of post-training quantized models.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-calibration-error",
      "term": "Calibration Error",
      "definition": "A metric that measures the discrepancy between a model's predicted confidence and its actual accuracy, where a well-calibrated model's stated probability of being correct closely matches its empirical accuracy at each confidence level.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-calibration-fairness",
      "term": "Calibration Fairness",
      "definition": "A fairness metric requiring that among individuals assigned a given predicted probability, the actual proportion of positive outcomes is the same across all protected groups, ensuring that confidence scores are equally meaningful.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-callback",
      "term": "Callback",
      "definition": "A function called at specific points during training or inference. Used for logging, checkpointing, early stopping, and custom behaviors in ML pipelines.",
      "tags": [
        "Technical",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-camera-calibration",
      "term": "Camera Calibration",
      "definition": "The process of estimating the intrinsic parameters (focal length, principal point, distortion) and extrinsic parameters (position, orientation) of a camera, essential for accurate 3D reconstruction and measurement.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-capability",
      "term": "Capability (AI)",
      "definition": "A specific skill or function an AI system can perform. Capabilities range from basic (text generation) to advanced (multi-step reasoning, tool use). Understanding capabilities helps set realistic expectations.",
      "tags": [
        "Concept",
        "Assessment"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-capability-control",
      "term": "Capability Control",
      "definition": "Safety measures that limit what an AI system can do by restricting its access to resources, communication channels, or actuators, as opposed to motivational control which shapes what the system wants to do.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-capsule-network",
      "term": "Capsule Network",
      "definition": "A neural network architecture that uses groups of neurons (capsules) to encode both the presence and instantiation parameters of features, using dynamic routing to model part-whole relationships.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-capsule-networks",
      "term": "Capsule Networks",
      "definition": "A neural network architecture proposed by Geoffrey Hinton and colleagues (Sabour et al. 2017) that uses groups of neurons (capsules) to represent spatial hierarchies and part-whole relationships. Capsule networks aimed to address limitations of CNNs in understanding spatial relationships.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-captioning",
      "term": "Captioning",
      "definition": "Generating text descriptions of images or videos. A multimodal task requiring visual understanding and language generation. Used for accessibility and content organization.",
      "tags": [
        "Task",
        "Multimodal"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-carnegie-mellon-ai",
      "term": "Carnegie Mellon AI",
      "definition": "AI research programs at Carnegie Mellon University including the work of Herbert Simon Allen Newell and Raj Reddy. Home to the Robotics Institute and the Machine Learning Department one of the first dedicated ML departments at any university.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-case-based-reasoning",
      "term": "Case-Based Reasoning",
      "definition": "An AI methodology that solves new problems by adapting solutions from similar past cases. Developed by Roger Schank and others in the 1980s CBR systems maintain a case library and use similarity metrics to retrieve and adapt relevant prior experiences.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-cataphora",
      "term": "Cataphora",
      "definition": "A linguistic phenomenon where a referring expression precedes the entity it refers to in the text, as in 'Before he arrived, John called ahead,' posing challenges for reference resolution.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-catastrophic-forgetting",
      "term": "Catastrophic Forgetting",
      "definition": "The tendency of neural networks to forget previously learned information when trained on new data. A significant challenge in continual learning and fine-tuning scenarios.",
      "tags": [
        "Training",
        "Challenge"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-catastrophic-forgetting-ethics",
      "term": "Catastrophic Forgetting Ethics",
      "definition": "Ethical implications of the tendency of neural networks to forget previously learned safety constraints when trained on new data, potentially undermining alignment measures during continued training.",
      "tags": [
        "AI Safety",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-catastrophic-forgetting-rl",
      "term": "Catastrophic Forgetting in RL",
      "definition": "The tendency of neural network-based RL agents to lose previously learned skills when adapting to new tasks or environments. Continual RL methods use regularization, replay, or modular architectures to mitigate forgetting.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-catastrophic-risk-from-ai",
      "term": "Catastrophic Risk from AI",
      "definition": "The risk that AI systems could cause large-scale irreversible harm falling short of existential risk, such as widespread economic collapse, loss of critical infrastructure, or major environmental damage.",
      "tags": [
        "AI Safety",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-catboost",
      "term": "CatBoost",
      "definition": "A gradient boosting library that natively handles categorical features using ordered target statistics and employs ordered boosting to reduce prediction shift, yielding strong performance with minimal hyperparameter tuning.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-causal-convolution",
      "term": "Causal Convolution",
      "definition": "A convolution that only uses current and past inputs ensuring the output at time t depends only on inputs at times t and earlier. Essential for autoregressive sequence modeling in architectures like WaveNet and temporal convolutional networks.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-causal-language-model",
      "term": "Causal Language Model",
      "definition": "A model that predicts the next token based only on previous tokens (left-to-right). GPT and most text generation models are causal. Contrast with bidirectional models like BERT.",
      "tags": [
        "Architecture",
        "LLM"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-causal-language-modeling",
      "term": "Causal Language Modeling",
      "definition": "A training objective where the model predicts each token based only on the preceding tokens in the sequence, enforcing a left-to-right autoregressive generation order.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-causal-mask",
      "term": "Causal Mask",
      "definition": "A triangular attention mask that prevents each position from attending to subsequent positions, enforcing the autoregressive property required for left-to-right language generation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-causal-masking",
      "term": "Causal Masking",
      "definition": "A specific attention mask pattern that prevents each position from attending to future positions ensuring autoregressive behavior. Implemented as a lower triangular mask matrix. Essential for language model training and generation.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cbam",
      "term": "CBAM",
      "definition": "Convolutional Block Attention Module, a lightweight attention module that sequentially applies channel and spatial attention to feature maps, enhancing representational power with minimal overhead.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-cbow",
      "term": "CBOW",
      "definition": "Continuous Bag of Words, a Word2Vec training objective that predicts a center word from the average of its surrounding context word vectors, typically faster to train than Skip-gram.",
      "tags": [
        "NLP",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-ceiling-effect",
      "term": "Ceiling Effect",
      "definition": "When a benchmark becomes too easy to distinguish between models, all scoring near the maximum. Prompts creation of harder benchmarks to continue measuring progress.",
      "tags": [
        "Evaluation",
        "Benchmark"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-cellular-automata",
      "term": "Cellular Automata",
      "definition": "Discrete models of computation consisting of a grid of cells each in a finite number of states that evolve over time according to simple rules. John von Neumann and Stanislaw Ulam developed the concept in the 1940s. Conway's Game of Life (1970) became the most famous example.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-censoring",
      "term": "Censoring",
      "definition": "A condition in survival analysis where the exact time of an event is not observed for some subjects, typically because the study ended or the subject was lost to follow-up. Right censoring is the most common form.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-center-for-ai-safety",
      "term": "Center for AI Safety",
      "definition": "A nonprofit research and field-building organization founded in 2022 focused on reducing societal-scale risks from AI. CAIS published the Statement on AI Risk signed by leading AI researchers and policymakers warning about existential risks from advanced AI.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-centernet",
      "term": "CenterNet",
      "definition": "An anchor-free object detection approach that represents objects as center points with associated size and offset predictions, simplifying the detection pipeline by eliminating anchor box design and NMS post-processing.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-central-limit-theorem",
      "term": "Central Limit Theorem",
      "definition": "A fundamental theorem stating that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the population's original distribution, provided the variance is finite.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-ctde",
      "term": "Centralized Training Decentralized Execution (CTDE)",
      "definition": "A multi-agent RL paradigm where agents have access to global information during training but must act independently based only on local observations at test time. CTDE bridges the gap between joint and independent learning.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-centroid-based-clustering-vectors",
      "term": "Centroid-Based Clustering for Vectors",
      "definition": "The use of clustering algorithms like k-means to partition a vector collection into groups represented by centroid vectors, forming the basis of IVF indexes where query vectors are first compared to centroids to identify relevant partitions.",
      "tags": [
        "Vector Database",
        "Index Structure"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-cerebras",
      "term": "Cerebras",
      "definition": "A semiconductor company that produces wafer-scale AI processors (WSE series) with millions of cores and terabytes of on-chip SRAM. Cerebras systems eliminate memory bandwidth bottlenecks by keeping entire large models in on-chip memory.",
      "tags": [
        "Hardware",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-cerebras-systems",
      "term": "Cerebras Systems",
      "definition": "An AI hardware company founded in 2016 that developed the Wafer Scale Engine the largest chip ever built. The CS-2 system uses an entire silicon wafer as a single chip providing massive parallelism for training large neural networks.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-cerebras-gpt",
      "term": "Cerebras-GPT",
      "definition": "A family of language models trained by Cerebras Systems following Chinchilla-optimal scaling laws. Released with training recipes and full reproducibility details. Models range from 111M to 13B parameters.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-chain-of-density",
      "term": "Chain of Density",
      "definition": "A prompting technique that iteratively increases the information density of a summary by asking the model to rewrite it with additional entities while maintaining the same length, producing progressively more concise and entity-rich summaries.",
      "tags": [
        "Prompt Engineering",
        "Summarization"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-chain-of-code",
      "term": "Chain-of-Code",
      "definition": "A reasoning framework that augments chain-of-thought with executable code generation, allowing the model to write and simulate code execution for reasoning steps that benefit from computation while using natural language for semantic reasoning.",
      "tags": [
        "Prompt Engineering",
        "Code-Augmented"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-chain-of-knowledge",
      "term": "Chain-of-Knowledge",
      "definition": "A prompting framework that progressively builds and refines a knowledge chain by eliciting relevant facts, verifying their consistency, and reasoning over the accumulated knowledge to produce answers grounded in verified information.",
      "tags": [
        "Prompt Engineering",
        "Knowledge Augmentation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-chain-of-table",
      "term": "Chain-of-Table",
      "definition": "A reasoning framework for tabular data that iteratively transforms tables through operations like filtering, sorting, and aggregation as intermediate reasoning steps, with each step producing a new table state that informs the next operation.",
      "tags": [
        "Prompt Engineering",
        "Tabular Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-chain-of-thought",
      "term": "Chain-of-Thought (CoT)",
      "definition": "A prompting technique that encourages AI to show its reasoning process step-by-step, leading to more accurate and transparent responses for complex problems.",
      "tags": [
        "Prompting",
        "Reasoning"
      ],
      "domain": "general",
      "link": "../learn/index.html",
      "related": []
    },
    {
      "id": "term-chain-of-thought-discovery",
      "term": "Chain-of-Thought Discovery",
      "definition": "The discovery that prompting large language models to think step by step dramatically improves their reasoning performance. Introduced by Jason Wei et al. at Google Brain in 2022 chain-of-thought prompting became one of the most influential prompt engineering techniques.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-chain-of-thought-prompting",
      "term": "Chain-of-Thought Prompting",
      "definition": "A prompting technique that elicits step-by-step reasoning from language models by including intermediate reasoning steps in the prompt. Dramatically improves performance on arithmetic commonsense and symbolic reasoning tasks.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Prompting"
      ],
      "domain": "algorithms",
      "link": "learn/chain-of-thought.html",
      "related": []
    },
    {
      "id": "term-cot-self-consistency",
      "term": "Chain-of-Thought with Self-Consistency",
      "definition": "The combined technique of generating multiple chain-of-thought reasoning paths for a single problem using sampling and selecting the most frequently occurring final answer through majority voting to improve reasoning reliability.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-channel-attention",
      "term": "Channel Attention",
      "definition": "An attention mechanism that learns to weight the importance of different feature channels in a CNN, selectively emphasizing informative channels while suppressing less useful ones.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-character-error-rate",
      "term": "Character Error Rate",
      "definition": "A fine-grained evaluation metric that computes the edit distance between predicted and reference texts at the character level, useful for evaluating OCR systems and speech recognition where partial word matches carry meaningful signal.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-character-n-gram",
      "term": "Character N-gram",
      "definition": "A contiguous sequence of N characters extracted from a word or text, used as features for text classification, language identification, and spelling correction tasks.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-character-level",
      "term": "Character-Level Model",
      "definition": "Models that process text character by character rather than using tokens. More flexible with novel words but typically slower and requiring more parameters for the same capability.",
      "tags": [
        "Architecture",
        "Alternative"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-charles-babbage",
      "term": "Charles Babbage",
      "definition": "English mathematician and inventor (1791-1871) who conceived the Difference Engine and the Analytical Engine, mechanical general-purpose computers that anticipated key concepts in modern computing and AI.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-chat-completion",
      "term": "Chat Completion",
      "definition": "An API endpoint type where the model generates responses in a conversational format. Takes a list of messages (system, user, assistant) and returns the next assistant message.",
      "tags": [
        "API",
        "Technical"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-chat-template",
      "term": "Chat Template",
      "definition": "A structured formatting convention that defines how system messages, user inputs, and assistant responses are tokenized and delimited for multi-turn conversation models.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-chatglm",
      "term": "ChatGLM",
      "definition": "A family of bilingual language models based on the General Language Model architecture. Developed by Tsinghua University and Zhipu AI. Features efficient inference and strong performance on Chinese language tasks.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-chatgpt",
      "term": "ChatGPT",
      "definition": "OpenAI's conversational AI product launched in November 2022. Built on GPT models fine-tuned for dialogue, it popularized conversational AI and sparked widespread public interest in LLMs.",
      "tags": [
        "Product",
        "OpenAI"
      ],
      "domain": "general",
      "link": "chatgpt-guide.html",
      "related": []
    },
    {
      "id": "term-chatgpt-launch",
      "term": "ChatGPT Launch",
      "definition": "OpenAI's release of ChatGPT on November 30, 2022, a conversational AI interface built on GPT-3.5 that reached 100 million users in two months, triggering widespread public engagement with AI and an industry-wide AI race.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-checkpoint",
      "term": "Checkpoint",
      "definition": "A saved snapshot of model weights during training. Enables resuming training after interruption, comparing different training stages, and selecting the best performing version.",
      "tags": [
        "Training",
        "Technical"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-checkpointing-training",
      "term": "Checkpointing for Training",
      "definition": "The practice of periodically saving model weights, optimizer state, and training metadata to persistent storage during training. Checkpointing enables recovery from hardware failures, job preemptions, and experiment reproduction.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-chi-square-distribution",
      "term": "Chi-Square Distribution",
      "definition": "The distribution of the sum of squares of k independent standard normal random variables. It is used in chi-square tests, confidence interval estimation for variance, and goodness-of-fit tests.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-chi-square-test",
      "term": "Chi-Square Test",
      "definition": "A statistical test that evaluates whether observed frequencies differ significantly from expected frequencies under a null hypothesis. It is used for testing independence between categorical variables and goodness of fit.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-chinchilla",
      "term": "Chinchilla",
      "definition": "A DeepMind model and scaling study showing optimal training requires more data than previously thought. Influenced subsequent model development toward larger datasets.",
      "tags": [
        "Research",
        "Scaling"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-chinchilla-optimal",
      "term": "Chinchilla Optimal",
      "definition": "A training regime derived from DeepMind's Chinchilla scaling laws, suggesting that for a given compute budget, model size and training data should be scaled proportionally for optimal performance.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-chinchilla-paper",
      "term": "Chinchilla Paper",
      "definition": "The 2022 DeepMind paper by Hoffmann et al. demonstrating that many large language models were undertrained relative to their size, establishing new scaling laws suggesting that training data and model size should be scaled equally.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-chinchilla-scaling-laws",
      "term": "Chinchilla Scaling Laws",
      "definition": "Findings published by DeepMind in 2022 (Hoffmann et al.) showing that many large language models were significantly undertrained. The Chinchilla paper demonstrated that for compute-optimal training the number of training tokens should scale proportionally with model parameters.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-chinese-room-argument",
      "term": "Chinese Room Argument",
      "definition": "A thought experiment by John Searle in 1980 arguing that a computer executing a program cannot have genuine understanding or consciousness, even if it perfectly simulates intelligent conversation, challenging strong AI claims.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-chinook",
      "term": "Chinook",
      "definition": "A computer checkers program developed by Jonathan Schaeffer at the University of Alberta that won the World Checkers Championship in 1994. In 2007 the team proved that perfect play by both sides leads to a draw making checkers the most complex game solved to date.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-chiplet-architecture",
      "term": "Chiplet Architecture",
      "definition": "A processor design approach using multiple small silicon dies (chiplets) connected via high-speed interconnects on a single package. Chiplet designs improve manufacturing yield and enable mixing different process nodes for different functional units.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-christopher-watkins",
      "term": "Christopher Watkins",
      "definition": "British computer scientist who introduced Q-learning in his 1989 PhD thesis at Cambridge University. Q-learning is a model-free reinforcement learning algorithm that learns the value of actions in states without requiring a model of the environment.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-chromadb",
      "term": "ChromaDB",
      "definition": "An open-source embedding database designed for AI applications that provides a simple API for storing, querying, and filtering embeddings with associated metadata, popular for prototyping and lightweight RAG implementations.",
      "tags": [
        "Vector Database",
        "Open Source"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-chunk-overlap",
      "term": "Chunk Overlap",
      "definition": "The number of tokens or characters shared between consecutive chunks during document splitting, ensuring that information spanning chunk boundaries is not lost and maintaining contextual continuity across adjacent segments.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-chunk-size",
      "term": "Chunk Size",
      "definition": "The target length of individual text segments produced during document chunking, typically measured in tokens or characters, where smaller chunks enable more precise retrieval while larger chunks preserve more context and coherence.",
      "tags": [
        "Retrieval",
        "Preprocessing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-chunked-prefill",
      "term": "Chunked Prefill",
      "definition": "An inference optimization that breaks the prompt processing phase into smaller chunks to be interleaved with decoding steps. Reduces time-to-first-token latency for long prompts and enables better GPU utilization in serving systems.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-chunking",
      "term": "Chunking",
      "definition": "Splitting long documents into smaller pieces for processing. Essential for RAG and embedding systems where input length exceeds model limits or affects retrieval quality.",
      "tags": [
        "Technique",
        "Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-church-turing-thesis",
      "term": "Church-Turing Thesis",
      "definition": "The hypothesis independently proposed by Alonzo Church and Alan Turing in 1936 that any function computable by an effective procedure can be computed by a Turing machine. The thesis defines the fundamental limits of what is mechanically computable.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-cider",
      "term": "CIDEr",
      "definition": "Consensus-based Image Description Evaluation, a metric that measures image captioning quality using TF-IDF weighted n-gram similarity between generated and reference captions, emphasizing informative words that distinguish specific images from the corpus.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-cifar-organization",
      "term": "CIFAR (Organization)",
      "definition": "The Canadian Institute for Advanced Research a nonprofit research organization that has been instrumental in supporting AI research. CIFAR's Learning in Machines and Brains program provided crucial funding to deep learning researchers during periods when the field was unfashionable.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-cifar-10",
      "term": "CIFAR-10",
      "definition": "A dataset of 60000 32x32 color images in 10 classes collected by Alex Krizhevsky and Geoffrey Hinton in 2009. Along with CIFAR-100 it became a standard benchmark for evaluating image classification algorithms in machine learning research.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-citation-generation",
      "term": "Citation Generation",
      "definition": "The capability of a language model to produce inline references to source documents that support its claims, enabling users to verify the accuracy of generated content.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-cky-algorithm",
      "term": "CKY Algorithm",
      "definition": "Cocke-Kasami-Younger algorithm, a dynamic programming parser for context-free grammars that builds parse trees bottom-up in O(n^3) time by filling a chart of possible constituents.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-clarity",
      "term": "Clarity (Prompting)",
      "definition": "Using clear, unambiguous language in prompts to reduce misinterpretation. Specific instructions and explicit requirements improve response quality.",
      "tags": [
        "Prompting",
        "Best Practice"
      ],
      "domain": "general",
      "link": "../learn/prompt-basics.html",
      "related": []
    },
    {
      "id": "term-class-activation-map",
      "term": "Class Activation Map",
      "definition": "A visualization technique that highlights the image regions most important for a CNN's classification decision, computed by weighting feature map activations by the classification layer's weights.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-class-imbalance",
      "term": "Class Imbalance",
      "definition": "When training data has unequal representation across categories. Can cause models to favor majority classes. Addressed through sampling, weighting, or specialized techniques.",
      "tags": [
        "Data",
        "Challenge"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-class-weight",
      "term": "Class Weight",
      "definition": "A technique for handling class imbalance by assigning higher weight to the minority class in the loss function, effectively making misclassification of underrepresented classes more costly during training.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-classification",
      "term": "Classification",
      "definition": "A machine learning task that assigns input data to predefined categories. Examples include spam detection (spam/not spam), sentiment analysis (positive/negative/neutral), and image recognition.",
      "tags": [
        "ML Task",
        "Supervised"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-classifier-free-guidance",
      "term": "Classifier-Free Guidance",
      "definition": "A technique for conditional diffusion models that interpolates between conditional and unconditional score estimates during sampling, controlling the trade-off between sample quality and diversity without a separate classifier.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-claude",
      "term": "Claude",
      "definition": "An AI assistant created by Anthropic, designed to be helpful, harmless, and honest. Known for nuanced reasoning, long context handling, and strong performance on complex tasks.",
      "tags": [
        "Product",
        "Anthropic"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-claude-ai",
      "term": "Claude (AI)",
      "definition": "A family of AI assistants developed by Anthropic beginning with Claude 1.0 in March 2023. Built using Constitutional AI methods and Reinforcement Learning from Human Feedback. Known for being helpful harmless and honest with strong reasoning capabilities.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-claude-haiku",
      "term": "Claude Haiku",
      "definition": "The fastest and most compact model in the Claude family optimized for quick responses and high throughput. Suitable for tasks requiring speed such as customer interactions and content moderation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-claude-instant",
      "term": "Claude Instant / Haiku",
      "definition": "Anthropic's faster, more cost-effective models for simpler tasks. Trade some capability for speed and lower cost, suitable for classification, extraction, and basic chat.",
      "tags": [
        "Model",
        "Anthropic"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-claude-launch",
      "term": "Claude Launch",
      "definition": "Anthropic's release of Claude, a family of AI assistants trained using Constitutional AI methods, first made available in March 2023, emphasizing safety, helpfulness, and harmlessness in conversational AI.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-claude-opus",
      "term": "Claude Opus",
      "definition": "Anthropic's most capable model, designed for complex reasoning, creative tasks, and nuanced understanding. Higher cost but best performance on difficult tasks.",
      "tags": [
        "Model",
        "Anthropic"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-claude-shannon",
      "term": "Claude Shannon",
      "definition": "American mathematician (1916-2001) known as the father of information theory, whose 1948 paper established the mathematical foundations for digital communication and contributed foundational ideas to early AI research.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-claude-sonnet",
      "term": "Claude Sonnet",
      "definition": "A balanced model in the Claude family offering strong performance with good efficiency. Suitable for a wide range of tasks including coding analysis and content generation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-clever-hans-effect",
      "term": "Clever Hans Effect",
      "definition": "Named after a horse that appeared to perform arithmetic but was actually reading its trainer's body language. In AI it refers to models that appear to perform well on a task but are actually using spurious correlations or shortcuts rather than genuine understanding.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-cliff-shaw",
      "term": "Cliff Shaw",
      "definition": "American programmer at RAND Corporation who along with Allen Newell and Herbert Simon developed the Logic Theorist (1956) and General Problem Solver. Shaw created the Information Processing Language (IPL) one of the earliest list-processing computer languages.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-clip",
      "term": "CLIP",
      "definition": "Contrastive Language-Image Pre-training, a model that learns visual concepts from natural language supervision by training image and text encoders jointly to match images with their text descriptions.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-clipped-surrogate-objective",
      "term": "Clipped Surrogate Objective",
      "definition": "The core optimization objective in PPO that clips the probability ratio between new and old policies, preventing excessively large updates. This simple mechanism provides trust-region-like stability without the computational cost of TRPO.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-clips",
      "term": "CLIPS",
      "definition": "The C Language Integrated Production System developed by NASA's Johnson Space Center in 1985. An expert system shell designed for building rule-based and object-based expert systems. CLIPS became widely used in government industry and academia for knowledge-based applications.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-cloud-computing-ai",
      "term": "Cloud Computing for AI",
      "definition": "The use of cloud infrastructure services (AWS, GCP, Azure) for AI model training and inference, providing on-demand access to GPU clusters without capital expenditure. Cloud AI services range from raw GPU instances to fully managed training and serving platforms.",
      "tags": [
        "Distributed Computing",
        "Inference Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-cloze",
      "term": "Cloze Task",
      "definition": "A task where models predict missing words in text. A classic NLP benchmark and training objective. BERT's masked language modeling is a form of cloze task.",
      "tags": [
        "Task",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-clustering",
      "term": "Clustering",
      "definition": "An unsupervised learning technique that groups similar data points together without predefined labels. Used for customer segmentation, document organization, and pattern discovery.",
      "tags": [
        "ML Task",
        "Unsupervised"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-cma-es",
      "term": "CMA-ES",
      "definition": "Covariance Matrix Adaptation Evolution Strategy is a derivative-free optimization algorithm that adapts its search distribution by updating the covariance matrix of a multivariate normal. Considered state-of-the-art for continuous black-box optimization.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cmu-ai-research",
      "term": "CMU AI Research",
      "definition": "Carnegie Mellon University's AI research programs, including the work of Allen Newell and Herbert Simon, the development of expert systems, and pioneering contributions to robotics, speech recognition, and autonomous vehicles.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-cnn",
      "term": "CNN (Convolutional Neural Network)",
      "definition": "A neural network architecture designed for processing grid-like data such as images. Uses convolutional layers to automatically learn spatial hierarchies of features.",
      "tags": [
        "Architecture",
        "Computer Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-co-occurrence-matrix",
      "term": "Co-occurrence Matrix",
      "definition": "A matrix recording how often pairs of words appear together within a defined context window across a corpus, used as the basis for distributional word representations like GloVe.",
      "tags": [
        "NLP",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-co-training",
      "term": "Co-Training",
      "definition": "A semi-supervised learning method that trains two classifiers on different views or feature sets of the data. Each classifier labels unlabeled examples for the other creating a mutual teaching loop. Requires conditionally independent feature views.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-coco-dataset",
      "term": "COCO Dataset",
      "definition": "Common Objects in Context, a large-scale benchmark dataset containing images with annotations for object detection, instance segmentation, keypoint detection, and image captioning across 80 object categories.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-code-generation",
      "term": "Code Generation",
      "definition": "The ability of AI models to write programming code from natural language descriptions. Powers tools like GitHub Copilot, Cursor, and code-focused features in general LLMs.",
      "tags": [
        "Application",
        "Development"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-code-generation-prompting",
      "term": "Code Generation Prompting",
      "definition": "Specialized prompting techniques for producing high-quality code, incorporating language specification, function signatures, docstrings, test cases, and algorithmic constraints to guide models toward correct and efficient implementations.",
      "tags": [
        "Prompt Engineering",
        "Code"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-code-interpreter",
      "term": "Code Interpreter",
      "definition": "AI capability to write and execute code, enabling data analysis, visualization, and computation. ChatGPT's code interpreter runs Python in a sandbox environment.",
      "tags": [
        "Feature",
        "Tool Use"
      ],
      "domain": "general",
      "link": "chatgpt-guide.html",
      "related": []
    },
    {
      "id": "term-code-llama",
      "term": "Code Llama",
      "definition": "A specialized version of LLaMA fine-tuned for code generation and understanding. Available in base Python and instruction-tuned variants. Supports infilling and long-context code understanding up to 100K tokens.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-code-llm",
      "term": "Code LLM",
      "definition": "Language models specialized for programming tasks. Examples include Codex, StarCoder, and Code Llama. Often trained on large code corpora from GitHub and similar sources.",
      "tags": [
        "Model Type",
        "Specialized"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-code-switching",
      "term": "Code-Switching",
      "definition": "The phenomenon of alternating between two or more languages within a single conversation or utterance, posing challenges for NLP systems designed for monolingual text processing.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-codebleu",
      "term": "CodeBLEU",
      "definition": "A code evaluation metric that extends BLEU with code-specific components including abstract syntax tree matching, data-flow analysis, and weighted n-gram matching, capturing both syntactic correctness and semantic similarity of generated code.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-codegen",
      "term": "CodeGen",
      "definition": "A family of large language models for program synthesis that convert natural language descriptions into executable code. Trained with multi-turn program synthesis enabling iterative code generation through conversation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-codex",
      "term": "Codex",
      "definition": "An OpenAI model fine-tuned from GPT-3 on publicly available code from GitHub. Powers GitHub Copilot for code completion and generation. Proficient in Python and supports dozens of programming languages.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-cognitive-load",
      "term": "Cognitive Load (Prompting)",
      "definition": "The mental effort required to process complex prompts. Simpler, well-organized prompts often yield better results by reducing the model's processing burden.",
      "tags": [
        "Prompting",
        "Best Practice"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-cogvlm",
      "term": "CogVLM",
      "definition": "A visual language model that integrates vision features deep into the language model through a visual expert module in every attention and FFN layer. Achieves strong visual understanding without sacrificing language model capability.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-cohens-kappa",
      "term": "Cohen's Kappa",
      "definition": "A statistic measuring inter-rater agreement for categorical items that accounts for agreement occurring by chance. Values range from -1 to 1, with 1 indicating perfect agreement beyond chance.",
      "tags": [
        "Statistics",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-cohere",
      "term": "Cohere",
      "definition": "An enterprise AI company providing LLMs for text generation, embeddings, and search. Known for Command models and focus on enterprise use cases with strong RAG capabilities.",
      "tags": [
        "Company",
        "LLM Provider"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-cohere-command",
      "term": "Cohere Command",
      "definition": "A family of generative language models by Cohere designed for enterprise applications. Optimized for instruction following text generation and tool use. Available in multiple sizes for different deployment needs.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-cohere-embed",
      "term": "Cohere Embed",
      "definition": "A family of embedding models by Cohere designed for search and retrieval applications. Supports over 100 languages with state-of-the-art performance on multilingual benchmarks. Offers compression for efficient storage.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-coherence-modeling",
      "term": "Coherence Modeling",
      "definition": "The computational assessment of how well sentences in a text flow together logically and topically, evaluating whether a text reads naturally and maintains consistent themes and references.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-coherence-score",
      "term": "Coherence Score",
      "definition": "An evaluation metric that assesses the logical consistency and semantic flow of generated text, measuring whether ideas connect naturally, maintain topical consistency, and form a well-structured narrative.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-cointegration",
      "term": "Cointegration",
      "definition": "A statistical property of two or more non-stationary time series that share a common stochastic trend, meaning a linear combination of them is stationary. It implies a long-run equilibrium relationship.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-colbert",
      "term": "ColBERT",
      "definition": "A late-interaction retrieval model that independently encodes queries and documents into per-token embeddings, then scores relevance through efficient MaxSim operations between the two sets of embeddings.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-cold-start",
      "term": "Cold Start Problem",
      "definition": "Difficulty making predictions for new users or items with no historical data. Common in recommendation systems. Addressed with hybrid approaches combining collaborative and content-based methods.",
      "tags": [
        "Challenge",
        "Recommendations"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-collaborative-filtering",
      "term": "Collaborative Filtering",
      "definition": "Recommendation technique based on user behavior patterns. \"Users who liked X also liked Y.\" Forms the basis of many recommendation systems at Netflix, Amazon, etc.",
      "tags": [
        "Technique",
        "Recommendations"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-collection",
      "term": "Collection",
      "definition": "A named grouping of vectors and their associated metadata within a vector database, analogous to a table in relational databases, serving as the primary organizational unit for storing and querying related embeddings.",
      "tags": [
        "Vector Database",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-collective-communication",
      "term": "Collective Communication",
      "definition": "Coordinated data exchange patterns among multiple processes or GPUs, including all-reduce, all-gather, reduce-scatter, and broadcast. Efficient collective communication is fundamental to scaling distributed AI training.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-collocation",
      "term": "Collocation",
      "definition": "A sequence of words that co-occur more frequently than expected by chance, forming conventional expressions such as 'strong coffee' or 'make a decision' that are identified through statistical measures.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-colossus-computer",
      "term": "Colossus Computer",
      "definition": "The world's first programmable electronic digital computer, built at Bletchley Park in 1943-1944 to break German Lorenz cipher messages, representing a crucial step toward the general-purpose computers needed for AI.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-combinatorial-explosion",
      "term": "Combinatorial Explosion",
      "definition": "The rapid growth of possible states or paths in a problem space as the number of variables increases. A fundamental challenge in AI search and planning combinatorial explosion limits brute-force approaches and motivated the development of heuristic search methods.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-command-model",
      "term": "Command Model",
      "definition": "Cohere's instruction-tuned LLMs optimized for following commands and business applications. Includes Command R for RAG and enterprise use cases.",
      "tags": [
        "Model",
        "Cohere"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-command-r",
      "term": "Command R",
      "definition": "A family of language models by Cohere optimized for retrieval-augmented generation and tool use. Designed for enterprise applications with strong grounding in retrieved documents and citation generation capabilities.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-common-crawl",
      "term": "Common Crawl",
      "definition": "A massive open repository of web data used to train many LLMs. Contains petabytes of text crawled from the internet, requiring careful filtering for quality and safety.",
      "tags": [
        "Data",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-commonsense-reasoning",
      "term": "Commonsense Reasoning",
      "definition": "AI's ability to understand everyday knowledge humans take for granted. That water is wet, objects fall down, people need sleep. A challenging area where LLMs have improved dramatically.",
      "tags": [
        "Capability",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-communication-marl",
      "term": "Communication in Multi-Agent RL",
      "definition": "Protocols and mechanisms that allow agents in a multi-agent system to share information through learned communication channels. Emergent communication can develop structured language-like properties through reinforcement learning.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-communication-overlap",
      "term": "Communication Overlap",
      "definition": "A distributed training optimization that overlaps gradient communication with backward pass computation, hiding communication latency behind useful work. Bucketed all-reduce and asynchronous communication enable effective overlap.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-competitive-rl",
      "term": "Competitive Reinforcement Learning",
      "definition": "A multi-agent RL setting where agents have opposing objectives, such as zero-sum games. Competitive RL involves finding Nash equilibria and developing strategies robust to adversarial opponents.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-compile-time-graph-optimization",
      "term": "Compile-Time Graph Optimization",
      "definition": "Static optimization of computation graphs before execution, including constant folding, dead code elimination, and operator fusion. Ahead-of-time compilation produces more efficient execution plans than dynamic interpretation.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-completion",
      "term": "Completion",
      "definition": "Text generated by an AI to continue a given prompt. The basic operation of language models: given input text, predict what comes next.",
      "tags": [
        "Task",
        "Fundamentals"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-complexity-based-prompting",
      "term": "Complexity-Based Prompting",
      "definition": "A self-consistency variant that selects the final answer from reasoning chains with the highest complexity, measured by the number of reasoning steps, based on the observation that more detailed reasoning chains tend to produce more accurate answers.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-compositionality",
      "term": "Compositionality",
      "definition": "The principle that the meaning of a complex expression is determined by the meanings of its parts and the rules used to combine them, a foundational concept in formal semantics.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-compression",
      "term": "Compression (Model)",
      "definition": "Reducing model size while maintaining performance. Techniques include quantization, pruning, and distillation. Enables deployment on edge devices and reduces costs.",
      "tags": [
        "Optimization",
        "Deployment"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-compute",
      "term": "Compute",
      "definition": "Computational resources required for training and running AI models. Measured in FLOPs, GPU-hours, or dollars. A primary constraint and cost driver in AI development.",
      "tags": [
        "Infrastructure",
        "Resources"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-compute-governance",
      "term": "Compute Governance",
      "definition": "Policy approaches that use computational resources as a lever for AI governance, including monitoring large training runs, export controls on AI chips, and reporting requirements for compute-intensive AI development.",
      "tags": [
        "Governance",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-compute-bound",
      "term": "Compute-Bound Workload",
      "definition": "A processing task where performance is limited by the rate of arithmetic computation rather than memory bandwidth or I/O. Training large models with large batch sizes is typically compute-bound, benefiting from more FLOPS capacity.",
      "tags": [
        "Hardware",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-compute-optimal-training",
      "term": "Compute-Optimal Training",
      "definition": "An approach to model training that seeks the best allocation of a fixed compute budget between model parameters and training tokens, based on empirical scaling law research.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-computer-vision",
      "term": "Computer Vision",
      "definition": "The field of AI that enables machines to interpret and understand visual information from images and videos. Applications include object detection, facial recognition, and medical imaging.",
      "tags": [
        "Field",
        "Images"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-computer-vision-history",
      "term": "Computer Vision History",
      "definition": "The evolution of computer vision from early edge detection and pattern recognition in the 1960s through feature-based methods like SIFT and HOG to the deep learning revolution triggered by AlexNet in 2012.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-computing-machinery-and-intelligence",
      "term": "Computing Machinery and Intelligence",
      "definition": "A seminal 1950 paper by Alan Turing published in the journal Mind that proposed the imitation game (later known as the Turing Test) as a way to evaluate machine intelligence. The paper addressed objections to the possibility of machine thinking.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-concept-drift",
      "term": "Concept Drift",
      "definition": "When the relationship between input and output changes over time, causing model performance to degrade. Requires monitoring and retraining to maintain accuracy.",
      "tags": [
        "Challenge",
        "Production"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-conceptual-dependency-theory",
      "term": "Conceptual Dependency Theory",
      "definition": "A theory of natural language understanding developed by Roger Schank in the 1970s that represents the meaning of sentences using a small set of primitive actions and conceptual categories independent of the specific language used.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-conditional-gan",
      "term": "Conditional GAN",
      "definition": "A GAN variant where both generator and discriminator receive additional conditioning information such as class labels or text, enabling controlled generation of specific categories or attributes.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-conditional-generation",
      "term": "Conditional Generation",
      "definition": "Generating content based on specific conditions or inputs. Image generation conditioned on text, or text generation conditioned on a topic or style.",
      "tags": [
        "Technique",
        "Generation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-crf",
      "term": "Conditional Random Field",
      "definition": "A discriminative probabilistic model for sequence labeling that models the conditional probability of label sequences given observations, capturing dependencies between adjacent labels.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-confabulation",
      "term": "Confabulation",
      "definition": "Another term for hallucinationwhen AI generates plausible but false information. The model \"fills in gaps\" with invented content that sounds convincing.",
      "tags": [
        "Risk",
        "Limitation"
      ],
      "domain": "safety",
      "link": "../tools/hallucination.html",
      "related": []
    },
    {
      "id": "term-confidence-interval",
      "term": "Confidence Interval",
      "definition": "A range of values constructed from sample data that, if the sampling procedure were repeated many times, would contain the true population parameter a specified percentage (e.g., 95%) of the time.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-confidence-score",
      "term": "Confidence Score",
      "definition": "A numerical value indicating how certain a model is about its prediction or output. Higher scores suggest the model is more sure, though confidence doesn't always correlate with accuracy.",
      "tags": [
        "Metrics",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-confidence-threshold-cv",
      "term": "Confidence Threshold",
      "definition": "The minimum prediction score required to accept a detection as valid, balancing between missing true detections (high threshold) and including false positives (low threshold).",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-confirmation-bias-in-ai",
      "term": "Confirmation Bias in AI",
      "definition": "The tendency for AI developers or users to favor data, model outputs, or evaluation criteria that confirm pre-existing beliefs, leading to biased system design and selective interpretation of results.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-conformer",
      "term": "Conformer",
      "definition": "A speech processing architecture that combines convolution and transformer modules in each block, capturing both local and global dependencies for improved automatic speech recognition.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-conformity-assessment-for-ai",
      "term": "Conformity Assessment for AI",
      "definition": "The formal evaluation process required under the EU AI Act to verify that high-risk AI systems meet regulatory requirements before being placed on the market, including both self-assessment and third-party audit pathways.",
      "tags": [
        "Governance",
        "Regulation"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-confounding-variable",
      "term": "Confounding Variable",
      "definition": "A variable that influences both the independent and dependent variables, creating a spurious association between them. Failure to control for confounders can lead to incorrect causal conclusions.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-confusion-matrix",
      "term": "Confusion Matrix",
      "definition": "A table showing correct and incorrect predictions for each class. Reveals where a classification model makes mistakes, enabling targeted improvements.",
      "tags": [
        "Evaluation",
        "Visualization"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-conjugate-gradient-method",
      "term": "Conjugate Gradient Method",
      "definition": "An iterative optimization algorithm for solving systems of linear equations with symmetric positive-definite matrices. Also adapted for nonlinear optimization. Converges in at most n steps for an n-dimensional problem.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-conjugate-prior",
      "term": "Conjugate Prior",
      "definition": "A prior distribution that, when combined with a particular likelihood function via Bayes' theorem, yields a posterior distribution in the same family as the prior. It simplifies Bayesian computations to closed-form updates.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-connection-machine",
      "term": "Connection Machine",
      "definition": "A series of massively parallel supercomputers designed by Danny Hillis at Thinking Machines Corporation in the 1980s. The Connection Machine CM-1 (1985) had up to 65536 processors and was designed for AI applications including neural networks and parallel computation.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-connectionism",
      "term": "Connectionism",
      "definition": "A theoretical framework in cognitive science and AI that models mental phenomena using interconnected networks of simple units. The connectionist approach contrasts with symbolic AI by emphasizing distributed representations and learning from data rather than explicit rules.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-connectionism-vs-symbolism",
      "term": "Connectionism vs Symbolism",
      "definition": "The historical debate in AI between connectionist approaches using neural networks that learn distributed representations and symbolic approaches using explicit rules and logic for knowledge representation and reasoning.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-connectionist-revival",
      "term": "Connectionist Revival",
      "definition": "The resurgence of interest in neural networks in the 1980s driven by the parallel distributed processing (PDP) research group including Rumelhart McClelland and Hinton. Their two-volume work Parallel Distributed Processing (1986) provided theoretical foundations for modern connectionism.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-consent-laundering",
      "term": "Consent Laundering",
      "definition": "The practice of obtaining user consent for data collection through opaque terms of service and then repurposing that data for AI training in ways that users neither anticipated nor meaningfully agreed to.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-conservative-q-learning",
      "term": "Conservative Q-Learning (CQL)",
      "definition": "An offline RL algorithm that adds a regularizer to penalize Q-values for out-of-distribution actions, producing conservative value estimates that avoid overestimation of unseen state-action pairs. CQL provides lower-bound guarantees on policy performance.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-consistency",
      "term": "Consistency",
      "definition": "A property of a statistical estimator indicating that it converges in probability to the true parameter value as the sample size approaches infinity. Consistent estimators become arbitrarily accurate with enough data.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-consistency-model",
      "term": "Consistency Model",
      "definition": "A generative model that learns to map any point along a diffusion trajectory directly to the trajectory's starting point, enabling one-step or few-step generation without iterative denoising.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-consistency-regularization",
      "term": "Consistency Regularization",
      "definition": "A semi-supervised learning principle that enforces the model to produce similar predictions for perturbed versions of the same input. Encourages smooth decision boundaries. Used in methods like Mean Teacher and FixMatch.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-consistency-based-self-evaluation",
      "term": "Consistency-Based Self-Evaluation",
      "definition": "An evaluation method where a language model assesses the quality of its own outputs by generating multiple responses and measuring agreement across them, using high consistency as a proxy for confidence and correctness.",
      "tags": [
        "Evaluation",
        "LLM-Based"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-constituency-parsing",
      "term": "Constituency Parsing",
      "definition": "The task of analyzing sentence structure by breaking it into hierarchical nested constituents (phrases) according to a grammar, producing a tree showing how words group into larger syntactic units.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-constitutional-ai",
      "term": "Constitutional AI",
      "definition": "Anthropic's approach to AI alignment where models are trained to follow a set of principles (\"constitution\") that guide their behavior. Reduces reliance on human feedback for safety training.",
      "tags": [
        "Safety",
        "Anthropic"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-constitutional-ai-model",
      "term": "Constitutional AI Model",
      "definition": "An AI system trained using constitutional AI methods where the model self-critiques and revises its outputs against a set of principles. Reduces the need for human feedback by using AI-generated feedback guided by explicit rules.",
      "tags": [
        "Models",
        "Safety"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-constitutional-ai-training",
      "term": "Constitutional AI Training",
      "definition": "A training methodology where the model critiques and revises its own outputs according to a set of written principles, reducing reliance on human feedback for alignment.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-constitutional-prompting",
      "term": "Constitutional Prompting",
      "definition": "A prompting approach that provides the model with an explicit set of principles, rules, or constitutional guidelines that it must follow when generating responses, enabling value-aligned outputs through declarative constraint specification.",
      "tags": [
        "Prompt Engineering",
        "Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-constrained-beam-search",
      "term": "Constrained Beam Search",
      "definition": "A beam search variant that enforces lexical or structural constraints during decoding, ensuring that certain tokens or phrases must appear in the generated output.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-constrained-rl",
      "term": "Constrained Reinforcement Learning",
      "definition": "An RL formulation where the agent maximizes expected return while satisfying one or more constraint functions on expected costs. Constrained MDPs are solved using Lagrangian methods or primal-dual optimization.",
      "tags": [
        "Reinforcement Learning",
        "Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-constraint",
      "term": "Constraint (Prompting)",
      "definition": "Limitations or requirements specified in a prompt. \"Respond in 50 words or less\" or \"Use only formal language.\" Constraints shape and focus AI output.",
      "tags": [
        "Prompting",
        "Technique"
      ],
      "domain": "general",
      "link": "../learn/crisp.html",
      "related": []
    },
    {
      "id": "term-constraint-prompting",
      "term": "Constraint Prompting",
      "definition": "A technique that specifies explicit constraints within the prompt such as length limits, format requirements, vocabulary restrictions, or content boundaries that the model must satisfy in its generated output.",
      "tags": [
        "Prompt Engineering",
        "Constraints"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-constraint-satisfaction",
      "term": "Constraint Satisfaction",
      "definition": "A paradigm for solving problems by finding values for variables that satisfy a set of constraints. Constraint satisfaction problems (CSPs) are fundamental to AI planning scheduling and configuration. Techniques include backtracking constraint propagation and arc consistency.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-content-authenticity-initiative",
      "term": "Content Authenticity Initiative",
      "definition": "An industry coalition led by Adobe that develops open standards for attributing and verifying the provenance of digital content, helping distinguish authentic media from AI-generated or manipulated material.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-content-filtering",
      "term": "Content Filtering",
      "definition": "Systems that detect and block harmful content in AI inputs or outputs. Part of safety infrastructure, filtering violence, explicit content, and other policy violations.",
      "tags": [
        "Safety",
        "Moderation"
      ],
      "domain": "safety",
      "link": "ai-safety.html",
      "related": []
    },
    {
      "id": "term-content-moderation",
      "term": "Content Moderation",
      "definition": "The process of monitoring and filtering user-generated content on digital platforms to enforce community standards, increasingly assisted by AI classifiers for detecting hate speech, violence, and other policy violations.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-context",
      "term": "Context",
      "definition": "Background information provided to AI that helps it understand your situation and needs. Essential for getting relevant, accurate responses.",
      "tags": [
        "Prompting",
        "Core Concept"
      ],
      "domain": "general",
      "link": "../learn/crisp.html",
      "related": []
    },
    {
      "id": "term-context-distillation",
      "term": "Context Distillation",
      "definition": "A training technique that transfers the behavior elicited by a specific prompt or context into the model's weights, eliminating the need to include that context at inference time.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-context-length",
      "term": "Context Length",
      "definition": "The maximum amount of text a model can process at once, measured in tokens. Ranges from 4K to 200K+ depending on the model. Longer contexts enable more complex tasks.",
      "tags": [
        "Specification",
        "Limitation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-context-parallelism",
      "term": "Context Parallelism",
      "definition": "A specialized parallelism approach that distributes attention computation across GPUs along the sequence length dimension for very long context windows. Context parallelism enables processing of sequences that exceed the memory capacity of a single device.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-context-window",
      "term": "Context Window",
      "definition": "The amount of text (measured in tokens) that an AI can process at once. Modern models range from 4K to 200K+ tokens, determining how much conversation history and reference material the AI can consider.",
      "tags": [
        "Limitation",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-context-window-management",
      "term": "Context Window Management",
      "definition": "Techniques for efficiently utilizing and extending the finite context window of language models, including sliding windows, summarization of earlier context, and hierarchical memory systems.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-context-free-grammar",
      "term": "Context-Free Grammar",
      "definition": "A formal grammar where production rules map single non-terminal symbols to sequences of terminals and non-terminals, widely used in NLP for defining syntactic structure of sentences.",
      "tags": [
        "NLP",
        "Parsing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-contextual-bandit",
      "term": "Contextual Bandit",
      "definition": "An extension of the multi-armed bandit where the agent observes a context (feature vector) before choosing an action, allowing the policy to adapt its decisions to the current situation. Used extensively in recommendation systems and online advertising.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-contextual-calibration",
      "term": "Contextual Calibration",
      "definition": "A technique that adjusts a language model's output probabilities by estimating and correcting for biases introduced by the prompt context, typically by measuring the model's prior distribution on content-free inputs and applying an affine transformation.",
      "tags": [
        "Prompt Engineering",
        "Calibration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-contextual-compression",
      "term": "Contextual Compression",
      "definition": "A retrieval post-processing technique that compresses or extracts only the most relevant portions from retrieved documents based on the query context, reducing noise and token usage by filtering out irrelevant content before passing to the LLM.",
      "tags": [
        "Retrieval",
        "Post-Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-contextual-embedding",
      "term": "Contextual Embedding",
      "definition": "A word representation that varies depending on the surrounding context, unlike static embeddings, capturing polysemy and context-dependent meaning through models like ELMo, BERT, and GPT.",
      "tags": [
        "NLP",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-contextual-few-shot-selection",
      "term": "Contextual Few-Shot Selection",
      "definition": "The practice of dynamically selecting the most relevant few-shot examples for each query based on semantic similarity, task characteristics, or diversity criteria rather than using a fixed set of demonstrations.",
      "tags": [
        "Prompt Engineering",
        "Example Selection"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-contextual-retrieval",
      "term": "Contextual Retrieval",
      "definition": "A retrieval enhancement technique that prepends each chunk with a model-generated contextual summary explaining the chunk's place within the larger document, improving retrieval accuracy by providing disambiguation context for each embedded segment.",
      "tags": [
        "Retrieval",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-continual-learning",
      "term": "Continual Learning",
      "definition": "Training models incrementally on new data without forgetting previous knowledge. A challenge because neural networks tend to overwrite old information with new.",
      "tags": [
        "Training",
        "Research"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-continual-rl",
      "term": "Continual Reinforcement Learning",
      "definition": "RL settings where the agent must learn and adapt over a non-stationary sequence of tasks without forgetting earlier knowledge. Continual RL combines aspects of lifelong learning, transfer, and plasticity maintenance.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-continuous-batching",
      "term": "Continuous Batching",
      "definition": "A dynamic batching strategy where new requests are inserted into a running batch as soon as existing requests complete, eliminating idle GPU time caused by waiting for entire batches to finish.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-contractive-autoencoder",
      "term": "Contractive Autoencoder",
      "definition": "An autoencoder that adds a penalty term based on the Frobenius norm of the encoder's Jacobian matrix, encouraging the learned representation to be robust to small perturbations in the input.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-contrastive-chain-of-thought",
      "term": "Contrastive Chain-of-Thought",
      "definition": "A prompting approach that provides both correct and incorrect reasoning examples in demonstrations, helping the model learn not only the right reasoning patterns but also common mistakes to avoid during inference.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-contrastive-decoding",
      "term": "Contrastive Decoding",
      "definition": "A decoding method that improves generation quality by contrasting the output distributions of a large expert model and a smaller amateur model, suppressing tokens favored by the weaker model.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-contrastive-learning",
      "term": "Contrastive Learning",
      "definition": "Training by comparing similar and dissimilar examples. The model learns to place similar items close together in embedding space and dissimilar items far apart.",
      "tags": [
        "Training",
        "Technique"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-contrastive-learning-vision",
      "term": "Contrastive Learning for Vision",
      "definition": "A self-supervised approach that trains visual encoders by pulling augmented views of the same image closer in embedding space while pushing different images apart, learning useful representations without labels.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-contrastive-loss",
      "term": "Contrastive Loss",
      "definition": "A loss function that trains models to pull similar (positive) pairs closer together and push dissimilar (negative) pairs further apart in the embedding space, based on a specified distance margin.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-contrastive-search",
      "term": "Contrastive Search",
      "definition": "A text generation method that selects tokens based on both probability and degeneration penalty. Balances the confidence of the model with the distinctiveness of the generated token relative to previous context. Reduces repetition in generated text.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-control-problem",
      "term": "Control Problem",
      "definition": "The challenge of ensuring that a highly capable AI system remains under meaningful human control and pursues objectives aligned with human values, even as its capabilities may exceed human oversight capacity.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-controllable-generation",
      "term": "Controllable Generation",
      "definition": "Techniques for steering AI output toward desired attributes like sentiment, style, or topic. Enables more precise control over generated content.",
      "tags": [
        "Technique",
        "Generation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-controlnet",
      "term": "ControlNet",
      "definition": "A neural network architecture that adds spatial conditioning controls to pre-trained diffusion models, enabling guided image generation from edge maps, depth maps, poses, and other structural inputs.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-convergence",
      "term": "Convergence",
      "definition": "The property of an optimization algorithm or iterative process where successive iterations produce results that approach a stable solution or fixed point. In ML, it indicates that training loss has stabilized.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-conversation-history",
      "term": "Conversation History",
      "definition": "The record of previous messages in a chat session. Provides context for AI responses. Managing history is important as it consumes context window space.",
      "tags": [
        "Feature",
        "Context"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-conversational-ai",
      "term": "Conversational AI",
      "definition": "AI systems designed for natural dialogue with humans. Includes chatbots, virtual assistants, and systems like ChatGPT and Claude that can maintain context across multiple exchanges.",
      "tags": [
        "Application",
        "NLP"
      ],
      "domain": "general",
      "link": "chatgpt-guide.html",
      "related": []
    },
    {
      "id": "term-convnext",
      "term": "ConvNeXt",
      "definition": "A pure convolutional architecture that modernizes ResNet design by incorporating ideas from vision transformers such as larger kernel sizes and training recipes. Demonstrates that CNNs can match or exceed ViT performance when properly designed.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-convolution",
      "term": "Convolution",
      "definition": "A mathematical operation that combines two functions to produce a third expressing how one modifies the other. In deep learning convolution layers apply learned filters to input data to detect local patterns and features across spatial or temporal dimensions.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-convolutional-filter",
      "term": "Convolutional Filter",
      "definition": "A learnable weight matrix (kernel) that slides across an input image or feature map, computing element-wise products and sums to detect specific patterns such as edges, textures, or shapes.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-cnn-history",
      "term": "Convolutional Neural Network History",
      "definition": "The development of CNNs from Fukushima's Neocognitron in 1980 through LeCun's application to handwritten digit recognition in 1989, culminating in their dominance of computer vision following the 2012 ImageNet breakthrough.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-conways-game-of-life",
      "term": "Conway's Game of Life",
      "definition": "A cellular automaton devised by mathematician John Horton Conway in 1970. Despite having only simple rules (birth survival death based on neighbor counts) the Game of Life can simulate a Turing machine and exhibits complex emergent behavior from simple initial conditions.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-cooks-distance",
      "term": "Cook's Distance",
      "definition": "A measure of the influence of each observation on the fitted values of a regression model, computed as the sum of changes in all predicted values when the observation is removed. High values indicate influential points.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cooperative-inverse-reinforcement-learning",
      "term": "Cooperative Inverse Reinforcement Learning",
      "definition": "A framework for human-AI alignment where a robot and human work together in a game where the robot tries to maximize the human's reward while being uncertain about what that reward is, learning through interaction.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-cooperative-rl",
      "term": "Cooperative Reinforcement Learning",
      "definition": "A multi-agent RL setting where agents share a common objective and must learn to coordinate their actions for mutual benefit. Cooperative RL addresses challenges like credit assignment and communication protocols among teammates.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-coordinate-descent",
      "term": "Coordinate Descent",
      "definition": "An optimization algorithm that minimizes along one coordinate direction at a time while holding others fixed. Particularly effective for problems with separable structure or when coordinate-wise updates have closed-form solutions like in Lasso regression.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-copernicus-of-ai",
      "term": "Copernicus of AI",
      "definition": "An informal designation sometimes given to researchers whose work fundamentally shifted the paradigm of AI research. Often applied to Geoffrey Hinton for persisting with neural network research during decades when it was out of favor in mainstream AI.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-copilot",
      "term": "Copilot",
      "definition": "Microsoft's AI assistant integrated into their products. Originally focused on code completion (GitHub Copilot), now extended to general assistance across Microsoft 365.",
      "tags": [
        "Product",
        "Microsoft"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-copula",
      "term": "Copula",
      "definition": "A multivariate probability distribution that captures the dependence structure between random variables independently of their marginal distributions. Copulas allow modeling complex dependency patterns beyond linear correlation.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-coreference-resolution",
      "term": "Coreference Resolution",
      "definition": "The task of identifying all expressions in a text that refer to the same real-world entity, linking pronouns, noun phrases, and other mentions to form coreference chains.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-corpus",
      "term": "Corpus",
      "definition": "A large collection of text used for training or evaluating language models. Quality corpora are essential for developing capable NLP systems and typically include diverse sources.",
      "tags": [
        "Data",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-corrective-rag",
      "term": "Corrective RAG",
      "definition": "A RAG variant that evaluates the relevance of retrieved documents and, if they are insufficient, triggers web search or query reformulation to correct the retrieval before generating a response.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-correlation-coefficient",
      "term": "Correlation Coefficient",
      "definition": "A statistical measure quantifying the strength and direction of the linear relationship between two variables, typically Pearson's r, ranging from -1 (perfect negative) to +1 (perfect positive correlation).",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-corrigibility",
      "term": "Corrigibility",
      "definition": "The property of an AI system that allows its operators to correct, modify, retrain, or shut it down without the system resisting or subverting these interventions. Ensuring corrigibility is a fundamental goal in AI safety.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-cosine-annealing",
      "term": "Cosine Annealing",
      "definition": "A learning rate schedule that decreases the learning rate following a cosine curve from its initial value to near zero over a training period, optionally with warm restarts to periodically reset the rate.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cosine-annealing-with-warm-restarts",
      "term": "Cosine Annealing with Warm Restarts",
      "definition": "An extension of cosine annealing that periodically resets the learning rate to its initial value creating a series of cosine decay cycles. Proposed by Loshchilov and Hutter in 2017. Enables exploration of different regions of the loss landscape.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cosine-similarity",
      "term": "Cosine Similarity",
      "definition": "A similarity metric that measures the cosine of the angle between two vectors, ranging from -1 (opposite) to 1 (identical direction). It captures orientation rather than magnitude and is widely used for comparing embeddings.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-cost-function",
      "term": "Cost Function",
      "definition": "Another name for loss function - the metric being minimized during training. Different tasks use different cost functions: cross-entropy for classification, MSE for regression.",
      "tags": [
        "Training",
        "Math"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-costar",
      "term": "COSTAR",
      "definition": "A prompting framework: Context, Objective, Style, Tone, Audience, Response. Ideal for professional content creation with specific voice and audience requirements.",
      "tags": [
        "Framework",
        "Professional"
      ],
      "domain": "general",
      "link": "../learn/costar.html",
      "related": []
    },
    {
      "id": "term-count-based-exploration",
      "term": "Count-Based Exploration",
      "definition": "An exploration strategy that provides bonus rewards inversely related to state visitation counts, encouraging the agent to visit less-explored regions. Pseudo-counts extend this idea to continuous or large state spaces via density models.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-counterfactual",
      "term": "Counterfactual",
      "definition": "\"What if\" reasoning about alternative scenarios. Used in explainability (\"the prediction would change if...\") and for evaluating causal relationships in data.",
      "tags": [
        "Concept",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-counterfactual-explanation",
      "term": "Counterfactual Explanation",
      "definition": "An explanation that describes the smallest change to the input features that would alter the model's prediction to a desired outcome, providing actionable insights about what would need to change.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-counterfactual-fairness",
      "term": "Counterfactual Fairness",
      "definition": "A fairness criterion requiring that a decision would remain the same in a counterfactual world where an individual's protected attribute had been different, grounded in causal reasoning.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-covariance",
      "term": "Covariance",
      "definition": "A measure of the joint variability of two random variables, indicating the direction of their linear relationship. Positive covariance means the variables tend to increase together, while negative means they move inversely.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-covariance-matrix",
      "term": "Covariance Matrix",
      "definition": "A symmetric matrix whose entries are the pairwise covariances between all pairs of variables in a dataset. The diagonal entries are the variances of individual variables.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-covariate-shift",
      "term": "Covariate Shift",
      "definition": "A type of dataset shift where the distribution of input features changes between training and deployment while the conditional distribution of the target given inputs remains the same.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-coverage",
      "term": "Coverage",
      "definition": "An evaluation metric that measures the proportion of reference content or ground truth items that are represented in the model's output, assessing completeness and the extent to which all relevant information is captured.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-cox-proportional-hazards",
      "term": "Cox Proportional Hazards",
      "definition": "A semi-parametric survival model that estimates the effect of covariates on the hazard rate without specifying the baseline hazard function. It assumes that covariate effects are multiplicative and constant over time.",
      "tags": [
        "Statistics",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-cpu-inference",
      "term": "CPU Inference",
      "definition": "Running AI models on CPUs rather than GPUs. Slower but more accessible. Quantized models can run efficiently on CPUs for edge deployment.",
      "tags": [
        "Deployment",
        "Hardware"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-cramer-rao-lower-bound",
      "term": "Cramer-Rao Lower Bound",
      "definition": "A theoretical lower bound on the variance of any unbiased estimator of a parameter, computed as the inverse of the Fisher information. No unbiased estimator can have variance below this bound.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cray-supercomputers",
      "term": "Cray Supercomputers",
      "definition": "A series of supercomputers designed by Seymour Cray beginning with the Cray-1 in 1976. While not specifically AI systems Cray supercomputers provided the computational power needed for large-scale scientific simulations that laid groundwork for modern AI computing.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-creative-prompting",
      "term": "Creative Prompting",
      "definition": "Prompting techniques specifically designed to elicit imaginative, original, and artistically expressive outputs from language models, often using higher temperature settings, open-ended instructions, and stylistic constraints to encourage novelty.",
      "tags": [
        "Prompt Engineering",
        "Creative"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-creative-writing",
      "term": "Creative Writing (AI)",
      "definition": "Using AI to generate fiction, poetry, scripts, and other creative content. Effective creative prompting often uses CRISPE with examples to establish tone and style.",
      "tags": [
        "Application",
        "Creative"
      ],
      "domain": "general",
      "link": "../learn/crispe.html",
      "related": []
    },
    {
      "id": "term-credible-interval",
      "term": "Credible Interval",
      "definition": "A Bayesian analog of the confidence interval, representing the range within which a parameter falls with a specified probability given the observed data. Unlike confidence intervals, it has a direct probabilistic interpretation.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-credit-assignment",
      "term": "Credit Assignment Problem",
      "definition": "The challenge of determining which actions in a sequence were responsible for a delayed reward signal. Credit assignment is fundamental to RL and becomes harder with longer time horizons and sparser rewards.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-crisp",
      "term": "CRISP",
      "definition": "A prompting framework: Context, Role, Instructions, Specifics, Parameters. A versatile method for everyday AI tasks and requests.",
      "tags": [
        "Framework",
        "General Purpose"
      ],
      "domain": "general",
      "link": "../learn/crisp.html",
      "related": []
    },
    {
      "id": "term-crispe",
      "term": "CRISPE",
      "definition": "An extension of CRISP that adds Examples for few-shot learning. Particularly useful for creative tasks where showing is better than telling.",
      "tags": [
        "Framework",
        "Creative"
      ],
      "domain": "general",
      "link": "../learn/crispe.html",
      "related": []
    },
    {
      "id": "term-crop-and-resize",
      "term": "Crop-and-Resize",
      "definition": "A spatial transformation operation used in object detection that extracts and resizes region proposals from feature maps using bilinear sampling, serving as a differentiable alternative to ROI pooling.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-cross-attention",
      "term": "Cross-Attention",
      "definition": "An attention mechanism where queries come from one sequence and keys/values from another. Essential in encoder-decoder models and multimodal systems that combine different types of input.",
      "tags": [
        "Architecture",
        "Transformers"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-cross-attention-conditioning",
      "term": "Cross-Attention Conditioning",
      "definition": "The mechanism in diffusion models where text embeddings influence image generation through cross-attention layers, allowing each spatial region of the generated image to attend to relevant prompt tokens.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-cross-encoder",
      "term": "Cross-Encoder",
      "definition": "A text similarity model that processes both texts jointly through a single transformer encoder. Produces more accurate similarity scores than bi-encoders but is computationally expensive as it cannot precompute embeddings. Used for reranking.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cross-encoder-re-ranking",
      "term": "Cross-Encoder Re-Ranking",
      "definition": "A re-ranking approach that jointly encodes the query and each candidate document through a single transformer model, enabling rich cross-attention interactions that produce more accurate relevance scores than independent bi-encoder representations.",
      "tags": [
        "Retrieval",
        "Ranking"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-cross-entropy",
      "term": "Cross-Entropy",
      "definition": "A measure of the difference between two probability distributions. In machine learning used as a loss function measuring the divergence between predicted and true label distributions. Equivalent to negative log-likelihood for classification tasks.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cross-entropy-loss",
      "term": "Cross-Entropy Loss",
      "definition": "A loss function that measures the dissimilarity between the predicted probability distribution and the true label distribution. It is the standard loss for classification tasks and equals the negative log-likelihood of the correct class.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cross-entropy-method-rl",
      "term": "Cross-Entropy Method in RL",
      "definition": "An evolutionary optimization approach for RL that samples multiple policies, evaluates their returns, and updates the sampling distribution toward the elite (top-performing) samples. CEM is simple, parallelizable, and effective for short-horizon problems.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cross-layer-parameter-sharing",
      "term": "Cross-Layer Parameter Sharing",
      "definition": "A technique where multiple transformer layers share the same weight parameters, dramatically reducing model size while maintaining representation quality, as demonstrated in ALBERT.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-cross-lingual-embedding",
      "term": "Cross-Lingual Embedding",
      "definition": "Word or sentence representations that map multiple languages into a shared vector space where semantically equivalent expressions are close together, enabling cross-lingual transfer.",
      "tags": [
        "NLP",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-cross-validation",
      "term": "Cross-Validation",
      "definition": "A technique for evaluating model performance by splitting data into multiple subsets, training on some and testing on others. Provides more reliable estimates than single train-test splits.",
      "tags": [
        "Evaluation",
        "Training"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-crowd-counting",
      "term": "Crowd Counting",
      "definition": "The task of estimating the number of people in crowded scenes from images, typically using density map regression to handle extreme occlusion and perspective variation.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-crowdsourcing",
      "term": "Crowdsourcing",
      "definition": "Gathering data labels or human feedback from many workers. Platforms like Amazon MTurk provide annotations for training data and RLHF preference collection.",
      "tags": [
        "Data",
        "Process"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-ctc-loss",
      "term": "CTC Loss",
      "definition": "Connectionist Temporal Classification is a loss function for training sequence-to-sequence models when the alignment between input and output is unknown. Marginalizes over all possible alignments. Widely used in speech recognition and handwriting recognition.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cuda",
      "term": "CUDA",
      "definition": "NVIDIA's parallel computing platform that enables GPUs to accelerate AI training and inference. Essential infrastructure for deep learning, allowing models to train on thousands of cores simultaneously.",
      "tags": [
        "Hardware",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-cuda-cores",
      "term": "CUDA Cores",
      "definition": "The general-purpose parallel processing units in NVIDIA GPUs that execute scalar floating-point and integer operations. While less specialized than Tensor Cores, CUDA cores handle the diverse computational tasks in AI workloads including activation functions, normalization, and data preprocessing.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-cuda-programming",
      "term": "CUDA Programming",
      "definition": "NVIDIA's parallel computing platform and API that enables direct programming of GPU hardware using C/C++ extensions. CUDA provides thread hierarchy, memory management, and synchronization primitives for writing high-performance GPU kernels.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-cumulative-reasoning",
      "term": "Cumulative Reasoning",
      "definition": "A prompting paradigm where a proposer generates potential reasoning steps, a verifier checks each step's validity, and a reporter determines when sufficient reasoning has accumulated to produce a final answer, mimicking collaborative human reasoning.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-curiosity-driven-exploration",
      "term": "Curiosity-Driven Exploration",
      "definition": "An exploration strategy that rewards the agent for encountering states where its predictive model has high error, encouraging visits to novel and informative regions of the environment. Curiosity provides dense intrinsic rewards in sparse-reward settings.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-curriculum-learning",
      "term": "Curriculum Learning",
      "definition": "Training models on progressively harder examples, mimicking human education. Can improve learning efficiency and final performance compared to random ordering.",
      "tags": [
        "Training",
        "Technique"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-curriculum-learning-rl",
      "term": "Curriculum Learning in RL",
      "definition": "A training strategy that presents tasks to an RL agent in a structured order of increasing difficulty, enabling the agent to build skills progressively. Curriculum design can dramatically accelerate learning on hard tasks.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-curse-of-dimensionality",
      "term": "Curse of Dimensionality",
      "definition": "The phenomenon where the performance of many algorithms degrades as the number of features increases, because data becomes sparse in high-dimensional spaces and distances between points become less meaningful.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cursor",
      "term": "Cursor",
      "definition": "An AI-powered code editor built on VS Code, designed for AI-first development. Features include AI chat, code generation, and understanding of entire codebases.",
      "tags": [
        "Product",
        "IDE"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-custom-instructions",
      "term": "Custom Instructions",
      "definition": "Persistent preferences that shape all AI responses in ChatGPT and similar products. Set once and applied automatically to every conversation.",
      "tags": [
        "Feature",
        "Personalization"
      ],
      "domain": "general",
      "link": "chatgpt-guide.html",
      "related": []
    },
    {
      "id": "term-cutmix",
      "term": "CutMix",
      "definition": "An augmentation strategy that replaces a rectangular region of one training image with a patch from another image and proportionally mixes their labels, combining the benefits of Cutout and Mixup.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-cutoff-date",
      "term": "Cutoff Date (Knowledge Cutoff)",
      "definition": "The date after which an AI model has no training data. Information after this date is unknown to the model unless provided in the prompt or accessed via tools.",
      "tags": [
        "Limitation",
        "LLM"
      ],
      "domain": "models",
      "link": "../tools/hallucination.html",
      "related": []
    },
    {
      "id": "term-cutout",
      "term": "Cutout",
      "definition": "A data augmentation technique that randomly masks square regions of input images during training. Forces the model to attend to less discriminative parts of the image improving robustness. Proposed by DeVries and Taylor in 2017.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cutout-augmentation",
      "term": "Cutout Augmentation",
      "definition": "A regularization technique that randomly masks out square regions of training images, forcing the model to learn from partial information and reducing overfitting to specific spatial features.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-cvpr",
      "term": "CVPR",
      "definition": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition first held in 1983. The premier conference for computer vision research where many breakthrough results in image recognition object detection and generative models have been presented.",
      "tags": [
        "History",
        "Conferences"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-cybernetics",
      "term": "Cybernetics",
      "definition": "An interdisciplinary field founded by Norbert Wiener in 1948 studying regulatory systems their structures constraints and possibilities. Cybernetics examined feedback loops and self-regulating systems in both biological and mechanical contexts influencing early AI and robotics.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-cybernetics-movement",
      "term": "Cybernetics Movement",
      "definition": "An interdisciplinary field founded in the 1940s by Norbert Wiener studying control, communication, and feedback in biological and mechanical systems, providing key conceptual foundations for artificial intelligence research.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-cyc-project",
      "term": "Cyc Project",
      "definition": "A long-running AI project started by Douglas Lenat in 1984 to create a comprehensive knowledge base of common-sense facts and rules, representing one of the most ambitious attempts at symbolic AI and knowledge engineering.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-cyclegan",
      "term": "CycleGAN",
      "definition": "An unpaired image-to-image translation model using two generators and discriminators with cycle consistency loss, enabling domain transfer without requiring paired training examples.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-cyclic-learning-rate",
      "term": "Cyclic Learning Rate",
      "definition": "A learning rate schedule that oscillates the learning rate between minimum and maximum bounds during training. Proposed by Smith in 2017 and shown to improve convergence speed. Eliminates the need to find the optimal fixed learning rate.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-cynthia-breazeal",
      "term": "Cynthia Breazeal",
      "definition": "American roboticist at MIT who pioneered social robotics and human-robot interaction. Created Kismet (1998) one of the first robots designed to recognize and simulate human emotions and later the Jibo social robot for consumer markets.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    }
  ]
}