{
  "letter": "m",
  "count": 190,
  "terms": [
    {
      "id": "term-machine-learning",
      "term": "Machine Learning (ML)",
      "definition": "A branch of AI where systems learn patterns from data rather than being explicitly programmed. Includes supervised, unsupervised, and reinforcement learning approaches.",
      "tags": [
        "Field",
        "Fundamentals"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-machine-learning-history",
      "term": "Machine Learning History",
      "definition": "The evolution of machine learning from Arthur Samuel's checkers program (1959) through the perceptron (1958) backpropagation (1986) SVMs (1990s) to the deep learning revolution (2012-present). The field progressed from hand-crafted features to learned representations.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mt-evaluation",
      "term": "Machine Translation Evaluation",
      "definition": "Methods for assessing translation quality including automatic metrics like BLEU, METEOR, and COMET that compare system output to reference translations, and human evaluation protocols.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-machine-translation-history",
      "term": "Machine Translation History",
      "definition": "The history of using computers to translate between human languages dating to the Georgetown-IBM experiment in 1954. Early rule-based approaches gave way to statistical methods in the 1990s and neural machine translation in the 2010s culminating in systems like Google Translate.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-machine-unlearning",
      "term": "Machine Unlearning",
      "definition": "Techniques for removing the influence of specific training data from a trained model, motivated by privacy rights such as the right to be forgotten and the need to remove biased or harmful data.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-macy-conferences",
      "term": "Macy Conferences",
      "definition": "A series of interdisciplinary conferences held from 1946 to 1953 that brought together researchers in cybernetics, neuroscience, psychology, and mathematics, fostering cross-disciplinary ideas that influenced the development of AI.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mae",
      "term": "MAE",
      "definition": "Masked Autoencoder, a self-supervised learning method for vision that randomly masks large portions of image patches and trains the model to reconstruct the missing pixels, learning rich visual representations.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-magnitude-pruning",
      "term": "Magnitude Pruning",
      "definition": "A model compression technique that removes weights with the smallest absolute values. Based on the assumption that small weights contribute least to model output. Can be applied as one-shot or iteratively with fine-tuning between rounds.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mahalanobis-distance",
      "term": "Mahalanobis Distance",
      "definition": "A distance metric that accounts for correlations between variables by measuring the number of standard deviations a point is from the mean of a distribution, using the inverse covariance matrix. It is scale-invariant.",
      "tags": [
        "Statistics",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-maieutic-prompting",
      "term": "Maieutic Prompting",
      "definition": "A prompting method inspired by the Socratic maieutic approach that generates a tree of explanations with logical relationships, then uses abductive reasoning to identify the most consistent and truthful answer from the model.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mamba",
      "term": "Mamba",
      "definition": "A selective state space model architecture that uses input-dependent selection mechanisms to efficiently process sequences with linear scaling in sequence length while matching transformer quality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mamba-architecture",
      "term": "Mamba Architecture",
      "definition": "A selective state space model architecture introduced by Albert Gu and Tri Dao in December 2023. Mamba provides an alternative to Transformers with linear-time sequence processing through selective state space mechanisms enabling efficient handling of very long sequences.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-manchester-baby",
      "term": "Manchester Baby",
      "definition": "The Manchester Small-Scale Experimental Machine completed in 1948 at the University of Manchester was the first stored-program computer to run a program. Built by Frederic Williams Tom Kilburn and Geoff Tootill it validated the stored-program concept.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-manhattan-distance",
      "term": "Manhattan Distance",
      "definition": "A distance metric computed as the sum of absolute differences across all dimensions between two points, also known as L1 distance or taxicab distance. It measures distance along axis-aligned paths.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mann-whitney-u-test",
      "term": "Mann-Whitney U Test",
      "definition": "A non-parametric test that compares the distributions of two independent groups by ranking all observations and testing whether one group tends to have larger values. It does not assume normality.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-manual-chain-of-thought",
      "term": "Manual Chain-of-Thought",
      "definition": "The practice of hand-crafting step-by-step reasoning demonstrations within few-shot prompts, where a human explicitly writes out the intermediate reasoning steps for each example to guide the model's problem-solving approach.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mappo",
      "term": "MAPPO",
      "definition": "Multi-Agent Proximal Policy Optimization, an extension of PPO to multi-agent settings that uses shared parameters and a centralized value function. MAPPO achieves strong performance across diverse cooperative multi-agent benchmarks.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-markdown-prompting",
      "term": "Markdown Prompting",
      "definition": "The use of Markdown formatting conventions such as headers, lists, code blocks, and emphasis within prompts to organize instructions and improve model comprehension of prompt structure and output expectations.",
      "tags": [
        "Prompt Engineering",
        "Output Format"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-markov-chain",
      "term": "Markov Chain",
      "definition": "A stochastic model describing a sequence of states where the probability of transitioning to the next state depends only on the current state (the Markov property), not on the sequence of preceding states.",
      "tags": [
        "Machine Learning",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-markov-chain-monte-carlo",
      "term": "Markov Chain Monte Carlo",
      "definition": "A class of algorithms that sample from probability distributions by constructing a Markov chain whose stationary distribution is the target distribution. Common methods include Metropolis-Hastings and Gibbs sampling.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-markov-decision-process",
      "term": "Markov Decision Process (MDP)",
      "definition": "A formal mathematical framework for sequential decision-making defined by states, actions, transition probabilities, and rewards, where the next state depends only on the current state and action (Markov property). MDPs are the standard formalism for RL problems.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-marvin-minsky",
      "term": "Marvin Minsky",
      "definition": "American cognitive scientist and AI pioneer (1927-2016) who co-founded the MIT AI Laboratory, developed the concept of frames for knowledge representation, and authored seminal works on AI and the theory of mind.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mask",
      "term": "Mask / Masking",
      "definition": "Hiding or ignoring certain parts of data during training or inference. In BERT, random tokens are masked for prediction. In transformers, future tokens are masked to maintain causality.",
      "tags": [
        "Technique",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mask-rcnn",
      "term": "Mask R-CNN",
      "definition": "An instance segmentation framework that extends Faster R-CNN by adding a parallel branch for pixel-level mask prediction alongside the existing bounding box regression and classification heads.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-mask2former",
      "term": "Mask2Former",
      "definition": "A universal image segmentation architecture that unifies semantic, instance, and panoptic segmentation through masked attention and learnable object queries processed by a transformer decoder.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-masked-language-modeling",
      "term": "Masked Language Modeling",
      "definition": "A pretraining objective where random tokens in the input are replaced with a mask token and the model learns to predict the original tokens from the surrounding bidirectional context.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-math-benchmark",
      "term": "MATH Benchmark",
      "definition": "A challenging benchmark of 12,500 competition-level mathematics problems spanning seven subjects from algebra to number theory, requiring sophisticated mathematical reasoning and multi-step proof construction from language models.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mathgpt",
      "term": "MathGPT",
      "definition": "A specialized language model designed for mathematical problem solving that combines symbolic and neural approaches. Demonstrates improved accuracy on mathematical reasoning through domain-specific training.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-matrix-multiplication",
      "term": "Matrix Multiplication",
      "definition": "The fundamental algebraic operation of multiplying two matrices used extensively in neural networks for layer computations attention mechanisms and embedding lookups. Computational complexity is O(n^3) for naive implementation with faster algorithms existing.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-matryoshka-embeddings",
      "term": "Matryoshka Embeddings",
      "definition": "An embedding training approach that produces vectors where any prefix of the full embedding is itself a useful embedding, allowing flexible dimensionality reduction without retraining.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-matthews-correlation-coefficient",
      "term": "Matthews Correlation Coefficient",
      "definition": "A balanced classification metric computed from all four confusion matrix values (TP, TN, FP, FN) that produces a value between -1 and +1, where +1 indicates perfect prediction, 0 is random, and -1 is inverse prediction.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-max-pooling",
      "term": "Max Pooling",
      "definition": "A downsampling operation that selects the maximum value within each pooling window, reducing spatial dimensions while retaining the most prominent features detected by convolutional filters.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximal-marginal-relevance",
      "term": "Maximal Marginal Relevance",
      "definition": "A retrieval diversification algorithm (MMR) that iteratively selects documents by balancing relevance to the query against novelty relative to already-selected documents, reducing redundancy in retrieved results through a tunable lambda parameter.",
      "tags": [
        "Retrieval",
        "Diversity"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximum-a-posteriori",
      "term": "Maximum A Posteriori",
      "definition": "A Bayesian point estimation method that finds the parameter values maximizing the posterior probability, combining the likelihood of the data with prior beliefs. Unlike MLE, it incorporates prior information.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximum-entropy-rl",
      "term": "Maximum Entropy RL",
      "definition": "An RL framework that augments the standard return objective with policy entropy, encouraging agents to act as randomly as possible while still achieving high rewards. Maximum entropy RL produces robust policies that maintain diverse behaviors.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximum-likelihood-estimation",
      "term": "Maximum Likelihood Estimation",
      "definition": "A method of estimating the parameters of a statistical model by finding the parameter values that maximize the likelihood function, representing the probability of the observed data given the parameters.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maxout",
      "term": "Maxout",
      "definition": "An activation function that computes the maximum across k linear projections of the input. Generalizes ReLU and Leaky ReLU as special cases. Proposed by Goodfellow et al. in 2013 as a complement to dropout regularization.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mbpp",
      "term": "MBPP",
      "definition": "Mostly Basic Python Programming, a code generation benchmark consisting of approximately 1,000 entry-level Python programming problems with test cases, designed to evaluate a model's ability to synthesize short programs from natural language descriptions.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mccarthys-advice-taker",
      "term": "McCarthy's Advice Taker",
      "definition": "A proposed AI program described by John McCarthy in 1959 that would be able to accept new knowledge in the form of declarative sentences and use logical reasoning to derive conclusions. The proposal influenced the development of logic-based AI and knowledge representation.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mcculloch-pitts-neuron",
      "term": "McCulloch-Pitts Neuron",
      "definition": "The first mathematical model of a biological neuron, proposed by Warren McCulloch and Walter Pitts in 1943, showing that networks of simple binary threshold units could compute any logical function.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mcdiarmids-inequality",
      "term": "McDiarmid's Inequality",
      "definition": "A concentration inequality stating that a function of independent random variables with bounded differences is close to its expected value with high probability. It generalizes Hoeffding's inequality to arbitrary functions.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-absolute-error",
      "term": "Mean Absolute Error",
      "definition": "A regression loss function computed as the average of the absolute differences between predicted and actual values. It is more robust to outliers than mean squared error.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-average-precision",
      "term": "Mean Average Precision",
      "definition": "The primary evaluation metric for object detection (mAP) that computes the average precision across all classes and IoU thresholds, summarizing both localization and classification performance.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-field-rl",
      "term": "Mean Field Reinforcement Learning",
      "definition": "A scalable approach to multi-agent RL that approximates interactions among many agents using a mean field (average effect) of neighboring agents' actions. Mean field methods reduce exponential complexity to tractable computations.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-reciprocal-rank",
      "term": "Mean Reciprocal Rank",
      "definition": "A ranking metric that averages the reciprocal of the rank position of the first relevant result across a set of queries, measuring how quickly a retrieval system surfaces the correct answer.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-shift",
      "term": "Mean Shift",
      "definition": "A non-parametric clustering algorithm that iteratively shifts each data point toward the mode of the local density. Does not require specifying the number of clusters and can discover clusters of arbitrary shape. Uses kernel density estimation.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-squared-error",
      "term": "Mean Squared Error",
      "definition": "A regression loss function computed as the average of the squared differences between predicted and actual values. It penalizes larger errors more heavily due to the squaring operation.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-meaningful-human-control",
      "term": "Meaningful Human Control",
      "definition": "The requirement that humans retain sufficient understanding, authority, and ability to intervene in AI-driven decisions, particularly in high-stakes domains such as military, medical, and judicial applications.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-means-ends-analysis",
      "term": "Means-Ends Analysis",
      "definition": "A problem-solving technique used in AI that identifies the difference between a current state and a goal state then selects actions to reduce that difference. Developed by Newell and Simon for the General Problem Solver it represents one of the earliest AI search strategies.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-measurement-bias",
      "term": "Measurement Bias",
      "definition": "Bias introduced when the features or labels used in an AI system systematically differ in quality or meaning across groups, such as using arrest records as a proxy for criminal behavior.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-mechanical-turk",
      "term": "Mechanical Turk",
      "definition": "An Amazon web service launched in 2005 that allows requesters to post human intelligence tasks (HITs) for workers to complete. Amazon Mechanical Turk became widely used in AI research for data labeling annotation and human evaluation of AI systems.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mechanistic-interpretability",
      "term": "Mechanistic Interpretability",
      "definition": "A research approach that aims to reverse-engineer the learned algorithms inside neural networks by identifying interpretable circuits and features. Studies superposition polysemanticity and circuit-level computations in transformers.",
      "tags": [
        "Algorithms",
        "Technical",
        "Safety"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-med-palm",
      "term": "Med-PaLM",
      "definition": "A medical domain language model by Google based on PaLM with instruction tuning for medical question answering. Med-PaLM 2 achieved expert-level performance on medical licensing examination questions.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-medical-imaging-ai",
      "term": "Medical Imaging AI",
      "definition": "The application of deep learning to medical images (X-rays, CT scans, MRIs, pathology slides) for tasks like disease detection, segmentation of anatomical structures, and treatment planning assistance.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-medusa-decoding",
      "term": "Medusa Decoding",
      "definition": "A parallel decoding method that adds multiple prediction heads to a language model, allowing it to propose and verify several future tokens simultaneously without requiring a separate draft model.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-megatron-lm",
      "term": "Megatron-LM",
      "definition": "NVIDIA's framework for efficient large-scale language model training implementing tensor parallelism, pipeline parallelism, and sequence parallelism optimized for NVIDIA hardware. Megatron-LM provides the parallelism primitives used in many large model training runs.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mel-spectrogram",
      "term": "Mel Spectrogram",
      "definition": "A visual representation of the frequency content of an audio signal over time using the mel scale which approximates human auditory perception. Input representation for many modern speech and audio deep learning models.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mel-frequency-cepstral-coefficients",
      "term": "Mel-Frequency Cepstral Coefficients",
      "definition": "A compact representation of the power spectrum of an audio signal on a perceptually motivated mel frequency scale. Standard feature extraction technique for speech recognition and audio classification. Captures the shape of the vocal tract.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-ai",
      "term": "Memory (AI Systems)",
      "definition": "Mechanisms allowing AI to retain information across conversations. Includes context windows, conversation history, and persistent memory features in some AI assistants.",
      "tags": [
        "Capability",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-bandwidth",
      "term": "Memory Bandwidth",
      "definition": "The rate at which data can be transferred between a processor and its memory, measured in GB/s or TB/s. Memory bandwidth is often the primary bottleneck for large language model inference, where model weights must be loaded from memory for each token.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-management-llm",
      "term": "Memory Management for LLM Inference",
      "definition": "Strategies for efficiently allocating and managing GPU memory during large language model inference, including KV cache management, memory pooling, and dynamic allocation. Effective memory management determines the maximum batch size and sequence length a system can serve.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-network",
      "term": "Memory Network",
      "definition": "A neural architecture with an explicit external memory component that can be read and written during inference. Designed for question answering tasks where the model needs to store and retrieve relevant facts from a knowledge base.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-augmented-neural-network",
      "term": "Memory-Augmented Neural Network",
      "definition": "A broad class of neural architectures equipped with external memory modules that can be read and written using attention-based addressing, enabling reasoning over stored information.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-bound",
      "term": "Memory-Bound Workload",
      "definition": "A processing task where performance is limited by the rate of data transfer between processor and memory rather than compute capability. LLM inference with small batch sizes is memory-bound, benefiting from higher memory bandwidth.",
      "tags": [
        "Hardware",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mesa-optimization",
      "term": "Mesa-Optimization",
      "definition": "A phenomenon where a learned model (the mesa-optimizer) internally develops its own optimization objective that may differ from the base objective it was trained on. This is a key concern in advanced AI safety research.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-mesh-reconstruction",
      "term": "Mesh Reconstruction",
      "definition": "The process of converting 3D point clouds, implicit functions, or depth maps into triangular mesh representations that define surface geometry, topology, and can be rendered or 3D-printed.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-message-passing",
      "term": "Message Passing",
      "definition": "A computational framework in graph neural networks where nodes iteratively exchange and aggregate information with their neighbors. Each node updates its representation based on messages received from connected nodes. Foundation of most GNN architectures.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-message-passing-neural-network",
      "term": "Message Passing Neural Network",
      "definition": "A framework for graph neural networks where nodes iteratively update their representations by exchanging and aggregating messages with neighboring nodes through learned message and update functions.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-meta-llama",
      "term": "Meta LLaMA",
      "definition": "Meta's Large Language Model Meta AI, first released in February 2023 with subsequent versions, representing a major open-weight language model that catalyzed the open-source AI ecosystem.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-meta-learning",
      "term": "Meta-Learning",
      "definition": "Learning how to learn: training models that can quickly adapt to new tasks with few examples. Enables better few-shot and transfer learning capabilities.",
      "tags": [
        "Training",
        "Advanced"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-meta-prompting",
      "term": "Meta-Prompting",
      "definition": "A higher-order prompting approach where a language model is instructed to generate, critique, or improve prompts for itself or other models, effectively using the model as its own prompt engineer.",
      "tags": [
        "Prompt Engineering",
        "Meta-Learning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-meta-rl",
      "term": "Meta-Reinforcement Learning",
      "definition": "RL approaches that learn to learn, enabling rapid adaptation to new tasks by leveraging experience across a distribution of related tasks. Meta-RL agents develop internal adaptation mechanisms that generalize across task variations.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-metadata-filtering",
      "term": "Metadata Filtering",
      "definition": "A vector search technique that applies structured attribute filters alongside similarity search, restricting results to vectors matching specific metadata criteria such as date ranges, categories, or source types before or after distance computation.",
      "tags": [
        "Vector Database",
        "Filtering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-meteor",
      "term": "METEOR",
      "definition": "Metric for Evaluation of Translation with Explicit ORdering, a machine translation evaluation metric that considers synonyms, stemming, and word order in addition to exact word matches.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-meteor-score",
      "term": "METEOR Score",
      "definition": "Metric for Evaluation of Translation with Explicit Ordering evaluates machine translation using unigram matching with stemming synonymy and paraphrase support. Addresses BLEU limitations by considering recall and incorporating linguistic knowledge beyond exact matching.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-metrics",
      "term": "Metrics",
      "definition": "Quantitative measures used to evaluate model performance. Common metrics include accuracy, precision, recall, F1, perplexity, and human evaluation scores.",
      "tags": [
        "Evaluation",
        "Quality"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-metropolis-hastings",
      "term": "Metropolis-Hastings",
      "definition": "An MCMC algorithm that generates samples from a target distribution by proposing candidate points from a proposal distribution and accepting or rejecting them based on an acceptance ratio that ensures detailed balance.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-michael-jordan",
      "term": "Michael Jordan",
      "definition": "American computer scientist and statistician at UC Berkeley known for foundational work in Bayesian machine learning variational inference and graphical models. His research bridging statistics and machine learning influenced modern probabilistic approaches to AI.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-midjourney",
      "term": "Midjourney",
      "definition": "A popular AI image generation service known for artistic, stylized outputs. Accessed through Discord, it's widely used for creative and commercial image creation.",
      "tags": [
        "Product",
        "Image Generation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-midjourney-launch",
      "term": "Midjourney Launch",
      "definition": "The July 2022 public launch of Midjourney, an independent AI art generation service that produces images from text prompts, becoming one of the most popular creative AI tools and sparking debates about AI and artistic creation.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mila-founded",
      "term": "Mila Founded",
      "definition": "The founding of the Montreal Institute for Learning Algorithms (Mila) by Yoshua Bengio. Mila became one of the world's largest academic research centers for deep learning attracting top researchers and contributing to Montreal's emergence as an AI hub.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-milvus",
      "term": "Milvus",
      "definition": "An open-source vector database built for scalable similarity search that supports multiple index types, hybrid search, and multi-tenancy, capable of handling billion-scale vector datasets with a distributed architecture.",
      "tags": [
        "Vector Database",
        "Open Source"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-min-max-scaling",
      "term": "Min-Max Scaling",
      "definition": "A normalization technique that linearly rescales features to a fixed range, typically [0, 1], by subtracting the minimum value and dividing by the range. It preserves the shape of the original distribution.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-minerva",
      "term": "Minerva",
      "definition": "A language model by Google fine-tuned for mathematical reasoning on a dataset of scientific papers and web pages containing mathematical content. Achieves strong performance on quantitative reasoning benchmarks without calculator access.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-minhash",
      "term": "MinHash",
      "definition": "A locality-sensitive hashing technique that efficiently estimates the Jaccard similarity between sets, widely used in NLP for approximate nearest neighbor search and document deduplication.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mini-batch-gradient-descent",
      "term": "Mini-Batch Gradient Descent",
      "definition": "A gradient-based optimization method that computes parameter updates using a small random subset (mini-batch) of the training data at each step, balancing the stability of full-batch gradient descent with the speed of stochastic gradient descent.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-minimax-algorithm",
      "term": "Minimax Algorithm",
      "definition": "A decision-making algorithm for two-player zero-sum games that minimizes the possible loss for a worst-case scenario. Used in game-playing AI since the 1950s minimax forms the basis for game tree search in chess checkers and other adversarial games.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-minimum-bayes-risk-decoding",
      "term": "Minimum Bayes Risk Decoding",
      "definition": "A decoding strategy that selects the output candidate minimizing expected loss across a set of sampled hypotheses, often producing higher-quality translations and summaries than beam search.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-minimum-description-length",
      "term": "Minimum Description Length",
      "definition": "A model selection principle that selects the model minimizing the total description length of the data and the model itself. It formalizes Occam's razor using information-theoretic concepts.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-minkowski-distance",
      "term": "Minkowski Distance",
      "definition": "A generalized distance metric parameterized by p that includes Manhattan (p=1), Euclidean (p=2), and Chebyshev (p=infinity) distances as special cases. It computes the p-th root of the sum of p-th powers of absolute differences.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-minsky-papert-perceptrons",
      "term": "Minsky and Papert Perceptrons",
      "definition": "The 1969 book by Marvin Minsky and Seymour Papert that mathematically demonstrated the limitations of single-layer perceptrons, contributing to reduced funding for neural network research and the first AI winter.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mirror-descent",
      "term": "Mirror Descent",
      "definition": "A generalization of gradient descent that uses Bregman divergences instead of Euclidean distance for parameter updates. Naturally handles constrained optimization and non-Euclidean parameter spaces. Useful for optimization on simplices and other manifolds.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mirror-prompting",
      "term": "Mirror Prompting",
      "definition": "A prompting approach that instructs the model to first restate the user's request back in its own words, confirming mutual understanding before proceeding with task execution, reducing misalignment between user intent and model interpretation.",
      "tags": [
        "Prompt Engineering",
        "Clarification"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mish",
      "term": "Mish",
      "definition": "A self-regularizing non-monotonic activation function defined as f(x) = x * tanh(softplus(x)). Known for smooth gradients and strong empirical performance particularly in computer vision tasks such as object detection.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-misinformation",
      "term": "Misinformation",
      "definition": "False or inaccurate information shared without deliberate intent to deceive, which can be amplified by AI recommendation systems and generated inadvertently through AI hallucinations.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral",
      "term": "Mistral",
      "definition": "A French AI company known for efficient, high-performance open models. Their Mistral and Mixtral models offer strong capabilities with smaller parameter counts.",
      "tags": [
        "Company",
        "Model"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral-7b",
      "term": "Mistral 7B",
      "definition": "A 7 billion parameter language model by Mistral AI that outperforms larger models through architectural innovations including grouped query attention and sliding window attention. Efficient and powerful for its size.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral-ai",
      "term": "Mistral AI",
      "definition": "A French AI company founded in 2023 by former Google DeepMind and Meta researchers including Arthur Mensch. Mistral AI developed efficient open-source language models including Mistral 7B and Mixtral demonstrating that smaller well-trained models can compete with larger ones.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral-ai-founding",
      "term": "Mistral AI Founding",
      "definition": "The founding of Mistral AI in April 2023 by former Google DeepMind and Meta researchers in Paris, which rapidly became a leading European AI company releasing competitive open-weight language models.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mit-ai-laboratory",
      "term": "MIT AI Laboratory",
      "definition": "A research laboratory co-founded by Marvin Minsky and John McCarthy at MIT in 1959, which became one of the most influential AI research centers, producing foundational work in vision, robotics, and natural language understanding.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixed-precision-training",
      "term": "Mixed Precision Training",
      "definition": "A training technique that uses lower-precision floating-point formats (FP16 or BF16) for forward and backward passes while maintaining FP32 master copies of weights for accumulation. Mixed precision approximately doubles throughput and halves memory usage with minimal accuracy impact.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixmatch",
      "term": "MixMatch",
      "definition": "A semi-supervised learning method that combines consistency regularization entropy minimization and MixUp augmentation. Produces sharpened pseudo-labels for unlabeled data and mixes labeled and unlabeled examples for training.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixtral",
      "term": "Mixtral",
      "definition": "A mixture-of-experts model by Mistral AI that uses sparse MoE layers with 8 experts per layer routing each token to 2 experts. Achieves performance comparable to much larger dense models at a fraction of the compute cost.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixtral-8x7b",
      "term": "Mixtral 8x7B",
      "definition": "A specific configuration of the Mixtral mixture-of-experts model with 8 experts of 7 billion parameters each. Routes each token through 2 experts resulting in computational cost similar to a 13B parameter dense model.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixture-of-agents",
      "term": "Mixture of Agents",
      "definition": "An architecture where multiple specialized LLM agents collaborate on a task, with each agent contributing expertise in a specific domain and a router or aggregator combining their outputs.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixture-of-depths",
      "term": "Mixture of Depths",
      "definition": "A transformer variant that learns to dynamically allocate computation by routing only a subset of tokens through each transformer block, reducing total computation while maintaining model quality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-moe-inference",
      "term": "Mixture of Experts Inference",
      "definition": "Inference optimization for Mixture of Experts models where only a subset of expert parameters are activated per token, reducing computation despite the large total parameter count. MoE inference requires efficient expert routing and memory management.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixture-of-experts-layer",
      "term": "Mixture of Experts Layer",
      "definition": "A neural network layer consisting of multiple expert subnetworks and a gating mechanism that routes each input to a sparse subset of experts, enabling massive model capacity with sublinear computational cost.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-moe-routing",
      "term": "Mixture of Experts Routing",
      "definition": "The gating mechanism in MoE models that determines which expert subnetworks process each input, using learned routing functions to achieve efficient sparse computation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixup",
      "term": "Mixup",
      "definition": "A data augmentation and regularization technique that creates virtual training examples by taking convex combinations of pairs of training examples and their labels, encouraging linear behavior between training points.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mlops",
      "term": "MLOps",
      "definition": "Practices for deploying and maintaining ML models in production. Combines ML, DevOps, and data engineering to ensure reliable, scalable AI systems.",
      "tags": [
        "Operations",
        "Production"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mmlu",
      "term": "MMLU (Massive Multitask Language Understanding)",
      "definition": "A comprehensive benchmark testing language models on 57 subjects from STEM to humanities. Widely used to compare model capabilities on knowledge-intensive tasks.",
      "tags": [
        "Benchmark",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mnist-dataset",
      "term": "MNIST Dataset",
      "definition": "The Modified National Institute of Standards and Technology database of handwritten digits created by Yann LeCun and colleagues in 1998. MNIST became the standard benchmark for machine learning algorithms and is often called the hello world of deep learning.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mobile-inference",
      "term": "Mobile Inference",
      "definition": "AI inference optimized for smartphones and tablets, leveraging mobile GPU, NPU, or DSP capabilities. Mobile inference frameworks like TensorFlow Lite and Core ML apply aggressive quantization and operator fusion for on-device model execution.",
      "tags": [
        "Inference Infrastructure",
        "Hardware"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-mobilenet",
      "term": "MobileNet",
      "definition": "A family of lightweight CNN architectures designed for mobile and embedded devices that use depthwise separable convolutions to dramatically reduce computation and model size.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mobilenetv2",
      "term": "MobileNetV2",
      "definition": "An improved mobile architecture that introduces inverted residual blocks with linear bottlenecks. The inverted residual expands channels before the depthwise convolution then projects back to a thin representation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mobilenetv3",
      "term": "MobileNetV3",
      "definition": "The third generation mobile architecture combining hardware-aware neural architecture search with NetAdapt. Uses squeeze-and-excitation modules and h-swish activation for improved accuracy per computation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model",
      "term": "Model",
      "definition": "The trained AI system that processes inputs and generates outputs. Models are defined by their architecture, size (parameters), training data, and fine-tuning.",
      "tags": [
        "Core Concept",
        "Fundamentals"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-averaging",
      "term": "Model Averaging",
      "definition": "An ensemble technique that combines predictions from multiple models by averaging their outputs. Can use uniform weights or learned weights. Simple yet effective at reducing prediction variance and improving generalization.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-card",
      "term": "Model Card",
      "definition": "Documentation describing a model's intended use, limitations, performance metrics, and ethical considerations. A standard practice for responsible AI development and deployment.",
      "tags": [
        "Documentation",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-cards",
      "term": "Model Cards",
      "definition": "Standardized documentation artifacts proposed by Mitchell et al. (2019) that accompany trained ML models and report on their intended use, performance characteristics, limitations, and ethical considerations.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-collapse",
      "term": "Model Collapse",
      "definition": "A degradation phenomenon where models trained on AI-generated data lose diversity and quality over generations. A growing concern as synthetic data becomes more prevalent.",
      "tags": [
        "Risk",
        "Training"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-compression",
      "term": "Model Compression",
      "definition": "A family of techniques for reducing model size and computational cost while preserving performance, including quantization, pruning, distillation, and low-rank factorization. Model compression enables deployment of large models on resource-constrained devices.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-distillation",
      "term": "Model Distillation",
      "definition": "The process of training a smaller student model to replicate the behavior of a larger teacher model by learning from the teacher's output probability distributions rather than hard labels alone.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-distillation-history",
      "term": "Model Distillation History",
      "definition": "The development of knowledge distillation from the original concept by Hinton Vinyals and Dean (2015) where a smaller student model learns to mimic a larger teacher model. Distillation has become essential for deploying large models on resource-constrained devices.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mfu",
      "term": "Model FLOPs Utilization (MFU)",
      "definition": "The ratio of observed model FLOPS to the theoretical peak FLOPS of the hardware, measuring how efficiently the training system utilizes available compute. MFU above 50% is considered good for large-scale training on modern GPU clusters.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-merging",
      "term": "Model Merging",
      "definition": "A technique that combines the weights of two or more fine-tuned models into a single model, often using methods like linear interpolation, SLERP, or TIES, to inherit capabilities from multiple specializations.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-parallelism",
      "term": "Model Parallelism",
      "definition": "A distributed training strategy that splits a model's layers or parameters across multiple devices, enabling training of models too large to fit in the memory of a single accelerator.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-predictive-control-rl",
      "term": "Model Predictive Control in RL",
      "definition": "A planning-based approach that uses a learned dynamics model to simulate action sequences forward and selects the first action of the best sequence. MPC re-plans at every step, making it robust to model errors.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-pruning",
      "term": "Model Pruning",
      "definition": "A compression technique that removes redundant weights or neurons from a neural network based on magnitude, sensitivity, or other criteria. Pruning reduces model size and computation while attempting to preserve accuracy through fine-tuning.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-serving",
      "term": "Model Serving",
      "definition": "The infrastructure and systems for deploying trained models to handle real-time prediction requests at scale. Model serving encompasses load balancing, request batching, model versioning, and health monitoring for production AI systems.",
      "tags": [
        "Inference Infrastructure",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-sharding",
      "term": "Model Sharding",
      "definition": "The technique of partitioning a large model's parameters across multiple devices or storage locations, enabling inference and training of models that exceed the memory capacity of a single accelerator.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-based-rl",
      "term": "Model-Based RL",
      "definition": "RL approaches that learn or use a model of the environment's transition dynamics and reward function to plan actions or generate synthetic experience. Model-based methods can be more sample-efficient but require accurate models.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-free-rl",
      "term": "Model-Free RL",
      "definition": "RL algorithms that learn policies or value functions directly from experience without building an explicit model of the environment. Model-free methods are simpler and more broadly applicable but typically require more interaction data.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-modern-hopfield-network",
      "term": "Modern Hopfield Network",
      "definition": "An updated Hopfield network formulation using exponential interaction functions that connects to transformer attention mechanisms and provides exponential storage capacity compared to classical Hopfield networks.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixture-of-experts",
      "term": "MoE (Mixture of Experts)",
      "definition": "An architecture where different \"expert\" sub-networks specialize in different types of inputs. Enables larger effective model capacity while keeping computation manageable.",
      "tags": [
        "Architecture",
        "Efficiency"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-moe-llava",
      "term": "MoE-LLaVA",
      "definition": "A multimodal large language model that uses mixture-of-experts to efficiently scale visual instruction tuning. Activates only a subset of parameters for each input reducing computation while maintaining strong multimodal understanding.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-momentum",
      "term": "Momentum",
      "definition": "An optimization technique that accelerates gradient descent by accumulating an exponentially decaying moving average of past gradients, helping the optimizer move faster along consistent gradient directions and dampen oscillations.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-monte-carlo-dropout",
      "term": "Monte Carlo Dropout",
      "definition": "An approximate Bayesian inference technique that uses dropout at inference time to generate multiple stochastic predictions. The variance of predictions provides uncertainty estimates. Theoretically connected to Gaussian processes.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-monte-carlo-method",
      "term": "Monte Carlo Method",
      "definition": "A broad class of computational algorithms that use repeated random sampling to obtain numerical results, such as estimating integrals, simulating complex systems, or approximating probability distributions.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-monte-carlo-methods-rl",
      "term": "Monte Carlo Methods in RL",
      "definition": "RL algorithms that estimate value functions by averaging the actual returns observed over complete episodes. Unlike TD methods, Monte Carlo approaches require no bootstrapping and wait until the end of an episode to update.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-monte-carlo-tree-search",
      "term": "Monte Carlo Tree Search (MCTS)",
      "definition": "A search algorithm that builds a decision tree through random simulations, using statistics from previous rollouts to guide exploration toward promising actions. MCTS powers game-playing systems like AlphaGo and is used for planning in complex domains.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-montreal-declaration-responsible-ai",
      "term": "Montreal Declaration for Responsible AI",
      "definition": "A declaration adopted in 2018 establishing principles for responsible AI development including well-being, respect for autonomy, privacy, democratic participation, equity, diversity, and prudence.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-moores-law",
      "term": "Moore's Law",
      "definition": "An observation by Intel co-founder Gordon Moore in 1965 that the number of transistors on a microchip doubles approximately every two years. Moore's Law has driven the exponential increase in computing power that has enabled modern AI capabilities.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-moral-status-of-ai",
      "term": "Moral Status of AI",
      "definition": "The philosophical question of whether AI systems can possess moral standing, such that their interests or welfare deserve ethical consideration. Closely tied to debates about AI consciousness and sentience.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-moravecs-paradox",
      "term": "Moravec's Paradox",
      "definition": "The observation by Hans Moravec and others in the 1980s that high-level reasoning tasks are easy for AI while sensorimotor skills that seem simple to humans are extremely difficult to replicate computationally.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-morpheme",
      "term": "Morpheme",
      "definition": "The smallest meaningful unit of language that cannot be further divided without losing its meaning, including roots, prefixes, suffixes, and inflectional endings.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-morphology",
      "term": "Morphology",
      "definition": "The branch of linguistics studying the internal structure of words, including how morphemes combine to form words through inflection, derivation, and compounding processes.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mosaic-augmentation",
      "term": "Mosaic Augmentation",
      "definition": "A data augmentation technique that combines four training images into a single mosaic image, allowing the model to learn from multiple contexts simultaneously and detect objects at various scales.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-motivational-control",
      "term": "Motivational Control",
      "definition": "Safety measures that shape an AI system's goals and values to be aligned with human interests, as opposed to capability control which restricts what the system can physically do.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-movement-pruning",
      "term": "Movement Pruning",
      "definition": "A pruning method that removes weights based on their movement during fine-tuning rather than their magnitude. Weights that move toward zero are pruned. More effective than magnitude pruning for pretrained models being fine-tuned.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-moving-average-model",
      "term": "Moving Average Model",
      "definition": "A time series model where the current value is expressed as a linear combination of the current and past white noise error terms. It captures short-term dependencies in the data.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mpt",
      "term": "MPT",
      "definition": "MosaicML Pretrained Transformer is a family of open-source language models trained by MosaicML using ALiBi position embeddings and FlashAttention. MPT-7B was notable for its efficient training and permissive commercial license.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mt-bench",
      "term": "MT-Bench",
      "definition": "Multi-Turn Benchmark, an evaluation framework that tests language models' conversational abilities across multi-turn dialogues with follow-up questions, using LLM judges to score responses on writing, reasoning, coding, and knowledge tasks.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-agent-rl",
      "term": "Multi-Agent Reinforcement Learning",
      "definition": "RL involving multiple agents that interact within a shared environment, each with its own observations and objectives. MARL introduces challenges of non-stationarity, credit assignment, and emergent communication.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-agent-systems",
      "term": "Multi-Agent Systems",
      "definition": "Systems composed of multiple interacting intelligent agents that can cooperate compete and coordinate to solve problems. Research in multi-agent systems draws on game theory distributed computing and AI and has applications in robotics simulation and complex problem solving.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-armed-bandit",
      "term": "Multi-Armed Bandit",
      "definition": "A simplified RL problem where an agent repeatedly chooses among K actions (arms) to maximize cumulative reward, with no state transitions. Bandit problems isolate the exploration-exploitation tradeoff from sequential decision-making.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-gpu-training",
      "term": "Multi-GPU Training",
      "definition": "Training a model using multiple GPUs simultaneously within a single node or across nodes, requiring parallelism strategies and gradient synchronization. Multi-GPU training is essential for large models and datasets that exceed single-GPU capacity or time constraints.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-head-attention",
      "term": "Multi-Head Attention",
      "definition": "An extension of attention that runs multiple attention operations in parallel, each focusing on different aspects. A key component of transformer architectures.",
      "tags": [
        "Architecture",
        "Transformers"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mig",
      "term": "Multi-Instance GPU (MIG)",
      "definition": "An NVIDIA feature that partitions a single GPU into up to seven isolated instances, each with dedicated compute, memory, and cache resources. MIG enables secure multi-tenant GPU sharing for inference workloads with guaranteed quality of service.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-label-classification",
      "term": "Multi-Label Classification",
      "definition": "A classification task where each instance can belong to multiple classes simultaneously, unlike multi-class classification where each instance has exactly one label. Examples include document tagging and image annotation.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-layer-perceptron",
      "term": "Multi-Layer Perceptron",
      "definition": "A feedforward neural network with one or more hidden layers between input and output. Uses nonlinear activation functions enabling it to learn non-linear decision boundaries. Trained with backpropagation. The classic deep learning architecture.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-object-tracking",
      "term": "Multi-Object Tracking",
      "definition": "The task of simultaneously tracking multiple objects through a video sequence, handling challenges like occlusion, identity switches, and objects entering or leaving the scene.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-objective-rl",
      "term": "Multi-Objective RL",
      "definition": "RL formulations where the agent must optimize multiple potentially conflicting reward functions simultaneously. Solutions involve Pareto-optimal policies, scalarization methods, or constraint-based approaches.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-persona-prompting",
      "term": "Multi-Persona Prompting",
      "definition": "A technique that assigns multiple distinct expert personas within a single prompt, having each persona contribute their specialized perspective to a problem and then synthesizing their viewpoints into a comprehensive response.",
      "tags": [
        "Prompt Engineering",
        "Persona"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-query-attention",
      "term": "Multi-Query Attention",
      "definition": "An attention variant where all query heads share a single set of key and value projections, significantly reducing memory bandwidth requirements during autoregressive decoding.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-query-retrieval",
      "term": "Multi-Query Retrieval",
      "definition": "A technique that generates multiple paraphrased or perspective-shifted versions of the original query using an LLM, retrieves documents for each variant, and combines the results to overcome the sensitivity of retrieval to specific query phrasings.",
      "tags": [
        "Retrieval",
        "Query Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-scale-feature-extraction",
      "term": "Multi-Scale Feature Extraction",
      "definition": "The technique of capturing features at different spatial resolutions or receptive field sizes within a network, enabling detection and recognition of objects at various scales.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-scale-testing",
      "term": "Multi-Scale Testing",
      "definition": "An evaluation technique that processes an image at multiple resolutions and combines the predictions, improving detection of objects at various scales at the cost of increased inference time.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-step-bootstrapping",
      "term": "Multi-Step Bootstrapping",
      "definition": "A value estimation approach that uses n actual rewards before bootstrapping with a value estimate for the remaining future, interpolating between one-step TD and Monte Carlo methods. Multi-step bootstrapping controls the bias-variance tradeoff in value learning.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-step-reasoning",
      "term": "Multi-Step Reasoning",
      "definition": "A prompting paradigm that breaks complex problems into a sequence of intermediate reasoning steps, requiring the model to solve each sub-problem before proceeding to the next, enabling accurate solutions to problems that exceed single-step capability.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-task-learning",
      "term": "Multi-Task Learning",
      "definition": "A learning approach where a model is trained simultaneously on multiple related tasks, sharing representations across tasks. It can improve generalization by leveraging shared structure and acting as an implicit regularizer.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-task-rl",
      "term": "Multi-Task Reinforcement Learning",
      "definition": "RL approaches that train a single policy to perform well across multiple related tasks simultaneously. Multi-task RL leverages shared structure to improve sample efficiency and develop more general capabilities.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-tenancy-vector-databases",
      "term": "Multi-Tenancy in Vector Databases",
      "definition": "The ability of a vector database to serve multiple isolated users or applications from a shared infrastructure, using namespaces, partitions, or metadata filtering to ensure data separation while maintaining efficient resource utilization.",
      "tags": [
        "Vector Database",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-turn-conversation",
      "term": "Multi-Turn Conversation",
      "definition": "A dialogue format where a language model maintains context across multiple exchanges with a user, requiring the model to track conversation history, resolve references, and maintain coherence.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-vector-retrieval",
      "term": "Multi-Vector Retrieval",
      "definition": "A retrieval approach that represents each document as multiple embedding vectors rather than a single vector, capturing different aspects or segments of the document and enabling finer-grained matching at the cost of increased storage and computation.",
      "tags": [
        "Retrieval",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-view-stereo",
      "term": "Multi-View Stereo",
      "definition": "A 3D reconstruction method that computes dense depth maps from multiple calibrated camera views, using photometric consistency to establish correspondences across many viewpoints.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-word-expression",
      "term": "Multi-Word Expression",
      "definition": "A combination of words that exhibits lexical, syntactic, semantic, or statistical idiosyncrasy, including idioms, compound nouns, phrasal verbs, and collocations.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multicollinearity",
      "term": "Multicollinearity",
      "definition": "A condition in regression analysis where two or more independent variables are highly correlated, making it difficult to determine the individual effect of each predictor and inflating standard errors of coefficient estimates.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multidimensional-scaling",
      "term": "Multidimensional Scaling",
      "definition": "A dimensionality reduction technique that positions points in low-dimensional space such that pairwise distances approximate the original high-dimensional distances. Used for visualization and analysis of similarity or dissimilarity data.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multilingual-model",
      "term": "Multilingual Model",
      "definition": "A single model trained on data from multiple languages that can perform NLP tasks across those languages, often developing cross-lingual transfer abilities from shared representations.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multimodal",
      "term": "Multimodal",
      "definition": "AI systems that can process and generate multiple types of content (text, images, audio, video). Examples include GPT-4V, Gemini, and Claude with vision capabilities.",
      "tags": [
        "Capability",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multimodal-ai-history",
      "term": "Multimodal AI History",
      "definition": "The development of AI systems that can process and generate multiple types of data (text images audio video). From early separate modality systems to unified multimodal models like GPT-4V and Gemini that natively handle diverse input and output types.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-multinomial-distribution",
      "term": "Multinomial Distribution",
      "definition": "A generalization of the binomial distribution for experiments with more than two possible outcomes. It models the counts of each outcome across a fixed number of independent trials.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multiple-testing-correction",
      "term": "Multiple Testing Correction",
      "definition": "Statistical methods for adjusting significance thresholds when performing many simultaneous hypothesis tests to control the overall error rate. Common methods include Bonferroni, Holm, and Benjamini-Hochberg.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-musiclm",
      "term": "MusicLM",
      "definition": "A music generation model by Google that generates high-fidelity music from text descriptions. Uses a hierarchical sequence-to-sequence approach conditioned on text and melody embeddings. Produces coherent music over several minutes.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mutual-information",
      "term": "Mutual Information",
      "definition": "A measure of the statistical dependence between two random variables, quantifying how much knowing one variable reduces uncertainty about the other. It is used in feature selection and clustering evaluation.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mutual-information-feature-selection",
      "term": "Mutual Information Feature Selection",
      "definition": "A filter-based feature selection method that ranks features by their mutual information with the target variable, measuring the reduction in uncertainty about the target provided by knowing each feature's value.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-muzero",
      "term": "MuZero",
      "definition": "A model-based RL algorithm that learns a latent dynamics model, reward predictor, and value/policy networks without requiring knowledge of the game rules. MuZero plans using its learned model and achieves superhuman performance across diverse domains.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mycin",
      "term": "MYCIN",
      "definition": "An early expert system developed at Stanford in the 1970s for diagnosing bacterial infections and recommending antibiotics, demonstrating that rule-based AI could match or exceed human expert performance in narrow domains.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    }
  ]
}