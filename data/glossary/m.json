{
  "letter": "m",
  "count": 387,
  "terms": [
    {
      "id": "term-m2m-100",
      "term": "M2M-100",
      "definition": "A multilingual machine translation model from Meta that can translate directly between any pair of 100 languages without relying on English as a pivot.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-m3gnet",
      "term": "M3GNet",
      "definition": "A universal graph neural network potential for materials that learns interatomic interactions across the periodic table for molecular dynamics simulations.",
      "tags": [
        "Models",
        "Scientific"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mace",
      "term": "MACE",
      "definition": "Multi-ACE is a higher-order equivariant message passing neural network for atomistic simulations that achieves high accuracy with computational efficiency.",
      "tags": [
        "Models",
        "Scientific"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-machine-learning",
      "term": "Machine Learning (ML)",
      "definition": "A branch of AI where systems learn patterns from data rather than being explicitly programmed. Includes supervised, unsupervised, and reinforcement learning approaches.",
      "tags": [
        "Field",
        "Fundamentals"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-machine-learning-history",
      "term": "Machine Learning History",
      "definition": "The evolution of machine learning from Arthur Samuel's checkers program (1959) through the perceptron (1958) backpropagation (1986) SVMs (1990s) to the deep learning revolution (2012-present). The field progressed from hand-crafted features to learned representations.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mt-evaluation",
      "term": "Machine Translation Evaluation",
      "definition": "Methods for assessing translation quality including automatic metrics like BLEU, METEOR, and COMET that compare system output to reference translations, and human evaluation protocols.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-machine-translation-history",
      "term": "Machine Translation History",
      "definition": "The history of using computers to translate between human languages dating to the Georgetown-IBM experiment in 1954. Early rule-based approaches gave way to statistical methods in the 1990s and neural machine translation in the 2010s culminating in systems like Google Translate.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-machine-unlearning",
      "term": "Machine Unlearning",
      "definition": "Techniques for removing the influence of specific training data from a trained model, motivated by privacy rights such as the right to be forgotten and the need to remove biased or harmful data.",
      "tags": [
        "Privacy",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-macy-conferences",
      "term": "Macy Conferences",
      "definition": "A series of interdisciplinary conferences held from 1946 to 1953 that brought together researchers in cybernetics, neuroscience, psychology, and mathematics, fostering cross-disciplinary ideas that influenced the development of AI.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-madlad-400",
      "term": "MADLAD-400",
      "definition": "A large-scale multilingual machine translation model from Google trained on web-crawled data in over 400 languages for broad translation coverage.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mae",
      "term": "MAE",
      "definition": "Masked Autoencoder, a self-supervised learning method for vision that randomly masks large portions of image patches and trains the model to reconstruct the missing pixels, learning rich visual representations.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-magic3d",
      "term": "Magic3D",
      "definition": "A two-stage text-to-3D generation framework from NVIDIA that creates high-resolution 3D meshes using coarse-to-fine optimization with diffusion priors.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-magicanimate",
      "term": "MagicAnimate",
      "definition": "A diffusion-based model for human image animation that generates temporally consistent videos from a reference image and motion sequence.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-magicbrush",
      "term": "MagicBrush",
      "definition": "An instruction-guided image editing model trained on manually annotated editing triplets for precise text-guided local and global image modifications.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-magicdrive",
      "term": "MagicDrive",
      "definition": "A street-view generation model conditioned on 3D geometry and road maps for creating diverse and controllable driving scene images for training data.",
      "tags": [
        "Models",
        "Technical",
        "Autonomous",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-magicoder-oss-instruct",
      "term": "Magicoder OSS-Instruct",
      "definition": "A dataset of coding instruction examples generated by prompting LLMs with real open-source code snippets. Produces more realistic and diverse coding instructions than pure synthesis.",
      "tags": [
        "Training Corpus",
        "Code"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-magnetic-core-memory",
      "term": "Magnetic Core Memory",
      "definition": "Early computer memory technology using tiny magnetic ferrite cores threaded on wires. Dominated computer memory from the mid-1950s through the 1970s before semiconductor DRAM replaced it.",
      "tags": [
        "Historical",
        "Memory",
        "Technology"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-magnetic-ram",
      "term": "Magnetic RAM",
      "definition": "Non-volatile memory technology using magnetic states to store data with DRAM-like speed. Promising for AI applications requiring fast persistent memory without wear-out limitations of flash.",
      "tags": [
        "Memory",
        "Emerging",
        "Non-Volatile"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-magnetic-tape-storage",
      "term": "Magnetic Tape Storage",
      "definition": "Sequential data storage medium using magnetic-coated tape. Used for computer data storage since the 1950s and still employed today for archival backup of large datasets.",
      "tags": [
        "Historical",
        "Storage",
        "Archival"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-magnitude-pruning",
      "term": "Magnitude Pruning",
      "definition": "A model compression technique that removes weights with the smallest absolute values. Based on the assumption that small weights contribute least to model output. Can be applied as one-shot or iteratively with fine-tuning between rounds.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mahalanobis-distance",
      "term": "Mahalanobis Distance",
      "definition": "A distance metric that accounts for correlations between variables by measuring the number of standard deviations a point is from the mean of a distribution, using the inverse covariance matrix. It is scale-invariant.",
      "tags": [
        "Statistics",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-maieutic-prompting",
      "term": "Maieutic Prompting",
      "definition": "A prompting method inspired by the Socratic maieutic approach that generates a tree of explanations with logical relationships, then uses abductive reasoning to identify the most consistent and truthful answer from the model.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-make-a-scene",
      "term": "Make-A-Scene",
      "definition": "A text-to-image generation model from Meta that incorporates optional scene layouts as additional conditioning for controllable image generation.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-make-a-video",
      "term": "Make-A-Video",
      "definition": "A text-to-video generation model from Meta AI that extends text-to-image diffusion models to generate short video clips from text descriptions.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-make-an-audio",
      "term": "Make-An-Audio",
      "definition": "A text-to-audio generation framework that uses a latent diffusion model with temporal-aware conditioning for generating diverse audio from text descriptions.",
      "tags": [
        "Models",
        "Technical",
        "Audio"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-malicious-use-of-ai",
      "term": "Malicious Use of AI",
      "definition": "The deliberate use of AI systems to cause harm including creating weapons surveillance tools disinformation campaigns and tools for cyberattacks fraud or harassment.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-mamba",
      "term": "Mamba",
      "definition": "A selective state space model architecture that uses input-dependent selection mechanisms to efficiently process sequences with linear scaling in sequence length while matching transformer quality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mamba-architecture",
      "term": "Mamba Architecture",
      "definition": "A selective state space model architecture introduced by Albert Gu and Tri Dao in December 2023. Mamba provides an alternative to Transformers with linear-time sequence processing through selective state space mechanisms enabling efficient handling of very long sequences.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mamba-2",
      "term": "Mamba-2",
      "definition": "An improved state space model architecture with a refined selective scan mechanism that increases hardware utilization and training throughput over the original Mamba.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mamba-codestral",
      "term": "Mamba-Codestral",
      "definition": "A code-generation model from Mistral AI that uses the Mamba state space architecture for efficient code completion and generation tasks.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-manchester-baby",
      "term": "Manchester Baby",
      "definition": "The Manchester Small-Scale Experimental Machine completed in 1948 at the University of Manchester was the first stored-program computer to run a program. Built by Frederic Williams Tom Kilburn and Geoff Tootill it validated the stored-program concept.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mandatory-reporting-for-ai",
      "term": "Mandatory Reporting for AI",
      "definition": "Legal requirements for organizations to report AI-related incidents failures or safety concerns to regulatory authorities. Analogous to mandatory reporting in healthcare aviation and financial services.",
      "tags": [
        "Safety",
        "Policy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-manhattan-distance",
      "term": "Manhattan Distance",
      "definition": "A distance metric computed as the sum of absolute differences across all dimensions between two points, also known as L1 distance or taxicab distance. It measures distance along axis-aligned paths.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mann-whitney-u-test",
      "term": "Mann-Whitney U Test",
      "definition": "A non-parametric test that compares the distributions of two independent groups by ranking all observations and testing whether one group tends to have larger values. It does not assume normality.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mantis",
      "term": "Mantis",
      "definition": "A multimodal model designed for multi-image reasoning that can compare and analyze relationships across multiple input images simultaneously.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-manual-chain-of-thought",
      "term": "Manual Chain-of-Thought",
      "definition": "The practice of hand-crafting step-by-step reasoning demonstrations within few-shot prompts, where a human explicitly writes out the intermediate reasoning steps for each example to guide the model's problem-solving approach.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-map-neo",
      "term": "MAP-Neo",
      "definition": "A fully open-source bilingual language model with transparent training data and code that aims for complete reproducibility in LLM research.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mappo",
      "term": "MAPPO",
      "definition": "Multi-Agent Proximal Policy Optimization, an extension of PPO to multi-agent settings that uses shared parameters and a centralized value function. MAPPO achieves strong performance across diverse cooperative multi-agent benchmarks.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mappo-algorithm",
      "term": "MAPPO Algorithm",
      "definition": "Multi-Agent Proximal Policy Optimization adapts PPO for multi-agent settings with a shared policy or centralized value function. Achieves strong performance across diverse cooperative multi-agent benchmarks.",
      "tags": [
        "Algorithms",
        "Technical",
        "RL"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mapreduce-algorithm",
      "term": "MapReduce Algorithm",
      "definition": "A distributed programming model for processing large datasets across a cluster by splitting work into independent map tasks and combining results with reduce tasks. Automatically handles parallelization and fault tolerance.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maptr",
      "term": "MapTR",
      "definition": "A structured end-to-end Transformer for online vectorized HD map construction that predicts map elements as point sets for autonomous driving.",
      "tags": [
        "Models",
        "Technical",
        "Autonomous",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-marching-cubes-algorithm",
      "term": "Marching Cubes Algorithm",
      "definition": "A surface extraction algorithm that generates a triangle mesh from a three-dimensional scalar field. Examines each cube of eight neighboring voxels and creates triangles based on a lookup table of 256 configurations.",
      "tags": [
        "Algorithms",
        "Technical",
        "Vision"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-marigold",
      "term": "Marigold",
      "definition": "A diffusion-based monocular depth estimation model that repurposes pre-trained latent diffusion models for zero-shot affine-invariant depth prediction.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-markdown-prompting",
      "term": "Markdown Prompting",
      "definition": "The use of Markdown formatting conventions such as headers, lists, code blocks, and emphasis within prompts to organize instructions and improve model comprehension of prompt structure and output expectations.",
      "tags": [
        "Prompt Engineering",
        "Output Format"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-markov-chain",
      "term": "Markov Chain",
      "definition": "A stochastic model describing a sequence of states where the probability of transitioning to the next state depends only on the current state (the Markov property), not on the sequence of preceding states.",
      "tags": [
        "Machine Learning",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-markov-chain-monte-carlo",
      "term": "Markov Chain Monte Carlo",
      "definition": "A class of algorithms that sample from probability distributions by constructing a Markov chain whose stationary distribution is the target distribution. Common methods include Metropolis-Hastings and Gibbs sampling.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-markov-decision-process",
      "term": "Markov Decision Process (MDP)",
      "definition": "A formal mathematical framework for sequential decision-making defined by states, actions, transition probabilities, and rewards, where the next state depends only on the current state and action (Markov property). MDPs are the standard formalism for RL problems.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-marvin-minsky",
      "term": "Marvin Minsky",
      "definition": "American cognitive scientist and AI pioneer (1927-2016) who co-founded the MIT AI Laboratory, developed the concept of frames for knowledge representation, and authored seminal works on AI and the theory of mind.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-masakhaner",
      "term": "MasakhaNER",
      "definition": "A named entity recognition dataset for 10 African languages. Part of the Masakhane initiative to advance NLP for African languages with community-driven annotation.",
      "tags": [
        "Benchmark",
        "NLP",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mask",
      "term": "Mask / Masking",
      "definition": "Hiding or ignoring certain parts of data during training or inference. In BERT, random tokens are masked for prediction. In transformers, future tokens are masked to maintain causality.",
      "tags": [
        "Technique",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mask-rcnn",
      "term": "Mask R-CNN",
      "definition": "An instance segmentation framework that extends Faster R-CNN by adding a parallel branch for pixel-level mask prediction alongside the existing bounding box regression and classification heads.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-mask2former",
      "term": "Mask2Former",
      "definition": "A universal image segmentation architecture that unifies semantic, instance, and panoptic segmentation through masked attention and learnable object queries processed by a transformer decoder.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-masked-autoencoder-mae",
      "term": "Masked Autoencoder (MAE)",
      "definition": "A self-supervised vision pre-training method that masks random patches of an image and trains an autoencoder to reconstruct the missing patches.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "Fundamentals"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-masked-language-modeling",
      "term": "Masked Language Modeling",
      "definition": "A pretraining objective where random tokens in the input are replaced with a mask token and the model learns to predict the original tokens from the surrounding bidirectional context.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-massive",
      "term": "MASSIVE",
      "definition": "Multilingual Amazon SLURP for Slot filling Intents and Virtual assistant Evaluation a dataset covering 51 languages with 1 million annotated utterances for natural language understanding.",
      "tags": [
        "Benchmark",
        "NLP",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-matcha",
      "term": "MatCha",
      "definition": "A mathematical chart understanding model from Google that pre-trains on chart derendering and numerical reasoning for visual math question answering.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-matched-filter",
      "term": "Matched Filter",
      "definition": "A signal processing filter that maximizes the signal-to-noise ratio for detecting a known signal in additive noise. The impulse response is the time-reversed conjugate of the expected signal template.",
      "tags": [
        "Algorithms",
        "Technical",
        "Signal Processing"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-math",
      "term": "MATH",
      "definition": "A dataset of 12500 competition mathematics problems from AMC AIME and other competitions. Tests advanced mathematical reasoning including algebra geometry number theory and calculus.",
      "tags": [
        "Benchmark",
        "NLP",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-math-benchmark",
      "term": "MATH Benchmark",
      "definition": "A challenging benchmark of 12,500 competition-level mathematics problems spanning seven subjects from algebra to number theory, requiring sophisticated mathematical reasoning and multi-step proof construction from language models.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mathgpt",
      "term": "MathGPT",
      "definition": "A specialized language model designed for mathematical problem solving that combines symbolic and neural approaches. Demonstrates improved accuracy on mathematical reasoning through domain-specific training.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mathinstruct",
      "term": "MathInstruct",
      "definition": "A curated dataset of mathematical problem-solving demonstrations for instruction tuning language models on mathematical reasoning across diverse problem types.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mathstral",
      "term": "Mathstral",
      "definition": "A mathematics-specialized language model from Mistral AI optimized for mathematical reasoning and problem-solving with strong performance on math benchmarks.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mathverse",
      "term": "MathVerse",
      "definition": "A multimodal math benchmark testing visual mathematical reasoning with problems that require understanding of mathematical figures and diagrams.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mathvista",
      "term": "MathVista",
      "definition": "A multimodal mathematical reasoning benchmark combining visual context with mathematical problems. Tests the ability to perform mathematical reasoning about charts diagrams and figures.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-matrix-exponentiation",
      "term": "Matrix Exponentiation",
      "definition": "A technique for computing the exponential of a matrix used in solving systems of linear differential equations and computing graph path counts. Methods include Pade approximation and scaling-and-squaring.",
      "tags": [
        "Algorithms",
        "Technical",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-matrix-factorization",
      "term": "Matrix Factorization",
      "definition": "A recommendation technique that decomposes a user-item interaction matrix into lower-dimensional latent factor matrices to predict missing ratings.",
      "tags": [
        "Models",
        "Fundamentals",
        "Recommendation"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-matrix-multiplication",
      "term": "Matrix Multiplication",
      "definition": "The fundamental algebraic operation of multiplying two matrices used extensively in neural networks for layer computations attention mechanisms and embedding lookups. Computational complexity is O(n^3) for naive implementation with faster algorithms existing.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-matryoshka-embeddings",
      "term": "Matryoshka Embeddings",
      "definition": "An embedding training approach that produces vectors where any prefix of the full embedding is itself a useful embedding, allowing flexible dimensionality reduction without retraining.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-matterport3d",
      "term": "Matterport3D",
      "definition": "A large RGB-D dataset of 90 building-scale scenes with surface reconstructions and semantic annotations. Provides photorealistic 3D environments for indoor scene understanding.",
      "tags": [
        "Benchmark",
        "3D",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-matthews-correlation-coefficient",
      "term": "Matthews Correlation Coefficient",
      "definition": "A balanced classification metric computed from all four confusion matrix values (TP, TN, FP, FN) that produces a value between -1 and +1, where +1 indicates perfect prediction, 0 is random, and -1 is inverse prediction.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mavil",
      "term": "MAViL",
      "definition": "Masked Audio-Visual Learner is a self-supervised model that learns audio-visual representations through masked prediction across both audio and video modalities.",
      "tags": [
        "Models",
        "Technical",
        "Audio",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-max-pooling",
      "term": "Max Pooling",
      "definition": "A downsampling operation that selects the maximum value within each pooling window, reducing spatial dimensions while retaining the most prominent features detected by convolutional filters.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximal-independent-set-algorithm",
      "term": "Maximal Independent Set Algorithm",
      "definition": "An algorithm that finds a maximal set of vertices in a graph such that no two are adjacent. Luby's parallel algorithm computes this in O(log n) rounds and is fundamental in distributed computing.",
      "tags": [
        "Algorithms",
        "Technical",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximal-marginal-relevance",
      "term": "Maximal Marginal Relevance",
      "definition": "A retrieval diversification algorithm (MMR) that iteratively selects documents by balancing relevance to the query against novelty relative to already-selected documents, reducing redundancy in retrieved results through a tunable lambda parameter.",
      "tags": [
        "Retrieval",
        "Diversity"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximum-a-posteriori",
      "term": "Maximum A Posteriori",
      "definition": "A Bayesian point estimation method that finds the parameter values maximizing the posterior probability, combining the likelihood of the data with prior beliefs. Unlike MLE, it incorporates prior information.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximum-a-posteriori-policy-optimization",
      "term": "Maximum a Posteriori Policy Optimization",
      "definition": "An RL algorithm that frames policy improvement as supervised learning by optimizing a lower bound on the expected return. Decouples sample collection from policy optimization and supports both discrete and continuous actions.",
      "tags": [
        "Algorithms",
        "Technical",
        "RL"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximum-bipartite-matching",
      "term": "Maximum Bipartite Matching",
      "definition": "An algorithm that finds the largest set of edges in a bipartite graph such that no two edges share a vertex. The Hopcroft-Karp algorithm solves this in O(E * sqrt(V)) time.",
      "tags": [
        "Algorithms",
        "Technical",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximum-entropy-rl",
      "term": "Maximum Entropy RL",
      "definition": "An RL framework that augments the standard return objective with policy entropy, encouraging agents to act as randomly as possible while still achieving high rewards. Maximum entropy RL produces robust policies that maintain diverse behaviors.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximum-flow-algorithm",
      "term": "Maximum Flow Algorithm",
      "definition": "A class of algorithms that find the maximum amount of flow that can be sent from a source to a sink in a flow network. The max-flow min-cut theorem states that maximum flow equals the minimum cut capacity.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximum-likelihood-estimation",
      "term": "Maximum Likelihood Estimation",
      "definition": "A method of estimating the parameters of a statistical model by finding the parameter values that maximize the likelihood function, representing the probability of the observed data given the parameters.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximum-variance-unfolding",
      "term": "Maximum Variance Unfolding",
      "definition": "A semidefinite programming approach to dimensionality reduction that maximizes the variance of the embedding while preserving local distances. Provides a global optimum unlike many iterative nonlinear methods.",
      "tags": [
        "Algorithms",
        "Technical",
        "Dimensionality Reduction"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maximum-weighted-independent-set",
      "term": "Maximum Weighted Independent Set",
      "definition": "An algorithm that finds the largest-weight subset of non-adjacent vertices in a graph. NP-hard in general but solvable in polynomial time on trees and interval graphs using dynamic programming.",
      "tags": [
        "Algorithms",
        "Technical",
        "Graph",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maxout",
      "term": "Maxout",
      "definition": "An activation function that computes the maximum across k linear projections of the input. Generalizes ReLU and Leaky ReLU as special cases. Proposed by Goodfellow et al. in 2013 as a complement to dropout regularization.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maxq-decomposition",
      "term": "MAXQ Decomposition",
      "definition": "A hierarchical reinforcement learning method that decomposes the value function of an MDP into a hierarchy of subtask value functions. Enables modular learning where subtask policies can be learned independently.",
      "tags": [
        "Algorithms",
        "Technical",
        "RL"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-maxvit",
      "term": "MaxViT",
      "definition": "A multi-axis vision Transformer that combines blocked local attention with dilated global attention for efficiently capturing both local and global visual features.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mbart",
      "term": "mBART",
      "definition": "A multilingual sequence-to-sequence model pre-trained by denoising full texts in 25 languages for machine translation and cross-lingual generation.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mbpp",
      "term": "MBPP",
      "definition": "Mostly Basic Python Programming, a code generation benchmark consisting of approximately 1,000 entry-level Python programming problems with test cases, designed to evaluate a model's ability to synthesize short programs from natural language descriptions.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mc4",
      "term": "mC4",
      "definition": "Multilingual C4 a cleaned Common Crawl corpus spanning 101 languages used to train the mT5 multilingual language model. Extends C4 filtering heuristics to non-English web text.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mccarthys-advice-taker",
      "term": "McCarthy's Advice Taker",
      "definition": "A proposed AI program described by John McCarthy in 1959 that would be able to accept new knowledge in the form of declarative sentences and use logical reasoning to derive conclusions. The proposal influenced the development of logic-based AI and knowledge representation.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mcculloch-pitts-neuron",
      "term": "McCulloch-Pitts Neuron",
      "definition": "The first mathematical model of a biological neuron, proposed by Warren McCulloch and Walter Pitts in 1943, showing that networks of simple binary threshold units could compute any logical function.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mcdiarmids-inequality",
      "term": "McDiarmid's Inequality",
      "definition": "A concentration inequality stating that a function of independent random variables with bounded differences is close to its expected value with high probability. It generalizes Hoeffding's inequality to arbitrary functions.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-md-judge",
      "term": "MD-Judge",
      "definition": "A safety evaluation model that judges whether language model responses are harmful or safe based on multi-dimensional safety criteria and rubrics.",
      "tags": [
        "Models",
        "Technical",
        "NLP",
        "Safety"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-absolute-error",
      "term": "Mean Absolute Error",
      "definition": "A regression loss function computed as the average of the absolute differences between predicted and actual values. It is more robust to outliers than mean squared error.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-average-precision",
      "term": "Mean Average Precision",
      "definition": "The primary evaluation metric for object detection (mAP) that computes the average precision across all classes and IoU thresholds, summarizing both localization and classification performance.",
      "tags": [
        "Computer Vision",
        "Object Detection"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-field-rl",
      "term": "Mean Field Reinforcement Learning",
      "definition": "A scalable approach to multi-agent RL that approximates interactions among many agents using a mean field (average effect) of neighboring agents' actions. Mean field methods reduce exponential complexity to tractable computations.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-reciprocal-rank",
      "term": "Mean Reciprocal Rank",
      "definition": "A ranking metric that averages the reciprocal of the rank position of the first relevant result across a set of queries, measuring how quickly a retrieval system surfaces the correct answer.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-shift",
      "term": "Mean Shift",
      "definition": "A non-parametric clustering algorithm that iteratively shifts each data point toward the mode of the local density. Does not require specifying the number of clusters and can discover clusters of arbitrary shape. Uses kernel density estimation.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-shift-clustering",
      "term": "Mean Shift Clustering",
      "definition": "A non-parametric clustering algorithm that iteratively shifts each point toward the mode of its local density. The number of clusters is determined automatically by the number of distinct convergence points.",
      "tags": [
        "Algorithms",
        "Technical",
        "Clustering"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-shift-segmentation",
      "term": "Mean Shift Segmentation",
      "definition": "An iterative algorithm that shifts each data point to the mode of its local density using kernel density estimation. Used in image segmentation to group pixels with similar color and spatial properties.",
      "tags": [
        "Algorithms",
        "Technical",
        "Vision",
        "Clustering"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mean-squared-error",
      "term": "Mean Squared Error",
      "definition": "A regression loss function computed as the average of the squared differences between predicted and actual values. It penalizes larger errors more heavily due to the squaring operation.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-meaningful-human-control",
      "term": "Meaningful Human Control",
      "definition": "The requirement that humans retain sufficient understanding, authority, and ability to intervene in AI-driven decisions, particularly in high-stakes domains such as military, medical, and judicial applications.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-means-ends-analysis",
      "term": "Means-Ends Analysis",
      "definition": "A problem-solving technique used in AI that identifies the difference between a current state and a goal state then selects actions to reduce that difference. Developed by Newell and Simon for the General Problem Solver it represents one of the earliest AI search strategies.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-measurement-bias",
      "term": "Measurement Bias",
      "definition": "Bias introduced when the features or labels used in an AI system systematically differ in quality or meaning across groups, such as using arrest records as a proxy for criminal behavior.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-mechanical-turk",
      "term": "Mechanical Turk",
      "definition": "An Amazon web service launched in 2005 that allows requesters to post human intelligence tasks (HITs) for workers to complete. Amazon Mechanical Turk became widely used in AI research for data labeling annotation and human evaluation of AI systems.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mechanistic-interpretability",
      "term": "Mechanistic Interpretability",
      "definition": "A research approach that aims to reverse-engineer the learned algorithms inside neural networks by identifying interpretable circuits and features. Studies superposition polysemanticity and circuit-level computations in transformers.",
      "tags": [
        "Algorithms",
        "Technical",
        "Safety"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-med-gemini",
      "term": "Med-Gemini",
      "definition": "A family of medical AI models from Google built on Gemini that excel at medical reasoning and multimodal clinical tasks.",
      "tags": [
        "Models",
        "Technical",
        "NLP",
        "Medical",
        "Products"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-med-palm",
      "term": "Med-PaLM",
      "definition": "A medical domain language model by Google based on PaLM with instruction tuning for medical question answering. Med-PaLM 2 achieved expert-level performance on medical licensing examination questions.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-medalpaca",
      "term": "MedAlpaca",
      "definition": "A medical language model fine-tuned from LLaMA on curated medical question-answer datasets for clinical and biomedical question answering.",
      "tags": [
        "Models",
        "Technical",
        "NLP",
        "Medical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-medclip",
      "term": "MedCLIP",
      "definition": "A contrastive learning model that aligns medical images with clinical text descriptions for zero-shot and few-shot medical image classification.",
      "tags": [
        "Models",
        "Technical",
        "Medical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-median-filter-algorithm",
      "term": "Median Filter Algorithm",
      "definition": "A nonlinear digital filter that replaces each sample with the median of neighboring samples within a window. Effective at removing salt-and-pepper noise while preserving edges better than linear filters.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Signal Processing"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mediasum",
      "term": "MediaSum",
      "definition": "A large-scale media interview summarization dataset containing 463000 transcripts from NPR and CNN with topic descriptions as summaries. Tests summarization of spoken media content.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mediatek-dimensity-9300",
      "term": "MediaTek Dimensity 9300",
      "definition": "MediaTek flagship mobile processor featuring an integrated APU (AI Processing Unit) for on-device generative AI and large language model inference on smartphones.",
      "tags": [
        "Mobile",
        "MediaTek",
        "SoC"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-medical-imaging-ai",
      "term": "Medical Imaging AI",
      "definition": "The application of deep learning to medical images (X-rays, CT scans, MRIs, pathology slides) for tasks like disease detection, segmentation of anatomical structures, and treatment planning assistance.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-meditron",
      "term": "Meditron",
      "definition": "A suite of open-source medical language models from EPFL built on Llama 2 with continued pre-training on curated medical guidelines and literature.",
      "tags": [
        "Models",
        "Technical",
        "NLP",
        "Medical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-medmcqa",
      "term": "MedMCQA",
      "definition": "A large-scale multiple-choice medical QA dataset containing over 194000 questions from Indian medical entrance exams. Covers 2400 healthcare topics across 21 medical subjects.",
      "tags": [
        "Benchmark",
        "NLP",
        "Medical"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-medqa",
      "term": "MedQA",
      "definition": "A medical question answering dataset derived from professional medical licensing examinations including USMLE. Tests clinical reasoning and medical knowledge in AI systems.",
      "tags": [
        "Benchmark",
        "NLP",
        "Medical"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-medsam",
      "term": "MedSAM",
      "definition": "Medical Segment Anything Model applies the SAM architecture to medical image segmentation across diverse imaging modalities and anatomical structures.",
      "tags": [
        "Models",
        "Technical",
        "Medical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-medusa-decoding",
      "term": "Medusa Decoding",
      "definition": "A parallel decoding method that adds multiple prediction heads to a language model, allowing it to propose and verify several future tokens simultaneously without requiring a separate draft model.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-megaface",
      "term": "MegaFace",
      "definition": "A face recognition benchmark containing over one million face images from 690000 individuals. Designed to evaluate face recognition at scale testing algorithms against a million-scale distractor set.",
      "tags": [
        "Benchmark",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-megatron-lm",
      "term": "Megatron-LM",
      "definition": "NVIDIA's framework for efficient large-scale language model training implementing tensor parallelism, pipeline parallelism, and sequence parallelism optimized for NVIDIA hardware. Megatron-LM provides the parallelism primitives used in many large model training runs.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-megnet",
      "term": "MEGNet",
      "definition": "MatErials Graph Network uses graph neural networks to predict molecular and crystal properties by learning element-level and structure-level representations.",
      "tags": [
        "Models",
        "Scientific"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mel-scale-algorithm",
      "term": "Mel Scale Algorithm",
      "definition": "A perceptual scale of pitches where equal distances correspond to equal perceived pitch differences. Used to compute mel-frequency cepstral coefficients by applying triangular filter banks on a mel-frequency axis.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Signal Processing"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mel-spectrogram",
      "term": "Mel Spectrogram",
      "definition": "A visual representation of the frequency content of an audio signal over time using the mel scale which approximates human auditory perception. Input representation for many modern speech and audio deep learning models.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mel-frequency-cepstral-coefficients",
      "term": "Mel-Frequency Cepstral Coefficients",
      "definition": "A compact representation of the power spectrum of an audio signal on a perceptually motivated mel frequency scale. Standard feature extraction technique for speech recognition and audio classification. Captures the shape of the vocal tract.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-membership-inference-attack",
      "term": "Membership Inference Attack",
      "definition": "An attack that determines whether a specific data record was used in training a machine learning model. Poses privacy risks by revealing information about training data membership.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-memetic-algorithm",
      "term": "Memetic Algorithm",
      "definition": "A hybrid optimization approach that combines a population-based global search (such as a genetic algorithm) with local search applied to individual solutions. Named after Richard Dawkins' concept of memes as units of cultural evolution.",
      "tags": [
        "Algorithms",
        "Technical",
        "Metaheuristic"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-ai",
      "term": "Memory (AI Systems)",
      "definition": "Mechanisms allowing AI to retain information across conversations. Includes context windows, conversation history, and persistent memory features in some AI assistants.",
      "tags": [
        "Capability",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-bandwidth",
      "term": "Memory Bandwidth",
      "definition": "The rate at which data can be transferred between a processor and its memory, measured in GB/s or TB/s. Memory bandwidth is often the primary bottleneck for large language model inference, where model weights must be loaded from memory for each token.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-controller",
      "term": "Memory Controller",
      "definition": "Hardware component that manages the flow of data between the processor and main memory. In AI accelerators optimized memory controllers are critical for sustaining high bandwidth utilization.",
      "tags": [
        "Memory",
        "Architecture"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-management-llm",
      "term": "Memory Management for LLM Inference",
      "definition": "Strategies for efficiently allocating and managing GPU memory during large language model inference, including KV cache management, memory pooling, and dynamic allocation. Effective memory management determines the maximum batch size and sequence length a system can serve.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-network",
      "term": "Memory Network",
      "definition": "A neural architecture with an explicit external memory component that can be read and written during inference. Designed for question answering tasks where the model needs to store and retrieve relevant facts from a knowledge base.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-pooling",
      "term": "Memory Pooling",
      "definition": "Technique of aggregating memory from multiple devices into a shared pool accessible by all processors. Enabled by CXL technology to address memory capacity limitations in AI workloads.",
      "tags": [
        "Memory",
        "Architecture"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-utilization-gpu",
      "term": "Memory Utilization (GPU)",
      "definition": "Percentage of GPU memory currently allocated by applications. Monitoring memory utilization is critical for optimizing batch sizes and model configurations in AI training.",
      "tags": [
        "Performance",
        "GPU",
        "Metric"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-wall",
      "term": "Memory Wall",
      "definition": "The growing disparity between processor speed and memory access speed that limits overall system performance. A fundamental challenge in AI hardware design driving innovations like HBM and on-chip memory.",
      "tags": [
        "Memory",
        "Performance",
        "Fundamentals"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-augmented-neural-network",
      "term": "Memory-Augmented Neural Network",
      "definition": "A broad class of neural architectures equipped with external memory modules that can be read and written using attention-based addressing, enabling reasoning over stored information.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-memory-bound",
      "term": "Memory-Bound Workload",
      "definition": "A processing task where performance is limited by the rate of data transfer between processor and memory rather than compute capability. LLM inference with small batch sizes is memory-bound, benefiting from higher memory bandwidth.",
      "tags": [
        "Hardware",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-memristor",
      "term": "Memristor",
      "definition": "Resistive memory device whose resistance depends on the history of current that has flowed through it. Promising for implementing synaptic weights in neuromorphic and analog AI hardware.",
      "tags": [
        "Neuromorphic",
        "Memory",
        "Component"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-merge-sort",
      "term": "Merge Sort",
      "definition": "A divide-and-conquer sorting algorithm that splits an array into halves and recursively sorts each half before merging the sorted halves. Guarantees O(n log n) time complexity and is stable but requires O(n) extra space.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Sorting"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-merkle-tree-algorithm",
      "term": "Merkle Tree Algorithm",
      "definition": "A tree data structure where every leaf node contains the hash of a data block and every non-leaf node contains the hash of its children. Enables efficient and secure verification of data integrity.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mesa-optimization",
      "term": "Mesa-Optimization",
      "definition": "A phenomenon where a learned model (the mesa-optimizer) internally develops its own optimization objective that may differ from the base objective it was trained on. This is a key concern in advanced AI safety research.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-mesh-reconstruction",
      "term": "Mesh Reconstruction",
      "definition": "The process of converting 3D point clouds, implicit functions, or depth maps into triangular mesh representations that define surface geometry, topology, and can be rendered or 3D-printed.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-message-passing",
      "term": "Message Passing",
      "definition": "A computational framework in graph neural networks where nodes iteratively exchange and aggregate information with their neighbors. Each node updates its representation based on messages received from connected nodes. Foundation of most GNN architectures.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-message-passing-neural-network",
      "term": "Message Passing Neural Network",
      "definition": "A framework for graph neural networks where nodes iteratively update their representations by exchanging and aggregating messages with neighboring nodes through learned message and update functions.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-meta-llama",
      "term": "Meta LLaMA",
      "definition": "Meta's Large Language Model Meta AI, first released in February 2023 with subsequent versions, representing a major open-weight language model that catalyzed the open-source AI ecosystem.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-meta-mtia",
      "term": "Meta MTIA",
      "definition": "Meta Training and Inference Accelerator custom chip designed for recommendation and ranking model inference. Part of Meta strategy to develop custom silicon for its specific AI workloads.",
      "tags": [
        "Accelerator",
        "Meta",
        "Custom"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-meta-research-supercluster",
      "term": "Meta Research SuperCluster",
      "definition": "Meta AI research supercomputer designed for training large AI models using thousands of NVIDIA A100 GPUs. One of the largest AI-dedicated training clusters in the world.",
      "tags": [
        "Supercomputer",
        "Meta",
        "Training"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-meta-learning",
      "term": "Meta-Learning",
      "definition": "Learning how to learn: training models that can quickly adapt to new tasks with few examples. Enables better few-shot and transfer learning capabilities.",
      "tags": [
        "Training",
        "Advanced"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-meta-prompting",
      "term": "Meta-Prompting",
      "definition": "A higher-order prompting approach where a language model is instructed to generate, critique, or improve prompts for itself or other models, effectively using the model as its own prompt engineer.",
      "tags": [
        "Prompt Engineering",
        "Meta-Learning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-meta-rl",
      "term": "Meta-Reinforcement Learning",
      "definition": "RL approaches that learn to learn, enabling rapid adaptation to new tasks by leveraging experience across a distribution of related tasks. Meta-RL agents develop internal adaptation mechanisms that generalize across task variations.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-metaai-voicebox",
      "term": "MetaAI Voicebox",
      "definition": "A non-autoregressive generative model from Meta AI for speech synthesis and editing that uses flow matching to generate speech in multiple styles and languages.",
      "tags": [
        "Models",
        "Technical",
        "Audio"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-metadata-filtering",
      "term": "Metadata Filtering",
      "definition": "A vector search technique that applies structured attribute filters alongside similarity search, restricting results to vectors matching specific metadata criteria such as date ranges, categories, or source types before or after distance computation.",
      "tags": [
        "Vector Database",
        "Filtering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-metamathqa",
      "term": "MetaMathQA",
      "definition": "A mathematical QA dataset created by rephrasing and augmenting questions from GSM8K and MATH. Used to improve mathematical reasoning through diverse problem formulations.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-metaworld",
      "term": "MetaWorld",
      "definition": "A benchmark of 50 robotic manipulation tasks built on MuJoCo for multi-task and meta-reinforcement learning. Provides standardized evaluation of generalization across manipulation skills.",
      "tags": [
        "Benchmark",
        "Reinforcement Learning",
        "Robotics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-meteor",
      "term": "METEOR",
      "definition": "Metric for Evaluation of Translation with Explicit ORdering, a machine translation evaluation metric that considers synonyms, stemming, and word order in addition to exact word matches.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-meteor-score",
      "term": "METEOR Score",
      "definition": "Metric for Evaluation of Translation with Explicit Ordering evaluates machine translation using unigram matching with stemming synonymy and paraphrase support. Addresses BLEU limitations by considering recall and incorporating linguistic knowledge beyond exact matching.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-method-of-lines",
      "term": "Method of Lines",
      "definition": "A technique for solving partial differential equations by discretizing all spatial dimensions while leaving the time dimension continuous. Converts the PDE into a system of ordinary differential equations.",
      "tags": [
        "Algorithms",
        "Technical",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-metrics",
      "term": "Metrics",
      "definition": "Quantitative measures used to evaluate model performance. Common metrics include accuracy, precision, recall, F1, perplexity, and human evaluation scores.",
      "tags": [
        "Evaluation",
        "Quality"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-metropolis-hastings",
      "term": "Metropolis-Hastings",
      "definition": "An MCMC algorithm that generates samples from a target distribution by proposing candidate points from a proposal distribution and accepting or rejecting them based on an acceptance ratio that ensures detailed balance.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mgpt-training-data",
      "term": "mGPT Training Data",
      "definition": "The multilingual training data used for the mGPT family of language models covering 60 languages. Demonstrates scaling language model pretraining across diverse languages.",
      "tags": [
        "Training Corpus",
        "NLP",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-michael-jordan",
      "term": "Michael Jordan",
      "definition": "American computer scientist and statistician at UC Berkeley known for foundational work in Bayesian machine learning variational inference and graphical models. His research bridging statistics and machine learning influenced modern probabilistic approaches to AI.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-microcontroller-for-ai",
      "term": "Microcontroller for AI",
      "definition": "Low-power embedded processor used to run tiny machine learning models for sensor processing and basic inference. Examples include ARM Cortex-M series and RISC-V based MCUs.",
      "tags": [
        "Edge",
        "Microcontroller"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-microprocessor",
      "term": "Microprocessor",
      "definition": "Complete CPU implemented on a single integrated circuit. The Intel 4004 in 1971 was the first commercial microprocessor launching the personal computing revolution.",
      "tags": [
        "Historical",
        "Processor",
        "Fundamentals"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-microsoft-cobalt",
      "term": "Microsoft Cobalt",
      "definition": "Microsoft custom ARM-based CPU designed for general-purpose Azure cloud workloads. Complements the Maia AI accelerator as part of Microsoft custom silicon strategy.",
      "tags": [
        "Processor",
        "Microsoft",
        "Cloud"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-microsoft-maia",
      "term": "Microsoft Maia",
      "definition": "Microsoft custom AI accelerator chip designed specifically for Azure cloud AI workloads. Developed to reduce dependence on third-party GPU suppliers for AI infrastructure.",
      "tags": [
        "Accelerator",
        "Microsoft",
        "Cloud"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-midjourney",
      "term": "Midjourney",
      "definition": "A popular AI image generation service known for artistic, stylized outputs. Accessed through Discord, it's widely used for creative and commercial image creation.",
      "tags": [
        "Product",
        "Image Generation"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-midjourney-launch",
      "term": "Midjourney Launch",
      "definition": "The July 2022 public launch of Midjourney, an independent AI art generation service that produces images from text prompts, becoming one of the most popular creative AI tools and sparking debates about AI and artistic creation.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mila-founded",
      "term": "Mila Founded",
      "definition": "The founding of the Montreal Institute for Learning Algorithms (Mila) by Yoshua Bengio. Mila became one of the world's largest academic research centers for deep learning attracting top researchers and contributing to Montreal's emergence as an AI hub.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-milvus",
      "term": "Milvus",
      "definition": "An open-source vector database built for scalable similarity search that supports multiple index types, hybrid search, and multi-tenancy, capable of handling billion-scale vector datasets with a distributed architecture.",
      "tags": [
        "Vector Database",
        "Open Source"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mimic-cxr",
      "term": "MIMIC-CXR",
      "definition": "A large dataset of 377000 chest X-ray images with associated radiology reports from Beth Israel Deaconess Medical Center. Used for medical image understanding and report generation.",
      "tags": [
        "Benchmark",
        "Medical",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mimic-iii",
      "term": "MIMIC-III",
      "definition": "Medical Information Mart for Intensive Care a deidentified health dataset of over 40000 ICU patients at Beth Israel Deaconess Medical Center. Includes clinical notes lab results and vital signs.",
      "tags": [
        "Training Corpus",
        "Medical"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mimic-iv",
      "term": "MIMIC-IV",
      "definition": "The fourth version of the MIMIC dataset with updated clinical data from 2008 to 2019. Provides comprehensive electronic health records for clinical NLP and predictive modeling research.",
      "tags": [
        "Training Corpus",
        "Medical"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-min-max-scaling",
      "term": "Min-Max Scaling",
      "definition": "A normalization technique that linearly rescales features to a fixed range, typically [0, 1], by subtracting the minimum value and dividing by the range. It preserves the shape of the original distribution.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-minari",
      "term": "Minari",
      "definition": "A standard format and library for offline reinforcement learning datasets built on Gymnasium. Provides tools for recording sharing and loading RL dataset trajectories.",
      "tags": [
        "Benchmark",
        "Reinforcement Learning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-minedreamer",
      "term": "MineDreamer",
      "definition": "A chain-of-imagination agent for open-ended Minecraft gameplay that uses imagination-based planning to solve complex multi-step tasks.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-minerva",
      "term": "Minerva",
      "definition": "A language model by Google fine-tuned for mathematical reasoning on a dataset of scientific papers and web pages containing mathematical content. Achieves strong performance on quantitative reasoning benchmarks without calculator access.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-minhash",
      "term": "MinHash",
      "definition": "A locality-sensitive hashing technique that efficiently estimates the Jaccard similarity between sets, widely used in NLP for approximate nearest neighbor search and document deduplication.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-minhash-algorithm",
      "term": "MinHash Algorithm",
      "definition": "A locality-sensitive hashing technique for estimating the Jaccard similarity between sets. Produces compact signatures such that the probability of hash collision equals the Jaccard similarity of the original sets.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mini-batch-gradient-descent",
      "term": "Mini-Batch Gradient Descent",
      "definition": "A gradient-based optimization method that computes parameter updates using a small random subset (mini-batch) of the training data at each step, balancing the stability of full-batch gradient descent with the speed of stochastic gradient descent.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mini-batch-k-means-algorithm",
      "term": "Mini-Batch K-Means Algorithm",
      "definition": "A variant of k-means that uses random mini-batches of data instead of the full dataset to update cluster centers. Significantly faster than standard k-means with only a slight reduction in cluster quality.",
      "tags": [
        "Algorithms",
        "Technical",
        "Clustering"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-minicpm",
      "term": "MiniCPM",
      "definition": "A family of compact language models from Tsinghua University and ModelBest that achieve strong performance at small parameter counts through efficient training.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-minicpm-v",
      "term": "MiniCPM-V",
      "definition": "A compact vision-language model that achieves strong multimodal performance with efficient architecture design for deployment on edge devices.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-minigrid",
      "term": "MiniGrid",
      "definition": "A minimalistic gridworld environment for reinforcement learning research. Provides configurable grid-based tasks for studying exploration planning and language-conditioned RL.",
      "tags": [
        "Benchmark",
        "Reinforcement Learning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-minilm",
      "term": "MiniLM",
      "definition": "A compact language model distilled from larger Transformers using deep self-attention distillation for efficient sentence embedding generation.",
      "tags": [
        "Models",
        "Technical",
        "Embedding",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-minimax-algorithm",
      "term": "Minimax Algorithm",
      "definition": "A decision-making algorithm for two-player zero-sum games that minimizes the possible loss for a worst-case scenario. Used in game-playing AI since the 1950s minimax forms the basis for game tree search in chess checkers and other adversarial games.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-minimum-bayes-risk-decoding",
      "term": "Minimum Bayes Risk Decoding",
      "definition": "A decoding strategy that selects the output candidate minimizing expected loss across a set of sampled hypotheses, often producing higher-quality translations and summaries than beam search.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-minimum-cut-algorithm",
      "term": "Minimum Cut Algorithm",
      "definition": "An algorithm that finds the smallest set of edges whose removal disconnects a graph into two or more components. Stoer-Wagner is a well-known deterministic algorithm for finding the global minimum cut in undirected weighted graphs.",
      "tags": [
        "Algorithms",
        "Technical",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-minimum-description-length",
      "term": "Minimum Description Length",
      "definition": "A model selection principle that selects the model minimizing the total description length of the data and the model itself. It formalizes Occam's razor using information-theoretic concepts.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-minimum-edit-distance-with-backtrace",
      "term": "Minimum Edit Distance with Backtrace",
      "definition": "An extension of the edit distance algorithm that records the operations performed at each step. Enables reconstruction of the optimal alignment between two strings for spell checking and sequence comparison.",
      "tags": [
        "Algorithms",
        "Technical",
        "NLP"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-minimum-spanning-tree",
      "term": "Minimum Spanning Tree",
      "definition": "A subset of edges in a connected weighted undirected graph that connects all vertices with the minimum total edge weight and contains no cycles. Can be found using Kruskal's or Prim's algorithm.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-minitron",
      "term": "Minitron",
      "definition": "A family of compressed language models from NVIDIA created through pruning and distillation of larger Nemotron models for efficient deployment.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-minkowski-distance",
      "term": "Minkowski Distance",
      "definition": "A generalized distance metric parameterized by p that includes Manhattan (p=1), Euclidean (p=2), and Chebyshev (p=infinity) distances as special cases. It computes the p-th root of the sum of p-th powers of absolute differences.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-minsky-papert-perceptrons",
      "term": "Minsky and Papert Perceptrons",
      "definition": "The 1969 book by Marvin Minsky and Seymour Papert that mathematically demonstrated the limitations of single-layer perceptrons, contributing to reduced funding for neural network research and the first AI winter.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mips-architecture",
      "term": "MIPS Architecture",
      "definition": "Reduced instruction set computer architecture developed by John Hennessy at Stanford. Influential in computer architecture education and used in embedded systems and early SGI workstations.",
      "tags": [
        "Architecture",
        "RISC",
        "Historical"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-mirror-descent",
      "term": "Mirror Descent",
      "definition": "A generalization of gradient descent that uses Bregman divergences instead of Euclidean distance for parameter updates. Naturally handles constrained optimization and non-Euclidean parameter spaces. Useful for optimization on simplices and other manifolds.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mirror-prompting",
      "term": "Mirror Prompting",
      "definition": "A prompting approach that instructs the model to first restate the user's request back in its own words, confirming mutual understanding before proceeding with task execution, reducing misalignment between user intent and model interpretation.",
      "tags": [
        "Prompt Engineering",
        "Clarification"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mish",
      "term": "Mish",
      "definition": "A self-regularizing non-monotonic activation function defined as f(x) = x * tanh(softplus(x)). Known for smooth gradients and strong empirical performance particularly in computer vision tasks such as object detection.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-misinformation",
      "term": "Misinformation",
      "definition": "False or inaccurate information shared without deliberate intent to deceive, which can be amplified by AI recommendation systems and generated inadvertently through AI hallucinations.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-misinformation-detection",
      "term": "Misinformation Detection",
      "definition": "AI techniques for identifying false or misleading information in text images and video. Combines natural language processing knowledge graph verification and source credibility assessment.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-misra-gries-algorithm",
      "term": "Misra-Gries Algorithm",
      "definition": "A streaming algorithm for finding frequent items (heavy hitters) in a data stream using limited memory. Maintains a set of candidate items and counts and guarantees finding all items above a frequency threshold.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral",
      "term": "Mistral",
      "definition": "A French AI company known for efficient, high-performance open models. Their Mistral and Mixtral models offer strong capabilities with smaller parameter counts.",
      "tags": [
        "Company",
        "Model"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral-7b",
      "term": "Mistral 7B",
      "definition": "A 7 billion parameter language model by Mistral AI that outperforms larger models through architectural innovations including grouped query attention and sliding window attention. Efficient and powerful for its size.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral-ai",
      "term": "Mistral AI",
      "definition": "A French AI company founded in 2023 by former Google DeepMind and Meta researchers including Arthur Mensch. Mistral AI developed efficient open-source language models including Mistral 7B and Mixtral demonstrating that smaller well-trained models can compete with larger ones.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral-ai-founding",
      "term": "Mistral AI Founding",
      "definition": "The founding of Mistral AI in April 2023 by former Google DeepMind and Meta researchers in Paris, which rapidly became a leading European AI company releasing competitive open-weight language models.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral-large",
      "term": "Mistral Large",
      "definition": "A flagship large language model from Mistral AI designed for complex reasoning and multilingual tasks with strong performance on benchmarks.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral-medium",
      "term": "Mistral Medium",
      "definition": "A mid-sized language model from Mistral AI positioned between Mistral Small and Mistral Large for balanced performance and cost efficiency.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral-nemo",
      "term": "Mistral Nemo",
      "definition": "A 12B parameter language model developed jointly by Mistral AI and NVIDIA offering strong multilingual and code generation performance.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral-small",
      "term": "Mistral Small",
      "definition": "A compact and efficient language model from Mistral AI optimized for low-latency applications while maintaining strong reasoning capabilities.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mistral-instruct",
      "term": "Mistral-Instruct",
      "definition": "Instruction-tuned variants of Mistral base models that are fine-tuned for conversational and instruction-following tasks with improved helpfulness.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mit-ai-laboratory",
      "term": "MIT AI Laboratory",
      "definition": "A research laboratory co-founded by Marvin Minsky and John McCarthy at MIT in 1959, which became one of the most influential AI research centers, producing foundational work in vision, robotics, and natural language understanding.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixed-precision-training",
      "term": "Mixed Precision Training",
      "definition": "A training technique that uses lower-precision floating-point formats (FP16 or BF16) for forward and backward passes while maintaining FP32 master copies of weights for accumulation. Mixed precision approximately doubles throughput and halves memory usage with minimal accuracy impact.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixmatch",
      "term": "MixMatch",
      "definition": "A semi-supervised learning method that combines consistency regularization entropy minimization and MixUp augmentation. Produces sharpened pseudo-labels for unlabeled data and mixes labeled and unlabeled examples for training.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixtral",
      "term": "Mixtral",
      "definition": "A mixture-of-experts model by Mistral AI that uses sparse MoE layers with 8 experts per layer routing each token to 2 experts. Achieves performance comparable to much larger dense models at a fraction of the compute cost.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixtral-8x22b",
      "term": "Mixtral 8x22B",
      "definition": "A large mixture-of-experts model from Mistral AI with 8 experts and 22 billion parameters per expert that achieves strong performance on reasoning tasks.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixtral-8x7b",
      "term": "Mixtral 8x7B",
      "definition": "A specific configuration of the Mixtral mixture-of-experts model with 8 experts of 7 billion parameters each. Routes each token through 2 experts resulting in computational cost similar to a 13B parameter dense model.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixtral-instruct",
      "term": "Mixtral-Instruct",
      "definition": "An instruction-tuned version of the Mixtral mixture-of-experts model optimized for following detailed instructions and multi-turn conversations.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixture-of-agents",
      "term": "Mixture of Agents",
      "definition": "An architecture where multiple specialized LLM agents collaborate on a task, with each agent contributing expertise in a specific domain and a router or aggregator combining their outputs.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixture-of-depths",
      "term": "Mixture of Depths",
      "definition": "A transformer variant that learns to dynamically allocate computation by routing only a subset of tokens through each transformer block, reducing total computation while maintaining model quality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-moe-inference",
      "term": "Mixture of Experts Inference",
      "definition": "Inference optimization for Mixture of Experts models where only a subset of expert parameters are activated per token, reducing computation despite the large total parameter count. MoE inference requires efficient expert routing and memory management.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixture-of-experts-layer",
      "term": "Mixture of Experts Layer",
      "definition": "A neural network layer consisting of multiple expert subnetworks and a gating mechanism that routes each input to a sparse subset of experts, enabling massive model capacity with sublinear computational cost.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-moe-routing",
      "term": "Mixture of Experts Routing",
      "definition": "The gating mechanism in MoE models that determines which expert subnetworks process each input, using learned routing functions to achieve efficient sparse computation.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixup",
      "term": "Mixup",
      "definition": "A data augmentation and regularization technique that creates virtual training examples by taking convex combinations of pairs of training examples and their labels, encouraging linear behavior between training points.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-ml-superb",
      "term": "ML-SUPERB",
      "definition": "Multilingual SUPERB extending the speech evaluation benchmark to cover tasks across multiple languages. Tests cross-lingual generalization of speech representation models.",
      "tags": [
        "Benchmark",
        "Speech",
        "Multilingual",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mlir",
      "term": "MLIR",
      "definition": "Multi-Level Intermediate Representation compiler infrastructure developed at Google for building reusable and extensible compiler frameworks. Used as the foundation for several AI compiler projects.",
      "tags": [
        "Compiler",
        "Google",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-mlops",
      "term": "MLOps",
      "definition": "Practices for deploying and maintaining ML models in production. Combines ML, DevOps, and data engineering to ensure reliable, scalable AI systems.",
      "tags": [
        "Operations",
        "Production"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-mlperf-inference",
      "term": "MLPerf Inference",
      "definition": "Industry benchmark suite measuring AI inference performance for latency and throughput across hardware platforms. Widely used to compare inference accelerator capabilities objectively.",
      "tags": [
        "Benchmark",
        "Performance",
        "Standard"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-mlperf-training",
      "term": "MLPerf Training",
      "definition": "Industry benchmark suite measuring AI training performance across multiple workloads and hardware platforms. Provides standardized comparisons between different AI hardware systems.",
      "tags": [
        "Benchmark",
        "Performance",
        "Standard"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-mlqa",
      "term": "MLQA",
      "definition": "Multilingual Question Answering a cross-lingual extractive QA benchmark with over 12000 instances in 7 languages. Tests the ability to find answers in passages written in different languages.",
      "tags": [
        "Benchmark",
        "NLP",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mls",
      "term": "MLS",
      "definition": "Multilingual LibriSpeech a large multilingual corpus derived from LibriVox audiobooks in 8 languages totaling 50000 hours. Used to train and evaluate multilingual speech recognition systems.",
      "tags": [
        "Benchmark",
        "Speech",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mls-benchmark",
      "term": "MLS Benchmark",
      "definition": "The Multilingual LibriSpeech benchmark evaluating speech recognition across 8 languages with standardized test sets. Provides consistent cross-lingual ASR evaluation.",
      "tags": [
        "Benchmark",
        "Speech",
        "Multilingual",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mm-vet",
      "term": "MM-Vet",
      "definition": "A multimodal benchmark evaluating integrated capabilities of vision-language models across recognition knowledge generation spatial awareness and math reasoning.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mm1",
      "term": "MM1",
      "definition": "A family of multimodal models from Apple that systematically study the importance of architecture and data choices for building effective vision-language models.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mmbench",
      "term": "MMBench",
      "definition": "A bilingual multimodal benchmark systematically evaluating vision-language models across 20 ability dimensions with 3000 single-choice questions in English and Chinese.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mmc4",
      "term": "MMC4",
      "definition": "Multimodal C4 a dataset of interleaved image-text documents derived from C4 web text. Provides naturally occurring multimodal content for pretraining vision-language models.",
      "tags": [
        "Training Corpus",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mme",
      "term": "MME",
      "definition": "A comprehensive multimodal evaluation benchmark testing perception and cognition abilities of large multimodal models across 14 subtasks with yes/no questions.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mmlu",
      "term": "MMLU (Massive Multitask Language Understanding)",
      "definition": "A comprehensive benchmark testing language models on 57 subjects from STEM to humanities. Widely used to compare model capabilities on knowledge-intensive tasks.",
      "tags": [
        "Benchmark",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mmlu-pro",
      "term": "MMLU-Pro",
      "definition": "An enhanced version of MMLU with more challenging questions expert-level reasoning requirements and reduced dataset noise. Designed to better differentiate among the most capable language models.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mmmu",
      "term": "MMMU",
      "definition": "Massive Multi-discipline Multimodal Understanding a benchmark of 11500 multimodal questions from college exams across 30 subjects. Tests expert-level multimodal reasoning.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mms",
      "term": "MMS",
      "definition": "Massively Multilingual Speech is a project from Meta AI providing speech recognition and synthesis and language identification models for over 1100 languages.",
      "tags": [
        "Models",
        "Technical",
        "Audio",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mmvet-v2",
      "term": "MMVET v2",
      "definition": "An updated multimodal evaluation benchmark testing integrated vision-language capabilities across recognition OCR knowledge spatial reasoning and math dimensions.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mnist",
      "term": "MNIST",
      "definition": "A dataset of 70000 handwritten digit images (28x28 grayscale) created by Yann LeCun and colleagues. One of the most widely used benchmarks in machine learning history often called the hello world of computer vision.",
      "tags": [
        "Benchmark",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mnist-dataset",
      "term": "MNIST Dataset",
      "definition": "The Modified National Institute of Standards and Technology database of handwritten digits created by Yann LeCun and colleagues in 1998. MNIST became the standard benchmark for machine learning algorithms and is often called the hello world of deep learning.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mnist-1d",
      "term": "MNIST-1D",
      "definition": "A one-dimensional synthetic analog of MNIST designed for efficient experimentation with deep learning concepts. Enables rapid prototyping of ideas that transfer to higher-dimensional settings.",
      "tags": [
        "Benchmark",
        "Synthetic"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mnli",
      "term": "MNLI",
      "definition": "The Multi-Genre Natural Language Inference corpus containing 433000 sentence pairs annotated with textual entailment labels across ten distinct genres of written and spoken English.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mobile-aloha",
      "term": "Mobile ALOHA",
      "definition": "An extension of ALOHA that adds a mobile base to enable whole-body bimanual manipulation allowing robots to perform household tasks requiring locomotion.",
      "tags": [
        "Models",
        "Technical",
        "Robotics"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mobile-inference",
      "term": "Mobile Inference",
      "definition": "AI inference optimized for smartphones and tablets, leveraging mobile GPU, NPU, or DSP capabilities. Mobile inference frameworks like TensorFlow Lite and Core ML apply aggressive quantization and operator fusion for on-device model execution.",
      "tags": [
        "Inference Infrastructure",
        "Hardware"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-mobilenet",
      "term": "MobileNet",
      "definition": "A family of lightweight CNN architectures designed for mobile and embedded devices that use depthwise separable convolutions to dramatically reduce computation and model size.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mobilenetv2",
      "term": "MobileNetV2",
      "definition": "An improved mobile architecture that introduces inverted residual blocks with linear bottlenecks. The inverted residual expands channels before the depthwise convolution then projects back to a thin representation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mobilenetv3",
      "term": "MobileNetV3",
      "definition": "The third generation mobile architecture combining hardware-aware neural architecture search with NetAdapt. Uses squeeze-and-excitation modules and h-swish activation for improved accuracy per computation.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mobilevit",
      "term": "MobileViT",
      "definition": "A lightweight vision Transformer from Apple that combines mobile-friendly convolutions with self-attention for efficient on-device image understanding.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mobilevitv2",
      "term": "MobileViTv2",
      "definition": "An improved mobile vision Transformer that uses separable self-attention to achieve linear complexity while maintaining competitive accuracy for mobile deployment.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model",
      "term": "Model",
      "definition": "The trained AI system that processes inputs and generates outputs. Models are defined by their architecture, size (parameters), training data, and fine-tuning.",
      "tags": [
        "Core Concept",
        "Fundamentals"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-auditing",
      "term": "Model Auditing",
      "definition": "The systematic examination of an AI model's behavior performance and fairness by an independent party. May include testing for bias reviewing training procedures and assessing documentation completeness.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-averaging",
      "term": "Model Averaging",
      "definition": "An ensemble technique that combines predictions from multiple models by averaging their outputs. Can use uniform weights or learned weights. Simple yet effective at reducing prediction variance and improving generalization.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-card",
      "term": "Model Card",
      "definition": "Documentation describing a model's intended use, limitations, performance metrics, and ethical considerations. A standard practice for responsible AI development and deployment.",
      "tags": [
        "Documentation",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-cards",
      "term": "Model Cards",
      "definition": "Standardized documentation artifacts proposed by Mitchell et al. (2019) that accompany trained ML models and report on their intended use, performance characteristics, limitations, and ethical considerations.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-collapse",
      "term": "Model Collapse",
      "definition": "A degradation phenomenon where models trained on AI-generated data lose diversity and quality over generations. A growing concern as synthetic data becomes more prevalent.",
      "tags": [
        "Risk",
        "Training"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-compilation",
      "term": "Model Compilation",
      "definition": "Process of converting a trained model into optimized machine code for specific hardware. Includes operator fusion memory planning and hardware-specific kernel selection for maximum performance.",
      "tags": [
        "Inference",
        "Optimization",
        "Compiler"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-compression",
      "term": "Model Compression",
      "definition": "A family of techniques for reducing model size and computational cost while preserving performance, including quantization, pruning, distillation, and low-rank factorization. Model compression enables deployment of large models on resource-constrained devices.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-distillation",
      "term": "Model Distillation",
      "definition": "The process of training a smaller student model to replicate the behavior of a larger teacher model by learning from the teacher's output probability distributions rather than hard labels alone.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-distillation-history",
      "term": "Model Distillation History",
      "definition": "The development of knowledge distillation from the original concept by Hinton Vinyals and Dean (2015) where a smaller student model learns to mimic a larger teacher model. Distillation has become essential for deploying large models on resource-constrained devices.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-extraction-attack",
      "term": "Model Extraction Attack",
      "definition": "An attack that creates a functionally equivalent copy of a target model by querying it and training a substitute on the responses. Threatens model intellectual property and can enable further attacks.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-mfu",
      "term": "Model FLOPs Utilization (MFU)",
      "definition": "The ratio of observed model FLOPS to the theoretical peak FLOPS of the hardware, measuring how efficiently the training system utilizes available compute. MFU above 50% is considered good for large-scale training on modern GPU clusters.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-governance",
      "term": "Model Governance",
      "definition": "Organizational policies and processes for managing the lifecycle of machine learning models from development through deployment monitoring and retirement. Ensures consistent safety and compliance standards.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-interpretability",
      "term": "Model Interpretability",
      "definition": "The degree to which a human can understand the cause of a model's decisions. Distinguished from explainability which focuses on providing post-hoc explanations of opaque model behavior.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-merging",
      "term": "Model Merging",
      "definition": "A technique that combines the weights of two or more fine-tuned models into a single model, often using methods like linear interpolation, SLERP, or TIES, to inherit capabilities from multiple specializations.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-parallelism",
      "term": "Model Parallelism",
      "definition": "A distributed training strategy that splits a model's layers or parameters across multiple devices, enabling training of models too large to fit in the memory of a single accelerator.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-predictive-control",
      "term": "Model Predictive Control",
      "definition": "A control algorithm that uses a model of the system to predict future states and optimizes a sequence of control actions over a finite horizon. Re-plans at each time step using the latest state observation.",
      "tags": [
        "Algorithms",
        "Technical",
        "RL",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-predictive-control-rl",
      "term": "Model Predictive Control in RL",
      "definition": "A planning-based approach that uses a learned dynamics model to simulate action sequences forward and selects the first action of the best sequence. MPC re-plans at every step, making it robust to model errors.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-pruning",
      "term": "Model Pruning",
      "definition": "A compression technique that removes redundant weights or neurons from a neural network based on magnitude, sensitivity, or other criteria. Pruning reduces model size and computation while attempting to preserve accuracy through fine-tuning.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-risk-management",
      "term": "Model Risk Management",
      "definition": "A framework for identifying assessing mitigating and monitoring risks associated with machine learning models in production. Extends traditional model risk management from financial services to broader AI applications.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-serving",
      "term": "Model Serving",
      "definition": "The infrastructure and systems for deploying trained models to handle real-time prediction requests at scale. Model serving encompasses load balancing, request batching, model versioning, and health monitoring for production AI systems.",
      "tags": [
        "Inference Infrastructure",
        "Distributed Computing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-serving-infrastructure",
      "term": "Model Serving Infrastructure",
      "definition": "Systems and software for deploying trained AI models to handle production inference requests. Includes load balancing batching model management and hardware orchestration.",
      "tags": [
        "Inference",
        "Infrastructure",
        "Deployment"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-sharding",
      "term": "Model Sharding",
      "definition": "The technique of partitioning a large model's parameters across multiple devices or storage locations, enabling inference and training of models that exceed the memory capacity of a single accelerator.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-specification",
      "term": "Model Specification",
      "definition": "A detailed document describing the intended behavior constraints and safety requirements for an AI model. Includes behavioral guidelines content policies and interaction principles that the model should follow.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-stealing",
      "term": "Model Stealing",
      "definition": "The unauthorized replication of a machine learning model's functionality through systematic querying. Also known as model extraction threatens intellectual property and can bypass access controls and safety measures.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-based-rl",
      "term": "Model-Based RL",
      "definition": "RL approaches that learn or use a model of the environment's transition dynamics and reward function to plan actions or generate synthetic experience. Model-based methods can be more sample-efficient but require accurate models.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-model-free-rl",
      "term": "Model-Free RL",
      "definition": "RL algorithms that learn policies or value functions directly from experience without building an explicit model of the environment. Model-free methods are simpler and more broadly applicable but typically require more interaction data.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-modelnet",
      "term": "ModelNet",
      "definition": "A dataset of 3D CAD models for 3D object recognition containing 151128 models across 662 categories. ModelNet40 a 40-category subset is the standard 3D classification benchmark.",
      "tags": [
        "Benchmark",
        "3D",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-modelscope-text-to-video",
      "term": "ModelScope Text-to-Video",
      "definition": "A text-to-video synthesis model that uses a multi-stage pipeline with spatio-temporal blocks for generating short videos from text descriptions.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-modern-hopfield-network",
      "term": "Modern Hopfield Network",
      "definition": "An updated Hopfield network formulation using exponential interaction functions that connects to transformer attention mechanisms and provides exponential storage capacity compared to classical Hopfield networks.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mixture-of-experts",
      "term": "MoE (Mixture of Experts)",
      "definition": "An architecture where different \"expert\" sub-networks specialize in different types of inputs. Enables larger effective model capacity while keeping computation manageable.",
      "tags": [
        "Architecture",
        "Efficiency"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-moe-llava",
      "term": "MoE-LLaVA",
      "definition": "A multimodal large language model that uses mixture-of-experts to efficiently scale visual instruction tuning. Activates only a subset of parameters for each input reducing computation while maintaining strong multimodal understanding.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-moirai",
      "term": "MOIRAI",
      "definition": "A universal time series forecasting model that uses masked encoder-based architecture with any-variate attention to handle diverse forecasting scenarios.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-moleculenet",
      "term": "MoleculeNet",
      "definition": "A benchmark collection of molecular datasets for evaluating molecular machine learning methods. Covers quantum mechanics physical chemistry biophysics and physiology tasks.",
      "tags": [
        "Benchmark",
        "Scientific"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-molmo",
      "term": "Molmo",
      "definition": "A family of open vision-language models from AI2 that achieve strong multimodal performance using carefully curated training data and efficient architectures.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-moment",
      "term": "Moment",
      "definition": "A family of open-source foundation models for general-purpose time series analysis that handles classification and forecasting and anomaly detection tasks.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-moments-in-time",
      "term": "Moments in Time",
      "definition": "A large-scale video dataset with one million 3-second clips labeled with 339 action or activity classes. Covers diverse events environments and viewpoints.",
      "tags": [
        "Benchmark",
        "Video"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-momentum",
      "term": "Momentum",
      "definition": "An optimization technique that accelerates gradient descent by accumulating an exponentially decaying moving average of past gradients, helping the optimizer move faster along consistent gradient directions and dampen oscillations.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-monitoring-bias",
      "term": "Monitoring Bias",
      "definition": "Bias that arises from differential surveillance or monitoring of certain populations by AI systems. Can create feedback loops where over-monitored groups appear to have higher rates of negative outcomes.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-monkey",
      "term": "Monkey",
      "definition": "A vision-language model designed for processing high-resolution document images by dividing them into patches and using a sliding window approach.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-monte-carlo-dropout",
      "term": "Monte Carlo Dropout",
      "definition": "An approximate Bayesian inference technique that uses dropout at inference time to generate multiple stochastic predictions. The variance of predictions provides uncertainty estimates. Theoretically connected to Gaussian processes.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-monte-carlo-integration",
      "term": "Monte Carlo Integration",
      "definition": "A numerical integration technique that estimates integrals by averaging function values at randomly sampled points. Convergence rate is independent of dimension making it effective for high-dimensional integration problems.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-monte-carlo-method",
      "term": "Monte Carlo Method",
      "definition": "A broad class of computational algorithms that use repeated random sampling to obtain numerical results, such as estimating integrals, simulating complex systems, or approximating probability distributions.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-monte-carlo-methods-rl",
      "term": "Monte Carlo Methods in RL",
      "definition": "RL algorithms that estimate value functions by averaging the actual returns observed over complete episodes. Unlike TD methods, Monte Carlo approaches require no bootstrapping and wait until the end of an episode to update.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-monte-carlo-tree-search",
      "term": "Monte Carlo Tree Search (MCTS)",
      "definition": "A search algorithm that builds a decision tree through random simulations, using statistics from previous rollouts to guide exploration toward promising actions. MCTS powers game-playing systems like AlphaGo and is used for planning in complex domains.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-montreal-declaration-responsible-ai",
      "term": "Montreal Declaration for Responsible AI",
      "definition": "A declaration adopted in 2018 establishing principles for responsible AI development including well-being, respect for autonomy, privacy, democratic participation, equity, diversity, and prudence.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-moore-threads",
      "term": "Moore Threads",
      "definition": "Chinese GPU company designing general-purpose GPUs with AI capabilities for the Chinese market. Aims to build a full-stack GPU computing ecosystem independent of NVIDIA.",
      "tags": [
        "GPU",
        "China"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-moores-law",
      "term": "Moore's Law",
      "definition": "An observation by Intel co-founder Gordon Moore in 1965 that the number of transistors on a microchip doubles approximately every two years. Moore's Law has driven the exponential increase in computing power that has enabled modern AI capabilities.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-moral-machine",
      "term": "Moral Machine",
      "definition": "An MIT research platform that crowdsources human moral preferences for autonomous vehicle dilemmas. Revealed significant cultural variation in ethical judgments about AI decision-making in life-or-death scenarios.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-moral-status-of-ai",
      "term": "Moral Status of AI",
      "definition": "The philosophical question of whether AI systems can possess moral standing, such that their interests or welfare deserve ethical consideration. Closely tied to debates about AI consciousness and sentience.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-moral-stories",
      "term": "Moral Stories",
      "definition": "A dataset of 12000 structured narratives illustrating moral and immoral actions with consequences. Tests the ability of models to understand social norms and moral reasoning.",
      "tags": [
        "Benchmark",
        "NLP",
        "Safety",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-moravecs-paradox",
      "term": "Moravec's Paradox",
      "definition": "The observation by Hans Moravec and others in the 1980s that high-level reasoning tasks are easy for AI while sensorimotor skills that seem simple to humans are extremely difficult to replicate computationally.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-morpheme",
      "term": "Morpheme",
      "definition": "The smallest meaningful unit of language that cannot be further divided without losing its meaning, including roots, prefixes, suffixes, and inflectional endings.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-morphological-operations-algorithm",
      "term": "Morphological Operations Algorithm",
      "definition": "A set of image processing operations based on set theory that process images according to a structuring element. Includes erosion and dilation and opening and closing for shape analysis and noise removal.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Vision"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-morphology",
      "term": "Morphology",
      "definition": "The branch of linguistics studying the internal structure of words, including how morphemes combine to form words through inflection, derivation, and compounding processes.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-morris-counter-algorithm",
      "term": "Morris Counter Algorithm",
      "definition": "An approximate counting algorithm that maintains a probabilistic counter using only O(log log n) bits to count up to n. Increments the counter with probability inversely proportional to its current estimated value.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mos-6502",
      "term": "MOS 6502",
      "definition": "Inexpensive 8-bit microprocessor from 1975 that powered the Apple II Commodore 64 and Nintendo NES. Its low cost democratized computing and inspired a generation of programmers.",
      "tags": [
        "Historical",
        "Processor",
        "Consumer"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-mosaic-augmentation",
      "term": "Mosaic Augmentation",
      "definition": "A data augmentation technique that combines four training images into a single mosaic image, allowing the model to learn from multiple contexts simultaneously and detect objects at various scales.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-moshi",
      "term": "Moshi",
      "definition": "A speech-text foundation model from Kyutai that enables real-time spoken dialogue with simultaneous listening and speaking capabilities.",
      "tags": [
        "Models",
        "Technical",
        "Audio",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-moth-flame-optimization",
      "term": "Moth-Flame Optimization",
      "definition": "A nature-inspired metaheuristic based on the navigation behavior of moths spiraling toward light sources. Moths update positions using a logarithmic spiral around the best solutions (flames) found so far.",
      "tags": [
        "Algorithms",
        "Technical",
        "Metaheuristic"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-motiondiffuser",
      "term": "MotionDiffuser",
      "definition": "A diffusion-based model for multi-agent motion prediction in autonomous driving that generates diverse and realistic future trajectory forecasts.",
      "tags": [
        "Models",
        "Technical",
        "Autonomous"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-motivational-control",
      "term": "Motivational Control",
      "definition": "Safety measures that shape an AI system's goals and values to be aligned with human interests, as opposed to capability control which restricts what the system can physically do.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-movement-pruning",
      "term": "Movement Pruning",
      "definition": "A pruning method that removes weights based on their movement during fine-tuning rather than their magnitude. Weights that move toward zero are pruned. More effective than magnitude pruning for pretrained models being fine-tuned.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-moving-average-model",
      "term": "Moving Average Model",
      "definition": "A time series model where the current value is expressed as a linear combination of the current and past white noise error terms. It captures short-term dependencies in the data.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mplug-docowl-15",
      "term": "mPLUG-DocOwl 1.5",
      "definition": "An improved document understanding model that unifies document structure and text recognition with a multi-granularity parsing approach.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mplug-owl2",
      "term": "mPLUG-Owl2",
      "definition": "A multimodal large language model that uses modality-adaptive modules to handle both visual and textual inputs for diverse multimodal tasks.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mpnet",
      "term": "MPNet",
      "definition": "A pre-trained language model that combines masked and permuted language modeling to capture both token dependencies and positional information for embeddings.",
      "tags": [
        "Models",
        "Technical",
        "Embedding",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mpt",
      "term": "MPT",
      "definition": "MosaicML Pretrained Transformer is a family of open-source language models trained by MosaicML using ALiBi position embeddings and FlashAttention. MPT-7B was notable for its efficient training and permissive commercial license.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mrpc",
      "term": "MRPC",
      "definition": "The Microsoft Research Paraphrase Corpus containing 5801 sentence pairs extracted from online news sources annotated for semantic equivalence. Tests paraphrase detection in the GLUE benchmark.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-ms-marco",
      "term": "MS MARCO",
      "definition": "Microsoft Machine Reading Comprehension a large-scale reading comprehension and information retrieval dataset. Contains 1 million real Bing queries with human-generated answers.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-msr-vtt",
      "term": "MSR-VTT",
      "definition": "Microsoft Research Video to Text a dataset of 10000 video clips with 200000 human-written descriptions. A primary benchmark for video captioning and video-text retrieval.",
      "tags": [
        "Benchmark",
        "Video",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-msr-vtt-retrieval",
      "term": "MSR-VTT Retrieval",
      "definition": "A video-text retrieval benchmark derived from the MSR-VTT captioning dataset. Tests the ability to match natural language descriptions with video clips.",
      "tags": [
        "Benchmark",
        "Video",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-msvd",
      "term": "MSVD",
      "definition": "Microsoft Research Video Description a dataset of 1970 video clips with multilingual descriptions. An early and widely used benchmark for video captioning evaluation.",
      "tags": [
        "Benchmark",
        "Video",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mt-bench",
      "term": "MT-Bench",
      "definition": "Multi-Turn Benchmark, an evaluation framework that tests language models' conversational abilities across multi-turn dialogues with follow-up questions, using LLM judges to score responses on writing, reasoning, coding, and knowledge tasks.",
      "tags": [
        "Evaluation",
        "Benchmarks"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mt5",
      "term": "mT5",
      "definition": "A multilingual variant of T5 pre-trained on the mC4 corpus covering 101 languages for cross-lingual text-to-text transfer tasks.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mteb",
      "term": "MTEB",
      "definition": "Massive Text Embedding Benchmark a comprehensive benchmark for evaluating text embedding models across 8 tasks and 58 datasets. Standard evaluation for sentence and document embeddings.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mteb-retrieval",
      "term": "MTEB Retrieval",
      "definition": "The retrieval subset of MTEB specifically evaluating passage and document retrieval capabilities of embedding models across diverse domains.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mujoco",
      "term": "MuJoCo",
      "definition": "Multi-Joint dynamics with Contact a physics engine and set of continuous control benchmarks for reinforcement learning. Includes locomotion manipulation and other robotic control tasks.",
      "tags": [
        "Benchmark",
        "Reinforcement Learning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-agent-rl",
      "term": "Multi-Agent Reinforcement Learning",
      "definition": "RL involving multiple agents that interact within a shared environment, each with its own observations and objectives. MARL introduces challenges of non-stationarity, credit assignment, and emergent communication.",
      "tags": [
        "Reinforcement Learning",
        "Multi-Agent"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-agent-systems",
      "term": "Multi-Agent Systems",
      "definition": "Systems composed of multiple interacting intelligent agents that can cooperate compete and coordinate to solve problems. Research in multi-agent systems draws on game theory distributed computing and AI and has applications in robotics simulation and complex problem solving.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-armed-bandit",
      "term": "Multi-Armed Bandit",
      "definition": "A simplified RL problem where an agent repeatedly chooses among K actions (arms) to maximize cumulative reward, with no state transitions. Bandit problems isolate the exploration-exploitation tradeoff from sequential decision-making.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-chip-module",
      "term": "Multi-Chip Module",
      "definition": "Package containing multiple semiconductor dies interconnected within a single housing. Modern GPUs and AI accelerators increasingly use multi-chip designs for higher performance.",
      "tags": [
        "Packaging",
        "Architecture",
        "Design"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-dimensional-scaling-algorithm",
      "term": "Multi-Dimensional Scaling Algorithm",
      "definition": "A family of techniques that place objects in a low-dimensional space such that the distances between them approximate their original dissimilarities. Classical MDS uses eigendecomposition of the doubly-centered distance matrix.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Dimensionality Reduction"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-gpu-training",
      "term": "Multi-GPU Training",
      "definition": "Training a model using multiple GPUs simultaneously within a single node or across nodes, requiring parallelism strategies and gradient synchronization. Multi-GPU training is essential for large models and datasets that exceed single-GPU capacity or time constraints.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-head-attention",
      "term": "Multi-Head Attention",
      "definition": "An extension of attention that runs multiple attention operations in parallel, each focusing on different aspects. A key component of transformer architectures.",
      "tags": [
        "Architecture",
        "Transformers"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mig",
      "term": "Multi-Instance GPU (MIG)",
      "definition": "An NVIDIA feature that partitions a single GPU into up to seven isolated instances, each with dedicated compute, memory, and cache resources. MIG enables secure multi-tenant GPU sharing for inference workloads with guaranteed quality of service.",
      "tags": [
        "Hardware",
        "GPU"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-label-classification",
      "term": "Multi-Label Classification",
      "definition": "A classification task where each instance can belong to multiple classes simultaneously, unlike multi-class classification where each instance has exactly one label. Examples include document tagging and image annotation.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-layer-perceptron",
      "term": "Multi-Layer Perceptron",
      "definition": "A feedforward neural network with one or more hidden layers between input and output. Uses nonlinear activation functions enabling it to learn non-linear decision boundaries. Trained with backpropagation. The classic deep learning architecture.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-layer-perceptron-model",
      "term": "Multi-Layer Perceptron Model",
      "definition": "A feedforward neural network with one or more hidden layers between input and output that can learn nonlinear decision boundaries through backpropagation.",
      "tags": [
        "Models",
        "Fundamentals",
        "History"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-news",
      "term": "Multi-News",
      "definition": "A multi-document summarization dataset containing 56000 article-summary pairs where each summary covers 2 to 10 source documents. Tests cross-document information synthesis.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-object-tracking",
      "term": "Multi-Object Tracking",
      "definition": "The task of simultaneously tracking multiple objects through a video sequence, handling challenges like occlusion, identity switches, and objects entering or leaving the scene.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-objective-reinforcement-learning",
      "term": "Multi-Objective Reinforcement Learning",
      "definition": "A framework for learning policies that optimize multiple potentially conflicting objectives simultaneously. Produces a set of Pareto-optimal policies representing different trade-offs between objectives.",
      "tags": [
        "Algorithms",
        "Technical",
        "RL"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-objective-rl",
      "term": "Multi-Objective RL",
      "definition": "RL formulations where the agent must optimize multiple potentially conflicting reward functions simultaneously. Solutions involve Pareto-optimal policies, scalarization methods, or constraint-based approaches.",
      "tags": [
        "Reinforcement Learning",
        "Reward Design"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-persona-prompting",
      "term": "Multi-Persona Prompting",
      "definition": "A technique that assigns multiple distinct expert personas within a single prompt, having each persona contribute their specialized perspective to a problem and then synthesizing their viewpoints into a comprehensive response.",
      "tags": [
        "Prompt Engineering",
        "Persona"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-query-attention",
      "term": "Multi-Query Attention",
      "definition": "An attention variant where all query heads share a single set of key and value projections, significantly reducing memory bandwidth requirements during autoregressive decoding.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-query-retrieval",
      "term": "Multi-Query Retrieval",
      "definition": "A technique that generates multiple paraphrased or perspective-shifted versions of the original query using an LLM, retrieves documents for each variant, and combines the results to overcome the sensitivity of retrieval to specific query phrasings.",
      "tags": [
        "Retrieval",
        "Query Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-scale-feature-extraction",
      "term": "Multi-Scale Feature Extraction",
      "definition": "The technique of capturing features at different spatial resolutions or receptive field sizes within a network, enabling detection and recognition of objects at various scales.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-scale-testing",
      "term": "Multi-Scale Testing",
      "definition": "An evaluation technique that processes an image at multiple resolutions and combines the predictions, improving detection of objects at various scales at the cost of increased inference time.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-stakeholder-ai-governance",
      "term": "Multi-Stakeholder AI Governance",
      "definition": "An approach to AI governance that involves representatives from government industry civil society academia and affected communities in decision-making about AI policy and regulation.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-step-bootstrapping",
      "term": "Multi-Step Bootstrapping",
      "definition": "A value estimation approach that uses n actual rewards before bootstrapping with a value estimate for the remaining future, interpolating between one-step TD and Monte Carlo methods. Multi-step bootstrapping controls the bias-variance tradeoff in value learning.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-step-reasoning",
      "term": "Multi-Step Reasoning",
      "definition": "A prompting paradigm that breaks complex problems into a sequence of intermediate reasoning steps, requiring the model to solve each sub-problem before proceeding to the next, enabling accurate solutions to problems that exceed single-step capability.",
      "tags": [
        "Prompt Engineering",
        "Reasoning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-task-learning",
      "term": "Multi-Task Learning",
      "definition": "A learning approach where a model is trained simultaneously on multiple related tasks, sharing representations across tasks. It can improve generalization by leveraging shared structure and acting as an implicit regularizer.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-task-rl",
      "term": "Multi-Task Reinforcement Learning",
      "definition": "RL approaches that train a single policy to perform well across multiple related tasks simultaneously. Multi-task RL leverages shared structure to improve sample efficiency and develop more general capabilities.",
      "tags": [
        "Reinforcement Learning",
        "Training Paradigms"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-tenancy-vector-databases",
      "term": "Multi-Tenancy in Vector Databases",
      "definition": "The ability of a vector database to serve multiple isolated users or applications from a shared infrastructure, using namespaces, partitions, or metadata filtering to ensure data separation while maintaining efficient resource utilization.",
      "tags": [
        "Vector Database",
        "Infrastructure"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-turn-conversation",
      "term": "Multi-Turn Conversation",
      "definition": "A dialogue format where a language model maintains context across multiple exchanges with a user, requiring the model to track conversation history, resolve references, and maintain coherence.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-vector-retrieval",
      "term": "Multi-Vector Retrieval",
      "definition": "A retrieval approach that represents each document as multiple embedding vectors rather than a single vector, capturing different aspects or segments of the document and enabling finer-grained matching at the cost of increased storage and computation.",
      "tags": [
        "Retrieval",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-view-clustering",
      "term": "Multi-View Clustering",
      "definition": "A clustering approach that integrates information from multiple representations or views of the same data. Seeks a consensus partition that is consistent across all views while exploiting complementary information.",
      "tags": [
        "Algorithms",
        "Technical",
        "Clustering"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-view-stereo",
      "term": "Multi-View Stereo",
      "definition": "A 3D reconstruction method that computes dense depth maps from multiple calibrated camera views, using photometric consistency to establish correspondences across many viewpoints.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-multi-word-expression",
      "term": "Multi-Word Expression",
      "definition": "A combination of words that exhibits lexical, syntactic, semantic, or statistical idiosyncrasy, including idioms, compound nouns, phrasal verbs, and collocations.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multicollinearity",
      "term": "Multicollinearity",
      "definition": "A condition in regression analysis where two or more independent variables are highly correlated, making it difficult to determine the individual effect of each predictor and inflating standard errors of coefficient estimates.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multidimensional-scaling",
      "term": "Multidimensional Scaling",
      "definition": "A dimensionality reduction technique that positions points in low-dimensional space such that pairwise distances approximate the original high-dimensional distances. Used for visualization and analysis of similarity or dissimilarity data.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multigrid-method",
      "term": "Multigrid Method",
      "definition": "A numerical method that accelerates the convergence of iterative solvers by using a hierarchy of discretization grids. Smooths errors at each grid level and transfers corrections between coarse and fine grids.",
      "tags": [
        "Algorithms",
        "Technical",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multihop-rag",
      "term": "MultiHop-RAG",
      "definition": "A benchmark for evaluating retrieval-augmented generation on multi-hop reasoning questions. Tests whether RAG systems can combine information across multiple retrieved documents.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-multilingual-model",
      "term": "Multilingual Model",
      "definition": "A single model trained on data from multiple languages that can perform NLP tasks across those languages, often developing cross-lingual transfer abilities from shared representations.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-multilingual-spoken-words-corpus",
      "term": "Multilingual Spoken Words Corpus",
      "definition": "A large dataset of spoken words in 50 languages from the Common Voice project. Provides keyword spotting data for multilingual speech processing research.",
      "tags": [
        "Training Corpus",
        "Speech",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-multimodal",
      "term": "Multimodal",
      "definition": "AI systems that can process and generate multiple types of content (text, images, audio, video). Examples include GPT-4V, Gemini, and Claude with vision capabilities.",
      "tags": [
        "Capability",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-multimodal-ai-history",
      "term": "Multimodal AI History",
      "definition": "The development of AI systems that can process and generate multiple types of data (text images audio video). From early separate modality systems to unified multimodal models like GPT-4V and Gemini that natively handle diverse input and output types.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-multinomial-distribution",
      "term": "Multinomial Distribution",
      "definition": "A generalization of the binomial distribution for experiments with more than two possible outcomes. It models the counts of each outcome across a fixed number of independent trials.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multipl-e",
      "term": "MultiPL-E",
      "definition": "A multi-programming language benchmark that translates HumanEval and MBPP problems into 18 programming languages. Enables evaluation of code generation across diverse language ecosystems.",
      "tags": [
        "Benchmark",
        "Code"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-multiple-testing-correction",
      "term": "Multiple Testing Correction",
      "definition": "Statistical methods for adjusting significance thresholds when performing many simultaneous hypothesis tests to control the overall error rate. Common methods include Bonferroni, Holm, and Benjamini-Hochberg.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-multirc",
      "term": "MultiRC",
      "definition": "Multi-Sentence Reading Comprehension a dataset where each question requires reasoning over multiple sentences in a paragraph. Answers are not constrained to spans of text in the passage.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-multiwoz",
      "term": "MultiWOZ",
      "definition": "Multi-Domain Wizard-of-Oz a large-scale multi-domain task-oriented dialogue dataset with over 10000 dialogues spanning 7 domains. A primary benchmark for dialogue state tracking.",
      "tags": [
        "Benchmark",
        "NLP",
        "Dialogue"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-musdb18",
      "term": "MUSDB18",
      "definition": "A dataset of 150 full-length music tracks with isolated stems for vocals drums bass and other instruments. The standard benchmark for music source separation research.",
      "tags": [
        "Benchmark",
        "Audio"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-music-algorithm",
      "term": "MUSIC Algorithm",
      "definition": "Multiple Signal Classification is a high-resolution frequency estimation method that separates the signal and noise subspaces using eigendecomposition of the autocorrelation matrix. Resolves closely spaced sinusoids better than Fourier methods.",
      "tags": [
        "Algorithms",
        "Technical",
        "Signal Processing"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-musiccaps",
      "term": "MusicCaps",
      "definition": "A dataset of 5500 music clips annotated with rich text descriptions by musicians. Used for evaluating music generation and music-text retrieval systems.",
      "tags": [
        "Benchmark",
        "Audio",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-musicgen",
      "term": "MusicGen",
      "definition": "A single-stage music generation model from Meta AI that uses a Transformer with efficient codebook interleaving to create music from text or melody prompts.",
      "tags": [
        "Models",
        "Technical",
        "Audio"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-musiclm",
      "term": "MusicLM",
      "definition": "A music generation model by Google that generates high-fidelity music from text descriptions. Uses a hierarchical sequence-to-sequence approach conditioned on text and melody embeddings. Produces coherent music over several minutes.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-musique",
      "term": "MuSiQue",
      "definition": "Multi-hop Questions via Single-hop Question Composition a benchmark of 25000 multi-hop questions constructed by composing single-hop questions. Tests multi-step reasoning ability.",
      "tags": [
        "Benchmark",
        "NLP",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mutr3d",
      "term": "MUTR3D",
      "definition": "Multi-camera Tracking Transformer for 3D tracking that uses track queries and camera-aware attention for multi-object tracking in autonomous driving.",
      "tags": [
        "Models",
        "Technical",
        "Autonomous",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mutual-information",
      "term": "Mutual Information",
      "definition": "A measure of the statistical dependence between two random variables, quantifying how much knowing one variable reduces uncertainty about the other. It is used in feature selection and clustering evaluation.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mutual-information-feature-selection",
      "term": "Mutual Information Feature Selection",
      "definition": "A filter-based feature selection method that ranks features by their mutual information with the target variable, measuring the reduction in uncertainty about the target provided by knowing each feature's value.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-muzero",
      "term": "MuZero",
      "definition": "A model-based RL algorithm that learns a latent dynamics model, reward predictor, and value/policy networks without requiring knowledge of the game rules. MuZero plans using its learned model and achieves superhuman performance across diverse domains.",
      "tags": [
        "Reinforcement Learning",
        "Planning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-muzero-algorithm",
      "term": "MuZero Algorithm",
      "definition": "A model-based reinforcement learning algorithm that learns a dynamic model of the environment without requiring knowledge of the game rules. Plans using the learned model with Monte Carlo tree search.",
      "tags": [
        "Algorithms",
        "Technical",
        "RL"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-mvbench",
      "term": "MVBench",
      "definition": "A multimodal video understanding benchmark testing temporal reasoning across 20 challenging video tasks. Evaluates the ability to understand dynamic visual content.",
      "tags": [
        "Benchmark",
        "Video",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-mvdream",
      "term": "MVDream",
      "definition": "A multi-view diffusion model that generates consistent multi-view images from text prompts by fine-tuning a 2D diffusion model with 3D-aware training.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-mycin",
      "term": "MYCIN",
      "definition": "An early expert system developed at Stanford in the 1970s for diagnosing bacterial infections and recommending antibiotics, demonstrating that rule-based AI could match or exceed human expert performance in narrow domains.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-mythic-ai",
      "term": "Mythic AI",
      "definition": "AI chip company developing analog matrix processors that perform neural network computations in flash memory arrays. Targets edge AI inference with very low power consumption.",
      "tags": [
        "Accelerator",
        "Startup",
        "Analog"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    }
  ]
}