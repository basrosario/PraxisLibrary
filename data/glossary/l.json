{
  "letter": "l",
  "count": 122,
  "terms": [
    {
      "id": "term-l-bfgs",
      "term": "L-BFGS",
      "definition": "Limited-memory Broyden-Fletcher-Goldfarb-Shanno is a quasi-Newton optimization method that approximates the inverse Hessian using a limited history of gradient updates. Widely used in traditional machine learning and fine-tuning tasks where second-order information is beneficial.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-l1-regularization",
      "term": "L1 Regularization",
      "definition": "A regularization technique that adds the sum of absolute values of model weights to the loss function, encouraging sparsity by driving some weights exactly to zero. Also known as Lasso regularization.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-l2-regularization",
      "term": "L2 Regularization",
      "definition": "A regularization technique that adds the sum of squared model weights to the loss function, penalizing large weights and encouraging them to be small but not exactly zero. Also known as Ridge regularization.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-label",
      "term": "Label",
      "definition": "The correct answer or category associated with training data in supervised learning. Human-provided labels teach models the patterns they should learn.",
      "tags": [
        "Data",
        "Supervised Learning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-label-bias",
      "term": "Label Bias",
      "definition": "Bias introduced into AI systems through inaccurate or subjective labels in training data. Human annotators may apply labels inconsistently or based on their own biases affecting model fairness.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-label-encoding",
      "term": "Label Encoding",
      "definition": "A technique that converts categorical values into integer codes, assigning each unique category a distinct numerical identifier. It introduces an implicit ordinal relationship that may not reflect the true data structure.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-label-propagation",
      "term": "Label Propagation",
      "definition": "A semi-supervised graph algorithm that propagates labels from labeled nodes to unlabeled nodes through the graph structure. Each node adopts the label most common among its neighbors iteratively until convergence. Simple and effective for community detection.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-label-smoothing",
      "term": "Label Smoothing",
      "definition": "A regularization technique that replaces hard one-hot target labels with soft labels that assign a small probability to incorrect classes. It prevents the model from becoming overconfident and improves generalization.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-label-smoothing-regularization",
      "term": "Label Smoothing Regularization",
      "definition": "A regularization technique that replaces hard one-hot target labels with soft labels distributing a small amount of probability mass across all classes. Prevents the model from becoming overconfident and improves calibration.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-laion",
      "term": "LAION",
      "definition": "The Large-scale Artificial Intelligence Open Network a nonprofit organization that created LAION-5B one of the largest openly available image-text datasets with 5.85 billion image-text pairs. LAION datasets were used to train models like Stable Diffusion.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-lamb",
      "term": "LAMB",
      "definition": "Layer-wise Adaptive Moments optimizer designed for large-batch distributed training. Extends LARS with Adam-style momentum. Used to train BERT in 76 minutes on TPU pods by enabling effective training with very large batch sizes.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lamb-optimizer",
      "term": "LAMB Optimizer",
      "definition": "Layer-wise Adaptive Moments optimizer for Batch training, a variant of Adam that applies layer-wise learning rate adaptation to enable stable training at very large batch sizes. LAMB was used to train BERT in 76 minutes.",
      "tags": [
        "Model Optimization",
        "Distributed Computing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lambda-calculus",
      "term": "Lambda Calculus",
      "definition": "A formal system in mathematical logic for expressing computation based on function abstraction and application developed by Alonzo Church in the 1930s. Lambda calculus influenced the design of functional programming languages and is closely related to the Turing machine model.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-lancedb",
      "term": "LanceDB",
      "definition": "An open-source serverless vector database built on the Lance columnar data format, supporting multimodal data storage with embedded and cloud-native deployment options and efficient disk-based indexing without requiring a separate server process.",
      "tags": [
        "Vector Database",
        "Open Source"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-lane-detection",
      "term": "Lane Detection",
      "definition": "The task of identifying and localizing lane markings on road surfaces in driving images or video, using curve fitting, segmentation, or anchor-based methods for autonomous driving applications.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-langchain",
      "term": "LangChain",
      "definition": "A popular framework for building applications with LLMs. Provides abstractions for chains, agents, memory, and tool use, simplifying complex AI application development.",
      "tags": [
        "Framework",
        "Application"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-langevin-dynamics",
      "term": "Langevin Dynamics",
      "definition": "A stochastic process that uses gradient information with added noise to sample from a probability distribution. Used in score-based generative models where samples are generated by running Langevin dynamics with the learned score function.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-language-identification",
      "term": "Language Identification",
      "definition": "The task of automatically determining what natural language a given text is written in, using features like character n-grams, word frequency patterns, and script detection.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-language-modeling",
      "term": "Language Modeling",
      "definition": "The task of learning a probability distribution over sequences of tokens, enabling the model to estimate the likelihood of a given text sequence or predict the next token in a sequence.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-language-understanding",
      "term": "Language Understanding (NLU)",
      "definition": "AI capability to comprehend the meaning, intent, and context of human language. Includes parsing structure, resolving references, and understanding implicit information.",
      "tags": [
        "NLP",
        "Capability"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-laplace-approximation",
      "term": "Laplace Approximation",
      "definition": "A technique for approximating a posterior distribution with a Gaussian centered at the mode (MAP estimate), using the curvature of the log-posterior (Hessian) to determine the covariance.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-large-batch-training",
      "term": "Large Batch Training",
      "definition": "Techniques for training neural networks with very large batch sizes (thousands to millions of samples) distributed across many GPUs. Large batch training requires careful learning rate scaling, warmup, and LARS/LAMB optimizers to maintain convergence.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llm",
      "term": "Large Language Model (LLM)",
      "definition": "An AI system trained on massive amounts of text data to understand and generate human language. Includes models like GPT-4, Claude, Gemini, and Llama with billions of parameters.",
      "tags": [
        "Model Type",
        "Core Concept"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lars",
      "term": "LARS",
      "definition": "Layer-wise Adaptive Rate Scaling adjusts the learning rate for each layer based on the ratio of weight norm to gradient norm. Enables training with extremely large batch sizes without accuracy degradation. Developed by You et al. in 2017.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lars-optimizer",
      "term": "LARS Optimizer",
      "definition": "Layer-wise Adaptive Rate Scaling, an optimizer that adjusts the learning rate per layer based on the ratio of weight norm to gradient norm. LARS enables training with batch sizes of up to 32K without accuracy degradation.",
      "tags": [
        "Model Optimization",
        "Distributed Computing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lasso-regression",
      "term": "Lasso Regression",
      "definition": "A linear regression method that applies L1 regularization to the coefficient estimates, performing both variable selection and regularization by driving some coefficients to exactly zero.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-late-chunking",
      "term": "Late Chunking",
      "definition": "A technique that first encodes an entire document through a long-context embedding model and then pools token embeddings into chunk-level representations, preserving cross-chunk contextual information.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-late-interaction",
      "term": "Late Interaction",
      "definition": "A neural retrieval paradigm where queries and documents are independently encoded into sets of token-level embeddings, and relevance is computed through lightweight interaction operations like MaxSim at query time, balancing efficiency and effectiveness.",
      "tags": [
        "Retrieval",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-latency",
      "term": "Latency",
      "definition": "The time delay between sending a prompt and receiving a response. Affected by model size, server load, prompt complexity, and output length.",
      "tags": [
        "Performance",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-latent-diffusion-model",
      "term": "Latent Diffusion Model",
      "definition": "A diffusion model that operates in the latent space of a pretrained autoencoder rather than pixel space, significantly reducing computational requirements while maintaining generation quality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-latent-dirichlet-allocation",
      "term": "Latent Dirichlet Allocation",
      "definition": "A generative probabilistic model for topic modeling that represents each document as a mixture of topics and each topic as a distribution over words, using Dirichlet priors for both distributions.",
      "tags": [
        "Machine Learning",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-latin-hypercube-sampling",
      "term": "Latin Hypercube Sampling",
      "definition": "A stratified sampling technique that divides each dimension into equal-probability intervals and ensures each interval is sampled exactly once, achieving more even coverage of the sample space than simple random sampling.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-law-of-large-numbers",
      "term": "Law of Large Numbers",
      "definition": "A theorem stating that as the number of independent trials increases, the sample average converges to the expected value. It provides the theoretical foundation for using sample statistics to estimate population parameters.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lawrence-fogel",
      "term": "Lawrence Fogel",
      "definition": "American engineer who introduced evolutionary programming in 1966 as a method for generating AI through simulated evolution. Along with John Holland and Ingo Rechenberg he is considered one of the fathers of evolutionary computation.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-layer-freezing",
      "term": "Layer Freezing",
      "definition": "A fine-tuning strategy that keeps some layers of a pretrained model fixed while only updating others. Typically earlier layers capturing general features are frozen while later task-specific layers are tuned. Reduces overfitting and training cost.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-layer-normalization",
      "term": "Layer Normalization",
      "definition": "A normalization technique that computes mean and variance across all features within a single training example rather than across the batch, making it suitable for variable-length sequences and small batch sizes.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-layer-wise-learning-rate-decay",
      "term": "Layer-wise Learning Rate Decay",
      "definition": "A technique that applies progressively smaller learning rates to earlier layers of a neural network during fine-tuning. Based on the insight that earlier layers learn more general features that require less adaptation.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-layout-analysis",
      "term": "Layout Analysis",
      "definition": "The process of detecting and classifying structural elements in document images (headers, paragraphs, tables, figures), establishing the reading order and hierarchical organization of content.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-leaky-relu",
      "term": "Leaky ReLU",
      "definition": "A variant of ReLU that allows a small non-zero gradient when the input is negative. Defined as f(x) = x if x > 0 and f(x) = alpha * x otherwise where alpha is typically 0.01. Addresses the dying ReLU problem where neurons can become permanently inactive.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-learned-positional-embedding",
      "term": "Learned Positional Embedding",
      "definition": "A trainable embedding table that assigns a learnable vector to each position in a sequence, allowing the model to discover optimal position representations during training.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-learning-curve",
      "term": "Learning Curve",
      "definition": "A plot showing model performance as a function of training set size or training iterations. It reveals whether a model suffers from high bias (underfitting) or high variance (overfitting) and guides data collection decisions.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-learning-rate",
      "term": "Learning Rate",
      "definition": "A hyperparameter controlling how much model weights are adjusted during training. Too high causes instability; too low causes slow training. Often scheduled to decrease over time.",
      "tags": [
        "Hyperparameter",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-learning-rate-schedule",
      "term": "Learning Rate Schedule",
      "definition": "A predefined strategy for adjusting the learning rate during training, such as step decay, exponential decay, or cosine annealing. Properly tuned schedules can significantly improve convergence speed and final model performance.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-learning-rate-warmup",
      "term": "Learning Rate Warmup",
      "definition": "A training technique that gradually increases the learning rate from near-zero to the target value over the first portion of training. Warmup stabilizes training dynamics when using large batch sizes or adaptive learning rates in distributed settings.",
      "tags": [
        "Model Optimization",
        "Distributed Computing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-least-to-most-decomposition",
      "term": "Least-to-Most Decomposition",
      "definition": "The first stage of least-to-most prompting where a complex problem is broken into a sequence of progressively more difficult sub-problems, with each sub-problem building on the solutions of easier preceding ones.",
      "tags": [
        "Prompt Engineering",
        "Decomposition"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-leave-one-out-cross-validation",
      "term": "Leave-One-Out Cross-Validation",
      "definition": "A cross-validation method where each observation serves as a single-element test set while all remaining observations form the training set. It provides nearly unbiased estimates but is computationally expensive for large datasets.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-legal-personhood-for-ai",
      "term": "Legal Personhood for AI",
      "definition": "The concept of granting AI systems legal rights and obligations similar to those of corporations. Debated as a potential framework for liability and accountability but criticized for potentially deflecting human responsibility.",
      "tags": [
        "Safety",
        "Policy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-legalbert",
      "term": "LegalBERT",
      "definition": "A BERT model pretrained on legal text corpora including court opinions legislation and contracts. Achieves improved performance on legal NLP tasks compared to general-purpose language models.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lemmatization",
      "term": "Lemmatization",
      "definition": "The process of reducing words to their dictionary base form (lemma) using morphological analysis and vocabulary lookup, producing valid words unlike stemming which may create non-words.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-lenet",
      "term": "LeNet",
      "definition": "A pioneering convolutional neural network designed by Yann LeCun in 1989 for handwritten digit recognition, successfully deployed by the US Postal Service and banks, demonstrating the practical viability of deep learning.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-length-penalty",
      "term": "Length Penalty",
      "definition": "A parameter in text generation that discourages or encourages longer outputs. Helps control verbosity and can be adjusted to match desired response length.",
      "tags": [
        "Generation",
        "Parameter"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-leslie-valiant",
      "term": "Leslie Valiant",
      "definition": "British-American computer scientist who received the Turing Award in 2010 for contributions to computational learning theory. His probably approximately correct (PAC) learning framework established formal foundations for machine learning theory.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-lethal-autonomous-weapons-systems",
      "term": "Lethal Autonomous Weapons Systems",
      "definition": "A class of autonomous weapons, sometimes called killer robots, capable of independently identifying and lethally engaging human targets. Their development is the subject of international campaigns for preemptive bans.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-levenshtein-distance",
      "term": "Levenshtein Distance",
      "definition": "A string metric measuring the minimum number of single-character insertions, deletions, and substitutions needed to transform one string into another, used in spell checking and fuzzy matching.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-leverage",
      "term": "Leverage",
      "definition": "A measure of how far an observation's predictor values are from the center of the predictor space. High-leverage points have an outsized potential to influence the regression fit, even if they are not outliers in the response.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lexical-ambiguity",
      "term": "Lexical Ambiguity",
      "definition": "The phenomenon where a word or phrase can be interpreted in multiple ways due to polysemy or homonymy, requiring context to determine the intended meaning.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-liability-for-ai",
      "term": "Liability for AI",
      "definition": "Legal responsibility for harm caused by AI systems. Current legal frameworks struggle to assign liability when AI makes autonomous decisions and new frameworks are being developed in various jurisdictions.",
      "tags": [
        "Safety",
        "Policy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-librispeech",
      "term": "LibriSpeech",
      "definition": "A corpus of approximately 1000 hours of read English speech derived from audiobooks created by Vassil Panayotov and colleagues in 2015. LibriSpeech became a standard benchmark for automatic speech recognition systems.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-licensing-for-ai",
      "term": "Licensing for AI",
      "definition": "Proposed regulatory requirements for AI developers or systems to obtain licenses before deployment similar to licensing in healthcare finance and aviation. Intended to ensure minimum safety and competency standards.",
      "tags": [
        "Safety",
        "Policy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-lidar",
      "term": "LiDAR",
      "definition": "Light Detection and Ranging, a remote sensing technology that measures distances by illuminating targets with laser pulses, producing dense 3D point clouds used in autonomous driving and mapping.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-lifecycle-assessment-for-ai",
      "term": "Lifecycle Assessment for AI",
      "definition": "Evaluation of the environmental social and economic impacts of an AI system throughout its entire lifecycle from data collection through training deployment and decommissioning.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-lightgbm",
      "term": "LightGBM",
      "definition": "A gradient boosting framework that uses histogram-based algorithms and leaf-wise tree growth for faster training on large datasets. It supports gradient-based one-side sampling and exclusive feature bundling.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lighthill-report",
      "term": "Lighthill Report",
      "definition": "A 1973 report by mathematician James Lighthill commissioned by the British Science Research Council that criticized AI research as failing to achieve its ambitious goals, leading to severe funding cuts in the UK.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-likelihood-function",
      "term": "Likelihood Function",
      "definition": "A function of the parameters of a statistical model, computed as the probability of the observed data for given parameter values. Unlike a probability distribution, it is evaluated over the parameter space with data fixed.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-likelihood-ratio-test",
      "term": "Likelihood Ratio Test",
      "definition": "A hypothesis test that compares the fit of two nested models by computing twice the difference in their log-likelihoods. Under the null hypothesis, this statistic follows an asymptotic chi-square distribution.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lime",
      "term": "LIME",
      "definition": "Local Interpretable Model-agnostic Explanations, a technique that explains individual predictions by fitting a simple interpretable model to perturbed samples around the instance of interest, weighted by proximity.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-linear-attention",
      "term": "Linear Attention",
      "definition": "An attention variant that replaces the softmax-based dot-product attention with a kernel-based approximation, achieving linear time and memory complexity with respect to sequence length.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-linear-discriminant-analysis",
      "term": "Linear Discriminant Analysis",
      "definition": "A supervised dimensionality reduction and classification technique that projects data onto directions that maximize the ratio of between-class variance to within-class variance, finding the most discriminative feature subspace.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-linear-function-approximation-rl",
      "term": "Linear Function Approximation in RL",
      "definition": "Value function estimation using a linear combination of state features, where the value is a weighted sum of feature values. Linear approximation offers convergence guarantees not available with nonlinear approximators like neural networks.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-linear-regression",
      "term": "Linear Regression",
      "definition": "A foundational ML algorithm that models the relationship between variables using a straight line. Simple but effective for many prediction tasks with linear relationships.",
      "tags": [
        "Algorithm",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-linear-transformer",
      "term": "Linear Transformer",
      "definition": "A transformer variant that replaces softmax attention with a kernel-based linear attention mechanism. Reduces computational complexity from quadratic to linear in sequence length. Trades some modeling quality for significant efficiency gains.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-linformer",
      "term": "Linformer",
      "definition": "A transformer that projects key and value matrices to a lower-dimensional space before computing attention, reducing the quadratic complexity of self-attention to linear in sequence length.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lion-optimizer",
      "term": "Lion Optimizer",
      "definition": "Evolved Sign Momentum optimizer discovered through program search by Google Brain in 2023. Uses only sign operations for updates making it more memory efficient than Adam. Shows improved performance on vision and language tasks.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-liquid-neural-network",
      "term": "Liquid Neural Network",
      "definition": "A continuous-time neural network inspired by biological neural circuits that can adapt its behavior based on input dynamics. Features time-varying parameters enabling compact models that handle sequential data with fewer neurons.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lisp",
      "term": "LISP",
      "definition": "A programming language family invented by John McCarthy in 1958 that became the dominant language for AI research for decades, featuring symbolic expression processing, garbage collection, and recursive functions.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-lisp-machine",
      "term": "LISP Machine",
      "definition": "Specialized computers designed to run the LISP programming language efficiently. Developed in the 1970s and 1980s at MIT and commercialized by companies like Symbolics and LMI LISP machines represented dedicated AI hardware. Their market collapse contributed to the second AI winter.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama",
      "term": "Llama",
      "definition": "Meta's open-weight family of large language models. Released with relatively permissive licenses, enabling widespread research and commercial use of capable open models.",
      "tags": [
        "Model",
        "Meta"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-2",
      "term": "LLaMA 2",
      "definition": "The second generation of Meta's open-weight language models available in 7B 13B and 70B parameter sizes. Includes chat-optimized variants fine-tuned with RLHF. Released with a permissive license for research and commercial use.",
      "tags": [
        "Models",
        "Fundamentals"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-3",
      "term": "LLaMA 3",
      "definition": "Meta's third generation of open-weight language models with improved pretraining data and architecture refinements. Available in multiple sizes with significantly improved performance on reasoning coding and multilingual tasks.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-guard",
      "term": "Llama Guard",
      "definition": "A safety classifier model fine-tuned from LLaMA for evaluating the safety of language model inputs and outputs. Classifies content against configurable safety taxonomies enabling customizable content moderation.",
      "tags": [
        "Models",
        "Safety"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-adapter",
      "term": "Llama-Adapter",
      "definition": "An efficient fine-tuning method that prepends a set of learnable adaptation prompts to the upper layers of a frozen LLaMA model. Achieves strong instruction following with only 1.2 million additional parameters.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-cpp",
      "term": "llama.cpp",
      "definition": "An open-source C/C++ library for efficient CPU and GPU inference of large language models using quantized weights. llama.cpp enables running models locally on consumer hardware through aggressive optimization and support for various quantization formats.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llamaindex",
      "term": "LlamaIndex",
      "definition": "A data framework for connecting LLMs to external data sources. Specializes in indexing, retrieval, and RAG applications with various data connectors and query engines.",
      "tags": [
        "Framework",
        "Application"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-llava",
      "term": "LLaVA",
      "definition": "Large Language and Vision Assistant is a multimodal model that connects a vision encoder to a language model using a simple projection layer. Achieves strong visual reasoning by fine-tuning on visual instruction-following data.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llemma",
      "term": "Llemma",
      "definition": "An open language model for mathematics built by continuing pretraining of Code Llama on a blend of mathematical documents and code. Achieves strong math performance while being fully open-source.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llm-as-judge",
      "term": "LLM-as-Judge",
      "definition": "An evaluation paradigm where a large language model is prompted to assess and score the quality of outputs from other models, providing scalable evaluation that approximates human judgment with explicit rating criteria and rubrics.",
      "tags": [
        "Evaluation",
        "LLM-Based"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llmlingua",
      "term": "LLMLingua",
      "definition": "A prompt compression framework that uses a small language model to identify and remove less informative tokens from prompts, achieving significant compression ratios while maintaining downstream task performance through budget-aware iterative token pruning.",
      "tags": [
        "Prompt Engineering",
        "Compression"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-load-balancing-loss",
      "term": "Load Balancing Loss",
      "definition": "An auxiliary loss term used in mixture-of-experts models to encourage uniform distribution of tokens across experts, preventing routing collapse where all inputs are sent to few experts.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-local-llm",
      "term": "Local LLM",
      "definition": "Running language models on personal hardware rather than through cloud APIs. Enables privacy, offline use, and cost savings, though requires capable hardware.",
      "tags": [
        "Deployment",
        "Privacy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-local-outlier-factor",
      "term": "Local Outlier Factor",
      "definition": "A density-based anomaly detection algorithm that compares the local density of a point to the densities of its neighbors. Points with substantially lower density than their neighbors are classified as outliers. Does not assume a specific data distribution.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-locality-sensitive-hashing",
      "term": "Locality-Sensitive Hashing",
      "definition": "An approximate nearest neighbor technique that hashes similar vectors into the same buckets with high probability using random projections, enabling sub-linear search time by only comparing vectors within matching hash buckets.",
      "tags": [
        "Vector Database",
        "Index Structure"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-locking-problem",
      "term": "Locking Problem",
      "definition": "The challenge that early AI systems may become entrenched and difficult to modify or replace once widely deployed creating path dependency that limits future safety improvements.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-loebner-prize",
      "term": "Loebner Prize",
      "definition": "An annual competition in artificial intelligence that awards prizes to the computer programs considered by the judges to be the most human-like. A practical implementation of the Turing Test that has run since 1991.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-loess",
      "term": "LOESS",
      "definition": "Locally Estimated Scatterplot Smoothing, a non-parametric regression method that fits local weighted polynomial regressions to subsets of the data, producing a smooth curve that adapts to local patterns.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lofti-zadeh-fuzzy-sets-paper",
      "term": "Lofti Zadeh Fuzzy Sets Paper",
      "definition": "The 1965 paper Fuzzy Sets by Lotfi Zadeh published in Information and Control that introduced fuzzy set theory. This work extended classical set theory to allow partial membership enabling mathematical treatment of vagueness and imprecision in reasoning.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-log-loss",
      "term": "Log Loss",
      "definition": "A loss function for binary classification that measures the negative log-likelihood of the true labels given the predicted probabilities. It heavily penalizes confident but incorrect predictions.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-log-likelihood",
      "term": "Log-Likelihood",
      "definition": "The natural logarithm of the likelihood function, used to simplify optimization because it converts products of probabilities into sums. Maximizing the log-likelihood is equivalent to maximizing the likelihood.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-log-normal-distribution",
      "term": "Log-Normal Distribution",
      "definition": "A continuous probability distribution of a random variable whose logarithm is normally distributed. It is used to model quantities that are products of many independent positive random variables.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-log-softmax",
      "term": "Log-Softmax",
      "definition": "The logarithm of the softmax function computed more numerically stably by using the log-sum-exp trick. Directly outputs log-probabilities. Used with negative log-likelihood loss for numerical precision in classification tasks.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-logic-programming",
      "term": "Logic Programming",
      "definition": "A programming paradigm based on formal logic where programs are expressed as a set of logical relations and computation proceeds by logical inference. Developed in the early 1970s by Robert Kowalski and Alain Colmerauer logic programming is exemplified by Prolog.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-logic-theorist",
      "term": "Logic Theorist",
      "definition": "A program written by Allen Newell and Herbert Simon in 1956 that could prove mathematical theorems from Principia Mathematica, widely considered the first artificial intelligence program.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-logistic-regression",
      "term": "Logistic Regression",
      "definition": "A linear classification model that predicts class probabilities using the logistic (sigmoid) function applied to a linear combination of input features. Despite its name, it is used for classification rather than regression.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-logistic-regression-history",
      "term": "Logistic Regression History",
      "definition": "The application of logistic regression to classification problems in machine learning. Originally developed for statistics by David Cox in 1958 logistic regression became a fundamental baseline classifier in machine learning and remains widely used for binary classification.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-logit",
      "term": "Logit",
      "definition": "The raw, unnormalized scores output by a model before converting to probabilities. In LLMs, logits represent the model's preference for each possible next token.",
      "tags": [
        "Technical",
        "Math"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-logit-bias",
      "term": "Logit Bias",
      "definition": "A technique that adds fixed values to the logits of specific tokens before sampling, used to encourage or suppress particular words or phrases in generated output.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-logit-lens",
      "term": "Logit Lens",
      "definition": "An interpretability technique that decodes intermediate transformer layer outputs through the final unembedding matrix to observe how predictions evolve through the network. Reveals how the model progressively refines its outputs across layers.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-long-context-fine-tuning",
      "term": "Long Context Fine-Tuning",
      "definition": "The process of adapting a pre-trained model to effectively utilize longer context windows than it was originally trained on, through continued training with progressively longer sequences and position interpolation.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lstm-history",
      "term": "Long Short-Term Memory",
      "definition": "A recurrent neural network architecture invented by Sepp Hochreiter and Jurgen Schmidhuber in 1997 that solved the vanishing gradient problem, enabling effective learning from long sequences and powering advances in speech and language processing.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-long-term-ai-safety",
      "term": "Long-term AI Safety",
      "definition": "Research focused on ensuring the safety of AI systems that may eventually match or exceed human-level capabilities. Addresses existential risks value alignment and the control problem for advanced AI.",
      "tags": [
        "Safety",
        "Fundamentals"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-longformer",
      "term": "Longformer",
      "definition": "A transformer variant that combines local sliding window attention with task-specific global attention on selected tokens, enabling efficient processing of documents with thousands of tokens.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lookahead-optimizer",
      "term": "Lookahead Optimizer",
      "definition": "A meta-optimizer that wraps around any base optimizer maintaining two sets of weights. The fast weights explore the loss landscape while slow weights are periodically updated toward the fast weights. Reduces variance and improves stability.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lora",
      "term": "LoRA (Low-Rank Adaptation)",
      "definition": "A parameter-efficient fine-tuning technique that trains only small additional matrices rather than the full model. Dramatically reduces memory and compute requirements for customization.",
      "tags": [
        "Training",
        "Efficiency"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-lora-diffusion",
      "term": "LoRA for Diffusion",
      "definition": "The application of Low-Rank Adaptation to diffusion models, enabling efficient fine-tuning of image generation models to learn new concepts, styles, or subjects with minimal additional parameters.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lora-fusion",
      "term": "LoRA Fusion",
      "definition": "The technique of combining multiple LoRA adapters trained for different tasks or styles into a single model by merging or dynamically weighting the adapter parameters during inference.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-loss-function",
      "term": "Loss Function",
      "definition": "A mathematical function measuring how wrong a model's predictions are. Training aims to minimize this loss, with common examples including cross-entropy and mean squared error.",
      "tags": [
        "Training",
        "Math"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-loss-scaling",
      "term": "Loss Scaling",
      "definition": "A technique used in FP16 mixed precision training that multiplies the loss by a large factor before backpropagation to prevent small gradient values from underflowing to zero. The scaled gradients are divided back before the optimizer step.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lost-in-the-middle",
      "term": "Lost in the Middle",
      "definition": "A documented phenomenon where language models perform worse at retrieving and using information placed in the middle of their context window compared to information at the beginning or end.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lotfi-zadeh",
      "term": "Lotfi Zadeh",
      "definition": "Azerbaijani-American mathematician and computer scientist (1921-2017) who invented fuzzy logic and fuzzy set theory in 1965, providing mathematical tools for handling uncertainty and imprecision in AI and control systems.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-lottery-ticket-hypothesis",
      "term": "Lottery Ticket Hypothesis",
      "definition": "The theory that dense randomly-initialized networks contain sparse subnetworks (winning tickets) that can be trained in isolation to match the full network's performance when initialized with their original weights.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-low-rank-approximation",
      "term": "Low-Rank Approximation",
      "definition": "A technique that approximates a matrix with a lower-rank matrix to reduce dimensionality computational cost or noise. The optimal rank-k approximation is given by truncated SVD. Fundamental to LoRA and other parameter-efficient methods.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-low-rank-factorization",
      "term": "Low-Rank Factorization",
      "definition": "A model compression technique that approximates large weight matrices as products of smaller matrices with reduced rank. Low-rank factorization reduces both parameters and computation, and is used in methods like LoRA for efficient fine-tuning.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lstm",
      "term": "LSTM (Long Short-Term Memory)",
      "definition": "A recurrent neural network architecture designed to capture long-range dependencies in sequences. Was the dominant NLP architecture before transformers emerged.",
      "tags": [
        "Architecture",
        "Historical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    }
  ]
}