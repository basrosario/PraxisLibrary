{
  "letter": "l",
  "count": 215,
  "terms": [
    {
      "id": "term-l-bfgs",
      "term": "L-BFGS",
      "definition": "Limited-memory Broyden-Fletcher-Goldfarb-Shanno is a quasi-Newton optimization method that approximates the inverse Hessian using a limited history of gradient updates. Widely used in traditional machine learning and fine-tuning tasks where second-order information is beneficial.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-l-eval",
      "term": "L-Eval",
      "definition": "A comprehensive long-context evaluation benchmark containing diverse tasks with inputs ranging from 3000 to 60000 words. Tests summarization QA and reasoning on long documents.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-l1-regularization",
      "term": "L1 Regularization",
      "definition": "A regularization technique that adds the sum of absolute values of model weights to the loss function, encouraging sparsity by driving some weights exactly to zero. Also known as Lasso regularization.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-l2-regularization",
      "term": "L2 Regularization",
      "definition": "A regularization technique that adds the sum of squared model weights to the loss function, penalizing large weights and encouraging them to be small but not exactly zero. Also known as Ridge regularization.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-label",
      "term": "Label",
      "definition": "The correct answer or category associated with training data in supervised learning. Human-provided labels teach models the patterns they should learn.",
      "tags": [
        "Data",
        "Supervised Learning"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-label-bias",
      "term": "Label Bias",
      "definition": "Bias introduced into AI systems through inaccurate or subjective labels in training data. Human annotators may apply labels inconsistently or based on their own biases affecting model fairness.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-label-encoding",
      "term": "Label Encoding",
      "definition": "A technique that converts categorical values into integer codes, assigning each unique category a distinct numerical identifier. It introduces an implicit ordinal relationship that may not reflect the true data structure.",
      "tags": [
        "Machine Learning",
        "Feature Engineering"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-label-propagation",
      "term": "Label Propagation",
      "definition": "A semi-supervised graph algorithm that propagates labels from labeled nodes to unlabeled nodes through the graph structure. Each node adopts the label most common among its neighbors iteratively until convergence. Simple and effective for community detection.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-label-smoothing",
      "term": "Label Smoothing",
      "definition": "A regularization technique that replaces hard one-hot target labels with soft labels that assign a small probability to incorrect classes. It prevents the model from becoming overconfident and improves generalization.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-label-smoothing-regularization",
      "term": "Label Smoothing Regularization",
      "definition": "A regularization technique that replaces hard one-hot target labels with soft labels distributing a small amount of probability mass across all classes. Prevents the model from becoming overconfident and improves calibration.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lag-llama",
      "term": "Lag-Llama",
      "definition": "A foundation model for probabilistic time series forecasting that uses a Llama-based architecture with lag features for zero-shot and few-shot prediction.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lagrangian-relaxation",
      "term": "Lagrangian Relaxation",
      "definition": "An optimization technique that relaxes hard constraints by incorporating them into the objective function with Lagrange multipliers. Provides bounds on the optimal value and is used in combinatorial optimization.",
      "tags": [
        "Algorithms",
        "Technical",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-laion",
      "term": "LAION",
      "definition": "The Large-scale Artificial Intelligence Open Network a nonprofit organization that created LAION-5B one of the largest openly available image-text datasets with 5.85 billion image-text pairs. LAION datasets were used to train models like Stable Diffusion.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-laion-400m",
      "term": "LAION-400M",
      "definition": "A dataset of 400 million image-text pairs from Common Crawl filtered using CLIP similarity scores. Used for training open-source vision-language and image generation models.",
      "tags": [
        "Training Corpus",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-laion-5b",
      "term": "LAION-5B",
      "definition": "A dataset of 5.85 billion image-text pairs filtered from Common Crawl. One of the largest publicly available datasets for training multimodal models like Stable Diffusion.",
      "tags": [
        "Training Corpus",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-laion-aesthetics",
      "term": "LAION-Aesthetics",
      "definition": "A subset of LAION-5B filtered for aesthetic quality using a learned aesthetic predictor. Used to train image generation models that produce more visually appealing outputs.",
      "tags": [
        "Training Corpus",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-lam-research",
      "term": "Lam Research",
      "definition": "American company manufacturing semiconductor fabrication equipment specializing in etching deposition and cleaning. Their tools are essential for producing the intricate features of modern AI chips.",
      "tags": [
        "Manufacturing",
        "Equipment",
        "Company"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-lamb",
      "term": "LAMB",
      "definition": "Layer-wise Adaptive Moments optimizer designed for large-batch distributed training. Extends LARS with Adam-style momentum. Used to train BERT in 76 minutes on TPU pods by enabling effective training with very large batch sizes.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lamb-optimizer",
      "term": "LAMB Optimizer",
      "definition": "Layer-wise Adaptive Moments optimizer for Batch training, a variant of Adam that applies layer-wise learning rate adaptation to enable stable training at very large batch sizes. LAMB was used to train BERT in 76 minutes.",
      "tags": [
        "Model Optimization",
        "Distributed Computing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lambada",
      "term": "LAMBADA",
      "definition": "Language Modeling Broadened to Account for Discourse Aspects a dataset testing the ability of language models to predict the final word of passages requiring broad discourse understanding.",
      "tags": [
        "Benchmark",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-lambda-calculus",
      "term": "Lambda Calculus",
      "definition": "A formal system in mathematical logic for expressing computation based on function abstraction and application developed by Alonzo Church in the 1930s. Lambda calculus influenced the design of functional programming languages and is closely related to the Turing machine model.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-lancedb",
      "term": "LanceDB",
      "definition": "An open-source serverless vector database built on the Lance columnar data format, supporting multimodal data storage with embedded and cloud-native deployment options and efficient disk-based indexing without requiring a separate server process.",
      "tags": [
        "Vector Database",
        "Open Source"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-lanczos-algorithm",
      "term": "Lanczos Algorithm",
      "definition": "A Krylov subspace method for computing eigenvalues and eigenvectors of large symmetric matrices. Produces a tridiagonal matrix whose eigenvalues approximate those of the original matrix with increasing accuracy.",
      "tags": [
        "Algorithms",
        "Technical",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lane-detection",
      "term": "Lane Detection",
      "definition": "The task of identifying and localizing lane markings on road surfaces in driving images or video, using curve fitting, segmentation, or anchor-based methods for autonomous driving applications.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-langchain",
      "term": "LangChain",
      "definition": "A popular framework for building applications with LLMs. Provides abstractions for chains, agents, memory, and tool use, simplifying complex AI application development.",
      "tags": [
        "Framework",
        "Application"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-langevin-dynamics",
      "term": "Langevin Dynamics",
      "definition": "A stochastic process that uses gradient information with added noise to sample from a probability distribution. Used in score-based generative models where samples are generated by running Langevin dynamics with the learned score function.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-language-identification",
      "term": "Language Identification",
      "definition": "The task of automatically determining what natural language a given text is written in, using features like character n-grams, word frequency patterns, and script detection.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-language-model-perplexity",
      "term": "Language Model Perplexity",
      "definition": "A metric for evaluating language models that measures how well the model predicts a held-out test set. Defined as the exponential of the average negative log-likelihood per token with lower values indicating better models.",
      "tags": [
        "Algorithms",
        "Technical",
        "NLP"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-language-modeling",
      "term": "Language Modeling",
      "definition": "The task of learning a probability distribution over sequences of tokens, enabling the model to estimate the likelihood of a given text sequence or predict the next token in a sequence.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-language-understanding",
      "term": "Language Understanding (NLU)",
      "definition": "AI capability to comprehend the meaning, intent, and context of human language. Includes parsing structure, resolving references, and understanding implicit information.",
      "tags": [
        "NLP",
        "Capability"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-laplace-approximation",
      "term": "Laplace Approximation",
      "definition": "A technique for approximating a posterior distribution with a Gaussian centered at the mode (MAP estimate), using the curvature of the log-posterior (Hessian) to determine the covariance.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-laplace-mechanism",
      "term": "Laplace Mechanism",
      "definition": "A differential privacy mechanism that adds noise drawn from the Laplace distribution calibrated to the sensitivity of the query. Provides pure epsilon-differential privacy for numerical query results.",
      "tags": [
        "Algorithms",
        "Technical",
        "Privacy"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-laplacian-eigenmaps-algorithm",
      "term": "Laplacian Eigenmaps Algorithm",
      "definition": "A spectral method for nonlinear dimensionality reduction that uses the graph Laplacian to preserve local neighborhood distances. Finds eigenvectors of the Laplacian that provide an optimal low-dimensional embedding.",
      "tags": [
        "Algorithms",
        "Technical",
        "Dimensionality Reduction"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-laplacian-of-gaussian",
      "term": "Laplacian of Gaussian",
      "definition": "An edge detection operator that applies a Gaussian blur followed by the Laplace operator to detect edges at a specific scale. Zero-crossings of the output indicate edge locations in the image.",
      "tags": [
        "Algorithms",
        "Technical",
        "Vision"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-large-batch-training",
      "term": "Large Batch Training",
      "definition": "Techniques for training neural networks with very large batch sizes (thousands to millions of samples) distributed across many GPUs. Large batch training requires careful learning rate scaling, warmup, and LARS/LAMB optimizers to maintain convergence.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llm",
      "term": "Large Language Model (LLM)",
      "definition": "An AI system trained on massive amounts of text data to understand and generate human language. Includes models like GPT-4, Claude, Gemini, and Llama with billions of parameters.",
      "tags": [
        "Model Type",
        "Core Concept"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-large-margin-nearest-neighbor",
      "term": "Large-Margin Nearest Neighbor",
      "definition": "A metric learning algorithm that learns a Mahalanobis distance metric for k-nearest-neighbor classification. Optimizes the metric to pull same-class neighbors closer while pushing different-class points beyond a margin.",
      "tags": [
        "Algorithms",
        "Technical",
        "Dimensionality Reduction"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lars",
      "term": "LARS",
      "definition": "Layer-wise Adaptive Rate Scaling adjusts the learning rate for each layer based on the ratio of weight norm to gradient norm. Enables training with extremely large batch sizes without accuracy degradation. Developed by You et al. in 2017.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lars-optimizer",
      "term": "LARS Optimizer",
      "definition": "Layer-wise Adaptive Rate Scaling, an optimizer that adjusts the learning rate per layer based on the ratio of weight norm to gradient norm. LARS enables training with batch sizes of up to 32K without accuracy degradation.",
      "tags": [
        "Model Optimization",
        "Distributed Computing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lasso-regression",
      "term": "Lasso Regression",
      "definition": "A linear regression method that applies L1 regularization to the coefficient estimates, performing both variable selection and regularization by driving some coefficients to exactly zero.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lastletterconcat",
      "term": "LastLetterConcat",
      "definition": "A benchmark testing the ability of language models to concatenate the last letters of a sequence of words. Tests basic symbolic manipulation capabilities.",
      "tags": [
        "Benchmark",
        "NLP",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-late-chunking",
      "term": "Late Chunking",
      "definition": "A technique that first encodes an entire document through a long-context embedding model and then pools token embeddings into chunk-level representations, preserving cross-chunk contextual information.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-late-interaction",
      "term": "Late Interaction",
      "definition": "A neural retrieval paradigm where queries and documents are independently encoded into sets of token-level embeddings, and relevance is computed through lightweight interaction operations like MaxSim at query time, balancing efficiency and effectiveness.",
      "tags": [
        "Retrieval",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-latency",
      "term": "Latency",
      "definition": "The time delay between sending a prompt and receiving a response. Affected by model size, server load, prompt complexity, and output length.",
      "tags": [
        "Performance",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-latent-diffusion-model",
      "term": "Latent Diffusion Model",
      "definition": "A diffusion model that operates in the latent space of a pretrained autoencoder rather than pixel space, significantly reducing computational requirements while maintaining generation quality.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-latent-dirichlet-allocation",
      "term": "Latent Dirichlet Allocation",
      "definition": "A generative probabilistic model for topic modeling that represents each document as a mixture of topics and each topic as a distribution over words, using Dirichlet priors for both distributions.",
      "tags": [
        "Machine Learning",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-latent-semantic-analysis",
      "term": "Latent Semantic Analysis",
      "definition": "A technique that applies SVD to a term-document matrix to discover latent semantic relationships between words and documents. Reduces dimensionality to capture conceptual associations beyond surface-level word matching.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "NLP"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-latin-hypercube-sampling",
      "term": "Latin Hypercube Sampling",
      "definition": "A stratified sampling technique that divides each dimension into equal-probability intervals and ensures each interval is sampled exactly once, achieving more even coverage of the sample space than simple random sampling.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lauragpt",
      "term": "LauraGPT",
      "definition": "A versatile large language model for audio that handles speech recognition and translation and synthesis and audio captioning in a unified architecture.",
      "tags": [
        "Models",
        "Technical",
        "Audio",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lave",
      "term": "LAVE",
      "definition": "Language-Aligned Visual Evaluation a methodology for evaluating generated images using language-vision models. Provides automated assessment of text-image alignment quality.",
      "tags": [
        "Evaluation",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-lavis",
      "term": "LAVIS",
      "definition": "A unified library and collection of vision-language pre-trained models from Salesforce for diverse multimodal tasks including visual QA and captioning.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-law-of-large-numbers",
      "term": "Law of Large Numbers",
      "definition": "A theorem stating that as the number of independent trials increases, the sample average converges to the expected value. It provides the theoretical foundation for using sample statistics to estimate population parameters.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lawrence-fogel",
      "term": "Lawrence Fogel",
      "definition": "American engineer who introduced evolutionary programming in 1966 as a method for generating AI through simulated evolution. Along with John Holland and Ingo Rechenberg he is considered one of the fathers of evolutionary computation.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-layer-freezing",
      "term": "Layer Freezing",
      "definition": "A fine-tuning strategy that keeps some layers of a pretrained model fixed while only updating others. Typically earlier layers capturing general features are frozen while later task-specific layers are tuned. Reduces overfitting and training cost.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-layer-normalization",
      "term": "Layer Normalization",
      "definition": "A normalization technique that computes mean and variance across all features within a single training example rather than across the batch, making it suitable for variable-length sequences and small batch sizes.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-layer-wise-learning-rate-decay",
      "term": "Layer-wise Learning Rate Decay",
      "definition": "A technique that applies progressively smaller learning rates to earlier layers of a neural network during fine-tuning. Based on the insight that earlier layers learn more general features that require less adaptation.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-layout-analysis",
      "term": "Layout Analysis",
      "definition": "The process of detecting and classifying structural elements in document images (headers, paragraphs, tables, figures), establishing the reading order and hierarchical organization of content.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-layoutlm",
      "term": "LayoutLM",
      "definition": "A pre-trained model from Microsoft that jointly models text and layout information from scanned documents for document understanding tasks.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-layoutlmv2",
      "term": "LayoutLMv2",
      "definition": "A multimodal pre-training approach for document AI that adds visual features and spatial-aware self-attention to the original LayoutLM architecture.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-layoutlmv3",
      "term": "LayoutLMv3",
      "definition": "A unified multimodal model for document AI that uses word-patch alignment and masked image modeling for pre-training without requiring pre-extracted visual features.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lcm",
      "term": "LCM",
      "definition": "Latent Consistency Model is a distilled diffusion model that enables high-quality image generation in very few inference steps through consistency distillation.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-ldpc-decoding-algorithm",
      "term": "LDPC Decoding Algorithm",
      "definition": "An algorithm for decoding low-density parity-check codes using iterative message passing on the factor graph. Achieves near-Shannon-limit performance and is used in WiFi and 5G and satellite communication standards.",
      "tags": [
        "Algorithms",
        "Technical",
        "Information Theory"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-le-chat-mistral",
      "term": "Le Chat Mistral",
      "definition": "The conversational AI assistant interface from Mistral AI powered by their language models for interactive dialogue and task completion.",
      "tags": [
        "Models",
        "Technical",
        "NLP",
        "Products"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-leaky-relu",
      "term": "Leaky ReLU",
      "definition": "A variant of ReLU that allows a small non-zero gradient when the input is negative. Defined as f(x) = x if x > 0 and f(x) = alpha * x otherwise where alpha is typically 0.01. Addresses the dying ReLU problem where neurons can become permanently inactive.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-leapfrog-integration",
      "term": "Leapfrog Integration",
      "definition": "A numerical method for solving differential equations that updates positions and velocities at interleaved time points. Symplectic and time-reversible making it ideal for Hamiltonian systems and molecular dynamics simulations.",
      "tags": [
        "Algorithms",
        "Technical",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-learned-positional-embedding",
      "term": "Learned Positional Embedding",
      "definition": "A trainable embedding table that assigns a learnable vector to each position in a sequence, allowing the model to discover optimal position representations during training.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-learning-curve",
      "term": "Learning Curve",
      "definition": "A plot showing model performance as a function of training set size or training iterations. It reveals whether a model suffers from high bias (underfitting) or high variance (overfitting) and guides data collection decisions.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-learning-rate",
      "term": "Learning Rate",
      "definition": "A hyperparameter controlling how much model weights are adjusted during training. Too high causes instability; too low causes slow training. Often scheduled to decrease over time.",
      "tags": [
        "Hyperparameter",
        "Training"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-learning-rate-schedule",
      "term": "Learning Rate Schedule",
      "definition": "A predefined strategy for adjusting the learning rate during training, such as step decay, exponential decay, or cosine annealing. Properly tuned schedules can significantly improve convergence speed and final model performance.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-learning-rate-warmup",
      "term": "Learning Rate Warmup",
      "definition": "A training technique that gradually increases the learning rate from near-zero to the target value over the first portion of training. Warmup stabilizes training dynamics when using large batch sizes or adaptive learning rates in distributed settings.",
      "tags": [
        "Model Optimization",
        "Distributed Computing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-least-to-most-decomposition",
      "term": "Least-to-Most Decomposition",
      "definition": "The first stage of least-to-most prompting where a complex problem is broken into a sequence of progressively more difficult sub-problems, with each sub-problem building on the solutions of easier preceding ones.",
      "tags": [
        "Prompt Engineering",
        "Decomposition"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-leave-one-out-cross-validation",
      "term": "Leave-One-Out Cross-Validation",
      "definition": "A cross-validation method where each observation serves as a single-element test set while all remaining observations form the training set. It provides nearly unbiased estimates but is computationally expensive for large datasets.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-leetcode-dataset",
      "term": "LeetCode Dataset",
      "definition": "Collections of programming problems from LeetCode with solutions used for evaluating code generation capabilities on algorithmic and data structure challenges.",
      "tags": [
        "Benchmark",
        "Code"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-leftist-heap-algorithm",
      "term": "Leftist Heap Algorithm",
      "definition": "A variant of a binary heap that supports efficient merging by maintaining the leftist property where the right spine is always the shortest path. Merge operations run in O(log n) time.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-legal-personhood-for-ai",
      "term": "Legal Personhood for AI",
      "definition": "The concept of granting AI systems legal rights and obligations similar to those of corporations. Debated as a potential framework for liability and accountability but criticized for potentially deflecting human responsibility.",
      "tags": [
        "Safety",
        "Policy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-legalbert",
      "term": "LegalBERT",
      "definition": "A BERT model pretrained on legal text corpora including court opinions legislation and contracts. Achieves improved performance on legal NLP tasks compared to general-purpose language models.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lemmatization",
      "term": "Lemmatization",
      "definition": "The process of reducing words to their dictionary base form (lemma) using morphological analysis and vocabulary lookup, producing valid words unlike stemming which may create non-words.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-lempel-ziv-77-algorithm",
      "term": "Lempel-Ziv 77 Algorithm",
      "definition": "A sliding-window dictionary compression algorithm that encodes repeated patterns as references to earlier occurrences in the data stream. Forms the basis of deflate compression used in gzip and ZIP formats.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Information Theory"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lempel-ziv-welch-algorithm",
      "term": "Lempel-Ziv-Welch Algorithm",
      "definition": "A dictionary-based lossless compression algorithm that builds a translation table from the input data during encoding. Used in GIF image format and Unix compress utility and does not require transmitting the dictionary.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Information Theory"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lenet",
      "term": "LeNet",
      "definition": "A pioneering convolutional neural network designed by Yann LeCun in 1989 for handwritten digit recognition, successfully deployed by the US Postal Service and banks, demonstrating the practical viability of deep learning.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-length-penalty",
      "term": "Length Penalty",
      "definition": "A parameter in text generation that discourages or encourages longer outputs. Helps control verbosity and can be adjusted to match desired response length.",
      "tags": [
        "Generation",
        "Parameter"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-leonardo-supercomputer",
      "term": "Leonardo Supercomputer",
      "definition": "Italian pre-exascale supercomputer using NVIDIA A100 GPUs operated by CINECA. Supports European AI research and scientific computing as part of the EuroHPC initiative.",
      "tags": [
        "Supercomputer",
        "NVIDIA",
        "Europe"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-leslie-valiant",
      "term": "Leslie Valiant",
      "definition": "British-American computer scientist who received the Turing Award in 2010 for contributions to computational learning theory. His probably approximately correct (PAC) learning framework established formal foundations for machine learning theory.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-lethal-autonomous-weapons-systems",
      "term": "Lethal Autonomous Weapons Systems",
      "definition": "A class of autonomous weapons, sometimes called killer robots, capable of independently identifying and lethally engaging human targets. Their development is the subject of international campaigns for preemptive bans.",
      "tags": [
        "AI Ethics",
        "AI Safety"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-level-set-method",
      "term": "Level Set Method",
      "definition": "A numerical technique for tracking interfaces and shapes that represents boundaries as the zero level set of a higher-dimensional function. Used in image segmentation and fluid dynamics and computational geometry.",
      "tags": [
        "Algorithms",
        "Technical",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-levenberg-marquardt-algorithm",
      "term": "Levenberg-Marquardt Algorithm",
      "definition": "An iterative method for solving nonlinear least squares problems that interpolates between Gauss-Newton and gradient descent. Adjusts a damping parameter to switch between the two strategies based on convergence behavior.",
      "tags": [
        "Algorithms",
        "Technical",
        "Optimization",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-levenshtein-distance",
      "term": "Levenshtein Distance",
      "definition": "A string metric measuring the minimum number of single-character insertions, deletions, and substitutions needed to transform one string into another, used in spell checking and fuzzy matching.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-leverage",
      "term": "Leverage",
      "definition": "A measure of how far an observation's predictor values are from the center of the predictor space. High-leverage points have an outsized potential to influence the regression fit, even if they are not outliers in the response.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-levit",
      "term": "LeViT",
      "definition": "A fast inference image classification model that introduces attention bias and shrinking attention to create a highly efficient vision Transformer.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lexical-ambiguity",
      "term": "Lexical Ambiguity",
      "definition": "The phenomenon where a word or phrase can be interpreted in multiple ways due to polysemy or homonymy, requiring context to determine the intended meaning.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-lfw",
      "term": "LFW",
      "definition": "Labeled Faces in the Wild a dataset of 13000 face images of 5749 people collected from the web. The standard benchmark for evaluating face verification algorithms in unconstrained conditions.",
      "tags": [
        "Benchmark",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-lgm",
      "term": "LGM",
      "definition": "Large Gaussian Model generates high-quality 3D objects from text or image inputs using a large multi-view Gaussian generation approach in a single forward pass.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-liability-for-ai",
      "term": "Liability for AI",
      "definition": "Legal responsibility for harm caused by AI systems. Current legal frameworks struggle to assign liability when AI makes autonomous decisions and new frameworks are being developed in various jurisdictions.",
      "tags": [
        "Safety",
        "Policy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-librilight",
      "term": "LibriLight",
      "definition": "A large-scale semi-supervised speech dataset containing 60000 hours of unlabeled English speech from LibriVox. Designed for research on low-resource and self-supervised speech recognition.",
      "tags": [
        "Training Corpus",
        "Speech"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-librispeech",
      "term": "LibriSpeech",
      "definition": "A corpus of approximately 1000 hours of read English speech derived from audiobooks created by Vassil Panayotov and colleagues in 2015. LibriSpeech became a standard benchmark for automatic speech recognition systems.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-licensing-for-ai",
      "term": "Licensing for AI",
      "definition": "Proposed regulatory requirements for AI developers or systems to obtain licenses before deployment similar to licensing in healthcare finance and aviation. Intended to ensure minimum safety and competency standards.",
      "tags": [
        "Safety",
        "Policy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-lidar",
      "term": "LiDAR",
      "definition": "Light Detection and Ranging, a remote sensing technology that measures distances by illuminating targets with laser pulses, producing dense 3D point clouds used in autonomous driving and mapping.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-lifecycle-assessment-for-ai",
      "term": "Lifecycle Assessment for AI",
      "definition": "Evaluation of the environmental social and economic impacts of an AI system throughout its entire lifecycle from data collection through training deployment and decommissioning.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-lightgbm",
      "term": "LightGBM",
      "definition": "A gradient boosting framework that uses histogram-based algorithms and leaf-wise tree growth for faster training on large datasets. It supports gradient-based one-side sampling and exclusive feature bundling.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lightgbm-model",
      "term": "LightGBM Model",
      "definition": "A gradient boosting framework from Microsoft that uses leaf-wise tree growth and histogram-based splitting for fast training on large datasets.",
      "tags": [
        "Models",
        "Fundamentals"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lightgcn",
      "term": "LightGCN",
      "definition": "A simplified graph convolution network for recommendation that removes feature transformation and nonlinear activation to improve performance and efficiency.",
      "tags": [
        "Models",
        "Technical",
        "Recommendation"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lighthill-report",
      "term": "Lighthill Report",
      "definition": "A 1973 report by mathematician James Lighthill commissioned by the British Science Research Council that criticized AI research as failing to achieve its ambitious goals, leading to severe funding cuts in the UK.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-lightmatter",
      "term": "Lightmatter",
      "definition": "Photonic AI hardware company developing optical interconnects and photonic compute chips that use light instead of electrons for matrix operations. Promises dramatic energy efficiency improvements.",
      "tags": [
        "Accelerator",
        "Startup",
        "Photonic"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-likelihood-function",
      "term": "Likelihood Function",
      "definition": "A function of the parameters of a statistical model, computed as the probability of the observed data for given parameter values. Unlike a probability distribution, it is evaluated over the parameter space with data fixed.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-likelihood-ratio-test",
      "term": "Likelihood Ratio Test",
      "definition": "A hypothesis test that compares the fit of two nested models by computing twice the difference in their log-likelihoods. Under the null hypothesis, this statistic follows an asymptotic chi-square distribution.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lila",
      "term": "LILA",
      "definition": "Labeled Information Library of Alexandria a repository of camera trap datasets for wildlife conservation. Aggregates millions of annotated wildlife images from conservation projects worldwide.",
      "tags": [
        "Benchmark",
        "Computer Vision",
        "Conservation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-lima-dataset",
      "term": "Lima Dataset",
      "definition": "A carefully curated dataset of 1000 high-quality instruction-response pairs demonstrating that a small amount of excellent data can produce capable instruction-following models.",
      "tags": [
        "Training Corpus",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-lime",
      "term": "LIME",
      "definition": "Local Interpretable Model-agnostic Explanations, a technique that explains individual predictions by fitting a simple interpretable model to perturbed samples around the instance of interest, weighted by proximity.",
      "tags": [
        "Machine Learning",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-linear-attention",
      "term": "Linear Attention",
      "definition": "An attention variant that replaces the softmax-based dot-product attention with a kernel-based approximation, achieving linear time and memory complexity with respect to sequence length.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-linear-discriminant-analysis",
      "term": "Linear Discriminant Analysis",
      "definition": "A supervised dimensionality reduction and classification technique that projects data onto directions that maximize the ratio of between-class variance to within-class variance, finding the most discriminative feature subspace.",
      "tags": [
        "Machine Learning",
        "Dimensionality Reduction"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-linear-function-approximation-rl",
      "term": "Linear Function Approximation in RL",
      "definition": "Value function estimation using a linear combination of state features, where the value is a weighted sum of feature values. Linear approximation offers convergence guarantees not available with nonlinear approximators like neural networks.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-linear-hashing-algorithm",
      "term": "Linear Hashing Algorithm",
      "definition": "A dynamic hashing scheme that splits buckets in a predetermined order rather than the overflowing bucket. Maintains a controlled load factor and grows the hash table incrementally without global reorganization.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-linear-regression",
      "term": "Linear Regression",
      "definition": "A foundational ML algorithm that models the relationship between variables using a straight line. Simple but effective for many prediction tasks with linear relationships.",
      "tags": [
        "Algorithm",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-linear-regression-model",
      "term": "Linear Regression Model",
      "definition": "A statistical model that predicts a continuous target variable as a weighted linear combination of input features plus an intercept term.",
      "tags": [
        "Models",
        "Fundamentals",
        "History"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-linear-transformer",
      "term": "Linear Transformer",
      "definition": "A transformer variant that replaces softmax attention with a kernel-based linear attention mechanism. Reduces computational complexity from quadratic to linear in sequence length. Trades some modeling quality for significant efficiency gains.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-linformer",
      "term": "Linformer",
      "definition": "A transformer that projects key and value matrices to a lower-dimensional space before computing attention, reducing the quadratic complexity of self-attention to linear in sequence length.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lingo-1",
      "term": "LINGO-1",
      "definition": "A visual language model from Wayve designed for autonomous driving that provides natural language explanations of driving decisions and scene understanding.",
      "tags": [
        "Models",
        "Technical",
        "Autonomous",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-link-cut-tree",
      "term": "Link-Cut Tree",
      "definition": "A data structure that represents a forest of rooted trees and supports dynamic link and cut operations. Enables path queries and updates in O(log n) amortized time using splay trees internally.",
      "tags": [
        "Algorithms",
        "Technical",
        "Graph",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-linpack-benchmark",
      "term": "LINPACK Benchmark",
      "definition": "Standard benchmark for measuring floating-point computation speed by solving dense linear equations. Used as the primary metric for the TOP500 supercomputer ranking.",
      "tags": [
        "Benchmark",
        "Performance",
        "Standard"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-lion-optimizer",
      "term": "Lion Optimizer",
      "definition": "Evolved Sign Momentum optimizer discovered through program search by Google Brain in 2023. Uses only sign operations for updates making it more memory efficient than Adam. Shows improved performance on vision and language tasks.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-liquid-cooling",
      "term": "Liquid Cooling",
      "definition": "Cooling method using liquid coolant to remove heat from computing components. Increasingly essential for high-power AI GPUs like H100 that generate hundreds of watts per chip.",
      "tags": [
        "Cooling",
        "Data Center"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-liquid-neural-network",
      "term": "Liquid Neural Network",
      "definition": "A continuous-time neural network inspired by biological neural circuits that can adapt its behavior based on input dynamics. Features time-varying parameters enabling compact models that handle sequential data with fewer neurons.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lisp",
      "term": "LISP",
      "definition": "A programming language family invented by John McCarthy in 1958 that became the dominant language for AI research for decades, featuring symbolic expression processing, garbage collection, and recursive functions.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-lisp-machine",
      "term": "LISP Machine",
      "definition": "Specialized computers designed to run the LISP programming language efficiently. Developed in the 1970s and 1980s at MIT and commercialized by companies like Symbolics and LMI LISP machines represented dedicated AI hardware. Their market collapse contributed to the second AI winter.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-lithography",
      "term": "Lithography",
      "definition": "Process of transferring circuit patterns onto semiconductor wafers using light. The most critical and complex step in chip manufacturing determining the minimum feature size achievable.",
      "tags": [
        "Fabrication",
        "Manufacturing",
        "Process"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-livebench",
      "term": "LiveBench",
      "definition": "A continuously updated benchmark for language models using questions from recent sources to prevent contamination. Provides fresh evaluation that models cannot have memorized.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-livecodebench",
      "term": "LiveCodeBench",
      "definition": "A continuously updated benchmark of competitive programming problems collected after model training cutoffs. Designed to prevent data contamination in code generation evaluation.",
      "tags": [
        "Benchmark",
        "Code"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-lj-speech",
      "term": "LJ Speech",
      "definition": "A public domain speech dataset of 13100 short audio clips from a single speaker reading passages from 7 non-fiction books. Widely used for text-to-speech system development.",
      "tags": [
        "Training Corpus",
        "Speech"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama",
      "term": "Llama",
      "definition": "Meta's open-weight family of large language models. Released with relatively permissive licenses, enabling widespread research and commercial use of capable open models.",
      "tags": [
        "Model",
        "Meta"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-2",
      "term": "LLaMA 2",
      "definition": "The second generation of Meta's open-weight language models available in 7B 13B and 70B parameter sizes. Includes chat-optimized variants fine-tuned with RLHF. Released with a permissive license for research and commercial use.",
      "tags": [
        "Models",
        "Fundamentals"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-3",
      "term": "LLaMA 3",
      "definition": "Meta's third generation of open-weight language models with improved pretraining data and architecture refinements. Available in multiple sizes with significantly improved performance on reasoning coding and multilingual tasks.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-31",
      "term": "Llama 3.1",
      "definition": "An extended version of Meta's Llama 3 family offering 8B and 70B and 405B parameter models with a 128K context window and multilingual support.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-32",
      "term": "Llama 3.2",
      "definition": "A multimodal extension of Meta's Llama 3 family that adds vision capabilities alongside text in lightweight 1B and 3B and larger 11B and 90B variants.",
      "tags": [
        "Models",
        "Technical",
        "NLP",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-33",
      "term": "Llama 3.3",
      "definition": "A 70B parameter model from Meta that delivers Llama 3.1 405B-level performance for text tasks at significantly lower computational cost.",
      "tags": [
        "Models",
        "Technical",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-guard",
      "term": "Llama Guard",
      "definition": "A safety classifier model fine-tuned from LLaMA for evaluating the safety of language model inputs and outputs. Classifies content against configurable safety taxonomies enabling customizable content moderation.",
      "tags": [
        "Models",
        "Safety"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-guard-2",
      "term": "Llama Guard 2",
      "definition": "A second-generation safety classifier from Meta AI built on Llama 3 that evaluates whether language model inputs and outputs are safe or harmful.",
      "tags": [
        "Models",
        "Technical",
        "NLP",
        "Safety"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-adapter",
      "term": "Llama-Adapter",
      "definition": "An efficient fine-tuning method that prepends a set of learnable adaptation prompts to the upper layers of a frozen LLaMA model. Achieves strong instruction following with only 1.2 million additional parameters.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llama-cpp",
      "term": "llama.cpp",
      "definition": "An open-source C/C++ library for efficient CPU and GPU inference of large language models using quantized weights. llama.cpp enables running models locally on consumer hardware through aggressive optimization and support for various quantization formats.",
      "tags": [
        "Inference Infrastructure",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llamaindex",
      "term": "LlamaIndex",
      "definition": "A data framework for connecting LLMs to external data sources. Specializes in indexing, retrieval, and RAG applications with various data connectors and query engines.",
      "tags": [
        "Framework",
        "Application"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-llava",
      "term": "LLaVA",
      "definition": "Large Language and Vision Assistant is a multimodal model that connects a vision encoder to a language model using a simple projection layer. Achieves strong visual reasoning by fine-tuning on visual instruction-following data.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llava-visual-instruct",
      "term": "LLaVA Visual Instruct",
      "definition": "A multimodal instruction-following dataset combining visual inputs with text instructions. Used to train the LLaVA vision-language model for visual chat and reasoning.",
      "tags": [
        "Training Corpus",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-llava-med",
      "term": "LLaVA-Med",
      "definition": "A vision-language model adapted for biomedical image understanding by fine-tuning LLaVA on curated biomedical image-text instruction data.",
      "tags": [
        "Models",
        "Technical",
        "Medical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llava-next",
      "term": "Llava-NeXT",
      "definition": "An improved version of LLaVA with higher resolution image support and better visual reasoning capabilities through enhanced training strategies.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llava-next-video",
      "term": "Llava-NeXT-Video",
      "definition": "An extension of LLaVA-NeXT that adds video understanding by representing videos as sequences of frames processed through the vision-language architecture.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llava-onevision",
      "term": "Llava-OneVision",
      "definition": "A unified vision-language model that handles single-image and multi-image and video understanding tasks through a carefully designed training pipeline.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llemma",
      "term": "Llemma",
      "definition": "An open language model for mathematics built by continuing pretraining of Code Llama on a blend of mathematical documents and code. Achieves strong math performance while being fully open-source.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llm-as-judge",
      "term": "LLM-as-Judge",
      "definition": "An evaluation paradigm where a large language model is prompted to assess and score the quality of outputs from other models, providing scalable evaluation that approximates human judgment with explicit rating criteria and rubrics.",
      "tags": [
        "Evaluation",
        "LLM-Based"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-llmlingua",
      "term": "LLMLingua",
      "definition": "A prompt compression framework that uses a small language model to identify and remove less informative tokens from prompts, achieving significant compression ratios while maintaining downstream task performance through budget-aware iterative token pruning.",
      "tags": [
        "Prompt Engineering",
        "Compression"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-lmdrive",
      "term": "LMDrive",
      "definition": "A language-guided end-to-end autonomous driving framework that uses large language models to process navigation instructions and sensor data for vehicle control.",
      "tags": [
        "Models",
        "Technical",
        "Autonomous",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lmsys-elo-ratings",
      "term": "LMSys Elo Ratings",
      "definition": "A ranking system for language models based on human preference votes from the Chatbot Arena. Provides empirical performance rankings based on crowdsourced comparisons.",
      "tags": [
        "Evaluation",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-lmsys-chat-1m",
      "term": "LMSYS-Chat-1M",
      "definition": "A dataset of one million real-world conversations with 25 LLMs collected from the LMSYS Chatbot Arena platform. Provides diverse user interactions for studying LLM behavior.",
      "tags": [
        "Training Corpus",
        "NLP"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-load-balancing-loss",
      "term": "Load Balancing Loss",
      "definition": "An auxiliary loss term used in mixture-of-experts models to encourage uniform distribution of tokens across experts, preventing routing collapse where all inputs are sent to few experts.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-local-differential-privacy",
      "term": "Local Differential Privacy",
      "definition": "A privacy model where each user perturbs their own data before sending it to the data collector. Provides stronger privacy guarantees than central differential privacy because the server never sees raw data.",
      "tags": [
        "Algorithms",
        "Technical",
        "Privacy"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-local-llm",
      "term": "Local LLM",
      "definition": "Running language models on personal hardware rather than through cloud APIs. Enables privacy, offline use, and cost savings, though requires capable hardware.",
      "tags": [
        "Deployment",
        "Privacy"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-local-outlier-factor",
      "term": "Local Outlier Factor",
      "definition": "A density-based anomaly detection algorithm that compares the local density of a point to the densities of its neighbors. Points with substantially lower density than their neighbors are classified as outliers. Does not assume a specific data distribution.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-locality-preserving-projection",
      "term": "Locality-Preserving Projection",
      "definition": "A linear dimensionality reduction method that preserves the local structure of the data as defined by a neighborhood graph. Finds a linear transformation that maps nearby points to nearby points in the low-dimensional space.",
      "tags": [
        "Algorithms",
        "Technical",
        "Dimensionality Reduction"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-locality-sensitive-hashing",
      "term": "Locality-Sensitive Hashing",
      "definition": "An approximate nearest neighbor technique that hashes similar vectors into the same buckets with high probability using random projections, enabling sub-linear search time by only comparing vectors within matching hash buckets.",
      "tags": [
        "Vector Database",
        "Index Structure"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-locality-sensitive-hashing-algorithm",
      "term": "Locality-Sensitive Hashing Algorithm",
      "definition": "A family of hashing methods that map similar items to the same hash buckets with high probability. Enables approximate nearest-neighbor search in sub-linear time for high-dimensional data.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Data Structure",
        "Searching"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-localized-narratives",
      "term": "Localized Narratives",
      "definition": "A dataset of detailed spoken descriptions of images where narrators simultaneously move their mouse over the image regions they describe. Provides grounded dense image captions.",
      "tags": [
        "Benchmark",
        "Multimodal"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-locally-linear-embedding",
      "term": "Locally Linear Embedding",
      "definition": "A nonlinear dimensionality reduction method that preserves local neighborhood relationships. Each point is reconstructed as a linear combination of its neighbors and the same weights are used in the low-dimensional embedding.",
      "tags": [
        "Algorithms",
        "Technical",
        "Dimensionality Reduction"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-locking-problem",
      "term": "Locking Problem",
      "definition": "The challenge that early AI systems may become entrenched and difficult to modify or replace once widely deployed creating path dependency that limits future safety improvements.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-loebner-prize",
      "term": "Loebner Prize",
      "definition": "An annual competition in artificial intelligence that awards prizes to the computer programs considered by the judges to be the most human-like. A practical implementation of the Turing Test that has run since 1991.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-loess",
      "term": "LOESS",
      "definition": "Locally Estimated Scatterplot Smoothing, a non-parametric regression method that fits local weighted polynomial regressions to subsets of the data, producing a smooth curve that adapts to local patterns.",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lofti-zadeh-fuzzy-sets-paper",
      "term": "Lofti Zadeh Fuzzy Sets Paper",
      "definition": "The 1965 paper Fuzzy Sets by Lotfi Zadeh published in Information and Control that introduced fuzzy set theory. This work extended classical set theory to allow partial membership enabling mathematical treatment of vagueness and imprecision in reasoning.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-log-loss",
      "term": "Log Loss",
      "definition": "A loss function for binary classification that measures the negative log-likelihood of the true labels given the predicted probabilities. It heavily penalizes confident but incorrect predictions.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-log-likelihood",
      "term": "Log-Likelihood",
      "definition": "The natural logarithm of the likelihood function, used to simplify optimization because it converts products of probabilities into sums. Maximizing the log-likelihood is equivalent to maximizing the likelihood.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-log-normal-distribution",
      "term": "Log-Normal Distribution",
      "definition": "A continuous probability distribution of a random variable whose logarithm is normally distributed. It is used to model quantities that are products of many independent positive random variables.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-log-softmax",
      "term": "Log-Softmax",
      "definition": "The logarithm of the softmax function computed more numerically stably by using the log-sum-exp trick. Directly outputs log-probabilities. Used with negative log-likelihood loss for numerical precision in classification tasks.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-logic-programming",
      "term": "Logic Programming",
      "definition": "A programming paradigm based on formal logic where programs are expressed as a set of logical relations and computation proceeds by logical inference. Developed in the early 1970s by Robert Kowalski and Alain Colmerauer logic programming is exemplified by Prolog.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-logic-synthesis",
      "term": "Logic Synthesis",
      "definition": "Process of converting a high-level hardware description into a gate-level netlist optimized for area timing and power. A key step in designing AI accelerator chips.",
      "tags": [
        "Manufacturing",
        "Design",
        "Process"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-logic-theorist",
      "term": "Logic Theorist",
      "definition": "A program written by Allen Newell and Herbert Simon in 1956 that could prove mathematical theorems from Principia Mathematica, widely considered the first artificial intelligence program.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-logiqa",
      "term": "LogiQA",
      "definition": "A logical reasoning QA dataset from the Chinese National Civil Service Examination. Tests deductive reasoning categorical reasoning and logical analysis abilities.",
      "tags": [
        "Benchmark",
        "NLP",
        "Reasoning"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-logiqa-20",
      "term": "LogiQA 2.0",
      "definition": "An expanded bilingual version of LogiQA with improved question quality and English translations. Tests formal and informal logical reasoning in both Chinese and English.",
      "tags": [
        "Benchmark",
        "NLP",
        "Reasoning",
        "Multilingual"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-logistic-regression",
      "term": "Logistic Regression",
      "definition": "A linear classification model that predicts class probabilities using the logistic (sigmoid) function applied to a linear combination of input features. Despite its name, it is used for classification rather than regression.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-logistic-regression-history",
      "term": "Logistic Regression History",
      "definition": "The application of logistic regression to classification problems in machine learning. Originally developed for statistics by David Cox in 1958 logistic regression became a fundamental baseline classifier in machine learning and remains widely used for binary classification.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-logistic-regression-model",
      "term": "Logistic Regression Model",
      "definition": "A classification model that uses the logistic sigmoid function to estimate the probability of a binary outcome from input features.",
      "tags": [
        "Models",
        "Fundamentals",
        "History"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-logit",
      "term": "Logit",
      "definition": "The raw, unnormalized scores output by a model before converting to probabilities. In LLMs, logits represent the model's preference for each possible next token.",
      "tags": [
        "Technical",
        "Math"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-logit-bias",
      "term": "Logit Bias",
      "definition": "A technique that adds fixed values to the logits of specific tokens before sampling, used to encourage or suppress particular words or phrases in generated output.",
      "tags": [
        "Generative AI",
        "Decoding"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-logit-lens",
      "term": "Logit Lens",
      "definition": "An interpretability technique that decodes intermediate transformer layer outputs through the final unembedding matrix to observe how predictions evolve through the network. Reveals how the model progressively refines its outputs across layers.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-long-context-fine-tuning",
      "term": "Long Context Fine-Tuning",
      "definition": "The process of adapting a pre-trained model to effectively utilize longer context windows than it was originally trained on, through continued training with progressively longer sequences and position interpolation.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lstm-history",
      "term": "Long Short-Term Memory",
      "definition": "A recurrent neural network architecture invented by Sepp Hochreiter and Jurgen Schmidhuber in 1997 that solved the vanishing gradient problem, enabling effective learning from long sequences and powering advances in speech and language processing.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-long-term-ai-safety",
      "term": "Long-term AI Safety",
      "definition": "Research focused on ensuring the safety of AI systems that may eventually match or exceed human-level capabilities. Addresses existential risks value alignment and the control problem for advanced AI.",
      "tags": [
        "Safety",
        "Fundamentals"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-longbench",
      "term": "Longbench",
      "definition": "A comprehensive benchmark for evaluating long-context capabilities of language models across 6 task categories. Tests performance on inputs up to 100K tokens.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-longest-common-subsequence",
      "term": "Longest Common Subsequence",
      "definition": "A dynamic programming algorithm that finds the longest subsequence common to two sequences. Runs in O(mn) time and is used in diff utilities and bioinformatics for sequence comparison.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "NLP"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-longest-common-substring",
      "term": "Longest Common Substring",
      "definition": "An algorithm that finds the longest string that is a contiguous substring of two or more input strings. Solvable in O(mn) time using dynamic programming or in O(n) time using suffix trees.",
      "tags": [
        "Algorithms",
        "Technical",
        "NLP"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-longformer",
      "term": "Longformer",
      "definition": "A transformer variant that combines local sliding window attention with task-specific global attention on selected tokens, enabling efficient processing of documents with thousands of tokens.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lookahead-optimizer",
      "term": "Lookahead Optimizer",
      "definition": "A meta-optimizer that wraps around any base optimizer maintaining two sets of weights. The fast weights explore the loss landscape while slow weights are periodically updated toward the fast weights. Reduces variance and improves stability.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-loopy-belief-propagation",
      "term": "Loopy Belief Propagation",
      "definition": "An approximate inference algorithm that applies belief propagation to graphs with cycles despite the lack of convergence guarantees. Often produces good approximations in practice and is widely used in computer vision and coding theory.",
      "tags": [
        "Algorithms",
        "Technical",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lora",
      "term": "LoRA (Low-Rank Adaptation)",
      "definition": "A parameter-efficient fine-tuning technique that trains only small additional matrices rather than the full model. Dramatically reduces memory and compute requirements for customization.",
      "tags": [
        "Training",
        "Efficiency"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-lora-diffusion",
      "term": "LoRA for Diffusion",
      "definition": "The application of Low-Rank Adaptation to diffusion models, enabling efficient fine-tuning of image generation models to learn new concepts, styles, or subjects with minimal additional parameters.",
      "tags": [
        "Generative AI",
        "Image Processing"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lora-fusion",
      "term": "LoRA Fusion",
      "definition": "The technique of combining multiple LoRA adapters trained for different tasks or styles into a single model by merging or dynamically weighting the adapter parameters during inference.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lora-land",
      "term": "LoRA-Land",
      "definition": "A benchmark and analysis of Low-Rank Adaptation fine-tuned models across 25 tasks. Studies the effectiveness and characteristics of LoRA fine-tuning compared to full fine-tuning.",
      "tags": [
        "Benchmark",
        "NLP",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-loss-function",
      "term": "Loss Function",
      "definition": "A mathematical function measuring how wrong a model's predictions are. Training aims to minimize this loss, with common examples including cross-entropy and mean squared error.",
      "tags": [
        "Training",
        "Math"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-loss-scaling",
      "term": "Loss Scaling",
      "definition": "A technique used in FP16 mixed precision training that multiplies the loss by a large factor before backpropagation to prevent small gradient values from underflowing to zero. The scaled gradients are divided back before the optimizer step.",
      "tags": [
        "Model Optimization",
        "Hardware"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lost-in-the-middle",
      "term": "Lost in the Middle",
      "definition": "A documented phenomenon where language models perform worse at retrieving and using information placed in the middle of their context window compared to information at the beginning or end.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lotfi-zadeh",
      "term": "Lotfi Zadeh",
      "definition": "Azerbaijani-American mathematician and computer scientist (1921-2017) who invented fuzzy logic and fuzzy set theory in 1965, providing mathematical tools for handling uncertainty and imprecision in AI and control systems.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-lottery-ticket-hypothesis",
      "term": "Lottery Ticket Hypothesis",
      "definition": "The theory that dense randomly-initialized networks contain sparse subnetworks (winning tickets) that can be trained in isolation to match the full network's performance when initialized with their original weights.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-louvain-algorithm",
      "term": "Louvain Algorithm",
      "definition": "A community detection algorithm for large networks that optimizes modularity through a greedy hierarchical approach. Iteratively merges nodes into communities and builds a new network of communities until modularity no longer increases.",
      "tags": [
        "Algorithms",
        "Technical",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-low-rank-approximation",
      "term": "Low-Rank Approximation",
      "definition": "A technique that approximates a matrix with a lower-rank matrix to reduce dimensionality computational cost or noise. The optimal rank-k approximation is given by truncated SVD. Fundamental to LoRA and other parameter-efficient methods.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-low-rank-factorization",
      "term": "Low-Rank Factorization",
      "definition": "A model compression technique that approximates large weight matrices as products of smaller matrices with reduced rank. Low-rank factorization reduces both parameters and computation, and is used in methods like LoRA for efficient fine-tuning.",
      "tags": [
        "Model Optimization",
        "Inference Infrastructure"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lowest-common-ancestor-algorithm",
      "term": "Lowest Common Ancestor Algorithm",
      "definition": "An algorithm that finds the deepest node that is an ancestor of two given nodes in a tree. Can be answered in O(1) time per query after O(n) preprocessing using sparse tables and Euler tours.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Graph"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lpddr5",
      "term": "LPDDR5",
      "definition": "Low Power Double Data Rate 5 memory used in mobile devices and edge AI processors. Balances bandwidth and power efficiency for on-device machine learning inference.",
      "tags": [
        "Memory",
        "Mobile",
        "Edge"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-lsf-load-sharing-facility",
      "term": "LSF (Load Sharing Facility)",
      "definition": "IBM workload management platform for scheduling and managing jobs across computing clusters. Used in some enterprise AI environments for managing GPU training workloads.",
      "tags": [
        "Infrastructure",
        "IBM",
        "Scheduling"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-lsm-tree-algorithm",
      "term": "LSM Tree Algorithm",
      "definition": "Log-Structured Merge Tree is a data structure optimized for write-heavy workloads that buffers writes in memory and periodically merges them into sorted on-disk structures. Used in many modern key-value stores.",
      "tags": [
        "Algorithms",
        "Technical",
        "Data Structure"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lstm",
      "term": "LSTM (Long Short-Term Memory)",
      "definition": "A recurrent neural network architecture designed to capture long-range dependencies in sequences. Was the dominant NLP architecture before transformers emerged.",
      "tags": [
        "Architecture",
        "Historical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lsun",
      "term": "LSUN",
      "definition": "The Large-scale Scene Understanding dataset containing millions of labeled images across 10 scene categories and 20 object categories. Used for benchmarking image generation and scene classification models.",
      "tags": [
        "Benchmark",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-lu-decomposition",
      "term": "LU Decomposition",
      "definition": "A matrix factorization method that decomposes a matrix into the product of a lower triangular matrix and an upper triangular matrix. Used for solving linear systems and computing determinants and matrix inverses efficiently.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Numerical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lucas-kanade-optical-flow",
      "term": "Lucas-Kanade Optical Flow",
      "definition": "A differential method for estimating optical flow that assumes constant flow within a local neighborhood of each pixel. Solves an over-determined system of equations using least squares and works best for small motions.",
      "tags": [
        "Algorithms",
        "Fundamentals",
        "Vision"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-lumi-supercomputer",
      "term": "LUMI Supercomputer",
      "definition": "European pre-exascale supercomputer in Finland using AMD EPYC CPUs and AMD Instinct MI250X GPUs. One of the most powerful systems available for European AI research.",
      "tags": [
        "Supercomputer",
        "AMD",
        "Europe"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-lumiere",
      "term": "Lumiere",
      "definition": "A text-to-video diffusion model from Google Research that generates temporally coherent videos using a Space-Time U-Net architecture for global temporal consistency.",
      "tags": [
        "Models",
        "Technical",
        "Vision"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lustre-file-system",
      "term": "Lustre File System",
      "definition": "Open-source parallel distributed file system designed for large-scale cluster computing. Used by many of the world largest supercomputers for high-bandwidth access to training data.",
      "tags": [
        "Storage",
        "Open Source",
        "Distributed"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-lvis",
      "term": "LVIS",
      "definition": "Large Vocabulary Instance Segmentation dataset containing over 2 million instance annotations for 1203 object categories in COCO images. Addresses long-tail recognition with rare category evaluation.",
      "tags": [
        "Benchmark",
        "Computer Vision"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-lvlm-ehub",
      "term": "LVLM-eHub",
      "definition": "A comprehensive evaluation hub for large vision-language models testing across multiple multimodal capabilities. Provides standardized comparison of LVLM performance.",
      "tags": [
        "Benchmark",
        "Multimodal",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-lwm",
      "term": "LWM",
      "definition": "Large World Model is a general-purpose model trained on long video and text sequences that can understand and generate content across both modalities.",
      "tags": [
        "Models",
        "Technical",
        "Vision",
        "NLP"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-lyra",
      "term": "Lyra",
      "definition": "A low-bitrate neural speech codec from Google that delivers high-quality speech at 3 kbps using a generative model for real-time communication.",
      "tags": [
        "Models",
        "Technical",
        "Audio"
      ],
      "domain": "models",
      "link": null,
      "related": []
    }
  ]
}