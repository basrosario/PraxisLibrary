{
  "letter": "e",
  "count": 125,
  "terms": [
    {
      "id": "term-e5",
      "term": "E5",
      "definition": "A family of text embedding models trained with contrastive learning on large-scale text pairs. Achieves strong performance across retrieval clustering and classification tasks. Available in multiple sizes from small to extra-large.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-early-stopping",
      "term": "Early Stopping",
      "definition": "A regularization technique that stops training when performance on a validation set stops improving. Prevents overfitting and saves computational resources.",
      "tags": [
        "Training",
        "Regularization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-early-stopping-criterion",
      "term": "Early Stopping Criterion",
      "definition": "A regularization technique that monitors validation performance during training and stops when it begins to deteriorate. Prevents overfitting by selecting the model from the epoch with best validation performance. Uses a patience parameter.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-earth-movers-distance",
      "term": "Earth Mover's Distance",
      "definition": "A metric for comparing probability distributions based on the minimum amount of work needed to transform one distribution into the other, where work is the amount of mass moved multiplied by the distance it is moved.",
      "tags": [
        "Statistics",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-eccv",
      "term": "ECCV",
      "definition": "The European Conference on Computer Vision held biennially since 1990. One of the top three computer vision conferences alongside CVPR and ICCV. Known for publishing influential papers on object recognition segmentation and visual understanding.",
      "tags": [
        "History",
        "Conferences"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-echo-state-network",
      "term": "Echo State Network",
      "definition": "A recurrent neural network where the recurrent layer is a large randomly generated reservoir with fixed weights. Only the output weights are trained. Part of the reservoir computing paradigm. Efficient for temporal pattern recognition.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-edge-ai",
      "term": "Edge AI",
      "definition": "Running AI models locally on devices (phones, IoT) rather than in the cloud. Enables faster responses, offline operation, and better privacy but requires efficient models.",
      "tags": [
        "Deployment",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-edge-case-safety",
      "term": "Edge Case Safety",
      "definition": "The challenge of ensuring AI systems behave safely in unusual or extreme situations that were not well represented in training data. Critical for safety-critical applications like autonomous driving.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-edge-inference",
      "term": "Edge Inference",
      "definition": "Running AI model inference directly on edge devices (phones, IoT sensors, embedded systems) rather than in the cloud, reducing latency and bandwidth requirements. Edge inference requires highly optimized, compressed models tailored to limited compute and memory.",
      "tags": [
        "Inference Infrastructure",
        "Hardware"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-edit-distance",
      "term": "Edit Distance",
      "definition": "A family of metrics quantifying the minimum number of operations required to transform one string into another, with variants including Levenshtein, Damerau-Levenshtein, and Hamming distance.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-edmund-clarke",
      "term": "Edmund Clarke",
      "definition": "American computer scientist who pioneered model checking a technique for automatically verifying the correctness of finite-state systems. Received the 2007 Turing Award for this work which has applications in verifying AI system safety and correctness.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-edvac",
      "term": "EDVAC",
      "definition": "The Electronic Discrete Variable Automatic Computer designed in the 1940s was one of the first stored-program computers. John von Neumann's 1945 First Draft of a Report on the EDVAC described the architecture that became the standard for digital computers.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-edward-feigenbaum",
      "term": "Edward Feigenbaum",
      "definition": "American computer scientist known as the father of expert systems, who led the development of DENDRAL and pioneered knowledge engineering at Stanford, demonstrating the commercial viability of AI in the 1970s-1980s.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-edward-shortliffe",
      "term": "Edward Shortliffe",
      "definition": "American biomedical informatician who developed MYCIN one of the most influential early expert systems for medical diagnosis. Shortliffe's work on MYCIN demonstrated the viability of AI in healthcare and introduced certainty factors for handling uncertainty in expert systems.",
      "tags": [
        "History",
        "Pioneers"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-effect-size",
      "term": "Effect Size",
      "definition": "A quantitative measure of the magnitude of a phenomenon or the practical significance of a result, independent of sample size. Common measures include Cohen's d, odds ratio, and correlation coefficient.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-efficiency",
      "term": "Efficiency",
      "definition": "A property of a statistical estimator related to how much information from the data it uses. An efficient estimator achieves the lowest possible variance among all unbiased estimators, reaching the Cramer-Rao bound.",
      "tags": [
        "Statistics",
        "Inference"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-efficientnet",
      "term": "EfficientNet",
      "definition": "A family of CNN models that use compound scaling to uniformly scale network width, depth, and resolution using a fixed set of scaling coefficients, achieving state-of-the-art accuracy with fewer parameters.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-efficientnetv2",
      "term": "EfficientNetV2",
      "definition": "An improved version of EfficientNet that uses progressive training and fused MBConv blocks for faster training. Achieves better accuracy and training speed than the original EfficientNet family.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-eigendecomposition",
      "term": "Eigendecomposition",
      "definition": "The factorization of a square matrix into eigenvalues and eigenvectors revealing the directions along which the transformation acts by simple scaling. Fundamental to PCA spectral clustering and many numerical methods in machine learning.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-einsum",
      "term": "Einsum",
      "definition": "Einstein Summation notation is a compact way to express multi-dimensional array operations including matrix multiplication outer products contractions and transpositions. Widely used in deep learning frameworks for expressing tensor operations concisely.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-elastic-net",
      "term": "Elastic Net",
      "definition": "A regularization method that linearly combines L1 and L2 penalty terms, balancing feature selection (sparsity) with weight shrinkage. The mixing ratio controls the relative contribution of each penalty.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-elastic-training",
      "term": "Elastic Training",
      "definition": "A distributed training approach that can dynamically scale the number of workers up or down during a training run without requiring a restart. Elastic training handles node failures and resource availability changes gracefully.",
      "tags": [
        "Distributed Computing",
        "Model Optimization"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-elastic-weight-consolidation",
      "term": "Elastic Weight Consolidation",
      "definition": "A continual learning technique that slows down learning on weights important for previous tasks. Uses the diagonal of the Fisher information matrix to estimate parameter importance. Allows sequential task learning while mitigating catastrophic forgetting.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-elbow-method",
      "term": "Elbow Method",
      "definition": "A heuristic for selecting the optimal number of clusters by plotting the within-cluster sum of squares against the number of clusters and identifying the point where additional clusters yield diminishing returns, forming an elbow shape.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-electra",
      "term": "ELECTRA",
      "definition": "A pretraining method that trains a discriminator to detect tokens replaced by a small generator network, providing more efficient training than masked language modeling by learning from all input tokens.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-electromagnetic-vulnerability-of-ai",
      "term": "Electromagnetic Vulnerability of AI",
      "definition": "The susceptibility of AI hardware and systems to electromagnetic interference or attack. Relevant for safety-critical deployments where electromagnetic disruption could cause dangerous failures.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-eleutherai",
      "term": "EleutherAI",
      "definition": "A grassroots collective of researchers founded in 2020 dedicated to open-source AI research. EleutherAI developed GPT-Neo GPT-J and GPT-NeoX demonstrating that large language models could be created and shared openly outside major corporate labs.",
      "tags": [
        "History",
        "Organizations"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-eliciting-latent-knowledge",
      "term": "Eliciting Latent Knowledge",
      "definition": "A research problem in AI alignment focused on extracting truthful information from a model that may have learned to represent the world accurately internally but could report misleadingly.",
      "tags": [
        "AI Safety",
        "Alignment"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-eligibility-trace",
      "term": "Eligibility Trace",
      "definition": "A decaying memory of recently visited states used in TD(lambda) and other RL algorithms to distribute credit backward in time. Eligibility traces enable efficient online computation of lambda-weighted multi-step updates.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-eliza",
      "term": "ELIZA",
      "definition": "A natural language processing program created by Joseph Weizenbaum at MIT in 1966 that simulated a Rogerian psychotherapist, demonstrating the illusion of understanding and becoming one of the first chatbots.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-eliza-effect",
      "term": "Eliza Effect",
      "definition": "The tendency of humans to unconsciously assume that computer behaviors are analogous to human behaviors. Named after the ELIZA chatbot the effect describes how people readily anthropomorphize even simple AI systems attributing understanding and emotions to them.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-ellipsis-resolution",
      "term": "Ellipsis Resolution",
      "definition": "The task of identifying and recovering omitted words or phrases in text that are understood from context, such as resolving 'John likes coffee and Mary tea' to include the implied verb.",
      "tags": [
        "NLP",
        "Linguistics"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-elmo",
      "term": "ELMo",
      "definition": "Embeddings from Language Models, a contextualized word representation method that generates word vectors as a function of the entire input sentence using a bidirectional LSTM language model.",
      "tags": [
        "NLP",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-elo-rating-for-models",
      "term": "ELO Rating for Models",
      "definition": "An adaptation of the chess ELO rating system to rank language models through pairwise comparisons, where models gain or lose rating points based on head-to-head evaluation outcomes judged by humans or automated evaluators.",
      "tags": [
        "Evaluation",
        "Ranking"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-elu",
      "term": "ELU",
      "definition": "Exponential Linear Unit activation function that uses an exponential curve for negative inputs. Defined as f(x) = x if x > 0 and f(x) = alpha * (exp(x) - 1) otherwise. Produces negative outputs which helps push mean activations closer to zero and improves learning dynamics.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-embedding",
      "term": "Embedding",
      "definition": "A dense vector representation of data (words, sentences, images) in a continuous space. Similar items have similar embeddings, enabling semantic search and comparison.",
      "tags": [
        "Representation",
        "NLP"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-embedding-caching",
      "term": "Embedding Caching",
      "definition": "The practice of storing previously computed embedding vectors for reuse, avoiding redundant embedding model inference for repeated or similar content and reducing latency and computational cost in production retrieval pipelines.",
      "tags": [
        "Vector Database",
        "Performance"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-embedding-dimension",
      "term": "Embedding Dimension",
      "definition": "The number of components in a vector embedding, determining the representational capacity and memory footprint of the embedding space, with typical values ranging from 384 to 4096 dimensions depending on the embedding model.",
      "tags": [
        "Vector Database",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-embedding-drift",
      "term": "Embedding Drift",
      "definition": "The phenomenon where the distribution of vector embeddings changes over time as source data evolves or embedding models are updated, potentially degrading retrieval quality and requiring index refresh or reindexing to maintain accuracy.",
      "tags": [
        "Vector Database",
        "Maintenance"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-embedding-fine-tuning",
      "term": "Embedding Fine-Tuning",
      "definition": "The process of further training a pre-trained embedding model on domain-specific data using contrastive learning or other objectives to produce embeddings better suited for a particular use case, improving retrieval relevance in specialized domains.",
      "tags": [
        "Vector Database",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-embedding-model",
      "term": "Embedding Model",
      "definition": "A model specifically designed to convert text, images, or other data into vector representations. Popular embedding models include OpenAI's text-embedding-ada-002 and open-source alternatives like E5.",
      "tags": [
        "Model Type",
        "Representation"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-embedding-quantization",
      "term": "Embedding Quantization",
      "definition": "The compression of high-dimensional embedding vectors from 32-bit floats to lower precision formats (binary, int8) to reduce storage costs and accelerate similarity search with minimal retrieval quality loss.",
      "tags": [
        "LLM",
        "Inference"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-embedding-similarity-search",
      "term": "Embedding Similarity Search",
      "definition": "The process of finding the most semantically similar items to a query by computing distances between their vector embeddings in a shared embedding space, forming the foundation of modern neural information retrieval systems.",
      "tags": [
        "Vector Database",
        "Search"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-embedding-space",
      "term": "Embedding Space",
      "definition": "The continuous high-dimensional vector space in which embeddings reside, where geometric relationships between vectors encode semantic relationships, with similar concepts located nearby and dissimilar ones far apart according to the chosen distance metric.",
      "tags": [
        "Vector Database",
        "Embeddings"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-embodied-ai",
      "term": "Embodied AI",
      "definition": "An approach to AI that emphasizes the role of physical embodiment and sensorimotor interaction with the environment in the development of intelligence. Proponents argue that intelligence cannot be fully understood or replicated without a body situated in the physical world.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-emergent-abilities",
      "term": "Emergent Abilities",
      "definition": "Capabilities that appear in large AI models that weren't present in smaller versions. Examples include complex reasoning, code generation, and following nuanced instructions.",
      "tags": [
        "Phenomenon",
        "Scaling"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-emergent-behavior-risk",
      "term": "Emergent Behavior Risk",
      "definition": "The risk that AI systems exhibit unexpected capabilities or behaviors that were not intended or predicted by their developers. Particularly concerning for large models where emergent properties are difficult to anticipate.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-emergent-capability",
      "term": "Emergent Capability",
      "definition": "An ability that appears in large language models only at sufficient scale and is absent in smaller models, such as multi-step reasoning or code generation, though the sharpness of this emergence is debated.",
      "tags": [
        "LLM",
        "Generative AI"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-emnlp",
      "term": "EMNLP",
      "definition": "The Conference on Empirical Methods in Natural Language Processing first held in 1996. A top-tier NLP conference known for emphasis on empirical and data-driven approaches to language processing alongside ACL and NAACL.",
      "tags": [
        "History",
        "Conferences"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-emotion-detection",
      "term": "Emotion Detection",
      "definition": "The task of identifying specific emotions such as joy, anger, sadness, fear, or surprise expressed in text, providing finer-grained analysis than binary sentiment classification.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-emotion-prompting",
      "term": "Emotion Prompting",
      "definition": "A technique that appends emotionally charged phrases to prompts such as urgency cues or importance markers, leveraging the observation that language models can respond to emotional stimuli with improved task performance.",
      "tags": [
        "Prompt Engineering",
        "Behavioral"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-emotion-recognition",
      "term": "Emotion Recognition",
      "definition": "The classification of facial expressions or body language into emotional categories using computer vision models trained on annotated datasets of human affect displays.",
      "tags": [
        "Computer Vision",
        "Image Processing"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-emotion-recognition-ai-ethics",
      "term": "Emotion Recognition AI Ethics",
      "definition": "Ethical concerns about AI systems that claim to detect human emotions from facial expressions, voice, or physiological signals, including scientific validity doubts, cultural bias, and privacy implications.",
      "tags": [
        "AI Ethics",
        "Fairness"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-emotional-ai-ethics",
      "term": "Emotional AI Ethics",
      "definition": "Ethical concerns related to AI systems that detect generate or respond to human emotions. Issues include accuracy across demographics privacy of emotional data and potential for manipulation.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-empirical-bayes",
      "term": "Empirical Bayes",
      "definition": "An approach that estimates prior distribution parameters from the data itself, rather than specifying them a priori. It blends Bayesian and frequentist ideas by using the data to inform both the prior and posterior.",
      "tags": [
        "Statistics",
        "Bayesian Methods"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-empirical-distribution-function",
      "term": "Empirical Distribution Function",
      "definition": "A cumulative distribution function constructed from sample data that assigns probability 1/n to each observed value. It converges uniformly to the true CDF as sample size increases (Glivenko-Cantelli theorem).",
      "tags": [
        "Statistics",
        "Data Science"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-empirical-risk-minimization",
      "term": "Empirical Risk Minimization",
      "definition": "A learning principle that selects the hypothesis minimizing the average loss on the training data. While simple and intuitive, it can lead to overfitting without regularization or capacity constraints.",
      "tags": [
        "Machine Learning",
        "Model Selection"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-empowerment",
      "term": "Empowerment",
      "definition": "An information-theoretic intrinsic motivation measure defined as the channel capacity between an agent's actions and its future states. Empowerment rewards the agent for maintaining maximum influence over its environment, driving exploration toward controllable regions.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-encodec",
      "term": "Encodec",
      "definition": "A neural audio codec by Meta AI that compresses audio at very low bitrates using residual vector quantization. Enables efficient storage and transmission of audio while maintaining high quality. Used in audio language models.",
      "tags": [
        "Models",
        "Technical"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-encoder",
      "term": "Encoder",
      "definition": "A neural network component that transforms input into a compressed representation. In transformers, encoder models (like BERT) process the entire input at once for understanding tasks.",
      "tags": [
        "Architecture",
        "Transformers"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-encoder-decoder-architecture",
      "term": "Encoder-Decoder Architecture",
      "definition": "A neural network design where an encoder processes input into a latent representation and a decoder generates output from that representation, commonly used for translation and summarization.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-encoder-only-architecture",
      "term": "Encoder-Only Architecture",
      "definition": "A transformer design using only bidirectional self-attention encoder blocks, producing contextualized representations of input tokens suited for classification and understanding tasks.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-endpoint",
      "term": "Endpoint",
      "definition": "A specific URL where an API can be accessed. AI services expose endpoints for different functions like chat completions, embeddings, and image generation.",
      "tags": [
        "API",
        "Technical"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-eniac",
      "term": "ENIAC",
      "definition": "The Electronic Numerical Integrator and Computer, completed in 1945 at the University of Pennsylvania, one of the earliest general-purpose electronic digital computers that demonstrated the feasibility of large-scale electronic computation.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-ensemble-learning",
      "term": "Ensemble Learning",
      "definition": "A machine learning paradigm that combines predictions from multiple models to produce a more robust and accurate prediction. Methods include bagging boosting and stacking. Generally outperforms individual models by reducing variance or bias.",
      "tags": [
        "Algorithms",
        "Fundamentals"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-ensemble",
      "term": "Ensemble Methods",
      "definition": "Techniques that combine multiple models to produce better results than any single model. Includes voting, bagging (Random Forest), and boosting (XGBoost). Often used in production for reliability.",
      "tags": [
        "Technique",
        "ML"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-ensemble-methods-history",
      "term": "Ensemble Methods History",
      "definition": "The development of ensemble methods from early voting approaches through bagging (Breiman 1996) boosting (Freund and Schapire 1997) stacking (Wolpert 1992) and random forests (Breiman 2001). Ensembles consistently outperform individual models across diverse tasks.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-enterprise-ai",
      "term": "Enterprise AI",
      "definition": "AI solutions designed for business environments with features like access controls, compliance, data privacy, and integration with existing systems. Different from consumer AI in security and governance requirements.",
      "tags": [
        "Business",
        "Application"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-entity-disambiguation",
      "term": "Entity Disambiguation",
      "definition": "The process of determining which specific real-world entity a textual mention refers to when the same name could refer to multiple entities, using context clues and knowledge bases.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-entropy",
      "term": "Entropy",
      "definition": "A measure from information theory quantifying the uncertainty or disorder in a random variable's distribution. In machine learning, it is used as a splitting criterion and as a component of loss functions like cross-entropy.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-entropy-regularization",
      "term": "Entropy Regularization",
      "definition": "A technique that adds the policy entropy to the RL objective function, discouraging the agent from committing to a single action too quickly. Entropy regularization promotes robust exploration and smoother optimization landscapes.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-environment",
      "term": "Environment",
      "definition": "The external system an RL agent interacts with, providing observations and rewards in response to actions. The environment defines the dynamics and rules governing state transitions.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-environmental-justice-in-ai",
      "term": "Environmental Justice in AI",
      "definition": "The principle that the environmental costs of AI development and deployment including energy use and e-waste should not disproportionately burden marginalized communities.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-epipolar-geometry",
      "term": "Epipolar Geometry",
      "definition": "The geometric relationship between two camera views of the same scene, defined by the fundamental or essential matrix, constraining where a point in one image can appear in the other.",
      "tags": [
        "Computer Vision",
        "3D Vision"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-episode",
      "term": "Episode",
      "definition": "A complete sequence of interaction from an initial state to a terminal state in episodic RL tasks. Episodes provide natural boundaries for computing returns and resetting the environment.",
      "tags": [
        "Reinforcement Learning",
        "Core Concepts"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-epistemic-autonomy",
      "term": "Epistemic Autonomy",
      "definition": "The right and capacity of individuals to form their own beliefs and make their own judgments rather than having these determined by AI recommendations and filter bubbles.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-epoch",
      "term": "Epoch",
      "definition": "One complete pass through the entire training dataset. Models typically train for multiple epochs, with each pass allowing weights to be refined based on all available data.",
      "tags": [
        "Training",
        "Technical"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-epsilon-greedy",
      "term": "Epsilon-Greedy Exploration",
      "definition": "An exploration strategy where the agent selects the greedy (best-known) action with probability 1-epsilon and a random action with probability epsilon. The epsilon parameter is typically annealed over training to shift from exploration to exploitation.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-equalized-odds",
      "term": "Equalized Odds",
      "definition": "A fairness criterion requiring that a classifier has equal true positive rates and equal false positive rates across all protected groups, ensuring that prediction accuracy does not vary by group membership.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-error-analysis",
      "term": "Error Analysis",
      "definition": "Systematic examination of model mistakes to understand failure patterns and guide improvements. Essential for iterating on model performance and identifying data or training issues.",
      "tags": [
        "Evaluation",
        "Process"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-escalation-protocol",
      "term": "Escalation Protocol",
      "definition": "A defined procedure for elevating AI safety concerns from technical teams to management and potentially to external regulators when certain risk thresholds are exceeded.",
      "tags": [
        "Safety",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-roce",
      "term": "Ethernet for AI (RoCE)",
      "definition": "RDMA over Converged Ethernet, a networking protocol that enables remote direct memory access over Ethernet infrastructure. RoCE provides a lower-cost alternative to InfiniBand for AI cluster networking with comparable performance using lossless Ethernet configurations.",
      "tags": [
        "Distributed Computing",
        "Hardware"
      ],
      "domain": "hardware",
      "link": null,
      "related": []
    },
    {
      "id": "term-ethical-ai-by-design",
      "term": "Ethical AI by Design",
      "definition": "An approach that integrates ethical considerations throughout the AI development lifecycle from initial requirements through design implementation testing and deployment.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-ethical-prompting",
      "term": "Ethical Prompting",
      "definition": "The practice of designing prompts that explicitly incorporate ethical guidelines, fairness constraints, and harm-avoidance instructions to steer model outputs toward responsible, unbiased, and socially beneficial responses.",
      "tags": [
        "Prompt Engineering",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-ethics-board",
      "term": "Ethics Board (AI)",
      "definition": "A group that reviews AI development and deployment for ethical concerns. Many major AI companies have ethics boards to evaluate potential harms and establish guidelines.",
      "tags": [
        "Governance",
        "Ethics"
      ],
      "domain": "safety",
      "link": "ai-safety.html",
      "related": []
    },
    {
      "id": "term-ethics-of-care-in-ai",
      "term": "Ethics of Care in AI",
      "definition": "An ethical framework that emphasizes relationships responsibility and contextual judgment in AI development. Contrasts with principle-based approaches by focusing on the needs of specific affected individuals and communities.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-ethics-washing",
      "term": "Ethics Washing",
      "definition": "The practice of organizations using ethics boards, principles, or frameworks as public relations tools without implementing meaningful changes to their AI development practices or governance structures.",
      "tags": [
        "AI Ethics",
        "Governance"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-eu-ai-act",
      "term": "EU AI Act",
      "definition": "The European Union's comprehensive regulatory framework for artificial intelligence, adopted in 2024, which classifies AI systems by risk level and imposes requirements ranging from transparency obligations to outright bans on certain uses.",
      "tags": [
        "Governance",
        "Regulation"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-euclidean-distance",
      "term": "Euclidean Distance",
      "definition": "The straight-line distance between two points in Euclidean space, computed as the square root of the sum of squared differences across all dimensions. It is the most common distance metric in machine learning.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-eurisko",
      "term": "Eurisko",
      "definition": "An AI program developed by Douglas Lenat in 1981 that could discover new heuristics and concepts. Built as a successor to AM Eurisko famously won the Traveller Trillion Credit Squadron naval wargame championship two consecutive years using discovered fleet designs.",
      "tags": [
        "History",
        "Systems"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-evaluation",
      "term": "Evaluation",
      "definition": "The process of measuring model performance using metrics, benchmarks, and human assessment. Critical for comparing models and ensuring they meet quality standards.",
      "tags": [
        "Process",
        "Quality"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-evaluation-bias",
      "term": "Evaluation Bias",
      "definition": "Bias introduced during model evaluation when benchmark datasets or metrics do not adequately represent the diversity of the deployment population, leading to overly optimistic performance estimates for some groups.",
      "tags": [
        "Fairness",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-eval-harness",
      "term": "Evaluation Harness",
      "definition": "A framework for systematically testing AI models across multiple benchmarks and tasks. Popular harnesses include lm-evaluation-harness used for open-source model comparisons.",
      "tags": [
        "Tools",
        "Evaluation"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-evasion-attack",
      "term": "Evasion Attack",
      "definition": "An adversarial attack that modifies inputs at test time to cause misclassification by a deployed model. Unlike poisoning attacks evasion attacks do not require access to the training process.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-event-extraction",
      "term": "Event Extraction",
      "definition": "The task of identifying event triggers and their arguments in text, determining what happened, who was involved, when, where, and other event-specific attributes.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-evidence-lower-bound",
      "term": "Evidence Lower Bound",
      "definition": "A lower bound on the log marginal likelihood (model evidence) that serves as the objective function in variational inference. Maximizing the ELBO is equivalent to minimizing the KL divergence between the variational and true posterior.",
      "tags": [
        "Machine Learning",
        "Bayesian Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-evolution-strategies-rl",
      "term": "Evolution Strategies for RL",
      "definition": "Black-box optimization methods that estimate policy gradients by perturbing parameters and evaluating returns, without requiring backpropagation through the environment. ES are highly parallelizable and can scale to thousands of workers.",
      "tags": [
        "Reinforcement Learning",
        "Policy Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-evolutionary-computation",
      "term": "Evolutionary Computation",
      "definition": "A family of optimization algorithms inspired by biological evolution including genetic algorithms genetic programming evolutionary strategies and differential evolution. Pioneered by researchers including John Holland Lawrence Fogel and Ingo Rechenberg from the 1960s onward.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-exact-match",
      "term": "Exact Match",
      "definition": "A strict evaluation metric that scores a prediction as correct only if it exactly matches the ground truth answer after normalization, commonly used in question answering and information extraction benchmarks.",
      "tags": [
        "Evaluation",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-exact-nearest-neighbor",
      "term": "Exact Nearest Neighbor",
      "definition": "A search approach that guarantees finding the true closest vectors to a query by exhaustively computing distances to all vectors in the index, providing perfect recall but with linear time complexity that limits scalability.",
      "tags": [
        "Vector Database",
        "Search"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-executive-order-on-ai",
      "term": "Executive Order on AI",
      "definition": "US Executive Order 14110 on Safe Secure and Trustworthy Development and Use of Artificial Intelligence signed by President Biden on October 30 2023. The order established new standards for AI safety and security and directed federal agencies to address AI risks.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-existential-risk-from-ai",
      "term": "Existential Risk from AI",
      "definition": "The hypothesis that sufficiently advanced AI systems could pose a threat to the continued existence of humanity or permanently curtail its potential, motivating research into alignment and AI safety.",
      "tags": [
        "AI Safety",
        "AI Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-expectation-maximization",
      "term": "Expectation-Maximization",
      "definition": "An iterative algorithm for finding maximum likelihood estimates in models with latent variables. It alternates between computing expected values of the latent variables (E-step) and maximizing the likelihood with respect to model parameters (M-step).",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-expectation-maximization-algorithm",
      "term": "Expectation-Maximization Algorithm",
      "definition": "An iterative statistical method for finding maximum likelihood estimates when data is incomplete or has latent variables. Published by Arthur Dempster Nan Laird and Donald Rubin in 1977 EM became fundamental to clustering topic modeling and mixture model training.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-expected-calibration-error",
      "term": "Expected Calibration Error",
      "definition": "A scalar summary of calibration quality computed by binning predictions by confidence, calculating the absolute difference between accuracy and confidence within each bin, and averaging weighted by bin size.",
      "tags": [
        "Machine Learning",
        "Metrics"
      ],
      "domain": "datasets",
      "link": null,
      "related": []
    },
    {
      "id": "term-experience-replay",
      "term": "Experience Replay",
      "definition": "A technique where an agent stores past transitions in a replay buffer and samples from it during training, breaking temporal correlations between consecutive samples. Experience replay improves data efficiency and training stability.",
      "tags": [
        "Reinforcement Learning",
        "Value Methods"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-expert-parallelism",
      "term": "Expert Parallelism",
      "definition": "A distributed computing strategy for mixture-of-experts models where different expert subnetworks are placed on different devices, with all-to-all communication routing tokens to their assigned experts.",
      "tags": [
        "Neural Networks",
        "Architecture"
      ],
      "domain": "models",
      "link": null,
      "related": []
    },
    {
      "id": "term-expert-prompting",
      "term": "Expert Prompting",
      "definition": "A prompting method that instructs the model to first identify the most qualified expert identity for a given question, adopt that expert's perspective and knowledge base, then provide an authoritative answer informed by that domain expertise.",
      "tags": [
        "Prompt Engineering",
        "Persona"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-expert-systems",
      "term": "Expert Systems",
      "definition": "AI programs popular in the 1970s and 1980s that encoded human expert knowledge as if-then rules to solve domain-specific problems, representing the dominant commercial AI paradigm before the rise of machine learning.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-explainability",
      "term": "Explainability (XAI)",
      "definition": "The ability to understand and explain how AI models make decisions. Important for trust, debugging, regulatory compliance, and identifying potential biases.",
      "tags": [
        "Transparency",
        "Trust"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-explainable-ai",
      "term": "Explainable AI",
      "definition": "A field of research focused on making AI system decisions understandable to humans. Includes techniques like feature attribution attention visualization concept-based explanations and counterfactual reasoning.",
      "tags": [
        "Safety",
        "Fundamentals"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-explainable-ai-history",
      "term": "Explainable AI History",
      "definition": "The evolution of explainable AI from early rule-based systems that were inherently interpretable through the challenge of black-box deep learning to modern XAI techniques like LIME SHAP attention visualization and concept-based explanations.",
      "tags": [
        "History",
        "Milestones"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-exploding-gradient",
      "term": "Exploding Gradient",
      "definition": "A training instability where gradients grow exponentially large during backpropagation through deep networks, causing weight updates to become excessively large and training to diverge.",
      "tags": [
        "Machine Learning",
        "Optimization"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-exploding-gradient-problem",
      "term": "Exploding Gradient Problem",
      "definition": "A complementary problem to vanishing gradients where gradients grow exponentially during backpropagation causing unstable training. Solutions include gradient clipping proper weight initialization and architectural innovations like LSTM and residual networks.",
      "tags": [
        "History",
        "Fundamentals"
      ],
      "domain": "history",
      "link": null,
      "related": []
    },
    {
      "id": "term-exploitation-exploration-tradeoff-safety",
      "term": "Exploitation-Exploration Tradeoff Safety",
      "definition": "Safety implications of the balance between exploiting known-safe actions and exploring new actions in reinforcement learning. Unsafe exploration can lead to catastrophic outcomes in real-world deployments.",
      "tags": [
        "Safety",
        "Technical"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-exploration-vs-exploitation",
      "term": "Exploration vs Exploitation",
      "definition": "The fundamental dilemma in RL between exploring unknown actions to discover potentially better strategies and exploiting current knowledge to maximize immediate reward. Balancing this tradeoff is essential for efficient learning.",
      "tags": [
        "Reinforcement Learning",
        "Exploration"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-exponential-distribution",
      "term": "Exponential Distribution",
      "definition": "A continuous probability distribution modeling the time between events in a Poisson process. It has the memoryless property: the probability of an event in the next interval is independent of elapsed time.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-exponential-family",
      "term": "Exponential Family",
      "definition": "A broad class of probability distributions characterized by a specific mathematical form, including normal, Poisson, binomial, exponential, and gamma distributions. They are foundational to generalized linear models and conjugate Bayesian analysis.",
      "tags": [
        "Statistics",
        "Probability"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-exponential-moving-average-model",
      "term": "Exponential Moving Average Model",
      "definition": "A technique that maintains a running exponential average of model weights during training and uses the averaged model for evaluation. Provides smoother more stable predictions than the final training checkpoint. Standard practice in modern training.",
      "tags": [
        "Algorithms",
        "Training"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-exponential-smoothing",
      "term": "Exponential Smoothing",
      "definition": "A family of time series forecasting methods that compute weighted averages of past observations with exponentially decreasing weights. Variants include simple, double (Holt's), and triple (Holt-Winters) exponential smoothing.",
      "tags": [
        "Data Science",
        "Statistics"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-extra-trees",
      "term": "Extra Trees",
      "definition": "Extremely Randomized Trees is an ensemble method similar to random forests but with additional randomization. Splits are chosen completely at random rather than searching for the best split. Faster training with competitive accuracy.",
      "tags": [
        "Algorithms",
        "Technical"
      ],
      "domain": "algorithms",
      "link": null,
      "related": []
    },
    {
      "id": "term-extraction",
      "term": "Extraction",
      "definition": "Using AI to identify and pull specific information from unstructured text. Applications include named entity extraction, key phrase extraction, and structured data extraction.",
      "tags": [
        "NLP Task",
        "Application"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-extractive-ai",
      "term": "Extractive AI",
      "definition": "AI development practices that extract value from communities through data collection and labor without providing equitable returns. Includes concerns about data exploitation and digital labor in the Global South.",
      "tags": [
        "Safety",
        "Ethics"
      ],
      "domain": "safety",
      "link": null,
      "related": []
    },
    {
      "id": "term-extractive-question-answering",
      "term": "Extractive Question Answering",
      "definition": "A QA task where the model identifies the answer as a contiguous span of text within a given context passage, predicting the start and end positions of the answer.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    },
    {
      "id": "term-extractive-summarization",
      "term": "Extractive Summarization",
      "definition": "A summarization approach that selects and concatenates the most important sentences or passages from the source document to form a summary without generating new text.",
      "tags": [
        "NLP",
        "Text Processing"
      ],
      "domain": "general",
      "link": null,
      "related": []
    }
  ]
}