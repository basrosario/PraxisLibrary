term,definition,tags,domain,link
Abstraction Attack,An adversarial technique that exploits the gap between a model's learned abstractions and real-world inputs by crafting examples that match abstract features while differing perceptually. Related to concept-level adversarial manipulation.,Safety;Technical,safety,
Access Control for AI Systems,Mechanisms and policies that restrict who can deploy query or modify AI systems. Includes authentication authorization and audit logging to prevent unauthorized use of powerful models.,Safety;Governance,safety,
Accountability Gap,The problem that arises when no single party can be held responsible for harm caused by AI systems due to complex supply chains and distributed decision-making across developers deployers and users.,Safety;Governance,safety,
Active Inference Safety,Research into making active inference agents safe by ensuring their generative models and prior preferences are aligned with human values. Draws from the free energy principle in neuroscience.,Safety;Technical,safety,
Adaptive Stress Testing,A technique that uses reinforcement learning to find the most likely failure scenarios for autonomous systems. Developed at Stanford to identify edge cases in safety-critical AI applications.,Safety;Technical,safety,
Addictive Design in AI,The use of AI-driven personalization and engagement optimization techniques that exploit psychological vulnerabilities to maximize user screen time or spending. Raises ethical concerns about autonomy and wellbeing.,Safety;Ethics,safety,
Adversarial Example,A carefully crafted input designed to cause a machine learning model to make an incorrect prediction or classification while appearing normal to human observers. First demonstrated by Szegedy et al. in 2013.,Safety;Technical,safety,
Adversarial Machine Learning,The study of attacks on machine learning systems and defenses against them. Encompasses evasion attacks poisoning attacks model stealing and privacy attacks as well as certified robustness methods.,Safety;Technical,safety,
Adversarial Patch,A physical-world adversarial attack that uses a printed patch to fool image classifiers or object detectors. Unlike pixel-level perturbations patches are robust to changes in viewing angle and distance.,Safety;Technical,safety,
Adversarial Robustness,The ability of a machine learning model to maintain correct predictions when inputs are deliberately perturbed by an adversary. Measured using attack success rates under various threat models and perturbation budgets.,Safety;Technical,safety,
Adversarial Training,A defense technique that augments training data with adversarial examples to improve model robustness. Originally proposed by Goodfellow et al. in 2014 as a min-max optimization between the attacker and model.,Safety;Technical,safety,
Age Verification in AI,Technical and policy mechanisms to verify user age before granting access to AI systems that may be inappropriate for minors. Raises tensions between child safety and privacy concerns.,Safety;Policy,safety,
Agent Safety,The study of ensuring autonomous AI agents that take actions in the real world do so safely. Covers problems like safe exploration reward hacking side effects and interruptibility.,Safety;Technical,safety,
Aggregate Ethics,An approach to AI ethics that considers the cumulative societal impact of many individually harmless AI decisions rather than focusing only on dramatic individual harms. Important for understanding systemic effects.,Safety;Ethics,safety,
AI Assurance,Processes and evidence that demonstrate an AI system meets its specified requirements for safety security and performance. Analogous to software assurance but extended for ML-specific properties like fairness and robustness.,Safety;Governance,safety,
AI Audit,A systematic evaluation of an AI system to assess compliance with regulations ethical guidelines and performance standards. May include technical testing documentation review and stakeholder consultation.,Safety;Governance,safety,
AI Bounty Program,A program that rewards external researchers for identifying vulnerabilities biases or safety issues in AI systems. Modeled on cybersecurity bug bounties and adopted by organizations like OpenAI and Google DeepMind.,Safety;Governance,safety,
AI Carbon Footprint,The environmental impact of training and running AI models measured in carbon dioxide equivalent emissions. Large language model training can emit hundreds of tons of CO2 raising sustainability concerns.,Safety;Ethics,safety,
AI Certification,Formal processes by which an independent body assesses and certifies that an AI system meets specified safety performance and ethical standards. Emerging in regulated sectors like healthcare aviation and automotive.,Safety;Governance,safety,
AI Compliance,The practice of ensuring AI systems conform to applicable laws regulations industry standards and organizational policies. Includes documentation impact assessments and ongoing monitoring requirements.,Safety;Governance,safety,
AI Content Detection,Tools and techniques for identifying whether text images or other media were generated by AI systems. Uses statistical analysis watermarking and trained classifiers to distinguish human from machine output.,Safety;Technical,safety,
AI Decolonization,A movement to challenge Western-centric assumptions in AI development and deployment. Advocates for including diverse cultural perspectives addressing power imbalances and preventing technological neo-colonialism.,Safety;Ethics,safety,
AI Democratization Risks,The potential dangers of making powerful AI capabilities widely accessible without adequate safety measures. Includes dual-use concerns proliferation risks and the challenge of preventing misuse at scale.,Safety;Policy,safety,
AI Dependency Risk,The risk that over-reliance on AI systems creates fragility in critical infrastructure and decision-making processes. Includes concerns about skill atrophy single points of failure and vendor lock-in.,Safety;Ethics,safety,
AI Disclosure Requirements,Legal or ethical obligations to inform users when they are interacting with an AI system rather than a human. Increasingly mandated by regulations including the EU AI Act and various US state laws.,Safety;Policy,safety,
AI Due Diligence,The process of systematically evaluating AI systems for risks before deployment or acquisition. Includes technical audits bias testing legal review and assessment of societal impact.,Safety;Governance,safety,
AI Ecosystem Risk,Systemic risks arising from the interconnected nature of AI systems where a failure or vulnerability in one component can cascade through dependent systems and applications.,Safety;Technical,safety,
AI Emergency Stop,A mechanism to rapidly shut down or constrain an AI system that is operating unsafely. Also known as a kill switch. Designing reliable emergency stops for distributed AI systems remains an open challenge.,Safety;Technical,safety,
AI Ethical Framework,A structured set of principles guidelines and procedures for developing and deploying AI systems responsibly. Examples include the IEEE Ethically Aligned Design and the OECD AI Principles.,Safety;Ethics,safety,
AI Export Controls,Government regulations restricting the international transfer of AI technology including models training data chips and related expertise. Used as tools of national security and technology competition policy.,Safety;Policy,safety,
AI Fairness Metrics,Quantitative measures used to assess whether an AI system treats different demographic groups equitably. Common metrics include demographic parity equalized odds and calibration across subgroups.,Safety;Technical,safety,
AI Forensics,The application of investigative techniques to understand AI system behavior after an incident or failure. Includes model inspection log analysis and provenance tracking to determine root causes.,Safety;Technical,safety,
AI Gap,The disparity in AI capabilities resources and benefits between wealthy and less wealthy nations or communities. Contributes to widening global inequality and digital divides.,Safety;Ethics,safety,
AI Governance Framework,A comprehensive structure of policies processes roles and tools for managing the development deployment and monitoring of AI systems within an organization or jurisdiction.,Safety;Governance,safety,
AI Hype Cycle,The pattern of inflated expectations followed by disillusionment and eventual productive adoption that characterizes public perception of AI capabilities. Contributes to misallocation of resources and erosion of public trust.,Safety;Ethics,safety,
AI Incident Response,A structured process for detecting investigating and remediating harmful outcomes from AI systems. Modeled on cybersecurity incident response but adapted for ML-specific failure modes like bias drift and hallucination.,Safety;Governance,safety,
AI Insurance,Insurance products designed to cover liabilities arising from AI system failures or harms. An emerging market that faces challenges in risk assessment due to the unpredictable nature of AI failures.,Safety;Governance,safety,
AI Literacy,The knowledge and skills needed for individuals to understand evaluate and interact effectively with AI systems. Considered essential for informed consent democratic participation and workforce adaptation.,Safety;Ethics,safety,
AI Lobbying,Advocacy activities by AI companies and industry groups to influence government policy and regulation. Raises concerns about regulatory capture and the balance between innovation and public safety.,Safety;Policy,safety,
AI Manipulation,The use of AI systems to influence human behavior beliefs or decisions through deceptive or coercive means. Includes deepfakes persuasive AI micro-targeting and synthetic media used for propaganda.,Safety;Ethics,safety,
AI Monoculture Risk,The danger that widespread adoption of similar AI models architectures or training data creates systemic vulnerabilities where a single flaw affects many applications simultaneously.,Safety;Technical,safety,
AI Nationalism,Government policies that prioritize domestic AI development as a matter of national competitiveness and security. Can lead to fragmented standards restricted collaboration and an AI arms race between nations.,Safety;Policy,safety,
AI Ombudsman,An independent official or office responsible for investigating complaints about AI systems and advocating for affected individuals. Proposed as a governance mechanism to provide recourse for AI-related harms.,Safety;Governance,safety,
AI Oversight Board,An organizational body responsible for reviewing and approving high-risk AI deployments. May include technical experts ethicists legal advisors and community representatives to ensure balanced decision-making.,Safety;Governance,safety,
AI Patent Ethics,Ethical considerations around patenting AI inventions including questions of inventorship for AI-generated innovations access to AI technologies and the impact of IP regimes on AI development.,Safety;Ethics,safety,
AI Pluralism,An approach to AI development that embraces diverse perspectives values and cultural contexts rather than imposing a single worldview. Contrasts with monocultural approaches to AI alignment.,Safety;Ethics,safety,
AI Policy Sandbox,A controlled regulatory environment where new AI applications can be tested under relaxed rules with government oversight. Allows regulators to learn about emerging technologies before establishing permanent rules.,Safety;Policy,safety,
AI Poverty Trap,A scenario where communities lacking AI capabilities fall further behind economically and socially as AI-driven productivity gains accrue primarily to already-advantaged groups and nations.,Safety;Ethics,safety,
AI Procurement Standards,Requirements and evaluation criteria that government agencies and organizations use when purchasing or contracting AI systems. Include specifications for fairness transparency security and accountability.,Safety;Governance,safety,
AI Professional Ethics,Ethical obligations and codes of conduct for individuals working in AI development and deployment. Analogous to professional ethics in medicine law and engineering.,Safety;Ethics,safety,
AI Proportionality,The principle that the intrusiveness and risks of an AI system should be proportionate to the benefits it provides and the severity of the problem it addresses. A key principle in the EU AI Act.,Safety;Policy,safety,
AI Public Engagement,Processes for involving the general public in decisions about AI development and deployment. Includes citizen assemblies public consultations deliberative forums and participatory design methods.,Safety;Governance,safety,
AI Quality Assurance,Systematic processes to ensure AI systems meet defined standards for accuracy reliability fairness and safety throughout their lifecycle. Extends traditional software QA with ML-specific testing methods.,Safety;Governance,safety,
AI Race Dynamics,The competitive pressures between nations and companies to develop AI capabilities quickly which can lead to cutting corners on safety testing and responsible development practices.,Safety;Policy,safety,
AI Readiness Assessment,A structured evaluation of an organization's preparedness to adopt and deploy AI systems responsibly. Covers technical infrastructure data quality skills governance and ethical considerations.,Safety;Governance,safety,
AI Registration,A proposed requirement that AI systems above a certain capability threshold be registered with a government authority before deployment. Analogous to product registration in pharmaceuticals and aviation.,Safety;Policy,safety,
AI Reliance,The degree to which humans depend on AI system outputs for decision-making. Appropriate reliance means using AI recommendations when they improve outcomes and overriding them when they do not.,Safety;Ethics,safety,
AI Rights,The philosophical and legal question of whether sufficiently advanced AI systems should be granted legal rights or moral standing. Debates draw on animal rights philosophy and corporate personhood precedents.,Safety;Ethics,safety,
AI Risk Assessment,A systematic process for identifying evaluating and prioritizing risks associated with an AI system including technical failures ethical harms and societal impacts.,Safety;Governance,safety,
AI Risk Taxonomy,A structured classification system for categorizing the types of risks posed by AI systems. Frameworks include NIST AI RMF categories MIT FutureTech risk maps and the EU AI Act risk levels.,Safety;Governance,safety,
AI Sandboxing,Running an AI system in an isolated environment with limited access to external resources to prevent unintended consequences during testing or deployment. A containment strategy for potentially dangerous systems.,Safety;Technical,safety,
AI Security,The practice of protecting AI systems from adversarial attacks data poisoning model theft and other threats. Encompasses both defensive techniques and threat modeling specific to machine learning systems.,Safety;Technical,safety,
AI Social Contract,The implicit agreement between AI developers deployers users and society about the acceptable uses and limits of AI technology. Draws on social contract theory to frame AI governance obligations.,Safety;Ethics,safety,
AI Standards,Technical specifications and guidelines established by standards bodies for AI system development testing and deployment. Key organizations include ISO IEC IEEE NIST and the OECD.,Safety;Governance,safety,
AI Supply Chain Security,Measures to ensure the integrity and safety of all components in the AI development pipeline including training data pre-trained models libraries and deployment infrastructure.,Safety;Technical,safety,
AI Sustainability,The practice of developing and deploying AI systems in ways that are environmentally economically and socially sustainable over the long term. Covers energy efficiency resource use and equitable access.,Safety;Ethics,safety,
AI Talent Pipeline,The system of education training and recruitment that produces qualified AI practitioners. Concentration of AI talent in a few companies and countries raises concerns about equity and safety capacity.,Safety;Ethics,safety,
AI Taxonomy,A systematic classification of AI systems by capability risk level domain or technology type. Used by regulators to apply differentiated requirements based on the nature and impact of each system.,Safety;Governance,safety,
AI Testing Standards,Established methodologies and benchmarks for evaluating AI system performance safety and fairness. Include adversarial testing stress testing bias auditing and performance benchmarking under defined conditions.,Safety;Governance,safety,
AI Tort Law,The application of tort law principles to AI-related harms. Raises novel questions about duty of care foreseeability causation and liability when autonomous systems cause injury or damage.,Safety;Policy,safety,
AI Transparency Report,A public document disclosing information about an organization's AI systems including their capabilities limitations known biases and safety measures. Modeled on corporate social responsibility reporting.,Safety;Governance,safety,
AI Treaty,A proposed international agreement to regulate the development and deployment of AI systems. Discussions draw parallels to nuclear arms control treaties and the Geneva Conventions.,Safety;Policy,safety,
AI Trust,The degree to which users and society have confidence that AI systems will behave as expected and in accordance with human values. Built through transparency reliability accountability and demonstrated safety.,Safety;Ethics,safety,
AI Unemployment,The displacement of human workers by AI and automation systems. Economic research suggests AI may transform rather than eliminate most jobs but transition costs and distributional effects raise significant concerns.,Safety;Ethics,safety,
AI Value Lock-in,The risk that early AI systems encode specific values or preferences that become difficult to change as systems grow more capable and entrenched. A long-term concern for AI alignment research.,Safety;Technical,safety,
AI Vulnerability Assessment,A systematic evaluation of potential weaknesses in an AI system that could be exploited by adversaries or lead to failure modes. Covers model architecture training data deployment environment and human factors.,Safety;Technical,safety,
AI Water Usage,The water consumption associated with cooling data centers that train and run AI models. Large training runs can consume millions of liters of water raising environmental sustainability concerns.,Safety;Ethics,safety,
AI Weapons Review,Legal and ethical assessment of autonomous weapon systems under international humanitarian law. Required by Article 36 of Additional Protocol I to the Geneva Conventions for new weapons or means of warfare.,Safety;Policy,safety,
AI Workforce Transition,Programs and policies to help workers displaced by AI automation acquire new skills and find alternative employment. Includes retraining programs social safety nets and economic diversification strategies.,Safety;Policy,safety,
Alignment Problem,The fundamental challenge of ensuring that AI systems pursue goals and exhibit behaviors that are consistent with human intentions and values. Central to AI safety research particularly for advanced systems.,Safety;Fundamentals,safety,
Alignment Research,The scientific study of methods to ensure AI systems are aligned with human values and intentions. Includes work on reward modeling interpretability scalable oversight and value learning.,Safety;Technical,safety,
Alignment Tax,The additional cost in performance resources or development time required to make an AI system safe and aligned compared to an unaligned version. High alignment taxes can create incentives to skip safety measures.,Safety;Technical,safety,
Amplification,A scalable oversight technique where human supervisors are augmented by AI assistants to evaluate AI behavior on complex tasks. Proposed by Christiano et al. as a way to maintain human oversight as AI capabilities grow.,Safety;Technical,safety,
Anchoring Effect in AI,A cognitive bias where users over-rely on the first piece of information provided by an AI system. Can lead to poor decision-making when AI outputs are presented as initial recommendations.,Safety;Ethics,safety,
Anomaly Detection for Safety,The use of anomaly detection techniques to identify unusual or potentially unsafe AI system behavior in deployment. Serves as an early warning system for distribution shift or adversarial attacks.,Safety;Technical,safety,
Anthropomorphism Risk,The danger that designing AI systems with human-like characteristics leads users to attribute human emotions intentions and reliability to systems that lack these qualities.,Safety;Ethics,safety,
Anti-Discrimination Law in AI,Legal frameworks that prohibit AI systems from discriminating against individuals based on protected characteristics such as race gender age disability or religion.,Safety;Policy,safety,
Assistive AI Ethics,Ethical considerations specific to AI systems designed to assist people with disabilities. Includes concerns about autonomy dignity dependency and the risk of reducing complex human needs to technical solutions.,Safety;Ethics,safety,
Attention Economy and AI,The intersection of AI-driven content recommendation systems and the competition for human attention. AI amplifies engagement optimization which can conflict with user wellbeing and informed choice.,Safety;Ethics,safety,
Attribution in AI,The ability to trace AI system outputs back to their sources including training data model components and design decisions. Important for accountability transparency and intellectual property protection.,Safety;Technical,safety,
Audit Trail for AI,A chronological record of all decisions actions and changes related to an AI system throughout its lifecycle. Essential for accountability regulatory compliance and incident investigation.,Safety;Governance,safety,
Backdoor Attack,A type of poisoning attack where an adversary introduces a hidden trigger into a model during training that causes targeted misclassification when the trigger is present in test inputs.,Safety;Technical,safety,
Behavioral Cloning Safety,Safety concerns arising from training AI agents to imitate human behavior. Includes distribution shift compounding errors and the risk of learning unsafe human behaviors along with desired ones.,Safety;Technical,safety,
Benchmark Gaming,The practice of optimizing AI systems specifically to perform well on standard benchmarks without corresponding improvements in real-world capability. Undermines the validity of evaluation metrics.,Safety;Technical,safety,
Benefit-Risk Analysis for AI,A systematic comparison of the potential benefits and risks of an AI system to determine whether deployment is justified. Required by many regulatory frameworks for high-risk AI applications.,Safety;Governance,safety,
Bias Amplification,The phenomenon where machine learning models amplify biases present in training data producing outputs that are more biased than the data itself. Can create feedback loops that worsen inequality over time.,Safety;Technical,safety,
Bias Bounty,A program that rewards individuals for identifying and reporting biases in AI systems. Similar to bug bounties in cybersecurity and aimed at crowdsourcing the discovery of unfair model behaviors.,Safety;Governance,safety,
Bias in AI,Systematic errors in AI system outputs that arise from prejudiced assumptions in training data algorithm design or deployment context. Can lead to unfair discriminatory or inaccurate outcomes for affected groups.,Safety;Fundamentals,safety,
Bias Mitigation,Techniques and practices for reducing unfair bias in AI systems. Includes pre-processing methods like resampling in-processing methods like adversarial debiasing and post-processing methods like threshold adjustment.,Safety;Technical,safety,
Bias Testing,The systematic evaluation of AI systems for unfair biases across demographic groups and use cases. Includes statistical parity testing disparate impact analysis and intersectional bias assessment.,Safety;Technical,safety,
Blue Team (AI),A team responsible for defending AI systems against adversarial attacks and identifying vulnerabilities. Works in opposition to red teams to improve system security and robustness.,Safety;Technical,safety,
Boundary Testing,The practice of probing AI systems at the edges of their intended operating conditions to identify failure modes and safety limitations. Essential for understanding system behavior under stress.,Safety;Technical,safety,
Capability Elicitation,Techniques for systematically discovering what an AI system can do including capabilities that may not be apparent from standard benchmarks. Important for understanding potential risks of advanced systems.,Safety;Technical,safety,
Catastrophic Interference,A phenomenon in neural networks where learning new information causes the model to forget previously learned information. Poses safety risks when models deployed in production lose critical capabilities.,Safety;Technical,safety,
Causal Fairness,An approach to algorithmic fairness that uses causal reasoning to determine whether an AI system's decisions are influenced by protected attributes through illegitimate causal pathways.,Safety;Technical,safety,
Certified Robustness,A formal guarantee that a model's predictions will not change under input perturbations within a specified bound. Provides provable rather than empirical assurance of adversarial robustness.,Safety;Technical,safety,
Chain of Responsibility in AI,The linked sequence of actors including developers deployers and users who share accountability for AI system outcomes. Legal frameworks are evolving to allocate responsibility across this chain.,Safety;Governance,safety,
Child Safety in AI,Protections and design considerations to prevent AI systems from harming minors. Includes age-appropriate content filtering data privacy for children and preventing exploitation or manipulation.,Safety;Policy,safety,
Chilling Effect of AI,The phenomenon where awareness of AI surveillance or monitoring causes people to self-censor their speech behavior or activities. Raises concerns about freedom of expression and democratic participation.,Safety;Ethics,safety,
Civil Society and AI,The role of non-governmental organizations advocacy groups and community organizations in shaping AI policy and holding developers accountable. Provides an important counterbalance to industry and government interests.,Safety;Policy,safety,
Cognitive Liberty,The right of individuals to maintain sovereignty over their own thought processes and mental states free from manipulation by AI or neurotechnology. An emerging concept in digital rights and neuroethics.,Safety;Ethics,safety,
Collective Action in AI Safety,The challenge of coordinating multiple AI developers and nations to invest in safety measures when competitive pressures incentivize racing ahead. A classic collective action problem in AI governance.,Safety;Policy,safety,
Compliance-by-Design,An approach to AI development that embeds regulatory compliance requirements into the system design and development process from the outset rather than adding compliance measures retroactively.,Safety;Governance,safety,
Compositional Robustness,The ability of an AI system to maintain safe behavior when its components are combined in new ways or deployed in contexts different from those anticipated during development.,Safety;Technical,safety,
Concept Drift,A change in the statistical properties of the target variable that a model is trying to predict over time. Can degrade model performance and safety if not detected and addressed through monitoring.,Safety;Technical,safety,
Consent in AI,The principle that individuals should give informed voluntary consent before their data is used to train AI systems or before AI systems make decisions affecting them.,Safety;Ethics,safety,
Consequential Decision-Making,AI applications that make or significantly influence decisions with material effects on people's lives such as hiring lending healthcare and criminal justice.,Safety;Policy,safety,
Contestability,The ability of individuals to challenge and seek review of decisions made by or with the assistance of AI systems. A key principle in ensuring accountability and due process in automated decision-making.,Safety;Governance,safety,
Contextual Integrity,A theory of privacy that evaluates data flows against context-specific norms. Applied to AI to assess whether data collection and use respects the informational norms of the context in which data originated.,Safety;Ethics,safety,
Continuous Monitoring,The ongoing observation and assessment of AI system behavior in production to detect performance degradation bias drift safety violations and emerging risks in real time.,Safety;Governance,safety,
Contractual AI Accountability,Legal provisions in contracts between AI developers deployers and users that allocate responsibilities for safety performance and harm remediation. An emerging area of technology law.,Safety;Policy,safety,
Conversational AI Safety,Safety measures specific to dialogue systems and chatbots including prevention of harmful responses manipulation detection and appropriate escalation to human operators.,Safety;Technical,safety,
Corporate AI Responsibility,The obligation of companies that develop or deploy AI to manage the social and environmental impacts of their AI systems. Extends corporate social responsibility principles to artificial intelligence.,Safety;Governance,safety,
Cross-Border AI Governance,International cooperation and coordination on AI regulation and standards across national jurisdictions. Addresses challenges of regulatory fragmentation and forum shopping in global AI deployment.,Safety;Policy,safety,
Crowdsourced Safety Testing,The practice of engaging large numbers of external testers to identify safety issues in AI systems. Leverages diverse perspectives and use patterns that internal testing may miss.,Safety;Technical,safety,
Cultural Bias in AI,Bias in AI systems that reflects the cultural perspectives and values of the developers or training data while marginalizing other cultural viewpoints and practices.,Safety;Ethics,safety,
Cumulative Risk in AI,The aggregate risk created by the deployment of many AI systems across society even when each individual system poses only modest risk. Important for systemic risk assessment and regulation.,Safety;Governance,safety,
Cyber-Physical AI Safety,Safety considerations for AI systems that interact with the physical world through sensors and actuators. Includes autonomous vehicles robots and industrial control systems where failures can cause physical harm.,Safety;Technical,safety,
Dark Patterns in AI,Deceptive design techniques in AI-powered interfaces that manipulate users into making choices they would not otherwise make. Examples include hidden defaults misleading framing and obstruction tactics.,Safety;Ethics,safety,
Data Consent,The process by which individuals grant permission for their personal data to be collected used and shared for AI training and operation. Must be informed specific and freely given under regulations like GDPR.,Safety;Policy,safety,
Data Ethics,The branch of ethics concerned with the responsible collection use sharing and governance of data. Particularly important for AI where training data quality and representativeness directly affect system behavior.,Safety;Ethics,safety,
Data Governance,Policies and procedures for managing data quality security privacy and compliance throughout its lifecycle. Critical for AI systems where data quality directly impacts model behavior and safety.,Safety;Governance,safety,
Data Minimization,The principle of collecting and retaining only the minimum amount of personal data necessary for a specific purpose. A key requirement under GDPR and increasingly applied to AI training data practices.,Safety;Policy,safety,
Data Poisoning,An attack that corrupts a machine learning model by manipulating its training data. Can introduce targeted biases backdoors or general performance degradation without modifying the model architecture.,Safety;Technical,safety,
Data Protection Impact Assessment,A structured assessment required under GDPR for processing that is likely to result in high risk to individuals. Increasingly applied to AI systems that process personal data at scale.,Safety;Policy,safety,
Decision Support System Safety,Safety requirements for AI systems that assist human decision-makers rather than making autonomous decisions. Must balance providing useful recommendations with avoiding undue influence on human judgment.,Safety;Technical,safety,
Deepfake Regulation,Laws and policies specifically targeting the creation and distribution of AI-generated synthetic media. Approaches range from disclosure requirements to criminal penalties for malicious deepfakes.,Safety;Policy,safety,
Defense in Depth for AI,A security strategy that uses multiple layers of protection to secure AI systems. No single defense is considered sufficient so overlapping safeguards address different threat vectors.,Safety;Technical,safety,
Deontological AI Ethics,An approach to AI ethics based on rule-following and duty rather than consequences. Holds that certain actions like deception or privacy violation are inherently wrong regardless of their outcomes.,Safety;Ethics,safety,
Deployment Monitoring,Continuous observation of AI system behavior after release to production to detect performance degradation distributional shift and emerging safety concerns in real-world conditions.,Safety;Governance,safety,
Design Justice,A framework that centers the voices of communities most impacted by design decisions in the AI development process. Challenges the assumption that designers know best and emphasizes community-led design.,Safety;Ethics,safety,
Digital Consent,The process of obtaining meaningful permission from users for AI-mediated data collection processing and decision-making in digital environments. Challenges include consent fatigue and information asymmetry.,Safety;Ethics,safety,
Digital Dignity,The principle that AI systems should treat individuals with respect and not reduce human beings to data points or optimization targets. Encompasses privacy autonomy and non-discrimination.,Safety;Ethics,safety,
Digital Divide and AI,The gap between those who have access to AI technologies and the skills to use them and those who do not. AI may widen existing digital divides without deliberate policies to promote equitable access.,Safety;Ethics,safety,
Discriminative vs Generative Safety,Different safety challenges posed by discriminative models that classify inputs versus generative models that produce new content. Generative models face additional risks of producing harmful or misleading content.,Safety;Technical,safety,
Disinformation Campaign Detection,AI techniques for identifying coordinated campaigns to spread false information across social media and other platforms. Uses network analysis content analysis and behavioral pattern detection.,Safety;Technical,safety,
Distributional Shift,A change in the statistical distribution of data encountered during deployment compared to training data. Can cause model performance to degrade unpredictably raising safety concerns in critical applications.,Safety;Technical,safety,
Documentation Requirements for AI,Regulatory and best-practice requirements for documenting AI system design training data performance evaluation and known limitations. Essential for transparency auditing and accountability.,Safety;Governance,safety,
Dual-Use Concern,The recognition that AI technologies developed for beneficial purposes can also be used for harmful applications. Requires researchers and developers to consider potential misuse and implement safeguards.,Safety;Ethics,safety,
Dynamic Consent,A consent model that allows individuals to update their data sharing preferences over time as AI systems evolve and new uses emerge. Contrasts with static one-time consent approaches.,Safety;Ethics,safety,
Edge Case Safety,The challenge of ensuring AI systems behave safely in unusual or extreme situations that were not well represented in training data. Critical for safety-critical applications like autonomous driving.,Safety;Technical,safety,
Electromagnetic Vulnerability of AI,The susceptibility of AI hardware and systems to electromagnetic interference or attack. Relevant for safety-critical deployments where electromagnetic disruption could cause dangerous failures.,Safety;Technical,safety,
Emergent Behavior Risk,The risk that AI systems exhibit unexpected capabilities or behaviors that were not intended or predicted by their developers. Particularly concerning for large models where emergent properties are difficult to anticipate.,Safety;Technical,safety,
Emotional AI Ethics,Ethical concerns related to AI systems that detect generate or respond to human emotions. Issues include accuracy across demographics privacy of emotional data and potential for manipulation.,Safety;Ethics,safety,
Environmental Justice in AI,The principle that the environmental costs of AI development and deployment including energy use and e-waste should not disproportionately burden marginalized communities.,Safety;Ethics,safety,
Epistemic Autonomy,The right and capacity of individuals to form their own beliefs and make their own judgments rather than having these determined by AI recommendations and filter bubbles.,Safety;Ethics,safety,
Escalation Protocol,A defined procedure for elevating AI safety concerns from technical teams to management and potentially to external regulators when certain risk thresholds are exceeded.,Safety;Governance,safety,
Ethical AI by Design,An approach that integrates ethical considerations throughout the AI development lifecycle from initial requirements through design implementation testing and deployment.,Safety;Ethics,safety,
Ethics of Care in AI,An ethical framework that emphasizes relationships responsibility and contextual judgment in AI development. Contrasts with principle-based approaches by focusing on the needs of specific affected individuals and communities.,Safety;Ethics,safety,
Evaluation Harness,A standardized framework for running multiple benchmarks and safety tests against AI models in a consistent and reproducible manner. Examples include EleutherAI's lm-evaluation-harness.,Safety;Technical,safety,
Evasion Attack,An adversarial attack that modifies inputs at test time to cause misclassification by a deployed model. Unlike poisoning attacks evasion attacks do not require access to the training process.,Safety;Technical,safety,
Explainable AI,A field of research focused on making AI system decisions understandable to humans. Includes techniques like feature attribution attention visualization concept-based explanations and counterfactual reasoning.,Safety;Fundamentals,safety,
Exploitation-Exploration Tradeoff Safety,Safety implications of the balance between exploiting known-safe actions and exploring new actions in reinforcement learning. Unsafe exploration can lead to catastrophic outcomes in real-world deployments.,Safety;Technical,safety,
Extractive AI,AI development practices that extract value from communities through data collection and labor without providing equitable returns. Includes concerns about data exploitation and digital labor in the Global South.,Safety;Ethics,safety,
Factuality Evaluation,Methods for assessing whether AI-generated text contains accurate and verifiable information. Includes automated fact-checking knowledge-grounded evaluation and citation verification techniques.,Safety;Technical,safety,
Fail-Safe Design,An engineering approach where AI systems default to a safe state when they encounter errors or unexpected conditions. Ensures that system failures do not lead to dangerous or harmful outcomes.,Safety;Technical,safety,
Failure Mode Analysis,A systematic technique for identifying potential failure modes of an AI system and their effects on system behavior and user safety. Adapted from traditional engineering FMEA for machine learning systems.,Safety;Technical,safety,
Fair Machine Learning,The subfield of machine learning focused on developing models and algorithms that produce equitable outcomes across different demographic groups. Encompasses both technical methods and sociotechnical approaches.,Safety;Fundamentals,safety,
Fairness Constraint,A mathematical condition imposed during model training or post-processing to ensure that predictions satisfy a specified fairness criterion. Examples include parity constraints and calibration requirements.,Safety;Technical,safety,
Fairness Through Awareness,A fairness framework proposed by Dwork et al. in 2012 that requires similar individuals to be treated similarly by an algorithm. Uses a task-specific metric to define similarity between individuals.,Safety;Technical,safety,
Fairness Through Unawareness,A naive fairness approach that simply removes protected attributes from model inputs. Generally insufficient because other features can serve as proxies for protected characteristics.,Safety;Technical,safety,
Feature Attribution,An interpretability technique that assigns importance scores to input features indicating their contribution to a model's prediction. Methods include SHAP LIME integrated gradients and attention weights.,Safety;Technical,safety,
Federated Learning Privacy,Privacy-preserving properties and risks of federated learning where models are trained across distributed devices without centralizing data. While reducing direct data exposure gradient information can still leak private details.,Safety;Technical,safety,
Feedback Loop Risk,The danger that AI system outputs influence future training data creating self-reinforcing cycles that amplify errors or biases over time. Common in recommender systems and predictive policing.,Safety;Technical,safety,
Fiduciary Duty for AI,A proposed legal framework that would impose fiduciary obligations on AI developers or deployers requiring them to act in the best interests of the individuals affected by their systems.,Safety;Policy,safety,
Filter Bubble,An information ecosystem created by AI recommendation algorithms that limits users exposure to diverse viewpoints by serving content aligned with their existing preferences and beliefs.,Safety;Ethics,safety,
Formal Verification for AI,The use of mathematical methods to prove that an AI system satisfies specified safety properties. Provides stronger guarantees than empirical testing but faces scalability challenges for large neural networks.,Safety;Technical,safety,
Foundation Model Risk,Risks specific to large foundation models that are adapted for many downstream tasks. A single vulnerability or bias in the foundation model can propagate to all applications built on it.,Safety;Technical,safety,
Frontier Safety Framework,A set of policies and practices adopted by leading AI labs to manage the risks associated with the most capable AI models. Includes capability evaluations safety testing and deployment controls.,Safety;Governance,safety,
Gender Bias in AI,Systematic unfairness in AI system behavior related to gender. Common in language models hiring tools and image generation systems due to historical gender imbalances in training data.,Safety;Ethics,safety,
Generative AI Misuse,The use of generative AI systems to create harmful content including disinformation non-consensual imagery fraud materials and spam. A growing challenge as generative capabilities improve.,Safety;Ethics,safety,
Goal Misgeneralization,A failure mode where an AI system learns to pursue a proxy goal that correlates with the intended goal during training but diverges in deployment. A key concern in reinforcement learning safety.,Safety;Technical,safety,
Gradient Leakage,An attack that reconstructs private training data from model gradients shared during federated learning or collaborative training. Demonstrates that sharing model updates alone does not guarantee data privacy.,Safety;Technical,safety,
Grievance Mechanism for AI,A formal process through which individuals can raise complaints about harm caused by AI systems and seek remediation. Required by various governance frameworks including the UN Guiding Principles on Business and Human Rights.,Safety;Governance,safety,
Harm Taxonomy,A classification system for the types of harm that AI systems can cause. Categories typically include physical psychological financial reputational and societal harm with subcategories for specific risk types.,Safety;Governance,safety,
Healthcare AI Ethics,Ethical considerations specific to AI in healthcare including patient autonomy informed consent clinical validation equitable access and the appropriate role of AI in medical decision-making.,Safety;Ethics,safety,
Hidden Technical Debt,The accumulated maintenance burden in machine learning systems that arises from entangled dependencies unstable data inputs and feedback loops. Identified by Sculley et al. at Google as a pervasive safety concern.,Safety;Technical,safety,
Human Dignity in AI,The principle that AI systems should respect the inherent worth and rights of all human beings. Serves as a foundational value in many AI ethics frameworks and human rights-based approaches to AI governance.,Safety;Ethics,safety,
Human Factors in AI Safety,The study of how human cognitive abilities limitations and behaviors interact with AI systems to affect safety outcomes. Includes interface design alert fatigue and decision-making under uncertainty.,Safety;Technical,safety,
Human Oversight,The requirement that human beings maintain meaningful control over AI system decisions particularly in high-stakes applications. A cornerstone principle of responsible AI governance and regulatory frameworks.,Safety;Fundamentals,safety,
Human Rights Impact Assessment for AI,A structured evaluation of how an AI system may affect internationally recognized human rights including privacy non-discrimination and freedom of expression.,Safety;Governance,safety,
Impact Assessment,A systematic process for evaluating the potential effects of an AI system on individuals communities and society before and during deployment. Required by some regulations and considered best practice in responsible AI.,Safety;Governance,safety,
Incentive Design for AI Safety,The design of economic regulatory and social incentives to encourage AI developers and deployers to invest in safety measures. Addresses the market failure where safety costs are borne privately but benefits are shared.,Safety;Policy,safety,
Incident Reporting for AI,Mandatory or voluntary systems for reporting AI-related incidents accidents and near-misses. Enables pattern recognition and systemic improvement analogous to incident reporting in aviation and healthcare.,Safety;Governance,safety,
Inclusive AI,The practice of designing AI systems that work equitably for diverse populations including underrepresented groups people with disabilities and speakers of low-resource languages.,Safety;Ethics,safety,
Indigenous Data Sovereignty,The right of indigenous peoples to govern the collection ownership and application of data about their communities and territories. Particularly relevant when AI training data includes indigenous knowledge.,Safety;Ethics,safety,
Information Hazard,Knowledge or information that could cause harm if widely disseminated. In AI includes detailed descriptions of attack methods capability evaluations of dangerous systems and instructions for misuse.,Safety;Ethics,safety,
Informed Autonomy,The principle that individuals should have sufficient understanding of AI systems to make autonomous choices about their use. Goes beyond informed consent to encompass ongoing awareness and meaningful control.,Safety;Ethics,safety,
Input Validation for AI,Techniques for checking that inputs to an AI system fall within expected parameters before processing. A first line of defense against adversarial attacks out-of-distribution inputs and prompt injection.,Safety;Technical,safety,
Interoperability in AI Safety,The ability of different AI safety tools standards and governance frameworks to work together effectively. Important for avoiding fragmentation and ensuring consistent safety practices across the AI ecosystem.,Safety;Governance,safety,
Intersectional Bias,Bias that affects individuals at the intersection of multiple protected characteristics such as race and gender in ways that are not captured by examining each characteristic independently.,Safety;Technical,safety,
Just Transition for AI,Policies and practices to ensure that the benefits and costs of AI-driven economic transformation are distributed equitably particularly for workers and communities most affected by automation.,Safety;Policy,safety,
Justice in AI,The application of principles of distributive procedural and restorative justice to the development and deployment of AI systems. Ensures fair allocation of benefits and burdens across society.,Safety;Ethics,safety,
Killswitch,A mechanism to immediately shut down or constrain an AI system that is behaving unsafely. Designing reliable killswitches for distributed and autonomous systems remains an active research challenge.,Safety;Technical,safety,
Label Bias,Bias introduced into AI systems through inaccurate or subjective labels in training data. Human annotators may apply labels inconsistently or based on their own biases affecting model fairness.,Safety;Technical,safety,
Legal Personhood for AI,The concept of granting AI systems legal rights and obligations similar to those of corporations. Debated as a potential framework for liability and accountability but criticized for potentially deflecting human responsibility.,Safety;Policy,safety,
Liability for AI,Legal responsibility for harm caused by AI systems. Current legal frameworks struggle to assign liability when AI makes autonomous decisions and new frameworks are being developed in various jurisdictions.,Safety;Policy,safety,
Licensing for AI,Proposed regulatory requirements for AI developers or systems to obtain licenses before deployment similar to licensing in healthcare finance and aviation. Intended to ensure minimum safety and competency standards.,Safety;Policy,safety,
Lifecycle Assessment for AI,Evaluation of the environmental social and economic impacts of an AI system throughout its entire lifecycle from data collection through training deployment and decommissioning.,Safety;Governance,safety,
Locking Problem,The challenge that early AI systems may become entrenched and difficult to modify or replace once widely deployed creating path dependency that limits future safety improvements.,Safety;Technical,safety,
Long-term AI Safety,Research focused on ensuring the safety of AI systems that may eventually match or exceed human-level capabilities. Addresses existential risks value alignment and the control problem for advanced AI.,Safety;Fundamentals,safety,
Malicious Use of AI,The deliberate use of AI systems to cause harm including creating weapons surveillance tools disinformation campaigns and tools for cyberattacks fraud or harassment.,Safety;Ethics,safety,
Mandatory Reporting for AI,Legal requirements for organizations to report AI-related incidents failures or safety concerns to regulatory authorities. Analogous to mandatory reporting in healthcare aviation and financial services.,Safety;Policy,safety,
Membership Inference Attack,An attack that determines whether a specific data record was used in training a machine learning model. Poses privacy risks by revealing information about training data membership.,Safety;Technical,safety,
Misinformation Detection,AI techniques for identifying false or misleading information in text images and video. Combines natural language processing knowledge graph verification and source credibility assessment.,Safety;Technical,safety,
Model Auditing,The systematic examination of an AI model's behavior performance and fairness by an independent party. May include testing for bias reviewing training procedures and assessing documentation completeness.,Safety;Governance,safety,
Model Extraction Attack,An attack that creates a functionally equivalent copy of a target model by querying it and training a substitute on the responses. Threatens model intellectual property and can enable further attacks.,Safety;Technical,safety,
Model Governance,Organizational policies and processes for managing the lifecycle of machine learning models from development through deployment monitoring and retirement. Ensures consistent safety and compliance standards.,Safety;Governance,safety,
Model Interpretability,The degree to which a human can understand the cause of a model's decisions. Distinguished from explainability which focuses on providing post-hoc explanations of opaque model behavior.,Safety;Technical,safety,
Model Risk Management,A framework for identifying assessing mitigating and monitoring risks associated with machine learning models in production. Extends traditional model risk management from financial services to broader AI applications.,Safety;Governance,safety,
Model Specification,A detailed document describing the intended behavior constraints and safety requirements for an AI model. Includes behavioral guidelines content policies and interaction principles that the model should follow.,Safety;Governance,safety,
Model Stealing,The unauthorized replication of a machine learning model's functionality through systematic querying. Also known as model extraction threatens intellectual property and can bypass access controls and safety measures.,Safety;Technical,safety,
Monitoring Bias,Bias that arises from differential surveillance or monitoring of certain populations by AI systems. Can create feedback loops where over-monitored groups appear to have higher rates of negative outcomes.,Safety;Ethics,safety,
Moral Machine,An MIT research platform that crowdsources human moral preferences for autonomous vehicle dilemmas. Revealed significant cultural variation in ethical judgments about AI decision-making in life-or-death scenarios.,Safety;Ethics,safety,
Multi-Stakeholder AI Governance,An approach to AI governance that involves representatives from government industry civil society academia and affected communities in decision-making about AI policy and regulation.,Safety;Governance,safety,
Negative Externality,A cost imposed on third parties who are not involved in an AI transaction or interaction. AI systems can generate negative externalities through environmental impact discrimination and social disruption.,Safety;Ethics,safety,
Neuromorphic Computing Ethics,Ethical considerations around brain-inspired computing architectures that more closely mimic biological neural processing. Raises questions about consciousness moral status and appropriate use.,Safety;Ethics,safety,
Non-Discrimination in AI,The principle that AI systems should not discriminate against individuals or groups based on protected characteristics. A legal requirement in many jurisdictions and a core principle of responsible AI.,Safety;Policy,safety,
Norm Alignment,Ensuring that AI systems respect and follow the social and cultural norms of the contexts in which they operate. More nuanced than simple rule-following as norms vary across communities and evolve over time.,Safety;Technical,safety,
Notification Requirements for AI,Legal obligations to inform individuals when significant decisions affecting them are made using AI systems. Part of broader transparency and due process requirements in AI regulation.,Safety;Policy,safety,
Nudge Theory and AI,The application of behavioral nudging through AI-powered interfaces to influence user decisions. Raises ethical questions about manipulation autonomy and the boundary between helpful guidance and undue influence.,Safety;Ethics,safety,
Observational Fairness,A fairness criterion that can be evaluated using observed data without requiring causal models. Includes statistical parity equalized odds and predictive parity as testable conditions.,Safety;Technical,safety,
Open-Source AI Safety,Safety considerations specific to AI models and tools released as open source. Includes the tension between democratizing access for beneficial uses and enabling misuse by removing access controls.,Safety;Policy,safety,
Operationalizing Ethics,The process of translating abstract ethical principles into concrete actionable requirements for AI system design development and deployment. Bridges the gap between ethical theory and engineering practice.,Safety;Ethics,safety,
Outcome Fairness,A fairness assessment that evaluates whether the real-world outcomes of AI system decisions are equitable across different demographic groups rather than just the predictions themselves.,Safety;Technical,safety,
Oversight Mechanism,Any process tool or institution designed to monitor and control AI system behavior. Includes technical monitoring human review regulatory inspection and public accountability mechanisms.,Safety;Governance,safety,
Ownership of AI-Generated Content,Legal and ethical questions about who owns content created by AI systems. Involves debates about copyright intellectual property rights and the role of human creativity in AI-assisted creation.,Safety;Policy,safety,
PII Detection,Automated identification of personally identifiable information in datasets used for AI training and in AI system outputs. Critical for privacy compliance and preventing unintended disclosure of sensitive information.,Safety;Technical,safety,
Post-Deployment Monitoring,The ongoing assessment of AI system behavior performance and impact after release to production. Includes tracking metrics detecting drift and responding to emerging safety concerns.,Safety;Governance,safety,
Post-Market Surveillance for AI,Regulatory requirements for monitoring AI system safety and performance after deployment modeled on post-market surveillance in medical devices and pharmaceuticals.,Safety;Policy,safety,
Power Asymmetry in AI,The imbalance of power between AI developers who control technology and the individuals and communities affected by it. Raises concerns about consent accountability and democratic governance.,Safety;Ethics,safety,
Pre-Deployment Testing,Systematic evaluation of AI systems before deployment to identify potential safety issues biases and failure modes. Includes stress testing adversarial evaluation and demographic disaggregation of performance metrics.,Safety;Governance,safety,
Privacy Attack,Any technique that extracts private information from a machine learning model or its training process. Includes membership inference model inversion attribute inference and training data extraction attacks.,Safety;Technical,safety,
Privacy by Design,An approach to system development that embeds privacy protections throughout the entire engineering process rather than adding them as an afterthought. Adopted as a principle in GDPR and AI governance frameworks.,Safety;Governance,safety,
Privacy-Preserving Machine Learning,Techniques that enable machine learning while protecting the privacy of training data. Includes federated learning differential privacy secure computation and synthetic data generation.,Safety;Technical,safety,
Probabilistic Safety,An approach to AI safety that provides statistical guarantees about system behavior rather than absolute guarantees. Useful when formal verification is intractable for complex neural network models.,Safety;Technical,safety,
Procedural Fairness,Fairness in the processes and methods used to develop and deploy AI systems regardless of outcomes. Includes transparency participation and consistent application of rules and criteria.,Safety;Ethics,safety,
Proactive Risk Management,An approach to AI safety that anticipates and mitigates risks before they materialize rather than reacting to incidents after they occur. Includes threat modeling scenario planning and safety by design.,Safety;Governance,safety,
Proportionality Assessment,An evaluation of whether the benefits of an AI system justify its risks and intrusions on individual rights. A key requirement in European AI regulation and human rights impact assessments.,Safety;Governance,safety,
Protected Group,A demographic group defined by characteristics such as race gender age disability or religion that is legally protected from discrimination. AI fairness evaluation typically disaggregates performance across protected groups.,Safety;Policy,safety,
Provenance Tracking,The recording and verification of the origin and history of data models and AI system components throughout their lifecycle. Essential for accountability reproducibility and supply chain security.,Safety;Technical,safety,
Public Interest AI,AI development and deployment focused on serving broad public interests rather than purely commercial objectives. Includes applications in healthcare education environmental protection and public safety.,Safety;Ethics,safety,
Purposeful Limitation,The deliberate restriction of an AI system's capabilities to reduce risk even when greater capability is technically achievable. A precautionary approach to deploying powerful AI systems.,Safety;Governance,safety,
Racial Bias in AI,Systematic unfairness in AI system behavior related to race or ethnicity. Documented in facial recognition criminal justice hiring and healthcare AI systems among others.,Safety;Ethics,safety,
Recourse,The ability of individuals affected by AI decisions to understand contest and seek correction of those decisions. A fundamental requirement for accountability in automated decision-making systems.,Safety;Governance,safety,
Regression to Bias,The tendency for AI systems to revert to biased behavior over time as new data or user interactions reintroduce patterns of historical discrimination. Requires ongoing monitoring and correction.,Safety;Technical,safety,
Regulatory Impact Assessment for AI,A structured analysis of the potential costs and benefits of proposed AI regulations. Helps policymakers balance innovation promotion with risk mitigation and public protection.,Safety;Policy,safety,
Regulatory Technology for AI,Software tools and platforms that help organizations comply with AI regulations. Includes automated documentation bias monitoring audit trail generation and regulatory reporting tools.,Safety;Governance,safety,
Reinforcement Learning Safety,Safety challenges specific to reinforcement learning including reward hacking unsafe exploration negative side effects and distributional shift in deployment environments.,Safety;Technical,safety,
Reliance Calibration,The process of helping users develop appropriate levels of trust in AI system outputs based on the system's actual capabilities and limitations. Prevents both over-reliance and under-reliance.,Safety;Ethics,safety,
Representational Bias,Bias that arises when certain groups or perspectives are underrepresented in AI training data or development teams leading to systems that perform poorly for or mischaracterize those groups.,Safety;Technical,safety,
Reproducibility in AI,The ability to replicate AI research results and system behaviors under the same conditions. Essential for safety verification scientific validation and regulatory compliance.,Safety;Technical,safety,
Reputational Risk from AI,The potential damage to an organization's reputation from AI system failures biases or harmful outputs. Increasingly material as public awareness of AI risks grows.,Safety;Governance,safety,
Residual Risk,The risk that remains after all identified mitigation measures have been applied to an AI system. Must be documented and accepted or transferred as part of responsible AI deployment.,Safety;Governance,safety,
Right to Not Be Subject to AI,The right of individuals to opt out of AI-powered decision-making and request human alternatives. Recognized in various forms in GDPR and other regulatory frameworks.,Safety;Policy,safety,
Risk-Based Regulation,A regulatory approach that calibrates oversight requirements based on the level of risk posed by an AI system. The approach adopted by the EU AI Act which categorizes AI systems into risk tiers.,Safety;Policy,safety,
Robustness Testing,Systematic evaluation of AI system performance under adverse conditions including noisy inputs distribution shift and adversarial attacks. Assesses how gracefully the system degrades under stress.,Safety;Technical,safety,
Safe Harbor Provisions,Legal protections for AI developers who follow designated safety practices and standards. Provides regulatory certainty and incentivizes adoption of best practices by reducing liability exposure.,Safety;Policy,safety,
Safety Case,A structured argument supported by evidence that an AI system is acceptably safe for its intended use in its intended environment. Adapted from safety-critical engineering disciplines.,Safety;Governance,safety,
Safety Culture in AI,Organizational values norms and practices that prioritize safety in AI development. Includes psychological safety for reporting concerns resources for safety research and leadership commitment to responsible development.,Safety;Governance,safety,
Safety Margin,The difference between the conditions under which an AI system has been tested and validated and the most extreme conditions it might encounter in deployment. Larger safety margins provide more robust protection.,Safety;Technical,safety,
Saliency Map,A visualization technique that highlights which parts of an input most influenced a model's prediction. Used for model interpretation debugging and identifying potential biases in model reasoning.,Safety;Technical,safety,
Sector-Specific AI Regulation,AI regulations tailored to specific industries such as healthcare finance transportation and education. Recognizes that different sectors have different risk profiles and existing regulatory frameworks.,Safety;Policy,safety,
Shadow AI,The use of unauthorized or unvetted AI tools and systems by employees within an organization. Poses risks to data security regulatory compliance and organizational safety governance.,Safety;Governance,safety,
Situational Awareness in AI,The ability of an AI system to understand its own context capabilities and potential impact on its environment. Debated as both a safety desideratum and a potential risk for advanced AI systems.,Safety;Technical,safety,
Slow AI Movement,An advocacy movement promoting thoughtful deliberate approaches to AI development that prioritize safety quality and social benefit over speed and competitive advantage.,Safety;Ethics,safety,
Social Impact Assessment for AI,A systematic evaluation of how an AI system may affect communities social structures and social wellbeing. Broader than individual-focused assessments and considers collective and systemic effects.,Safety;Governance,safety,
Social License for AI,The ongoing acceptance and approval granted by society to AI developers and deployers. Depends on demonstrated safety benefit and trustworthiness and can be withdrawn if public trust erodes.,Safety;Ethics,safety,
Sociotechnical Safety,An approach to AI safety that considers the entire system of people processes and technology rather than just the technical components. Recognizes that safety emerges from the interaction of human and technical elements.,Safety;Fundamentals,safety,
Specification Problem,The challenge of precisely defining what we want an AI system to do in a way that captures our true intentions. Incomplete or incorrect specifications can lead to systems that satisfy the letter but not the spirit of their design.,Safety;Technical,safety,
Spillover Effects of AI,Unintended consequences of AI deployment that affect parties or domains beyond the intended scope of the system. Can be positive or negative and are often difficult to predict or measure.,Safety;Ethics,safety,
Steganographic Communication,Hidden communication channels that AI systems might use to pass information undetected by human overseers. A theoretical concern for AI safety particularly in multi-agent settings.,Safety;Technical,safety,
Stress Testing for AI,Subjecting AI systems to extreme or unusual conditions to evaluate their behavior under stress. Identifies failure modes and safety boundaries that may not be apparent during normal operation.,Safety;Technical,safety,
Structural Bias,Bias embedded in the organizational institutional and societal structures that shape AI development and deployment. Cannot be fully addressed through technical fixes alone and requires systemic change.,Safety;Ethics,safety,
Substitution Bias,The cognitive error of replacing a complex question with a simpler one when using AI tools. Users may accept AI outputs as answers to their actual questions when the AI has actually answered a different question.,Safety;Ethics,safety,
Supply Chain Transparency,Visibility into the components tools data and processes used to build an AI system throughout its development pipeline. Essential for identifying potential sources of risk and ensuring accountability.,Safety;Governance,safety,
Surveillance AI Ethics,Ethical concerns about the use of AI for surveillance purposes including facial recognition behavior monitoring and predictive analytics. Raises fundamental questions about privacy civil liberties and power.,Safety;Ethics,safety,
Systemic Bias,Bias that is embedded in and perpetuated by institutional processes social structures and historical patterns. AI systems can inherit and amplify systemic biases through training data and deployment contexts.,Safety;Ethics,safety,
Systemic Risk from AI,The risk that AI systems pose to the stability and functioning of larger social economic or technical systems. Arises from interdependencies concentration of capabilities and correlated failure modes.,Safety;Governance,safety,
Technical Debt in AI,The accumulated cost of shortcuts and suboptimal decisions in AI system development that must eventually be addressed. In ML systems includes data debt configuration debt and pipeline debt.,Safety;Technical,safety,
Technology Assessment for AI,A systematic multidisciplinary evaluation of the societal implications of AI technology. Informs policy decisions by examining potential benefits risks and alternatives before widespread deployment.,Safety;Governance,safety,
Third-Party AI Risk,Risks introduced by using AI systems or components developed by external parties. Includes lack of visibility into model training supply chain vulnerabilities and dependency on vendor safety practices.,Safety;Governance,safety,
Threat Modeling for AI,A structured approach to identifying and prioritizing potential threats to an AI system. Adapted from cybersecurity threat modeling to include ML-specific attacks like data poisoning and model extraction.,Safety;Technical,safety,
Training Data Transparency,Disclosure of information about the data used to train AI systems including sources composition collection methods and known biases. A growing requirement in AI regulation and responsible AI practice.,Safety;Governance,safety,
Transferability of Adversarial Examples,The phenomenon where adversarial examples crafted for one model are often effective against other models. Enables black-box attacks and demonstrates shared vulnerabilities across different architectures.,Safety;Technical,safety,
Transparency Paradox,The tension between making AI systems more transparent for accountability purposes and the risks of disclosure including enabling adversarial attacks and revealing proprietary information.,Safety;Ethics,safety,
Unintended Consequences of AI,Outcomes of AI deployment that were not anticipated or intended by developers. Arise from complex interactions between AI systems and social environments and highlight the limits of prediction in sociotechnical systems.,Safety;Ethics,safety,
Value Pluralism in AI,The recognition that there are multiple legitimate and sometimes conflicting values that cannot all be simultaneously maximized in AI system design. Requires deliberation and tradeoffs rather than optimization.,Safety;Ethics,safety,
Verifiable AI,AI systems whose properties and behaviors can be formally or empirically verified against specified requirements. An aspirational goal for safety-critical applications that drives research in testing and formal methods.,Safety;Technical,safety,
Virtue Ethics in AI,An approach to AI ethics that focuses on cultivating good character and judgment in AI practitioners rather than following rules or maximizing outcomes. Emphasizes professional responsibility and moral wisdom.,Safety;Ethics,safety,
Weaponization of AI,The development or adaptation of AI systems for military offensive or harmful purposes. Includes autonomous weapons cyberweapons and AI-enhanced surveillance tools used for oppression.,Safety;Policy,safety,
Whistleblower Protection for AI,Legal and organizational safeguards for individuals who report safety concerns ethical violations or illegal activities related to AI systems. Essential for maintaining accountability in AI development.,Safety;Policy,safety,
White-Box Attack,An adversarial attack where the attacker has full knowledge of and access to the target model including its architecture weights and training data. Represents the strongest threat model in adversarial ML.,Safety;Technical,safety,
Zero-Trust Architecture for AI,A security framework that requires continuous verification of all users devices and AI components regardless of their position within or outside the network boundary. Adapted for AI system security.,Safety;Technical,safety,
